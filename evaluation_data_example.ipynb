{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e04b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhthuy/anaconda3/envs/chatbot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from call_model import call_chat_once\n",
    "import asyncio\n",
    "import httpx\n",
    "from uuid import uuid4\n",
    "import json\n",
    "import datasets\n",
    "from typing import Optional\n",
    "import os\n",
    "import csv\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b253fd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since m-ric/huggingface_doc_qa_eval couldn't be found on the Hugging Face Hub\n",
      "2025-08-13 17:25:46,310 - WARNING - Using the latest cached version of the dataset since m-ric/huggingface_doc_qa_eval couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/minhthuy/.cache/huggingface/datasets/m-ric___huggingface_doc_qa_eval/default/0.0.0/5f70aa9a1e2430f528ac3f27f01f0ba8719c0704 (last modified on Mon Aug 11 14:41:56 2025).\n",
      "2025-08-13 17:25:46,318 - WARNING - Found the latest cached dataset configuration 'default' at /home/minhthuy/.cache/huggingface/datasets/m-ric___huggingface_doc_qa_eval/default/0.0.0/5f70aa9a1e2430f528ac3f27f01f0ba8719c0704 (last modified on Mon Aug 11 14:41:56 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['context', 'question', 'answer', 'source_doc', 'standalone_score', 'standalone_eval', 'relatedness_score', 'relatedness_eval', 'relevance_score', 'relevance_eval'],\n",
      "    num_rows: 65\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\") # or load from data/test.csv\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62badec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"data/rag_results_default_as_retriever.csv\"\n",
    "\n",
    "def load_existing_results(file_path):\n",
    "    dataset = []\n",
    "    existing_questions = set()\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                existing_questions.add(row[\"user_input\"])\n",
    "                # parse retrieved_contexts l·∫°i t·ª´ JSON\n",
    "                row[\"retrieved_contexts\"] = json.loads(row[\"retrieved_contexts\"])\n",
    "                dataset.append(row)\n",
    "    return existing_questions, dataset\n",
    "\n",
    "\n",
    "def append_result_to_csv(file_path, row, fieldnames):\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    with open(file_path, \"a\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    verbose: Optional[bool] = False\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    fieldnames = [\"user_input\", \"retrieved_contexts\", \"response\", \"reference\"]\n",
    "\n",
    "    processed_questions, dataset = load_existing_results(OUTPUT_FILE)\n",
    "    print(f\"ƒê√£ c√≥ {len(processed_questions)} c√¢u h·ªèi x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥, s·∫Ω b·ªè qua ch√∫ng.\")\n",
    "\n",
    "\n",
    "    for i, example in enumerate(tqdm(eval_dataset)):\n",
    "        question = example[\"question\"]\n",
    "\n",
    "        if question in processed_questions:\n",
    "            continue\n",
    "        \n",
    "        payload = {\n",
    "            \"question\": question,\n",
    "            \"session_id\": str(uuid4()),\n",
    "            \"chat_history\": []\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            relevant_docs, answer = asyncio.run(call_chat_once(payload))\n",
    "        except Exception as e:\n",
    "            print(f\"Error from question: '{question}': {e}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "\n",
    "        row = {\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": json.dumps(\n",
    "                [doc['kwargs'].get('page_content') for doc in relevant_docs],\n",
    "                ensure_ascii=False\n",
    "            ),\n",
    "            \"response\": answer,\n",
    "            \"reference\": example['answer']\n",
    "        }\n",
    "        append_result_to_csv(OUTPUT_FILE, row, fieldnames)\n",
    "        dataset.append({\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": json.loads(row[\"retrieved_contexts\"]),\n",
    "            \"response\": answer,\n",
    "            \"reference\": example['answer']\n",
    "        })\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12bd2530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ c√≥ 0 c√¢u h·ªèi x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥, s·∫Ω b·ªè qua ch√∫ng.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/65 [00:00<?, ?it/s]2025-08-13 17:25:50,033 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      "  2%|‚ñè         | 1/65 [00:03<03:51,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What architecture is the `tokenizers-linux-x64-musl` binary designed for?\\n', 'type': 'human', 'id': '8be83939-ff1b-4cd9-bfb8-d771f2b64c66'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_iTUG5dqzmzRzV2QxuHLXepUd', 'function': {'arguments': '{\"query\":\"tokenizers-linux-x64-musl architecture\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--0a6bf289-050a-43ea-b8c5-ca9fff42ea01-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'tokenizers-linux-x64-musl architecture'}, 'id': 'call_iTUG5dqzmzRzV2QxuHLXepUd', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 23, 'total_tokens': 388, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '`tokenizers-linux-x64-musl`\\nThis is the \\\\*\\\\*x86\\\\_64-unknown-linux-musl\\\\*\\\\* binary for `tokenizers`\\n\\n`tokenizers-linux-arm64-musl`\\nThis is the \\\\*\\\\*aarch64-unknown-linux-musl\\\\*\\\\* binary for `tokenizers`\\n\\n`tokenizers-linux-arm64-gnu`\\nThis is the \\\\*\\\\*aarch64-unknown-linux-gnu\\\\*\\\\* binary for `tokenizers`\\n\\n`tokenizers-linux-x64-gnu`\\nThis is the \\\\*\\\\*x86\\\\_64-unknown-linux-gnu\\\\*\\\\* binary for `tokenizers`', 'type': 'tool', 'name': 'data_retriever', 'id': '0f21bc5b-2bb3-4fc2-934e-d89162a28f51', 'tool_call_id': 'call_iTUG5dqzmzRzV2QxuHLXepUd', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab0d33e684812d15f65a4', 'page_content': '`tokenizers-linux-x64-musl`\\nThis is the \\\\*\\\\*x86\\\\_64-unknown-linux-musl\\\\*\\\\* binary for `tokenizers`', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b506af3d2ebb270a0250', 'page_content': '`tokenizers-linux-arm64-musl`\\nThis is the \\\\*\\\\*aarch64-unknown-linux-musl\\\\*\\\\* binary for `tokenizers`', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaf8e3e684812d15f6453', 'page_content': '`tokenizers-linux-arm64-gnu`\\nThis is the \\\\*\\\\*aarch64-unknown-linux-gnu\\\\*\\\\* binary for `tokenizers`', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aabfc3e684812d15f6038', 'page_content': '`tokenizers-linux-x64-gnu`\\nThis is the \\\\*\\\\*x86\\\\_64-unknown-linux-gnu\\\\*\\\\* binary for `tokenizers`', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--87308c2f-9dfc-4631-bb63-180c171e8004-0', 'usage_metadata': {'input_tokens': 409, 'output_tokens': 28, 'total_tokens': 437, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:25:56,109 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      "  3%|‚ñé         | 2/65 [00:09<05:19,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of the BLIP-Diffusion model?\\n', 'type': 'human', 'id': 'cf0a1dd1-451c-4ba9-8a26-a76867c62d17'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_30gknCmeGQtVnVm3cQNcGgOw', 'function': {'arguments': '{\"query\":\"BLIP-Diffusion model purpose\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--795b3785-55a8-46de-86b7-f415bdb11009-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'BLIP-Diffusion model purpose'}, 'id': 'call_30gknCmeGQtVnVm3cQNcGgOw', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 361, 'output_tokens': 22, 'total_tokens': 383, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# BLIP-Diffusion\\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. The abstract from the paper is:\\n\\\\*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).\\\\*\\nThe original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization. `BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## BlipDiffusionPipeline\\n[[autodoc]] BlipDiffusionPipeline\\n- all\\n- \\\\_\\\\_call\\\\_\\\\_\\n## BlipDiffusionControlNetPipeline\\n[[autodoc]] BlipDiffusionControlNetPipeline\\n- all\\n- \\\\_\\\\_call\\\\_\\\\_\\n\\n--\\ntitle: The Annotated Diffusion Model\\nthumbnail: /blog/assets/78\\\\_annotated-diffusion/thumbnail.png\\nauthors:\\n- user: nielsr\\n- user: kashif\\n---\\n# The Annotated Diffusion Model\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb)\\nIn this blog post, we\\'ll take a deeper look into \\\\*\\\\*Denoising Diffusion Probabilistic Models\\\\*\\\\* (also known as DDPMs, diffusion models, score-based generative models or simply [autoencoders](https://benanne.github.io/2022/01/31/diffusion.html)) as researchers have been able to achieve remarkable results with them for (un)conditional image/audio/video generation. Popular examples (at the time of writing) include [GLIDE](https://arxiv.org/abs/2112.10741) and [DALL-E 2](https://openai.com/dall-e-2/) by OpenAI, [Latent Diffusion](https://github.com/CompVis/latent-diffusion) by the University of Heidelberg and [ImageGen](https://imagen.research.google/) by Google Brain. We\\'ll go over the original DDPM paper by ([Ho et al., 2020](https://arxiv.org/abs/2006.11239)), implementing it step-by-step in PyTorch, based on Phil Wang\\'s [implementation](https://github.com/lucidrains/denoising-diffusion-pytorch) - which itself is based on the [original TensorFlow implementation](https://github.com/hojonathanho/diffusion). Note that the idea of diffusion for generative modeling was actually already introduced in ([Sohl-Dickstein et al., 2015](https://arxiv.org/abs/1503.03585)). However, it took until ([Song et al., 2019](https://arxiv.org/abs/1907.05600)) (at Stanford University), and then ([Ho et al., 2020](https://arxiv.org/abs/2006.11239)) (at Google Brain) who independently improved the approach. Note that there are [several perspectives](https://twitter.com/sedielem/status/1530894256168222722?s=20&t=mfv4afx1GcNQU5fZklpACw) on diffusion models. Here, we employ the discrete-time (latent variable model) perspective, but be sure to check out the other perspectives as well. Alright, let\\'s dive in! ```python\\nfrom IPython.display import Image\\nImage(filename=\\'assets/78\\\\_annotated-diffusion/ddpm\\\\_paper.png\\')\\n```\\n\\n![](assets/78_annotated-diffusion/ddpm_paper.png)\\n\\nWe\\'ll install and import the required libraries first (assuming you have [PyTorch](https://pytorch.org/) installed). ```python\\n!pip install -q -U einops datasets matplotlib tqdm\\nimport math\\nfrom inspect import isfunction\\nfrom functools import partial\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nfrom tqdm.auto import tqdm\\nfrom einops import rearrange, reduce\\nfrom einops.layers.torch import Rearrange\\nimport torch\\nfrom torch import nn, einsum\\nimport torch.nn.functional as F\\n```\\n## What is a diffusion model?\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Text-to-image\\nThe Stable Diffusion model was created by researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/), [Runway](https://github.com/runwayml), and [LAION](https://laion.ai/). The [`StableDiffusionPipeline`] is capable of generating photorealistic images given any text input. It\\'s trained on 512x512 images from a subset of the LAION-5B dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs. Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://huggingface.co/papers/2112.10752) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj√∂rn Ommer. The abstract from the paper is:\\n\\\\*By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.\\n\\nüß® Diffusers Pipelines\\nPipelines provide a simple way to run state-of-the-art diffusion models in inference. Most diffusion systems consist of multiple independently-trained models and highly adaptable scheduler\\ncomponents - all of which are needed to have a functioning end-to-end diffusion system. As an example, [Stable Diffusion](https://huggingface.co/blog/stable\\\\_diffusion) has three independently trained models:\\n- [Autoencoder](https://github.com/huggingface/diffusers/blob/5cbed8e0d157f65d3ddc2420dfd09f2df630e978/src/diffusers/models/vae.py#L392)\\n- [Conditional Unet](https://github.com/huggingface/diffusers/blob/5cbed8e0d157f65d3ddc2420dfd09f2df630e978/src/diffusers/models/unet\\\\_2d\\\\_condition.py#L12)\\n- [CLIP text encoder](https://huggingface.co/docs/transformers/main/en/model\\\\_doc/clip#transformers.CLIPTextModel)\\n- a scheduler component, [scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling\\\\_pndm.py),\\n- a [CLIPImageProcessor](https://huggingface.co/docs/transformers/main/en/model\\\\_doc/clip#transformers.CLIPImageProcessor),\\n- as well as a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\\\_diffusion/safety\\\\_checker.py). All of these components are necessary to run stable diffusion in inference even though they were trained\\nor created independently from each other.', 'type': 'tool', 'name': 'data_retriever', 'id': 'f70da8bf-433b-43ac-9a0e-d579e98f6f04', 'tool_call_id': 'call_30gknCmeGQtVnVm3cQNcGgOw', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab90a3e684812d15f6f5c', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# BLIP-Diffusion\\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. The abstract from the paper is:\\n\\\\*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).\\\\*\\nThe original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization. `BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## BlipDiffusionPipeline\\n[[autodoc]] BlipDiffusionPipeline\\n- all\\n- \\\\_\\\\_call\\\\_\\\\_\\n## BlipDiffusionControlNetPipeline\\n[[autodoc]] BlipDiffusionControlNetPipeline\\n- all\\n- \\\\_\\\\_call\\\\_\\\\_', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899be25666af48523f51c27', 'page_content': \"--\\ntitle: The Annotated Diffusion Model\\nthumbnail: /blog/assets/78\\\\_annotated-diffusion/thumbnail.png\\nauthors:\\n- user: nielsr\\n- user: kashif\\n---\\n# The Annotated Diffusion Model\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb)\\nIn this blog post, we'll take a deeper look into \\\\*\\\\*Denoising Diffusion Probabilistic Models\\\\*\\\\* (also known as DDPMs, diffusion models, score-based generative models or simply [autoencoders](https://benanne.github.io/2022/01/31/diffusion.html)) as researchers have been able to achieve remarkable results with them for (un)conditional image/audio/video generation. Popular examples (at the time of writing) include [GLIDE](https://arxiv.org/abs/2112.10741) and [DALL-E 2](https://openai.com/dall-e-2/) by OpenAI, [Latent Diffusion](https://github.com/CompVis/latent-diffusion) by the University of Heidelberg and [ImageGen](https://imagen.research.google/) by Google Brain. We'll go over the original DDPM paper by ([Ho et al., 2020](https://arxiv.org/abs/2006.11239)), implementing it step-by-step in PyTorch, based on Phil Wang's [implementation](https://github.com/lucidrains/denoising-diffusion-pytorch) - which itself is based on the [original TensorFlow implementation](https://github.com/hojonathanho/diffusion). Note that the idea of diffusion for generative modeling was actually already introduced in ([Sohl-Dickstein et al., 2015](https://arxiv.org/abs/1503.03585)). However, it took until ([Song et al., 2019](https://arxiv.org/abs/1907.05600)) (at Stanford University), and then ([Ho et al., 2020](https://arxiv.org/abs/2006.11239)) (at Google Brain) who independently improved the approach. Note that there are [several perspectives](https://twitter.com/sedielem/status/1530894256168222722?s=20&t=mfv4afx1GcNQU5fZklpACw) on diffusion models. Here, we employ the discrete-time (latent variable model) perspective, but be sure to check out the other perspectives as well. Alright, let's dive in! ```python\\nfrom IPython.display import Image\\nImage(filename='assets/78\\\\_annotated-diffusion/ddpm\\\\_paper.png')\\n```\\n\\n![](assets/78_annotated-diffusion/ddpm_paper.png)\\n\\nWe'll install and import the required libraries first (assuming you have [PyTorch](https://pytorch.org/) installed). ```python\\n!pip install -q -U einops datasets matplotlib tqdm\\nimport math\\nfrom inspect import isfunction\\nfrom functools import partial\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nfrom tqdm.auto import tqdm\\nfrom einops import rearrange, reduce\\nfrom einops.layers.torch import Rearrange\\nimport torch\\nfrom torch import nn, einsum\\nimport torch.nn.functional as F\\n```\\n## What is a diffusion model?\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab24b3e684812d15f6765', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Text-to-image\\nThe Stable Diffusion model was created by researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/), [Runway](https://github.com/runwayml), and [LAION](https://laion.ai/). The [`StableDiffusionPipeline`] is capable of generating photorealistic images given any text input. It\\'s trained on 512x512 images from a subset of the LAION-5B dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs. Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://huggingface.co/papers/2112.10752) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj√∂rn Ommer. The abstract from the paper is:\\n\\\\*By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aac503e684812d15f6098', 'page_content': 'üß® Diffusers Pipelines\\nPipelines provide a simple way to run state-of-the-art diffusion models in inference. Most diffusion systems consist of multiple independently-trained models and highly adaptable scheduler\\ncomponents - all of which are needed to have a functioning end-to-end diffusion system. As an example, [Stable Diffusion](https://huggingface.co/blog/stable\\\\_diffusion) has three independently trained models:\\n- [Autoencoder](https://github.com/huggingface/diffusers/blob/5cbed8e0d157f65d3ddc2420dfd09f2df630e978/src/diffusers/models/vae.py#L392)\\n- [Conditional Unet](https://github.com/huggingface/diffusers/blob/5cbed8e0d157f65d3ddc2420dfd09f2df630e978/src/diffusers/models/unet\\\\_2d\\\\_condition.py#L12)\\n- [CLIP text encoder](https://huggingface.co/docs/transformers/main/en/model\\\\_doc/clip#transformers.CLIPTextModel)\\n- a scheduler component, [scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling\\\\_pndm.py),\\n- a [CLIPImageProcessor](https://huggingface.co/docs/transformers/main/en/model\\\\_doc/clip#transformers.CLIPImageProcessor),\\n- as well as a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\\\_diffusion/safety\\\\_checker.py). All of these components are necessary to run stable diffusion in inference even though they were trained\\nor created independently from each other.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation in text-to-image generation tasks. It creates novel renditions of an input subject based on text prompts while overcoming limitations such as lengthy fine-tuning and difficulties in preserving subject fidelity. BLIP-Diffusion introduces a new multimodal encoder pre-trained to provide subject representation, allowing for efficient fine-tuning and flexible combinations with existing techniques for novel subject-driven generation and editing applications.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--618a29f0-53b7-41c2-aa71-f7fb99e723b0-0', 'usage_metadata': {'input_tokens': 2668, 'output_tokens': 100, 'total_tokens': 2768, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:25:59,924 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      "  5%|‚ñç         | 3/65 [00:13<04:38,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'How can a user claim authorship of a paper on the Hugging Face Hub?\\n', 'type': 'human', 'id': 'a12bf66d-284e-457c-9724-1d363d482209'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_3rQj6FAHCWRmaNoX5oLFaRjN', 'function': {'arguments': '{\"query\":\"claim authorship of a paper on the Hugging Face Hub\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--b10f8c01-5c9d-43a3-9488-21d03c1191c7-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'claim authorship of a paper on the Hugging Face Hub'}, 'id': 'call_3rQj6FAHCWRmaNoX5oLFaRjN', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 27, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Paper Pages\\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png)\\n\\n## Linking a Paper to a model, dataset or Space\\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository\\'s tags. Clicking on the arxiv tag will let you:\\n\\\\* Visit the Paper page. \\\\* Filter for other models or datasets on the Hub that cite the same paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\\n\\n## Claiming authorship to a Paper\\nThe Hub will attempt to automatically match paper to users based on their email. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png)\\n\\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\".\\n\\n!---\\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n# How to contribute to huggingface\\\\_hub, the GitHub repository? Everyone is welcome to contribute, and we value everybody\\'s contribution. Code is not the only way to help the community. Answering questions, helping others, reaching out and improving the documentations are immensely valuable to the community. It also helps us if you spread the word: reference the library from blog posts\\non the awesome projects it made possible, shout out on Twitter every time it has\\nhelped you, or simply star the repo to say \"thank you\". Whichever way you choose to contribute, please be mindful to respect our\\n[code of conduct](https://github.com/huggingface/huggingface\\\\_hub/blob/main/CODE\\\\_OF\\\\_CONDUCT.md). > Looking for a good first issue to work on? > Please check out our contributing guide below and then select an issue from our [curated list](https://github.com/huggingface/huggingface\\\\_hub/contribute). > Pick one and get started with it! ### The client library, `huggingface\\\\_hub`\\nThis repository hosts the `huggingface\\\\_hub`, the client library that interfaces any Python script with the Hugging Face Hub. Its implementation lives in `src/huggingface\\\\_hub` while the tests are located in `tests/`. There are many ways you can contribute to this client library:\\n\\\\* Fixing outstanding issues with the existing code;\\n\\\\* Contributing to the examples or to the documentation;\\n\\\\* Submitting issues related to bugs or desired new features.\\n\\n| Partially handled by `huggingface\\\\_hub`. |\\n\\np align=\"center\">\\n\\n![huggingface_hub library logo](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingface_hub.svg)\\n\\n*The official Python client for the Huggingface Hub.*\\n\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc)](https://huggingface.co/docs/huggingface_hub/en/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub/releases)\\n[![PyPi version](https://img.shields.io/pypi/pyversions/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub)\\n[![downloads](https://static.pepy.tech/badge/huggingface_hub/month)](https://pypi.org/project/huggingface-hub)\\n[![Code coverage](https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL)](https://codecov.io/gh/huggingface/huggingface_hub)\\n\\n#### **English** | [Deutsch](https://github.com/huggingface/huggingface_hub/blob/main/README_de.md) | [‡§π‡§ø‡§Ç‡§¶‡•Ä](https://github.com/huggingface/huggingface_hub/blob/main/README_hi.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/huggingface_hub/blob/main/README_ko.md) | [‰∏≠ÊñáÔºàÁÆÄ‰ΩìÔºâ](https://github.com/huggingface/huggingface_hub/blob/main/README_cn.md)\\n\\n---\\n\\\\*\\\\*Documentation\\\\*\\\\*: <https://hf.co/docs/huggingface_hub>\\n\\\\*\\\\*Source Code\\\\*\\\\*: <https://github.com/huggingface/huggingface_hub>\\n---\\n## Welcome to the huggingface\\\\_hub library\\nThe `huggingface\\\\_hub` library allows you to interact with the [Hugging Face Hub](https://huggingface.co/), a platform democratizing open-source Machine Learning for creators and collaborators.', 'type': 'tool', 'name': 'data_retriever', 'id': '9aa87493-0408-47a5-95ee-fcc00f68f70e', 'tool_call_id': 'call_3rQj6FAHCWRmaNoX5oLFaRjN', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab6613e684812d15f6c54', 'page_content': 'Paper Pages\\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png)\\n\\n## Linking a Paper to a model, dataset or Space\\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository\\'s tags. Clicking on the arxiv tag will let you:\\n\\\\* Visit the Paper page. \\\\* Filter for other models or datasets on the Hub that cite the same paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\\n\\n## Claiming authorship to a Paper\\nThe Hub will attempt to automatically match paper to users based on their email. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png)\\n\\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\".', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b3edaf3d2ebb270a0128', 'page_content': '!---\\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n# How to contribute to huggingface\\\\_hub, the GitHub repository? Everyone is welcome to contribute, and we value everybody\\'s contribution. Code is not the only way to help the community. Answering questions, helping others, reaching out and improving the documentations are immensely valuable to the community. It also helps us if you spread the word: reference the library from blog posts\\non the awesome projects it made possible, shout out on Twitter every time it has\\nhelped you, or simply star the repo to say \"thank you\". Whichever way you choose to contribute, please be mindful to respect our\\n[code of conduct](https://github.com/huggingface/huggingface\\\\_hub/blob/main/CODE\\\\_OF\\\\_CONDUCT.md). > Looking for a good first issue to work on? > Please check out our contributing guide below and then select an issue from our [curated list](https://github.com/huggingface/huggingface\\\\_hub/contribute). > Pick one and get started with it! ### The client library, `huggingface\\\\_hub`\\nThis repository hosts the `huggingface\\\\_hub`, the client library that interfaces any Python script with the Hugging Face Hub. Its implementation lives in `src/huggingface\\\\_hub` while the tests are located in `tests/`. There are many ways you can contribute to this client library:\\n\\\\* Fixing outstanding issues with the existing code;\\n\\\\* Contributing to the examples or to the documentation;\\n\\\\* Submitting issues related to bugs or desired new features.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaeb93e684812d15f637d', 'page_content': '| Partially handled by `huggingface\\\\_hub`. |', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bd7baf3d2ebb270a0c42', 'page_content': 'p align=\"center\">\\n\\n![huggingface_hub library logo](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingface_hub.svg)\\n\\n*The official Python client for the Huggingface Hub.*\\n\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc)](https://huggingface.co/docs/huggingface_hub/en/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub/releases)\\n[![PyPi version](https://img.shields.io/pypi/pyversions/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub)\\n[![downloads](https://static.pepy.tech/badge/huggingface_hub/month)](https://pypi.org/project/huggingface-hub)\\n[![Code coverage](https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL)](https://codecov.io/gh/huggingface/huggingface_hub)\\n\\n#### **English** | [Deutsch](https://github.com/huggingface/huggingface_hub/blob/main/README_de.md) | [‡§π‡§ø‡§Ç‡§¶‡•Ä](https://github.com/huggingface/huggingface_hub/blob/main/README_hi.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/huggingface_hub/blob/main/README_ko.md) | [‰∏≠ÊñáÔºàÁÆÄ‰ΩìÔºâ](https://github.com/huggingface/huggingface_hub/blob/main/README_cn.md)\\n\\n---\\n\\\\*\\\\*Documentation\\\\*\\\\*: <https://hf.co/docs/huggingface_hub>\\n\\\\*\\\\*Source Code\\\\*\\\\*: <https://github.com/huggingface/huggingface_hub>\\n---\\n## Welcome to the huggingface\\\\_hub library\\nThe `huggingface\\\\_hub` library allows you to interact with the [Hugging Face Hub](https://huggingface.co/), a platform democratizing open-source Machine Learning for creators and collaborators.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'A user can claim authorship of a paper on the Hugging Face Hub by clicking on their name in the corresponding Paper page and then selecting the \"claim authorship\" option. The Hub will also attempt to automatically match papers to users based on their email.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--52c7a67d-acf7-4e65-8348-85cfa16e953d-0', 'usage_metadata': {'input_tokens': 1598, 'output_tokens': 52, 'total_tokens': 1650, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:26:03,182 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      "  6%|‚ñå         | 4/65 [00:16<04:04,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of the /healthcheck endpoint in the Datasets server API?\\n', 'type': 'human', 'id': 'cea00673-1ed3-4884-8659-5a71cb35cb1d'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_0XIV6KxRIsT64nS3iRsUyIqh', 'function': {'arguments': '{\"query\":\"/healthcheck endpoint in the Datasets server API\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--55f4fd60-7f9b-4c70-8250-60f67610afb6-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': '/healthcheck endpoint in the Datasets server API'}, 'id': 'call_0XIV6KxRIsT64nS3iRsUyIqh', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 25, 'total_tokens': 390, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Datasets server API - rows endpoint\\n> /rows endpoint\\n## Configuration\\nThe service can be configured using environment variables. They are grouped by scope. ### API service\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\\nSee https://huggingface.co/docs/datasets-server\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /rows: get a slice of rows of a dataset split\\n\\nDatasets server API\\n> API on ü§ó datasets\\n## Configuration\\nThe service can be configured using environment variables. They are grouped by scope. ### API service\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\\nSee https://huggingface.co/docs/datasets-server\\n- /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /webhook: Add, update or remove a dataset\\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first\\\\_rows) for a dataset split\\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset\\n\\nCheck dataset validity\\nBefore you download a dataset from the Hub, it is helpful to know if a specific dataset you\\'re interested in is available. Datasets Server provides the `/is-valid` endpoint to check if a specific dataset works without any errors. The API endpoint will return an error for datasets that cannot be loaded with the [ü§ó Datasets](https://github.com/huggingface/datasets) library, for example, because the data hasn\\'t been uploaded or the format is not supported. The largest datasets are partially supported by Datasets Server. If they are{\" \"}\\n[streamable](https://huggingface.co/docs/datasets/stream), Datasets\\nServer can extract the first 100 rows without downloading the whole dataset. This is especially useful for previewing large datasets where downloading the\\nwhole dataset may take hours! See the `preview` field in the\\nresponse of `/is-valid` to check if a dataset is partially\\nsupported. This guide shows you how to check dataset validity programmatically, but free to try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-17b761d0-b2b8-4638-a4f7-73be9049c324), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/isValidDataset). ## Check if a dataset is valid\\n`/is-valid` checks whether a specific dataset loads without any error. This endpoint\\'s query parameter requires you to specify the name of the dataset:\\n\\n```python\\nimport requests\\nheaders = {\"Authorization\": f\"Bearer {API\\\\_TOKEN}\"}\\nAPI\\\\_URL = \"https://datasets-server.huggingface.co/is-valid?dataset=rotten\\\\_tomatoes\"\\ndef query():\\nresponse = requests.get(API\\\\_URL, headers=headers)\\nreturn response.json()\\ndata = query()\\n```\\n\\n```js\\nimport fetch from \"node-fetch\";\\nasync function query(data) {\\nconst response = await fetch(\\n\"https://datasets-server.huggingface.co/is-valid?dataset=rotten\\\\_tomatoes\",\\n{\\nheaders: { Authorization: `Bearer ${API\\\\_TOKEN}` },\\nmethod: \"GET\"\\n}\\n);\\nconst result = await response.json();\\nreturn result;\\n}\\nquery().then((response) => {\\nconsole.log(JSON.stringify(response));\\n});\\n```\\n\\n```curl\\ncurl https://datasets-server.huggingface.co/is-valid?dataset=rotten\\\\_tomatoes \\\\\\n-X GET \\\\\\n-H \"Authorization: Bearer ${API\\\\_TOKEN}\"\\n```\\nThe response looks like this if a dataset is valid:\\n```json\\n{\\n\"viewer\": true,\\n\"preview\": true\\n}\\n```\\nIf only the first rows of a dataset are available, then the response looks like:\\n```json\\n{\\n\"viewer\": false,\\n\"preview\": true\\n}\\n```\\nFinally, if the dataset is not valid at all, then the response is:\\n```json\\n{\\n\"viewer\": false,\\n\"preview\": false\\n}\\n```\\nSome cases where a dataset is not valid are:\\n- the dataset viewer is disabled\\n- the dataset is gated but the access is not granted: no token is passed or the passed token is not authorized\\n- the dataset is private\\n- the dataset contains no data or the data format is not supported\\nRemember if a dataset is [gated](./quick_start#gated-datasets),\\nyou\\'ll need to provide your user token to submit a successful query!\\n\\nDatasets server API - search service\\n> /search endpoint\\n> /filter endpoint\\n## Configuration\\nThe service can be configured using environment variables. They are grouped by scope. ### Duckdb index full text search\\n- `DUCKDB\\\\_INDEX\\\\_CACHE\\\\_DIRECTORY`: directory where the temporal duckdb index files are downloaded. Defaults to empty. - `DUCKDB\\\\_INDEX\\\\_TARGET\\\\_REVISION`: the git revision of the dataset where the index file is stored in the dataset repository. ### API service\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\\nSee https://huggingface.co/docs/datasets-server\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /search: get a slice of a search result over a dataset split\\n- /filter: filter rows of a dataset split', 'type': 'tool', 'name': 'data_retriever', 'id': '26c29b81-2a4c-4a9e-be0a-64a6d55c378f', 'tool_call_id': 'call_0XIV6KxRIsT64nS3iRsUyIqh', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab5983e684812d15f6b67', 'page_content': 'Datasets server API - rows endpoint\\n> /rows endpoint\\n## Configuration\\nThe service can be configured using environment variables. They are grouped by scope. ### API service\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\\nSee https://huggingface.co/docs/datasets-server\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /rows: get a slice of rows of a dataset split', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aba4b3e684812d15f70be', 'page_content': 'Datasets server API\\n> API on ü§ó datasets\\n## Configuration\\nThe service can be configured using environment variables. They are grouped by scope. ### API service\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\\nSee https://huggingface.co/docs/datasets-server\\n- /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /webhook: Add, update or remove a dataset\\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first\\\\_rows) for a dataset split\\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899ba05af3d2ebb270a0858', 'page_content': 'Check dataset validity\\nBefore you download a dataset from the Hub, it is helpful to know if a specific dataset you\\'re interested in is available. Datasets Server provides the `/is-valid` endpoint to check if a specific dataset works without any errors. The API endpoint will return an error for datasets that cannot be loaded with the [ü§ó Datasets](https://github.com/huggingface/datasets) library, for example, because the data hasn\\'t been uploaded or the format is not supported. The largest datasets are partially supported by Datasets Server. If they are{\" \"}\\n[streamable](https://huggingface.co/docs/datasets/stream), Datasets\\nServer can extract the first 100 rows without downloading the whole dataset. This is especially useful for previewing large datasets where downloading the\\nwhole dataset may take hours! See the `preview` field in the\\nresponse of `/is-valid` to check if a dataset is partially\\nsupported. This guide shows you how to check dataset validity programmatically, but free to try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-17b761d0-b2b8-4638-a4f7-73be9049c324), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/isValidDataset). ## Check if a dataset is valid\\n`/is-valid` checks whether a specific dataset loads without any error. This endpoint\\'s query parameter requires you to specify the name of the dataset:\\n\\n```python\\nimport requests\\nheaders = {\"Authorization\": f\"Bearer {API\\\\_TOKEN}\"}\\nAPI\\\\_URL = \"https://datasets-server.huggingface.co/is-valid?dataset=rotten\\\\_tomatoes\"\\ndef query():\\nresponse = requests.get(API\\\\_URL, headers=headers)\\nreturn response.json()\\ndata = query()\\n```\\n\\n```js\\nimport fetch from \"node-fetch\";\\nasync function query(data) {\\nconst response = await fetch(\\n\"https://datasets-server.huggingface.co/is-valid?dataset=rotten\\\\_tomatoes\",\\n{\\nheaders: { Authorization: `Bearer ${API\\\\_TOKEN}` },\\nmethod: \"GET\"\\n}\\n);\\nconst result = await response.json();\\nreturn result;\\n}\\nquery().then((response) => {\\nconsole.log(JSON.stringify(response));\\n});\\n```\\n\\n```curl\\ncurl https://datasets-server.huggingface.co/is-valid?dataset=rotten\\\\_tomatoes \\\\\\n-X GET \\\\\\n-H \"Authorization: Bearer ${API\\\\_TOKEN}\"\\n```\\nThe response looks like this if a dataset is valid:\\n```json\\n{\\n\"viewer\": true,\\n\"preview\": true\\n}\\n```\\nIf only the first rows of a dataset are available, then the response looks like:\\n```json\\n{\\n\"viewer\": false,\\n\"preview\": true\\n}\\n```\\nFinally, if the dataset is not valid at all, then the response is:\\n```json\\n{\\n\"viewer\": false,\\n\"preview\": false\\n}\\n```\\nSome cases where a dataset is not valid are:\\n- the dataset viewer is disabled\\n- the dataset is gated but the access is not granted: no token is passed or the passed token is not authorized\\n- the dataset is private\\n- the dataset contains no data or the data format is not supported\\nRemember if a dataset is [gated](./quick_start#gated-datasets),\\nyou\\'ll need to provide your user token to submit a successful query!', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689abb063e684812d15f718b', 'page_content': 'Datasets server API - search service\\n> /search endpoint\\n> /filter endpoint\\n## Configuration\\nThe service can be configured using environment variables. They are grouped by scope. ### Duckdb index full text search\\n- `DUCKDB\\\\_INDEX\\\\_CACHE\\\\_DIRECTORY`: directory where the temporal duckdb index files are downloaded. Defaults to empty. - `DUCKDB\\\\_INDEX\\\\_TARGET\\\\_REVISION`: the git revision of the dataset where the index file is stored in the dataset repository. ### API service\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\\nSee https://huggingface.co/docs/datasets-server\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /search: get a slice of a search result over a dataset split\\n- /filter: filter rows of a dataset split', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the app is running.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--302143d2-8e22-4cf1-863c-cfda321de374-0', 'usage_metadata': {'input_tokens': 1679, 'output_tokens': 23, 'total_tokens': 1702, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:26:06,493 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      "  8%|‚ñä         | 5/65 [00:20<03:45,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the default context window size for Local Attention in the LongT5 model?\\n', 'type': 'human', 'id': '27ce1a67-f9b0-4286-a60c-6b7033764326'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_Q178fXcyNaDITxlNCvA0ukEm', 'function': {'arguments': '{\"query\":\"default context window size for Local Attention in LongT5 model\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--c32704e3-39b1-4ae2-925b-1bd654c70dd1-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'default context window size for Local Attention in LongT5 model'}, 'id': 'call_Q178fXcyNaDITxlNCvA0ukEm', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 27, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# LongT5\\n## Overview\\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It\\'s an\\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\\nTransient-Global attention. The abstract from the paper is the following:\\n\\\\*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\\\em Transient Global}\\n(TGlobal), which mimics ETC\\'s local/global attention mechanism, but without requiring additional side-inputs. We are\\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\\nquestion answering tasks.\\\\*\\nThis model was contributed by [stancld](https://huggingface.co/stancld). The original code can be found [here](https://github.com/google-research/longt5). ## Usage tips\\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\\nencoder \\\\*self-attention\\\\* layer with efficient either \\\\*local\\\\* attention or \\\\*transient-global\\\\* (\\\\*tglobal\\\\*) attention. - Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\\ninspired by the pre-training of [`PegasusForConditionalGeneration`]. - LongT5 model is designed to work efficiently and very well on long-range \\\\*sequence-to-sequence\\\\* tasks where the\\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens. - For \\\\*Local Attention\\\\*, the sparse sliding-window local attention operation allows a given token to attend only `r`\\ntokens to the left and right of it (with `r=127` by default). \\\\*Local Attention\\\\* does not introduce any new parameters\\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l\\\\*r)`. - \\\\*Transient Global Attention\\\\* is an extension of the \\\\*Local Attention\\\\*. It, furthermore, allows each input token to\\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\\nalso every global token like in the case of standard global attention (\\\\*transient\\\\* represents the fact the global tokens\\nare constructed dynamically within each attention operation). As a consequence, \\\\*TGlobal\\\\* attention introduces\\na few new parameters -- global relative position biases and a layer normalization for global token\\'s embedding.\\n\\nsummarization). The memory consumption is thus reduced from \\\\\\\\(n\\\\_s^2+ n\\\\_s n\\\\_t +n\\\\_t^2\\\\\\\\) to \\\\\\\\(w n\\\\_s +gn\\\\_s +n\\\\_s n\\\\_t +n\\\\_t^2\\\\\\\\) with \\\\\\\\(n\\\\_s\\\\\\\\) and \\\\\\\\(n\\\\_t\\\\\\\\) being the source (encoder input) and target (decoder input) lengths respectively. For Longformer Encoder-Decoder to be efficient, it is assumed that \\\\\\\\(n\\\\_s\\\\\\\\) is much bigger than \\\\\\\\(n\\\\_t\\\\\\\\). #### Main findings\\n\\\\* The authors proposed the dilated windowed self-attention (Figure c) and showed that it yields better results on language modeling compared to just windowed/sparse self-attention (Figure b). The window sizes are increased through the layers. This pattern further outperforms previous architectures (such as Transformer-XL, or adaptive span attention) on downstream benchmarks. \\\\* Global attention allows the information to flow through the whole sequence and applying the global attention to task-motivated tokens (such as the tokens of the question in QA, CLS token for sentence classification) leads to stronger performance on downstream tasks. Using this global pattern, Longformer can be successfully applied to document-level NLP tasks in the transfer learning setting.\\n\\nThe Authors\\' code can be found [here](https://github.com/allenai/longformer). ## Usage tips\\n- Since the Longformer is based on RoBERTa, it doesn\\'t have `token\\\\_type\\\\_ids`. You don\\'t need to indicate which\\ntoken belongs to which segment. Just separate your segments with the separation token `tokenizer.sep\\\\_token` (or\\n``). - A transformer model replacing the attention matrices by sparse matrices to go faster. Often, the local context (e.g., what are the two tokens left and right?) is enough to take action for a given token. Some preselected input tokens are still given global attention, but the attention matrix has way less parameters, resulting in a speed-up. See the local attention section for more information. ## Longformer Self Attention\\nLongformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\\nattend \"locally\" to each other meaning that each token attends to its \\\\\\\\(\\\\frac{1}{2} w\\\\\\\\) previous tokens and\\n\\\\\\\\(\\\\frac{1}{2} w\\\\\\\\) succeeding tokens with \\\\\\\\(w\\\\\\\\) being the window length as defined in\\n`config.attention\\\\_window`. Note that `config.attention\\\\_window` can be of type `List` to define a\\ndifferent \\\\\\\\(w\\\\\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\\nconventionally done for all tokens in `BertSelfAttention`. Note that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\\nthat every \"locally\" attending token not only attends to tokens within its window \\\\\\\\(w\\\\\\\\), but also to all \"globally\"\\nattending tokens so that global attention is \\\\*symmetric\\\\*. The user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\\n`global\\\\_attention\\\\_mask` at run-time appropriately. All Longformer models employ the following logic for\\n`global\\\\_attention\\\\_mask`:\\n- 0: the token attends \"locally\",\\n- 1: the token attends \"globally\". For more information please also refer to [`~LongformerModel.forward`] method. Using Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\\nrepresents the memory and time bottleneck, can be reduced from \\\\\\\\(\\\\mathcal{O}(n\\\\_s \\\\times n\\\\_s)\\\\\\\\) to\\n\\\\\\\\(\\\\mathcal{O}(n\\\\_s \\\\times w)\\\\\\\\), with \\\\\\\\(n\\\\_s\\\\\\\\) being the sequence length and \\\\\\\\(w\\\\\\\\) being the average window\\nsize.\\n\\nThe first drawback of this architecture becomes obvious: Some input vectors have no access to their immediate context, \\\\*e.g.\\\\* \\\\\\\\(\\\\mathbf{x}\\\\_9\\\\\\\\) has no access to \\\\\\\\(\\\\mathbf{x}\\\\_{8}\\\\\\\\) and vice-versa in our example. This is problematic because these tokens are not able to learn word representations that take their immediate context into account. A simple remedy is to augment each chunk with `config.local\\\\_num\\\\_chunks\\\\_before`, \\\\*i.e.\\\\* \\\\\\\\(n\\\\_{p}\\\\\\\\), chunks and `config.local\\\\_num\\\\_chunks\\\\_after`, \\\\*i.e.\\\\* \\\\\\\\(n\\\\_{a}\\\\\\\\), so that every input vector has at least access to \\\\\\\\(n\\\\_{p}\\\\\\\\) previous input vectors and \\\\\\\\(n\\\\_{a}\\\\\\\\) following input vectors. This can also be understood as chunking with overlap whereas \\\\\\\\(n\\\\_{p}\\\\\\\\) and \\\\\\\\(n\\\\_{a}\\\\\\\\) define the amount of overlap each chunk has with all previous chunks and following chunks. We denote this extended local self-attention as follows:\\n$$\\\\mathbf{Z}^{\\\\text{loc}} = \\\\left[\\\\mathbf{Z}\\\\_{1:l\\\\_{c}}^{\\\\text{loc}}, \\\\ldots, \\\\mathbf{Z}\\\\_{(n\\\\_{c} - 1) \\\\* l\\\\_{c} : n\\\\_{c} \\\\* l\\\\_{c}}^{\\\\text{loc}}\\\\right], $$\\nwith\\n$$\\\\mathbf{Z}\\\\_{l\\\\_{c} \\\\* (i - 1) + 1 : l\\\\_{c} \\\\* i}^{\\\\text{loc}} = \\\\text{SelfAttn}(\\\\mathbf{X}\\\\_{l\\\\_{c} \\\\* (i - 1 - n\\\\_{p}) + 1: l\\\\_{c} \\\\* (i + n\\\\_{a})})\\\\left[n\\\\_{p} \\\\* l\\\\_{c}: -n\\\\_{a} \\\\* l\\\\_{c}\\\\right], \\\\forall i \\\\in \\\\{1, \\\\ldots, n\\\\_{c} \\\\}$$\\nOkay, this formula looks quite complicated. Let\\'s make it easier. In Reformer\\'s self-attention layers \\\\\\\\(n\\\\_{a}\\\\\\\\) is usually set to 0 and \\\\\\\\(n\\\\_{p}\\\\\\\\) is set to 1, so let\\'s write down the formula again for \\\\\\\\(i = 1\\\\\\\\):\\n$$\\\\mathbf{Z}\\\\_{1:l\\\\_{c}}^{\\\\text{loc}} = \\\\text{SelfAttn}(\\\\mathbf{X}\\\\_{-l\\\\_{c} + 1: l\\\\_{c}})\\\\left[l\\\\_{c}:\\\\right]$$\\nWe notice that we have a circular relationship so that the first segment can attend the last segment as well. Let\\'s illustrate this slightly enhanced local attention again. First, we apply self-attention within each windowed segment and keep only the central output segment. ![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific\\\\_images/master/reformer\\\\_benchmark/local\\\\_attention\\\\_2.png)\\nFinally, the relevant output is concatenated to \\\\\\\\(\\\\mathbf{Z}^{\\\\text{loc}}\\\\\\\\) and looks as follows. ![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific\\\\_images/master/reformer\\\\_benchmark/local\\\\_attention\\\\_3.png)\\nNote that local self-attention is implemented efficiently way so that no output is computed and subsequently \"thrown-out\" as shown here for illustration purposes by the red cross. It\\'s important to note here that extending the input vectors for each chunked self-attention function allows \\\\*each\\\\* single output vector \\\\\\\\( \\\\mathbf{z}\\\\_{i} \\\\\\\\) of this self-attention function to learn better vector representations.', 'type': 'tool', 'name': 'data_retriever', 'id': '27833bb9-39d7-46be-bd29-4d846d94c662', 'tool_call_id': 'call_Q178fXcyNaDITxlNCvA0ukEm', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa9783e684812d15f5d02', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# LongT5\\n## Overview\\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It\\'s an\\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\\nTransient-Global attention. The abstract from the paper is the following:\\n\\\\*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\\\em Transient Global}\\n(TGlobal), which mimics ETC\\'s local/global attention mechanism, but without requiring additional side-inputs. We are\\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\\nquestion answering tasks.\\\\*\\nThis model was contributed by [stancld](https://huggingface.co/stancld). The original code can be found [here](https://github.com/google-research/longt5). ## Usage tips\\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\\nencoder \\\\*self-attention\\\\* layer with efficient either \\\\*local\\\\* attention or \\\\*transient-global\\\\* (\\\\*tglobal\\\\*) attention. - Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\\ninspired by the pre-training of [`PegasusForConditionalGeneration`]. - LongT5 model is designed to work efficiently and very well on long-range \\\\*sequence-to-sequence\\\\* tasks where the\\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens. - For \\\\*Local Attention\\\\*, the sparse sliding-window local attention operation allows a given token to attend only `r`\\ntokens to the left and right of it (with `r=127` by default). \\\\*Local Attention\\\\* does not introduce any new parameters\\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l\\\\*r)`. - \\\\*Transient Global Attention\\\\* is an extension of the \\\\*Local Attention\\\\*. It, furthermore, allows each input token to\\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\\nalso every global token like in the case of standard global attention (\\\\*transient\\\\* represents the fact the global tokens\\nare constructed dynamically within each attention operation). As a consequence, \\\\*TGlobal\\\\* attention introduces\\na few new parameters -- global relative position biases and a layer normalization for global token\\'s embedding.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab02f3e684812d15f64ee', 'page_content': 'summarization). The memory consumption is thus reduced from \\\\\\\\(n\\\\_s^2+ n\\\\_s n\\\\_t +n\\\\_t^2\\\\\\\\) to \\\\\\\\(w n\\\\_s +gn\\\\_s +n\\\\_s n\\\\_t +n\\\\_t^2\\\\\\\\) with \\\\\\\\(n\\\\_s\\\\\\\\) and \\\\\\\\(n\\\\_t\\\\\\\\) being the source (encoder input) and target (decoder input) lengths respectively. For Longformer Encoder-Decoder to be efficient, it is assumed that \\\\\\\\(n\\\\_s\\\\\\\\) is much bigger than \\\\\\\\(n\\\\_t\\\\\\\\). #### Main findings\\n\\\\* The authors proposed the dilated windowed self-attention (Figure c) and showed that it yields better results on language modeling compared to just windowed/sparse self-attention (Figure b). The window sizes are increased through the layers. This pattern further outperforms previous architectures (such as Transformer-XL, or adaptive span attention) on downstream benchmarks. \\\\* Global attention allows the information to flow through the whole sequence and applying the global attention to task-motivated tokens (such as the tokens of the question in QA, CLS token for sentence classification) leads to stronger performance on downstream tasks. Using this global pattern, Longformer can be successfully applied to document-level NLP tasks in the transfer learning setting.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab90e3e684812d15f6f60', 'page_content': 'The Authors\\' code can be found [here](https://github.com/allenai/longformer). ## Usage tips\\n- Since the Longformer is based on RoBERTa, it doesn\\'t have `token\\\\_type\\\\_ids`. You don\\'t need to indicate which\\ntoken belongs to which segment. Just separate your segments with the separation token `tokenizer.sep\\\\_token` (or\\n``). - A transformer model replacing the attention matrices by sparse matrices to go faster. Often, the local context (e.g., what are the two tokens left and right?) is enough to take action for a given token. Some preselected input tokens are still given global attention, but the attention matrix has way less parameters, resulting in a speed-up. See the local attention section for more information. ## Longformer Self Attention\\nLongformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\\nattend \"locally\" to each other meaning that each token attends to its \\\\\\\\(\\\\frac{1}{2} w\\\\\\\\) previous tokens and\\n\\\\\\\\(\\\\frac{1}{2} w\\\\\\\\) succeeding tokens with \\\\\\\\(w\\\\\\\\) being the window length as defined in\\n`config.attention\\\\_window`. Note that `config.attention\\\\_window` can be of type `List` to define a\\ndifferent \\\\\\\\(w\\\\\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\\nconventionally done for all tokens in `BertSelfAttention`. Note that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\\nthat every \"locally\" attending token not only attends to tokens within its window \\\\\\\\(w\\\\\\\\), but also to all \"globally\"\\nattending tokens so that global attention is \\\\*symmetric\\\\*. The user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\\n`global\\\\_attention\\\\_mask` at run-time appropriately. All Longformer models employ the following logic for\\n`global\\\\_attention\\\\_mask`:\\n- 0: the token attends \"locally\",\\n- 1: the token attends \"globally\". For more information please also refer to [`~LongformerModel.forward`] method. Using Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\\nrepresents the memory and time bottleneck, can be reduced from \\\\\\\\(\\\\mathcal{O}(n\\\\_s \\\\times n\\\\_s)\\\\\\\\) to\\n\\\\\\\\(\\\\mathcal{O}(n\\\\_s \\\\times w)\\\\\\\\), with \\\\\\\\(n\\\\_s\\\\\\\\) being the sequence length and \\\\\\\\(w\\\\\\\\) being the average window\\nsize.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab5ad3e684812d15f6b7a', 'page_content': 'The first drawback of this architecture becomes obvious: Some input vectors have no access to their immediate context, \\\\*e.g.\\\\* \\\\\\\\(\\\\mathbf{x}\\\\_9\\\\\\\\) has no access to \\\\\\\\(\\\\mathbf{x}\\\\_{8}\\\\\\\\) and vice-versa in our example. This is problematic because these tokens are not able to learn word representations that take their immediate context into account. A simple remedy is to augment each chunk with `config.local\\\\_num\\\\_chunks\\\\_before`, \\\\*i.e.\\\\* \\\\\\\\(n\\\\_{p}\\\\\\\\), chunks and `config.local\\\\_num\\\\_chunks\\\\_after`, \\\\*i.e.\\\\* \\\\\\\\(n\\\\_{a}\\\\\\\\), so that every input vector has at least access to \\\\\\\\(n\\\\_{p}\\\\\\\\) previous input vectors and \\\\\\\\(n\\\\_{a}\\\\\\\\) following input vectors. This can also be understood as chunking with overlap whereas \\\\\\\\(n\\\\_{p}\\\\\\\\) and \\\\\\\\(n\\\\_{a}\\\\\\\\) define the amount of overlap each chunk has with all previous chunks and following chunks. We denote this extended local self-attention as follows:\\n$$\\\\mathbf{Z}^{\\\\text{loc}} = \\\\left[\\\\mathbf{Z}\\\\_{1:l\\\\_{c}}^{\\\\text{loc}}, \\\\ldots, \\\\mathbf{Z}\\\\_{(n\\\\_{c} - 1) \\\\* l\\\\_{c} : n\\\\_{c} \\\\* l\\\\_{c}}^{\\\\text{loc}}\\\\right], $$\\nwith\\n$$\\\\mathbf{Z}\\\\_{l\\\\_{c} \\\\* (i - 1) + 1 : l\\\\_{c} \\\\* i}^{\\\\text{loc}} = \\\\text{SelfAttn}(\\\\mathbf{X}\\\\_{l\\\\_{c} \\\\* (i - 1 - n\\\\_{p}) + 1: l\\\\_{c} \\\\* (i + n\\\\_{a})})\\\\left[n\\\\_{p} \\\\* l\\\\_{c}: -n\\\\_{a} \\\\* l\\\\_{c}\\\\right], \\\\forall i \\\\in \\\\{1, \\\\ldots, n\\\\_{c} \\\\}$$\\nOkay, this formula looks quite complicated. Let\\'s make it easier. In Reformer\\'s self-attention layers \\\\\\\\(n\\\\_{a}\\\\\\\\) is usually set to 0 and \\\\\\\\(n\\\\_{p}\\\\\\\\) is set to 1, so let\\'s write down the formula again for \\\\\\\\(i = 1\\\\\\\\):\\n$$\\\\mathbf{Z}\\\\_{1:l\\\\_{c}}^{\\\\text{loc}} = \\\\text{SelfAttn}(\\\\mathbf{X}\\\\_{-l\\\\_{c} + 1: l\\\\_{c}})\\\\left[l\\\\_{c}:\\\\right]$$\\nWe notice that we have a circular relationship so that the first segment can attend the last segment as well. Let\\'s illustrate this slightly enhanced local attention again. First, we apply self-attention within each windowed segment and keep only the central output segment. ![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific\\\\_images/master/reformer\\\\_benchmark/local\\\\_attention\\\\_2.png)\\nFinally, the relevant output is concatenated to \\\\\\\\(\\\\mathbf{Z}^{\\\\text{loc}}\\\\\\\\) and looks as follows. ![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific\\\\_images/master/reformer\\\\_benchmark/local\\\\_attention\\\\_3.png)\\nNote that local self-attention is implemented efficiently way so that no output is computed and subsequently \"thrown-out\" as shown here for illustration purposes by the red cross. It\\'s important to note here that extending the input vectors for each chunked self-attention function allows \\\\*each\\\\* single output vector \\\\\\\\( \\\\mathbf{z}\\\\_{i} \\\\\\\\) of this self-attention function to learn better vector representations.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The default context window size for Local Attention in the LongT5 model is `r=127`, which means a given token can attend to 127 tokens to the left and 127 tokens to the right of it.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--81ac05a9-de81-429a-bf7f-9d2dd99bf7bc-0', 'usage_metadata': {'input_tokens': 2914, 'output_tokens': 44, 'total_tokens': 2958, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:26:10,047 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      "  9%|‚ñâ         | 6/65 [00:23<03:37,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What method is used to load a checkpoint for a task using `AutoPipeline`?\\n', 'type': 'human', 'id': '04d1069e-8ca4-4625-b2a1-041b8418c751'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_QzaBTOBh0IJrEYv4apbGeQiw', 'function': {'arguments': '{\"query\":\"load a checkpoint for a task using AutoPipeline\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--677bb1e1-3a34-41b6-ae07-e0b9b1abc375-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'load a checkpoint for a task using AutoPipeline'}, 'id': 'call_QzaBTOBh0IJrEYv4apbGeQiw', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 24, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'The `AutoPipeline` class is designed to simplify the variety of pipelines in ü§ó Diffusers. It is a generic, \\\\*task-first\\\\* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name. Take a look at the [AutoPipeline](../api/pipelines/auto\\\\_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting. This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights. ## Choose an AutoPipeline for your task\\nStart by picking a checkpoint. For example, if you\\'re interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\\n```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\npipeline = AutoPipelineForText2Image.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n).to(\"cuda\")\\nprompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"\\nimage = pipeline(prompt, num\\\\_inference\\\\_steps=25).images[0]\\nimage\\n```\\n\\n![generated image of peasant fighting dragon in wood cutting style](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png)\\n\\nUnder the hood, [`AutoPipelineForText2Image`]:\\n1. automatically detects a `\"stable-diffusion\"` class from the [`model\\\\_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model\\\\_index.json) file\\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\"stable-diffusion\"` class name\\nLikewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\"stable-diffusion\"` checkpoint from the `model\\\\_index.json` file and it\\'ll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image:\\n```py\\nfrom diffusers import AutoPipelineForImage2Image\\nimport torch\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\npipeline = AutoPipelineForImage2Image.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\",\\ntorch\\\\_dtype=torch.float16,\\nuse\\\\_safetensors=True,\\n).to(\"cuda\")\\nprompt = \"a portrait of a dog wearing a pearl earring\"\\nurl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665\\\\_Girl\\\\_with\\\\_a\\\\_Pearl\\\\_Earring.jpg/800px-1665\\\\_Girl\\\\_with\\\\_a\\\\_Pearl\\\\_Earring.jpg\"\\nresponse = requests.get(url)\\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\\nimage.thumbnail((768, 768))\\nimage = pipeline(prompt, image, num\\\\_inference\\\\_steps=200, strength=0.75, guidance\\\\_scale=10.5).images[0]\\nimage\\n```\\n\\n![generated image of a vermeer portrait of a dog wearing a pearl earring](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png)\\n\\nAnd if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way:\\n```py\\nfrom diffusers import AutoPipelineForInpainting\\nfrom diffusers.utils import load\\\\_image\\nimport torch\\npipeline = AutoPipelineForInpainting.from\\\\_pretrained(\\n\"stabilityai/stable-diffusion-xl-base-1.0\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n).to(\"cuda\")\\nimg\\\\_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\\\_examples/overture-creations-5sI6fQgYIuo.png\"\\nmask\\\\_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\\\_examples/overture-creations-5sI6fQgYIuo\\\\_mask.png\"\\ninit\\\\_image = load\\\\_image(img\\\\_url).convert(\"RGB\")\\nmask\\\\_image = load\\\\_image(mask\\\\_url).convert(\"RGB\")\\nprompt = \"A majestic tiger sitting on a bench\"\\nimage = pipeline(prompt, image=init\\\\_image, mask\\\\_image=mask\\\\_image, num\\\\_inference\\\\_steps=50, strength=0.80).images[0]\\nimage\\n```\\n\\n![generated image of a tiger sitting on a bench](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png)\\n\\nIf you try to load an unsupported checkpoint, it\\'ll throw an error:\\n```py\\nfrom diffusers import AutoPipelineForImage2Image\\nimport torch\\npipeline = AutoPipelineForImage2Image.from\\\\_pretrained(\\n\"openai/shap-e-img2img\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n)\\n\"ValueError: AutoPipeline can\\'t find a pipeline linked to ShapEImg2ImgPipeline for None\"\\n```\\n## Use multiple pipelines\\nFor some workflows or if you\\'re loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory.\\n\\nFor example, if you\\'re using a checkpoint for text-to-image and you want to use it again for image-to-image, use the [`~AutoPipelineForImage2Image.from\\\\_pipe`] method. This method creates a new pipeline from the components of a previously loaded pipeline at no additional memory cost. The [`~AutoPipelineForImage2Image.from\\\\_pipe`] method detects the original pipeline class and maps it to the new pipeline class corresponding to the task you want to do. For example, if you load a `\"stable-diffusion\"` class pipeline for text-to-image:\\n```py\\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\\nimport torch\\npipeline\\\\_text2img = AutoPipelineForText2Image.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n)\\nprint(type(pipeline\\\\_text2img))\\n\"\"\\n```\\nThen [`~AutoPipelineForImage2Image.from\\\\_pipe`] maps the original `\"stable-diffusion\"` pipeline class to [`StableDiffusionImg2ImgPipeline`]:\\n```py\\npipeline\\\\_img2img = AutoPipelineForImage2Image.from\\\\_pipe(pipeline\\\\_text2img)\\nprint(type(pipeline\\\\_img2img))\\n\"\"\\n```\\nIf you passed an optional argument - like disabling the safety checker - to the original pipeline, this argument is also passed on to the new pipeline:\\n```py\\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\\nimport torch\\npipeline\\\\_text2img = AutoPipelineForText2Image.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\",\\ntorch\\\\_dtype=torch.float16,\\nuse\\\\_safetensors=True,\\nrequires\\\\_safety\\\\_checker=False,\\n).to(\"cuda\")\\npipeline\\\\_img2img = AutoPipelineForImage2Image.from\\\\_pipe(pipeline\\\\_text2img)\\nprint(pipeline\\\\_img2img.config.requires\\\\_safety\\\\_checker)\\n\"False\"\\n```\\nYou can overwrite any of the arguments and even configuration from the original pipeline if you want to change the behavior of the new pipeline. For example, to turn the safety checker back on and add the `strength` argument:\\n```py\\npipeline\\\\_img2img = AutoPipelineForImage2Image.from\\\\_pipe(pipeline\\\\_text2img, requires\\\\_safety\\\\_checker=True, strength=0.3)\\nprint(pipeline\\\\_img2img.config.requires\\\\_safety\\\\_checker)\\n\"True\"\\n```\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# AutoPipeline\\n`AutoPipeline` is designed to:\\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\\n2. use multiple pipelines in your workflow\\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from\\\\_pretrained()` method. To seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from\\\\_pipe()` method to transfer the components from the original pipeline to the new one. ```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\npipeline = AutoPipelineForText2Image.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n).to(\"cuda\")\\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\\nimage = pipeline(prompt, num\\\\_inference\\\\_steps=25).images[0]\\n```\\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API! `AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\\n- [Stable Diffusion](./stable\\\\_diffusion/overview)\\n- [ControlNet](./controlnet)\\n- [Stable Diffusion XL (SDXL)](./stable\\\\_diffusion/stable\\\\_diffusion\\\\_xl)\\n- [DeepFloyd IF](./deepfloyd\\\\_if)\\n- [Kandinsky 2.1](./kandinsky)\\n- [Kandinsky 2.2](./kandinsky\\\\_v22)\\n## AutoPipelineForText2Image\\n[[autodoc]] AutoPipelineForText2Image\\n- all\\n- from\\\\_pretrained\\n- from\\\\_pipe\\n## AutoPipelineForImage2Image\\n[[autodoc]] AutoPipelineForImage2Image\\n- all\\n- from\\\\_pretrained\\n- from\\\\_pipe\\n## AutoPipelineForInpainting\\n[[autodoc]] AutoPipelineForInpainting\\n- all\\n- from\\\\_pretrained\\n- from\\\\_pipe\\n\\nThis is a custom pipeline that merges upto 3 pretrained model checkpoints as long as they are in the HuggingFace model\\\\_index.json format. The checkpoint merging is currently memory intensive as it modifies the weights of a DiffusionPipeline object in place. Expect atleast 13GB RAM Usage on Kaggle GPU kernels and\\non colab you might run out of the 12GB memory even while merging two checkpoints. Usage:-\\n```python\\nfrom diffusers import DiffusionPipeline\\n#Return a CheckpointMergerPipeline class that allows you to merge checkpoints. #The checkpoint passed here is ignored. But still pass one of the checkpoints you plan to\\n#merge for convenience\\npipe = DiffusionPipeline.from\\\\_pretrained(\"CompVis/stable-diffusion-v1-4\", custom\\\\_pipeline=\"checkpoint\\\\_merger\")\\n#There are multiple possible scenarios:\\n#The pipeline with the merged checkpoints is returned in all the scenarios\\n#Compatible checkpoints a.k.a matched model\\\\_index.json files. Ignores the meta attributes in model\\\\_index.json during comparison.( attrs with \\\\_ as prefix )\\nmerged\\\\_pipe = pipe.merge([\"CompVis/stable-diffusion-v1-4\",\"CompVis/stable-diffusion-v1-2\"], interp = \"sigmoid\", alpha = 0.4)\\n#Incompatible checkpoints in model\\\\_index.json but merge might be possible. Use force = True to ignore model\\\\_index.json compatibility\\nmerged\\\\_pipe\\\\_1 = pipe.merge([\"CompVis/stable-diffusion-v1-4\",\"hakurei/waifu-diffusion\"], force = True, interp = \"sigmoid\", alpha = 0.4)\\n#Three checkpoint merging. Only \"add\\\\_difference\" method actually works on all three checkpoints. Using any other options will ignore the 3rd checkpoint. merged\\\\_pipe\\\\_2 = pipe.merge([\"CompVis/stable-diffusion-v1-4\",\"hakurei/waifu-diffusion\",\"prompthero/openjourney\"], force = True, interp = \"add\\\\_difference\", alpha = 0.4)\\nprompt = \"An astronaut riding a horse on Mars\"\\nimage = merged\\\\_pipe(prompt).images[0]\\n```\\nSome examples along with the merge details:\\n1. \"CompVis/stable-diffusion-v1-4\" + \"hakurei/waifu-diffusion\" ; Sigmoid interpolation; alpha = 0.8\\n![Stable plus Waifu Sigmoid 0.8](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/stability\\\\_v1\\\\_4\\\\_waifu\\\\_sig\\\\_0.8.png)\\n2. \"hakurei/waifu-diffusion\" + \"prompthero/openjourney\" ; Inverse Sigmoid interpolation; alpha = 0.8\\n![Stable plus Waifu Sigmoid 0.8](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/waifu\\\\_openjourney\\\\_inv\\\\_sig\\\\_0.8.png)\\n3. \"CompVis/stable-diffusion-v1-4\" + \"hakurei/waifu-diffusion\" + \"prompthero/openjourney\"; Add Difference interpolation; alpha = 0.5\\n![Stable plus Waifu plus openjourney add\\\\_diff 0.5](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/stable\\\\_waifu\\\\_openjourney\\\\_add\\\\_diff\\\\_0.5.png)\\n### Stable Diffusion Comparisons\\nThis Community Pipeline enables the comparison between the 4 checkpoints that exist for Stable Diffusion.', 'type': 'tool', 'name': 'data_retriever', 'id': '3d51fca6-1182-43fe-b4c9-8917fe58785f', 'tool_call_id': 'call_QzaBTOBh0IJrEYv4apbGeQiw', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b3c6af3d2ebb270a00fe', 'page_content': 'The `AutoPipeline` class is designed to simplify the variety of pipelines in ü§ó Diffusers. It is a generic, \\\\*task-first\\\\* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name. Take a look at the [AutoPipeline](../api/pipelines/auto\\\\_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting. This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights. ## Choose an AutoPipeline for your task\\nStart by picking a checkpoint. For example, if you\\'re interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\\n```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\npipeline = AutoPipelineForText2Image.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n).to(\"cuda\")\\nprompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"\\nimage = pipeline(prompt, num\\\\_inference\\\\_steps=25).images[0]\\nimage\\n```\\n\\n![generated image of peasant fighting dragon in wood cutting style](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png)\\n\\nUnder the hood, [`AutoPipelineForText2Image`]:\\n1. automatically detects a `\"stable-diffusion\"` class from the [`model\\\\_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model\\\\_index.json) file\\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\"stable-diffusion\"` class name\\nLikewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\"stable-diffusion\"` checkpoint from the `model\\\\_index.json` file and it\\'ll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image:\\n```py\\nfrom diffusers import AutoPipelineForImage2Image\\nimport torch\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\npipeline = AutoPipelineForImage2Image.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\",\\ntorch\\\\_dtype=torch.float16,\\nuse\\\\_safetensors=True,\\n).to(\"cuda\")\\nprompt = \"a portrait of a dog wearing a pearl earring\"\\nurl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665\\\\_Girl\\\\_with\\\\_a\\\\_Pearl\\\\_Earring.jpg/800px-1665\\\\_Girl\\\\_with\\\\_a\\\\_Pearl\\\\_Earring.jpg\"\\nresponse = requests.get(url)\\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\\nimage.thumbnail((768, 768))\\nimage = pipeline(prompt, image, num\\\\_inference\\\\_steps=200, strength=0.75, guidance\\\\_scale=10.5).images[0]\\nimage\\n```\\n\\n![generated image of a vermeer portrait of a dog wearing a pearl earring](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png)\\n\\nAnd if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way:\\n```py\\nfrom diffusers import AutoPipelineForInpainting\\nfrom diffusers.utils import load\\\\_image\\nimport torch\\npipeline = AutoPipelineForInpainting.from\\\\_pretrained(\\n\"stabilityai/stable-diffusion-xl-base-1.0\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n).to(\"cuda\")\\nimg\\\\_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\\\_examples/overture-creations-5sI6fQgYIuo.png\"\\nmask\\\\_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\\\_examples/overture-creations-5sI6fQgYIuo\\\\_mask.png\"\\ninit\\\\_image = load\\\\_image(img\\\\_url).convert(\"RGB\")\\nmask\\\\_image = load\\\\_image(mask\\\\_url).convert(\"RGB\")\\nprompt = \"A majestic tiger sitting on a bench\"\\nimage = pipeline(prompt, image=init\\\\_image, mask\\\\_image=mask\\\\_image, num\\\\_inference\\\\_steps=50, strength=0.80).images[0]\\nimage\\n```\\n\\n![generated image of a tiger sitting on a bench](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png)\\n\\nIf you try to load an unsupported checkpoint, it\\'ll throw an error:\\n```py\\nfrom diffusers import AutoPipelineForImage2Image\\nimport torch\\npipeline = AutoPipelineForImage2Image.from\\\\_pretrained(\\n\"openai/shap-e-img2img\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n)\\n\"ValueError: AutoPipeline can\\'t find a pipeline linked to ShapEImg2ImgPipeline for None\"\\n```\\n## Use multiple pipelines\\nFor some workflows or if you\\'re loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b3c6af3d2ebb270a00ff', 'page_content': 'For example, if you\\'re using a checkpoint for text-to-image and you want to use it again for image-to-image, use the [`~AutoPipelineForImage2Image.from\\\\_pipe`] method. This method creates a new pipeline from the components of a previously loaded pipeline at no additional memory cost. The [`~AutoPipelineForImage2Image.from\\\\_pipe`] method detects the original pipeline class and maps it to the new pipeline class corresponding to the task you want to do. For example, if you load a `\"stable-diffusion\"` class pipeline for text-to-image:\\n```py\\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\\nimport torch\\npipeline\\\\_text2img = AutoPipelineForText2Image.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n)\\nprint(type(pipeline\\\\_text2img))\\n\"\"\\n```\\nThen [`~AutoPipelineForImage2Image.from\\\\_pipe`] maps the original `\"stable-diffusion\"` pipeline class to [`StableDiffusionImg2ImgPipeline`]:\\n```py\\npipeline\\\\_img2img = AutoPipelineForImage2Image.from\\\\_pipe(pipeline\\\\_text2img)\\nprint(type(pipeline\\\\_img2img))\\n\"\"\\n```\\nIf you passed an optional argument - like disabling the safety checker - to the original pipeline, this argument is also passed on to the new pipeline:\\n```py\\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\\nimport torch\\npipeline\\\\_text2img = AutoPipelineForText2Image.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\",\\ntorch\\\\_dtype=torch.float16,\\nuse\\\\_safetensors=True,\\nrequires\\\\_safety\\\\_checker=False,\\n).to(\"cuda\")\\npipeline\\\\_img2img = AutoPipelineForImage2Image.from\\\\_pipe(pipeline\\\\_text2img)\\nprint(pipeline\\\\_img2img.config.requires\\\\_safety\\\\_checker)\\n\"False\"\\n```\\nYou can overwrite any of the arguments and even configuration from the original pipeline if you want to change the behavior of the new pipeline. For example, to turn the safety checker back on and add the `strength` argument:\\n```py\\npipeline\\\\_img2img = AutoPipelineForImage2Image.from\\\\_pipe(pipeline\\\\_text2img, requires\\\\_safety\\\\_checker=True, strength=0.3)\\nprint(pipeline\\\\_img2img.config.requires\\\\_safety\\\\_checker)\\n\"True\"\\n```', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab0233e684812d15f64e5', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# AutoPipeline\\n`AutoPipeline` is designed to:\\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\\n2. use multiple pipelines in your workflow\\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from\\\\_pretrained()` method. To seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from\\\\_pipe()` method to transfer the components from the original pipeline to the new one. ```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\npipeline = AutoPipelineForText2Image.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n).to(\"cuda\")\\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\\nimage = pipeline(prompt, num\\\\_inference\\\\_steps=25).images[0]\\n```\\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API! `AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\\n- [Stable Diffusion](./stable\\\\_diffusion/overview)\\n- [ControlNet](./controlnet)\\n- [Stable Diffusion XL (SDXL)](./stable\\\\_diffusion/stable\\\\_diffusion\\\\_xl)\\n- [DeepFloyd IF](./deepfloyd\\\\_if)\\n- [Kandinsky 2.1](./kandinsky)\\n- [Kandinsky 2.2](./kandinsky\\\\_v22)\\n## AutoPipelineForText2Image\\n[[autodoc]] AutoPipelineForText2Image\\n- all\\n- from\\\\_pretrained\\n- from\\\\_pipe\\n## AutoPipelineForImage2Image\\n[[autodoc]] AutoPipelineForImage2Image\\n- all\\n- from\\\\_pretrained\\n- from\\\\_pipe\\n## AutoPipelineForInpainting\\n[[autodoc]] AutoPipelineForInpainting\\n- all\\n- from\\\\_pretrained\\n- from\\\\_pipe', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab4ef3e684812d15f6a94', 'page_content': 'This is a custom pipeline that merges upto 3 pretrained model checkpoints as long as they are in the HuggingFace model\\\\_index.json format. The checkpoint merging is currently memory intensive as it modifies the weights of a DiffusionPipeline object in place. Expect atleast 13GB RAM Usage on Kaggle GPU kernels and\\non colab you might run out of the 12GB memory even while merging two checkpoints. Usage:-\\n```python\\nfrom diffusers import DiffusionPipeline\\n#Return a CheckpointMergerPipeline class that allows you to merge checkpoints. #The checkpoint passed here is ignored. But still pass one of the checkpoints you plan to\\n#merge for convenience\\npipe = DiffusionPipeline.from\\\\_pretrained(\"CompVis/stable-diffusion-v1-4\", custom\\\\_pipeline=\"checkpoint\\\\_merger\")\\n#There are multiple possible scenarios:\\n#The pipeline with the merged checkpoints is returned in all the scenarios\\n#Compatible checkpoints a.k.a matched model\\\\_index.json files. Ignores the meta attributes in model\\\\_index.json during comparison.( attrs with \\\\_ as prefix )\\nmerged\\\\_pipe = pipe.merge([\"CompVis/stable-diffusion-v1-4\",\"CompVis/stable-diffusion-v1-2\"], interp = \"sigmoid\", alpha = 0.4)\\n#Incompatible checkpoints in model\\\\_index.json but merge might be possible. Use force = True to ignore model\\\\_index.json compatibility\\nmerged\\\\_pipe\\\\_1 = pipe.merge([\"CompVis/stable-diffusion-v1-4\",\"hakurei/waifu-diffusion\"], force = True, interp = \"sigmoid\", alpha = 0.4)\\n#Three checkpoint merging. Only \"add\\\\_difference\" method actually works on all three checkpoints. Using any other options will ignore the 3rd checkpoint. merged\\\\_pipe\\\\_2 = pipe.merge([\"CompVis/stable-diffusion-v1-4\",\"hakurei/waifu-diffusion\",\"prompthero/openjourney\"], force = True, interp = \"add\\\\_difference\", alpha = 0.4)\\nprompt = \"An astronaut riding a horse on Mars\"\\nimage = merged\\\\_pipe(prompt).images[0]\\n```\\nSome examples along with the merge details:\\n1. \"CompVis/stable-diffusion-v1-4\" + \"hakurei/waifu-diffusion\" ; Sigmoid interpolation; alpha = 0.8\\n![Stable plus Waifu Sigmoid 0.8](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/stability\\\\_v1\\\\_4\\\\_waifu\\\\_sig\\\\_0.8.png)\\n2. \"hakurei/waifu-diffusion\" + \"prompthero/openjourney\" ; Inverse Sigmoid interpolation; alpha = 0.8\\n![Stable plus Waifu Sigmoid 0.8](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/waifu\\\\_openjourney\\\\_inv\\\\_sig\\\\_0.8.png)\\n3. \"CompVis/stable-diffusion-v1-4\" + \"hakurei/waifu-diffusion\" + \"prompthero/openjourney\"; Add Difference interpolation; alpha = 0.5\\n![Stable plus Waifu plus openjourney add\\\\_diff 0.5](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/stable\\\\_waifu\\\\_openjourney\\\\_add\\\\_diff\\\\_0.5.png)\\n### Stable Diffusion Comparisons\\nThis Community Pipeline enables the comparison between the 4 checkpoints that exist for Stable Diffusion.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The method used to load a checkpoint for a task using `AutoPipeline` is the `from_pretrained()` method. This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--af4c6cad-d760-4154-97e4-983d22e18e96-0', 'usage_metadata': {'input_tokens': 3524, 'output_tokens': 42, 'total_tokens': 3566, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:26:15,577 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 11%|‚ñà         | 7/65 [00:29<04:08,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of Diffusers library?\\n', 'type': 'human', 'id': 'd98e89af-8806-4874-822f-96156d6a924f'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_4hpOFTHdBoh8KtOkXcHfjvk0', 'function': {'arguments': '{\"query\":\"Diffusers library purpose\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f647d764-8494-47ce-a12c-e3a5b9582409-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Diffusers library purpose'}, 'id': 'call_4hpOFTHdBoh8KtOkXcHfjvk0', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 357, 'output_tokens': 19, 'total_tokens': 376, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'üß® Diffusers Experimental\\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n\\n![](https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg)\\n\\n# Diffusers\\nü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\\'re looking for a simple inference solution or want to train your own diffusion model, ü§ó Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). The library has three main components:\\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in ü§ó Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve. - Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality. - Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. [Tutorials\\n\\nLearn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you\\'re using ü§ó Diffusers for the first time!](./tutorials/tutorial_overview)\\n[How-to guides\\n\\nPractical guides for helping you load pipelines, models, and schedulers. You\\'ll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.](./using-diffusers/loading_overview)\\n[Conceptual guides\\n\\nUnderstand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.](./conceptual/philosophy)\\n[Reference\\n\\nTechnical descriptions of how ü§ó Diffusers classes and methods work.](./api/models/overview)\\n\\n!---\\nCopyright 2022 - The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![](https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg)\\n\\n[![GitHub](https://img.shields.io/github/license/huggingface/datasets.svg?color=blue)](https://github.com/huggingface/diffusers/blob/main/LICENSE)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/diffusers.svg)](https://github.com/huggingface/diffusers/releases)\\n[![GitHub release](https://static.pepy.tech/badge/diffusers/month)](https://pepy.tech/project/diffusers)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CODE_OF_CONDUCT.md)\\n[![X account](https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&label=Follow%20%40diffuserslib)](https://twitter.com/diffuserslib)\\n\\nü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\\'re looking for a simple inference solution or training your own diffusion models, ü§ó Diffusers is a modular toolbox that supports both.\\n\\n!---\\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n# üß® Diffusers Examples\\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\\nfor a variety of use cases involving training or fine-tuning. \\\\*\\\\*Note\\\\*\\\\*: If you are looking for \\\\*\\\\*official\\\\*\\\\* examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines).', 'type': 'tool', 'name': 'data_retriever', 'id': 'c1949edd-9abe-48bd-9bfe-50b870a72fbd', 'tool_call_id': 'call_4hpOFTHdBoh8KtOkXcHfjvk0', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b6efaf3d2ebb270a043c', 'page_content': 'üß® Diffusers Experimental\\nWe are adding experimental code to support novel applications and usages of the Diffusers library.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aacf13e684812d15f6180', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n\\n![](https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg)\\n\\n# Diffusers\\nü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\\'re looking for a simple inference solution or want to train your own diffusion model, ü§ó Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). The library has three main components:\\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in ü§ó Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve. - Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality. - Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. [Tutorials\\n\\nLearn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you\\'re using ü§ó Diffusers for the first time!](./tutorials/tutorial_overview)\\n[How-to guides\\n\\nPractical guides for helping you load pipelines, models, and schedulers. You\\'ll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.](./using-diffusers/loading_overview)\\n[Conceptual guides\\n\\nUnderstand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.](./conceptual/philosophy)\\n[Reference\\n\\nTechnical descriptions of how ü§ó Diffusers classes and methods work.](./api/models/overview)', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bf25666af48523f51d80', 'page_content': '!---\\nCopyright 2022 - The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![](https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg)\\n\\n[![GitHub](https://img.shields.io/github/license/huggingface/datasets.svg?color=blue)](https://github.com/huggingface/diffusers/blob/main/LICENSE)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/diffusers.svg)](https://github.com/huggingface/diffusers/releases)\\n[![GitHub release](https://static.pepy.tech/badge/diffusers/month)](https://pepy.tech/project/diffusers)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CODE_OF_CONDUCT.md)\\n[![X account](https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&label=Follow%20%40diffuserslib)](https://twitter.com/diffuserslib)\\n\\nü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\\'re looking for a simple inference solution or training your own diffusion models, ü§ó Diffusers is a modular toolbox that supports both.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b29caf3d2ebb2709fedf', 'page_content': '!---\\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n# üß® Diffusers Examples\\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\\nfor a variety of use cases involving training or fine-tuning. \\\\*\\\\*Note\\\\*\\\\*: If you are looking for \\\\*\\\\*official\\\\*\\\\* examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines).', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The purpose of the ü§ó Diffusers library is to provide state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. It serves as a modular toolbox that supports both simple inference solutions and the training of custom diffusion models. The library focuses on usability, simplicity, and customizability, making it accessible for various applications in the field of generative modeling.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--cf9d4260-11fa-4168-b01e-d6373e4d89e8-0', 'usage_metadata': {'input_tokens': 1522, 'output_tokens': 81, 'total_tokens': 1603, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:26:19,281 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 12%|‚ñà‚ñè        | 8/65 [00:32<03:53,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What method does the EulerAncestralDiscreteScheduler use for sampling?\\n', 'type': 'human', 'id': '6e4762dd-b524-4f5b-b2eb-bcaf55ab45bd'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_5WCmbaF73UlBDIXj7vJxsQAT', 'function': {'arguments': '{\"query\":\"EulerAncestralDiscreteScheduler sampling method\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--95c60274-fc2d-4f0d-a0da-ab262e37de11-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'EulerAncestralDiscreteScheduler sampling method'}, 'id': 'call_5WCmbaF73UlBDIXj7vJxsQAT', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 23, 'total_tokens': 385, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# EulerAncestralDiscreteScheduler\\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\\\_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerAncestralDiscreteScheduler\\n[[autodoc]] EulerAncestralDiscreteScheduler\\n## EulerAncestralDiscreteSchedulerOutput\\n[[autodoc]] schedulers.scheduling\\\\_euler\\\\_ancestral\\\\_discrete.EulerAncestralDiscreteSchedulerOutput\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# EulerDiscreteScheduler\\nThe Euler scheduler (Algorithm 2) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\\\_diffusion/sampling.py#L51) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerDiscreteScheduler\\n[[autodoc]] EulerDiscreteScheduler\\n## EulerDiscreteSchedulerOutput\\n[[autodoc]] schedulers.scheduling\\\\_euler\\\\_discrete.EulerDiscreteSchedulerOutput\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# KDPM2AncestralDiscreteScheduler\\nThe `KDPM2DiscreteScheduler` with ancestral sampling is inspired by the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper, and the scheduler is ported from and created by [Katherine Crowson](https://github.com/crowsonkb/). The original codebase can be found at [crowsonkb/k-diffusion](https://github.com/crowsonkb/k-diffusion). ## KDPM2AncestralDiscreteScheduler\\n[[autodoc]] KDPM2AncestralDiscreteScheduler\\n## SchedulerOutput\\n[[autodoc]] schedulers.scheduling\\\\_utils.SchedulerOutput\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# HeunDiscreteScheduler\\nThe Heun scheduler (Algorithm 1) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. The scheduler is ported from the [k-diffusion](https://github.com/crowsonkb/k-diffusion) library and created by [Katherine Crowson](https://github.com/crowsonkb/). ## HeunDiscreteScheduler\\n[[autodoc]] HeunDiscreteScheduler\\n## SchedulerOutput\\n[[autodoc]] schedulers.scheduling\\\\_utils.SchedulerOutput', 'type': 'tool', 'name': 'data_retriever', 'id': '5af83198-7a7c-415a-9635-2823d71895f7', 'tool_call_id': 'call_5WCmbaF73UlBDIXj7vJxsQAT', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab8943e684812d15f6edd', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# EulerAncestralDiscreteScheduler\\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\\\_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerAncestralDiscreteScheduler\\n[[autodoc]] EulerAncestralDiscreteScheduler\\n## EulerAncestralDiscreteSchedulerOutput\\n[[autodoc]] schedulers.scheduling\\\\_euler\\\\_ancestral\\\\_discrete.EulerAncestralDiscreteSchedulerOutput', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aab103e684812d15f5f13', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# EulerDiscreteScheduler\\nThe Euler scheduler (Algorithm 2) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\\\_diffusion/sampling.py#L51) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerDiscreteScheduler\\n[[autodoc]] EulerDiscreteScheduler\\n## EulerDiscreteSchedulerOutput\\n[[autodoc]] schedulers.scheduling\\\\_euler\\\\_discrete.EulerDiscreteSchedulerOutput', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bc7aaf3d2ebb270a0b17', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# KDPM2AncestralDiscreteScheduler\\nThe `KDPM2DiscreteScheduler` with ancestral sampling is inspired by the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper, and the scheduler is ported from and created by [Katherine Crowson](https://github.com/crowsonkb/). The original codebase can be found at [crowsonkb/k-diffusion](https://github.com/crowsonkb/k-diffusion). ## KDPM2AncestralDiscreteScheduler\\n[[autodoc]] KDPM2AncestralDiscreteScheduler\\n## SchedulerOutput\\n[[autodoc]] schedulers.scheduling\\\\_utils.SchedulerOutput', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaedf3e684812d15f63a7', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# HeunDiscreteScheduler\\nThe Heun scheduler (Algorithm 1) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. The scheduler is ported from the [k-diffusion](https://github.com/crowsonkb/k-diffusion) library and created by [Katherine Crowson](https://github.com/crowsonkb/). ## HeunDiscreteScheduler\\n[[autodoc]] HeunDiscreteScheduler\\n## SchedulerOutput\\n[[autodoc]] schedulers.scheduling\\\\_utils.SchedulerOutput', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--9048c366-a484-4283-8f3f-87389786541c-0', 'usage_metadata': {'input_tokens': 1429, 'output_tokens': 17, 'total_tokens': 1446, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:26:22,699 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 14%|‚ñà‚ñç        | 9/65 [00:36<03:37,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\\n', 'type': 'human', 'id': '1d6d712e-7333-4836-bb91-5693a721d58f'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_ZTrCfrOe1ABB1sbdz3cmQXeF', 'function': {'arguments': '{\"query\":\"large multimodal model based on Flamingo\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--5a56ebc4-7842-41c1-96d2-0b0ba40cc49d-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'large multimodal model based on Flamingo'}, 'id': 'call_ZTrCfrOe1ABB1sbdz3cmQXeF', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 371, 'output_tokens': 23, 'total_tokens': 394, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# FLAVA\\n## Overview\\nThe FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022. The paper aims at creating a single unified foundation model which can work across vision, language\\nas well as vision-and-language multimodal tasks. The abstract from the paper is the following:\\n\\\\*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety\\nof downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal\\n(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising\\ndirection would be to use a single holistic universal model, as a \"foundation\", that targets all modalities\\nat once -- a true vision and language foundation model should be good at vision tasks, language tasks, and\\ncross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate\\nimpressive performance on a wide range of 35 tasks spanning these target modalities.\\\\*\\nThis model was contributed by [aps](https://huggingface.co/aps). The original code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava). ## FlavaConfig\\n[[autodoc]] FlavaConfig\\n## FlavaTextConfig\\n[[autodoc]] FlavaTextConfig\\n## FlavaImageConfig\\n[[autodoc]] FlavaImageConfig\\n## FlavaMultimodalConfig\\n[[autodoc]] FlavaMultimodalConfig\\n## FlavaImageCodebookConfig\\n[[autodoc]] FlavaImageCodebookConfig\\n## FlavaProcessor\\n[[autodoc]] FlavaProcessor\\n## FlavaFeatureExtractor\\n[[autodoc]] FlavaFeatureExtractor\\n## FlavaImageProcessor\\n[[autodoc]] FlavaImageProcessor\\n- preprocess\\n## FlavaForPreTraining\\n[[autodoc]] FlavaForPreTraining\\n- forward\\n## FlavaModel\\n[[autodoc]] FlavaModel\\n- forward\\n- get\\\\_text\\\\_features\\n- get\\\\_image\\\\_features\\n## FlavaImageCodebook\\n[[autodoc]] FlavaImageCodebook\\n- forward\\n- get\\\\_codebook\\\\_indices\\n- get\\\\_codebook\\\\_probs\\n## FlavaTextModel\\n[[autodoc]] FlavaTextModel\\n- forward\\n## FlavaImageModel\\n[[autodoc]] FlavaImageModel\\n- forward\\n## FlavaMultimodalModel\\n[[autodoc]] FlavaMultimodalModel\\n- forward\\n\\nModels such as [SimVLM](https://arxiv.org/abs/2108.10904) and [VirTex](https://arxiv.org/abs/2006.06666v3) use this pre-training objective and feature a unified multi-modal architecture consisting of a transformer encoder and transformer decoder, similar to that of an autoregressive language model. Let‚Äôs break this down and see how this works. Language models with a prefix objective predict the next token given an input text as the prefix. For example, given the sequence ‚ÄúA man is standing at the corner‚Äù, we can use ‚ÄúA man is standing at the‚Äù as the prefix and train the model with the objective of predicting the next token - ‚Äúcorner‚Äù or another plausible continuation of the prefix. Visual transformers (ViT) apply the same concept of the prefix to images by dividing each image into a number of patches and sequentially feeding these patches to the model as inputs. Leveraging this idea, SimVLM features an architecture where the encoder receives a concatenated image patch sequence and prefix text sequence as the prefix input, and the decoder then predicts the continuation of the textual sequence. The diagram above depicts this idea. The SimVLM model is first pre-trained on a text dataset without image patches present in the prefix and then on an aligned image-text dataset. These models are used for image-conditioned text generation/captioning and VQA tasks. Models that leverage a unified multi-modal architecture to fuse visual information into a language model (LM) for image-guided tasks show impressive capabilities. However, models that solely use the PrefixLM strategy can be limited in terms of application areas as they are mainly designed for image captioning or visual question-answering downstream tasks. For example, given an image of a group of people, we can query the image to write a description of the image (e.g., ‚ÄúA group of people is standing together in front of a building and smiling‚Äù) or query it with questions that require visual reasoning: ‚ÄúHow many people are wearing red t-shirts?‚Äù. On the other hand, models that learn multi-modal representations or adopt hybrid approaches can be adapted for various other downstream tasks, such as object detection and image segmentation. #### Frozen PrefixLM\\n\\n![Frozen PrefixLM](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/frozen_prefixlm.png)  \\n*Frozen PrefixLM pre-training strategy ([image source](https://lilianweng.github.io/posts/2022-06-09-vlm))*\\n\\nWhile fusing visual information into a language model is highly effective, being able to use a pre-trained language model (LM) without the need for fine-tuning would be much more efficient. Hence, another pre-training objective in vision-language models is learning image embeddings that are aligned with a frozen language model. Models such as [Frozen](https://arxiv.org/abs/2106.13884) and [ClipCap](https://arxiv.org/abs/2111.09734) use this Frozen PrefixLM pre-training objective. They only update the parameters of the image encoder during training to generate image embeddings that can be used as a prefix to the pre-trained, frozen language model in a similar fashion to the PrefixLM objective discussed above. Both Frozen and ClipCap are trained on aligned image-text (caption) datasets with the objective of generating the next token in the caption, given the image embeddings and the prefix text. Finally, models such as [MAPL](https://arxiv.org/abs/2210.07179) and [Flamingo](https://arxiv.org/abs/2204.14198) keep both the pre-trained vision encoder and language model frozen. Flamingo sets a new state-of-the-art in few-shot learning on a wide range of open-ended vision and language tasks by adding Perceiver Resampler modules on top of the pre-trained frozen vision model and inserting new cross-attention layers between existing pre-trained and frozen LM layers to condition the LM on visual data. A nifty advantage of the Frozen PrefixLM pre-training objective is it enables training with limited aligned image-text data, which is particularly useful for domains where aligned multi-modal datasets are not available. ### 3) Multi-modal Fusing with Cross Attention\\n\\n![Cross Attention Fusing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/cross_attention_fusing.png)  \\n *Fusing visual information with a cross-attention mechanism as shown ([image source](https://www.semanticscholar.org/paper/VisualGPT%3A-Data-efficient-Adaptation-of-Pretrained-Chen-Guo/616e0ed02ca024a8c1d4b86167f7486ea92a13d9))*\\n\\nAnother approach to leveraging pre-trained language models for multi-modal tasks is to directly fuse visual information into the layers of a language model decoder using a cross-attention mechanism instead of using images as additional prefixes to the language model.\\n\\nAlso, thanks to Niels Rogge, Younes Belkada, and Suraj Patil, among many others at Hugging Face, who laid out the foundations for increasing the use of multi-modal models from Transformers.\\\\*\\n\\nIn addition, FLAVA is trained with both unimodal and multi-modal pre-training objectives and can be used for both unimodal vision or language tasks and multi-modal tasks. For example, OWL-ViT [enables](https://huggingface.co/spaces/adirik/OWL-ViT) zero-shot / text-guided and one-shot / image-guided object detection, CLIPSeg and GroupViT [enable](https://huggingface.co/spaces/nielsr/CLIPSeg) text and image-guided image segmentation, and VisualBERT, GIT and ViLT [enable](https://huggingface.co/spaces/nielsr/vilt-vqa) visual question answering as well as various other tasks. X-CLIP is a multi-modal model trained with video and text modalities and [enables](https://huggingface.co/spaces/fcakyon/zero-shot-video-classification) zero-shot video classification similar to CLIP‚Äôs zero-shot image classification capabilities. Unlike other models, the `VisionEncoderDecoderModel` is a cookie-cutter model that can be used to initialize an image-to-text model with any pre-trained Transformer-based vision model as the encoder (e.g.', 'type': 'tool', 'name': 'data_retriever', 'id': '7b9b7c73-b810-4c9c-879d-9501afd7a7fe', 'tool_call_id': 'call_ZTrCfrOe1ABB1sbdz3cmQXeF', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa55f3e684812d15f5859', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# FLAVA\\n## Overview\\nThe FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022. The paper aims at creating a single unified foundation model which can work across vision, language\\nas well as vision-and-language multimodal tasks. The abstract from the paper is the following:\\n\\\\*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety\\nof downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal\\n(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising\\ndirection would be to use a single holistic universal model, as a \"foundation\", that targets all modalities\\nat once -- a true vision and language foundation model should be good at vision tasks, language tasks, and\\ncross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate\\nimpressive performance on a wide range of 35 tasks spanning these target modalities.\\\\*\\nThis model was contributed by [aps](https://huggingface.co/aps). The original code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava). ## FlavaConfig\\n[[autodoc]] FlavaConfig\\n## FlavaTextConfig\\n[[autodoc]] FlavaTextConfig\\n## FlavaImageConfig\\n[[autodoc]] FlavaImageConfig\\n## FlavaMultimodalConfig\\n[[autodoc]] FlavaMultimodalConfig\\n## FlavaImageCodebookConfig\\n[[autodoc]] FlavaImageCodebookConfig\\n## FlavaProcessor\\n[[autodoc]] FlavaProcessor\\n## FlavaFeatureExtractor\\n[[autodoc]] FlavaFeatureExtractor\\n## FlavaImageProcessor\\n[[autodoc]] FlavaImageProcessor\\n- preprocess\\n## FlavaForPreTraining\\n[[autodoc]] FlavaForPreTraining\\n- forward\\n## FlavaModel\\n[[autodoc]] FlavaModel\\n- forward\\n- get\\\\_text\\\\_features\\n- get\\\\_image\\\\_features\\n## FlavaImageCodebook\\n[[autodoc]] FlavaImageCodebook\\n- forward\\n- get\\\\_codebook\\\\_indices\\n- get\\\\_codebook\\\\_probs\\n## FlavaTextModel\\n[[autodoc]] FlavaTextModel\\n- forward\\n## FlavaImageModel\\n[[autodoc]] FlavaImageModel\\n- forward\\n## FlavaMultimodalModel\\n[[autodoc]] FlavaMultimodalModel\\n- forward', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b595af3d2ebb270a02dd', 'page_content': 'Models such as [SimVLM](https://arxiv.org/abs/2108.10904) and [VirTex](https://arxiv.org/abs/2006.06666v3) use this pre-training objective and feature a unified multi-modal architecture consisting of a transformer encoder and transformer decoder, similar to that of an autoregressive language model. Let‚Äôs break this down and see how this works. Language models with a prefix objective predict the next token given an input text as the prefix. For example, given the sequence ‚ÄúA man is standing at the corner‚Äù, we can use ‚ÄúA man is standing at the‚Äù as the prefix and train the model with the objective of predicting the next token - ‚Äúcorner‚Äù or another plausible continuation of the prefix. Visual transformers (ViT) apply the same concept of the prefix to images by dividing each image into a number of patches and sequentially feeding these patches to the model as inputs. Leveraging this idea, SimVLM features an architecture where the encoder receives a concatenated image patch sequence and prefix text sequence as the prefix input, and the decoder then predicts the continuation of the textual sequence. The diagram above depicts this idea. The SimVLM model is first pre-trained on a text dataset without image patches present in the prefix and then on an aligned image-text dataset. These models are used for image-conditioned text generation/captioning and VQA tasks. Models that leverage a unified multi-modal architecture to fuse visual information into a language model (LM) for image-guided tasks show impressive capabilities. However, models that solely use the PrefixLM strategy can be limited in terms of application areas as they are mainly designed for image captioning or visual question-answering downstream tasks. For example, given an image of a group of people, we can query the image to write a description of the image (e.g., ‚ÄúA group of people is standing together in front of a building and smiling‚Äù) or query it with questions that require visual reasoning: ‚ÄúHow many people are wearing red t-shirts?‚Äù. On the other hand, models that learn multi-modal representations or adopt hybrid approaches can be adapted for various other downstream tasks, such as object detection and image segmentation. #### Frozen PrefixLM\\n\\n![Frozen PrefixLM](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/frozen_prefixlm.png)  \\n*Frozen PrefixLM pre-training strategy ([image source](https://lilianweng.github.io/posts/2022-06-09-vlm))*\\n\\nWhile fusing visual information into a language model is highly effective, being able to use a pre-trained language model (LM) without the need for fine-tuning would be much more efficient. Hence, another pre-training objective in vision-language models is learning image embeddings that are aligned with a frozen language model. Models such as [Frozen](https://arxiv.org/abs/2106.13884) and [ClipCap](https://arxiv.org/abs/2111.09734) use this Frozen PrefixLM pre-training objective. They only update the parameters of the image encoder during training to generate image embeddings that can be used as a prefix to the pre-trained, frozen language model in a similar fashion to the PrefixLM objective discussed above. Both Frozen and ClipCap are trained on aligned image-text (caption) datasets with the objective of generating the next token in the caption, given the image embeddings and the prefix text. Finally, models such as [MAPL](https://arxiv.org/abs/2210.07179) and [Flamingo](https://arxiv.org/abs/2204.14198) keep both the pre-trained vision encoder and language model frozen. Flamingo sets a new state-of-the-art in few-shot learning on a wide range of open-ended vision and language tasks by adding Perceiver Resampler modules on top of the pre-trained frozen vision model and inserting new cross-attention layers between existing pre-trained and frozen LM layers to condition the LM on visual data. A nifty advantage of the Frozen PrefixLM pre-training objective is it enables training with limited aligned image-text data, which is particularly useful for domains where aligned multi-modal datasets are not available. ### 3) Multi-modal Fusing with Cross Attention\\n\\n![Cross Attention Fusing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/cross_attention_fusing.png)  \\n *Fusing visual information with a cross-attention mechanism as shown ([image source](https://www.semanticscholar.org/paper/VisualGPT%3A-Data-efficient-Adaptation-of-Pretrained-Chen-Guo/616e0ed02ca024a8c1d4b86167f7486ea92a13d9))*\\n\\nAnother approach to leveraging pre-trained language models for multi-modal tasks is to directly fuse visual information into the layers of a language model decoder using a cross-attention mechanism instead of using images as additional prefixes to the language model.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b596af3d2ebb270a02e7', 'page_content': 'Also, thanks to Niels Rogge, Younes Belkada, and Suraj Patil, among many others at Hugging Face, who laid out the foundations for increasing the use of multi-modal models from Transformers.\\\\*', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b595af3d2ebb270a02e2', 'page_content': 'In addition, FLAVA is trained with both unimodal and multi-modal pre-training objectives and can be used for both unimodal vision or language tasks and multi-modal tasks. For example, OWL-ViT [enables](https://huggingface.co/spaces/adirik/OWL-ViT) zero-shot / text-guided and one-shot / image-guided object detection, CLIPSeg and GroupViT [enable](https://huggingface.co/spaces/nielsr/CLIPSeg) text and image-guided image segmentation, and VisualBERT, GIT and ViLT [enable](https://huggingface.co/spaces/nielsr/vilt-vqa) visual question answering as well as various other tasks. X-CLIP is a multi-modal model trained with video and text modalities and [enables](https://huggingface.co/spaces/fcakyon/zero-shot-video-classification) zero-shot video classification similar to CLIP‚Äôs zero-shot image classification capabilities. Unlike other models, the `VisionEncoderDecoderModel` is a cookie-cutter model that can be used to initialize an image-to-text model with any pre-trained Transformer-based vision model as the encoder (e.g.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The large multimodal model that can solve image-text tasks and is based on Flamingo is called FLAVA.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--4187843e-c2e7-4554-ab46-a19b793ce633-0', 'usage_metadata': {'input_tokens': 2387, 'output_tokens': 22, 'total_tokens': 2409, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:26:26,726 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 15%|‚ñà‚ñå        | 10/65 [00:40<03:36,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of the `gradio.Blocks` API?\\n', 'type': 'human', 'id': '1df5bb64-2669-42e0-9a63-2fb37653f823'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_IRBSA4SBunt5cLnJSi5Pa2x2', 'function': {'arguments': '{\"query\":\"gradio.Blocks API purpose\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--7c9e6dac-8431-4517-9dc3-0b4976b6e51f-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'gradio.Blocks API purpose'}, 'id': 'call_IRBSA4SBunt5cLnJSi5Pa2x2', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 21, 'total_tokens': 383, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our \\\\*\\\\*newly developed\\\\*\\\\* low-level API called `gradio.Blocks`. Now, what\\'s the difference between `Interface` and `Blocks`? - ‚ö° `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs. - üß± `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \"building blocks\"). ### Why Blocks üß±?[[why-blocks-]]\\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\\n- Group together related demos as multiple tabs in one web application\\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\\n- Change a component\\'s properties (for example, the choices in a dropdown) or its visibility based on user input\\nWe will explore all of these concepts below. ### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook. ```py\\nimport gradio as gr\\ndef flip\\\\_text(x):\\nreturn x[::-1]\\ndemo = gr.Blocks()\\nwith demo:\\ngr.Markdown(\\n\"\"\"\\n# Flip Text! Start typing below to see the output. \"\"\"\\n)\\ninput = gr.Textbox(placeholder=\"Flip this text\")\\noutput = gr.Textbox()\\ninput.change(fn=flip\\\\_text, inputs=input, outputs=output)\\ndemo.launch()\\n```\\nThis simple example above introduces 4 concepts that underlie Blocks:\\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.\\n\\nBlocks and Event Listeners\\nWe briefly descirbed the Blocks class in the [Quickstart](/main/guides/quickstart#custom-demos-with-gr-blocks) as a way to build custom demos. Let\\'s dive deeper. ## Blocks Structure\\nTake a look at the demo below. $code\\\\_hello\\\\_blocks\\n$demo\\\\_hello\\\\_blocks\\n- First, note the `with gr.Blocks() as demo:` clause. The Blocks app code will be contained within this clause. - Next come the Components. These are the same Components used in `Interface`. However, instead of being passed to some constructor, Components are automatically added to the Blocks as they are created within the `with` clause. - Finally, the `click()` event listener. Event listeners define the data flow within the app. In the example above, the listener ties the two Textboxes together. The Textbox `name` acts as the input and Textbox `output` acts as the output to the `greet` method. This dataflow is triggered when the Button `greet\\\\_btn` is clicked. Like an Interface, an event listener can take multiple inputs or outputs. You can also attach event listeners using decorators - skip the `fn` argument and assign `inputs` and `outputs` directly:\\n$code\\\\_hello\\\\_blocks\\\\_decorator\\n## Event Listeners and Interactivity\\nIn the example above, you\\'ll notice that you are able to edit Textbox `name`, but not Textbox `output`. This is because any Component that acts as an input to an event listener is made interactive. However, since Textbox `output` acts only as an output, Gradio determines that it should not be made interactive. You can override the default behavior and directly configure the interactivity of a Component with the boolean `interactive` keyword argument. ```python\\noutput = gr.Textbox(label=\"Output\", interactive=True)\\n```\\n\\\\_Note\\\\_: What happens if a Gradio component is neither an input nor an output?\\n\\nBy building these demos, we can make state-of-the-art machine learning accessible, not just to engineers, but anyone who can use an Internet browser! Even if you\\'ve never used Gradio before, this is the perfect time to start, because the Blocks Party is running until the end of May. We\\'ll be giving out ü§ó merch and other prizes at the end of the Party for demos built using Blocks. Learn more about Blocks Party here: https://huggingface.co/spaces/Gradio-Blocks/README\\n\\n### 7. Select all the steps necessary for adding state to your Gradio interface\\n### 8. Which of the following are components included in the Gradio library? ### 9. What does Gradio `Blocks` allow you to do? ### 10. You can share a public link to a `Blocks` demo and host a `Blocks` demo on Hugging Face spaces.', 'type': 'tool', 'name': 'data_retriever', 'id': 'eff77f24-c65d-4214-b9a2-30321f7eacc5', 'tool_call_id': 'call_IRBSA4SBunt5cLnJSi5Pa2x2', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa7693e684812d15f5ace', 'page_content': 'Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our \\\\*\\\\*newly developed\\\\*\\\\* low-level API called `gradio.Blocks`. Now, what\\'s the difference between `Interface` and `Blocks`? - ‚ö° `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs. - üß± `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \"building blocks\"). ### Why Blocks üß±?[[why-blocks-]]\\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\\n- Group together related demos as multiple tabs in one web application\\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\\n- Change a component\\'s properties (for example, the choices in a dropdown) or its visibility based on user input\\nWe will explore all of these concepts below. ### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook. ```py\\nimport gradio as gr\\ndef flip\\\\_text(x):\\nreturn x[::-1]\\ndemo = gr.Blocks()\\nwith demo:\\ngr.Markdown(\\n\"\"\"\\n# Flip Text! Start typing below to see the output. \"\"\"\\n)\\ninput = gr.Textbox(placeholder=\"Flip this text\")\\noutput = gr.Textbox()\\ninput.change(fn=flip\\\\_text, inputs=input, outputs=output)\\ndemo.launch()\\n```\\nThis simple example above introduces 4 concepts that underlie Blocks:\\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bb0caf3d2ebb270a0985', 'page_content': 'Blocks and Event Listeners\\nWe briefly descirbed the Blocks class in the [Quickstart](/main/guides/quickstart#custom-demos-with-gr-blocks) as a way to build custom demos. Let\\'s dive deeper. ## Blocks Structure\\nTake a look at the demo below. $code\\\\_hello\\\\_blocks\\n$demo\\\\_hello\\\\_blocks\\n- First, note the `with gr.Blocks() as demo:` clause. The Blocks app code will be contained within this clause. - Next come the Components. These are the same Components used in `Interface`. However, instead of being passed to some constructor, Components are automatically added to the Blocks as they are created within the `with` clause. - Finally, the `click()` event listener. Event listeners define the data flow within the app. In the example above, the listener ties the two Textboxes together. The Textbox `name` acts as the input and Textbox `output` acts as the output to the `greet` method. This dataflow is triggered when the Button `greet\\\\_btn` is clicked. Like an Interface, an event listener can take multiple inputs or outputs. You can also attach event listeners using decorators - skip the `fn` argument and assign `inputs` and `outputs` directly:\\n$code\\\\_hello\\\\_blocks\\\\_decorator\\n## Event Listeners and Interactivity\\nIn the example above, you\\'ll notice that you are able to edit Textbox `name`, but not Textbox `output`. This is because any Component that acts as an input to an event listener is made interactive. However, since Textbox `output` acts only as an output, Gradio determines that it should not be made interactive. You can override the default behavior and directly configure the interactivity of a Component with the boolean `interactive` keyword argument. ```python\\noutput = gr.Textbox(label=\"Output\", interactive=True)\\n```\\n\\\\_Note\\\\_: What happens if a Gradio component is neither an input nor an output?', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa9e73e684812d15f5d71', 'page_content': \"By building these demos, we can make state-of-the-art machine learning accessible, not just to engineers, but anyone who can use an Internet browser! Even if you've never used Gradio before, this is the perfect time to start, because the Blocks Party is running until the end of May. We'll be giving out ü§ó merch and other prizes at the end of the Party for demos built using Blocks. Learn more about Blocks Party here: https://huggingface.co/spaces/Gradio-Blocks/README\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa7c33e684812d15f5b3a', 'page_content': '### 7. Select all the steps necessary for adding state to your Gradio interface\\n### 8. Which of the following are components included in the Gradio library? ### 9. What does Gradio `Blocks` allow you to do? ### 10. You can share a public link to a `Blocks` demo and host a `Blocks` demo on Hugging Face spaces.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The purpose of the `gradio.Blocks` API is to provide a low-level API that allows users to have full control over the data flows and layout of their applications. It enables the creation of complex, multi-step applications by allowing developers to group related demos, change layouts, implement flexible data flows, and modify component properties based on user input. This flexibility is in contrast to the high-level `Interface` API, which is easier to use but lacks the same level of customization.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--c4c85ef5-25b3-4c68-a5a7-2aa9ff5e7ed9-0', 'usage_metadata': {'input_tokens': 1383, 'output_tokens': 97, 'total_tokens': 1480, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:26:31,514 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 17%|‚ñà‚ñã        | 11/65 [00:45<03:46,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\\n', 'type': 'human', 'id': 'ae19c242-0f44-40de-9325-29f7b5e485d7'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_MPDAEcf85buLKaCjerbyF4Vn', 'function': {'arguments': '{\"query\":\"Hierarchical Text-Conditional Image Generation with CLIP Latents\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--4627bddf-174b-4fe8-af73-0757e97786e7-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Hierarchical Text-Conditional Image Generation with CLIP Latents'}, 'id': 'call_MPDAEcf85buLKaCjerbyF4Vn', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 376, 'output_tokens': 27, 'total_tokens': 403, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# unCLIP\\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in ü§ó Diffusers comes from kakaobrain\\'s [karlo](https://github.com/kakaobrain/karlo). The abstract from the paper is following:\\n\\\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\\\*\\nYou can find lucidrains\\' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Prior Transformer\\nThe Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process. The abstract from the paper is:\\n\\\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\\\*\\n## PriorTransformer\\n[[autodoc]] PriorTransformer\\n## PriorTransformerOutput\\n[[autodoc]] models.prior\\\\_transformer.PriorTransformerOutput\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Stable unCLIP\\nStable unCLIP checkpoints are finetuned from [Stable Diffusion 2.1](./stable\\\\_diffusion/stable\\\\_diffusion\\\\_2) checkpoints to condition on CLIP image embeddings. Stable unCLIP still conditions on text embeddings. Given the two separate conditionings, stable unCLIP can be used\\nfor text guided image variation. When combined with an unCLIP prior, it can also be used for full text to image generation. The abstract from the paper is:\\n\\\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\\\*\\n## Tips\\nStable unCLIP takes `noise\\\\_level` as input during inference which determines how much noise is added to the image embeddings.\\n\\n!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# CLIP\\n## Overview\\nThe CLIP model was proposed in [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. CLIP\\n(Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs.', 'type': 'tool', 'name': 'data_retriever', 'id': '774c0069-334e-4612-9ee1-c6be00ba5eb8', 'tool_call_id': 'call_MPDAEcf85buLKaCjerbyF4Vn', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b493af3d2ebb270a01d4', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# unCLIP\\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in ü§ó Diffusers comes from kakaobrain\\'s [karlo](https://github.com/kakaobrain/karlo). The abstract from the paper is following:\\n\\\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\\\*\\nYou can find lucidrains\\' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c0ca666af48523f51f51', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Prior Transformer\\nThe Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process. The abstract from the paper is:\\n\\\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\\\*\\n## PriorTransformer\\n[[autodoc]] PriorTransformer\\n## PriorTransformerOutput\\n[[autodoc]] models.prior\\\\_transformer.PriorTransformerOutput', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab8523e684812d15f6ea2', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Stable unCLIP\\nStable unCLIP checkpoints are finetuned from [Stable Diffusion 2.1](./stable\\\\_diffusion/stable\\\\_diffusion\\\\_2) checkpoints to condition on CLIP image embeddings. Stable unCLIP still conditions on text embeddings. Given the two separate conditionings, stable unCLIP can be used\\nfor text guided image variation. When combined with an unCLIP prior, it can also be used for full text to image generation. The abstract from the paper is:\\n\\\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\\\*\\n## Tips\\nStable unCLIP takes `noise\\\\_level` as input during inference which determines how much noise is added to the image embeddings.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899baeeaf3d2ebb270a095f', 'page_content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# CLIP\\n## Overview\\nThe CLIP model was proposed in [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. CLIP\\n(Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" is to leverage robust representations of images learned by contrastive models like CLIP for image generation. The model consists of a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. This approach aims to improve image diversity with minimal loss in photorealism and caption similarity, allowing for variations of an image that preserve its semantics and style while varying non-essential details. Additionally, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f25ae815-fe0e-4381-9ffe-3320135f9291-0', 'usage_metadata': {'input_tokens': 1949, 'output_tokens': 134, 'total_tokens': 2083, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:26:35,296 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 18%|‚ñà‚ñä        | 12/65 [00:48<03:35,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What command is used to install the requirements for a research project using ü§ó Transformers?\\n', 'type': 'human', 'id': 'b970b030-357b-4d21-a53d-9ef7397c63b0'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_RCuBsX7egfkZVdMYGISj0Xw3', 'function': {'arguments': '{\"query\":\"install requirements for research project using ü§ó Transformers\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--86126926-d70a-4162-9314-db08a09b8c71-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'install requirements for research project using ü§ó Transformers'}, 'id': 'call_RCuBsX7egfkZVdMYGISj0Xw3', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 24, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!---\\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n# Research projects\\nThis folder contains various research projects using ü§ó Transformers. They are not maintained and require a specific\\nversion of ü§ó Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\\n```\\npip install -r requirements.txt\\n```\\ninside the folder of your choice. If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder.\\n\\n!---\\nCopyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Installation\\nInstall ü§ó Transformers for whichever deep learning library you\\'re working with, setup your cache, and optionally configure ü§ó Transformers to run offline. ü§ó Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:\\n\\\\* [PyTorch](https://pytorch.org/get-started/locally/) installation instructions.\\n\\nMost parts of the code you will add for\\nthe new ü§ó Transformers model already exist somewhere in ü§ó\\nTransformers. Take some time to find similar, already existing\\nmodels and tokenizers you can copy from. [grep](https://www.gnu.org/software/grep/) and\\n[rg](https://github.com/BurntSushi/ripgrep) are your friends. Note\\nthat it might very well happen that your model\\'s tokenizer is based\\non one model implementation, and your model\\'s modeling code on\\nanother one. \\\\*E.g.\\\\*, FSMT\\'s modeling code is based on BART, while\\nFSMT\\'s tokenizer code is based on XLM. - It\\'s more of an engineering challenge than a scientific challenge. You should spend more time on creating an efficient debugging\\nenvironment than trying to understand all theoretical aspects of the\\nmodel in the paper. - Ask for help when you\\'re stuck! Models are the core component of ü§ó\\nTransformers so we, at Hugging Face, are more than happy to help\\nyou at every step to add your model. Don\\'t hesitate to ask if you\\nnotice you are not making progress. In the following, we try to give you a general recipe that we found most\\nuseful when porting a model to ü§ó Transformers. The following list is a summary of everything that has to be done to add\\na model and can be used by you as a To-Do List:\\n1.\\n\\nMost parts of the code you will add for\\nthe new ü§ó Transformers model already exist somewhere in ü§ó\\nTransformers. Take some time to find similar, already existing\\nmodels and tokenizers you can copy from. [grep](https://www.gnu.org/software/grep/) and\\n[rg](https://github.com/BurntSushi/ripgrep) are your friends. Note\\nthat it might very well happen that your model\\'s tokenizer is based\\non one model implementation, and your model\\'s modeling code on\\nanother one. \\\\*E.g.\\\\*, FSMT\\'s modeling code is based on BART, while\\nFSMT\\'s tokenizer code is based on XLM. - It\\'s more of an engineering challenge than a scientific challenge. You should spend more time on creating an efficient debugging\\nenvironment than trying to understand all theoretical aspects of the\\nmodel in the paper. - Ask for help when you\\'re stuck! Models are the core component of ü§ó\\nTransformers so we, at Hugging Face, are more than happy to help\\nyou at every step to add your model. Don\\'t hesitate to ask if you\\nnotice you are not making progress. In the following, we try to give you a general recipe that we found most\\nuseful when porting a model to ü§ó Transformers. The following list is a summary of everything that has to be done to add\\na model and can be used by you as a To-Do List:\\n1.', 'type': 'tool', 'name': 'data_retriever', 'id': 'e79e448f-43ba-4dd4-8a1c-e0f15a32ba7f', 'tool_call_id': 'call_RCuBsX7egfkZVdMYGISj0Xw3', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b91aaf3d2ebb270a0725', 'page_content': '!---\\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n# Research projects\\nThis folder contains various research projects using ü§ó Transformers. They are not maintained and require a specific\\nversion of ü§ó Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\\n```\\npip install -r requirements.txt\\n```\\ninside the folder of your choice. If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa6af3e684812d15f59de', 'page_content': '!---\\nCopyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Installation\\nInstall ü§ó Transformers for whichever deep learning library you\\'re working with, setup your cache, and optionally configure ü§ó Transformers to run offline. ü§ó Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:\\n\\\\* [PyTorch](https://pytorch.org/get-started/locally/) installation instructions.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b2deaf3d2ebb2709ff2a', 'page_content': \"Most parts of the code you will add for\\nthe new ü§ó Transformers model already exist somewhere in ü§ó\\nTransformers. Take some time to find similar, already existing\\nmodels and tokenizers you can copy from. [grep](https://www.gnu.org/software/grep/) and\\n[rg](https://github.com/BurntSushi/ripgrep) are your friends. Note\\nthat it might very well happen that your model's tokenizer is based\\non one model implementation, and your model's modeling code on\\nanother one. \\\\*E.g.\\\\*, FSMT's modeling code is based on BART, while\\nFSMT's tokenizer code is based on XLM. - It's more of an engineering challenge than a scientific challenge. You should spend more time on creating an efficient debugging\\nenvironment than trying to understand all theoretical aspects of the\\nmodel in the paper. - Ask for help when you're stuck! Models are the core component of ü§ó\\nTransformers so we, at Hugging Face, are more than happy to help\\nyou at every step to add your model. Don't hesitate to ask if you\\nnotice you are not making progress. In the following, we try to give you a general recipe that we found most\\nuseful when porting a model to ü§ó Transformers. The following list is a summary of everything that has to be done to add\\na model and can be used by you as a To-Do List:\\n1.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aba463e684812d15f70b1', 'page_content': \"Most parts of the code you will add for\\nthe new ü§ó Transformers model already exist somewhere in ü§ó\\nTransformers. Take some time to find similar, already existing\\nmodels and tokenizers you can copy from. [grep](https://www.gnu.org/software/grep/) and\\n[rg](https://github.com/BurntSushi/ripgrep) are your friends. Note\\nthat it might very well happen that your model's tokenizer is based\\non one model implementation, and your model's modeling code on\\nanother one. \\\\*E.g.\\\\*, FSMT's modeling code is based on BART, while\\nFSMT's tokenizer code is based on XLM. - It's more of an engineering challenge than a scientific challenge. You should spend more time on creating an efficient debugging\\nenvironment than trying to understand all theoretical aspects of the\\nmodel in the paper. - Ask for help when you're stuck! Models are the core component of ü§ó\\nTransformers so we, at Hugging Face, are more than happy to help\\nyou at every step to add your model. Don't hesitate to ask if you\\nnotice you are not making progress. In the following, we try to give you a general recipe that we found most\\nuseful when porting a model to ü§ó Transformers. The following list is a summary of everything that has to be done to add\\na model and can be used by you as a To-Do List:\\n1.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The command used to install the requirements for a research project using ü§ó Transformers is:\\n\\n```\\npip install -r requirements.txt\\n``` \\n\\nYou should run this command inside the folder of your choice.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--3cc3c5ab-cb91-4c5a-bd76-f3dba64435f1-0', 'usage_metadata': {'input_tokens': 1368, 'output_tokens': 40, 'total_tokens': 1408, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:26:40,486 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 20%|‚ñà‚ñà        | 13/65 [00:54<03:49,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What task does the `roberta-large-mnli` checkpoint perform?\\n', 'type': 'human', 'id': '3fea9174-7de7-424d-b755-e368e0ae2074'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_dNy8yGaVlwgBhs8z5d1cbFhq', 'function': {'arguments': '{\"query\":\"roberta-large-mnli checkpoint task\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--1bb2706f-7747-4bbf-9dbe-fc0df0536038-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'roberta-large-mnli checkpoint task'}, 'id': 'call_dNy8yGaVlwgBhs8z5d1cbFhq', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 363, 'output_tokens': 23, 'total_tokens': 386, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '- [Question answering task guide](../tasks/question\\\\_answering)\\n\\\\*\\\\*Multiple choice\\\\*\\\\*\\n- [`RobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice.ipynb). - [`TFRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice-tf.ipynb). - [Multiple choice task guide](../tasks/multiple\\\\_choice)\\n## RobertaConfig\\n[[autodoc]] RobertaConfig\\n## RobertaTokenizer\\n[[autodoc]] RobertaTokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## RobertaTokenizerFast\\n[[autodoc]] RobertaTokenizerFast\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n\\n## RobertaModel\\n[[autodoc]] RobertaModel\\n- forward\\n## RobertaForCausalLM\\n[[autodoc]] RobertaForCausalLM\\n- forward\\n## RobertaForMaskedLM\\n[[autodoc]] RobertaForMaskedLM\\n- forward\\n## RobertaForSequenceClassification\\n[[autodoc]] RobertaForSequenceClassification\\n- forward\\n## RobertaForMultipleChoice\\n[[autodoc]] RobertaForMultipleChoice\\n- forward\\n## RobertaForTokenClassification\\n[[autodoc]] RobertaForTokenClassification\\n- forward\\n## RobertaForQuestionAnswering\\n[[autodoc]] RobertaForQuestionAnswering\\n- forward\\n\\n## TFRobertaModel\\n[[autodoc]] TFRobertaModel\\n- call\\n## TFRobertaForCausalLM\\n[[autodoc]] TFRobertaForCausalLM\\n- call\\n## TFRobertaForMaskedLM\\n[[autodoc]] TFRobertaForMaskedLM\\n- call\\n## TFRobertaForSequenceClassification\\n[[autodoc]] TFRobertaForSequenceClassification\\n- call\\n## TFRobertaForMultipleChoice\\n[[autodoc]] TFRobertaForMultipleChoice\\n- call\\n## TFRobertaForTokenClassification\\n[[autodoc]] TFRobertaForTokenClassification\\n- call\\n## TFRobertaForQuestionAnswering\\n[[autodoc]] TFRobertaForQuestionAnswering\\n- call\\n\\n## FlaxRobertaModel\\n[[autodoc]] FlaxRobertaModel\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForCausalLM\\n[[autodoc]] FlaxRobertaForCausalLM\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForMaskedLM\\n[[autodoc]] FlaxRobertaForMaskedLM\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForSequenceClassification\\n[[autodoc]] FlaxRobertaForSequenceClassification\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForMultipleChoice\\n[[autodoc]] FlaxRobertaForMultipleChoice\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForTokenClassification\\n[[autodoc]] FlaxRobertaForTokenClassification\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForQuestionAnswering\\n[[autodoc]] FlaxRobertaForQuestionAnswering\\n- \\\\_\\\\_call\\\\_\\\\_\\n\\n- Uses RoBERTa tricks on the XLM approach, but does not use the translation language modeling objective. It only uses masked language modeling on sentences coming from one language. ## Resources\\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with XLM-RoBERTa. If you\\'re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\\'ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. - A blog post on how to [finetune XLM RoBERTa for multiclass classification with Habana Gaudi on AWS](https://www.philschmid.de/habana-distributed-training)\\n- [`XLMRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\\\_classification.ipynb). - [`TFXLMRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\\\_classification-tf.ipynb). - [`FlaxXLMRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\\\_classification\\\\_flax.ipynb). - [Text classification](https://huggingface.co/docs/transformers/tasks/sequence\\\\_classification) chapter of the ü§ó Hugging Face Task Guides. - [Text classification task guide](../tasks/sequence\\\\_classification)\\n- [`XLMRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token\\\\_classification.ipynb). - [`TFXLMRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token\\\\_classification-tf.ipynb). - [`FlaxXLMRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification). - [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the ü§ó Hugging Face Course. - [Token classification task guide](../tasks/token\\\\_classification)\\n- [`XLMRobertaForCausalLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language\\\\_modeling.ipynb). - [Causal language modeling](https://huggingface.co/docs/transformers/tasks/language\\\\_modeling) chapter of the ü§ó Hugging Face Task Guides. - [Causal language modeling task guide](../tasks/language\\\\_modeling)\\n- [`XLMRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language\\\\_modeling.ipynb). - [`TFXLMRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run\\\\_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language\\\\_modeling-tf.ipynb). - [`FlaxXLMRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked\\\\_language\\\\_modeling\\\\_flax.ipynb). - [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the ü§ó Hugging Face Course. - [Masked language modeling](../tasks/masked\\\\_language\\\\_modeling)\\n- [`XLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question\\\\_answering.ipynb). - [`TFXLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question\\\\_answering-tf.ipynb). - [`FlaxXLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering). - [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the ü§ó Hugging Face Course. - [Question answering task guide](../tasks/question\\\\_answering)\\n\\\\*\\\\*Multiple choice\\\\*\\\\*\\n- [`XLMRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice.ipynb). - [`TFXLMRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice-tf.ipynb). - [Multiple choice task guide](../tasks/multiple\\\\_choice)\\nüöÄ Deploy\\n- A blog post on how to [Deploy Serverless XLM RoBERTa on AWS Lambda](https://www.philschmid.de/multilingual-serverless-xlm-roberta-with-huggingface).\\n\\nAs a result, the first word of the sentence is encoded differently if it is not preceded by a white space. To ensure the first word includes a space, we set `add\\\\_prefix\\\\_space=True`. Also, to maintain consistent pre-processing for all three models, we set the parameter to \\'True\\' for Llama 2 and Mistral 7b. - Define the preprocessing function for converting one row of the dataframe:\\n```python\\ndef roberta\\\\_preprocessing\\\\_function(examples):\\nreturn roberta\\\\_tokenizer(examples[\\'text\\'], truncation=True, max\\\\_length=MAX\\\\_LEN)\\n```\\nBy applying the preprocessing function to the first example of our training dataset, we have the tokenized inputs (`input\\\\_ids`) and the attention mask:\\n```python\\nroberta\\\\_preprocessing\\\\_function(data[\\'train\\'][0])\\n```\\n```\\n{\\'input\\\\_ids\\': [0, 127, 2373, 2490, 4, 1205, 640, 90, 4, 876, 73, 118, 725, 398, 13083, 329, 398, 119, 1343, 246, 2], \\'attention\\\\_mask\\': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\\n```\\n- Now, let\\'s apply the preprocessing function to the entire dataset:\\n```python\\ncol\\\\_to\\\\_delete = [\\'id\\', \\'keyword\\',\\'location\\', \\'text\\']\\n# Apply the preprocessing function and remove the undesired columns\\nroberta\\\\_tokenized\\\\_datasets = data.map(roberta\\\\_preprocessing\\\\_function, batched=True, remove\\\\_columns=col\\\\_to\\\\_delete)\\n# Rename the target to label as for HugginFace standards\\nroberta\\\\_tokenized\\\\_datasets = roberta\\\\_tokenized\\\\_datasets.rename\\\\_column(\"target\", \"label\")\\n# Set to torch format\\nroberta\\\\_tokenized\\\\_datasets.set\\\\_format(\"torch\")\\n```\\n\\\\*\\\\*Note:\\\\*\\\\* we deleted the undesired columns from our data: id, keyword, location and text. We have deleted the text because we have already converted it into the inputs ids and the attention mask:\\nWe can have a look into our tokenized training dataset:\\n```python\\nroberta\\\\_tokenized\\\\_datasets[\\'train\\'][0]\\n```\\n```\\n{\\'label\\': tensor(0),\\n\\'input\\\\_ids\\': tensor([ 0, 127, 2373, 2490, 4, 1205, 640, 90, 4, 876,\\n73, 118, 725, 398, 13083, 329, 398, 119, 1343, 246,\\n2]),\\n\\'attention\\\\_mask\\': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\\n```\\n- For generating the training batches, we also need to pad the rows of a given batch to the maximum length found in the batch. For that, we will use the `DataCollatorWithPadding` class:\\n```python\\n# Data collator for padding a batch of examples to the maximum length seen in the batch\\nfrom transformers import DataCollatorWithPadding\\nroberta\\\\_data\\\\_collator = DataCollatorWithPadding(tokenizer=roberta\\\\_tokenizer)\\n```\\nYou can follow the same steps for preparing the data for Mistral 7B and Llama 2 models:\\n\\\\*\\\\*Note\\\\*\\\\* that Llama 2 and Mistral 7B don\\'t have a default `pad\\\\_token\\\\_id`. So, we use the `eos\\\\_token\\\\_id` for padding as well. - Mistral 7B:\\n```python\\n# Load Mistral 7B Tokenizer\\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\\nmistral\\\\_tokenizer = AutoTokenizer.from\\\\_pretrained(mistral\\\\_checkpoint, add\\\\_prefix\\\\_space=True)\\nmistral\\\\_tokenizer.pad\\\\_token\\\\_id = mistral\\\\_tokenizer.eos\\\\_token\\\\_id\\nmistral\\\\_tokenizer.pad\\\\_token = mistral\\\\_tokenizer.eos\\\\_token\\ndef mistral\\\\_preprocessing\\\\_function(examples):\\nreturn mistral\\\\_tokenizer(examples[\\'text\\'], truncation=True, max\\\\_length=MAX\\\\_LEN)\\nmistral\\\\_tokenized\\\\_datasets = data.map(mistral\\\\_preprocessing\\\\_function, batched=True, remove\\\\_columns=col\\\\_to\\\\_delete)\\nmistral\\\\_tokenized\\\\_datasets = mistral\\\\_tokenized\\\\_datasets.rename\\\\_column(\"target\", \"label\")\\nmistral\\\\_tokenized\\\\_datasets.set\\\\_format(\"torch\")\\n# Data collator for padding a batch of examples to the maximum length seen in the batch\\nmistral\\\\_data\\\\_collator = DataCollatorWithPadding(tokenizer=mistral\\\\_tokenizer)\\n```\\n- Llama 2:\\n```python\\n# Load Llama 2 Tokenizer\\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\\nllama\\\\_tokenizer = AutoTokenizer.from\\\\_pretrained(llama\\\\_checkpoint, add\\\\_prefix\\\\_space=True)\\nllama\\\\_tokenizer.pad\\\\_token\\\\_id = llama\\\\_tokenizer.eos\\\\_token\\\\_id\\nllama\\\\_tokenizer.pad\\\\_token = llama\\\\_tokenizer.eos\\\\_token\\ndef llama\\\\_preprocessing\\\\_function(examples):\\nreturn llama\\\\_tokenizer(examples[\\'text\\'], truncation=True, max\\\\_length=MAX\\\\_LEN)\\nllama\\\\_tokenized\\\\_datasets = data.map(llama\\\\_preprocessing\\\\_function, batched=True, remove\\\\_columns=col\\\\_to\\\\_delete)\\nllama\\\\_tokenized\\\\_datasets = llama\\\\_tokenized\\\\_datasets.rename\\\\_column(\"target\", \"label\")\\nllama\\\\_tokenized\\\\_datasets.set\\\\_format(\"torch\")\\n# Data collator for padding a batch of examples to the maximum length seen in the batch\\nllama\\\\_data\\\\_collator = DataCollatorWithPadding(tokenizer=llama\\\\_tokenizer)\\n```\\nNow that we have prepared the tokenized datasets, the next section will showcase how to load the pre-trained LLMs checkpoints and how to set the LoRa weights. ## Models\\n### RoBERTa\\n#### Load RoBERTa Checkpoints for the Classification Task\\nWe load the pre-trained RoBERTa model with a sequence classification head using the Hugging Face `AutoModelForSequenceClassification` class:\\n```python\\nfrom transformers import AutoModelForSequenceClassification\\nroberta\\\\_model = AutoModelForSequenceClassification.from\\\\_pretrained(roberta\\\\_checkpoint, num\\\\_labels=2)\\n```\\n#### LoRA setup for RoBERTa classifier\\nWe import LoRa configuration and set some parameters for RoBERTa classifier:\\n- TaskType: Sequence classification\\n- r(rank): Rank for our decomposition matrices\\n- lora\\\\_alpha: Alpha parameter to scale the learned weights.\\n\\nIts primary objective is to reduce the model\\'s trainable parameters. LoRA\\'s operation involves\\nlearning a low rank update matrix while keeping the pre-trained weights frozen. ![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral/lora.png)\\n## Setup\\nRoBERTa has a limitatiom of maximum sequence length of 512, so we set the `MAX\\\\_LEN=512` for all models to ensure a fair comparison. ```python\\nMAX\\\\_LEN = 512\\nroberta\\\\_checkpoint = \"roberta-large\"\\nmistral\\\\_checkpoint = \"mistralai/Mistral-7B-v0.1\"\\nllama\\\\_checkpoint = \"meta-llama/Llama-2-7b-hf\"\\n```\\n## Data preparation\\n### Data loading\\nWe will load the dataset from Hugging Face:\\n```python\\nfrom datasets import load\\\\_dataset\\ndataset = load\\\\_dataset(\"mehdiiraqui/twitter\\\\_disaster\")\\n```\\nNow, let\\'s split the dataset into training and validation datasets. Then add the test set:\\n```python\\nfrom datasets import Dataset\\n# Split the dataset into training and validation datasets\\ndata = dataset[\\'train\\'].train\\\\_test\\\\_split(train\\\\_size=0.8, seed=42)\\n# Rename the default \"test\" split to \"validation\"\\ndata[\\'val\\'] = data.pop(\"test\")\\n# Convert the test dataframe to HuggingFace dataset and add it into the first dataset\\ndata[\\'test\\'] = dataset[\\'test\\']\\n```\\nHere\\'s an overview of the dataset:\\n```bash\\nDatasetDict({\\ntrain: Dataset({\\nfeatures: [\\'id\\', \\'keyword\\', \\'location\\', \\'text\\', \\'target\\'],\\nnum\\\\_rows: 6090\\n})\\nval: Dataset({\\nfeatures: [\\'id\\', \\'keyword\\', \\'location\\', \\'text\\', \\'target\\'],\\nnum\\\\_rows: 1523\\n})\\ntest: Dataset({\\nfeatures: [\\'id\\', \\'keyword\\', \\'location\\', \\'text\\', \\'target\\'],\\nnum\\\\_rows: 3263\\n})\\n})\\n```\\nLet\\'s check the data distribution:\\n```python\\nimport pandas as pd\\ndata[\\'train\\'].to\\\\_pandas().info()\\ndata[\\'test\\'].to\\\\_pandas().info()\\n```\\n- Train dataset\\n```\\nRangeIndex: 7613 entries, 0 to 7612\\nData columns (total 5 columns):\\n# Column Non-Null Count Dtype\\n--- ------ -------------- -----\\n0 id 7613 non-null int64\\n1 keyword 7552 non-null object\\n2 location 5080 non-null object\\n3 text 7613 non-null object\\n4 target 7613 non-null int64\\ndtypes: int64(2), object(3)\\nmemory usage: 297.5+ KB\\n```\\n- Test dataset\\n```\\nRangeIndex: 3263 entries, 0 to 3262\\nData columns (total 5 columns):\\n# Column Non-Null Count Dtype\\n--- ------ -------------- -----\\n0 id 3263 non-null int64\\n1 keyword 3237 non-null object\\n2 location 2158 non-null object\\n3 text 3263 non-null object\\n4 target 3263 non-null int64\\ndtypes: int64(2), object(3)\\nmemory usage: 127.6+ KB\\n```\\n\\\\*\\\\*Target distribution in the train dataset\\\\*\\\\*\\n```\\ntarget\\n0 4342\\n1 3271\\nName: count, dtype: int64\\n```\\nAs the classes are not balanced, we will compute the positive and negative weights and use them for loss calculation later:\\n```python\\npos\\\\_weights = len(data[\\'train\\'].to\\\\_pandas()) / (2 \\\\* data[\\'train\\'].to\\\\_pandas().target.value\\\\_counts()[1])\\nneg\\\\_weights = len(data[\\'train\\'].to\\\\_pandas()) / (2 \\\\* data[\\'train\\'].to\\\\_pandas().target.value\\\\_counts()[0])\\n```\\nThe final weights are:\\n```\\nPOS\\\\_WEIGHT, NEG\\\\_WEIGHT = (1.1637114032405993, 0.8766697374481806)\\n```\\nThen, we compute the maximum length of the column text:\\n```python\\n# Number of Characters\\nmax\\\\_char = data[\\'train\\'].to\\\\_pandas()[\\'text\\'].str.len().max()\\n# Number of Words\\nmax\\\\_words = data[\\'train\\'].to\\\\_pandas()[\\'text\\'].str.split().str.len().max()\\n```\\n```\\nThe maximum number of characters is 152.', 'type': 'tool', 'name': 'data_retriever', 'id': '69e32469-5e98-472e-8e62-77c12ac83b65', 'tool_call_id': 'call_dNy8yGaVlwgBhs8z5d1cbFhq', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bc8daf3d2ebb270a0b2c', 'page_content': '- [Question answering task guide](../tasks/question\\\\_answering)\\n\\\\*\\\\*Multiple choice\\\\*\\\\*\\n- [`RobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice.ipynb). - [`TFRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice-tf.ipynb). - [Multiple choice task guide](../tasks/multiple\\\\_choice)\\n## RobertaConfig\\n[[autodoc]] RobertaConfig\\n## RobertaTokenizer\\n[[autodoc]] RobertaTokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## RobertaTokenizerFast\\n[[autodoc]] RobertaTokenizerFast\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n\\n## RobertaModel\\n[[autodoc]] RobertaModel\\n- forward\\n## RobertaForCausalLM\\n[[autodoc]] RobertaForCausalLM\\n- forward\\n## RobertaForMaskedLM\\n[[autodoc]] RobertaForMaskedLM\\n- forward\\n## RobertaForSequenceClassification\\n[[autodoc]] RobertaForSequenceClassification\\n- forward\\n## RobertaForMultipleChoice\\n[[autodoc]] RobertaForMultipleChoice\\n- forward\\n## RobertaForTokenClassification\\n[[autodoc]] RobertaForTokenClassification\\n- forward\\n## RobertaForQuestionAnswering\\n[[autodoc]] RobertaForQuestionAnswering\\n- forward\\n\\n## TFRobertaModel\\n[[autodoc]] TFRobertaModel\\n- call\\n## TFRobertaForCausalLM\\n[[autodoc]] TFRobertaForCausalLM\\n- call\\n## TFRobertaForMaskedLM\\n[[autodoc]] TFRobertaForMaskedLM\\n- call\\n## TFRobertaForSequenceClassification\\n[[autodoc]] TFRobertaForSequenceClassification\\n- call\\n## TFRobertaForMultipleChoice\\n[[autodoc]] TFRobertaForMultipleChoice\\n- call\\n## TFRobertaForTokenClassification\\n[[autodoc]] TFRobertaForTokenClassification\\n- call\\n## TFRobertaForQuestionAnswering\\n[[autodoc]] TFRobertaForQuestionAnswering\\n- call\\n\\n## FlaxRobertaModel\\n[[autodoc]] FlaxRobertaModel\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForCausalLM\\n[[autodoc]] FlaxRobertaForCausalLM\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForMaskedLM\\n[[autodoc]] FlaxRobertaForMaskedLM\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForSequenceClassification\\n[[autodoc]] FlaxRobertaForSequenceClassification\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForMultipleChoice\\n[[autodoc]] FlaxRobertaForMultipleChoice\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForTokenClassification\\n[[autodoc]] FlaxRobertaForTokenClassification\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForQuestionAnswering\\n[[autodoc]] FlaxRobertaForQuestionAnswering\\n- \\\\_\\\\_call\\\\_\\\\_', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b39baf3d2ebb270a004e', 'page_content': \"- Uses RoBERTa tricks on the XLM approach, but does not use the translation language modeling objective. It only uses masked language modeling on sentences coming from one language. ## Resources\\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with XLM-RoBERTa. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. - A blog post on how to [finetune XLM RoBERTa for multiclass classification with Habana Gaudi on AWS](https://www.philschmid.de/habana-distributed-training)\\n- [`XLMRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\\\_classification.ipynb). - [`TFXLMRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\\\_classification-tf.ipynb). - [`FlaxXLMRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\\\_classification\\\\_flax.ipynb). - [Text classification](https://huggingface.co/docs/transformers/tasks/sequence\\\\_classification) chapter of the ü§ó Hugging Face Task Guides. - [Text classification task guide](../tasks/sequence\\\\_classification)\\n- [`XLMRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token\\\\_classification.ipynb). - [`TFXLMRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token\\\\_classification-tf.ipynb). - [`FlaxXLMRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification). - [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the ü§ó Hugging Face Course. - [Token classification task guide](../tasks/token\\\\_classification)\\n- [`XLMRobertaForCausalLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language\\\\_modeling.ipynb). - [Causal language modeling](https://huggingface.co/docs/transformers/tasks/language\\\\_modeling) chapter of the ü§ó Hugging Face Task Guides. - [Causal language modeling task guide](../tasks/language\\\\_modeling)\\n- [`XLMRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language\\\\_modeling.ipynb). - [`TFXLMRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run\\\\_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language\\\\_modeling-tf.ipynb). - [`FlaxXLMRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked\\\\_language\\\\_modeling\\\\_flax.ipynb). - [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the ü§ó Hugging Face Course. - [Masked language modeling](../tasks/masked\\\\_language\\\\_modeling)\\n- [`XLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question\\\\_answering.ipynb). - [`TFXLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question\\\\_answering-tf.ipynb). - [`FlaxXLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering). - [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the ü§ó Hugging Face Course. - [Question answering task guide](../tasks/question\\\\_answering)\\n\\\\*\\\\*Multiple choice\\\\*\\\\*\\n- [`XLMRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice.ipynb). - [`TFXLMRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice-tf.ipynb). - [Multiple choice task guide](../tasks/multiple\\\\_choice)\\nüöÄ Deploy\\n- A blog post on how to [Deploy Serverless XLM RoBERTa on AWS Lambda](https://www.philschmid.de/multilingual-serverless-xlm-roberta-with-huggingface).\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b5a3af3d2ebb270a02f5', 'page_content': 'As a result, the first word of the sentence is encoded differently if it is not preceded by a white space. To ensure the first word includes a space, we set `add\\\\_prefix\\\\_space=True`. Also, to maintain consistent pre-processing for all three models, we set the parameter to \\'True\\' for Llama 2 and Mistral 7b. - Define the preprocessing function for converting one row of the dataframe:\\n```python\\ndef roberta\\\\_preprocessing\\\\_function(examples):\\nreturn roberta\\\\_tokenizer(examples[\\'text\\'], truncation=True, max\\\\_length=MAX\\\\_LEN)\\n```\\nBy applying the preprocessing function to the first example of our training dataset, we have the tokenized inputs (`input\\\\_ids`) and the attention mask:\\n```python\\nroberta\\\\_preprocessing\\\\_function(data[\\'train\\'][0])\\n```\\n```\\n{\\'input\\\\_ids\\': [0, 127, 2373, 2490, 4, 1205, 640, 90, 4, 876, 73, 118, 725, 398, 13083, 329, 398, 119, 1343, 246, 2], \\'attention\\\\_mask\\': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\\n```\\n- Now, let\\'s apply the preprocessing function to the entire dataset:\\n```python\\ncol\\\\_to\\\\_delete = [\\'id\\', \\'keyword\\',\\'location\\', \\'text\\']\\n# Apply the preprocessing function and remove the undesired columns\\nroberta\\\\_tokenized\\\\_datasets = data.map(roberta\\\\_preprocessing\\\\_function, batched=True, remove\\\\_columns=col\\\\_to\\\\_delete)\\n# Rename the target to label as for HugginFace standards\\nroberta\\\\_tokenized\\\\_datasets = roberta\\\\_tokenized\\\\_datasets.rename\\\\_column(\"target\", \"label\")\\n# Set to torch format\\nroberta\\\\_tokenized\\\\_datasets.set\\\\_format(\"torch\")\\n```\\n\\\\*\\\\*Note:\\\\*\\\\* we deleted the undesired columns from our data: id, keyword, location and text. We have deleted the text because we have already converted it into the inputs ids and the attention mask:\\nWe can have a look into our tokenized training dataset:\\n```python\\nroberta\\\\_tokenized\\\\_datasets[\\'train\\'][0]\\n```\\n```\\n{\\'label\\': tensor(0),\\n\\'input\\\\_ids\\': tensor([ 0, 127, 2373, 2490, 4, 1205, 640, 90, 4, 876,\\n73, 118, 725, 398, 13083, 329, 398, 119, 1343, 246,\\n2]),\\n\\'attention\\\\_mask\\': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\\n```\\n- For generating the training batches, we also need to pad the rows of a given batch to the maximum length found in the batch. For that, we will use the `DataCollatorWithPadding` class:\\n```python\\n# Data collator for padding a batch of examples to the maximum length seen in the batch\\nfrom transformers import DataCollatorWithPadding\\nroberta\\\\_data\\\\_collator = DataCollatorWithPadding(tokenizer=roberta\\\\_tokenizer)\\n```\\nYou can follow the same steps for preparing the data for Mistral 7B and Llama 2 models:\\n\\\\*\\\\*Note\\\\*\\\\* that Llama 2 and Mistral 7B don\\'t have a default `pad\\\\_token\\\\_id`. So, we use the `eos\\\\_token\\\\_id` for padding as well. - Mistral 7B:\\n```python\\n# Load Mistral 7B Tokenizer\\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\\nmistral\\\\_tokenizer = AutoTokenizer.from\\\\_pretrained(mistral\\\\_checkpoint, add\\\\_prefix\\\\_space=True)\\nmistral\\\\_tokenizer.pad\\\\_token\\\\_id = mistral\\\\_tokenizer.eos\\\\_token\\\\_id\\nmistral\\\\_tokenizer.pad\\\\_token = mistral\\\\_tokenizer.eos\\\\_token\\ndef mistral\\\\_preprocessing\\\\_function(examples):\\nreturn mistral\\\\_tokenizer(examples[\\'text\\'], truncation=True, max\\\\_length=MAX\\\\_LEN)\\nmistral\\\\_tokenized\\\\_datasets = data.map(mistral\\\\_preprocessing\\\\_function, batched=True, remove\\\\_columns=col\\\\_to\\\\_delete)\\nmistral\\\\_tokenized\\\\_datasets = mistral\\\\_tokenized\\\\_datasets.rename\\\\_column(\"target\", \"label\")\\nmistral\\\\_tokenized\\\\_datasets.set\\\\_format(\"torch\")\\n# Data collator for padding a batch of examples to the maximum length seen in the batch\\nmistral\\\\_data\\\\_collator = DataCollatorWithPadding(tokenizer=mistral\\\\_tokenizer)\\n```\\n- Llama 2:\\n```python\\n# Load Llama 2 Tokenizer\\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\\nllama\\\\_tokenizer = AutoTokenizer.from\\\\_pretrained(llama\\\\_checkpoint, add\\\\_prefix\\\\_space=True)\\nllama\\\\_tokenizer.pad\\\\_token\\\\_id = llama\\\\_tokenizer.eos\\\\_token\\\\_id\\nllama\\\\_tokenizer.pad\\\\_token = llama\\\\_tokenizer.eos\\\\_token\\ndef llama\\\\_preprocessing\\\\_function(examples):\\nreturn llama\\\\_tokenizer(examples[\\'text\\'], truncation=True, max\\\\_length=MAX\\\\_LEN)\\nllama\\\\_tokenized\\\\_datasets = data.map(llama\\\\_preprocessing\\\\_function, batched=True, remove\\\\_columns=col\\\\_to\\\\_delete)\\nllama\\\\_tokenized\\\\_datasets = llama\\\\_tokenized\\\\_datasets.rename\\\\_column(\"target\", \"label\")\\nllama\\\\_tokenized\\\\_datasets.set\\\\_format(\"torch\")\\n# Data collator for padding a batch of examples to the maximum length seen in the batch\\nllama\\\\_data\\\\_collator = DataCollatorWithPadding(tokenizer=llama\\\\_tokenizer)\\n```\\nNow that we have prepared the tokenized datasets, the next section will showcase how to load the pre-trained LLMs checkpoints and how to set the LoRa weights. ## Models\\n### RoBERTa\\n#### Load RoBERTa Checkpoints for the Classification Task\\nWe load the pre-trained RoBERTa model with a sequence classification head using the Hugging Face `AutoModelForSequenceClassification` class:\\n```python\\nfrom transformers import AutoModelForSequenceClassification\\nroberta\\\\_model = AutoModelForSequenceClassification.from\\\\_pretrained(roberta\\\\_checkpoint, num\\\\_labels=2)\\n```\\n#### LoRA setup for RoBERTa classifier\\nWe import LoRa configuration and set some parameters for RoBERTa classifier:\\n- TaskType: Sequence classification\\n- r(rank): Rank for our decomposition matrices\\n- lora\\\\_alpha: Alpha parameter to scale the learned weights.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b5a2af3d2ebb270a02f3', 'page_content': 'Its primary objective is to reduce the model\\'s trainable parameters. LoRA\\'s operation involves\\nlearning a low rank update matrix while keeping the pre-trained weights frozen. ![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral/lora.png)\\n## Setup\\nRoBERTa has a limitatiom of maximum sequence length of 512, so we set the `MAX\\\\_LEN=512` for all models to ensure a fair comparison. ```python\\nMAX\\\\_LEN = 512\\nroberta\\\\_checkpoint = \"roberta-large\"\\nmistral\\\\_checkpoint = \"mistralai/Mistral-7B-v0.1\"\\nllama\\\\_checkpoint = \"meta-llama/Llama-2-7b-hf\"\\n```\\n## Data preparation\\n### Data loading\\nWe will load the dataset from Hugging Face:\\n```python\\nfrom datasets import load\\\\_dataset\\ndataset = load\\\\_dataset(\"mehdiiraqui/twitter\\\\_disaster\")\\n```\\nNow, let\\'s split the dataset into training and validation datasets. Then add the test set:\\n```python\\nfrom datasets import Dataset\\n# Split the dataset into training and validation datasets\\ndata = dataset[\\'train\\'].train\\\\_test\\\\_split(train\\\\_size=0.8, seed=42)\\n# Rename the default \"test\" split to \"validation\"\\ndata[\\'val\\'] = data.pop(\"test\")\\n# Convert the test dataframe to HuggingFace dataset and add it into the first dataset\\ndata[\\'test\\'] = dataset[\\'test\\']\\n```\\nHere\\'s an overview of the dataset:\\n```bash\\nDatasetDict({\\ntrain: Dataset({\\nfeatures: [\\'id\\', \\'keyword\\', \\'location\\', \\'text\\', \\'target\\'],\\nnum\\\\_rows: 6090\\n})\\nval: Dataset({\\nfeatures: [\\'id\\', \\'keyword\\', \\'location\\', \\'text\\', \\'target\\'],\\nnum\\\\_rows: 1523\\n})\\ntest: Dataset({\\nfeatures: [\\'id\\', \\'keyword\\', \\'location\\', \\'text\\', \\'target\\'],\\nnum\\\\_rows: 3263\\n})\\n})\\n```\\nLet\\'s check the data distribution:\\n```python\\nimport pandas as pd\\ndata[\\'train\\'].to\\\\_pandas().info()\\ndata[\\'test\\'].to\\\\_pandas().info()\\n```\\n- Train dataset\\n```\\nRangeIndex: 7613 entries, 0 to 7612\\nData columns (total 5 columns):\\n# Column Non-Null Count Dtype\\n--- ------ -------------- -----\\n0 id 7613 non-null int64\\n1 keyword 7552 non-null object\\n2 location 5080 non-null object\\n3 text 7613 non-null object\\n4 target 7613 non-null int64\\ndtypes: int64(2), object(3)\\nmemory usage: 297.5+ KB\\n```\\n- Test dataset\\n```\\nRangeIndex: 3263 entries, 0 to 3262\\nData columns (total 5 columns):\\n# Column Non-Null Count Dtype\\n--- ------ -------------- -----\\n0 id 3263 non-null int64\\n1 keyword 3237 non-null object\\n2 location 2158 non-null object\\n3 text 3263 non-null object\\n4 target 3263 non-null int64\\ndtypes: int64(2), object(3)\\nmemory usage: 127.6+ KB\\n```\\n\\\\*\\\\*Target distribution in the train dataset\\\\*\\\\*\\n```\\ntarget\\n0 4342\\n1 3271\\nName: count, dtype: int64\\n```\\nAs the classes are not balanced, we will compute the positive and negative weights and use them for loss calculation later:\\n```python\\npos\\\\_weights = len(data[\\'train\\'].to\\\\_pandas()) / (2 \\\\* data[\\'train\\'].to\\\\_pandas().target.value\\\\_counts()[1])\\nneg\\\\_weights = len(data[\\'train\\'].to\\\\_pandas()) / (2 \\\\* data[\\'train\\'].to\\\\_pandas().target.value\\\\_counts()[0])\\n```\\nThe final weights are:\\n```\\nPOS\\\\_WEIGHT, NEG\\\\_WEIGHT = (1.1637114032405993, 0.8766697374481806)\\n```\\nThen, we compute the maximum length of the column text:\\n```python\\n# Number of Characters\\nmax\\\\_char = data[\\'train\\'].to\\\\_pandas()[\\'text\\'].str.len().max()\\n# Number of Words\\nmax\\\\_words = data[\\'train\\'].to\\\\_pandas()[\\'text\\'].str.split().str.len().max()\\n```\\n```\\nThe maximum number of characters is 152.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The `roberta-large-mnli` checkpoint is designed for the task of natural language inference (NLI), specifically for the Multi-Genre Natural Language Inference (MNLI) dataset. This task involves determining the relationship between a premise and a hypothesis, classifying them into categories such as entailment, contradiction, or neutral.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--9b70b0f8-e75e-41c6-b15a-60b729b75a05-0', 'usage_metadata': {'input_tokens': 5268, 'output_tokens': 67, 'total_tokens': 5335, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:26:46,132 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 22%|‚ñà‚ñà‚ñè       | 14/65 [00:59<04:03,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What service is replacing the Paid tier of the Inference API at Hugging Face?\\n', 'type': 'human', 'id': 'f55a639f-1cb2-4384-a550-1c478e87204b'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_xLS7KZzIpZ3isoVFMIdTYTEJ', 'function': {'arguments': '{\"query\":\"Paid tier of the Inference API replacement service Hugging Face\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--db70e445-18fa-4896-bd53-b7bdc431a661-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Paid tier of the Inference API replacement service Hugging Face'}, 'id': 'call_xLS7KZzIpZ3isoVFMIdTYTEJ', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 27, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Access ü§ó Inference Endpoints\\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account. You can check your [billing](https://huggingface.co/settings/billing) if you\\'re unsure whether you have an active payment method. There are two pricing plans:\\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. - There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you\\'re interested! After you\\'ve added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying!\\n\\n## Can I send large volumes of requests? Can I get accelerated APIs? If you are interested in accelerated inference, higher volumes of requests, or an SLA, please contact us at `api-enterprise at huggingface.co`. ## How can I see my usage? You can head to the [Inference API dashboard](https://api-inference.huggingface.co/dashboard/). Learn more about it in the [Inference API documentation](https://huggingface.co/docs/api-inference/usage). ## Is there programmatic access to the Inference API? Yes, the `huggingface\\\\_hub` library has a client wrapper documented [here](https://huggingface.co/docs/huggingface\\\\_hub/how-to-inference).\\n\\n!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Inference\\nInference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive,\\nrunning on a dedicated server can be an interesting option. The `huggingface\\\\_hub` library provides an easy way to call a\\nservice that runs inference for hosted models. There are several services you can connect to:\\n- [Inference API](https://huggingface.co/docs/api-inference/index): a service that allows you to run accelerated inference\\non Hugging Face\\'s infrastructure for free. This service is a fast way to get started, test different models, and\\nprototype AI products. - [Inference Endpoints](https://huggingface.co/inference-endpoints): a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice.\\n\\nThe model is loaded on-demand on our servers and unloaded when it\\'s not needed anymore. You don\\'t have to write any code and the feature is free. What\\'s not to love? ## Free Inference API\\nThe [Inference API](https://huggingface.co/docs/api-inference/) is what powers the Inference widget under the hood. With a simple HTTP request, you can load any hub model and predict your data with it in seconds. The model URL and a valid hub token are all you need. Here\\'s how I can load and predict with the `xlm-roberta-base` [model](https://huggingface.co/xlm-roberta-base) in a single line:\\n```\\ncurl https://api-inference.huggingface.co/models/xlm-roberta-base \\\\\\n-X POST \\\\\\n-d \\'{\"inputs\": \"The answer to the universe is .\"}\\' \\\\\\n-H \"Authorization: Bearer HF\\\\_TOKEN\"\\n```\\nThe Inference API is the simplest way to build a prediction service that you can immediately call from your application during development and tests. No need for a bespoke API, or a model server. In addition, you can instantly switch from one model to the next and compare their performance in your application. And guess what? The Inference API is free to use. As rate limiting is enforced, we don\\'t recommend using the Inference API for production. Instead, you should consider Inference Endpoints. ## Production with Inference Endpoints\\nOnce you\\'re happy with the performance of your ML model, it\\'s time to deploy it for production. Unfortunately, when leaving the sandbox, everything becomes a concern: security, scaling, monitoring, etc. This is where a lot of ML stumble and sometimes fall. We built [Inference Endpoints](https://huggingface.co/inference-endpoints) to solve this problem. In just a few clicks, Inference Endpoints let you deploy any hub model on secure and scalable infrastructure, hosted in your AWS or Azure region of choice. Additional settings include CPU and GPU hosting, built-in auto-scaling, and more. This makes finding the appropriate cost/performance ratio easy, with [pricing](https://huggingface.co/pricing#endpoints) starting as low as $0.06 per hour. Inference Endpoints support three security levels:\\n\\\\* Public: the endpoint runs in a public Hugging Face subnet, and anyone on the Internet can access it without any authentication. \\\\* Protected: the endpoint runs in a public Hugging Face subnet, and anyone on the Internet with the appropriate Hugging Face token can access it. \\\\* Private: the endpoint runs in a private Hugging Face subnet and is not accessible on the Internet. It\\'s only available through a private connection in your AWS or Azure account.', 'type': 'tool', 'name': 'data_retriever', 'id': 'e3eacb4b-6b21-4d5a-8c53-9f54a8d47b12', 'tool_call_id': 'call_xLS7KZzIpZ3isoVFMIdTYTEJ', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aab343e684812d15f5f3c', 'page_content': \"Access ü§ó Inference Endpoints\\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account. You can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method. There are two pricing plans:\\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. - There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested! After you've added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying!\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aacd13e684812d15f6144', 'page_content': '## Can I send large volumes of requests? Can I get accelerated APIs? If you are interested in accelerated inference, higher volumes of requests, or an SLA, please contact us at `api-enterprise at huggingface.co`. ## How can I see my usage? You can head to the [Inference API dashboard](https://api-inference.huggingface.co/dashboard/). Learn more about it in the [Inference API documentation](https://huggingface.co/docs/api-inference/usage). ## Is there programmatic access to the Inference API? Yes, the `huggingface\\\\_hub` library has a client wrapper documented [here](https://huggingface.co/docs/huggingface\\\\_hub/how-to-inference).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aba2f3e684812d15f70a1', 'page_content': \"!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Inference\\nInference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive,\\nrunning on a dedicated server can be an interesting option. The `huggingface\\\\_hub` library provides an easy way to call a\\nservice that runs inference for hosted models. There are several services you can connect to:\\n- [Inference API](https://huggingface.co/docs/api-inference/index): a service that allows you to run accelerated inference\\non Hugging Face's infrastructure for free. This service is a fast way to get started, test different models, and\\nprototype AI products. - [Inference Endpoints](https://huggingface.co/inference-endpoints): a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaf083e684812d15f63c9', 'page_content': 'The model is loaded on-demand on our servers and unloaded when it\\'s not needed anymore. You don\\'t have to write any code and the feature is free. What\\'s not to love? ## Free Inference API\\nThe [Inference API](https://huggingface.co/docs/api-inference/) is what powers the Inference widget under the hood. With a simple HTTP request, you can load any hub model and predict your data with it in seconds. The model URL and a valid hub token are all you need. Here\\'s how I can load and predict with the `xlm-roberta-base` [model](https://huggingface.co/xlm-roberta-base) in a single line:\\n```\\ncurl https://api-inference.huggingface.co/models/xlm-roberta-base \\\\\\n-X POST \\\\\\n-d \\'{\"inputs\": \"The answer to the universe is .\"}\\' \\\\\\n-H \"Authorization: Bearer HF\\\\_TOKEN\"\\n```\\nThe Inference API is the simplest way to build a prediction service that you can immediately call from your application during development and tests. No need for a bespoke API, or a model server. In addition, you can instantly switch from one model to the next and compare their performance in your application. And guess what? The Inference API is free to use. As rate limiting is enforced, we don\\'t recommend using the Inference API for production. Instead, you should consider Inference Endpoints. ## Production with Inference Endpoints\\nOnce you\\'re happy with the performance of your ML model, it\\'s time to deploy it for production. Unfortunately, when leaving the sandbox, everything becomes a concern: security, scaling, monitoring, etc. This is where a lot of ML stumble and sometimes fall. We built [Inference Endpoints](https://huggingface.co/inference-endpoints) to solve this problem. In just a few clicks, Inference Endpoints let you deploy any hub model on secure and scalable infrastructure, hosted in your AWS or Azure region of choice. Additional settings include CPU and GPU hosting, built-in auto-scaling, and more. This makes finding the appropriate cost/performance ratio easy, with [pricing](https://huggingface.co/pricing#endpoints) starting as low as $0.06 per hour. Inference Endpoints support three security levels:\\n\\\\* Public: the endpoint runs in a public Hugging Face subnet, and anyone on the Internet can access it without any authentication. \\\\* Protected: the endpoint runs in a public Hugging Face subnet, and anyone on the Internet with the appropriate Hugging Face token can access it. \\\\* Private: the endpoint runs in a private Hugging Face subnet and is not accessible on the Internet. It\\'s only available through a private connection in your AWS or Azure account.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The service replacing the Paid tier of the Inference API at Hugging Face is the Inference Endpoints. Inference Endpoints provide a way to deploy models to production on secure and scalable infrastructure, offering additional features such as CPU and GPU hosting, built-in auto-scaling, and various security levels.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--c87301c9-f381-4474-9889-b28caa84ed6e-0', 'usage_metadata': {'input_tokens': 1455, 'output_tokens': 61, 'total_tokens': 1516, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:26:51,861 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 23%|‚ñà‚ñà‚ñé       | 15/65 [01:05<04:13,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\\n', 'type': 'human', 'id': 'a6834deb-d624-4611-a1fa-9d4532af3559'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_ZtPfaDrVWUvUKfZ5lSS6c6xc', 'function': {'arguments': '{\"query\":\"SqueezeBERT architectural feature instead of fully-connected layers for Q, K, V, and FFN layers\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--5cc84c79-5dcd-4c38-b141-e25cfd396184-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'SqueezeBERT architectural feature instead of fully-connected layers for Q, K, V, and FFN layers'}, 'id': 'call_ZtPfaDrVWUvUKfZ5lSS6c6xc', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 375, 'output_tokens': 37, 'total_tokens': 412, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# SqueezeBERT\\n## Overview\\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It\\'s a\\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\\ninstead of fully-connected layers for the Q, K, V and FFN layers.\\n\\nThe abstract from the paper is the following:\\n\\\\*Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets,\\nlarge computing systems, and better neural network models, natural language processing (NLP) technology has made\\nsignificant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant\\nopportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. In particular, we\\nconsider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today\\'s\\nhighly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with\\nBERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. In this work, we observe that methods\\nsuch as grouped convolutions have yielded significant speedups for computer vision networks, but many of these\\ntechniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\\nset. The SqueezeBERT code will be released.\\\\*\\nThis model was contributed by [forresti](https://huggingface.co/forresti). ## Usage tips\\n- SqueezeBERT is a model with absolute position embeddings so it\\'s usually advised to pad the inputs on the right\\nrather than the left. - SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\\nefficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\\nwith a causal language modeling (CLM) objective are better in that regard. - For best results when finetuning on sequence classification tasks, it is recommended to start with the\\n\\\\*squeezebert/squeezebert-mnli-headless\\\\* checkpoint. ## Resources\\n- [Text classification task guide](../tasks/sequence\\\\_classification)\\n- [Token classification task guide](../tasks/token\\\\_classification)\\n- [Question answering task guide](../tasks/question\\\\_answering)\\n- [Masked language modeling task guide](../tasks/masked\\\\_language\\\\_modeling)\\n- [Multiple choice task guide](../tasks/multiple\\\\_choice)\\n## SqueezeBertConfig\\n[[autodoc]] SqueezeBertConfig\\n## SqueezeBertTokenizer\\n[[autodoc]] SqueezeBertTokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## SqueezeBertTokenizerFast\\n[[autodoc]] SqueezeBertTokenizerFast\\n## SqueezeBertModel\\n[[autodoc]] SqueezeBertModel\\n## SqueezeBertForMaskedLM\\n[[autodoc]] SqueezeBertForMaskedLM\\n## SqueezeBertForSequenceClassification\\n[[autodoc]] SqueezeBertForSequenceClassification\\n## SqueezeBertForMultipleChoice\\n[[autodoc]] SqueezeBertForMultipleChoice\\n## SqueezeBertForTokenClassification\\n[[autodoc]] SqueezeBertForTokenClassification\\n## SqueezeBertForQuestionAnswering\\n[[autodoc]] SqueezeBertForQuestionAnswering\\n\\nIt presents two parameter-reduction techniques to lower memory consumption and increase the training\\nspeed of BERT:\\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\\n\\\\*Increasing model size when pretraining natural language representations often results in improved performance on\\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\\\*\\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\\n- ALBERT is a model with absolute position embeddings so it\\'s usually advised to pad the inputs on the right rather\\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it\\'s more logical to have H >> E. Also, the embedding matrix is large since it\\'s V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A.\\n\\n!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# ConvBERT\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-convbert-blueviolet)](https://huggingface.co/models?filter=convbert)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/conv-bert-base)\\n\\n## Overview\\nThe ConvBERT model was proposed in [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng\\nYan.', 'type': 'tool', 'name': 'data_retriever', 'id': 'f640b8d6-8e32-4bb7-9b19-5d54e6465fde', 'tool_call_id': 'call_ZtPfaDrVWUvUKfZ5lSS6c6xc', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaa5a3e684812d15f5e3f', 'page_content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# SqueezeBERT\\n## Overview\\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It\\'s a\\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\\ninstead of fully-connected layers for the Q, K, V and FFN layers.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaa5a3e684812d15f5e41', 'page_content': \"The abstract from the paper is the following:\\n\\\\*Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets,\\nlarge computing systems, and better neural network models, natural language processing (NLP) technology has made\\nsignificant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant\\nopportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. In particular, we\\nconsider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today's\\nhighly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with\\nBERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. In this work, we observe that methods\\nsuch as grouped convolutions have yielded significant speedups for computer vision networks, but many of these\\ntechniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\\nset. The SqueezeBERT code will be released.\\\\*\\nThis model was contributed by [forresti](https://huggingface.co/forresti). ## Usage tips\\n- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\\nrather than the left. - SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\\nefficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\\nwith a causal language modeling (CLM) objective are better in that regard. - For best results when finetuning on sequence classification tasks, it is recommended to start with the\\n\\\\*squeezebert/squeezebert-mnli-headless\\\\* checkpoint. ## Resources\\n- [Text classification task guide](../tasks/sequence\\\\_classification)\\n- [Token classification task guide](../tasks/token\\\\_classification)\\n- [Question answering task guide](../tasks/question\\\\_answering)\\n- [Masked language modeling task guide](../tasks/masked\\\\_language\\\\_modeling)\\n- [Multiple choice task guide](../tasks/multiple\\\\_choice)\\n## SqueezeBertConfig\\n[[autodoc]] SqueezeBertConfig\\n## SqueezeBertTokenizer\\n[[autodoc]] SqueezeBertTokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## SqueezeBertTokenizerFast\\n[[autodoc]] SqueezeBertTokenizerFast\\n## SqueezeBertModel\\n[[autodoc]] SqueezeBertModel\\n## SqueezeBertForMaskedLM\\n[[autodoc]] SqueezeBertForMaskedLM\\n## SqueezeBertForSequenceClassification\\n[[autodoc]] SqueezeBertForSequenceClassification\\n## SqueezeBertForMultipleChoice\\n[[autodoc]] SqueezeBertForMultipleChoice\\n## SqueezeBertForTokenClassification\\n[[autodoc]] SqueezeBertForTokenClassification\\n## SqueezeBertForQuestionAnswering\\n[[autodoc]] SqueezeBertForQuestionAnswering\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3d33e684812d15f692e', 'page_content': \"It presents two parameter-reduction techniques to lower memory consumption and increase the training\\nspeed of BERT:\\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\\n\\\\*Increasing model size when pretraining natural language representations often results in improved performance on\\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\\\*\\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa7fd3e684812d15f5b79', 'page_content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# ConvBERT\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-convbert-blueviolet)](https://huggingface.co/models?filter=convbert)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/conv-bert-base)\\n\\n## Overview\\nThe ConvBERT model was proposed in [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng\\nYan.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--0297230a-da52-48f7-ba52-a2c858cd0e52-0', 'usage_metadata': {'input_tokens': 2181, 'output_tokens': 26, 'total_tokens': 2207, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:26:54,879 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 25%|‚ñà‚ñà‚ñç       | 16/65 [01:08<03:38,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': \"What type of license is the HuggingFace Team's software distributed under?\\n\", 'type': 'human', 'id': '6345f636-19d3-416a-97b1-4a144be4ef1f'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_jB1HEhLZIDBN0iTiCXHH22No', 'function': {'arguments': '{\"query\":\"HuggingFace software license\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--802e95a4-fc74-4415-bbef-37a5c75a43a9-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'HuggingFace software license'}, 'id': 'call_jB1HEhLZIDBN0iTiCXHH22No', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 363, 'output_tokens': 21, 'total_tokens': 384, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Under construction\\n\\n!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Under construction\\n\\nFor more details, reach out to HF Support to address any questions or concerns at website@huggingface.co\\n\\np align=\"center\">\\n\\n![huggingface_hub library logo](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingface_hub.svg)\\n\\n*The official Python client for the Huggingface Hub.*\\n\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc)](https://huggingface.co/docs/huggingface_hub/en/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub/releases)\\n[![PyPi version](https://img.shields.io/pypi/pyversions/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub)\\n[![downloads](https://static.pepy.tech/badge/huggingface_hub/month)](https://pypi.org/project/huggingface-hub)\\n[![Code coverage](https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL)](https://codecov.io/gh/huggingface/huggingface_hub)\\n\\n#### **English** | [Deutsch](https://github.com/huggingface/huggingface_hub/blob/main/README_de.md) | [‡§π‡§ø‡§Ç‡§¶‡•Ä](https://github.com/huggingface/huggingface_hub/blob/main/README_hi.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/huggingface_hub/blob/main/README_ko.md) | [‰∏≠ÊñáÔºàÁÆÄ‰ΩìÔºâ](https://github.com/huggingface/huggingface_hub/blob/main/README_cn.md)\\n\\n---\\n\\\\*\\\\*Documentation\\\\*\\\\*: <https://hf.co/docs/huggingface_hub>\\n\\\\*\\\\*Source Code\\\\*\\\\*: <https://github.com/huggingface/huggingface_hub>\\n---\\n## Welcome to the huggingface\\\\_hub library\\nThe `huggingface\\\\_hub` library allows you to interact with the [Hugging Face Hub](https://huggingface.co/), a platform democratizing open-source Machine Learning for creators and collaborators.', 'type': 'tool', 'name': 'data_retriever', 'id': 'a4b4788d-6fef-4468-b3a8-a69f0d7cae72', 'tool_call_id': 'call_jB1HEhLZIDBN0iTiCXHH22No', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b9e6af3d2ebb270a082f', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Under construction', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bca0af3d2ebb270a0b3d', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Under construction', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c1ad666af48523f52064', 'page_content': 'For more details, reach out to HF Support to address any questions or concerns at website@huggingface.co', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bd7baf3d2ebb270a0c42', 'page_content': 'p align=\"center\">\\n\\n![huggingface_hub library logo](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingface_hub.svg)\\n\\n*The official Python client for the Huggingface Hub.*\\n\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc)](https://huggingface.co/docs/huggingface_hub/en/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub/releases)\\n[![PyPi version](https://img.shields.io/pypi/pyversions/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub)\\n[![downloads](https://static.pepy.tech/badge/huggingface_hub/month)](https://pypi.org/project/huggingface-hub)\\n[![Code coverage](https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL)](https://codecov.io/gh/huggingface/huggingface_hub)\\n\\n#### **English** | [Deutsch](https://github.com/huggingface/huggingface_hub/blob/main/README_de.md) | [‡§π‡§ø‡§Ç‡§¶‡•Ä](https://github.com/huggingface/huggingface_hub/blob/main/README_hi.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/huggingface_hub/blob/main/README_ko.md) | [‰∏≠ÊñáÔºàÁÆÄ‰ΩìÔºâ](https://github.com/huggingface/huggingface_hub/blob/main/README_cn.md)\\n\\n---\\n\\\\*\\\\*Documentation\\\\*\\\\*: <https://hf.co/docs/huggingface_hub>\\n\\\\*\\\\*Source Code\\\\*\\\\*: <https://github.com/huggingface/huggingface_hub>\\n---\\n## Welcome to the huggingface\\\\_hub library\\nThe `huggingface\\\\_hub` library allows you to interact with the [Hugging Face Hub](https://huggingface.co/), a platform democratizing open-source Machine Learning for creators and collaborators.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"The HuggingFace Team's software is distributed under the Apache License, Version 2.0.\", 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--ef202f74-0905-4912-aa72-73fbd89f9c34-0', 'usage_metadata': {'input_tokens': 1062, 'output_tokens': 20, 'total_tokens': 1082, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:26:58,411 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 26%|‚ñà‚ñà‚ñå       | 17/65 [01:12<03:20,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\\n', 'type': 'human', 'id': 'eb5117ca-d3d9-48c9-a9fb-0c2fc4ee67c0'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_xQh2AgwfByjPU2KLEtIQmmDf', 'function': {'arguments': '{\"query\":\"ALBERT model parameter-reduction techniques\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--b6471f2d-8820-4068-b819-62cb7f4f981f-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'ALBERT model parameter-reduction techniques'}, 'id': 'call_xQh2AgwfByjPU2KLEtIQmmDf', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 372, 'output_tokens': 23, 'total_tokens': 395, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'It presents two parameter-reduction techniques to lower memory consumption and increase the training\\nspeed of BERT:\\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\\n\\\\*Increasing model size when pretraining natural language representations often results in improved performance on\\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\\\*\\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\\n- ALBERT is a model with absolute position embeddings so it\\'s usually advised to pad the inputs on the right rather\\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it\\'s more logical to have H >> E. Also, the embedding matrix is large since it\\'s V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A.\\n\\n!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# ALBERT\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-albert-blueviolet)](https://huggingface.co/models?filter=albert)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/albert-base-v2)\\n\\n## Overview\\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\\nRadu Soricut.\\n\\nHowever, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model\\\\_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model\\\\_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model\\\\_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix. ### Decoder[[nlp-decoder]]\\n[GPT-2](model\\\\_doc/gpt2) is a decoder-only Transformer that predicts the next word in the sequence.\\n\\n- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the ü§ó Hugging Face Course. - Check the [Question answering task guide](../tasks/question\\\\_answering) on how to use the model. \\\\*\\\\*Multiple choice\\\\*\\\\*\\n- [`AlbertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice.ipynb). - [`TFAlbertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice-tf.ipynb). - Check the [Multiple choice task guide](../tasks/multiple\\\\_choice) on how to use the model. ## AlbertConfig\\n[[autodoc]] AlbertConfig\\n## AlbertTokenizer\\n[[autodoc]] AlbertTokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## AlbertTokenizerFast\\n[[autodoc]] AlbertTokenizerFast\\n## Albert specific outputs\\n[[autodoc]] models.albert.modeling\\\\_albert.AlbertForPreTrainingOutput\\n[[autodoc]] models.albert.modeling\\\\_tf\\\\_albert.TFAlbertForPreTrainingOutput\\n\\n## AlbertModel\\n[[autodoc]] AlbertModel\\n- forward\\n## AlbertForPreTraining\\n[[autodoc]] AlbertForPreTraining\\n- forward\\n## AlbertForMaskedLM\\n[[autodoc]] AlbertForMaskedLM\\n- forward\\n## AlbertForSequenceClassification\\n[[autodoc]] AlbertForSequenceClassification\\n- forward\\n## AlbertForMultipleChoice\\n[[autodoc]] AlbertForMultipleChoice\\n## AlbertForTokenClassification\\n[[autodoc]] AlbertForTokenClassification\\n- forward\\n## AlbertForQuestionAnswering\\n[[autodoc]] AlbertForQuestionAnswering\\n- forward\\n\\n## TFAlbertModel\\n[[autodoc]] TFAlbertModel\\n- call\\n## TFAlbertForPreTraining\\n[[autodoc]] TFAlbertForPreTraining\\n- call\\n## TFAlbertForMaskedLM\\n[[autodoc]] TFAlbertForMaskedLM\\n- call\\n## TFAlbertForSequenceClassification\\n[[autodoc]] TFAlbertForSequenceClassification\\n- call\\n## TFAlbertForMultipleChoice\\n[[autodoc]] TFAlbertForMultipleChoice\\n- call\\n## TFAlbertForTokenClassification\\n[[autodoc]] TFAlbertForTokenClassification\\n- call\\n## TFAlbertForQuestionAnswering\\n[[autodoc]] TFAlbertForQuestionAnswering\\n- call\\n\\n## FlaxAlbertModel\\n[[autodoc]] FlaxAlbertModel\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxAlbertForPreTraining\\n[[autodoc]] FlaxAlbertForPreTraining\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxAlbertForMaskedLM\\n[[autodoc]] FlaxAlbertForMaskedLM\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxAlbertForSequenceClassification\\n[[autodoc]] FlaxAlbertForSequenceClassification\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxAlbertForMultipleChoice\\n[[autodoc]] FlaxAlbertForMultipleChoice\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxAlbertForTokenClassification\\n[[autodoc]] FlaxAlbertForTokenClassification\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxAlbertForQuestionAnswering\\n[[autodoc]] FlaxAlbertForQuestionAnswering\\n- \\\\_\\\\_call\\\\_\\\\_', 'type': 'tool', 'name': 'data_retriever', 'id': '7e10d75e-2ee1-461d-a905-5d9ef0e7a9ca', 'tool_call_id': 'call_xQh2AgwfByjPU2KLEtIQmmDf', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3d33e684812d15f692e', 'page_content': \"It presents two parameter-reduction techniques to lower memory consumption and increase the training\\nspeed of BERT:\\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\\n\\\\*Increasing model size when pretraining natural language representations often results in improved performance on\\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\\\*\\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3d33e684812d15f692d', 'page_content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# ALBERT\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-albert-blueviolet)](https://huggingface.co/models?filter=albert)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/albert-base-v2)\\n\\n## Overview\\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\\nRadu Soricut.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b99baf3d2ebb270a07d0', 'page_content': 'However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model\\\\_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model\\\\_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model\\\\_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix. ### Decoder[[nlp-decoder]]\\n[GPT-2](model\\\\_doc/gpt2) is a decoder-only Transformer that predicts the next word in the sequence.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3d33e684812d15f6931', 'page_content': '- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the ü§ó Hugging Face Course. - Check the [Question answering task guide](../tasks/question\\\\_answering) on how to use the model. \\\\*\\\\*Multiple choice\\\\*\\\\*\\n- [`AlbertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice.ipynb). - [`TFAlbertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice-tf.ipynb). - Check the [Multiple choice task guide](../tasks/multiple\\\\_choice) on how to use the model. ## AlbertConfig\\n[[autodoc]] AlbertConfig\\n## AlbertTokenizer\\n[[autodoc]] AlbertTokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## AlbertTokenizerFast\\n[[autodoc]] AlbertTokenizerFast\\n## Albert specific outputs\\n[[autodoc]] models.albert.modeling\\\\_albert.AlbertForPreTrainingOutput\\n[[autodoc]] models.albert.modeling\\\\_tf\\\\_albert.TFAlbertForPreTrainingOutput\\n\\n## AlbertModel\\n[[autodoc]] AlbertModel\\n- forward\\n## AlbertForPreTraining\\n[[autodoc]] AlbertForPreTraining\\n- forward\\n## AlbertForMaskedLM\\n[[autodoc]] AlbertForMaskedLM\\n- forward\\n## AlbertForSequenceClassification\\n[[autodoc]] AlbertForSequenceClassification\\n- forward\\n## AlbertForMultipleChoice\\n[[autodoc]] AlbertForMultipleChoice\\n## AlbertForTokenClassification\\n[[autodoc]] AlbertForTokenClassification\\n- forward\\n## AlbertForQuestionAnswering\\n[[autodoc]] AlbertForQuestionAnswering\\n- forward\\n\\n## TFAlbertModel\\n[[autodoc]] TFAlbertModel\\n- call\\n## TFAlbertForPreTraining\\n[[autodoc]] TFAlbertForPreTraining\\n- call\\n## TFAlbertForMaskedLM\\n[[autodoc]] TFAlbertForMaskedLM\\n- call\\n## TFAlbertForSequenceClassification\\n[[autodoc]] TFAlbertForSequenceClassification\\n- call\\n## TFAlbertForMultipleChoice\\n[[autodoc]] TFAlbertForMultipleChoice\\n- call\\n## TFAlbertForTokenClassification\\n[[autodoc]] TFAlbertForTokenClassification\\n- call\\n## TFAlbertForQuestionAnswering\\n[[autodoc]] TFAlbertForQuestionAnswering\\n- call\\n\\n## FlaxAlbertModel\\n[[autodoc]] FlaxAlbertModel\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxAlbertForPreTraining\\n[[autodoc]] FlaxAlbertForPreTraining\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxAlbertForMaskedLM\\n[[autodoc]] FlaxAlbertForMaskedLM\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxAlbertForSequenceClassification\\n[[autodoc]] FlaxAlbertForSequenceClassification\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxAlbertForMultipleChoice\\n[[autodoc]] FlaxAlbertForMultipleChoice\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxAlbertForTokenClassification\\n[[autodoc]] FlaxAlbertForTokenClassification\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxAlbertForQuestionAnswering\\n[[autodoc]] FlaxAlbertForQuestionAnswering\\n- \\\\_\\\\_call\\\\_\\\\_', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:\\n\\n1. Splitting the embedding matrix into two smaller matrices.\\n2. Using repeating layers split among groups.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--702d1f59-2df9-44e4-843c-17f14cccb84f-0', 'usage_metadata': {'input_tokens': 2182, 'output_tokens': 44, 'total_tokens': 2226, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:27:02,364 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 28%|‚ñà‚ñà‚ñä       | 18/65 [01:15<03:13,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What are the three main steps for fine-tuning a model with the ü§ó Datasets library?\\n', 'type': 'human', 'id': 'd592584e-edbe-471d-b506-9ce0ef789633'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_XbTPRjDgX6HPaSWHXGTUbKNi', 'function': {'arguments': '{\"query\":\"three main steps for fine-tuning a model with the ü§ó Datasets library\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--56ad210f-332b-40d2-b4e4-e9b606176ba2-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'three main steps for fine-tuning a model with the ü§ó Datasets library'}, 'id': 'call_XbTPRjDgX6HPaSWHXGTUbKNi', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 368, 'output_tokens': 31, 'total_tokens': 399, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Introduction[[introduction]]\\nIn [Chapter 3](/course/chapter3) you got your first taste of the ü§ó Datasets library and saw that there were three main steps when it came to fine-tuning a model:\\n1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics. But this is just scratching the surface of what ü§ó Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we\\'ll find answers to the following questions:\\n\\\\* What do you do when your dataset is not on the Hub? \\\\* How can you slice and dice a dataset? (And what if you \\\\_really\\\\_ need to use Pandas?)\\n\\\\* What do you do when your dataset is huge and will melt your laptop\\'s RAM? \\\\* What the heck are \"memory mapping\" and Apache Arrow? \\\\* How can you create your own dataset and push it to the Hub? The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let\\'s get started!\\n\\n!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Fine-tune a pretrained model\\n[[open-in-colab]]\\nThere are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ü§ó Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:\\n\\\\* Fine-tune a pretrained model with ü§ó Transformers [`Trainer`]. \\\\* Fine-tune a pretrained model in TensorFlow with Keras. \\\\* Fine-tune a pretrained model in native PyTorch. ## Prepare a dataset\\nBefore you can fine-tune a pretrained model, download a dataset and prepare it for training. The previous tutorial showed you how to process data for training, and now you get an opportunity to put those skills to the test! Begin by loading the [Yelp Reviews](https://huggingface.co/datasets/yelp\\\\_review\\\\_full) dataset:\\n```py\\n>>> from datasets import load\\\\_dataset\\n>>> dataset = load\\\\_dataset(\"yelp\\\\_review\\\\_full\")\\n>>> dataset[\"train\"][100]\\n{\\'label\\': 0,\\n\\'text\\': \\'My expectations for McDonalds are t rarely high.\\n\\nFor more information about using ü§ó Datasets, check out the [tutorials](https://huggingface.co/docs/datasets/tutorial) and [how-to guides](https://huggingface.co/docs/datasets/how\\\\_to) available in the ü§ó Datasets documentation.\\n\\nOverview\\nWelcome to the ü§ó Datasets tutorials! These beginner-friendly tutorials will guide you through the fundamentals of working with ü§ó Datasets. You\\'ll load and prepare a dataset for training with your machine learning framework of choice. Along the way, you\\'ll learn how to load different dataset configurations and splits, interact with and see what\\'s inside your dataset, preprocess, and share a dataset to the [Hub](https://huggingface.co/datasets). The tutorials assume some basic knowledge of Python and a machine learning framework like PyTorch or TensorFlow. If you\\'re already familiar with these, feel free to check out the [quickstart](./quickstart) to see what you can do with ü§ó Datasets. The tutorials only cover the basic skills you need to use ü§ó Datasets. There are many other useful functionalities and applications that aren\\'t discussed here. If you\\'re interested in learning more, take a look at [Chapter 5](https://huggingface.co/course/chapter5/1?fw=pt) of the Hugging Face course. If you have any questions about ü§ó Datasets, feel free to join and ask the community on our [forum](https://discuss.huggingface.co/c/datasets/10).', 'type': 'tool', 'name': 'data_retriever', 'id': '76aeed8a-a878-4e9d-adec-bfe25ea16a89', 'tool_call_id': 'call_XbTPRjDgX6HPaSWHXGTUbKNi', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899ba48af3d2ebb270a089c', 'page_content': 'Introduction[[introduction]]\\nIn [Chapter 3](/course/chapter3) you got your first taste of the ü§ó Datasets library and saw that there were three main steps when it came to fine-tuning a model:\\n1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics. But this is just scratching the surface of what ü§ó Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we\\'ll find answers to the following questions:\\n\\\\* What do you do when your dataset is not on the Hub? \\\\* How can you slice and dice a dataset? (And what if you \\\\_really\\\\_ need to use Pandas?)\\n\\\\* What do you do when your dataset is huge and will melt your laptop\\'s RAM? \\\\* What the heck are \"memory mapping\" and Apache Arrow? \\\\* How can you create your own dataset and push it to the Hub? The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let\\'s get started!', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab2ba3e684812d15f67e3', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Fine-tune a pretrained model\\n[[open-in-colab]]\\nThere are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ü§ó Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:\\n\\\\* Fine-tune a pretrained model with ü§ó Transformers [`Trainer`]. \\\\* Fine-tune a pretrained model in TensorFlow with Keras. \\\\* Fine-tune a pretrained model in native PyTorch. ## Prepare a dataset\\nBefore you can fine-tune a pretrained model, download a dataset and prepare it for training. The previous tutorial showed you how to process data for training, and now you get an opportunity to put those skills to the test! Begin by loading the [Yelp Reviews](https://huggingface.co/datasets/yelp\\\\_review\\\\_full) dataset:\\n```py\\n>>> from datasets import load\\\\_dataset\\n>>> dataset = load\\\\_dataset(\"yelp\\\\_review\\\\_full\")\\n>>> dataset[\"train\"][100]\\n{\\'label\\': 0,\\n\\'text\\': \\'My expectations for McDonalds are t rarely high.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bec1666af48523f51d13', 'page_content': 'For more information about using ü§ó Datasets, check out the [tutorials](https://huggingface.co/docs/datasets/tutorial) and [how-to guides](https://huggingface.co/docs/datasets/how\\\\_to) available in the ü§ó Datasets documentation.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c236666af48523f52149', 'page_content': \"Overview\\nWelcome to the ü§ó Datasets tutorials! These beginner-friendly tutorials will guide you through the fundamentals of working with ü§ó Datasets. You'll load and prepare a dataset for training with your machine learning framework of choice. Along the way, you'll learn how to load different dataset configurations and splits, interact with and see what's inside your dataset, preprocess, and share a dataset to the [Hub](https://huggingface.co/datasets). The tutorials assume some basic knowledge of Python and a machine learning framework like PyTorch or TensorFlow. If you're already familiar with these, feel free to check out the [quickstart](./quickstart) to see what you can do with ü§ó Datasets. The tutorials only cover the basic skills you need to use ü§ó Datasets. There are many other useful functionalities and applications that aren't discussed here. If you're interested in learning more, take a look at [Chapter 5](https://huggingface.co/course/chapter5/1?fw=pt) of the Hugging Face course. If you have any questions about ü§ó Datasets, feel free to join and ask the community on our [forum](https://discuss.huggingface.co/c/datasets/10).\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The three main steps for fine-tuning a model with the ü§ó Datasets library are:\\n\\n1. Load a dataset from the Hugging Face Hub.\\n2. Preprocess the data with `Dataset.map()`.\\n3. Load and compute metrics.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--9989cad9-2c11-4cc5-90ba-a37042c09f79-0', 'usage_metadata': {'input_tokens': 1312, 'output_tokens': 50, 'total_tokens': 1362, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:27:06,095 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 29%|‚ñà‚ñà‚ñâ       | 19/65 [01:19<03:03,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\\n', 'type': 'human', 'id': 'e78f66d4-aef1-41ec-978a-fd596745f826'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_Acx2ZyDCxmWIjP3aLIpyDgQJ', 'function': {'arguments': '{\"query\":\"maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--acbc4e9a-2802-48da-bb48-16348c3133b1-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers'}, 'id': 'call_Acx2ZyDCxmWIjP3aLIpyDgQJ', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 366, 'output_tokens': 29, 'total_tokens': 395, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark. Figure 4. Latency (Batch=1, Physical Cores=2)\\n\\n---\\n## Conclusion\\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput. Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens. The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial)\\n## Resources\\n\\\\* [Hugging Face Infinity](https://huggingface.co/infinity)\\n\\\\* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\\n\\\\* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i)\\n\\\\* [DistilBERT](https://huggingface.co/docs/transformers/model\\\\_doc/distilbert)\\n\\\\* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\\n\\\\* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\\n\\\\* [ü§ó Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)\\n\\nYou can test it and get a feeling for the performance of Infinity. Below you will find a `curl` command on how to send a request to the hosted endpoint. The API returns a `x-compute-time` HTTP Header, which contains the duration of the end-to-end pipeline. ```bash\\ncurl --request POST `-i` \\\\\\n--url https://infinity.huggingface.co/cpu/distilbert-base-uncased-emotion \\\\\\n--header \\'Content-Type: application/json\\' \\\\\\n--data \\'{\"inputs\":\"I like you. I love you\"}\\'\\n```\\n### Throughput\\nBelow you can find the throughput comparison for running infinity on 2 physical cores with batch size 1, compared with vanilla transformers. Figure 3. Throughput: Infinity vs Transformers\\n\\n| Sequence Length | Infinity | Transformers | improvement |\\n|-----------------|-------------|--------------|-------------|\\n| 8 | 248 req/sec | 49 req/sec | +506% |\\n| 16 | 212 req/sec | 50 req/sec | +424% |\\n| 32 | 150 req/sec | 40 req/sec | +375% |\\n| 64 | 97 req/sec | 28 req/sec | +346% |\\n| 128 | 55 req/sec | 18 req/sec | +305% |\\n| 256 | 27 req/sec | 9 req/sec | +300% |\\n| 384 | 17 req/sec | 5 req/sec | +340% |\\n| 512 | 12 req/sec | 4 req/sec | +300% |\\n### Latency\\nBelow, you can find the latency results for an experiment running Hugging Face Infinity on 2 Physical Cores with Batch Size 1.\\n\\n--\\ntitle: \"Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\"\\nthumbnail: /blog/assets/46\\\\_infinity\\\\_cpu\\\\_performance/thumbnail.png\\nauthors:\\n- user: philschmid\\n- user: jeffboudier\\n- user: mfuntowicz\\n---\\n# Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\\n\\nDecember 2022 Update: Infinity is no longer offered by Hugging Face as a commercial inference solution. To deploy and accelerate your models, we recommend the following new solutions:\\n\\\\* [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) to easily deploy models on dedicated infrastructure managed by Hugging Face. \\\\* Our open-source optimization libraries, [ü§ó Optimum Intel](https://huggingface.co/blog/openvino) and [ü§ó Optimum ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview), to get the highest efficiency out of training and running models for inference. \\\\* Hugging Face [Expert Acceleration Program](https://huggingface.co/support), a commercial service for Hugging Face experts to work directly with your team to accelerate your Machine Learning roadmap and models. ## Introduction\\nTransfer learning has changed Machine Learning by reaching new levels of accuracy from Natural Language Processing (NLP) to Audio and Computer Vision tasks. At Hugging Face, we work hard to make these new complex models and large checkpoints as easily accessible and usable as possible. But while researchers and data scientists have converted to the new world of Transformers, few companies have been able to deploy these large, complex models in production at scale. The main bottleneck is the latency of predictions which can make large deployments expensive to run and real-time use cases impractical. Solving this is a difficult engineering challenge for any Machine Learning Engineering team and requires the use of advanced techniques to optimize models all the way down to the hardware. With [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments. ## What is Hugging Face Infinity\\nHugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure. Hugging Face Infinity consists of 2 main services:\\n\\\\* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container. \\\\* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container. The Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference. Figure 1. Infinity Overview\\n\\nAn Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learning tasks as defined in the [Transformers Pipelines documentation](https://huggingface.co/docs/transformers/master/en/main\\\\_classes/pipelines).\\n\\n--\\ntitle: \"How we sped up transformer inference 100x for ü§ó API customers\"\\nthumbnail: /blog/assets/09\\\\_accelerated\\\\_inference/thumbnail.png\\n---\\n# How we sped up transformer inference 100x for ü§ó API customers\\nü§ó Transformers has become the default library for data scientists all around the world to explore state of the art NLP models and build new NLP features. With over 5,000 pre-trained and fine-tuned models available, in over 250 languages, it is a rich playground, easily accessible whichever framework you are working in. While experimenting with models in ü§ó Transformers is easy, deploying these large models into production with maximum performance, and managing them into an architecture that scales with usage is a \\\\*\\\\*hard engineering challenge\\\\*\\\\* for any Machine Learning Engineer. This 100x performance gain and built-in scalability is why subscribers of our hosted [Accelerated Inference API](https://huggingface.co/pricing) chose to build their NLP features on top of it. To get to the \\\\*\\\\*last 10x of performance\\\\*\\\\* boost, the optimizations need to be low-level, specific to the model, and to the target hardware. This post shares some of our approaches squeezing every drop of compute juice for our customers. üçã\\n## Getting to the first 10x speedup\\nThe first leg of the optimization journey is the most accessible, all about using the best combination of techniques offered by the [Hugging Face libraries](https://github.com/huggingface/), independent of the target hardware. We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main\\\\_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:\\n-| Naive version | Optimized version |\\n-|:---------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------:|\\n-|![](/blog/assets/09\\\\_accelerated\\\\_inference/unoptimized\\\\_graph.png)|![](/blog/assets/09\\\\_accelerated\\\\_inference/optimized\\\\_graph.png)|\\nTokenization is often a bottleneck for efficiency during inference. We use the most efficient methods from the [ü§ó Tokenizers](https://github.com/huggingface/tokenizers/) library, leveraging the Rust implementation of the model tokenizer in combination with smart caching to get up to 10x speedup for the overall latency. Leveraging the latest features of the Hugging Face libraries, we achieve a reliable 10x speed up compared to an out-of-box deployment for a given model/hardware pair. As new releases of Transformers and Tokenizers typically ship every month, our API customers do not need to constantly adapt to new optimization opportunities, their models just keep running faster.', 'type': 'tool', 'name': 'data_retriever', 'id': '2ac3123f-1f89-45f4-b108-597920c45c8b', 'tool_call_id': 'call_Acx2ZyDCxmWIjP3aLIpyDgQJ', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab7f83e684812d15f6e43', 'page_content': 'It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark. Figure 4. Latency (Batch=1, Physical Cores=2)\\n\\n---\\n## Conclusion\\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput. Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens. The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial)\\n## Resources\\n\\\\* [Hugging Face Infinity](https://huggingface.co/infinity)\\n\\\\* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\\n\\\\* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i)\\n\\\\* [DistilBERT](https://huggingface.co/docs/transformers/model\\\\_doc/distilbert)\\n\\\\* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\\n\\\\* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\\n\\\\* [ü§ó Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab7f83e684812d15f6e42', 'page_content': 'You can test it and get a feeling for the performance of Infinity. Below you will find a `curl` command on how to send a request to the hosted endpoint. The API returns a `x-compute-time` HTTP Header, which contains the duration of the end-to-end pipeline. ```bash\\ncurl --request POST `-i` \\\\\\n--url https://infinity.huggingface.co/cpu/distilbert-base-uncased-emotion \\\\\\n--header \\'Content-Type: application/json\\' \\\\\\n--data \\'{\"inputs\":\"I like you. I love you\"}\\'\\n```\\n### Throughput\\nBelow you can find the throughput comparison for running infinity on 2 physical cores with batch size 1, compared with vanilla transformers. Figure 3. Throughput: Infinity vs Transformers\\n\\n| Sequence Length | Infinity | Transformers | improvement |\\n|-----------------|-------------|--------------|-------------|\\n| 8 | 248 req/sec | 49 req/sec | +506% |\\n| 16 | 212 req/sec | 50 req/sec | +424% |\\n| 32 | 150 req/sec | 40 req/sec | +375% |\\n| 64 | 97 req/sec | 28 req/sec | +346% |\\n| 128 | 55 req/sec | 18 req/sec | +305% |\\n| 256 | 27 req/sec | 9 req/sec | +300% |\\n| 384 | 17 req/sec | 5 req/sec | +340% |\\n| 512 | 12 req/sec | 4 req/sec | +300% |\\n### Latency\\nBelow, you can find the latency results for an experiment running Hugging Face Infinity on 2 Physical Cores with Batch Size 1.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab7f83e684812d15f6e3f', 'page_content': '--\\ntitle: \"Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\"\\nthumbnail: /blog/assets/46\\\\_infinity\\\\_cpu\\\\_performance/thumbnail.png\\nauthors:\\n- user: philschmid\\n- user: jeffboudier\\n- user: mfuntowicz\\n---\\n# Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\\n\\nDecember 2022 Update: Infinity is no longer offered by Hugging Face as a commercial inference solution. To deploy and accelerate your models, we recommend the following new solutions:\\n\\\\* [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) to easily deploy models on dedicated infrastructure managed by Hugging Face. \\\\* Our open-source optimization libraries, [ü§ó Optimum Intel](https://huggingface.co/blog/openvino) and [ü§ó Optimum ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview), to get the highest efficiency out of training and running models for inference. \\\\* Hugging Face [Expert Acceleration Program](https://huggingface.co/support), a commercial service for Hugging Face experts to work directly with your team to accelerate your Machine Learning roadmap and models. ## Introduction\\nTransfer learning has changed Machine Learning by reaching new levels of accuracy from Natural Language Processing (NLP) to Audio and Computer Vision tasks. At Hugging Face, we work hard to make these new complex models and large checkpoints as easily accessible and usable as possible. But while researchers and data scientists have converted to the new world of Transformers, few companies have been able to deploy these large, complex models in production at scale. The main bottleneck is the latency of predictions which can make large deployments expensive to run and real-time use cases impractical. Solving this is a difficult engineering challenge for any Machine Learning Engineering team and requires the use of advanced techniques to optimize models all the way down to the hardware. With [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments. ## What is Hugging Face Infinity\\nHugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure. Hugging Face Infinity consists of 2 main services:\\n\\\\* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container. \\\\* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container. The Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference. Figure 1. Infinity Overview\\n\\nAn Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learning tasks as defined in the [Transformers Pipelines documentation](https://huggingface.co/docs/transformers/master/en/main\\\\_classes/pipelines).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b3a7af3d2ebb270a0066', 'page_content': '--\\ntitle: \"How we sped up transformer inference 100x for ü§ó API customers\"\\nthumbnail: /blog/assets/09\\\\_accelerated\\\\_inference/thumbnail.png\\n---\\n# How we sped up transformer inference 100x for ü§ó API customers\\nü§ó Transformers has become the default library for data scientists all around the world to explore state of the art NLP models and build new NLP features. With over 5,000 pre-trained and fine-tuned models available, in over 250 languages, it is a rich playground, easily accessible whichever framework you are working in. While experimenting with models in ü§ó Transformers is easy, deploying these large models into production with maximum performance, and managing them into an architecture that scales with usage is a \\\\*\\\\*hard engineering challenge\\\\*\\\\* for any Machine Learning Engineer. This 100x performance gain and built-in scalability is why subscribers of our hosted [Accelerated Inference API](https://huggingface.co/pricing) chose to build their NLP features on top of it. To get to the \\\\*\\\\*last 10x of performance\\\\*\\\\* boost, the optimizations need to be low-level, specific to the model, and to the target hardware. This post shares some of our approaches squeezing every drop of compute juice for our customers. üçã\\n## Getting to the first 10x speedup\\nThe first leg of the optimization journey is the most accessible, all about using the best combination of techniques offered by the [Hugging Face libraries](https://github.com/huggingface/), independent of the target hardware. We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main\\\\_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:\\n-| Naive version | Optimized version |\\n-|:---------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------:|\\n-|![](/blog/assets/09\\\\_accelerated\\\\_inference/unoptimized\\\\_graph.png)|![](/blog/assets/09\\\\_accelerated\\\\_inference/optimized\\\\_graph.png)|\\nTokenization is often a bottleneck for efficiency during inference. We use the most efficient methods from the [ü§ó Tokenizers](https://github.com/huggingface/tokenizers/) library, leveraging the Rust implementation of the model tokenizer in combination with smart caching to get up to 10x speedup for the overall latency. Leveraging the latest features of the Hugging Face libraries, we achieve a reliable 10x speed up compared to an out-of-box deployment for a given model/hardware pair. As new releases of Transformers and Tokenizers typically ship every month, our API customers do not need to constantly adapt to new optimization opportunities, their models just keep running faster.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--34d58fcc-0222-4408-8720-b918755c95c2-0', 'usage_metadata': {'input_tokens': 2418, 'output_tokens': 19, 'total_tokens': 2437, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:27:10,856 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 31%|‚ñà‚ñà‚ñà       | 20/65 [01:24<03:10,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the command to upload a spaCy pipeline to the Hugging Face Hub?\\n', 'type': 'human', 'id': 'cc3b9b1b-f8f4-4c39-a727-b01778bc4812'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_K3xCCa4kaZIm8mxZr1anWQNG', 'function': {'arguments': '{\"query\":\"upload spaCy pipeline to Hugging Face Hub\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--1e43061a-2499-48c6-930c-cb11c81805f0-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'upload spaCy pipeline to Hugging Face Hub'}, 'id': 'call_K3xCCa4kaZIm8mxZr1anWQNG', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 24, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Hub methods\\nMethods for using the Hugging Face Hub:\\n## Push to hub\\n[[autodoc]] evaluate.push\\\\_to\\\\_hub\\n\\n--\\ntitle: \"Welcome spaCy to the Hugging Face Hub\"\\nthumbnail: /blog/assets/23\\\\_spacy/thumbnail.png\\nauthors:\\n- user: osanseviero\\n- user: ines\\n---\\n# Welcome spaCy to the Hugging Face Hub\\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text. Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You\\'ll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production! ### Finding models\\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy. ### Widgets\\nThis integration includes support for NER widgets, so all models with a NER component will have this out of the box! Coming soon there will be support for text classification and POS. [`spacy/en_core_web_sm`](/spacy/en_core_web_sm)\\n\\nHosted inference API\\n\\nToken Classification\\n\\nCompute\\n\\nThis model is currently loaded and running on the Inference API. JSON Output \\nMaximize\\n\\n### Using existing models\\nAll models from the Hub can be directly installed using `pip install`. ```bash\\npip install https://huggingface.co/spacy/en\\\\_core\\\\_web\\\\_sm/resolve/main/en\\\\_core\\\\_web\\\\_sm-any-py3-none-any.whl\\n```\\n```python\\n# Using spacy.load(). import spacy\\nnlp = spacy.load(\"en\\\\_core\\\\_web\\\\_sm\")\\n# Importing as module. import en\\\\_core\\\\_web\\\\_sm\\nnlp = en\\\\_core\\\\_web\\\\_sm.load()\\n```\\nWhen you open a repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model!\\n\\nHugging Face Hub documentation\\nThe Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. The Hub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine Learning. Are you ready to join the path towards open source Machine Learning? ü§ó\\n\\nRepositories\\n\\n[Introduction](./repositories)\\n[Getting Started](./repositories-getting-started)\\n[Repository Settings](./repositories-settings)\\n[Pull requests and Discussions](./repositories-pull-requests-discussions)\\n[Notifications](./notifications)\\n[Collections](./collections)\\n[Webhooks](./webhooks)\\n[Next Steps](./repositories-next-steps)\\n[Licenses](./repositories-licenses)\\n\\nModels\\n\\n[Introduction](./models)\\n[The Model Hub](./models-the-hub)\\n[Model Cards](./model-cards)\\n[Gated Models](./models-gated)\\n[Uploading Models](./models-uploading)\\n[Downloading Models](./models-downloading)\\n[Libraries](./models-libraries)\\n[Tasks](./models-tasks)\\n[Widgets](./models-widgets)\\n[Inference API](./models-inference)\\n[Download Stats](./models-download-stats)\\n\\nDatasets\\n\\n[Introduction](./datasets)\\n[Datasets Overview](./datasets-overview)\\n[Dataset Cards](./datasets-cards)\\n[Gated Datasets](./datasets-gated)\\n[Uploading Datasets](./datasets-adding)\\n[Downloading Datasets](./datasets-downloading)\\n[Libraries](./datasets-libraries)\\n[Dataset Viewer](./datasets-viewer)\\n[Download Stats](./datasets-download-stats)\\n[Data files Configuration](./datasets-data-files-configuration)\\n\\nSpaces\\n\\n[Introduction](./spaces)\\n[Spaces Overview](./spaces-overview)\\n[Gradio Spaces](./spaces-sdks-gradio)\\n[Streamlit Spaces](./spaces-sdks-streamlit)\\n[Static HTML Spaces](./spaces-sdks-static)\\n[Docker Spaces](./spaces-sdks-docker)\\n[Embed your Space](./spaces-embed)\\n[Run with Docker](./spaces-run-with-docker)\\n[Reference](./spaces-config-reference)\\n[Changelog](./spaces-changelog)\\n[Advanced Topics](./spaces-advanced)\\n[Sign in with HF](./spaces-oauth)\\n\\nOther\\n\\n[Organizations](./organizations)\\n[Enterprise Hub](./enterprise-hub)\\n[Billing](./billing)\\n[Security](./security)\\n[Moderation](./moderation)\\n[Paper Pages](./paper-pages)\\n[Search](./search)\\n[Digital Object Identifier (DOI)](./doi)\\n[Hub API Endpoints](./api)\\n[Sign in with HF](./oauth)\\n[Contributor Code of Conduct](https://huggingface.co/code-of-conduct)\\n[Content Guidelines](https://huggingface.co/content-guidelines)\\n\\n## What\\'s the Hugging Face Hub?\\n\\n| Partially handled by `huggingface\\\\_hub`. |', 'type': 'tool', 'name': 'data_retriever', 'id': 'f6728308-378b-4b60-ab1a-fa20fb13fe99', 'tool_call_id': 'call_K3xCCa4kaZIm8mxZr1anWQNG', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab48b3e684812d15f6a0b', 'page_content': 'Hub methods\\nMethods for using the Hugging Face Hub:\\n## Push to hub\\n[[autodoc]] evaluate.push\\\\_to\\\\_hub', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aab923e684812d15f5fa9', 'page_content': '--\\ntitle: \"Welcome spaCy to the Hugging Face Hub\"\\nthumbnail: /blog/assets/23\\\\_spacy/thumbnail.png\\nauthors:\\n- user: osanseviero\\n- user: ines\\n---\\n# Welcome spaCy to the Hugging Face Hub\\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text. Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You\\'ll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production! ### Finding models\\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy. ### Widgets\\nThis integration includes support for NER widgets, so all models with a NER component will have this out of the box! Coming soon there will be support for text classification and POS. [`spacy/en_core_web_sm`](/spacy/en_core_web_sm)\\n\\nHosted inference API\\n\\nToken Classification\\n\\nCompute\\n\\nThis model is currently loaded and running on the Inference API. JSON Output \\nMaximize\\n\\n### Using existing models\\nAll models from the Hub can be directly installed using `pip install`. ```bash\\npip install https://huggingface.co/spacy/en\\\\_core\\\\_web\\\\_sm/resolve/main/en\\\\_core\\\\_web\\\\_sm-any-py3-none-any.whl\\n```\\n```python\\n# Using spacy.load(). import spacy\\nnlp = spacy.load(\"en\\\\_core\\\\_web\\\\_sm\")\\n# Importing as module. import en\\\\_core\\\\_web\\\\_sm\\nnlp = en\\\\_core\\\\_web\\\\_sm.load()\\n```\\nWhen you open a repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model!', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bdce666af48523f51bb1', 'page_content': \"Hugging Face Hub documentation\\nThe Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. The Hub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine Learning. Are you ready to join the path towards open source Machine Learning? ü§ó\\n\\nRepositories\\n\\n[Introduction](./repositories)\\n[Getting Started](./repositories-getting-started)\\n[Repository Settings](./repositories-settings)\\n[Pull requests and Discussions](./repositories-pull-requests-discussions)\\n[Notifications](./notifications)\\n[Collections](./collections)\\n[Webhooks](./webhooks)\\n[Next Steps](./repositories-next-steps)\\n[Licenses](./repositories-licenses)\\n\\nModels\\n\\n[Introduction](./models)\\n[The Model Hub](./models-the-hub)\\n[Model Cards](./model-cards)\\n[Gated Models](./models-gated)\\n[Uploading Models](./models-uploading)\\n[Downloading Models](./models-downloading)\\n[Libraries](./models-libraries)\\n[Tasks](./models-tasks)\\n[Widgets](./models-widgets)\\n[Inference API](./models-inference)\\n[Download Stats](./models-download-stats)\\n\\nDatasets\\n\\n[Introduction](./datasets)\\n[Datasets Overview](./datasets-overview)\\n[Dataset Cards](./datasets-cards)\\n[Gated Datasets](./datasets-gated)\\n[Uploading Datasets](./datasets-adding)\\n[Downloading Datasets](./datasets-downloading)\\n[Libraries](./datasets-libraries)\\n[Dataset Viewer](./datasets-viewer)\\n[Download Stats](./datasets-download-stats)\\n[Data files Configuration](./datasets-data-files-configuration)\\n\\nSpaces\\n\\n[Introduction](./spaces)\\n[Spaces Overview](./spaces-overview)\\n[Gradio Spaces](./spaces-sdks-gradio)\\n[Streamlit Spaces](./spaces-sdks-streamlit)\\n[Static HTML Spaces](./spaces-sdks-static)\\n[Docker Spaces](./spaces-sdks-docker)\\n[Embed your Space](./spaces-embed)\\n[Run with Docker](./spaces-run-with-docker)\\n[Reference](./spaces-config-reference)\\n[Changelog](./spaces-changelog)\\n[Advanced Topics](./spaces-advanced)\\n[Sign in with HF](./spaces-oauth)\\n\\nOther\\n\\n[Organizations](./organizations)\\n[Enterprise Hub](./enterprise-hub)\\n[Billing](./billing)\\n[Security](./security)\\n[Moderation](./moderation)\\n[Paper Pages](./paper-pages)\\n[Search](./search)\\n[Digital Object Identifier (DOI)](./doi)\\n[Hub API Endpoints](./api)\\n[Sign in with HF](./oauth)\\n[Contributor Code of Conduct](https://huggingface.co/code-of-conduct)\\n[Content Guidelines](https://huggingface.co/content-guidelines)\\n\\n## What's the Hugging Face Hub?\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaeb93e684812d15f637d', 'page_content': '| Partially handled by `huggingface\\\\_hub`. |', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'To upload a spaCy pipeline to the Hugging Face Hub, you can use the command:\\n\\n```bash\\nevaluate.push_to_hub\\n``` \\n\\nThis command allows you to upload any pipeline package along with a model card and all required metadata auto-generated for you.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--0ec20676-6e98-4059-a2c0-01e12ff0e013-0', 'usage_metadata': {'input_tokens': 1508, 'output_tokens': 53, 'total_tokens': 1561, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:27:14,549 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 21/65 [01:28<02:58,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': \"What is the time and memory complexity of the Nystr√∂mformer's approximation of self-attention?\\n\", 'type': 'human', 'id': 'd180482e-0b5b-4a9a-a376-506f4e4cf6f7'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_DYvPivPVkP6L9whnVZfKuiJB', 'function': {'arguments': '{\"query\":\"Nystr√∂mformer time and memory complexity\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--895a166e-547e-4c0f-af40-5ffbdbd6de5d-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Nystr√∂mformer time and memory complexity'}, 'id': 'call_DYvPivPVkP6L9whnVZfKuiJB', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 367, 'output_tokens': 22, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '--\\ntitle: \"Nystr√∂mformer: Approximating self-attention in linear time and memory via the Nystr√∂m method\"\\nthumbnail: /blog/assets/86\\\\_nystromformer/thumbnail.png\\nauthors:\\n- user: asi\\nguest: true\\n---\\n# Nystr√∂mformer: Approximating self-attention in linear time and memory via the Nystr√∂m method\\n## Introduction\\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\\\\\(O(n^2)\\\\\\\\) (where \\\\\\\\(n\\\\\\\\) is the length of the input sequence), making it expensive to train on long input sequences. The [Nystr√∂mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\\\\\(O(n)\\\\\\\\) complexity. Nystr√∂mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nystr√∂m method and how it can be adapted to approximate self-attention. ## Nystr√∂m method for matrix approximation\\nAt the heart of Nystr√∂mformer is the Nystr√∂m method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let\\'s consider a matrix \\\\\\\\(P^{n \\\\times n}\\\\\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nystr√∂m method. We start by sampling \\\\\\\\(m\\\\\\\\) rows and columns from \\\\\\\\(P\\\\\\\\). We can then arrange the sampled rows and columns as follows:\\n\\nRepresenting P as a block matrix\\n\\nWe now have four submatrices: \\\\\\\\(A\\\\_P, B\\\\_P, F\\\\_P,\\\\\\\\) and \\\\\\\\(C\\\\_P\\\\\\\\), with sizes \\\\\\\\(m \\\\times m, m \\\\times (n - m), (n - m) \\\\times m\\\\\\\\) and\\n\\\\\\\\((n - m) \\\\times (n - m)\\\\\\\\) respectively. The \\\\\\\\(m\\\\\\\\) sampled columns are contained in \\\\\\\\(A\\\\_P\\\\\\\\) and \\\\\\\\(F\\\\_P\\\\\\\\), whereas the \\\\\\\\(m\\\\\\\\) sampled rows are contained in \\\\\\\\(A\\\\_P\\\\\\\\) and \\\\\\\\(B\\\\_P\\\\\\\\). So, the entries of \\\\\\\\(A\\\\_P, B\\\\_P,\\\\\\\\) and \\\\\\\\(F\\\\_P\\\\\\\\) are known to us, and we will estimate \\\\\\\\(C\\\\_P\\\\\\\\). According to the Nystr√∂m method, \\\\\\\\(C\\\\_P\\\\\\\\) is given by:\\n$$C\\\\_P = F\\\\_P A\\\\_P^+ B\\\\_P$$\\nHere, \\\\\\\\(+\\\\\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). Thus, the Nystr√∂m approximation of \\\\\\\\(P, \\\\hat{P}\\\\\\\\) can be written as:\\n\\nNystr√∂m approximation of P\\n\\nAs shown in the second line, \\\\\\\\(\\\\hat{P}\\\\\\\\) can be expressed as a product of three matrices.\\n\\nThe abstract from the paper is the following:\\n\\\\*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\\ncommunity. To address this limitation, we propose Nystr√∂mformer -- a model that exhibits favorable scalability as a\\nfunction of sequence length. Our idea is based on adapting the Nystr√∂m method to approximate standard self-attention\\nwith O(n) complexity. The scalability of Nystr√∂mformer enables application to longer sequences with thousands of\\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\\nsequence length, and find that our Nystr√∂mformer performs comparably, or in a few cases, even slightly better, than\\nstandard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr√∂mformer performs\\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.\\\\*\\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/Nystromformer). ## Resources\\n- [Text classification task guide](../tasks/sequence\\\\_classification)\\n- [Token classification task guide](../tasks/token\\\\_classification)\\n- [Question answering task guide](../tasks/question\\\\_answering)\\n- [Masked language modeling task guide](../tasks/masked\\\\_language\\\\_modeling)\\n- [Multiple choice task guide](../tasks/multiple\\\\_choice)\\n## NystromformerConfig\\n[[autodoc]] NystromformerConfig\\n## NystromformerModel\\n[[autodoc]] NystromformerModel\\n- forward\\n## NystromformerForMaskedLM\\n[[autodoc]] NystromformerForMaskedLM\\n- forward\\n## NystromformerForSequenceClassification\\n[[autodoc]] NystromformerForSequenceClassification\\n- forward\\n## NystromformerForMultipleChoice\\n[[autodoc]] NystromformerForMultipleChoice\\n- forward\\n## NystromformerForTokenClassification\\n[[autodoc]] NystromformerForTokenClassification\\n- forward\\n## NystromformerForQuestionAnswering\\n[[autodoc]] NystromformerForQuestionAnswering\\n- forward\\n\\n!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Nystr√∂mformer\\n## Overview\\nThe Nystr√∂mformer model was proposed in [\\\\*Nystr√∂mformer: A Nystr√∂m-Based Algorithm for Approximating Self-Attention\\\\*](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn\\nFung, Yin Li, and Vikas Singh.\\n\\nAlso, notice that there is a DConv box. This corresponds to a skip connection added to the values using a 1D depthwise convolution. ## How is Nystr√∂mformer implemented? The original implementation of Nystr√∂mformer can be found [here](https://github.com/mlpen/Nystromformer) and the HuggingFace implementation can be found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/nystromformer/modeling\\\\_nystromformer.py). Let\\'s take a look at a few lines of code (with some comments added) from the HuggingFace implementation. Note that some details such as normalization, attention masking, and depthwise convolution are avoided for simplicity. ```python\\nkey\\\\_layer = self.transpose\\\\_for\\\\_scores(self.key(hidden\\\\_states)) # K\\nvalue\\\\_layer = self.transpose\\\\_for\\\\_scores(self.value(hidden\\\\_states)) # V\\nquery\\\\_layer = self.transpose\\\\_for\\\\_scores(mixed\\\\_query\\\\_layer) # Q\\nq\\\\_landmarks = query\\\\_layer.reshape(\\n-1,\\nself.num\\\\_attention\\\\_heads,\\nself.num\\\\_landmarks,\\nself.seq\\\\_len // self.num\\\\_landmarks,\\nself.attention\\\\_head\\\\_size,\\n).mean(dim=-2) # \\\\tilde{Q}\\nk\\\\_landmarks = key\\\\_layer.reshape(\\n-1,\\nself.num\\\\_attention\\\\_heads,\\nself.num\\\\_landmarks,\\nself.seq\\\\_len // self.num\\\\_landmarks,\\nself.attention\\\\_head\\\\_size,\\n).mean(dim=-2) # \\\\tilde{K}\\nkernel\\\\_1 = torch.nn.functional.softmax(torch.matmul(query\\\\_layer, k\\\\_landmarks.transpose(-1, -2)), dim=-1) # \\\\tilde{F}\\nkernel\\\\_2 = torch.nn.functional.softmax(torch.matmul(q\\\\_landmarks, k\\\\_landmarks.transpose(-1, -2)), dim=-1) # \\\\tilde{A} before pseudo-inverse\\nattention\\\\_scores = torch.matmul(q\\\\_landmarks, key\\\\_layer.transpose(-1, -2)) # \\\\tilde{B} before softmax\\nkernel\\\\_3 = nn.functional.softmax(attention\\\\_scores, dim=-1) # \\\\tilde{B}\\nattention\\\\_probs = torch.matmul(kernel\\\\_1, self.iterative\\\\_inv(kernel\\\\_2)) # \\\\tilde{F} \\\\* \\\\tilde{A}\\nnew\\\\_value\\\\_layer = torch.matmul(kernel\\\\_3, value\\\\_layer) # \\\\tilde{B} \\\\* V\\ncontext\\\\_layer = torch.matmul(attention\\\\_probs, new\\\\_value\\\\_layer) # \\\\tilde{F} \\\\* \\\\tilde{A} \\\\* \\\\tilde{B} \\\\* V\\n```\\n## Using Nystr√∂mformer with HuggingFace\\nNystr√∂mformer for Masked Language Modeling (MLM) is available on HuggingFace. Currently, there are 4 checkpoints, corresponding to various sequence lengths: [`nystromformer-512`](https://huggingface.co/uw-madison/nystromformer-512), [`nystromformer-1024`](https://huggingface.co/uw-madison/nystromformer-1024), [`nystromformer-2048`](https://huggingface.co/uw-madison/nystromformer-2048), and [`nystromformer-4096`](https://huggingface.co/uw-madison/nystromformer-4096). The number of landmarks, \\\\\\\\(m\\\\\\\\), can be controlled using the `num\\\\_landmarks` parameter in the [`NystromformerConfig`](https://huggingface.co/docs/transformers/v4.18.0/en/model\\\\_doc/nystromformer#transformers.NystromformerConfig). Let\\'s take a look at a minimal example of Nystr√∂mformer for MLM:\\n```python\\nfrom transformers import AutoTokenizer, NystromformerForMaskedLM\\nimport torch\\ntokenizer = AutoTokenizer.from\\\\_pretrained(\"uw-madison/nystromformer-512\")\\nmodel = NystromformerForMaskedLM.from\\\\_pretrained(\"uw-madison/nystromformer-512\")\\ninputs = tokenizer(\"Paris is the [MASK] of France.\", return\\\\_tensors=\"pt\")\\nwith torch.no\\\\_grad():\\nlogits = model(\\\\*\\\\*inputs).logits\\n# retrieve index of [MASK]\\nmask\\\\_token\\\\_index = (inputs.input\\\\_ids == tokenizer.mask\\\\_token\\\\_id)[0].nonzero(as\\\\_tuple=True)[0]\\npredicted\\\\_token\\\\_id = logits[0, mask\\\\_token\\\\_index].argmax(axis=-1)\\ntokenizer.decode(predicted\\\\_token\\\\_id)\\n```\\n\\nOutput:\\n----------------------------------------------------------------------------------------------------\\ncapital\\n\\nAlternatively, we can use the [pipeline API](https://huggingface.co/docs/transformers/main\\\\_classes/pipelines) (which handles all the complexity for us):\\n```python\\nfrom transformers import pipeline\\nunmasker = pipeline(\\'fill-mask\\', model=\\'uw-madison/nystromformer-512\\')\\nunmasker(\"Paris is the [MASK] of France.\")\\n```\\n\\nOutput:\\n----------------------------------------------------------------------------------------------------\\n[{\\'score\\': 0.829957902431488,\\n\\'token\\': 1030,\\n\\'token\\\\_str\\': \\'capital\\',\\n\\'sequence\\': \\'paris is the capital of france.\\'},\\n{\\'score\\': 0.022157637402415276,\\n\\'token\\': 16081,\\n\\'token\\\\_str\\': \\'birthplace\\',\\n\\'sequence\\': \\'paris is the birthplace of france.\\'},\\n{\\'score\\': 0.01904447190463543,\\n\\'token\\': 197,\\n\\'token\\\\_str\\': \\'name\\',\\n\\'sequence\\': \\'paris is the name of france.\\'},\\n{\\'score\\': 0.017583081498742104,\\n\\'token\\': 1107,\\n\\'token\\\\_str\\': \\'kingdom\\',\\n\\'sequence\\': \\'paris is the kingdom of france.\\'},\\n{\\'score\\': 0.005948934704065323,\\n\\'token\\': 148,\\n\\'token\\\\_str\\': \\'city\\',\\n\\'sequence\\': \\'paris is the city of france.\\'}]\\n\\n## Conclusion\\nNystr√∂mformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other linear self-attention schemes. In this blog post, we went over a high-level overview of the Nystr√∂m method and how it can be leveraged for self-attention.', 'type': 'tool', 'name': 'data_retriever', 'id': '55a1cc27-4cee-4cef-b695-9235962d698b', 'tool_call_id': 'call_DYvPivPVkP6L9whnVZfKuiJB', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aacb93e684812d15f6128', 'page_content': '--\\ntitle: \"Nystr√∂mformer: Approximating self-attention in linear time and memory via the Nystr√∂m method\"\\nthumbnail: /blog/assets/86\\\\_nystromformer/thumbnail.png\\nauthors:\\n- user: asi\\nguest: true\\n---\\n# Nystr√∂mformer: Approximating self-attention in linear time and memory via the Nystr√∂m method\\n## Introduction\\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\\\\\(O(n^2)\\\\\\\\) (where \\\\\\\\(n\\\\\\\\) is the length of the input sequence), making it expensive to train on long input sequences. The [Nystr√∂mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\\\\\(O(n)\\\\\\\\) complexity. Nystr√∂mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nystr√∂m method and how it can be adapted to approximate self-attention. ## Nystr√∂m method for matrix approximation\\nAt the heart of Nystr√∂mformer is the Nystr√∂m method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let\\'s consider a matrix \\\\\\\\(P^{n \\\\times n}\\\\\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nystr√∂m method. We start by sampling \\\\\\\\(m\\\\\\\\) rows and columns from \\\\\\\\(P\\\\\\\\). We can then arrange the sampled rows and columns as follows:\\n\\nRepresenting P as a block matrix\\n\\nWe now have four submatrices: \\\\\\\\(A\\\\_P, B\\\\_P, F\\\\_P,\\\\\\\\) and \\\\\\\\(C\\\\_P\\\\\\\\), with sizes \\\\\\\\(m \\\\times m, m \\\\times (n - m), (n - m) \\\\times m\\\\\\\\) and\\n\\\\\\\\((n - m) \\\\times (n - m)\\\\\\\\) respectively. The \\\\\\\\(m\\\\\\\\) sampled columns are contained in \\\\\\\\(A\\\\_P\\\\\\\\) and \\\\\\\\(F\\\\_P\\\\\\\\), whereas the \\\\\\\\(m\\\\\\\\) sampled rows are contained in \\\\\\\\(A\\\\_P\\\\\\\\) and \\\\\\\\(B\\\\_P\\\\\\\\). So, the entries of \\\\\\\\(A\\\\_P, B\\\\_P,\\\\\\\\) and \\\\\\\\(F\\\\_P\\\\\\\\) are known to us, and we will estimate \\\\\\\\(C\\\\_P\\\\\\\\). According to the Nystr√∂m method, \\\\\\\\(C\\\\_P\\\\\\\\) is given by:\\n$$C\\\\_P = F\\\\_P A\\\\_P^+ B\\\\_P$$\\nHere, \\\\\\\\(+\\\\\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). Thus, the Nystr√∂m approximation of \\\\\\\\(P, \\\\hat{P}\\\\\\\\) can be written as:\\n\\nNystr√∂m approximation of P\\n\\nAs shown in the second line, \\\\\\\\(\\\\hat{P}\\\\\\\\) can be expressed as a product of three matrices.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab5e03e684812d15f6bcf', 'page_content': 'The abstract from the paper is the following:\\n\\\\*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\\ncommunity. To address this limitation, we propose Nystr√∂mformer -- a model that exhibits favorable scalability as a\\nfunction of sequence length. Our idea is based on adapting the Nystr√∂m method to approximate standard self-attention\\nwith O(n) complexity. The scalability of Nystr√∂mformer enables application to longer sequences with thousands of\\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\\nsequence length, and find that our Nystr√∂mformer performs comparably, or in a few cases, even slightly better, than\\nstandard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr√∂mformer performs\\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.\\\\*\\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/Nystromformer). ## Resources\\n- [Text classification task guide](../tasks/sequence\\\\_classification)\\n- [Token classification task guide](../tasks/token\\\\_classification)\\n- [Question answering task guide](../tasks/question\\\\_answering)\\n- [Masked language modeling task guide](../tasks/masked\\\\_language\\\\_modeling)\\n- [Multiple choice task guide](../tasks/multiple\\\\_choice)\\n## NystromformerConfig\\n[[autodoc]] NystromformerConfig\\n## NystromformerModel\\n[[autodoc]] NystromformerModel\\n- forward\\n## NystromformerForMaskedLM\\n[[autodoc]] NystromformerForMaskedLM\\n- forward\\n## NystromformerForSequenceClassification\\n[[autodoc]] NystromformerForSequenceClassification\\n- forward\\n## NystromformerForMultipleChoice\\n[[autodoc]] NystromformerForMultipleChoice\\n- forward\\n## NystromformerForTokenClassification\\n[[autodoc]] NystromformerForTokenClassification\\n- forward\\n## NystromformerForQuestionAnswering\\n[[autodoc]] NystromformerForQuestionAnswering\\n- forward', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab5e03e684812d15f6bce', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Nystr√∂mformer\\n## Overview\\nThe Nystr√∂mformer model was proposed in [\\\\*Nystr√∂mformer: A Nystr√∂m-Based Algorithm for Approximating Self-Attention\\\\*](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn\\nFung, Yin Li, and Vikas Singh.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aacb93e684812d15f612b', 'page_content': 'Also, notice that there is a DConv box. This corresponds to a skip connection added to the values using a 1D depthwise convolution. ## How is Nystr√∂mformer implemented? The original implementation of Nystr√∂mformer can be found [here](https://github.com/mlpen/Nystromformer) and the HuggingFace implementation can be found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/nystromformer/modeling\\\\_nystromformer.py). Let\\'s take a look at a few lines of code (with some comments added) from the HuggingFace implementation. Note that some details such as normalization, attention masking, and depthwise convolution are avoided for simplicity. ```python\\nkey\\\\_layer = self.transpose\\\\_for\\\\_scores(self.key(hidden\\\\_states)) # K\\nvalue\\\\_layer = self.transpose\\\\_for\\\\_scores(self.value(hidden\\\\_states)) # V\\nquery\\\\_layer = self.transpose\\\\_for\\\\_scores(mixed\\\\_query\\\\_layer) # Q\\nq\\\\_landmarks = query\\\\_layer.reshape(\\n-1,\\nself.num\\\\_attention\\\\_heads,\\nself.num\\\\_landmarks,\\nself.seq\\\\_len // self.num\\\\_landmarks,\\nself.attention\\\\_head\\\\_size,\\n).mean(dim=-2) # \\\\tilde{Q}\\nk\\\\_landmarks = key\\\\_layer.reshape(\\n-1,\\nself.num\\\\_attention\\\\_heads,\\nself.num\\\\_landmarks,\\nself.seq\\\\_len // self.num\\\\_landmarks,\\nself.attention\\\\_head\\\\_size,\\n).mean(dim=-2) # \\\\tilde{K}\\nkernel\\\\_1 = torch.nn.functional.softmax(torch.matmul(query\\\\_layer, k\\\\_landmarks.transpose(-1, -2)), dim=-1) # \\\\tilde{F}\\nkernel\\\\_2 = torch.nn.functional.softmax(torch.matmul(q\\\\_landmarks, k\\\\_landmarks.transpose(-1, -2)), dim=-1) # \\\\tilde{A} before pseudo-inverse\\nattention\\\\_scores = torch.matmul(q\\\\_landmarks, key\\\\_layer.transpose(-1, -2)) # \\\\tilde{B} before softmax\\nkernel\\\\_3 = nn.functional.softmax(attention\\\\_scores, dim=-1) # \\\\tilde{B}\\nattention\\\\_probs = torch.matmul(kernel\\\\_1, self.iterative\\\\_inv(kernel\\\\_2)) # \\\\tilde{F} \\\\* \\\\tilde{A}\\nnew\\\\_value\\\\_layer = torch.matmul(kernel\\\\_3, value\\\\_layer) # \\\\tilde{B} \\\\* V\\ncontext\\\\_layer = torch.matmul(attention\\\\_probs, new\\\\_value\\\\_layer) # \\\\tilde{F} \\\\* \\\\tilde{A} \\\\* \\\\tilde{B} \\\\* V\\n```\\n## Using Nystr√∂mformer with HuggingFace\\nNystr√∂mformer for Masked Language Modeling (MLM) is available on HuggingFace. Currently, there are 4 checkpoints, corresponding to various sequence lengths: [`nystromformer-512`](https://huggingface.co/uw-madison/nystromformer-512), [`nystromformer-1024`](https://huggingface.co/uw-madison/nystromformer-1024), [`nystromformer-2048`](https://huggingface.co/uw-madison/nystromformer-2048), and [`nystromformer-4096`](https://huggingface.co/uw-madison/nystromformer-4096). The number of landmarks, \\\\\\\\(m\\\\\\\\), can be controlled using the `num\\\\_landmarks` parameter in the [`NystromformerConfig`](https://huggingface.co/docs/transformers/v4.18.0/en/model\\\\_doc/nystromformer#transformers.NystromformerConfig). Let\\'s take a look at a minimal example of Nystr√∂mformer for MLM:\\n```python\\nfrom transformers import AutoTokenizer, NystromformerForMaskedLM\\nimport torch\\ntokenizer = AutoTokenizer.from\\\\_pretrained(\"uw-madison/nystromformer-512\")\\nmodel = NystromformerForMaskedLM.from\\\\_pretrained(\"uw-madison/nystromformer-512\")\\ninputs = tokenizer(\"Paris is the [MASK] of France.\", return\\\\_tensors=\"pt\")\\nwith torch.no\\\\_grad():\\nlogits = model(\\\\*\\\\*inputs).logits\\n# retrieve index of [MASK]\\nmask\\\\_token\\\\_index = (inputs.input\\\\_ids == tokenizer.mask\\\\_token\\\\_id)[0].nonzero(as\\\\_tuple=True)[0]\\npredicted\\\\_token\\\\_id = logits[0, mask\\\\_token\\\\_index].argmax(axis=-1)\\ntokenizer.decode(predicted\\\\_token\\\\_id)\\n```\\n\\nOutput:\\n----------------------------------------------------------------------------------------------------\\ncapital\\n\\nAlternatively, we can use the [pipeline API](https://huggingface.co/docs/transformers/main\\\\_classes/pipelines) (which handles all the complexity for us):\\n```python\\nfrom transformers import pipeline\\nunmasker = pipeline(\\'fill-mask\\', model=\\'uw-madison/nystromformer-512\\')\\nunmasker(\"Paris is the [MASK] of France.\")\\n```\\n\\nOutput:\\n----------------------------------------------------------------------------------------------------\\n[{\\'score\\': 0.829957902431488,\\n\\'token\\': 1030,\\n\\'token\\\\_str\\': \\'capital\\',\\n\\'sequence\\': \\'paris is the capital of france.\\'},\\n{\\'score\\': 0.022157637402415276,\\n\\'token\\': 16081,\\n\\'token\\\\_str\\': \\'birthplace\\',\\n\\'sequence\\': \\'paris is the birthplace of france.\\'},\\n{\\'score\\': 0.01904447190463543,\\n\\'token\\': 197,\\n\\'token\\\\_str\\': \\'name\\',\\n\\'sequence\\': \\'paris is the name of france.\\'},\\n{\\'score\\': 0.017583081498742104,\\n\\'token\\': 1107,\\n\\'token\\\\_str\\': \\'kingdom\\',\\n\\'sequence\\': \\'paris is the kingdom of france.\\'},\\n{\\'score\\': 0.005948934704065323,\\n\\'token\\': 148,\\n\\'token\\\\_str\\': \\'city\\',\\n\\'sequence\\': \\'paris is the city of france.\\'}]\\n\\n## Conclusion\\nNystr√∂mformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other linear self-attention schemes. In this blog post, we went over a high-level overview of the Nystr√∂m method and how it can be leveraged for self-attention.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"The Nystr√∂mformer's approximation of self-attention has a time and memory complexity of \\\\(O(n)\\\\), where \\\\(n\\\\) is the length of the input sequence. This is an improvement over the standard self-attention mechanism, which has a complexity of \\\\(O(n^2)\\\\).\", 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--379d4ce3-2b29-4d9d-a8f2-fd632dfc8124-0', 'usage_metadata': {'input_tokens': 3156, 'output_tokens': 59, 'total_tokens': 3215, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:27:19,354 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 22/65 [01:32<03:04,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the goal of the Named Entity Recognition task in token classification?\\n', 'type': 'human', 'id': 'e66fdb9f-a27a-4cae-a72e-21ad958c967d'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_7jS5HWMcjGtmZx8H1GoEkkzn', 'function': {'arguments': '{\"query\":\"goal of Named Entity Recognition in token classification\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--483c1e02-f2bf-427b-a083-0c3ec5b635bf-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'goal of Named Entity Recognition in token classification'}, 'id': 'call_7jS5HWMcjGtmZx8H1GoEkkzn', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 23, 'total_tokens': 385, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. Two common types of token classification are:\\n\\\\* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names. \\\\* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb). ```py\\n>>> from transformers import pipeline\\n>>> classifier = pipeline(task=\"ner\")\\n>>> preds = classifier(\"Hugging Face is a French company based in New York City.\")\\n>>> preds = [\\n... {\\n... \"entity\": pred[\"entity\"],\\n... \"score\": round(pred[\"score\"], 4),\\n... \"index\": pred[\"index\"],\\n... \"word\": pred[\"word\"],\\n... \"start\": pred[\"start\"],\\n... \"end\": pred[\"end\"],\\n...\\n\\n!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Token classification\\n[[open-in-colab]]\\nToken classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization. This guide will show you how to:\\n1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [WNUT 17](https://huggingface.co/datasets/wnut\\\\_17) dataset to detect new entities.\\n\\nNamed-Entity Recognition\\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert\\\\_ner\\\\_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial\\\\_Analyst\\\\_AI\\nTags: NER, TEXT, HIGHLIGHT\\n## Introduction\\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \"token\") into different categories, such as names of people or names of locations, or different parts of speech. For example, given the sentence:\\n> Does Chicago have any Pakistani restaurants? A named-entity recognition algorithm may identify:\\n- \"Chicago\" as a \\\\*\\\\*location\\\\*\\\\*\\n- \"Pakistani\" as an \\\\*\\\\*ethnicity\\\\*\\\\*\\nand so on. Using `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team. Here is an example of a demo that you\\'ll be able to build:\\n$demo\\\\_ner\\\\_pipeline\\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!\\n\\net\\'s study how to preprocess a dataset for token classification! Token classification regroups any task that can be framed as labelling each word (or token) in a sentence, like identifying the persons, organizations and locations for instance. For our example, we will use the Conll dataset, in which we remove columns we won\\'t use and rename the other ones to get to a dataset with just two columns: words and labels. If you have your own dataset for token classification, just make sure you clean your data to get to the same point, with one column containing words (as list of strings) and another containing labels (as integers spanning from to to your number of labels -1).() Make sure you have your label names stored somewhere - here we get them from the dataset features - so you are able to map the integers to some real labels when inspecting your data! Here we are doing named entity recognitions, so ours labels are either O for words that do not belong to any entity, LOC, for location, PER, for person, ORG for organization and MISC for miscellaneous. Each label has two versions: the B- labels indicate a word that begins an entity while the I- labels indicate a word that is inside an entity.', 'type': 'tool', 'name': 'data_retriever', 'id': '5f3865e2-164f-4a80-b8d2-1d712ea56bfb', 'tool_call_id': 'call_7jS5HWMcjGtmZx8H1GoEkkzn', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b9faaf3d2ebb270a0845', 'page_content': 'These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. Two common types of token classification are:\\n\\\\* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names. \\\\* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb). ```py\\n>>> from transformers import pipeline\\n>>> classifier = pipeline(task=\"ner\")\\n>>> preds = classifier(\"Hugging Face is a French company based in New York City.\")\\n>>> preds = [\\n... {\\n... \"entity\": pred[\"entity\"],\\n... \"score\": round(pred[\"score\"], 4),\\n... \"index\": pred[\"index\"],\\n... \"word\": pred[\"word\"],\\n... \"start\": pred[\"start\"],\\n... \"end\": pred[\"end\"],\\n...', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3ec3e684812d15f6947', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Token classification\\n[[open-in-colab]]\\nToken classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization. This guide will show you how to:\\n1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [WNUT 17](https://huggingface.co/datasets/wnut\\\\_17) dataset to detect new entities.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c2c7666af48523f521f7', 'page_content': 'Named-Entity Recognition\\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert\\\\_ner\\\\_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial\\\\_Analyst\\\\_AI\\nTags: NER, TEXT, HIGHLIGHT\\n## Introduction\\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \"token\") into different categories, such as names of people or names of locations, or different parts of speech. For example, given the sentence:\\n> Does Chicago have any Pakistani restaurants? A named-entity recognition algorithm may identify:\\n- \"Chicago\" as a \\\\*\\\\*location\\\\*\\\\*\\n- \"Pakistani\" as an \\\\*\\\\*ethnicity\\\\*\\\\*\\nand so on. Using `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team. Here is an example of a demo that you\\'ll be able to build:\\n$demo\\\\_ner\\\\_pipeline\\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aab5a3e684812d15f5f6c', 'page_content': \"et's study how to preprocess a dataset for token classification! Token classification regroups any task that can be framed as labelling each word (or token) in a sentence, like identifying the persons, organizations and locations for instance. For our example, we will use the Conll dataset, in which we remove columns we won't use and rename the other ones to get to a dataset with just two columns: words and labels. If you have your own dataset for token classification, just make sure you clean your data to get to the same point, with one column containing words (as list of strings) and another containing labels (as integers spanning from to to your number of labels -1).() Make sure you have your label names stored somewhere - here we get them from the dataset features - so you are able to map the integers to some real labels when inspecting your data! Here we are doing named entity recognitions, so ours labels are either O for words that do not belong to any entity, LOC, for location, PER, for person, ORG for organization and MISC for miscellaneous. Each label has two versions: the B- labels indicate a word that begins an entity while the I- labels indicate a word that is inside an entity.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The goal of the Named Entity Recognition (NER) task in token classification is to classify every word (or \"token\") in a sentence into different categories, such as names of people, locations, organizations, or other entities. For example, in a sentence like \"Does Chicago have any Pakistani restaurants?\", a NER algorithm may identify \"Chicago\" as a location and \"Pakistani\" as an ethnicity.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--0f47ce30-1f89-4c3b-8ec7-3c9823296abb-0', 'usage_metadata': {'input_tokens': 1343, 'output_tokens': 81, 'total_tokens': 1424, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:27:22,524 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 35%|‚ñà‚ñà‚ñà‚ñå      | 23/65 [01:36<02:45,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the resolution of images used by the CLIPSeg model?\\n', 'type': 'human', 'id': '1756e875-b059-4068-9132-d6e6fddda43c'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_u4w28a0E6oZQMYNKUNvrOgYn', 'function': {'arguments': '{\"query\":\"CLIPSeg model image resolution\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--34f1b0f0-b9a4-49c9-ad07-76f2cd7bdec6-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'CLIPSeg model image resolution'}, 'id': 'call_u4w28a0E6oZQMYNKUNvrOgYn', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 21, 'total_tokens': 383, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm\\\\_source=hf&utm\\\\_medium=blog&utm\\\\_campaign=clipseg). Before we describe how to do that, let‚Äôs first take a look at how CLIPSeg works. ## CLIP: the magic model behind CLIPSeg\\n[CLIP](https://huggingface.co/docs/transformers/main/en/model\\\\_doc/clip), which stands for \\\\*\\\\*C\\\\*\\\\*ontrastive \\\\*\\\\*L\\\\*\\\\*anguage‚Äì\\\\*\\\\*I\\\\*\\\\*mage \\\\*\\\\*P\\\\*\\\\*re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract \\\\*representation\\\\* of your input. This abstract representation, also called an \\\\*embedding\\\\*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together).\\n\\n!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# CLIPSeg\\n## Overview\\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo L√ºddecke\\nand Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero- and one-shot image segmentation.\\n\\nNow you see why CLIP was truly a breakthrough in machine learning. The reason why CLIP works so well is that the model was trained on a huge dataset of images with text captions. The dataset contained a whopping 400 million image-text pairs taken from the internet. These images contain a wide variety of objects and concepts, and CLIP is great at creating a representation for each of them. ## CLIPSeg: image segmentation with CLIP\\n[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model that uses CLIP representations to create image segmentation masks. It was published by Timo L√ºddecke and Alexander Ecker. They achieved zero-shot image segmentation by training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn‚Äôt only use the final CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of CLIP. [Source](https://arxiv.org/abs/2112.10003)\\n\\nThe decoder is trained on the [PhraseCut dataset](https://arxiv.org/abs/2008.01187), which contains over 340,000 phrases with corresponding image segmentation masks.\\n\\nTaken from the [original paper.](https://arxiv.org/abs/2112.10003) \\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/timojl/clipseg). ## Usage tips\\n- [`CLIPSegForImageSegmentation`] adds a decoder on top of [`CLIPSegModel`]. The latter is identical to [`CLIPModel`]. - [`CLIPSegForImageSegmentation`] can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text\\n(provided to the model as `input\\\\_ids`) or an image (provided to the model as `conditional\\\\_pixel\\\\_values`). One can also provide custom\\nconditional embeddings (provided to the model as `conditional\\\\_embeddings`). ## Resources\\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with CLIPSeg. If you\\'re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\\'ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. - A notebook that illustrates [zero-shot image segmentation with CLIPSeg](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero\\\\_shot\\\\_image\\\\_segmentation\\\\_with\\\\_CLIPSeg.ipynb). ## CLIPSegConfig\\n[[autodoc]] CLIPSegConfig\\n- from\\\\_text\\\\_vision\\\\_configs\\n## CLIPSegTextConfig\\n[[autodoc]] CLIPSegTextConfig\\n## CLIPSegVisionConfig\\n[[autodoc]] CLIPSegVisionConfig\\n## CLIPSegProcessor\\n[[autodoc]] CLIPSegProcessor\\n## CLIPSegModel\\n[[autodoc]] CLIPSegModel\\n- forward\\n- get\\\\_text\\\\_features\\n- get\\\\_image\\\\_features\\n## CLIPSegTextModel\\n[[autodoc]] CLIPSegTextModel\\n- forward\\n## CLIPSegVisionModel\\n[[autodoc]] CLIPSegVisionModel\\n- forward\\n## CLIPSegForImageSegmentation\\n[[autodoc]] CLIPSegForImageSegmentation\\n- forward', 'type': 'tool', 'name': 'data_retriever', 'id': '9196b701-bc64-4818-85cc-7ebe3c152ffe', 'tool_call_id': 'call_u4w28a0E6oZQMYNKUNvrOgYn', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab0393e684812d15f6503', 'page_content': 'For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm\\\\_source=hf&utm\\\\_medium=blog&utm\\\\_campaign=clipseg). Before we describe how to do that, let‚Äôs first take a look at how CLIPSeg works. ## CLIP: the magic model behind CLIPSeg\\n[CLIP](https://huggingface.co/docs/transformers/main/en/model\\\\_doc/clip), which stands for \\\\*\\\\*C\\\\*\\\\*ontrastive \\\\*\\\\*L\\\\*\\\\*anguage‚Äì\\\\*\\\\*I\\\\*\\\\*mage \\\\*\\\\*P\\\\*\\\\*re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract \\\\*representation\\\\* of your input. This abstract representation, also called an \\\\*embedding\\\\*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689abb503e684812d15f71d1', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# CLIPSeg\\n## Overview\\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo L√ºddecke\\nand Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero- and one-shot image segmentation.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab0393e684812d15f6505', 'page_content': 'Now you see why CLIP was truly a breakthrough in machine learning. The reason why CLIP works so well is that the model was trained on a huge dataset of images with text captions. The dataset contained a whopping 400 million image-text pairs taken from the internet. These images contain a wide variety of objects and concepts, and CLIP is great at creating a representation for each of them. ## CLIPSeg: image segmentation with CLIP\\n[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model that uses CLIP representations to create image segmentation masks. It was published by Timo L√ºddecke and Alexander Ecker. They achieved zero-shot image segmentation by training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn‚Äôt only use the final CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of CLIP. [Source](https://arxiv.org/abs/2112.10003)\\n\\nThe decoder is trained on the [PhraseCut dataset](https://arxiv.org/abs/2008.01187), which contains over 340,000 phrases with corresponding image segmentation masks.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689abb503e684812d15f71d3', 'page_content': \"Taken from the [original paper.](https://arxiv.org/abs/2112.10003) \\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/timojl/clipseg). ## Usage tips\\n- [`CLIPSegForImageSegmentation`] adds a decoder on top of [`CLIPSegModel`]. The latter is identical to [`CLIPModel`]. - [`CLIPSegForImageSegmentation`] can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text\\n(provided to the model as `input\\\\_ids`) or an image (provided to the model as `conditional\\\\_pixel\\\\_values`). One can also provide custom\\nconditional embeddings (provided to the model as `conditional\\\\_embeddings`). ## Resources\\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with CLIPSeg. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. - A notebook that illustrates [zero-shot image segmentation with CLIPSeg](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero\\\\_shot\\\\_image\\\\_segmentation\\\\_with\\\\_CLIPSeg.ipynb). ## CLIPSegConfig\\n[[autodoc]] CLIPSegConfig\\n- from\\\\_text\\\\_vision\\\\_configs\\n## CLIPSegTextConfig\\n[[autodoc]] CLIPSegTextConfig\\n## CLIPSegVisionConfig\\n[[autodoc]] CLIPSegVisionConfig\\n## CLIPSegProcessor\\n[[autodoc]] CLIPSegProcessor\\n## CLIPSegModel\\n[[autodoc]] CLIPSegModel\\n- forward\\n- get\\\\_text\\\\_features\\n- get\\\\_image\\\\_features\\n## CLIPSegTextModel\\n[[autodoc]] CLIPSegTextModel\\n- forward\\n## CLIPSegVisionModel\\n[[autodoc]] CLIPSegVisionModel\\n- forward\\n## CLIPSegForImageSegmentation\\n[[autodoc]] CLIPSegForImageSegmentation\\n- forward\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The CLIPSeg model uses images of 352 x 352 pixels, which results in relatively low-resolution outputs.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--1c0c4e38-562b-4693-9f1e-30f9a30a1f54-0', 'usage_metadata': {'input_tokens': 1686, 'output_tokens': 23, 'total_tokens': 1709, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:27:27,180 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 37%|‚ñà‚ñà‚ñà‚ñã      | 24/65 [01:40<02:50,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What can you use Gradio for?\\n', 'type': 'human', 'id': '5724e3e4-ee0f-404e-9541-9732014ef778'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_mlu23oOrE6FyHrRDkwyGJ2eP', 'function': {'arguments': '{\"query\":\"Gradio use cases\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--cf17545e-3dd3-4f3b-9d31-3f37a1bf1d83-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Gradio use cases'}, 'id': 'call_mlu23oOrE6FyHrRDkwyGJ2eP', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 356, 'output_tokens': 19, 'total_tokens': 375, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': \"### What's Next? Keep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features). Or, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/). ## Questions? If you'd like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help. If you like Gradio, please leave us a ‚≠ê on GitHub! ## Open Source Stack\\nGradio is built on top of many wonderful open-source libraries! [![huggingface](readme_files/huggingface_mini.svg)](https://huggingface.co)\\n[![python](readme_files/python.svg)](https://www.python.org)\\n[![fastapi](readme_files/fastapi.svg)](https://fastapi.tiangolo.com)\\n[![encode](readme_files/encode.svg)](https://www.encode.io)\\n[![svelte](readme_files/svelte.svg)](https://svelte.dev)\\n[![vite](readme_files/vite.svg)](https://vitejs.dev)\\n[![pnpm](readme_files/pnpm.svg)](https://pnpm.io)\\n[![tailwind](readme_files/tailwind.svg)](https://tailwindcss.com)\\n[![storybook](readme_files/storybook.svg)](https://storybook.js.org/)\\n[![chromatic](readme_files/chromatic.svg)](https://www.chromatic.com/)\\n## License\\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository. ## Citation\\nAlso check out the paper \\\\_[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019\\\\_, and please cite it if you use Gradio in your work.\\n\\nIt uses Gradio's Audio and Label components.\\n\\n## What's Next? Keep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features). Or, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).\\n\\nhe simplest possible Gradio demo.\", 'type': 'tool', 'name': 'data_retriever', 'id': '08e0e4e0-43df-4513-b144-5280321b329d', 'tool_call_id': 'call_mlu23oOrE6FyHrRDkwyGJ2eP', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c2d5666af48523f52208', 'page_content': \"### What's Next? Keep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features). Or, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/). ## Questions? If you'd like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help. If you like Gradio, please leave us a ‚≠ê on GitHub! ## Open Source Stack\\nGradio is built on top of many wonderful open-source libraries! [![huggingface](readme_files/huggingface_mini.svg)](https://huggingface.co)\\n[![python](readme_files/python.svg)](https://www.python.org)\\n[![fastapi](readme_files/fastapi.svg)](https://fastapi.tiangolo.com)\\n[![encode](readme_files/encode.svg)](https://www.encode.io)\\n[![svelte](readme_files/svelte.svg)](https://svelte.dev)\\n[![vite](readme_files/vite.svg)](https://vitejs.dev)\\n[![pnpm](readme_files/pnpm.svg)](https://pnpm.io)\\n[![tailwind](readme_files/tailwind.svg)](https://tailwindcss.com)\\n[![storybook](readme_files/storybook.svg)](https://storybook.js.org/)\\n[![chromatic](readme_files/chromatic.svg)](https://www.chromatic.com/)\\n## License\\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository. ## Citation\\nAlso check out the paper \\\\_[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019\\\\_, and please cite it if you use Gradio in your work.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab0283e684812d15f64eb', 'page_content': \"It uses Gradio's Audio and Label components.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab8cc3e684812d15f6f1e', 'page_content': \"## What's Next? Keep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features). Or, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aab363e684812d15f5f3f', 'page_content': 'he simplest possible Gradio demo.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'Gradio can be used for creating user interfaces for machine learning models, allowing users to easily interact with and test these models. It provides a simple way to share and demonstrate models in a web-based format, making it easier for developers and researchers to showcase their work. Gradio supports various input and output components, such as text, images, audio, and more, enabling a wide range of applications in machine learning and data science.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--497b72ae-c29e-4e59-855a-e47f60fa97df-0', 'usage_metadata': {'input_tokens': 860, 'output_tokens': 86, 'total_tokens': 946, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:27:30,183 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 25/65 [01:43<02:32,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What TensorFlow API function is used to load a saved tensor file?\\n', 'type': 'human', 'id': 'ba358c36-8fc3-41de-bdd9-eeeecab88a4c'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_p3XiNKNMwH2GceZVL8ksILHC', 'function': {'arguments': '{\"query\":\"TensorFlow API function to load a saved tensor file\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--ff355a99-2cc9-4e9b-aad5-9eb218dc5701-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'TensorFlow API function to load a saved tensor file'}, 'id': 'call_p3XiNKNMwH2GceZVL8ksILHC', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 25, 'total_tokens': 387, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Tensorflow API\\n[[autodoc]] safetensors.tensorflow.load\\\\_file\\n[[autodoc]] safetensors.tensorflow.load\\n[[autodoc]] safetensors.tensorflow.save\\\\_file\\n[[autodoc]] safetensors.tensorflow.save\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Load safetensors\\n[[open-in-colab]]\\n[safetensors](https://github.com/huggingface/safetensors) is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or \\\\*pickled\\\\* into a `.bin` file with Python\\'s [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights. This guide will show you how you load `.safetensor` files, and how to convert Stable Diffusion model weights stored in other formats to `.safetensor`. Before you start, make sure you have safetensors installed:\\n```py\\n# uncomment to install the necessary libraries in Colab\\n#!pip install safetensors\\n```\\nIf you look at the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main) repository, you\\'ll see weights inside the `text\\\\_encoder`, `unet` and `vae` subfolders are stored in the `.safetensors` format. By default, ü§ó Diffusers automatically loads these `.safetensors` files from their subfolders if they\\'re available in the model repository. For more explicit control, you can optionally set `use\\\\_safetensors=True` (if `safetensors` is not installed, you\\'ll get an error message asking you to install it):\\n```py\\nfrom diffusers import DiffusionPipeline\\npipeline = DiffusionPipeline.from\\\\_pretrained(\"runwayml/stable-diffusion-v1-5\", use\\\\_safetensors=True)\\n```\\nHowever, model weights are not necessarily stored in separate subfolders like in the example above. Sometimes, all the weights are stored in a single `.safetensors` file. In this case, if the weights are Stable Diffusion weights, you can load the file directly with the [`~diffusers.loaders.FromSingleFileMixin.from\\\\_single\\\\_file`] method:\\n```py\\nfrom diffusers import StableDiffusionPipeline\\npipeline = StableDiffusionPipeline.from\\\\_single\\\\_file(\\n\"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\"\\n)\\n```\\n## Convert to safetensors\\nNot all weights on the Hub are available in the `.safetensors` format, and you may encounter weights stored as `.bin`.\\n\\nTorch shared tensors\\n## TL;DR\\nUsing specific functions, which should work in most cases for you. This is not without side effects. ```python\\nfrom safetensors.torch import load\\\\_model, save\\\\_model\\nsave\\\\_model(model, \"model.safetensors\")\\n# Instead of save\\\\_file(model.state\\\\_dict(), \"model.safetensors\")\\nload\\\\_model(model, \"model.safetensors\")\\n# Instead of model.load\\\\_state\\\\_dict(load\\\\_file(\"model.safetensors\"))\\n```\\n## What are shared tensors ? Pytorch uses shared tensors for some computation. This is extremely interesting to reduce memory usage in general. One very classic use case is in transformers the `embeddings` are shared with\\n`lm\\\\_head`. By using the same matrix, the model uses less parameters, and gradients\\nflow much better to the `embeddings` (which is the start of the model, so they don\\'t\\nflow easily there, whereas `lm\\\\_head` is at the tail of the model, so gradients are\\nextremely good over there, since they are the same tensors, they both benefit)\\n```python\\nfrom torch import nn\\nclass Model(nn.Module):\\ndef \\\\_\\\\_init\\\\_\\\\_(self):\\nsuper().\\\\_\\\\_init\\\\_\\\\_()\\nself.a = nn.Linear(100, 100)\\nself.b = self.a\\ndef forward(self, x):\\nreturn self.b(self.a(x))\\nmodel = Model()\\nprint(model.state\\\\_dict())\\n# odict\\\\_keys([\\'a.weight\\', \\'a.bias\\', \\'b.weight\\', \\'b.bias\\'])\\ntorch.save(model.state\\\\_dict(), \"model.bin\")\\n# This file is now 41k instead of ~80k, because A and B are the same weight hence only 1 is saved on disk with both `a` and `b` pointing to the same buffer\\n```\\n## Why are shared tensors not saved in `safetensors` ?\\n\\nHowever, many collators also support returning these values as TensorFlow tensors if we specify `return\\\\_tensor=\"tf\"`. This was crucial for our data pipeline to be compatible with TPU training. Thankfully, TensorFlow provides seamless support for reading files from a GCS bucket:\\n```python\\ntraining\\\\_records = tf.io.gfile.glob(os.path.join(args.train\\\\_dataset, \"\\\\*.tfrecord\"))\\n```\\nIf `args.dataset` contains the `gs://` identifier, TensorFlow will understand that it needs to look into a GCS bucket. Loading locally is as easy as removing the `gs://` identifier. For the rest of the data pipeline-related code, you can refer to [this section](https://github.com/huggingface/transformers/blob/474bf508dfe0d46fc38585a1bb793e5ba74fddfd/examples/tensorflow/language-modeling-tpu/run\\\\_mlm.py#L186-#L201) in the training script. Once the datasets have been prepared, the model and the optimizer have been initialized, and the model has been compiled, we can do the community‚Äôs favorite - `model.fit()`. For training, we didn‚Äôt do extensive hyperparameter tuning. We just trained it for longer with a learning rate of 1e-4. We also leveraged the [`PushToHubCallback`](https://huggingface.co/docs/transformers/main\\\\_classes/keras\\\\_callbacks#transformers.PushToHubCallback) for model checkpointing and syncing them with the Hub. You can find the hyperparameter details and a trained model here: [https://huggingface.co/tf-tpu/roberta-base-epochs-500-no-wd](https://huggingface.co/tf-tpu/roberta-base-epochs-500-no-wd). Once the model is trained, running inference with it is as easy as:\\n```python\\nfrom transformers import pipeline\\nmodel\\\\_id = \"tf-tpu/roberta-base-epochs-500-no-wd\"\\nunmasker = pipeline(\"fill-mask\", model=model\\\\_id, framework=\"tf\")\\nunmasker(\"Goal of my life is to [MASK].\")\\n[{\\'score\\': 0.1003185287117958,\\n\\'token\\': 52,\\n\\'token\\\\_str\\': \\'be\\',\\n\\'sequence\\': \\'Goal of my life is to be.\\'},\\n{\\'score\\': 0.032648514956235886,\\n\\'token\\': 5,\\n\\'token\\\\_str\\': \\'\\',\\n\\'sequence\\': \\'Goal of my life is to .\\'},\\n{\\'score\\': 0.02152673341333866,\\n\\'token\\': 138,\\n\\'token\\\\_str\\': \\'work\\',\\n\\'sequence\\': \\'Goal of my life is to work.\\'},\\n{\\'score\\': 0.019547373056411743,\\n\\'token\\': 984,\\n\\'token\\\\_str\\': \\'act\\',\\n\\'sequence\\': \\'Goal of my life is to act.\\'},\\n{\\'score\\': 0.01939118467271328,\\n\\'token\\': 73,\\n\\'token\\\\_str\\': \\'have\\',\\n\\'sequence\\': \\'Goal of my life is to have.\\'}]\\n```\\n## Conclusion\\nIf there‚Äôs one thing we want to emphasize with this example, it‚Äôs that TPU training is \\\\*\\\\*powerful, scalable and easy.\\\\*\\\\* In fact, if you‚Äôre already using Transformers models with TF/Keras and streaming data from `tf.data`, you might be shocked at how little work it takes to move your whole training pipeline to TPU.', 'type': 'tool', 'name': 'data_retriever', 'id': '214b6743-834e-4b61-999e-c10ccfbba3f1', 'tool_call_id': 'call_p3XiNKNMwH2GceZVL8ksILHC', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab5fc3e684812d15f6bea', 'page_content': 'Tensorflow API\\n[[autodoc]] safetensors.tensorflow.load\\\\_file\\n[[autodoc]] safetensors.tensorflow.load\\n[[autodoc]] safetensors.tensorflow.save\\\\_file\\n[[autodoc]] safetensors.tensorflow.save', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c21b666af48523f52102', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Load safetensors\\n[[open-in-colab]]\\n[safetensors](https://github.com/huggingface/safetensors) is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or \\\\*pickled\\\\* into a `.bin` file with Python\\'s [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights. This guide will show you how you load `.safetensor` files, and how to convert Stable Diffusion model weights stored in other formats to `.safetensor`. Before you start, make sure you have safetensors installed:\\n```py\\n# uncomment to install the necessary libraries in Colab\\n#!pip install safetensors\\n```\\nIf you look at the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main) repository, you\\'ll see weights inside the `text\\\\_encoder`, `unet` and `vae` subfolders are stored in the `.safetensors` format. By default, ü§ó Diffusers automatically loads these `.safetensors` files from their subfolders if they\\'re available in the model repository. For more explicit control, you can optionally set `use\\\\_safetensors=True` (if `safetensors` is not installed, you\\'ll get an error message asking you to install it):\\n```py\\nfrom diffusers import DiffusionPipeline\\npipeline = DiffusionPipeline.from\\\\_pretrained(\"runwayml/stable-diffusion-v1-5\", use\\\\_safetensors=True)\\n```\\nHowever, model weights are not necessarily stored in separate subfolders like in the example above. Sometimes, all the weights are stored in a single `.safetensors` file. In this case, if the weights are Stable Diffusion weights, you can load the file directly with the [`~diffusers.loaders.FromSingleFileMixin.from\\\\_single\\\\_file`] method:\\n```py\\nfrom diffusers import StableDiffusionPipeline\\npipeline = StableDiffusionPipeline.from\\\\_single\\\\_file(\\n\"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\"\\n)\\n```\\n## Convert to safetensors\\nNot all weights on the Hub are available in the `.safetensors` format, and you may encounter weights stored as `.bin`.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aba063e684812d15f707c', 'page_content': 'Torch shared tensors\\n## TL;DR\\nUsing specific functions, which should work in most cases for you. This is not without side effects. ```python\\nfrom safetensors.torch import load\\\\_model, save\\\\_model\\nsave\\\\_model(model, \"model.safetensors\")\\n# Instead of save\\\\_file(model.state\\\\_dict(), \"model.safetensors\")\\nload\\\\_model(model, \"model.safetensors\")\\n# Instead of model.load\\\\_state\\\\_dict(load\\\\_file(\"model.safetensors\"))\\n```\\n## What are shared tensors ? Pytorch uses shared tensors for some computation. This is extremely interesting to reduce memory usage in general. One very classic use case is in transformers the `embeddings` are shared with\\n`lm\\\\_head`. By using the same matrix, the model uses less parameters, and gradients\\nflow much better to the `embeddings` (which is the start of the model, so they don\\'t\\nflow easily there, whereas `lm\\\\_head` is at the tail of the model, so gradients are\\nextremely good over there, since they are the same tensors, they both benefit)\\n```python\\nfrom torch import nn\\nclass Model(nn.Module):\\ndef \\\\_\\\\_init\\\\_\\\\_(self):\\nsuper().\\\\_\\\\_init\\\\_\\\\_()\\nself.a = nn.Linear(100, 100)\\nself.b = self.a\\ndef forward(self, x):\\nreturn self.b(self.a(x))\\nmodel = Model()\\nprint(model.state\\\\_dict())\\n# odict\\\\_keys([\\'a.weight\\', \\'a.bias\\', \\'b.weight\\', \\'b.bias\\'])\\ntorch.save(model.state\\\\_dict(), \"model.bin\")\\n# This file is now 41k instead of ~80k, because A and B are the same weight hence only 1 is saved on disk with both `a` and `b` pointing to the same buffer\\n```\\n## Why are shared tensors not saved in `safetensors` ?', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b544af3d2ebb270a028f', 'page_content': 'However, many collators also support returning these values as TensorFlow tensors if we specify `return\\\\_tensor=\"tf\"`. This was crucial for our data pipeline to be compatible with TPU training. Thankfully, TensorFlow provides seamless support for reading files from a GCS bucket:\\n```python\\ntraining\\\\_records = tf.io.gfile.glob(os.path.join(args.train\\\\_dataset, \"\\\\*.tfrecord\"))\\n```\\nIf `args.dataset` contains the `gs://` identifier, TensorFlow will understand that it needs to look into a GCS bucket. Loading locally is as easy as removing the `gs://` identifier. For the rest of the data pipeline-related code, you can refer to [this section](https://github.com/huggingface/transformers/blob/474bf508dfe0d46fc38585a1bb793e5ba74fddfd/examples/tensorflow/language-modeling-tpu/run\\\\_mlm.py#L186-#L201) in the training script. Once the datasets have been prepared, the model and the optimizer have been initialized, and the model has been compiled, we can do the community‚Äôs favorite - `model.fit()`. For training, we didn‚Äôt do extensive hyperparameter tuning. We just trained it for longer with a learning rate of 1e-4. We also leveraged the [`PushToHubCallback`](https://huggingface.co/docs/transformers/main\\\\_classes/keras\\\\_callbacks#transformers.PushToHubCallback) for model checkpointing and syncing them with the Hub. You can find the hyperparameter details and a trained model here: [https://huggingface.co/tf-tpu/roberta-base-epochs-500-no-wd](https://huggingface.co/tf-tpu/roberta-base-epochs-500-no-wd). Once the model is trained, running inference with it is as easy as:\\n```python\\nfrom transformers import pipeline\\nmodel\\\\_id = \"tf-tpu/roberta-base-epochs-500-no-wd\"\\nunmasker = pipeline(\"fill-mask\", model=model\\\\_id, framework=\"tf\")\\nunmasker(\"Goal of my life is to [MASK].\")\\n[{\\'score\\': 0.1003185287117958,\\n\\'token\\': 52,\\n\\'token\\\\_str\\': \\'be\\',\\n\\'sequence\\': \\'Goal of my life is to be.\\'},\\n{\\'score\\': 0.032648514956235886,\\n\\'token\\': 5,\\n\\'token\\\\_str\\': \\'\\',\\n\\'sequence\\': \\'Goal of my life is to .\\'},\\n{\\'score\\': 0.02152673341333866,\\n\\'token\\': 138,\\n\\'token\\\\_str\\': \\'work\\',\\n\\'sequence\\': \\'Goal of my life is to work.\\'},\\n{\\'score\\': 0.019547373056411743,\\n\\'token\\': 984,\\n\\'token\\\\_str\\': \\'act\\',\\n\\'sequence\\': \\'Goal of my life is to act.\\'},\\n{\\'score\\': 0.01939118467271328,\\n\\'token\\': 73,\\n\\'token\\\\_str\\': \\'have\\',\\n\\'sequence\\': \\'Goal of my life is to have.\\'}]\\n```\\n## Conclusion\\nIf there‚Äôs one thing we want to emphasize with this example, it‚Äôs that TPU training is \\\\*\\\\*powerful, scalable and easy.\\\\*\\\\* In fact, if you‚Äôre already using Transformers models with TF/Keras and streaming data from `tf.data`, you might be shocked at how little work it takes to move your whole training pipeline to TPU.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--bee6f105-773c-40dd-b2a5-5b640560eba5-0', 'usage_metadata': {'input_tokens': 2155, 'output_tokens': 22, 'total_tokens': 2177, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:27:35,145 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 26/65 [01:48<02:42,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Where can you access the logs of your Endpoints in Hugging Face Endpoints?\\n', 'type': 'human', 'id': '0d169d02-6d35-4e76-8265-27810094bb03'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_JkopRoxySNkarhzwfVJfmWdj', 'function': {'arguments': '{\"query\":\"Hugging Face Endpoints logs access\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--b7573f4e-5cad-43e8-87f2-882f405b9763-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Hugging Face Endpoints logs access'}, 'id': 'call_JkopRoxySNkarhzwfVJfmWdj', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 23, 'total_tokens': 388, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Access and view Metrics\\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the ‚ÄúAnalytics‚Äù tab of your Endpoints. ![metric dashboard](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png)\\n## Access Metrics via API\\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api\\\\_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.\\n\\nThose endpoints are not officially supported in `huggingface\\\\_hub` or `huggingface.js` yet but [this code snippet](https://github.com/huggingface/huggingface\\\\_hub/issues/1535#issuecomment-1614693412) (in Python) might help you getting started. NEW! There\\'s an [open PR](https://github.com/huggingface/huggingface\\\\_hub/pull/1905) in `huggingface\\\\_hub` to add official support from our Python library. ### Download access report\\nYou can download a report of all access requests for a gated model with the \\\\*\\\\*download user access report\\\\*\\\\* button. Click on it to download a json file with a list of users. For each entry, you have:\\n- \\\\*\\\\*user\\\\*\\\\*: the user id. Example: \\\\*julien-c\\\\*. - \\\\*\\\\*fullname\\\\*\\\\*: name of the user on the Hub. Example: \\\\*Julien Chaumond\\\\*. - \\\\*\\\\*status\\\\*\\\\*: status of the request. Either `\"pending\"`, `\"accepted\"` or `\"rejected\"`. - \\\\*\\\\*email\\\\*\\\\*: email of the user. - \\\\*\\\\*time\\\\*\\\\*: datetime when the user initially made the request. ### Customize requested information\\nBy default, users landing on your gated model will be asked to share their contact information (email and username) by clicking the \\\\*\\\\*Agree and send request to access repo\\\\*\\\\* button.\\n\\n- \\\\*\\\\*Protected\\\\*\\\\*: A Protected Endpoint is available from the internet, secured with TLS/SSL, and requires a valid Hugging Face token for authentication. - \\\\*\\\\*Private\\\\*\\\\* A Private Endpoint is only available through an intra-region secured AWS or Azure PrivateLink connection. Private Endpoints are not accessible from the internet. Public and Protected Endpoints do not require any additional configuration. For Private Endpoints, you need to provide the AWS account ID of the account which also should have access to ü§ó Inference Endpoints. ![endpoint types](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/endpoint_types.png)\\n\\nAccess and read Logs\\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the ‚ÄúLogs‚Äù tab of your Endpoint. You will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference. ![select logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png)\\nThe Container Logs are only available when your Endpoint is in the ‚ÄúRunning‚Äù state. \\\\_Note: If your Endpoint creation is in the ‚ÄúFailed‚Äù state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc.\\\\_\\n\\\\*\\\\*Build Logs:\\\\*\\\\*\\n![build logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png)\\n\\\\*\\\\*Container Logs:\\\\*\\\\*\\n![container logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png)', 'type': 'tool', 'name': 'data_retriever', 'id': '27242a4c-5733-440d-9628-6a619761c95c', 'tool_call_id': 'call_JkopRoxySNkarhzwfVJfmWdj', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b643af3d2ebb270a0394', 'page_content': 'Access and view Metrics\\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the ‚ÄúAnalytics‚Äù tab of your Endpoints. ![metric dashboard](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png)\\n## Access Metrics via API\\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api\\\\_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab5b13e684812d15f6b8d', 'page_content': 'Those endpoints are not officially supported in `huggingface\\\\_hub` or `huggingface.js` yet but [this code snippet](https://github.com/huggingface/huggingface\\\\_hub/issues/1535#issuecomment-1614693412) (in Python) might help you getting started. NEW! There\\'s an [open PR](https://github.com/huggingface/huggingface\\\\_hub/pull/1905) in `huggingface\\\\_hub` to add official support from our Python library. ### Download access report\\nYou can download a report of all access requests for a gated model with the \\\\*\\\\*download user access report\\\\*\\\\* button. Click on it to download a json file with a list of users. For each entry, you have:\\n- \\\\*\\\\*user\\\\*\\\\*: the user id. Example: \\\\*julien-c\\\\*. - \\\\*\\\\*fullname\\\\*\\\\*: name of the user on the Hub. Example: \\\\*Julien Chaumond\\\\*. - \\\\*\\\\*status\\\\*\\\\*: status of the request. Either `\"pending\"`, `\"accepted\"` or `\"rejected\"`. - \\\\*\\\\*email\\\\*\\\\*: email of the user. - \\\\*\\\\*time\\\\*\\\\*: datetime when the user initially made the request. ### Customize requested information\\nBy default, users landing on your gated model will be asked to share their contact information (email and username) by clicking the \\\\*\\\\*Agree and send request to access repo\\\\*\\\\* button.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c2d8666af48523f5220c', 'page_content': '- \\\\*\\\\*Protected\\\\*\\\\*: A Protected Endpoint is available from the internet, secured with TLS/SSL, and requires a valid Hugging Face token for authentication. - \\\\*\\\\*Private\\\\*\\\\* A Private Endpoint is only available through an intra-region secured AWS or Azure PrivateLink connection. Private Endpoints are not accessible from the internet. Public and Protected Endpoints do not require any additional configuration. For Private Endpoints, you need to provide the AWS account ID of the account which also should have access to ü§ó Inference Endpoints. ![endpoint types](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/endpoint_types.png)', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b190af3d2ebb2709fdbc', 'page_content': 'Access and read Logs\\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the ‚ÄúLogs‚Äù tab of your Endpoint. You will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference. ![select logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png)\\nThe Container Logs are only available when your Endpoint is in the ‚ÄúRunning‚Äù state. \\\\_Note: If your Endpoint creation is in the ‚ÄúFailed‚Äù state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc.\\\\_\\n\\\\*\\\\*Build Logs:\\\\*\\\\*\\n![build logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png)\\n\\\\*\\\\*Container Logs:\\\\*\\\\*\\n![container logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png)', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'You can access the logs of your Endpoints through the UI in the ‚ÄúLogs‚Äù tab of your Endpoint in Hugging Face Endpoints. You will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference. The Container Logs are only available when your Endpoint is in the ‚ÄúRunning‚Äù state. If your Endpoint creation is in the ‚ÄúFailed‚Äù state, you can check the Build Logs to see what the reason was, such as a wrong version of a dependency.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--3166824d-ce91-4638-a1bf-2ea5f312442d-0', 'usage_metadata': {'input_tokens': 1068, 'output_tokens': 102, 'total_tokens': 1170, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:27:38,740 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 27/65 [01:52<02:31,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the latest task added to Hugging Face AutoTrain for Computer Vision?\\n', 'type': 'human', 'id': 'df4c353a-ef7f-4099-94cb-7a450f54f35c'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_UnMtC4it92JuxoXqQvoRrqP9', 'function': {'arguments': '{\"query\":\"latest task added to Hugging Face AutoTrain for Computer Vision\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--09ef9f39-5687-4c81-a366-e68540a405ae-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'latest task added to Hugging Face AutoTrain for Computer Vision'}, 'id': 'call_UnMtC4it92JuxoXqQvoRrqP9', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 364, 'output_tokens': 27, 'total_tokens': 391, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '--\\ntitle: Image Classification with AutoTrain\\nthumbnail: /blog/assets/105\\\\_autotrain-image-classification/thumbnail.png\\nauthors:\\n- user: nimaboscarino\\n---\\n# Image Classification with AutoTrain\\nSo you‚Äôve heard all about the cool things that are happening in the machine learning world, and you want to join in. There‚Äôs just one problem ‚Äì you don‚Äôt know how to code! üò± Or maybe you‚Äôre a seasoned software engineer who wants to add some ML to your side-project, but you don‚Äôt have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. That‚Äôs why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature we‚Äôve just added, we‚Äôre making ‚Äúno-code‚Äù machine learning better than ever. Best of all, you can create your first project for ‚ú® free! ‚ú®\\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with \\\\*\\\\*zero\\\\*\\\\* configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there\\'s even a good chance that you\\'ll end up with a model that performs better than a model that\\'s been hand-trained by an engineer ü§Ø We‚Äôve been expanding the number of tasks that we support, and we‚Äôre proud to announce that \\\\*\\\\*you can now use AutoTrain for Computer Vision\\\\*\\\\*!\\n\\nState of the art is constantly advancing, and hardly anyone can keep up with its relentless pace. Remember that Google Vision Transformer model I mentioned earlier? Would you like to test it here and now? With Hugging Face, it\\'s [the simplest thing](https://huggingface.co/google/vit-base-patch16-224). `![](assets/31_age_of_ml_as_code/05_vision_transformer.png)`\\nHow about the latest [zero-shot text generation models](https://huggingface.co/bigscience) from the [Big Science project](https://bigscience.huggingface.co/)? `![](assets/31_age_of_ml_as_code/06_big_science.png)`\\nYou can do the same with another [16,000+ models](https://huggingface.co/models) and [1,600+ datasets](https://huggingface.co/datasets), with additional tools for [inference](https://huggingface.co/inference-api), [AutoNLP](https://huggingface.co/autonlp), [latency optimization](https://huggingface.co/infinity), and [hardware acceleration](https://huggingface.co/hardware). We can also help you get your project off the ground, [from modeling to production](https://huggingface.co/support). Our mission at Hugging Face is to make Machine Learning as friendly and as productive as possible, for beginners and experts alike. We believe in writing as little code as possible to train, optimize, and deploy models. We believe in built-in best practices. We believe in making infrastructure as transparent as possible. We believe that nothing beats high quality models in production, fast. ### Machine Learning as Code, right here, right now!\\n\\nTo know more about Diffusers and the different use cases, check out the [official documentation](https://huggingface.co/docs/diffusers). The literature on Diffusion-based models is developing at a rapid pace which is why we partnered with [Jonathan Whitaker](https://github.com/johnowhitaker) to develop a course on it. The course is free, and you can check it out [here](https://github.com/huggingface/diffusion-models-class). ## Support for third-party libraries\\nCentral to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from ü§ó Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models. When you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/cv_state/task_widget_generation.png)\\n\\n## Spaces for computer vision demos\\nWith [Spaces](https://huggingface.co/docs/hub/spaces-overview), one can easily demonstrate their Machine Learning models. Spaces support direct integrations with [Gradio](https://gradio.app/), [Streamlit](https://streamlit.io/), and [Docker](https://www.docker.com/) empowering practitioners to have a great amount of flexibility while showcasing their models. You can bring in your own Machine Learning framework to build a demo with  Spaces. The Gradio library provides several components for building Computer Vision applications on  Spaces such as [Video](https://gradio.app/docs/#video), [Gallery](https://gradio.app/docs/#gallery), and [Model3D](https://gradio.app/docs/#model3d). The community has been hard at work building some amazing Computer Vision applications that are powered by Spaces:\\n- [Generate 3D voxels from a predicted depth map of an input image](https://huggingface.co/spaces/radames/dpt-depth-estimation-3d-voxels)\\n- [Open vocabulary semantic segmentation](https://huggingface.co/spaces/facebook/ov-seg)\\n- [Narrate videos by generating captions](https://huggingface.co/spaces/nateraw/lavila)\\n- [Classify videos from YouTube](https://huggingface.co/spaces/fcakyon/video-classification)\\n- [Zero-shot video classification](https://huggingface.co/spaces/fcakyon/zero-shot-video-classification)\\n- [Visual question-answering](https://huggingface.co/spaces/nielsr/vilt-vqa)\\n- [Use zero-shot image classification to find best captions for an image to generate similar images](https://huggingface.co/spaces/pharma/CLIP-Interrogator)\\n## ü§ó AutoTrain\\n[AutoTrain](https://huggingface.co/autotrain) provides a ‚Äúno-code‚Äù solution to train state-of-the-art Machine Learning models for tasks like text classification, text summarization, named entity recognition, and more.\\n\\nFor Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage. AutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate ü§ó Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details. ## The technical philosophy\\nIn this section, we wanted to share our philosophy behind adding support for Computer Vision in ü§ó Transformers so that the community is aware of the design choices specific to this area. Even though Transformers started with NLP, we support multiple modalities today, for example ‚Äì vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:\\n- Easy model download with a single line of code with `from\\\\_pretrained()`\\n- Easy model upload with `push\\\\_to\\\\_hub()`\\n- Support for loading huge checkpoints with efficient checkpoint sharding techniques\\n- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))\\n- Initialization from model configurations\\n- Support for both PyTorch and TensorFlow (non-exhaustive)\\n- and many more\\nUnlike tokenizers, we have preprocessors (such as [this](https://huggingface.co/docs/transformers/model\\\\_doc/vit#transformers.ViTImageProcessor)) that take care of preparing data for the vision models. We have worked hard to ensure the user experience of using a vision model still feels easy and similar:\\n```py\\nfrom transformers import ViTImageProcessor, ViTForImageClassification\\nimport torch\\nfrom datasets import load\\\\_dataset\\ndataset = load\\\\_dataset(\"huggingface/cats-image\")\\nimage = dataset[\"test\"][\"image\"][0]\\nimage\\\\_processor  = ViTImageProcessor.from\\\\_pretrained(\"google/vit-base-patch16-224\")\\nmodel = ViTForImageClassification.from\\\\_pretrained(\"google/vit-base-patch16-224\")\\ninputs = image\\\\_processor(image, return\\\\_tensors=\"pt\")\\nwith torch.no\\\\_grad():\\nlogits = model(\\\\*\\\\*inputs).logits\\n# model predicts one of the 1000 ImageNet classes\\npredicted\\\\_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted\\\\_label])\\n# Egyptian cat\\n```\\nEven for a difficult task like object detection, the user experience doesn‚Äôt change very much:\\n```py\\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage\\\\_processor = AutoImageProcessor.from\\\\_pretrained(\"microsoft/conditional-detr-resnet-50\")\\nmodel = AutoModelForObjectDetection.from\\\\_pretrained(\"microsoft/conditional-detr-resnet-50\")\\ninputs = image\\\\_processor(images=image, return\\\\_tensors=\"pt\")\\noutputs = model(\\\\*\\\\*inputs)\\n# convert outputs (bounding boxes and class logits) to COCO API\\ntarget\\\\_sizes = torch.tensor([image.size[::-1]])\\nresults = image\\\\_processor.post\\\\_process\\\\_object\\\\_detection(\\noutputs, threshold=0.5, target\\\\_sizes=target\\\\_sizes\\n)[0]\\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\\nbox = [round(i, 2) for i in box.tolist()]\\nprint(\\nf\"Detected {model.config.id2label[label.item()]} with confidence \"\\nf\"{round(score.item(), 3)} at location {box}\"\\n)\\n```\\nLeads to:\\n```bash\\nDetected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\\nDetected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\\nDetected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\\nDetected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\\nDetected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\\n```\\n## Zero-shot models for vision\\nThere‚Äôs been a surge of models that reformulate core vision tasks like segmentation and detection in interesting ways and introduce even more flexibility.', 'type': 'tool', 'name': 'data_retriever', 'id': '056fbff0-f23b-465e-b141-9d19915a4206', 'tool_call_id': 'call_UnMtC4it92JuxoXqQvoRrqP9', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab93f3e684812d15f6f88', 'page_content': \"--\\ntitle: Image Classification with AutoTrain\\nthumbnail: /blog/assets/105\\\\_autotrain-image-classification/thumbnail.png\\nauthors:\\n- user: nimaboscarino\\n---\\n# Image Classification with AutoTrain\\nSo you‚Äôve heard all about the cool things that are happening in the machine learning world, and you want to join in. There‚Äôs just one problem ‚Äì you don‚Äôt know how to code! üò± Or maybe you‚Äôre a seasoned software engineer who wants to add some ML to your side-project, but you don‚Äôt have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. That‚Äôs why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature we‚Äôve just added, we‚Äôre making ‚Äúno-code‚Äù machine learning better than ever. Best of all, you can create your first project for ‚ú® free! ‚ú®\\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with \\\\*\\\\*zero\\\\*\\\\* configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer ü§Ø We‚Äôve been expanding the number of tasks that we support, and we‚Äôre proud to announce that \\\\*\\\\*you can now use AutoTrain for Computer Vision\\\\*\\\\*!\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aafad3e684812d15f6474', 'page_content': \"State of the art is constantly advancing, and hardly anyone can keep up with its relentless pace. Remember that Google Vision Transformer model I mentioned earlier? Would you like to test it here and now? With Hugging Face, it's [the simplest thing](https://huggingface.co/google/vit-base-patch16-224). `![](assets/31_age_of_ml_as_code/05_vision_transformer.png)`\\nHow about the latest [zero-shot text generation models](https://huggingface.co/bigscience) from the [Big Science project](https://bigscience.huggingface.co/)? `![](assets/31_age_of_ml_as_code/06_big_science.png)`\\nYou can do the same with another [16,000+ models](https://huggingface.co/models) and [1,600+ datasets](https://huggingface.co/datasets), with additional tools for [inference](https://huggingface.co/inference-api), [AutoNLP](https://huggingface.co/autonlp), [latency optimization](https://huggingface.co/infinity), and [hardware acceleration](https://huggingface.co/hardware). We can also help you get your project off the ground, [from modeling to production](https://huggingface.co/support). Our mission at Hugging Face is to make Machine Learning as friendly and as productive as possible, for beginners and experts alike. We believe in writing as little code as possible to train, optimize, and deploy models. We believe in built-in best practices. We believe in making infrastructure as transparent as possible. We believe that nothing beats high quality models in production, fast. ### Machine Learning as Code, right here, right now!\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bf06666af48523f51d62', 'page_content': 'To know more about Diffusers and the different use cases, check out the [official documentation](https://huggingface.co/docs/diffusers). The literature on Diffusion-based models is developing at a rapid pace which is why we partnered with [Jonathan Whitaker](https://github.com/johnowhitaker) to develop a course on it. The course is free, and you can check it out [here](https://github.com/huggingface/diffusion-models-class). ## Support for third-party libraries\\nCentral to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from ü§ó Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models. When you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/cv_state/task_widget_generation.png)\\n\\n## Spaces for computer vision demos\\nWith [Spaces](https://huggingface.co/docs/hub/spaces-overview), one can easily demonstrate their Machine Learning models. Spaces support direct integrations with [Gradio](https://gradio.app/), [Streamlit](https://streamlit.io/), and [Docker](https://www.docker.com/) empowering practitioners to have a great amount of flexibility while showcasing their models. You can bring in your own Machine Learning framework to build a demo with  Spaces. The Gradio library provides several components for building Computer Vision applications on  Spaces such as [Video](https://gradio.app/docs/#video), [Gallery](https://gradio.app/docs/#gallery), and [Model3D](https://gradio.app/docs/#model3d). The community has been hard at work building some amazing Computer Vision applications that are powered by Spaces:\\n- [Generate 3D voxels from a predicted depth map of an input image](https://huggingface.co/spaces/radames/dpt-depth-estimation-3d-voxels)\\n- [Open vocabulary semantic segmentation](https://huggingface.co/spaces/facebook/ov-seg)\\n- [Narrate videos by generating captions](https://huggingface.co/spaces/nateraw/lavila)\\n- [Classify videos from YouTube](https://huggingface.co/spaces/fcakyon/video-classification)\\n- [Zero-shot video classification](https://huggingface.co/spaces/fcakyon/zero-shot-video-classification)\\n- [Visual question-answering](https://huggingface.co/spaces/nielsr/vilt-vqa)\\n- [Use zero-shot image classification to find best captions for an image to generate similar images](https://huggingface.co/spaces/pharma/CLIP-Interrogator)\\n## ü§ó AutoTrain\\n[AutoTrain](https://huggingface.co/autotrain) provides a ‚Äúno-code‚Äù solution to train state-of-the-art Machine Learning models for tasks like text classification, text summarization, named entity recognition, and more.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bf06666af48523f51d63', 'page_content': 'For Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage. AutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate ü§ó Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details. ## The technical philosophy\\nIn this section, we wanted to share our philosophy behind adding support for Computer Vision in ü§ó Transformers so that the community is aware of the design choices specific to this area. Even though Transformers started with NLP, we support multiple modalities today, for example ‚Äì vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:\\n- Easy model download with a single line of code with `from\\\\_pretrained()`\\n- Easy model upload with `push\\\\_to\\\\_hub()`\\n- Support for loading huge checkpoints with efficient checkpoint sharding techniques\\n- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))\\n- Initialization from model configurations\\n- Support for both PyTorch and TensorFlow (non-exhaustive)\\n- and many more\\nUnlike tokenizers, we have preprocessors (such as [this](https://huggingface.co/docs/transformers/model\\\\_doc/vit#transformers.ViTImageProcessor)) that take care of preparing data for the vision models. We have worked hard to ensure the user experience of using a vision model still feels easy and similar:\\n```py\\nfrom transformers import ViTImageProcessor, ViTForImageClassification\\nimport torch\\nfrom datasets import load\\\\_dataset\\ndataset = load\\\\_dataset(\"huggingface/cats-image\")\\nimage = dataset[\"test\"][\"image\"][0]\\nimage\\\\_processor  = ViTImageProcessor.from\\\\_pretrained(\"google/vit-base-patch16-224\")\\nmodel = ViTForImageClassification.from\\\\_pretrained(\"google/vit-base-patch16-224\")\\ninputs = image\\\\_processor(image, return\\\\_tensors=\"pt\")\\nwith torch.no\\\\_grad():\\nlogits = model(\\\\*\\\\*inputs).logits\\n# model predicts one of the 1000 ImageNet classes\\npredicted\\\\_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted\\\\_label])\\n# Egyptian cat\\n```\\nEven for a difficult task like object detection, the user experience doesn‚Äôt change very much:\\n```py\\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage\\\\_processor = AutoImageProcessor.from\\\\_pretrained(\"microsoft/conditional-detr-resnet-50\")\\nmodel = AutoModelForObjectDetection.from\\\\_pretrained(\"microsoft/conditional-detr-resnet-50\")\\ninputs = image\\\\_processor(images=image, return\\\\_tensors=\"pt\")\\noutputs = model(\\\\*\\\\*inputs)\\n# convert outputs (bounding boxes and class logits) to COCO API\\ntarget\\\\_sizes = torch.tensor([image.size[::-1]])\\nresults = image\\\\_processor.post\\\\_process\\\\_object\\\\_detection(\\noutputs, threshold=0.5, target\\\\_sizes=target\\\\_sizes\\n)[0]\\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\\nbox = [round(i, 2) for i in box.tolist()]\\nprint(\\nf\"Detected {model.config.id2label[label.item()]} with confidence \"\\nf\"{round(score.item(), 3)} at location {box}\"\\n)\\n```\\nLeads to:\\n```bash\\nDetected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\\nDetected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\\nDetected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\\nDetected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\\nDetected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\\n```\\n## Zero-shot models for vision\\nThere‚Äôs been a surge of models that reformulate core vision tasks like segmentation and detection in interesting ways and introduce even more flexibility.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The latest task added to Hugging Face AutoTrain for Computer Vision is **image classification**.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--5d946b02-627f-4eee-84c4-af2df999dcd0-0', 'usage_metadata': {'input_tokens': 2799, 'output_tokens': 19, 'total_tokens': 2818, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:27:42,129 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 28/65 [01:55<02:20,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the default repository type created by the `create_repo` function on Hugging Face Hub?\\n', 'type': 'human', 'id': 'ecdf059b-dd2d-4720-bed2-7704c16f1870'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_qAb0orq0kB9FbAtqHrdhtkdq', 'function': {'arguments': '{\"query\":\"default repository type created by create_repo function Hugging Face Hub\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--e279a58c-da1a-4598-971f-05ddc1a720fa-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'default repository type created by create_repo function Hugging Face Hub'}, 'id': 'call_qAb0orq0kB9FbAtqHrdhtkdq', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 368, 'output_tokens': 27, 'total_tokens': 395, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Firstly, there are a few methods to manage repository creation, deletion, and others:\\n```python no-format\\nfrom huggingface\\\\_hub import (\\n# User management\\nlogin,\\nlogout,\\nwhoami,\\n# Repository creation and management\\ncreate\\\\_repo,\\ndelete\\\\_repo,\\nupdate\\\\_repo\\\\_visibility,\\n# And some methods to retrieve/change information about the content\\nlist\\\\_models,\\nlist\\\\_datasets,\\nlist\\\\_metrics,\\nlist\\\\_repo\\\\_files,\\nupload\\\\_file,\\ndelete\\\\_file,\\n)\\n```\\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them. The `create\\\\_repo` method can be used to create a new repository on the hub:\\n```py\\nfrom huggingface\\\\_hub import create\\\\_repo\\ncreate\\\\_repo(\"dummy-model\")\\n```\\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\\n```py\\nfrom huggingface\\\\_hub import create\\\\_repo\\ncreate\\\\_repo(\"dummy-model\", organization=\"huggingface\")\\n```\\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization.\\n\\n!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Create and manage a repository\\nThe Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\\ndevelopment to easily version projects when working collaboratively. This guide will show you how to interact with the\\nrepositories on the Hub, especially:\\n- Create and delete a repository. - Manage branches and tags. - Rename your repository. - Update your repository visibility. - Manage a local copy of your repository. If you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct\\nmight be to use `git` CLI to clone your repo (`git clone`), commit changes (`git add, git commit`) and push them\\n(`git push`). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do\\nnot share the same requirements and workflows. Model repositories might maintain large model weight files for different\\nframeworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As\\na result, it may be more efficient to use our custom HTTP methods. You can read our [Git vs HTTP paradigm](../concepts/git\\\\_vs\\\\_http)\\nexplanation page for more details. If you want to create and manage a repository on the Hub, your machine must be logged in. If you are not, please refer to\\n[this section](../quick-start#authentication). In the rest of this guide, we will assume that your machine is logged in. ## Repo creation and deletion\\nThe first step is to know how to create and delete repositories. You can only manage repositories that you own (under\\nyour username namespace) or from organizations in which you have write permissions. ### Create a repository\\nCreate an empty repository with [`create\\\\_repo`] and give it a name with the `repo\\\\_id` parameter. The `repo\\\\_id` is your namespace followed by the repository name: `username\\\\_or\\\\_org/repo\\\\_name`. ```py\\n>>> from huggingface\\\\_hub import create\\\\_repo\\n>>> create\\\\_repo(\"lysandre/test-model\")\\n\\'https://huggingface.co/lysandre/test-model\\'\\n```\\nBy default, [`create\\\\_repo`] creates a model repository. But you can use the `repo\\\\_type` parameter to specify another repository type. For example, if you want to create a dataset repository:\\n```py\\n>>> from huggingface\\\\_hub import create\\\\_repo\\n>>> create\\\\_repo(\"lysandre/test-dataset\", repo\\\\_type=\"dataset\")\\n\\'https://huggingface.co/datasets/lysandre/test-dataset\\'\\n```\\nWhen you create a repository, you can set your repository visibility with the `private` parameter. ```py\\n>>> from huggingface\\\\_hub import create\\\\_repo\\n>>> create\\\\_repo(\"lysandre/test-private\", private=True)\\n```\\nIf you want to change the repository visibility at a later time, you can use the [`update\\\\_repo\\\\_visibility`] function.\\n\\nRepositories\\nModels, Spaces, and Datasets are hosted on the Hugging Face Hub as [Git repositories](https://git-scm.com/about), which means that version control and collaboration are core elements of the Hub. In a nutshell, a repository (also known as a \\\\*\\\\*repo\\\\*\\\\*) is a place where code and assets can be stored to back up your work, share it with the community, and work in a team. In these pages, you will go over the basics of getting started with Git and interacting with repositories on the Hub. Once you get the hang of it, you can explore the best practices and next steps that we\\'ve compiled for effective repository usage. ## Contents\\n- [Getting Started with Repositories](./repositories-getting-started)\\n- [Settings](./repositories-settings)\\n- [Pull Requests & Discussions](./repositories-pull-requests-discussions)\\n- [Pull Requests advanced usage](./repositories-pull-requests-discussions#pull-requests-advanced-usage)\\n- [Webhooks](./webhooks)\\n- [Notifications](./notifications)\\n- [Collections](./collections)\\n- [Repository size recommendations](./repositories-recommendations)\\n- [Next Steps](./repositories-next-steps)\\n- [Licenses](./repositories-licenses)\\n\\nHowever, in some cases you might be interested in having\\na local copy of your repository and interact with it using the Git commands you are familiar with. The [`Repository`] class allows you to interact with files and repositories on the Hub with functions similar to Git commands. It is a wrapper over Git and Git-LFS methods to use the Git commands you already know and love. Before starting, please make sure you have Git-LFS installed (see [here](https://git-lfs.github.com/) for installation instructions). [`Repository`] is deprecated in favor of the http-based alternatives implemented in [`HfApi`]. Given its large adoption in legacy code, the complete removal of [`Repository`] will only happen in release `v1.0`. For more details, please read [this explanation page](./concepts/git\\\\_vs\\\\_http). ### Use a local repository\\nInstantiate a [`Repository`] object with a path to a local repository:\\n```py\\n>>> from huggingface\\\\_hub import Repository\\n>>> repo = Repository(local\\\\_dir=\"//\")\\n```\\n### Clone\\nThe `clone\\\\_from` parameter clones a repository from a Hugging Face repository ID to a local directory specified by the `local\\\\_dir` argument:\\n```py\\n>>> from huggingface\\\\_hub import Repository\\n>>> repo = Repository(local\\\\_dir=\"w2v2\", clone\\\\_from=\"facebook/wav2vec2-large-960h-lv60\")\\n```\\n`clone\\\\_from` can also clone a repository using a URL:\\n```py\\n>>> repo = Repository(local\\\\_dir=\"huggingface-hub\", clone\\\\_from=\"https://huggingface.co/facebook/wav2vec2-large-960h-lv60\")\\n```\\nYou can combine the `clone\\\\_from` parameter with [`create\\\\_repo`] to create and clone a repository:\\n```py\\n>>> repo\\\\_url = create\\\\_repo(repo\\\\_id=\"repo\\\\_name\")\\n>>> repo = Repository(local\\\\_dir=\"repo\\\\_local\\\\_path\", clone\\\\_from=repo\\\\_url)\\n```\\nYou can also configure a Git username and email to a cloned repository by specifying the `git\\\\_user` and `git\\\\_email` parameters when you clone a repository.', 'type': 'tool', 'name': 'data_retriever', 'id': 'd3940291-2655-404d-a08b-1fd38d436eb9', 'tool_call_id': 'call_qAb0orq0kB9FbAtqHrdhtkdq', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b46caf3d2ebb270a01b3', 'page_content': 'Firstly, there are a few methods to manage repository creation, deletion, and others:\\n```python no-format\\nfrom huggingface\\\\_hub import (\\n# User management\\nlogin,\\nlogout,\\nwhoami,\\n# Repository creation and management\\ncreate\\\\_repo,\\ndelete\\\\_repo,\\nupdate\\\\_repo\\\\_visibility,\\n# And some methods to retrieve/change information about the content\\nlist\\\\_models,\\nlist\\\\_datasets,\\nlist\\\\_metrics,\\nlist\\\\_repo\\\\_files,\\nupload\\\\_file,\\ndelete\\\\_file,\\n)\\n```\\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them. The `create\\\\_repo` method can be used to create a new repository on the hub:\\n```py\\nfrom huggingface\\\\_hub import create\\\\_repo\\ncreate\\\\_repo(\"dummy-model\")\\n```\\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\\n```py\\nfrom huggingface\\\\_hub import create\\\\_repo\\ncreate\\\\_repo(\"dummy-model\", organization=\"huggingface\")\\n```\\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa8213e684812d15f5ba4', 'page_content': '!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Create and manage a repository\\nThe Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\\ndevelopment to easily version projects when working collaboratively. This guide will show you how to interact with the\\nrepositories on the Hub, especially:\\n- Create and delete a repository. - Manage branches and tags. - Rename your repository. - Update your repository visibility. - Manage a local copy of your repository. If you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct\\nmight be to use `git` CLI to clone your repo (`git clone`), commit changes (`git add, git commit`) and push them\\n(`git push`). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do\\nnot share the same requirements and workflows. Model repositories might maintain large model weight files for different\\nframeworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As\\na result, it may be more efficient to use our custom HTTP methods. You can read our [Git vs HTTP paradigm](../concepts/git\\\\_vs\\\\_http)\\nexplanation page for more details. If you want to create and manage a repository on the Hub, your machine must be logged in. If you are not, please refer to\\n[this section](../quick-start#authentication). In the rest of this guide, we will assume that your machine is logged in. ## Repo creation and deletion\\nThe first step is to know how to create and delete repositories. You can only manage repositories that you own (under\\nyour username namespace) or from organizations in which you have write permissions. ### Create a repository\\nCreate an empty repository with [`create\\\\_repo`] and give it a name with the `repo\\\\_id` parameter. The `repo\\\\_id` is your namespace followed by the repository name: `username\\\\_or\\\\_org/repo\\\\_name`. ```py\\n>>> from huggingface\\\\_hub import create\\\\_repo\\n>>> create\\\\_repo(\"lysandre/test-model\")\\n\\'https://huggingface.co/lysandre/test-model\\'\\n```\\nBy default, [`create\\\\_repo`] creates a model repository. But you can use the `repo\\\\_type` parameter to specify another repository type. For example, if you want to create a dataset repository:\\n```py\\n>>> from huggingface\\\\_hub import create\\\\_repo\\n>>> create\\\\_repo(\"lysandre/test-dataset\", repo\\\\_type=\"dataset\")\\n\\'https://huggingface.co/datasets/lysandre/test-dataset\\'\\n```\\nWhen you create a repository, you can set your repository visibility with the `private` parameter. ```py\\n>>> from huggingface\\\\_hub import create\\\\_repo\\n>>> create\\\\_repo(\"lysandre/test-private\", private=True)\\n```\\nIf you want to change the repository visibility at a later time, you can use the [`update\\\\_repo\\\\_visibility`] function.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab28c3e684812d15f67b8', 'page_content': \"Repositories\\nModels, Spaces, and Datasets are hosted on the Hugging Face Hub as [Git repositories](https://git-scm.com/about), which means that version control and collaboration are core elements of the Hub. In a nutshell, a repository (also known as a \\\\*\\\\*repo\\\\*\\\\*) is a place where code and assets can be stored to back up your work, share it with the community, and work in a team. In these pages, you will go over the basics of getting started with Git and interacting with repositories on the Hub. Once you get the hang of it, you can explore the best practices and next steps that we've compiled for effective repository usage. ## Contents\\n- [Getting Started with Repositories](./repositories-getting-started)\\n- [Settings](./repositories-settings)\\n- [Pull Requests & Discussions](./repositories-pull-requests-discussions)\\n- [Pull Requests advanced usage](./repositories-pull-requests-discussions#pull-requests-advanced-usage)\\n- [Webhooks](./webhooks)\\n- [Notifications](./notifications)\\n- [Collections](./collections)\\n- [Repository size recommendations](./repositories-recommendations)\\n- [Next Steps](./repositories-next-steps)\\n- [Licenses](./repositories-licenses)\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa8223e684812d15f5ba8', 'page_content': 'However, in some cases you might be interested in having\\na local copy of your repository and interact with it using the Git commands you are familiar with. The [`Repository`] class allows you to interact with files and repositories on the Hub with functions similar to Git commands. It is a wrapper over Git and Git-LFS methods to use the Git commands you already know and love. Before starting, please make sure you have Git-LFS installed (see [here](https://git-lfs.github.com/) for installation instructions). [`Repository`] is deprecated in favor of the http-based alternatives implemented in [`HfApi`]. Given its large adoption in legacy code, the complete removal of [`Repository`] will only happen in release `v1.0`. For more details, please read [this explanation page](./concepts/git\\\\_vs\\\\_http). ### Use a local repository\\nInstantiate a [`Repository`] object with a path to a local repository:\\n```py\\n>>> from huggingface\\\\_hub import Repository\\n>>> repo = Repository(local\\\\_dir=\"//\")\\n```\\n### Clone\\nThe `clone\\\\_from` parameter clones a repository from a Hugging Face repository ID to a local directory specified by the `local\\\\_dir` argument:\\n```py\\n>>> from huggingface\\\\_hub import Repository\\n>>> repo = Repository(local\\\\_dir=\"w2v2\", clone\\\\_from=\"facebook/wav2vec2-large-960h-lv60\")\\n```\\n`clone\\\\_from` can also clone a repository using a URL:\\n```py\\n>>> repo = Repository(local\\\\_dir=\"huggingface-hub\", clone\\\\_from=\"https://huggingface.co/facebook/wav2vec2-large-960h-lv60\")\\n```\\nYou can combine the `clone\\\\_from` parameter with [`create\\\\_repo`] to create and clone a repository:\\n```py\\n>>> repo\\\\_url = create\\\\_repo(repo\\\\_id=\"repo\\\\_name\")\\n>>> repo = Repository(local\\\\_dir=\"repo\\\\_local\\\\_path\", clone\\\\_from=repo\\\\_url)\\n```\\nYou can also configure a Git username and email to a cloned repository by specifying the `git\\\\_user` and `git\\\\_email` parameters when you clone a repository.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--4d9472c6-0301-42c8-9515-2886003c895a-0', 'usage_metadata': {'input_tokens': 1973, 'output_tokens': 22, 'total_tokens': 1995, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:27:44,994 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 29/65 [01:58<02:06,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'How many splits does the \"duorc\" dataset have?\\n', 'type': 'human', 'id': '860f3688-7ea0-4790-be2f-de168c2de4a6'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_5WPCOE2DQ3YbN0gVtGiR5EEL', 'function': {'arguments': '{\"query\":\"duorc dataset splits\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--55b8d572-8321-423f-8eac-c021fe689206-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'duorc dataset splits'}, 'id': 'call_5WPCOE2DQ3YbN0gVtGiR5EEL', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 360, 'output_tokens': 19, 'total_tokens': 379, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'List splits and configurations\\nDatasets typically have splits and may also have configurations. A \\\\_split\\\\_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A \\\\_configuration\\\\_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you\\'re interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load\\\\_hub)! This guide shows you how to use Datasets Server\\'s `/splits` endpoint to retrieve a dataset\\'s splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\\nThe `/splits` endpoint accepts the dataset name as its query parameter:\\n\\n```python\\nimport requests\\nheaders = {\"Authorization\": f\"Bearer {API\\\\_TOKEN}\"}\\nAPI\\\\_URL = \"https://datasets-server.huggingface.co/splits?dataset=duorc\"\\ndef query():\\nresponse = requests.get(API\\\\_URL, headers=headers)\\nreturn response.json()\\ndata = query()\\n```\\n\\n```js\\nimport fetch from \"node-fetch\";\\nasync function query(data) {\\nconst response = await fetch(\\n\"https://datasets-server.huggingface.co/splits?dataset=duorc\",\\n{\\nheaders: { Authorization: `Bearer ${API\\\\_TOKEN}` },\\nmethod: \"GET\"\\n}\\n);\\nconst result = await response.json();\\nreturn result;\\n}\\nquery().then((response) => {\\nconsole.log(JSON.stringify(response));\\n});\\n```\\n\\n```curl\\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\\\\n-X GET \\\\\\n-H \"Authorization: Bearer ${API\\\\_TOKEN}\"\\n```\\nThe endpoint response is a JSON containing a list of the dataset\\'s splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\\n```json\\n{\\n\"splits\": [\\n{ \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\\n{ \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"validation\" },\\n{ \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"test\" },\\n{ \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"train\" },\\n{ \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"validation\" },\\n{ \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"test\" }\\n],\\n\"pending\": [],\\n\"failed\": []\\n}\\n```\\n\\nIts structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package\\\\_reference/main\\\\_classes#datasets.DatasetInfo) object of the `datasets` library. ```json\\n{\\n\"dataset\\\\_info\": {\\n\"description\": \"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\\\n\",\\n\"citation\": \"@inproceedings{DuoRC,\\\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\\\nyear = {2018}\\\\n}\\\\n\",\\n\"homepage\": \"https://duorc.github.io/\",\\n\"license\": \"https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\",\\n\"features\": {\\n\"plot\\\\_id\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"plot\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"title\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"question\\\\_id\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"question\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"answers\": {\\n\"feature\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"\\\\_type\": \"Sequence\"\\n},\\n\"no\\\\_answer\": {\\n\"dtype\": \"bool\",\\n\"\\\\_type\": \"Value\"\\n}\\n},\\n\"builder\\\\_name\": \"duorc\",\\n\"config\\\\_name\": \"SelfRC\",\\n\"version\": {\\n\"version\\\\_str\": \"1.0.0\",\\n\"major\": 1,\\n\"minor\": 0,\\n\"patch\": 0\\n},\\n\"splits\": {\\n\"train\": {\\n\"name\": \"train\",\\n\"num\\\\_bytes\": 239852729,\\n\"num\\\\_examples\": 60721,\\n\"dataset\\\\_name\": \"duorc\"\\n},\\n\"validation\": {\\n\"name\": \"validation\",\\n\"num\\\\_bytes\": 51662519,\\n\"num\\\\_examples\": 12961,\\n\"dataset\\\\_name\": \"duorc\"\\n},\\n\"test\": {\\n\"name\": \"test\",\\n\"num\\\\_bytes\": 49142710,\\n\"num\\\\_examples\": 12559,\\n\"dataset\\\\_name\": \"duorc\"\\n}\\n},\\n\"download\\\\_checksums\": {\\n\"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\\\_train.json\": {\\n\"num\\\\_bytes\": 24388192,\\n\"checksum\": null\\n},\\n\"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\\\_dev.json\": {\\n\"num\\\\_bytes\": 5051240,\\n\"checksum\": null\\n},\\n\"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\\\_test.json\": {\\n\"num\\\\_bytes\": 5023228,\\n\"checksum\": null\\n}\\n},\\n\"download\\\\_size\": 34462660,\\n\"dataset\\\\_size\": 340657958,\\n\"size\\\\_in\\\\_bytes\": 375120618\\n}\\n}\\n```\\n\\nIt provides the number of rows, the number of colums (where applicable) and the size in bytes for the different forms of the data: original files, size in memory (RAM) and auto-converted parquet files. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has 187.213 rows along all its configurations and splits, for a total of 97MB. ```json\\n{\\n\"size\": {\\n\"dataset\": {\\n\"dataset\": \"duorc\",\\n\"num\\\\_bytes\\\\_original\\\\_files\": 97383710,\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 58710973,\\n\"num\\\\_bytes\\\\_memory\": 1059067116,\\n\"num\\\\_rows\": 187213\\n},\\n\"configs\": [\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"num\\\\_bytes\\\\_original\\\\_files\": 62921050,\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 37709127,\\n\"num\\\\_bytes\\\\_memory\": 718409158,\\n\"num\\\\_rows\": 100972,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"num\\\\_bytes\\\\_original\\\\_files\": 34462660,\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 21001846,\\n\"num\\\\_bytes\\\\_memory\": 340657958,\\n\"num\\\\_rows\": 86241,\\n\"num\\\\_columns\": 7\\n}\\n],\\n\"splits\": [\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"split\": \"train\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 26005668,\\n\"num\\\\_bytes\\\\_memory\": 496682909,\\n\"num\\\\_rows\": 69524,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"split\": \"validation\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 5566868,\\n\"num\\\\_bytes\\\\_memory\": 106510489,\\n\"num\\\\_rows\": 15591,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"split\": \"test\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 6136591,\\n\"num\\\\_bytes\\\\_memory\": 115215760,\\n\"num\\\\_rows\": 15857,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"split\": \"train\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 14851720,\\n\"num\\\\_bytes\\\\_memory\": 239852729,\\n\"num\\\\_rows\": 60721,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"split\": \"validation\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 3114390,\\n\"num\\\\_bytes\\\\_memory\": 51662519,\\n\"num\\\\_rows\": 12961,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"split\": \"test\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 3035736,\\n\"num\\\\_bytes\\\\_memory\": 49142710,\\n\"num\\\\_rows\": 12559,\\n\"num\\\\_columns\": 7\\n}\\n]\\n},\\n\"pending\": [],\\n\"failed\": [],\\n\"partial\": false\\n}\\n```\\nIf the size has `partial: true` it means that the actual size of the dataset couldn\\'t been determined because it\\'s too big. In that case the number of rows and bytes can be inferior to the actual numbers.\\n\\n## Offline\\nEven if you don\\'t have an internet connection, it is still possible to load a dataset. As long as you\\'ve downloaded a dataset from the Hub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline. If you know you won\\'t have internet access, you can run ü§ó Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, ü§ó Datasets will look directly in the cache. Set the environment variable `HF\\\\_DATASETS\\\\_OFFLINE` to `1` to enable full offline mode. ## Slice splits\\nYou can also choose only to load specific slices of a split. There are two options for slicing a split: using strings or the [`ReadInstruction`] API. Strings are more compact and readable for simple cases, while [`ReadInstruction`] is easier to use with variable slicing parameters. Concatenate a `train` and `test` split by:\\n```py\\n>>> train\\\\_test\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train+test\")\\n===STRINGAPI-READINSTRUCTION-SPLIT===\\n>>> ri = datasets.ReadInstruction(\"train\") + datasets.ReadInstruction(\"test\")\\n>>> train\\\\_test\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=ri)\\n```\\nSelect specific rows of the `train` split:\\n```py\\n>>> train\\\\_10\\\\_20\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train[10:20]\")\\n===STRINGAPI-READINSTRUCTION-SPLIT===\\n>>> train\\\\_10\\\\_20\\\\_ds = datasets.load\\\\_dataset(\"bookcorpu\", split=datasets.ReadInstruction(\"train\", from\\\\_=10, to=20, unit=\"abs\"))\\n```\\nOr select a percentage of a split with:\\n```py\\n>>> train\\\\_10pct\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train[:10%]\")\\n===STRINGAPI-READINSTRUCTION-SPLIT===\\n>>> train\\\\_10\\\\_20\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=datasets.ReadInstruction(\"train\", to=10, unit=\"%\"))\\n```\\nSelect a combination of percentages from each split:\\n```py\\n>>> train\\\\_10\\\\_80pct\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train[:10%]+train[-80%:]\")\\n===STRINGAPI-READINSTRUCTION-SPLIT===\\n>>> ri = (datasets.ReadInstruction(\"train\", to=10, unit=\"%\") + datasets.ReadInstruction(\"train\", from\\\\_=-80, unit=\"%\"))\\n>>> train\\\\_10\\\\_80pct\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=ri)\\n```\\nFinally, you can even create cross-validated splits. The example below creates 10-fold cross-validated splits. Each validation dataset is a 10% chunk, and the training dataset makes up the remaining complementary 90% chunk:\\n```py\\n>>> val\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=[f\"train[{k}%:{k+10}%]\" for k in range(0, 100, 10)])\\n>>> train\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=[f\"train[:{k}%]+train[{k+10}%:]\" for k in range(0, 100, 10)])\\n===STRINGAPI-READINSTRUCTION-SPLIT===\\n>>> val\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", [datasets.ReadInstruction(\"train\", from\\\\_=k, to=k+10, unit=\"%\") for k in range(0, 100, 10)])\\n>>> train\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", [(datasets.ReadInstruction(\"train\", to=k, unit=\"%\") + datasets.ReadInstruction(\"train\", from\\\\_=k+10, unit=\"%\")) for k in range(0, 100, 10)])\\n```\\n### Percent slicing and rounding\\nThe default behavior is to round the boundaries to the nearest integer for datasets where the requested slice boundaries do not divide evenly by 100. As shown below, some slices may contain more examples than others. For instance, if the following train split includes 999 records, then:\\n```py\\n# 19 records, from 500 (included) to 519 (excluded). >>> train\\\\_50\\\\_52\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train[50%:52%]\")\\n# 20 records, from 519 (included) to 539 (excluded). >>> train\\\\_52\\\\_54\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train[52%:54%]\")\\n```\\nIf you want equal sized splits, use `pct1\\\\_dropremainder` rounding instead. This treats the specified percentage boundaries as multiples of 1%. ```py\\n# 18 records, from 450 (included) to 468 (excluded). >>> train\\\\_50\\\\_52pct1\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=datasets.ReadInstruction(\"train\", from\\\\_=50, to=52, unit=\"%\", rounding=\"pct1\\\\_dropremainder\"))\\n# 18 records, from 468 (included) to 486 (excluded). >>> train\\\\_52\\\\_54pct1\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=datasets.ReadInstruction(\"train\",from\\\\_=52, to=54, unit=\"%\", rounding=\"pct1\\\\_dropremainder\"))\\n# Or equivalently:\\n>>> train\\\\_50\\\\_52pct1\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train[50%:52%](pct1\\\\_dropremainder)\")\\n>>> train\\\\_52\\\\_54pct1\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train[52%:54%](pct1\\\\_dropremainder)\")\\n```\\n`pct1\\\\_dropremainder` rounding may truncate the last examples in a dataset if the number of examples in your dataset don\\'t divide evenly by 100.', 'type': 'tool', 'name': 'data_retriever', 'id': '3442b0c0-da56-4dd9-a353-9f9717b85dca', 'tool_call_id': 'call_5WPCOE2DQ3YbN0gVtGiR5EEL', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab9bc3e684812d15f7024', 'page_content': 'List splits and configurations\\nDatasets typically have splits and may also have configurations. A \\\\_split\\\\_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A \\\\_configuration\\\\_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you\\'re interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load\\\\_hub)! This guide shows you how to use Datasets Server\\'s `/splits` endpoint to retrieve a dataset\\'s splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\\nThe `/splits` endpoint accepts the dataset name as its query parameter:\\n\\n```python\\nimport requests\\nheaders = {\"Authorization\": f\"Bearer {API\\\\_TOKEN}\"}\\nAPI\\\\_URL = \"https://datasets-server.huggingface.co/splits?dataset=duorc\"\\ndef query():\\nresponse = requests.get(API\\\\_URL, headers=headers)\\nreturn response.json()\\ndata = query()\\n```\\n\\n```js\\nimport fetch from \"node-fetch\";\\nasync function query(data) {\\nconst response = await fetch(\\n\"https://datasets-server.huggingface.co/splits?dataset=duorc\",\\n{\\nheaders: { Authorization: `Bearer ${API\\\\_TOKEN}` },\\nmethod: \"GET\"\\n}\\n);\\nconst result = await response.json();\\nreturn result;\\n}\\nquery().then((response) => {\\nconsole.log(JSON.stringify(response));\\n});\\n```\\n\\n```curl\\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\\\\n-X GET \\\\\\n-H \"Authorization: Bearer ${API\\\\_TOKEN}\"\\n```\\nThe endpoint response is a JSON containing a list of the dataset\\'s splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\\n```json\\n{\\n\"splits\": [\\n{ \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\\n{ \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"validation\" },\\n{ \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"test\" },\\n{ \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"train\" },\\n{ \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"validation\" },\\n{ \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"test\" }\\n],\\n\"pending\": [],\\n\"failed\": []\\n}\\n```', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bca7af3d2ebb270a0b44', 'page_content': 'Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package\\\\_reference/main\\\\_classes#datasets.DatasetInfo) object of the `datasets` library. ```json\\n{\\n\"dataset\\\\_info\": {\\n\"description\": \"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\\\n\",\\n\"citation\": \"@inproceedings{DuoRC,\\\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\\\nyear = {2018}\\\\n}\\\\n\",\\n\"homepage\": \"https://duorc.github.io/\",\\n\"license\": \"https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\",\\n\"features\": {\\n\"plot\\\\_id\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"plot\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"title\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"question\\\\_id\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"question\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"answers\": {\\n\"feature\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"\\\\_type\": \"Sequence\"\\n},\\n\"no\\\\_answer\": {\\n\"dtype\": \"bool\",\\n\"\\\\_type\": \"Value\"\\n}\\n},\\n\"builder\\\\_name\": \"duorc\",\\n\"config\\\\_name\": \"SelfRC\",\\n\"version\": {\\n\"version\\\\_str\": \"1.0.0\",\\n\"major\": 1,\\n\"minor\": 0,\\n\"patch\": 0\\n},\\n\"splits\": {\\n\"train\": {\\n\"name\": \"train\",\\n\"num\\\\_bytes\": 239852729,\\n\"num\\\\_examples\": 60721,\\n\"dataset\\\\_name\": \"duorc\"\\n},\\n\"validation\": {\\n\"name\": \"validation\",\\n\"num\\\\_bytes\": 51662519,\\n\"num\\\\_examples\": 12961,\\n\"dataset\\\\_name\": \"duorc\"\\n},\\n\"test\": {\\n\"name\": \"test\",\\n\"num\\\\_bytes\": 49142710,\\n\"num\\\\_examples\": 12559,\\n\"dataset\\\\_name\": \"duorc\"\\n}\\n},\\n\"download\\\\_checksums\": {\\n\"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\\\_train.json\": {\\n\"num\\\\_bytes\": 24388192,\\n\"checksum\": null\\n},\\n\"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\\\_dev.json\": {\\n\"num\\\\_bytes\": 5051240,\\n\"checksum\": null\\n},\\n\"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\\\_test.json\": {\\n\"num\\\\_bytes\": 5023228,\\n\"checksum\": null\\n}\\n},\\n\"download\\\\_size\": 34462660,\\n\"dataset\\\\_size\": 340657958,\\n\"size\\\\_in\\\\_bytes\": 375120618\\n}\\n}\\n```', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab34b3e684812d15f687c', 'page_content': 'It provides the number of rows, the number of colums (where applicable) and the size in bytes for the different forms of the data: original files, size in memory (RAM) and auto-converted parquet files. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has 187.213 rows along all its configurations and splits, for a total of 97MB. ```json\\n{\\n\"size\": {\\n\"dataset\": {\\n\"dataset\": \"duorc\",\\n\"num\\\\_bytes\\\\_original\\\\_files\": 97383710,\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 58710973,\\n\"num\\\\_bytes\\\\_memory\": 1059067116,\\n\"num\\\\_rows\": 187213\\n},\\n\"configs\": [\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"num\\\\_bytes\\\\_original\\\\_files\": 62921050,\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 37709127,\\n\"num\\\\_bytes\\\\_memory\": 718409158,\\n\"num\\\\_rows\": 100972,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"num\\\\_bytes\\\\_original\\\\_files\": 34462660,\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 21001846,\\n\"num\\\\_bytes\\\\_memory\": 340657958,\\n\"num\\\\_rows\": 86241,\\n\"num\\\\_columns\": 7\\n}\\n],\\n\"splits\": [\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"split\": \"train\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 26005668,\\n\"num\\\\_bytes\\\\_memory\": 496682909,\\n\"num\\\\_rows\": 69524,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"split\": \"validation\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 5566868,\\n\"num\\\\_bytes\\\\_memory\": 106510489,\\n\"num\\\\_rows\": 15591,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"split\": \"test\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 6136591,\\n\"num\\\\_bytes\\\\_memory\": 115215760,\\n\"num\\\\_rows\": 15857,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"split\": \"train\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 14851720,\\n\"num\\\\_bytes\\\\_memory\": 239852729,\\n\"num\\\\_rows\": 60721,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"split\": \"validation\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 3114390,\\n\"num\\\\_bytes\\\\_memory\": 51662519,\\n\"num\\\\_rows\": 12961,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"split\": \"test\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 3035736,\\n\"num\\\\_bytes\\\\_memory\": 49142710,\\n\"num\\\\_rows\": 12559,\\n\"num\\\\_columns\": 7\\n}\\n]\\n},\\n\"pending\": [],\\n\"failed\": [],\\n\"partial\": false\\n}\\n```\\nIf the size has `partial: true` it means that the actual size of the dataset couldn\\'t been determined because it\\'s too big. In that case the number of rows and bytes can be inferior to the actual numbers.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa8143e684812d15f5b97', 'page_content': '## Offline\\nEven if you don\\'t have an internet connection, it is still possible to load a dataset. As long as you\\'ve downloaded a dataset from the Hub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline. If you know you won\\'t have internet access, you can run ü§ó Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, ü§ó Datasets will look directly in the cache. Set the environment variable `HF\\\\_DATASETS\\\\_OFFLINE` to `1` to enable full offline mode. ## Slice splits\\nYou can also choose only to load specific slices of a split. There are two options for slicing a split: using strings or the [`ReadInstruction`] API. Strings are more compact and readable for simple cases, while [`ReadInstruction`] is easier to use with variable slicing parameters. Concatenate a `train` and `test` split by:\\n```py\\n>>> train\\\\_test\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train+test\")\\n===STRINGAPI-READINSTRUCTION-SPLIT===\\n>>> ri = datasets.ReadInstruction(\"train\") + datasets.ReadInstruction(\"test\")\\n>>> train\\\\_test\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=ri)\\n```\\nSelect specific rows of the `train` split:\\n```py\\n>>> train\\\\_10\\\\_20\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train[10:20]\")\\n===STRINGAPI-READINSTRUCTION-SPLIT===\\n>>> train\\\\_10\\\\_20\\\\_ds = datasets.load\\\\_dataset(\"bookcorpu\", split=datasets.ReadInstruction(\"train\", from\\\\_=10, to=20, unit=\"abs\"))\\n```\\nOr select a percentage of a split with:\\n```py\\n>>> train\\\\_10pct\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train[:10%]\")\\n===STRINGAPI-READINSTRUCTION-SPLIT===\\n>>> train\\\\_10\\\\_20\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=datasets.ReadInstruction(\"train\", to=10, unit=\"%\"))\\n```\\nSelect a combination of percentages from each split:\\n```py\\n>>> train\\\\_10\\\\_80pct\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train[:10%]+train[-80%:]\")\\n===STRINGAPI-READINSTRUCTION-SPLIT===\\n>>> ri = (datasets.ReadInstruction(\"train\", to=10, unit=\"%\") + datasets.ReadInstruction(\"train\", from\\\\_=-80, unit=\"%\"))\\n>>> train\\\\_10\\\\_80pct\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=ri)\\n```\\nFinally, you can even create cross-validated splits. The example below creates 10-fold cross-validated splits. Each validation dataset is a 10% chunk, and the training dataset makes up the remaining complementary 90% chunk:\\n```py\\n>>> val\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=[f\"train[{k}%:{k+10}%]\" for k in range(0, 100, 10)])\\n>>> train\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=[f\"train[:{k}%]+train[{k+10}%:]\" for k in range(0, 100, 10)])\\n===STRINGAPI-READINSTRUCTION-SPLIT===\\n>>> val\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", [datasets.ReadInstruction(\"train\", from\\\\_=k, to=k+10, unit=\"%\") for k in range(0, 100, 10)])\\n>>> train\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", [(datasets.ReadInstruction(\"train\", to=k, unit=\"%\") + datasets.ReadInstruction(\"train\", from\\\\_=k+10, unit=\"%\")) for k in range(0, 100, 10)])\\n```\\n### Percent slicing and rounding\\nThe default behavior is to round the boundaries to the nearest integer for datasets where the requested slice boundaries do not divide evenly by 100. As shown below, some slices may contain more examples than others. For instance, if the following train split includes 999 records, then:\\n```py\\n# 19 records, from 500 (included) to 519 (excluded). >>> train\\\\_50\\\\_52\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train[50%:52%]\")\\n# 20 records, from 519 (included) to 539 (excluded). >>> train\\\\_52\\\\_54\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train[52%:54%]\")\\n```\\nIf you want equal sized splits, use `pct1\\\\_dropremainder` rounding instead. This treats the specified percentage boundaries as multiples of 1%. ```py\\n# 18 records, from 450 (included) to 468 (excluded). >>> train\\\\_50\\\\_52pct1\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=datasets.ReadInstruction(\"train\", from\\\\_=50, to=52, unit=\"%\", rounding=\"pct1\\\\_dropremainder\"))\\n# 18 records, from 468 (included) to 486 (excluded). >>> train\\\\_52\\\\_54pct1\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=datasets.ReadInstruction(\"train\",from\\\\_=52, to=54, unit=\"%\", rounding=\"pct1\\\\_dropremainder\"))\\n# Or equivalently:\\n>>> train\\\\_50\\\\_52pct1\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train[50%:52%](pct1\\\\_dropremainder)\")\\n>>> train\\\\_52\\\\_54pct1\\\\_ds = datasets.load\\\\_dataset(\"bookcorpus\", split=\"train[52%:54%](pct1\\\\_dropremainder)\")\\n```\\n`pct1\\\\_dropremainder` rounding may truncate the last examples in a dataset if the number of examples in your dataset don\\'t divide evenly by 100.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The \"duorc\" dataset has six splits.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--1bc5603a-1146-473c-bafb-77b26f071dcd-0', 'usage_metadata': {'input_tokens': 3847, 'output_tokens': 10, 'total_tokens': 3857, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:27:49,915 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 30/65 [02:03<02:18,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\\n', 'type': 'human', 'id': '250dacc4-229a-4875-adc9-6e35ecd79b74'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_HMx1hojnTueaGU8HA2Qdd1I7', 'function': {'arguments': '{\"query\":\"Fully Sharded Data Parallel (FSDP) purpose in distributed training\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--bbe28b5a-baa1-4760-ab3e-1657627b1eba-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Fully Sharded Data Parallel (FSDP) purpose in distributed training'}, 'id': 'call_HMx1hojnTueaGU8HA2Qdd1I7', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 367, 'output_tokens': 29, 'total_tokens': 396, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Fully Sharded Data Parallel\\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model\\'s parameters, gradients and optimizer states across the number of available GPUs (also called workers or \\\\*rank\\\\*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class. Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer. ```bash\\npip install accelerate\\n```\\n## FSDP configuration\\nTo start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package\\\\_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`. ```bash\\naccelerate config\\n```\\nWhen you run `accelerate config`, you\\'ll be prompted with a series of options to configure your training environment.\\n\\nBy sharding the model parameters, optimizer and gradient states, and even offloading them to the CPU when they\\'re inactive, FSDP can reduce the high cost of large-scale training. If you\\'re interested in learning more, the following may be helpful:\\n\\\\* Follow along with the more in-depth Accelerate guide for [FSDP](https://huggingface.co/docs/accelerate/usage\\\\_guides/fsdp). \\\\* Read the [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) blog post. \\\\* Read the [Scaling PyTorch models on Cloud TPUs with FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/) blog post.\\n\\n--\\ntitle: \"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\"\\nthumbnail: /blog/assets/62\\\\_pytorch\\\\_fsdp/fsdp-thumbnail.png\\nauthors:\\n- user: smangrul\\n- user: sgugger\\n---\\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\\nIn this post we will look at how we can leverage \\\\*\\\\*[Accelerate](https://github.com/huggingface/accelerate)\\\\*\\\\* Library for training large models which enables users to leverage the latest features of \\\\*\\\\*[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)\\\\*\\\\*. # Motivation ü§ó\\n\\\\*\\\\*With the ever increasing scale, size and parameters of the Machine Learning (ML) models, ML practitioners are finding it difficult to train or even load such large models on their hardware.\\\\*\\\\* On one hand, it has been found that large models learn quickly (data and compute efficient) and are significantly more performant when compared to smaller models [1]; on the other hand, it becomes prohibitive to train such models on most of the available hardware.\\n\\n!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Fully Sharded Data Parallel\\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes. Currently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates. FSDP is supported in ü§ó Accelerate, and you can use it with ü§ó PEFT. This guide will help you learn how to use our FSDP [training script](https://github.com/huggingface/peft/blob/main/examples/conditional\\\\_generation/peft\\\\_lora\\\\_seq2seq\\\\_accelerate\\\\_fsdp.py). You\\'ll configure the script to train a large model for conditional generation. ## Configuration\\nBegin by running the following command to [create a FSDP configuration file](https://huggingface.co/docs/accelerate/main/en/usage\\\\_guides/fsdp) with ü§ó Accelerate. Use the `--config\\\\_file` flag to save the configuration file to a specific location, otherwise it is saved as a `default\\\\_config.yaml` file in the ü§ó Accelerate cache. The configuration file is used to set the default options when you launch the training script. ```bash\\naccelerate config --config\\\\_file fsdp\\\\_config.yaml\\n```\\nYou\\'ll be asked a few questions about your setup, and configure the following arguments. For this example, make sure you fully shard the model parameters, gradients, optimizer states, leverage the CPU for offloading, and wrap model layers based on the Transformer layer class name. ```bash\\n`Sharding Strategy`: [1] FULL\\\\_SHARD (shards optimizer states, gradients and parameters), [2] SHARD\\\\_GRAD\\\\_OP (shards optimizer states and gradients), [3] NO\\\\_SHARD\\n`Offload Params`: Decides Whether to offload parameters and gradients to CPU\\n`Auto Wrap Policy`: [1] TRANSFORMER\\\\_BASED\\\\_WRAP, [2] SIZE\\\\_BASED\\\\_WRAP, [3] NO\\\\_WRAP\\n`Transformer Layer Class to Wrap`: When using `TRANSFORMER\\\\_BASED\\\\_WRAP`, user specifies comma-separated string of transformer layer class names (case-sensitive) to wrap ,e.g,\\n`BertLayer`, `GPTJBlock`, `T5Block`, `BertLayer,BertEmbeddings,BertSelfOutput`... `Min Num Params`: minimum number of parameters when using `SIZE\\\\_BASED\\\\_WRAP`\\n`Backward Prefetch`: [1] BACKWARD\\\\_PRE, [2] BACKWARD\\\\_POST, [3] NO\\\\_PREFETCH\\n`State Dict Type`: [1] FULL\\\\_STATE\\\\_DICT, [2] LOCAL\\\\_STATE\\\\_DICT, [3] SHARDED\\\\_STATE\\\\_DICT\\n```\\nFor example, your FSDP configuration file may look like the following:\\n```yaml\\ncommand\\\\_file: null\\ncommands: null\\ncompute\\\\_environment: LOCAL\\\\_MACHINE\\ndeepspeed\\\\_config: {}\\ndistributed\\\\_type: FSDP\\ndowncast\\\\_bf16: \\'no\\'\\ndynamo\\\\_backend: \\'NO\\'\\nfsdp\\\\_config:\\nfsdp\\\\_auto\\\\_wrap\\\\_policy: TRANSFORMER\\\\_BASED\\\\_WRAP\\nfsdp\\\\_backward\\\\_prefetch\\\\_policy: BACKWARD\\\\_PRE\\nfsdp\\\\_offload\\\\_params: true\\nfsdp\\\\_sharding\\\\_strategy: 1\\nfsdp\\\\_state\\\\_dict\\\\_type: FULL\\\\_STATE\\\\_DICT\\nfsdp\\\\_transformer\\\\_layer\\\\_cls\\\\_to\\\\_wrap: T5Block\\ngpu\\\\_ids: null\\nmachine\\\\_rank: 0\\nmain\\\\_process\\\\_ip: null\\nmain\\\\_process\\\\_port: null\\nmain\\\\_training\\\\_function: main\\nmegatron\\\\_lm\\\\_config: {}\\nmixed\\\\_precision: \\'no\\'\\nnum\\\\_machines: 1\\nnum\\\\_processes: 2\\nrdzv\\\\_backend: static\\nsame\\\\_network: true\\ntpu\\\\_name: null\\ntpu\\\\_zone: null\\nuse\\\\_cpu: false\\n```\\n## The important parts\\nLet\\'s dig a bit deeper into the training script to understand how it works.', 'type': 'tool', 'name': 'data_retriever', 'id': '0c42f467-b211-49d5-9183-ef5d53176d54', 'tool_call_id': 'call_HMx1hojnTueaGU8HA2Qdd1I7', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab6403e684812d15f6c35', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Fully Sharded Data Parallel\\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model\\'s parameters, gradients and optimizer states across the number of available GPUs (also called workers or \\\\*rank\\\\*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class. Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer. ```bash\\npip install accelerate\\n```\\n## FSDP configuration\\nTo start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package\\\\_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`. ```bash\\naccelerate config\\n```\\nWhen you run `accelerate config`, you\\'ll be prompted with a series of options to configure your training environment.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab6403e684812d15f6c38', 'page_content': \"By sharding the model parameters, optimizer and gradient states, and even offloading them to the CPU when they're inactive, FSDP can reduce the high cost of large-scale training. If you're interested in learning more, the following may be helpful:\\n\\\\* Follow along with the more in-depth Accelerate guide for [FSDP](https://huggingface.co/docs/accelerate/usage\\\\_guides/fsdp). \\\\* Read the [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) blog post. \\\\* Read the [Scaling PyTorch models on Cloud TPUs with FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/) blog post.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab14a3e684812d15f6633', 'page_content': '--\\ntitle: \"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\"\\nthumbnail: /blog/assets/62\\\\_pytorch\\\\_fsdp/fsdp-thumbnail.png\\nauthors:\\n- user: smangrul\\n- user: sgugger\\n---\\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\\nIn this post we will look at how we can leverage \\\\*\\\\*[Accelerate](https://github.com/huggingface/accelerate)\\\\*\\\\* Library for training large models which enables users to leverage the latest features of \\\\*\\\\*[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)\\\\*\\\\*. # Motivation ü§ó\\n\\\\*\\\\*With the ever increasing scale, size and parameters of the Machine Learning (ML) models, ML practitioners are finding it difficult to train or even load such large models on their hardware.\\\\*\\\\* On one hand, it has been found that large models learn quickly (data and compute efficient) and are significantly more performant when compared to smaller models [1]; on the other hand, it becomes prohibitive to train such models on most of the available hardware.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa96c3e684812d15f5cee', 'page_content': \"!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Fully Sharded Data Parallel\\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes. Currently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates. FSDP is supported in ü§ó Accelerate, and you can use it with ü§ó PEFT. This guide will help you learn how to use our FSDP [training script](https://github.com/huggingface/peft/blob/main/examples/conditional\\\\_generation/peft\\\\_lora\\\\_seq2seq\\\\_accelerate\\\\_fsdp.py). You'll configure the script to train a large model for conditional generation. ## Configuration\\nBegin by running the following command to [create a FSDP configuration file](https://huggingface.co/docs/accelerate/main/en/usage\\\\_guides/fsdp) with ü§ó Accelerate. Use the `--config\\\\_file` flag to save the configuration file to a specific location, otherwise it is saved as a `default\\\\_config.yaml` file in the ü§ó Accelerate cache. The configuration file is used to set the default options when you launch the training script. ```bash\\naccelerate config --config\\\\_file fsdp\\\\_config.yaml\\n```\\nYou'll be asked a few questions about your setup, and configure the following arguments. For this example, make sure you fully shard the model parameters, gradients, optimizer states, leverage the CPU for offloading, and wrap model layers based on the Transformer layer class name. ```bash\\n`Sharding Strategy`: [1] FULL\\\\_SHARD (shards optimizer states, gradients and parameters), [2] SHARD\\\\_GRAD\\\\_OP (shards optimizer states and gradients), [3] NO\\\\_SHARD\\n`Offload Params`: Decides Whether to offload parameters and gradients to CPU\\n`Auto Wrap Policy`: [1] TRANSFORMER\\\\_BASED\\\\_WRAP, [2] SIZE\\\\_BASED\\\\_WRAP, [3] NO\\\\_WRAP\\n`Transformer Layer Class to Wrap`: When using `TRANSFORMER\\\\_BASED\\\\_WRAP`, user specifies comma-separated string of transformer layer class names (case-sensitive) to wrap ,e.g,\\n`BertLayer`, `GPTJBlock`, `T5Block`, `BertLayer,BertEmbeddings,BertSelfOutput`... `Min Num Params`: minimum number of parameters when using `SIZE\\\\_BASED\\\\_WRAP`\\n`Backward Prefetch`: [1] BACKWARD\\\\_PRE, [2] BACKWARD\\\\_POST, [3] NO\\\\_PREFETCH\\n`State Dict Type`: [1] FULL\\\\_STATE\\\\_DICT, [2] LOCAL\\\\_STATE\\\\_DICT, [3] SHARDED\\\\_STATE\\\\_DICT\\n```\\nFor example, your FSDP configuration file may look like the following:\\n```yaml\\ncommand\\\\_file: null\\ncommands: null\\ncompute\\\\_environment: LOCAL\\\\_MACHINE\\ndeepspeed\\\\_config: {}\\ndistributed\\\\_type: FSDP\\ndowncast\\\\_bf16: 'no'\\ndynamo\\\\_backend: 'NO'\\nfsdp\\\\_config:\\nfsdp\\\\_auto\\\\_wrap\\\\_policy: TRANSFORMER\\\\_BASED\\\\_WRAP\\nfsdp\\\\_backward\\\\_prefetch\\\\_policy: BACKWARD\\\\_PRE\\nfsdp\\\\_offload\\\\_params: true\\nfsdp\\\\_sharding\\\\_strategy: 1\\nfsdp\\\\_state\\\\_dict\\\\_type: FULL\\\\_STATE\\\\_DICT\\nfsdp\\\\_transformer\\\\_layer\\\\_cls\\\\_to\\\\_wrap: T5Block\\ngpu\\\\_ids: null\\nmachine\\\\_rank: 0\\nmain\\\\_process\\\\_ip: null\\nmain\\\\_process\\\\_port: null\\nmain\\\\_training\\\\_function: main\\nmegatron\\\\_lm\\\\_config: {}\\nmixed\\\\_precision: 'no'\\nnum\\\\_machines: 1\\nnum\\\\_processes: 2\\nrdzv\\\\_backend: static\\nsame\\\\_network: true\\ntpu\\\\_name: null\\ntpu\\\\_zone: null\\nuse\\\\_cpu: false\\n```\\n## The important parts\\nLet's dig a bit deeper into the training script to understand how it works.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"Fully Sharded Data Parallel (FSDP) is a data parallel method designed for distributed training of large pretrained models, allowing for efficient sharding of a model's parameters, gradients, and optimizer states across multiple GPUs. The primary purpose of FSDP is to reduce memory usage by sharding these components, which improves GPU memory efficiency and enables the training of much larger models on fewer GPUs. Additionally, FSDP can offload inactive model parameters to the CPU, further enhancing memory efficiency during large-scale training.\", 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f8853e3f-b632-43da-8c1a-c7a1376afdee-0', 'usage_metadata': {'input_tokens': 2262, 'output_tokens': 103, 'total_tokens': 2365, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:27:54,709 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 31/65 [02:08<02:22,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What file format is used to save and store PyTorch model weights more securely than `.bin` files?\\n', 'type': 'human', 'id': '528601cb-ec6a-4863-b45c-c7fa9bfd6cce'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_bU6Utbc6GD4QnxHbhwiMdmFf', 'function': {'arguments': '{\"query\":\"PyTorch model weights file format more secure than .bin\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--189d79c2-90a6-47dc-8222-7d53008b1ab3-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'PyTorch model weights file format more secure than .bin'}, 'id': 'call_bU6Utbc6GD4QnxHbhwiMdmFf', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 369, 'output_tokens': 26, 'total_tokens': 395, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Convert weights to safetensors\\nPyTorch model weights are commonly saved and stored as `.bin` files with Python\\'s [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`. The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository. For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights. Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Load safetensors\\n[[open-in-colab]]\\n[safetensors](https://github.com/huggingface/safetensors) is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or \\\\*pickled\\\\* into a `.bin` file with Python\\'s [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights. This guide will show you how you load `.safetensor` files, and how to convert Stable Diffusion model weights stored in other formats to `.safetensor`. Before you start, make sure you have safetensors installed:\\n```py\\n# uncomment to install the necessary libraries in Colab\\n#!pip install safetensors\\n```\\nIf you look at the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main) repository, you\\'ll see weights inside the `text\\\\_encoder`, `unet` and `vae` subfolders are stored in the `.safetensors` format. By default, ü§ó Diffusers automatically loads these `.safetensors` files from their subfolders if they\\'re available in the model repository. For more explicit control, you can optionally set `use\\\\_safetensors=True` (if `safetensors` is not installed, you\\'ll get an error message asking you to install it):\\n```py\\nfrom diffusers import DiffusionPipeline\\npipeline = DiffusionPipeline.from\\\\_pretrained(\"runwayml/stable-diffusion-v1-5\", use\\\\_safetensors=True)\\n```\\nHowever, model weights are not necessarily stored in separate subfolders like in the example above. Sometimes, all the weights are stored in a single `.safetensors` file. In this case, if the weights are Stable Diffusion weights, you can load the file directly with the [`~diffusers.loaders.FromSingleFileMixin.from\\\\_single\\\\_file`] method:\\n```py\\nfrom diffusers import StableDiffusionPipeline\\npipeline = StableDiffusionPipeline.from\\\\_single\\\\_file(\\n\"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\"\\n)\\n```\\n## Convert to safetensors\\nNot all weights on the Hub are available in the `.safetensors` format, and you may encounter weights stored as `.bin`.\\n\\nIn PyTorch, the\\nname of a layer is defined by the name of the class attribute you give the layer. Let\\'s define a dummy model in\\nPyTorch, called `SimpleModel` as follows:\\n```python\\nfrom torch import nn\\nclass SimpleModel(nn.Module):\\ndef \\\\_\\\\_init\\\\_\\\\_(self):\\nsuper().\\\\_\\\\_init\\\\_\\\\_()\\nself.dense = nn.Linear(10, 10)\\nself.intermediate = nn.Linear(10, 10)\\nself.layer\\\\_norm = nn.LayerNorm(10)\\n```\\nNow we can create an instance of this model definition which will fill all weights: `dense`, `intermediate`,\\n`layer\\\\_norm` with random weights. We can print the model to see its architecture\\n```python\\nmodel = SimpleModel()\\nprint(model)\\n```\\nThis will print out the following:\\n```\\nSimpleModel(\\n(dense): Linear(in\\\\_features=10, out\\\\_features=10, bias=True)\\n(intermediate): Linear(in\\\\_features=10, out\\\\_features=10, bias=True)\\n(layer\\\\_norm): LayerNorm((10,), eps=1e-05, elementwise\\\\_affine=True)\\n)\\n```\\nWe can see that the layer names are defined by the name of the class attribute in PyTorch. You can print out the weight\\nvalues of a specific layer:\\n```python\\nprint(model.dense.weight.data)\\n```\\nto see that the weights were randomly initialized\\n```\\ntensor([[-0.0818, 0.2207, -0.0749, -0.0030, 0.0045, -0.1569, -0.1598, 0.0212,\\n-0.2077, 0.2157],\\n[ 0.1044, 0.0201, 0.0990, 0.2482, 0.3116, 0.2509, 0.2866, -0.2190,\\n0.2166, -0.0212],\\n[-0.2000, 0.1107, -0.1999, -0.3119, 0.1559, 0.0993, 0.1776, -0.1950,\\n-0.1023, -0.0447],\\n[-0.0888, -0.1092, 0.2281, 0.0336, 0.1817, -0.0115, 0.2096, 0.1415,\\n-0.1876, -0.2467],\\n[ 0.2208, -0.2352, -0.1426, -0.2636, -0.2889, -0.2061, -0.2849, -0.0465,\\n0.2577, 0.0402],\\n[ 0.1502, 0.2465, 0.2566, 0.0693, 0.2352, -0.0530, 0.1859, -0.0604,\\n0.2132, 0.1680],\\n[ 0.1733, -0.2407, -0.1721, 0.1484, 0.0358, -0.0633, -0.0721, -0.0090,\\n0.2707, -0.2509],\\n[-0.1173, 0.1561, 0.2945, 0.0595, -0.1996, 0.2988, -0.0802, 0.0407,\\n0.1829, -0.1568],\\n[-0.1164, -0.2228, -0.0403, 0.0428, 0.1339, 0.0047, 0.1967, 0.2923,\\n0.0333, -0.0536],\\n[-0.1492, -0.1616, 0.1057, 0.1950, -0.2807, -0.2710, -0.1586, 0.0739,\\n0.2220, 0.2358]]). ```\\nIn the conversion script, you should fill those randomly initialized weights with the exact weights of the\\ncorresponding layer in the checkpoint. \\\\*E.g.\\\\*\\n```python\\n# retrieve matching layer weights, e.g. by\\n# recursive algorithm\\nlayer\\\\_name = \"dense\"\\npretrained\\\\_weight = array\\\\_of\\\\_dense\\\\_layer\\nmodel\\\\_pointer = getattr(model, \"dense\")\\nmodel\\\\_pointer.weight.data = torch.from\\\\_numpy(pretrained\\\\_weight)\\n```\\nWhile doing so, you must verify that each randomly initialized weight of your PyTorch model and its corresponding\\npretrained checkpoint weight exactly match in both \\\\*\\\\*shape and name\\\\*\\\\*. To do so, it is \\\\*\\\\*necessary\\\\*\\\\* to add assert\\nstatements for the shape and print out the names of the checkpoints weights. E.g. you should add statements like:\\n```python\\nassert (\\nmodel\\\\_pointer.weight.shape == pretrained\\\\_weight.shape\\n), f\"Pointer shape of random weight {model\\\\_pointer.shape} and array shape of checkpoint weight {pretrained\\\\_weight.shape} mismatched\"\\n```\\nBesides, you should also print out the names of both weights to make sure they match, \\\\*e.g.\\\\*\\n```python\\nlogger.info(f\"Initialize PyTorch weight {layer\\\\_name} from {pretrained\\\\_weight.name}\")\\n```\\nIf either the shape or the name doesn\\'t match, you probably assigned the wrong checkpoint weight to a randomly\\ninitialized layer of the ü§ó Transformers implementation. An incorrect shape is most likely due to an incorrect setting of the config parameters in `BrandNewBertConfig()` that\\ndo not exactly match those that were used for the checkpoint you want to convert.\\n\\nI\\'ve taken care to make sure any third party code included or adapted has compatible (permissive) licenses such as MIT, BSD, etc. I\\'ve made an effort to avoid any GPL / LGPL conflicts. That said, it is your responsibility to ensure you comply with licenses here and conditions of any dependent licenses. Where applicable, I\\'ve linked the sources/references for various components in docstrings. If you think I\\'ve missed anything please create an issue. ### Pretrained Weights\\nSo far all of the pretrained weights available here are pretrained on ImageNet with a select few that have some additional pretraining (see extra note below). ImageNet was released for non-commercial research purposes only (https://image-net.org/download). It\\'s not clear what the implications of that are for the use of pretrained weights from that dataset. Any models I have trained with ImageNet are done for research purposes and one should assume that the original dataset license applies to the weights. It\\'s best to seek legal advice if you intend to use the pretrained weights in a commercial product. #### Pretrained on more than ImageNet\\nSeveral weights included or references here were pretrained with proprietary datasets that I do not have access to. These include the Facebook WSL, SSL, SWSL ResNe(Xt) and the Google Noisy Student EfficientNet models. The Facebook models have an explicit non-commercial license (CC-BY-NC 4.0, https://github.com/facebookresearch/semi-supervised-ImageNet1K-models, https://github.com/facebookresearch/WSL-Images). The Google models do not appear to have any restriction beyond the Apache 2.0 license (and ImageNet concerns). In either case, you should contact Facebook or Google with any questions. ## Citing\\n### BibTeX\\n```bibtex\\n@misc{rw2019timm,\\nauthor = {Ross Wightman},\\ntitle = {PyTorch Image Models},\\nyear = {2019},\\npublisher = {GitHub},\\njournal = {GitHub repository},\\ndoi = {10.5281/zenodo.4414861},\\nhowpublished = {\\\\url{https://github.com/rwightman/pytorch-image-models}}\\n}\\n```\\n### Latest DOI\\n[![DOI](https://zenodo.org/badge/168799526.svg)](https://zenodo.org/badge/latestdoi/168799526)', 'type': 'tool', 'name': 'data_retriever', 'id': '164c6ca7-6e59-4b41-8112-eb1604492bd4', 'tool_call_id': 'call_bU6Utbc6GD4QnxHbhwiMdmFf', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c0ba666af48523f51f40', 'page_content': \"Convert weights to safetensors\\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`. The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository. For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights. Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c21b666af48523f52102', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Load safetensors\\n[[open-in-colab]]\\n[safetensors](https://github.com/huggingface/safetensors) is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or \\\\*pickled\\\\* into a `.bin` file with Python\\'s [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights. This guide will show you how you load `.safetensor` files, and how to convert Stable Diffusion model weights stored in other formats to `.safetensor`. Before you start, make sure you have safetensors installed:\\n```py\\n# uncomment to install the necessary libraries in Colab\\n#!pip install safetensors\\n```\\nIf you look at the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main) repository, you\\'ll see weights inside the `text\\\\_encoder`, `unet` and `vae` subfolders are stored in the `.safetensors` format. By default, ü§ó Diffusers automatically loads these `.safetensors` files from their subfolders if they\\'re available in the model repository. For more explicit control, you can optionally set `use\\\\_safetensors=True` (if `safetensors` is not installed, you\\'ll get an error message asking you to install it):\\n```py\\nfrom diffusers import DiffusionPipeline\\npipeline = DiffusionPipeline.from\\\\_pretrained(\"runwayml/stable-diffusion-v1-5\", use\\\\_safetensors=True)\\n```\\nHowever, model weights are not necessarily stored in separate subfolders like in the example above. Sometimes, all the weights are stored in a single `.safetensors` file. In this case, if the weights are Stable Diffusion weights, you can load the file directly with the [`~diffusers.loaders.FromSingleFileMixin.from\\\\_single\\\\_file`] method:\\n```py\\nfrom diffusers import StableDiffusionPipeline\\npipeline = StableDiffusionPipeline.from\\\\_single\\\\_file(\\n\"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\"\\n)\\n```\\n## Convert to safetensors\\nNot all weights on the Hub are available in the `.safetensors` format, and you may encounter weights stored as `.bin`.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aad423e684812d15f61d6', 'page_content': 'In PyTorch, the\\nname of a layer is defined by the name of the class attribute you give the layer. Let\\'s define a dummy model in\\nPyTorch, called `SimpleModel` as follows:\\n```python\\nfrom torch import nn\\nclass SimpleModel(nn.Module):\\ndef \\\\_\\\\_init\\\\_\\\\_(self):\\nsuper().\\\\_\\\\_init\\\\_\\\\_()\\nself.dense = nn.Linear(10, 10)\\nself.intermediate = nn.Linear(10, 10)\\nself.layer\\\\_norm = nn.LayerNorm(10)\\n```\\nNow we can create an instance of this model definition which will fill all weights: `dense`, `intermediate`,\\n`layer\\\\_norm` with random weights. We can print the model to see its architecture\\n```python\\nmodel = SimpleModel()\\nprint(model)\\n```\\nThis will print out the following:\\n```\\nSimpleModel(\\n(dense): Linear(in\\\\_features=10, out\\\\_features=10, bias=True)\\n(intermediate): Linear(in\\\\_features=10, out\\\\_features=10, bias=True)\\n(layer\\\\_norm): LayerNorm((10,), eps=1e-05, elementwise\\\\_affine=True)\\n)\\n```\\nWe can see that the layer names are defined by the name of the class attribute in PyTorch. You can print out the weight\\nvalues of a specific layer:\\n```python\\nprint(model.dense.weight.data)\\n```\\nto see that the weights were randomly initialized\\n```\\ntensor([[-0.0818, 0.2207, -0.0749, -0.0030, 0.0045, -0.1569, -0.1598, 0.0212,\\n-0.2077, 0.2157],\\n[ 0.1044, 0.0201, 0.0990, 0.2482, 0.3116, 0.2509, 0.2866, -0.2190,\\n0.2166, -0.0212],\\n[-0.2000, 0.1107, -0.1999, -0.3119, 0.1559, 0.0993, 0.1776, -0.1950,\\n-0.1023, -0.0447],\\n[-0.0888, -0.1092, 0.2281, 0.0336, 0.1817, -0.0115, 0.2096, 0.1415,\\n-0.1876, -0.2467],\\n[ 0.2208, -0.2352, -0.1426, -0.2636, -0.2889, -0.2061, -0.2849, -0.0465,\\n0.2577, 0.0402],\\n[ 0.1502, 0.2465, 0.2566, 0.0693, 0.2352, -0.0530, 0.1859, -0.0604,\\n0.2132, 0.1680],\\n[ 0.1733, -0.2407, -0.1721, 0.1484, 0.0358, -0.0633, -0.0721, -0.0090,\\n0.2707, -0.2509],\\n[-0.1173, 0.1561, 0.2945, 0.0595, -0.1996, 0.2988, -0.0802, 0.0407,\\n0.1829, -0.1568],\\n[-0.1164, -0.2228, -0.0403, 0.0428, 0.1339, 0.0047, 0.1967, 0.2923,\\n0.0333, -0.0536],\\n[-0.1492, -0.1616, 0.1057, 0.1950, -0.2807, -0.2710, -0.1586, 0.0739,\\n0.2220, 0.2358]]). ```\\nIn the conversion script, you should fill those randomly initialized weights with the exact weights of the\\ncorresponding layer in the checkpoint. \\\\*E.g.\\\\*\\n```python\\n# retrieve matching layer weights, e.g. by\\n# recursive algorithm\\nlayer\\\\_name = \"dense\"\\npretrained\\\\_weight = array\\\\_of\\\\_dense\\\\_layer\\nmodel\\\\_pointer = getattr(model, \"dense\")\\nmodel\\\\_pointer.weight.data = torch.from\\\\_numpy(pretrained\\\\_weight)\\n```\\nWhile doing so, you must verify that each randomly initialized weight of your PyTorch model and its corresponding\\npretrained checkpoint weight exactly match in both \\\\*\\\\*shape and name\\\\*\\\\*. To do so, it is \\\\*\\\\*necessary\\\\*\\\\* to add assert\\nstatements for the shape and print out the names of the checkpoints weights. E.g. you should add statements like:\\n```python\\nassert (\\nmodel\\\\_pointer.weight.shape == pretrained\\\\_weight.shape\\n), f\"Pointer shape of random weight {model\\\\_pointer.shape} and array shape of checkpoint weight {pretrained\\\\_weight.shape} mismatched\"\\n```\\nBesides, you should also print out the names of both weights to make sure they match, \\\\*e.g.\\\\*\\n```python\\nlogger.info(f\"Initialize PyTorch weight {layer\\\\_name} from {pretrained\\\\_weight.name}\")\\n```\\nIf either the shape or the name doesn\\'t match, you probably assigned the wrong checkpoint weight to a randomly\\ninitialized layer of the ü§ó Transformers implementation. An incorrect shape is most likely due to an incorrect setting of the config parameters in `BrandNewBertConfig()` that\\ndo not exactly match those that were used for the checkpoint you want to convert.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab4a63e684812d15f6a35', 'page_content': \"I've taken care to make sure any third party code included or adapted has compatible (permissive) licenses such as MIT, BSD, etc. I've made an effort to avoid any GPL / LGPL conflicts. That said, it is your responsibility to ensure you comply with licenses here and conditions of any dependent licenses. Where applicable, I've linked the sources/references for various components in docstrings. If you think I've missed anything please create an issue. ### Pretrained Weights\\nSo far all of the pretrained weights available here are pretrained on ImageNet with a select few that have some additional pretraining (see extra note below). ImageNet was released for non-commercial research purposes only (https://image-net.org/download). It's not clear what the implications of that are for the use of pretrained weights from that dataset. Any models I have trained with ImageNet are done for research purposes and one should assume that the original dataset license applies to the weights. It's best to seek legal advice if you intend to use the pretrained weights in a commercial product. #### Pretrained on more than ImageNet\\nSeveral weights included or references here were pretrained with proprietary datasets that I do not have access to. These include the Facebook WSL, SSL, SWSL ResNe(Xt) and the Google Noisy Student EfficientNet models. The Facebook models have an explicit non-commercial license (CC-BY-NC 4.0, https://github.com/facebookresearch/semi-supervised-ImageNet1K-models, https://github.com/facebookresearch/WSL-Images). The Google models do not appear to have any restriction beyond the Apache 2.0 license (and ImageNet concerns). In either case, you should contact Facebook or Google with any questions. ## Citing\\n### BibTeX\\n```bibtex\\n@misc{rw2019timm,\\nauthor = {Ross Wightman},\\ntitle = {PyTorch Image Models},\\nyear = {2019},\\npublisher = {GitHub},\\njournal = {GitHub repository},\\ndoi = {10.5281/zenodo.4414861},\\nhowpublished = {\\\\url{https://github.com/rwightman/pytorch-image-models}}\\n}\\n```\\n### Latest DOI\\n[![DOI](https://zenodo.org/badge/168799526.svg)](https://zenodo.org/badge/latestdoi/168799526)\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The more secure file format used to save and store PyTorch model weights is `.safetensors`.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--75f1bb1a-df83-46dc-93ca-c292741238e7-0', 'usage_metadata': {'input_tokens': 2948, 'output_tokens': 20, 'total_tokens': 2968, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:01,141 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 32/65 [02:14<02:40,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What type of security certification does Hugging Face have?\\n', 'type': 'human', 'id': 'b5569a6b-a8a9-4dc6-8196-c79f0996dbc0'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_K2Mie7qycdBz1uHLsQXLinVr', 'function': {'arguments': '{\"query\":\"Hugging Face security certification\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--3b18b9cf-86ae-40bc-841c-3744edd50f20-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Hugging Face security certification'}, 'id': 'call_K2Mie7qycdBz1uHLsQXLinVr', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 359, 'output_tokens': 21, 'total_tokens': 380, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Security\\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you\\'ll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\\n- [User Access Tokens](./security-tokens)\\n- [Git over SSH](./security-git-ssh)\\n- [Signing commits with GPG](./security-gpg)\\n- [Single Sign-On (SSO)](./security-sso)\\n- [Malware Scanning](./security-malware)\\n- [Pickle Scanning](./security-pickle)\\n- [Secrets Scanning](./security-secrets)\\n\\nSecurity Policy\\n## Reporting a Vulnerability\\nü§ó We have our bug bounty program set up with HackerOne. Please feel free to submit vulnerability reports to our private program at https://hackerone.com/hugging\\\\_face. Note that you\\'ll need to be invited to our program, so send us a quick email at security@huggingface.co if you\\'ve found a vulnerability.\\n\\n!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Under construction\\n\\n!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Under construction', 'type': 'tool', 'name': 'data_retriever', 'id': '4591d380-9128-46e5-ad18-5aaf291b919a', 'tool_call_id': 'call_K2Mie7qycdBz1uHLsQXLinVr', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aae003e684812d15f62c7', 'page_content': \"Security\\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\\n- [User Access Tokens](./security-tokens)\\n- [Git over SSH](./security-git-ssh)\\n- [Signing commits with GPG](./security-gpg)\\n- [Single Sign-On (SSO)](./security-sso)\\n- [Malware Scanning](./security-malware)\\n- [Pickle Scanning](./security-pickle)\\n- [Secrets Scanning](./security-secrets)\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aadc43e684812d15f627a', 'page_content': \"Security Policy\\n## Reporting a Vulnerability\\nü§ó We have our bug bounty program set up with HackerOne. Please feel free to submit vulnerability reports to our private program at https://hackerone.com/hugging\\\\_face. Note that you'll need to be invited to our program, so send us a quick email at security@huggingface.co if you've found a vulnerability.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b9e6af3d2ebb270a082f', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Under construction', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bca0af3d2ebb270a0b3d', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Under construction', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--da91581c-d349-41db-bc6c-721cc31fab47-0', 'usage_metadata': {'input_tokens': 929, 'output_tokens': 30, 'total_tokens': 959, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:04,704 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 33/65 [02:18<02:23,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What do RAG models combine to generate outputs?\\n', 'type': 'human', 'id': '2399f264-a9d4-459e-b0de-088a359a2f00'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_TJTWw1gz5MZ3tZnAORqrWanz', 'function': {'arguments': '{\"query\":\"RAG models combine to generate outputs\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--a5c5d995-cc9e-49ef-b5f3-7e1c18e7b96d-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'RAG models combine to generate outputs'}, 'id': 'call_TJTWw1gz5MZ3tZnAORqrWanz', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 358, 'output_tokens': 22, 'total_tokens': 380, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# RAG\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-rag-blueviolet)](https://huggingface.co/models?filter=rag)\\n\\n## Overview\\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and\\nsequence-to-sequence models.\\n\\nRAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate\\noutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\\nboth retrieval and generation to adapt to downstream tasks. It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\\nKarpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela. The abstract from the paper is the following:\\n\\\\*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve\\nstate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely\\nmanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind\\ntask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge\\nremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametric\\nmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a\\ngeneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) ‚Äî models which combine pre-trained\\nparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a\\npre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our\\nmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation\\ntasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art\\nparametric-only seq2seq baseline.\\\\*\\nThis model was contributed by [ola13](https://huggingface.co/ola13). ## Usage tips\\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq\\nmodules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt\\nto downstream tasks. ## RagConfig\\n[[autodoc]] RagConfig\\n## RagTokenizer\\n[[autodoc]] RagTokenizer\\n## Rag specific outputs\\n[[autodoc]] models.rag.modeling\\\\_rag.RetrievAugLMMarginOutput\\n[[autodoc]] models.rag.modeling\\\\_rag.RetrievAugLMOutput\\n## RagRetriever\\n[[autodoc]] RagRetriever\\n\\n## RagModel\\n[[autodoc]] RagModel\\n- forward\\n## RagSequenceForGeneration\\n[[autodoc]] RagSequenceForGeneration\\n- forward\\n- generate\\n## RagTokenForGeneration\\n[[autodoc]] RagTokenForGeneration\\n- forward\\n- generate\\n\\n## TFRagModel\\n[[autodoc]] TFRagModel\\n- call\\n## TFRagSequenceForGeneration\\n[[autodoc]] TFRagSequenceForGeneration\\n- call\\n- generate\\n## TFRagTokenForGeneration\\n[[autodoc]] TFRagTokenForGeneration\\n- call\\n- generate\\n\\nThe model retrieves contextual documents from an external dataset as part of its execution. These contextual documents are used in conjunction with the original input to produce an output. The GIF is taken from [Facebook\\'s original blog post](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models).\\\\_\\nRecently, [Huggingface](https://huggingface.co/) partnered with [Facebook AI](https://ai.facebook.com/) to introduce the [RAG](https://twitter.com/huggingface/status/1310597560906780680) model as part of its Transformers library. [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) acts just like any other [seq2seq model](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html). However, [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) has an intermediate component that retrieves contextual documents from an external knowledge base (like a Wikipedia text corpus). These documents are then used in conjunction with the input sequence and passed into the underlying seq2seq [generator](https://huggingface.co/blog/how-to-generate).\\n\\nIntro\\nAuthors: @patrickvonplaten and @lhoestq\\nAimed at tackling the knowledge-intensive NLP tasks (think tasks a human wouldn\\'t be expected to solve without access to external knowledge sources), RAG models are seq2seq models with access to a retrieval mechanism providing relevant context documents at training and evaluation time. A RAG model encapsulates two core components: a question encoder and a generator. During a forward pass, we encode the input with the question encoder and pass it\\nto the retriever to extract relevant context documents. The documents are then prepended to the input. Such contextualized inputs are passed to the generator. Read more about RAG at https://arxiv.org/abs/2005.11401. # Note\\n‚ö†Ô∏è This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability\\n# Finetuning\\nOur finetuning logic is based on scripts from [`examples/legacy/seq2seq`](https://github.com/huggingface/transformers/tree/main/examples/legacy/seq2seq). We accept training data in the same format as specified there - we expect a directory consisting of 6 text files:\\n```bash\\ntrain.source\\ntrain.target\\nval.source\\nval.target\\ntest.source\\ntest.target\\n```\\nA sample finetuning command (run ` ./examples/research\\\\_projects/rag/finetune\\\\_rag.py --help` to list all available options):\\n```bash\\npython examples/research\\\\_projects/rag/finetune\\\\_rag.py \\\\\\n--data\\\\_dir $DATA\\\\_DIR \\\\\\n--output\\\\_dir $OUTPUT\\\\_DIR \\\\\\n--model\\\\_name\\\\_or\\\\_path $MODEL\\\\_NAME\\\\_OR\\\\_PATH \\\\\\n--model\\\\_type rag\\\\_sequence \\\\\\n--fp16 \\\\\\n--gpus 8\\n```\\nWe publish two `base` models which can serve as a starting point for finetuning on downstream tasks (use them as `model\\\\_name\\\\_or\\\\_path`):\\n- [`facebook/rag-sequence-base`](https://huggingface.co/facebook/rag-sequence-base) - a base for finetuning `RagSequenceForGeneration` models,\\n- [`facebook/rag-token-base`](https://huggingface.co/facebook/rag-token-base) - a base for finetuning `RagTokenForGeneration` models. The `base` models initialize the question encoder with [`facebook/dpr-question\\\\_encoder-single-nq-base`](https://huggingface.co/facebook/dpr-question\\\\_encoder-single-nq-base) and the generator with [`facebook/bart-large`](https://huggingface.co/facebook/bart-large). If you would like to initialize finetuning with a base model using different question encoder and generator architectures, you can build it with a consolidation script, e.g.:\\n```\\npython examples/research\\\\_projects/rag/consolidate\\\\_rag\\\\_checkpoint.py \\\\\\n--model\\\\_type rag\\\\_sequence \\\\\\n--generator\\\\_name\\\\_or\\\\_path facebook/bart-large-cnn \\\\\\n--question\\\\_encoder\\\\_name\\\\_or\\\\_path facebook/dpr-question\\\\_encoder-single-nq-base \\\\\\n--dest path/to/checkpoint\\n```\\nYou will then be able to pass `path/to/checkpoint` as `model\\\\_name\\\\_or\\\\_path` to the `finetune\\\\_rag.py` script. ## Document Retrieval\\nWhen running distributed fine-tuning, each training worker needs to retrieve contextual documents\\nfor its input by querying a index loaded into memory. RAG provides two implementations for document retrieval,\\none with [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) communication package and the other\\nwith [`Ray`](https://docs.ray.io/en/master/). This option can be configured with the `--distributed\\\\_retriever` flag which can either be set to `pytorch` or `ray`. By default this flag is set to `pytorch`. For the Pytorch implementation, only training worker 0 loads the index into CPU memory, and a gather/scatter pattern is used\\nto collect the inputs from the other training workers and send back the corresponding document embeddings. For the Ray implementation, the index is loaded in \\\\*separate\\\\* process(es). The training workers randomly select which\\nretriever worker to query. To use Ray for distributed retrieval, you have to set the `--distributed\\\\_retriever` arg to `ray`. To configure the number of retrieval workers (the number of processes that load the index), you can set the `num\\\\_retrieval\\\\_workers` flag.', 'type': 'tool', 'name': 'data_retriever', 'id': '6eb80659-3b8f-4faf-9653-27ea873cd14a', 'tool_call_id': 'call_TJTWw1gz5MZ3tZnAORqrWanz', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaac53e684812d15f5ebc', 'page_content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# RAG\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-rag-blueviolet)](https://huggingface.co/models?filter=rag)\\n\\n## Overview\\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and\\nsequence-to-sequence models.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaac63e684812d15f5ebd', 'page_content': 'RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate\\noutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\\nboth retrieval and generation to adapt to downstream tasks. It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\\nKarpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela. The abstract from the paper is the following:\\n\\\\*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve\\nstate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely\\nmanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind\\ntask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge\\nremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametric\\nmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a\\ngeneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) ‚Äî models which combine pre-trained\\nparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a\\npre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our\\nmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation\\ntasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art\\nparametric-only seq2seq baseline.\\\\*\\nThis model was contributed by [ola13](https://huggingface.co/ola13). ## Usage tips\\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq\\nmodules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt\\nto downstream tasks. ## RagConfig\\n[[autodoc]] RagConfig\\n## RagTokenizer\\n[[autodoc]] RagTokenizer\\n## Rag specific outputs\\n[[autodoc]] models.rag.modeling\\\\_rag.RetrievAugLMMarginOutput\\n[[autodoc]] models.rag.modeling\\\\_rag.RetrievAugLMOutput\\n## RagRetriever\\n[[autodoc]] RagRetriever\\n\\n## RagModel\\n[[autodoc]] RagModel\\n- forward\\n## RagSequenceForGeneration\\n[[autodoc]] RagSequenceForGeneration\\n- forward\\n- generate\\n## RagTokenForGeneration\\n[[autodoc]] RagTokenForGeneration\\n- forward\\n- generate\\n\\n## TFRagModel\\n[[autodoc]] TFRagModel\\n- call\\n## TFRagSequenceForGeneration\\n[[autodoc]] TFRagSequenceForGeneration\\n- call\\n- generate\\n## TFRagTokenForGeneration\\n[[autodoc]] TFRagTokenForGeneration\\n- call\\n- generate', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b5feaf3d2ebb270a034c', 'page_content': \"The model retrieves contextual documents from an external dataset as part of its execution. These contextual documents are used in conjunction with the original input to produce an output. The GIF is taken from [Facebook's original blog post](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models).\\\\_\\nRecently, [Huggingface](https://huggingface.co/) partnered with [Facebook AI](https://ai.facebook.com/) to introduce the [RAG](https://twitter.com/huggingface/status/1310597560906780680) model as part of its Transformers library. [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) acts just like any other [seq2seq model](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html). However, [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) has an intermediate component that retrieves contextual documents from an external knowledge base (like a Wikipedia text corpus). These documents are then used in conjunction with the input sequence and passed into the underlying seq2seq [generator](https://huggingface.co/blog/how-to-generate).\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab2c83e684812d15f67f7', 'page_content': \"Intro\\nAuthors: @patrickvonplaten and @lhoestq\\nAimed at tackling the knowledge-intensive NLP tasks (think tasks a human wouldn't be expected to solve without access to external knowledge sources), RAG models are seq2seq models with access to a retrieval mechanism providing relevant context documents at training and evaluation time. A RAG model encapsulates two core components: a question encoder and a generator. During a forward pass, we encode the input with the question encoder and pass it\\nto the retriever to extract relevant context documents. The documents are then prepended to the input. Such contextualized inputs are passed to the generator. Read more about RAG at https://arxiv.org/abs/2005.11401. # Note\\n‚ö†Ô∏è This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability\\n# Finetuning\\nOur finetuning logic is based on scripts from [`examples/legacy/seq2seq`](https://github.com/huggingface/transformers/tree/main/examples/legacy/seq2seq). We accept training data in the same format as specified there - we expect a directory consisting of 6 text files:\\n```bash\\ntrain.source\\ntrain.target\\nval.source\\nval.target\\ntest.source\\ntest.target\\n```\\nA sample finetuning command (run ` ./examples/research\\\\_projects/rag/finetune\\\\_rag.py --help` to list all available options):\\n```bash\\npython examples/research\\\\_projects/rag/finetune\\\\_rag.py \\\\\\n--data\\\\_dir $DATA\\\\_DIR \\\\\\n--output\\\\_dir $OUTPUT\\\\_DIR \\\\\\n--model\\\\_name\\\\_or\\\\_path $MODEL\\\\_NAME\\\\_OR\\\\_PATH \\\\\\n--model\\\\_type rag\\\\_sequence \\\\\\n--fp16 \\\\\\n--gpus 8\\n```\\nWe publish two `base` models which can serve as a starting point for finetuning on downstream tasks (use them as `model\\\\_name\\\\_or\\\\_path`):\\n- [`facebook/rag-sequence-base`](https://huggingface.co/facebook/rag-sequence-base) - a base for finetuning `RagSequenceForGeneration` models,\\n- [`facebook/rag-token-base`](https://huggingface.co/facebook/rag-token-base) - a base for finetuning `RagTokenForGeneration` models. The `base` models initialize the question encoder with [`facebook/dpr-question\\\\_encoder-single-nq-base`](https://huggingface.co/facebook/dpr-question\\\\_encoder-single-nq-base) and the generator with [`facebook/bart-large`](https://huggingface.co/facebook/bart-large). If you would like to initialize finetuning with a base model using different question encoder and generator architectures, you can build it with a consolidation script, e.g.:\\n```\\npython examples/research\\\\_projects/rag/consolidate\\\\_rag\\\\_checkpoint.py \\\\\\n--model\\\\_type rag\\\\_sequence \\\\\\n--generator\\\\_name\\\\_or\\\\_path facebook/bart-large-cnn \\\\\\n--question\\\\_encoder\\\\_name\\\\_or\\\\_path facebook/dpr-question\\\\_encoder-single-nq-base \\\\\\n--dest path/to/checkpoint\\n```\\nYou will then be able to pass `path/to/checkpoint` as `model\\\\_name\\\\_or\\\\_path` to the `finetune\\\\_rag.py` script. ## Document Retrieval\\nWhen running distributed fine-tuning, each training worker needs to retrieve contextual documents\\nfor its input by querying a index loaded into memory. RAG provides two implementations for document retrieval,\\none with [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) communication package and the other\\nwith [`Ray`](https://docs.ray.io/en/master/). This option can be configured with the `--distributed\\\\_retriever` flag which can either be set to `pytorch` or `ray`. By default this flag is set to `pytorch`. For the Pytorch implementation, only training worker 0 loads the index into CPU memory, and a gather/scatter pattern is used\\nto collect the inputs from the other training workers and send back the corresponding document embeddings. For the Ray implementation, the index is loaded in \\\\*separate\\\\* process(es). The training workers randomly select which\\nretriever worker to query. To use Ray for distributed retrieval, you have to set the `--distributed\\\\_retriever` arg to `ray`. To configure the number of retrieval workers (the number of processes that load the index), you can set the `num\\\\_retrieval\\\\_workers` flag.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence (seq2seq) models to generate outputs. They retrieve documents, pass them to a seq2seq model, and then marginalize to produce the final output.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--63d7caab-f7be-453c-b663-be0174ad9c61-0', 'usage_metadata': {'input_tokens': 2561, 'output_tokens': 52, 'total_tokens': 2613, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:08,246 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 34/65 [02:21<02:10,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\\n', 'type': 'human', 'id': '754270ee-5c66-4db8-8ad9-106b455dd4f6'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_NFtiabzaGmCvIjpKgPuTzqUk', 'function': {'arguments': '{\"query\":\"MarkupLMFeatureExtractor library for extracting data from HTML and XML files\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--8bd4337a-65dc-4aef-bdda-64640b5d1cf5-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'MarkupLMFeatureExtractor library for extracting data from HTML and XML files'}, 'id': 'call_NFtiabzaGmCvIjpKgPuTzqUk', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 366, 'output_tokens': 28, 'total_tokens': 394, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# MarkupLM\\n## Overview\\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\\nperformance, similar to [LayoutLM](layoutlm). The model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\\nstate-of-the-art results on 2 important benchmarks:\\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\\n- [SWDE](https://www.researchgate.net/publication/221299838\\\\_From\\\\_one\\\\_tree\\\\_to\\\\_a\\\\_forest\\\\_a\\\\_unified\\\\_solution\\\\_for\\\\_structured\\\\_web\\\\_data\\\\_extraction), a dataset\\nfor information extraction from web pages (basically named-entity recogntion on web pages)\\nThe abstract from the paper is the following:\\n\\\\*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\\ntasks. The pre-trained model and code will be publicly available.\\\\*\\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm). ## Usage tips\\n- In addition to `input\\\\_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath\\\\_tags\\\\_seq` and `xpath\\\\_subs\\\\_seq`. These are the XPATH tags and subscripts respectively for each token in the input sequence. - One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info. ![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg)\\n MarkupLM architecture. Taken from the [original paper.](https://arxiv.org/abs/2110.08518) \\n## Usage: MarkupLMProcessor\\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\\ntoken-level inputs of the model (`input\\\\_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\\nif you only want to handle one of the two tasks. ```python\\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\\nfeature\\\\_extractor = MarkupLMFeatureExtractor()\\ntokenizer = MarkupLMTokenizerFast.from\\\\_pretrained(\"microsoft/markuplm-base\")\\nprocessor = MarkupLMProcessor(feature\\\\_extractor, tokenizer)\\n```\\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\\nand it will create the inputs expected by the model. Internally, the processor first uses\\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\\nto token-level `input\\\\_ids`, `attention\\\\_mask`, `token\\\\_type\\\\_ids`, `xpath\\\\_subs\\\\_seq`, `xpath\\\\_tags\\\\_seq`. Optionally, one can provide node labels to the processor, which are turned into token-level `labels`. [`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].\\n\\n!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Feature Extractor\\nA feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction from sequences, e.g., pre-processing audio files to generate Log-Mel Spectrogram features, feature extraction from images, e.g., cropping image files, but also padding, normalization, and conversion to NumPy, PyTorch, and TensorFlow tensors. ## FeatureExtractionMixin\\n[[autodoc]] feature\\\\_extraction\\\\_utils.FeatureExtractionMixin\\n- from\\\\_pretrained\\n- save\\\\_pretrained\\n## SequenceFeatureExtractor\\n[[autodoc]] SequenceFeatureExtractor\\n- pad\\n## BatchFeature\\n[[autodoc]] BatchFeature\\n## ImageFeatureExtractionMixin\\n[[autodoc]] image\\\\_utils.ImageFeatureExtractionMixin\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Utilities for `FeatureExtractors`\\nThis page lists all the utility functions that can be used by the audio [`FeatureExtractor`] in order to compute special features from a raw audio using common algorithms such as \\\\*Short Time Fourier Transform\\\\* or \\\\*log mel spectrogram\\\\*. Most of those are only useful if you are studying the code of the audio processors in the library. ## Audio Transformations\\n[[autodoc]] audio\\\\_utils.hertz\\\\_to\\\\_mel\\n[[autodoc]] audio\\\\_utils.mel\\\\_to\\\\_hertz\\n[[autodoc]] audio\\\\_utils.mel\\\\_filter\\\\_bank\\n[[autodoc]] audio\\\\_utils.optimal\\\\_fft\\\\_length\\n[[autodoc]] audio\\\\_utils.window\\\\_function\\n[[autodoc]] audio\\\\_utils.spectrogram\\n[[autodoc]] audio\\\\_utils.power\\\\_to\\\\_db\\n[[autodoc]] audio\\\\_utils.amplitude\\\\_to\\\\_db\\n\\n!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# XLM\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-xlm-blueviolet)](https://huggingface.co/models?filter=xlm)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/xlm-mlm-en-2048)\\n\\n## Overview\\nThe XLM model was proposed in [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by\\nGuillaume Lample, Alexis Conneau. It\\'s a transformer pretrained using one of the following objectives:\\n- a causal language modeling (CLM) objective (next token prediction),\\n- a masked language modeling (MLM) objective (BERT-like), or\\n- a Translation Language Modeling (TLM) object (extension of BERT\\'s MLM to multiple language inputs)\\nThe abstract from the paper is the following:\\n\\\\*Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding.', 'type': 'tool', 'name': 'data_retriever', 'id': '06df102d-9234-4228-949a-38983584db20', 'tool_call_id': 'call_NFtiabzaGmCvIjpKgPuTzqUk', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899ba72af3d2ebb270a08cd', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# MarkupLM\\n## Overview\\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\\nperformance, similar to [LayoutLM](layoutlm). The model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\\nstate-of-the-art results on 2 important benchmarks:\\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\\n- [SWDE](https://www.researchgate.net/publication/221299838\\\\_From\\\\_one\\\\_tree\\\\_to\\\\_a\\\\_forest\\\\_a\\\\_unified\\\\_solution\\\\_for\\\\_structured\\\\_web\\\\_data\\\\_extraction), a dataset\\nfor information extraction from web pages (basically named-entity recogntion on web pages)\\nThe abstract from the paper is the following:\\n\\\\*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\\ntasks. The pre-trained model and code will be publicly available.\\\\*\\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm). ## Usage tips\\n- In addition to `input\\\\_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath\\\\_tags\\\\_seq` and `xpath\\\\_subs\\\\_seq`. These are the XPATH tags and subscripts respectively for each token in the input sequence. - One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info. ![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg)\\n MarkupLM architecture. Taken from the [original paper.](https://arxiv.org/abs/2110.08518) \\n## Usage: MarkupLMProcessor\\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\\ntoken-level inputs of the model (`input\\\\_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\\nif you only want to handle one of the two tasks. ```python\\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\\nfeature\\\\_extractor = MarkupLMFeatureExtractor()\\ntokenizer = MarkupLMTokenizerFast.from\\\\_pretrained(\"microsoft/markuplm-base\")\\nprocessor = MarkupLMProcessor(feature\\\\_extractor, tokenizer)\\n```\\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\\nand it will create the inputs expected by the model. Internally, the processor first uses\\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\\nto token-level `input\\\\_ids`, `attention\\\\_mask`, `token\\\\_type\\\\_ids`, `xpath\\\\_subs\\\\_seq`, `xpath\\\\_tags\\\\_seq`. Optionally, one can provide node labels to the processor, which are turned into token-level `labels`. [`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa5773e684812d15f5870', 'page_content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Feature Extractor\\nA feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction from sequences, e.g., pre-processing audio files to generate Log-Mel Spectrogram features, feature extraction from images, e.g., cropping image files, but also padding, normalization, and conversion to NumPy, PyTorch, and TensorFlow tensors. ## FeatureExtractionMixin\\n[[autodoc]] feature\\\\_extraction\\\\_utils.FeatureExtractionMixin\\n- from\\\\_pretrained\\n- save\\\\_pretrained\\n## SequenceFeatureExtractor\\n[[autodoc]] SequenceFeatureExtractor\\n- pad\\n## BatchFeature\\n[[autodoc]] BatchFeature\\n## ImageFeatureExtractionMixin\\n[[autodoc]] image\\\\_utils.ImageFeatureExtractionMixin', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bb9aaf3d2ebb270a0a2b', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Utilities for `FeatureExtractors`\\nThis page lists all the utility functions that can be used by the audio [`FeatureExtractor`] in order to compute special features from a raw audio using common algorithms such as \\\\*Short Time Fourier Transform\\\\* or \\\\*log mel spectrogram\\\\*. Most of those are only useful if you are studying the code of the audio processors in the library. ## Audio Transformations\\n[[autodoc]] audio\\\\_utils.hertz\\\\_to\\\\_mel\\n[[autodoc]] audio\\\\_utils.mel\\\\_to\\\\_hertz\\n[[autodoc]] audio\\\\_utils.mel\\\\_filter\\\\_bank\\n[[autodoc]] audio\\\\_utils.optimal\\\\_fft\\\\_length\\n[[autodoc]] audio\\\\_utils.window\\\\_function\\n[[autodoc]] audio\\\\_utils.spectrogram\\n[[autodoc]] audio\\\\_utils.power\\\\_to\\\\_db\\n[[autodoc]] audio\\\\_utils.amplitude\\\\_to\\\\_db', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bf48666af48523f51da2', 'page_content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# XLM\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-xlm-blueviolet)](https://huggingface.co/models?filter=xlm)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/xlm-mlm-en-2048)\\n\\n## Overview\\nThe XLM model was proposed in [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by\\nGuillaume Lample, Alexis Conneau. It\\'s a transformer pretrained using one of the following objectives:\\n- a causal language modeling (CLM) objective (next token prediction),\\n- a masked language modeling (MLM) objective (BERT-like), or\\n- a Translation Language Modeling (TLM) object (extension of BERT\\'s MLM to multiple language inputs)\\nThe abstract from the paper is the following:\\n\\\\*Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'MarkupLMFeatureExtractor uses Beautiful Soup, a Python library for pulling data out of HTML and XML files, to extract data from HTML and XML files.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--9a883213-d7e8-4006-bf44-4797e1020965-0', 'usage_metadata': {'input_tokens': 2609, 'output_tokens': 30, 'total_tokens': 2639, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:11,482 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 35/65 [02:25<01:57,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the file size limit for syncing to HF Spaces without using Git-LFS?\\n', 'type': 'human', 'id': '2f53a3e6-1122-4110-bf2f-41b1de91a86b'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_kGGoyBwEjkWXb23EXGfrQSuB', 'function': {'arguments': '{\"query\":\"file size limit for syncing to HF Spaces without using Git-LFS\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--90bdb3c3-b81f-4f7c-8ecb-c69b0dd21b0d-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'file size limit for syncing to HF Spaces without using Git-LFS'}, 'id': 'call_kGGoyBwEjkWXb23EXGfrQSuB', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 28, 'total_tokens': 393, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'There are a few reasons for this:\\n- Uploading and downloading smaller files is much easier both for you and the other users. Connection issues can always\\nhappen when streaming data and smaller files avoid resuming from the beginning in case of errors. - Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\\nleading to a slower download speed. In all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size. - \\\\*\\\\*Number of commits\\\\*\\\\*: There is no hard limit for the total number of commits on your repo history. However, from\\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\\nwrites. If your repo\\'s history gets very large, it is always possible to squash all the commits to get a\\nfresh start using `huggingface\\\\_hub`\\'s [`super\\\\_squash\\\\_history`](https://huggingface.co/docs/huggingface\\\\_hub/main/en/package\\\\_reference/hf\\\\_api#huggingface\\\\_hub.HfApi.super\\\\_squash\\\\_history).\\n\\nManaging Spaces with Github Actions\\nYou can keep your app in sync with your GitHub repository with \\\\*\\\\*Github Actions\\\\*\\\\*. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don\\'t want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup. First, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository. ```bash\\ngit remote add space https://huggingface.co/spaces/HF\\\\_USERNAME/SPACE\\\\_NAME\\n```\\nThen force push to sync everything for the first time:\\n```bash\\ngit push --force space main\\n```\\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:\\n\\\\* Replace `HF\\\\_USERNAME` with your username and `SPACE\\\\_NAME` with your Space name. \\\\* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF\\\\_TOKEN`. You can find your Hugging Face API token under \\\\*\\\\*API Tokens\\\\*\\\\* on your Hugging Face profile. ```yaml\\nname: Sync to Hugging Face hub\\non:\\npush:\\nbranches: [main]\\n# to run this workflow manually from the Actions tab\\nworkflow\\\\_dispatch:\\njobs:\\nsync-to-hub:\\nruns-on: ubuntu-latest\\nsteps:\\n- uses: actions/checkout@v3\\nwith:\\nfetch-depth: 0\\nlfs: true\\n- name: Push to hub\\nenv:\\nHF\\\\_TOKEN: ${{ secrets.HF\\\\_TOKEN }}\\nrun: git push https://HF\\\\_USERNAME:$HF\\\\_TOKEN@huggingface.co/spaces/HF\\\\_USERNAME/SPACE\\\\_NAME main\\n```\\nFinally, create an Action that automatically checks the file size of any new pull request:\\n```yaml\\nname: Check file size\\non: # or directly `on: [push]` to run the action on every push on any branch\\npull\\\\_request:\\nbranches: [main]\\n# to run this workflow manually from the Actions tab\\nworkflow\\\\_dispatch:\\njobs:\\nsync-to-hub:\\nruns-on: ubuntu-latest\\nsteps:\\n- name: Check large files\\nuses: ActionsDesk/lfs-warning@v2.0\\nwith:\\nfilesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\\n```\\n\\nRepository limitations and recommendations\\nThere are some limitations to be aware of when dealing with a large amount of data in your repo. Given the time it takes to stream the data,\\ngetting an upload/push to fail at the end of the process or encountering a degraded experience, be it on hf.co or when working locally, can be very annoying. ## Recommendations\\nWe gathered a list of tips and recommendations for structuring your repo. If you are looking for more practical tips, check out [this guide](https://huggingface.co/docs/huggingface\\\\_hub/main/en/guides/upload#practical-tips) on how to upload large amount of data using the Python library. | Characteristic | Recommended | Tips |\\n| ---------------- | ------------------ | ------------------------------------------------------ |\\n| Repo size | - | contact us for large repos (TBs of data) |\\n| Files per repo | <100k | merge data into fewer files |\\n| Entries per folder | <10k | use subdirectories in repo |\\n| File size | <5GB | split data into chunked files |\\n| Commit size | <100 files\\\\* | upload files in multiple commits |\\n| Commits per repo | - | upload multiple files per commit and/or squash history |\\n\\\\_\\\\* Not relevant when using `git` CLI directly\\\\_\\nPlease read the next section to understand better those limits and how to deal with them.\\n\\n## Explanations\\nWhat are we talking about when we say \"large uploads\", and what are their associated limitations? Large uploads can be\\nvery diverse, from repositories with a few huge files (e.g. model weights) to repositories with thousands of small files\\n(e.g. an image dataset). Under the hood, the Hub uses Git to version the data, which has structural implications on what you can do in your repo. If your repo is crossing some of the numbers mentioned in the previous section, \\\\*\\\\*we strongly encourage you to check out [`git-sizer`](https://github.com/github/git-sizer)\\\\*\\\\*,\\nwhich has very detailed documentation about the different factors that will impact your experience. Here is a TL;DR of factors to consider:\\n- \\\\*\\\\*Repository size\\\\*\\\\*: The total size of the data you\\'re planning to upload. There is no hard limit on a Hub repository size. However, if you plan to upload hundreds of GBs or even TBs of data, we would appreciate it if you could let us know in advance so we can better help you if you have any questions during the process. You can contact us at datasets@huggingface.co or on [our Discord](http://hf.co/join/discord). - \\\\*\\\\*Number of files\\\\*\\\\*:\\n- For optimal experience, we recommend keeping the total number of files under 100k. Try merging the data into fewer files if you have more. For example, json files can be merged into a single jsonl file, or large datasets can be exported as Parquet files. - The maximum number of files per folder cannot exceed 10k files per folder. A simple solution is to\\ncreate a repository structure that uses subdirectories. For example, a repo with 1k folders from `000/` to `999/`, each containing at most 1000 files, is already enough. - \\\\*\\\\*File size\\\\*\\\\*: In the case of uploading large files (e.g. model weights), we strongly recommend splitting them \\\\*\\\\*into chunks of around 5GB each\\\\*\\\\*.', 'type': 'tool', 'name': 'data_retriever', 'id': '00cf5382-4739-45e0-98b2-cecb124cf1e3', 'tool_call_id': 'call_kGGoyBwEjkWXb23EXGfrQSuB', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b41eaf3d2ebb270a0163', 'page_content': \"There are a few reasons for this:\\n- Uploading and downloading smaller files is much easier both for you and the other users. Connection issues can always\\nhappen when streaming data and smaller files avoid resuming from the beginning in case of errors. - Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\\nleading to a slower download speed. In all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size. - \\\\*\\\\*Number of commits\\\\*\\\\*: There is no hard limit for the total number of commits on your repo history. However, from\\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a\\nfresh start using `huggingface\\\\_hub`'s [`super\\\\_squash\\\\_history`](https://huggingface.co/docs/huggingface\\\\_hub/main/en/package\\\\_reference/hf\\\\_api#huggingface\\\\_hub.HfApi.super\\\\_squash\\\\_history).\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b9d3af3d2ebb270a0811', 'page_content': \"Managing Spaces with Github Actions\\nYou can keep your app in sync with your GitHub repository with \\\\*\\\\*Github Actions\\\\*\\\\*. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup. First, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository. ```bash\\ngit remote add space https://huggingface.co/spaces/HF\\\\_USERNAME/SPACE\\\\_NAME\\n```\\nThen force push to sync everything for the first time:\\n```bash\\ngit push --force space main\\n```\\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:\\n\\\\* Replace `HF\\\\_USERNAME` with your username and `SPACE\\\\_NAME` with your Space name. \\\\* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF\\\\_TOKEN`. You can find your Hugging Face API token under \\\\*\\\\*API Tokens\\\\*\\\\* on your Hugging Face profile. ```yaml\\nname: Sync to Hugging Face hub\\non:\\npush:\\nbranches: [main]\\n# to run this workflow manually from the Actions tab\\nworkflow\\\\_dispatch:\\njobs:\\nsync-to-hub:\\nruns-on: ubuntu-latest\\nsteps:\\n- uses: actions/checkout@v3\\nwith:\\nfetch-depth: 0\\nlfs: true\\n- name: Push to hub\\nenv:\\nHF\\\\_TOKEN: ${{ secrets.HF\\\\_TOKEN }}\\nrun: git push https://HF\\\\_USERNAME:$HF\\\\_TOKEN@huggingface.co/spaces/HF\\\\_USERNAME/SPACE\\\\_NAME main\\n```\\nFinally, create an Action that automatically checks the file size of any new pull request:\\n```yaml\\nname: Check file size\\non: # or directly `on: [push]` to run the action on every push on any branch\\npull\\\\_request:\\nbranches: [main]\\n# to run this workflow manually from the Actions tab\\nworkflow\\\\_dispatch:\\njobs:\\nsync-to-hub:\\nruns-on: ubuntu-latest\\nsteps:\\n- name: Check large files\\nuses: ActionsDesk/lfs-warning@v2.0\\nwith:\\nfilesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\\n```\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b41eaf3d2ebb270a0161', 'page_content': 'Repository limitations and recommendations\\nThere are some limitations to be aware of when dealing with a large amount of data in your repo. Given the time it takes to stream the data,\\ngetting an upload/push to fail at the end of the process or encountering a degraded experience, be it on hf.co or when working locally, can be very annoying. ## Recommendations\\nWe gathered a list of tips and recommendations for structuring your repo. If you are looking for more practical tips, check out [this guide](https://huggingface.co/docs/huggingface\\\\_hub/main/en/guides/upload#practical-tips) on how to upload large amount of data using the Python library. | Characteristic | Recommended | Tips |\\n| ---------------- | ------------------ | ------------------------------------------------------ |\\n| Repo size | - | contact us for large repos (TBs of data) |\\n| Files per repo | <100k | merge data into fewer files |\\n| Entries per folder | <10k | use subdirectories in repo |\\n| File size | <5GB | split data into chunked files |\\n| Commit size | <100 files\\\\* | upload files in multiple commits |\\n| Commits per repo | - | upload multiple files per commit and/or squash history |\\n\\\\_\\\\* Not relevant when using `git` CLI directly\\\\_\\nPlease read the next section to understand better those limits and how to deal with them.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b41eaf3d2ebb270a0162', 'page_content': '## Explanations\\nWhat are we talking about when we say \"large uploads\", and what are their associated limitations? Large uploads can be\\nvery diverse, from repositories with a few huge files (e.g. model weights) to repositories with thousands of small files\\n(e.g. an image dataset). Under the hood, the Hub uses Git to version the data, which has structural implications on what you can do in your repo. If your repo is crossing some of the numbers mentioned in the previous section, \\\\*\\\\*we strongly encourage you to check out [`git-sizer`](https://github.com/github/git-sizer)\\\\*\\\\*,\\nwhich has very detailed documentation about the different factors that will impact your experience. Here is a TL;DR of factors to consider:\\n- \\\\*\\\\*Repository size\\\\*\\\\*: The total size of the data you\\'re planning to upload. There is no hard limit on a Hub repository size. However, if you plan to upload hundreds of GBs or even TBs of data, we would appreciate it if you could let us know in advance so we can better help you if you have any questions during the process. You can contact us at datasets@huggingface.co or on [our Discord](http://hf.co/join/discord). - \\\\*\\\\*Number of files\\\\*\\\\*:\\n- For optimal experience, we recommend keeping the total number of files under 100k. Try merging the data into fewer files if you have more. For example, json files can be merged into a single jsonl file, or large datasets can be exported as Parquet files. - The maximum number of files per folder cannot exceed 10k files per folder. A simple solution is to\\ncreate a repository structure that uses subdirectories. For example, a repo with 1k folders from `000/` to `999/`, each containing at most 1000 files, is already enough. - \\\\*\\\\*File size\\\\*\\\\*: In the case of uploading large files (e.g. model weights), we strongly recommend splitting them \\\\*\\\\*into chunks of around 5GB each\\\\*\\\\*.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. If you have files larger than this, you will need to use Git-LFS.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--754ef2cf-9a22-4887-838d-66d94217f181-0', 'usage_metadata': {'input_tokens': 1862, 'output_tokens': 36, 'total_tokens': 1898, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:14,468 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 36/65 [02:28<01:45,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the title of the paper introducing the ByT5 model?\\n', 'type': 'human', 'id': '7a6aa0f8-7056-435c-9b80-2784d50d0274'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_FD0Q8lgUrGcPHZ4T0Q3RhHPc', 'function': {'arguments': '{\"query\":\"ByT5 model paper title\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--cad8f78a-31bf-47f2-96f9-63d6dc9478e4-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'ByT5 model paper title'}, 'id': 'call_FD0Q8lgUrGcPHZ4T0Q3RhHPc', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 21, 'total_tokens': 383, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# ByT5\\n## Overview\\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\\nKale, Adam Roberts, Colin Raffel.\\n\\nüåé\\n- A notebook on how to [finetune T5 for question answering on a TPU](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5\\\\_on\\\\_TPU.ipynb#scrollTo=QLGiFCDqvuil). üöÄ \\\\*\\\\*Deploy\\\\*\\\\*\\n- A blog post on how to deploy [T5 11B for inference for less than $500](https://www.philschmid.de/deploy-t5-11b). ## T5Config\\n[[autodoc]] T5Config\\n## T5Tokenizer\\n[[autodoc]] T5Tokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## T5TokenizerFast\\n[[autodoc]] T5TokenizerFast\\n\\n## T5Model\\n[[autodoc]] T5Model\\n- forward\\n## T5ForConditionalGeneration\\n[[autodoc]] T5ForConditionalGeneration\\n- forward\\n## T5EncoderModel\\n[[autodoc]] T5EncoderModel\\n- forward\\n## T5ForSequenceClassification\\n[[autodoc]] T5ForSequenceClassification\\n- forward\\n## T5ForQuestionAnswering\\n[[autodoc]] T5ForQuestionAnswering\\n- forward\\n\\n## TFT5Model\\n[[autodoc]] TFT5Model\\n- call\\n## TFT5ForConditionalGeneration\\n[[autodoc]] TFT5ForConditionalGeneration\\n- call\\n## TFT5EncoderModel\\n[[autodoc]] TFT5EncoderModel\\n- call\\n\\n## FlaxT5Model\\n[[autodoc]] FlaxT5Model\\n- \\\\_\\\\_call\\\\_\\\\_\\n- encode\\n- decode\\n## FlaxT5ForConditionalGeneration\\n[[autodoc]] FlaxT5ForConditionalGeneration\\n- \\\\_\\\\_call\\\\_\\\\_\\n- encode\\n- decode\\n## FlaxT5EncoderModel\\n[[autodoc]] FlaxT5EncoderModel\\n- \\\\_\\\\_call\\\\_\\\\_\\n\\n!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# T5\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-t5-blueviolet)](https://huggingface.co/models?filter=t5)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/t5-base)\\n[![Paper page](https://img.shields.io/badge/Paper%20page-1910.10683-green)](https://huggingface.co/papers/1910.10683)\\n\\n## Overview\\nThe T5 model was presented in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) by [Colin Raffel](https://huggingface.co/craffel), Noam Shazeer, [Adam Roberts](https://huggingface.co/adarob), Katherine Lee, Sharan Narang,\\nMichael Matena, Yanqi Zhou, Wei Li, [Peter J.\\n\\n!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# T5v1.1\\n## Overview\\nT5v1.1 was released in the [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released\\\\_checkpoints.md#t511)\\nrepository by Colin Raffel et al. It\\'s an improved version of the original T5 model.', 'type': 'tool', 'name': 'data_retriever', 'id': 'fb440f77-0686-4db1-87b8-333ed12cc473', 'tool_call_id': 'call_FD0Q8lgUrGcPHZ4T0Q3RhHPc', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa9f53e684812d15f5da3', 'page_content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# ByT5\\n## Overview\\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\\nKale, Adam Roberts, Colin Raffel.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab36d3e684812d15f68ae', 'page_content': 'üåé\\n- A notebook on how to [finetune T5 for question answering on a TPU](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5\\\\_on\\\\_TPU.ipynb#scrollTo=QLGiFCDqvuil). üöÄ \\\\*\\\\*Deploy\\\\*\\\\*\\n- A blog post on how to deploy [T5 11B for inference for less than $500](https://www.philschmid.de/deploy-t5-11b). ## T5Config\\n[[autodoc]] T5Config\\n## T5Tokenizer\\n[[autodoc]] T5Tokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## T5TokenizerFast\\n[[autodoc]] T5TokenizerFast\\n\\n## T5Model\\n[[autodoc]] T5Model\\n- forward\\n## T5ForConditionalGeneration\\n[[autodoc]] T5ForConditionalGeneration\\n- forward\\n## T5EncoderModel\\n[[autodoc]] T5EncoderModel\\n- forward\\n## T5ForSequenceClassification\\n[[autodoc]] T5ForSequenceClassification\\n- forward\\n## T5ForQuestionAnswering\\n[[autodoc]] T5ForQuestionAnswering\\n- forward\\n\\n## TFT5Model\\n[[autodoc]] TFT5Model\\n- call\\n## TFT5ForConditionalGeneration\\n[[autodoc]] TFT5ForConditionalGeneration\\n- call\\n## TFT5EncoderModel\\n[[autodoc]] TFT5EncoderModel\\n- call\\n\\n## FlaxT5Model\\n[[autodoc]] FlaxT5Model\\n- \\\\_\\\\_call\\\\_\\\\_\\n- encode\\n- decode\\n## FlaxT5ForConditionalGeneration\\n[[autodoc]] FlaxT5ForConditionalGeneration\\n- \\\\_\\\\_call\\\\_\\\\_\\n- encode\\n- decode\\n## FlaxT5EncoderModel\\n[[autodoc]] FlaxT5EncoderModel\\n- \\\\_\\\\_call\\\\_\\\\_', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3683e684812d15f68a2', 'page_content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# T5\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-t5-blueviolet)](https://huggingface.co/models?filter=t5)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/t5-base)\\n[![Paper page](https://img.shields.io/badge/Paper%20page-1910.10683-green)](https://huggingface.co/papers/1910.10683)\\n\\n## Overview\\nThe T5 model was presented in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) by [Colin Raffel](https://huggingface.co/craffel), Noam Shazeer, [Adam Roberts](https://huggingface.co/adarob), Katherine Lee, Sharan Narang,\\nMichael Matena, Yanqi Zhou, Wei Li, [Peter J.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa5723e684812d15f586b', 'page_content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# T5v1.1\\n## Overview\\nT5v1.1 was released in the [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released\\\\_checkpoints.md#t511)\\nrepository by Colin Raffel et al. It\\'s an improved version of the original T5 model.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models.\"', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--17887879-807c-4eca-9e57-394bc232fa9d-0', 'usage_metadata': {'input_tokens': 1618, 'output_tokens': 30, 'total_tokens': 1648, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:17,550 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 37/65 [02:31<01:37,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the dimension of the feature vector for the base BERT model?\\n', 'type': 'human', 'id': 'ce76e3f7-3aca-4d46-841d-be7957ac7cc1'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_eoiGxLRD175z18Gkrtqo1iOs', 'function': {'arguments': '{\"query\":\"dimension of the feature vector for the base BERT model\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--c8ab5ec5-5628-457b-8ad9-41546cdc62ad-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'dimension of the feature vector for the base BERT model'}, 'id': 'call_eoiGxLRD175z18Gkrtqo1iOs', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 363, 'output_tokens': 26, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# BERT\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\\n\\n## Overview\\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\\'s a\\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\\n\\\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\\narchitecture modifications.\\\\*\\n\\\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\\\*\\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf).\\n\\n|\\n| Transformer Layers: | Number of Transformer blocks. A transformer block transforms a sequence of word representations to a sequence of contextualized words (numbered representations). |\\n| Hidden Size: | Layers of mathematical functions, located between the input and output, that assign weights (to words) to produce a desired result. |\\n| Attention Heads: | The size of a Transformer block. |\\n| Processing: | Type of processing unit used to train the model. |\\n| Length of Training: | Time it took to train the model. Here‚Äôs how many of the above ML architecture parts BERTbase and BERTlarge has:\\n| | Transformer Layers | Hidden Size | Attention Heads | Parameters | Processing | Length of Training |\\n|-----------|--------------------|-------------|-----------------|------------|------------|--------------------|\\n| BERTbase | 12 | 768 | 12 | 110M | 4 TPUs | 4 days |\\n| BERTlarge | 24 | 1024 | 16 | 340M | 16 TPUs | 4 days |\\nLet‚Äôs take a look at how BERTlarge‚Äôs additional layers, attention heads, and parameters have increased its performance across NLP tasks.\\n\\nn this video, we\\'ll study the encoder architecture. An example of a popular encoder-only architecture is BERT, which is the most popular model of its kind. Let\\'s first start by understanding how it works. We\\'ll use a small example, using three words. We use these as inputs, and pass them through the encoder. We retrieve a numerical representation of each word. Here, for example, the encoder converts the three words ‚ÄúWelcome to NYC‚Äù in these three sequences of numbers. The encoder outputs exactly one sequence of numbers per input word. This numerical representation can also be called a \"Feature vector\", or \"Feature tensor\". Let\\'s dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \"to\", isn\\'t the representation of only the \"to\" word. It also takes into account the words around it, which we call the ‚Äúcontext‚Äù.As in, it looks to the left context, the word on the left of the one we\\'re studying (here the word \"Welcome\") and the context on the right (here the word \"NYC\") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \"meaning\" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence.\\n\\nThe original code can be found [here](https://github.com/google-research/bert). ## Usage tips\\n- BERT is a model with absolute position embeddings so it\\'s usually advised to pad the inputs on the right rather than\\nthe left. - BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is\\nefficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. - Corrupts the inputs by using random masking, more precisely, during pretraining, a given percentage of tokens (usually 15%) is masked by:\\n\\\\* a special mask token with probability 0.8\\n\\\\* a random token different from the one masked with probability 0.1\\n\\\\* the same token with probability 0.1\\n- The model must predict the original sentence, but has a second objective: inputs are two sentences A and B (with a separation token in between). With probability 50%, the sentences are consecutive in the corpus, in the remaining 50% they are not related. The model has to predict if the sentences are consecutive or not. ## Resources\\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with BERT.', 'type': 'tool', 'name': 'data_retriever', 'id': 'e440194a-0645-4ec0-85f5-63295e028187', 'tool_call_id': 'call_eoiGxLRD175z18Gkrtqo1iOs', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689abb613e684812d15f71e1', 'page_content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# BERT\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\\n\\n## Overview\\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\\'s a\\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\\n\\\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\\narchitecture modifications.\\\\*\\n\\\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\\\*\\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bf7c666af48523f51dd1', 'page_content': '|\\n| Transformer Layers: | Number of Transformer blocks. A transformer block transforms a sequence of word representations to a sequence of contextualized words (numbered representations). |\\n| Hidden Size: | Layers of mathematical functions, located between the input and output, that assign weights (to words) to produce a desired result. |\\n| Attention Heads: | The size of a Transformer block. |\\n| Processing: | Type of processing unit used to train the model. |\\n| Length of Training: | Time it took to train the model. Here‚Äôs how many of the above ML architecture parts BERTbase and BERTlarge has:\\n| | Transformer Layers | Hidden Size | Attention Heads | Parameters | Processing | Length of Training |\\n|-----------|--------------------|-------------|-----------------|------------|------------|--------------------|\\n| BERTbase | 12 | 768 | 12 | 110M | 4 TPUs | 4 days |\\n| BERTlarge | 24 | 1024 | 16 | 340M | 16 TPUs | 4 days |\\nLet‚Äôs take a look at how BERTlarge‚Äôs additional layers, attention heads, and parameters have increased its performance across NLP tasks.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899ba2eaf3d2ebb270a0877', 'page_content': 'n this video, we\\'ll study the encoder architecture. An example of a popular encoder-only architecture is BERT, which is the most popular model of its kind. Let\\'s first start by understanding how it works. We\\'ll use a small example, using three words. We use these as inputs, and pass them through the encoder. We retrieve a numerical representation of each word. Here, for example, the encoder converts the three words ‚ÄúWelcome to NYC‚Äù in these three sequences of numbers. The encoder outputs exactly one sequence of numbers per input word. This numerical representation can also be called a \"Feature vector\", or \"Feature tensor\". Let\\'s dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \"to\", isn\\'t the representation of only the \"to\" word. It also takes into account the words around it, which we call the ‚Äúcontext‚Äù.As in, it looks to the left context, the word on the left of the one we\\'re studying (here the word \"Welcome\") and the context on the right (here the word \"NYC\") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \"meaning\" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689abb613e684812d15f71e2', 'page_content': \"The original code can be found [here](https://github.com/google-research/bert). ## Usage tips\\n- BERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\\nthe left. - BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is\\nefficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. - Corrupts the inputs by using random masking, more precisely, during pretraining, a given percentage of tokens (usually 15%) is masked by:\\n\\\\* a special mask token with probability 0.8\\n\\\\* a random token different from the one masked with probability 0.1\\n\\\\* the same token with probability 0.1\\n- The model must predict the original sentence, but has a second objective: inputs are two sentences A and B (with a separation token in between). With probability 50%, the sentences are consecutive in the corpus, in the remaining 50% they are not related. The model has to predict if the sentences are consecutive or not. ## Resources\\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with BERT.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The dimension of the feature vector for the base BERT model is 768.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--64b21132-ce20-45e6-a447-e6c5d452d06b-0', 'usage_metadata': {'input_tokens': 1755, 'output_tokens': 16, 'total_tokens': 1771, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:22,158 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 38/65 [02:35<01:42,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What special identifier does the WordPiece Model use for continuing subwords?\\n', 'type': 'human', 'id': '33784b78-32a4-4043-bc72-9cffa9325184'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_aEAcQ7IXVuYFsNkHCnhfC1C7', 'function': {'arguments': '{\"query\":\"WordPiece Model special identifier for continuing subwords\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--00d6373c-1fb9-41d1-ae56-ff5762576959-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'WordPiece Model special identifier for continuing subwords'}, 'id': 'call_aEAcQ7IXVuYFsNkHCnhfC1C7', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 24, 'total_tokens': 386, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'In turn, the model will now be able to make sense of token in different situations. It will understand that the words token, tokens, tokenizing, and tokenization are linked and have a similar meaning. It will also understand that tokenization, modernization, and immunization, which all have the same suffixes, are probably used in the same syntactic situations. Subword-based tokenizers generally have a way to identify which tokens are start of words, and which tokens complete start of words: token as the start of a word. ##ization as completing a word. Here the ## prefix indicates that ization is part of a word rather than the beginning of it. The ## comes from the BERT tokenizer, based on the WordPiece algorithm. Other tokenizers use other prefixes, which can be placed to indicate part of words like seen here, or start of words instead! There are a lot of different algorithms that can be used for subword tokenization, and most models obtaining state-of-the-art results in English today use some kind of subword-tokenization algorithm.\\n\\net\\'s see together what is the training strategy of the WordPiece algorithm and how it performs the tokenization of a text once trained WordPiece is a tokenization algorithm introduced by Google. It is used for example by Bert. To our knowledge, the code of Word Pieces has not been open sourced, so we base our explanations on our own interpretation of the published literature. What is the training strategy of WordPiece? Similarly to the BPE algorithm, WordPiece starts by establishing an initial vocabulary composed of elementary units and then increases this vocabulary to the desired size. To build the initial vocabulary, we divide each word in the training corpus into the sequence of letters that make it up. As you can see, there is a small subtlety: we add a 2 hashtags in front of the letters that do not start a word. By keeping only one occurrence per elementary unit we now have our initial vocabulary. We will list all the existing pairs in our corpus. Once we have this list, we will calculate a score for each of these pairs. As for the BPE algorithm, we will select the pair with the highest score. Taking for example the first pair composed of H and U. The score of a pair is simply equal to the frequency of appearance of the pair divided by the product of the frequency of appearance of the first token by the frequency of appearance of the second token. Thus at a fixed frequency of appearance of the pair, if the subparts of the pair are very frequent in the corpus then this score will be decreased. In our example, the pair \"hu\" appears 4 times, the letter \"h\" 4 times and the letter u 4 times.\\n\\nWordPiece tokenization[[wordpiece-tokenization]]\\nWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It\\'s very similar to BPE in terms of the training, but the actual tokenization is done differently. üí° This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm. ## Training algorithm[[training-algorithm]]\\n‚ö†Ô∏è Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate. Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this:\\n```\\nw ##o ##r ##d\\n```\\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix. Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\\n$$\\\\mathrm{score} = (\\\\mathrm{freq\\\\\\\\_of\\\\\\\\_pair}) / (\\\\mathrm{freq\\\\\\\\_of\\\\\\\\_first\\\\\\\\_element} \\\\times \\\\mathrm{freq\\\\\\\\_of\\\\\\\\_second\\\\\\\\_element})$$\\nBy dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary.\\n\\nWith this kind of tokenizer, we can end up with some pretty large \"vocabularies,\" where a vocabulary is defined by the total number of independent tokens that we have in our corpus. Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word. If we want to completely cover a language with a word-based tokenizer, we\\'ll need to have an identifier for each word in the language, which will generate a huge amount of tokens. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we\\'d need to keep track of that many IDs. Furthermore, words like \"dog\" are represented differently from words like \"dogs\", and the model will initially have no way of knowing that \"dog\" and \"dogs\" are similar: it will identify the two words as unrelated. The same applies to other similar words, like \"run\" and \"running\", which the model will not see as being similar initially. Finally, we need a custom token to represent words that are not in our vocabulary. This is known as the \"unknown\" token, often represented as \"[UNK]\" or \"<unk>\". It\\'s generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn\\'t able to retrieve a sensible representation of a word and you\\'re losing information along the way. The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token. One way to reduce the amount of unknown tokens is to go one level deeper, using a \\\\_character-based\\\\_ tokenizer. ## Character-based[[character-based]]\\nCharacter-based tokenizers split the text into characters, rather than words. This has two primary benefits:\\n- The vocabulary is much smaller. - There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters. But here too some questions arise concerning spaces and punctuation:\\n\\n![An example of character-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg)\\n![An example of character-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg)\\n\\nThis approach isn\\'t perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, it\\'s less meaningful: each character doesn\\'t mean a lot on its own, whereas that is the case with words. However, this again differs according to the language; in Chinese, for example, each character carries more information than a character in a Latin language. Another thing to consider is that we\\'ll end up with a very large amount of tokens to be processed by our model: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters. To get the best of both worlds, we can use a third technique that combines the two approaches: \\\\*subword tokenization\\\\*. ## Subword tokenization[[subword-tokenization]]\\nSubword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. For instance, \"annoyingly\" might be considered a rare word and could be decomposed into \"annoying\" and \"ly\". These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of \"annoyingly\" is kept by the composite meaning of \"annoying\" and \"ly\". Here is an example showing how a subword tokenization algorithm would tokenize the sequence \"Let\\'s do tokenization!\":\\n\\n![A subword tokenization algorithm.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg)\\n![A subword tokenization algorithm.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg)\\n\\nThese subwords end up providing a lot of semantic meaning: for instance, in the example above \"tokenization\" was split into \"token\" and \"ization\", two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word).', 'type': 'tool', 'name': 'data_retriever', 'id': 'cc4eec5f-a904-47a1-94b2-dd1016dac428', 'tool_call_id': 'call_aEAcQ7IXVuYFsNkHCnhfC1C7', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaa303e684812d15f5e18', 'page_content': 'In turn, the model will now be able to make sense of token in different situations. It will understand that the words token, tokens, tokenizing, and tokenization are linked and have a similar meaning. It will also understand that tokenization, modernization, and immunization, which all have the same suffixes, are probably used in the same syntactic situations. Subword-based tokenizers generally have a way to identify which tokens are start of words, and which tokens complete start of words: token as the start of a word. ##ization as completing a word. Here the ## prefix indicates that ization is part of a word rather than the beginning of it. The ## comes from the BERT tokenizer, based on the WordPiece algorithm. Other tokenizers use other prefixes, which can be placed to indicate part of words like seen here, or start of words instead! There are a lot of different algorithms that can be used for subword tokenization, and most models obtaining state-of-the-art results in English today use some kind of subword-tokenization algorithm.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaa393e684812d15f5e23', 'page_content': 'et\\'s see together what is the training strategy of the WordPiece algorithm and how it performs the tokenization of a text once trained WordPiece is a tokenization algorithm introduced by Google. It is used for example by Bert. To our knowledge, the code of Word Pieces has not been open sourced, so we base our explanations on our own interpretation of the published literature. What is the training strategy of WordPiece? Similarly to the BPE algorithm, WordPiece starts by establishing an initial vocabulary composed of elementary units and then increases this vocabulary to the desired size. To build the initial vocabulary, we divide each word in the training corpus into the sequence of letters that make it up. As you can see, there is a small subtlety: we add a 2 hashtags in front of the letters that do not start a word. By keeping only one occurrence per elementary unit we now have our initial vocabulary. We will list all the existing pairs in our corpus. Once we have this list, we will calculate a score for each of these pairs. As for the BPE algorithm, we will select the pair with the highest score. Taking for example the first pair composed of H and U. The score of a pair is simply equal to the frequency of appearance of the pair divided by the product of the frequency of appearance of the first token by the frequency of appearance of the second token. Thus at a fixed frequency of appearance of the pair, if the subparts of the pair are very frequent in the corpus then this score will be decreased. In our example, the pair \"hu\" appears 4 times, the letter \"h\" 4 times and the letter u 4 times.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaf263e684812d15f63e9', 'page_content': 'WordPiece tokenization[[wordpiece-tokenization]]\\nWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It\\'s very similar to BPE in terms of the training, but the actual tokenization is done differently. üí° This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm. ## Training algorithm[[training-algorithm]]\\n‚ö†Ô∏è Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate. Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this:\\n```\\nw ##o ##r ##d\\n```\\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix. Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\\n$$\\\\mathrm{score} = (\\\\mathrm{freq\\\\\\\\_of\\\\\\\\_pair}) / (\\\\mathrm{freq\\\\\\\\_of\\\\\\\\_first\\\\\\\\_element} \\\\times \\\\mathrm{freq\\\\\\\\_of\\\\\\\\_second\\\\\\\\_element})$$\\nBy dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab8b63e684812d15f6f01', 'page_content': 'With this kind of tokenizer, we can end up with some pretty large \"vocabularies,\" where a vocabulary is defined by the total number of independent tokens that we have in our corpus. Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word. If we want to completely cover a language with a word-based tokenizer, we\\'ll need to have an identifier for each word in the language, which will generate a huge amount of tokens. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we\\'d need to keep track of that many IDs. Furthermore, words like \"dog\" are represented differently from words like \"dogs\", and the model will initially have no way of knowing that \"dog\" and \"dogs\" are similar: it will identify the two words as unrelated. The same applies to other similar words, like \"run\" and \"running\", which the model will not see as being similar initially. Finally, we need a custom token to represent words that are not in our vocabulary. This is known as the \"unknown\" token, often represented as \"[UNK]\" or \"<unk>\". It\\'s generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn\\'t able to retrieve a sensible representation of a word and you\\'re losing information along the way. The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token. One way to reduce the amount of unknown tokens is to go one level deeper, using a \\\\_character-based\\\\_ tokenizer. ## Character-based[[character-based]]\\nCharacter-based tokenizers split the text into characters, rather than words. This has two primary benefits:\\n- The vocabulary is much smaller. - There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters. But here too some questions arise concerning spaces and punctuation:\\n\\n![An example of character-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg)\\n![An example of character-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg)\\n\\nThis approach isn\\'t perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, it\\'s less meaningful: each character doesn\\'t mean a lot on its own, whereas that is the case with words. However, this again differs according to the language; in Chinese, for example, each character carries more information than a character in a Latin language. Another thing to consider is that we\\'ll end up with a very large amount of tokens to be processed by our model: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters. To get the best of both worlds, we can use a third technique that combines the two approaches: \\\\*subword tokenization\\\\*. ## Subword tokenization[[subword-tokenization]]\\nSubword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. For instance, \"annoyingly\" might be considered a rare word and could be decomposed into \"annoying\" and \"ly\". These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of \"annoyingly\" is kept by the composite meaning of \"annoying\" and \"ly\". Here is an example showing how a subword tokenization algorithm would tokenize the sequence \"Let\\'s do tokenization!\":\\n\\n![A subword tokenization algorithm.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg)\\n![A subword tokenization algorithm.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg)\\n\\nThese subwords end up providing a lot of semantic meaning: for instance, in the example above \"tokenization\" was split into \"token\" and \"ization\", two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word).', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The WordPiece model uses the prefix `##` to indicate that a subword is part of a larger word rather than the beginning of it. For example, in the tokenization of the word \"word,\" it would be split as `w ##o ##r ##d`, where `##o`, `##r`, and `##d` indicate that these tokens are continuing subwords.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--c1977055-597a-4093-b663-e68303e9ed5d-0', 'usage_metadata': {'input_tokens': 2175, 'output_tokens': 79, 'total_tokens': 2254, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:27,690 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 39/65 [02:41<01:52,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of the üß® Diffusers tutorials?\\n', 'type': 'human', 'id': '331c1b85-0453-4e75-8b40-420cadcfb8ef'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_PnVxlKujqa61sQiPQMhFJIUR', 'function': {'arguments': '{\"query\":\"Diffusers tutorials purpose\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--8e529add-f7a2-4e53-b883-ed433c31874c-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Diffusers tutorials purpose'}, 'id': 'call_PnVxlKujqa61sQiPQMhFJIUR', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 361, 'output_tokens': 19, 'total_tokens': 380, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'üß® Diffusers Experimental\\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Overview\\nWelcome to üß® Diffusers! If you\\'re new to diffusion models and generative AI, and want to learn more, then you\\'ve come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how üß® Diffusers is meant to be used. You\\'ll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you\\'ll learn how to train your own diffusion model to generate what you want. After completing the tutorials, you\\'ll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications. Feel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers! Let\\'s start diffusing!\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n\\n![](https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg)\\n\\n# Diffusers\\nü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\\'re looking for a simple inference solution or want to train your own diffusion model, ü§ó Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). The library has three main components:\\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in ü§ó Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve. - Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality. - Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. [Tutorials\\n\\nLearn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you\\'re using ü§ó Diffusers for the first time!](./tutorials/tutorial_overview)\\n[How-to guides\\n\\nPractical guides for helping you load pipelines, models, and schedulers. You\\'ll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.](./using-diffusers/loading_overview)\\n[Conceptual guides\\n\\nUnderstand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.](./conceptual/philosophy)\\n[Reference\\n\\nTechnical descriptions of how ü§ó Diffusers classes and methods work.](./api/models/overview)\\n\\n!---\\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n# üß® Diffusers Examples\\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\\nfor a variety of use cases involving training or fine-tuning. \\\\*\\\\*Note\\\\*\\\\*: If you are looking for \\\\*\\\\*official\\\\*\\\\* examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines).', 'type': 'tool', 'name': 'data_retriever', 'id': 'ceef70d2-5832-4efb-a9fa-800000b51852', 'tool_call_id': 'call_PnVxlKujqa61sQiPQMhFJIUR', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b6efaf3d2ebb270a043c', 'page_content': 'üß® Diffusers Experimental\\nWe are adding experimental code to support novel applications and usages of the Diffusers library.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa5443e684812d15f5836', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Overview\\nWelcome to üß® Diffusers! If you\\'re new to diffusion models and generative AI, and want to learn more, then you\\'ve come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how üß® Diffusers is meant to be used. You\\'ll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you\\'ll learn how to train your own diffusion model to generate what you want. After completing the tutorials, you\\'ll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications. Feel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers! Let\\'s start diffusing!', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aacf13e684812d15f6180', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n\\n![](https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg)\\n\\n# Diffusers\\nü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\\'re looking for a simple inference solution or want to train your own diffusion model, ü§ó Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). The library has three main components:\\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in ü§ó Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve. - Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality. - Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. [Tutorials\\n\\nLearn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you\\'re using ü§ó Diffusers for the first time!](./tutorials/tutorial_overview)\\n[How-to guides\\n\\nPractical guides for helping you load pipelines, models, and schedulers. You\\'ll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.](./using-diffusers/loading_overview)\\n[Conceptual guides\\n\\nUnderstand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.](./conceptual/philosophy)\\n[Reference\\n\\nTechnical descriptions of how ü§ó Diffusers classes and methods work.](./api/models/overview)', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b29caf3d2ebb2709fedf', 'page_content': '!---\\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n# üß® Diffusers Examples\\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\\nfor a variety of use cases involving training or fine-tuning. \\\\*\\\\*Note\\\\*\\\\*: If you are looking for \\\\*\\\\*official\\\\*\\\\* examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines).', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The purpose of the üß® Diffusers tutorials is to provide a gentle introduction to diffusion models and help users understand the fundamentals of the library. These beginner-friendly tutorials are designed to teach users how to use a pipeline for inference to rapidly generate outputs, as well as how to deconstruct that pipeline to understand the library as a modular toolbox for building their own diffusion systems. After completing the tutorials, users will have gained the necessary skills to explore the library on their own and apply it to their own projects and applications.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f1aec7cd-9e1a-4c2c-9c70-95fea47e4da7-0', 'usage_metadata': {'input_tokens': 1481, 'output_tokens': 103, 'total_tokens': 1584, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:31,007 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 40/65 [02:44<01:40,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': \"What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\\n\", 'type': 'human', 'id': '7cc10e95-3c87-4b5a-a304-a1e4c00afe36'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_zRd5CLrKnUBdB4T46qf7IKRU', 'function': {'arguments': '{\"query\":\"default setting for the allow_flagging parameter in Gradio\\'s Interface\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f0514336-c54e-4db6-bb0a-69c1480a32ba-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': \"default setting for the allow_flagging parameter in Gradio's Interface\"}, 'id': 'call_zRd5CLrKnUBdB4T46qf7IKRU', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 369, 'output_tokens': 28, 'total_tokens': 397, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Using Flagging\\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\\nTags: FLAGGING, DATA\\n## Introduction\\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \"hard\" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust. Gradio simplifies the collection of this data by including a \\\\*\\\\*Flag\\\\*\\\\* button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`. ## The \\\\*\\\\*Flag\\\\*\\\\* button in `gradio.Interface`\\nFlagging with Gradio\\'s `Interface` is especially easy. By default, underneath the output components, there is a button marked \\\\*\\\\*Flag\\\\*\\\\*. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file. There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail. - `allow\\\\_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`. - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked. - `auto`: users will not see a button to flag, but every sample will be flagged automatically. - `never`: users will not see a button to flag, and no sample will be flagged. - `flagging\\\\_options`: this parameter can be either `None` (default) or a list of strings. - If `None`, then the user simply clicks on the \\\\*\\\\*Flag\\\\*\\\\* button and no additional options are shown. - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled \\\\*\\\\*Flag as Incorrect\\\\*\\\\* and \\\\*\\\\*Flag as Ambiguous\\\\*\\\\* appear. This only applies if `allow\\\\_flagging` is `\"manual\"`. - The chosen option is then logged along with the input and output. - `flagging\\\\_dir`: this parameter takes a string. - It represents what to name the directory where flagged data is stored. - `flagging\\\\_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\\n- Using this parameter allows you to write custom code that gets run when the flag button is clicked\\n- By default, this is set to an instance of `gr.CSVLogger`\\n- One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset.\\n\\n- `description`: which accepts text, markdown or HTML and places it right under the title. - `article`: which also accepts text, markdown or HTML and places it below the interface. ![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)\\nIf you\\'re using the `Blocks` API instead, you can insert text, markdown, or HTML anywhere using the `gr.Markdown(...)` or `gr.HTML(...)` components, with descriptive content inside the `Component` constructor. Another useful keyword argument is `label=`, which is present in every `Component`. This modifies the label text at the top of each `Component`. You can also add the `info=` keyword argument to form elements like `Textbox` or `Radio` to provide further information on their usage. ```python\\ngr.Number(label=\\'Age\\', info=\\'In years, must be greater than 0\\')\\n```\\n## Flagging\\nBy default, an `Interface` will have \"Flag\" button. When a user testing your `Interface` sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the `flagging\\\\_dir=` argument to the `Interface` constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:\\n```directory\\n+-- calculator.py\\n+-- flagged/\\n| +-- logs.csv\\n```\\n\\\\_flagged/logs.csv\\\\_\\n```csv\\nnum1,operation,num2,Output\\n5,add,7,12\\n6,subtract,1.5,4.5\\n```\\nWith the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:\\n```directory\\n+-- sepia.py\\n+-- flagged/\\n| +-- logs.csv\\n| +-- im/\\n| | +-- 0.png\\n| | +-- 1.png\\n| +-- Output/\\n| | +-- 0.png\\n| | +-- 1.png\\n```\\n\\\\_flagged/logs.csv\\\\_\\n```csv\\nim,Output\\nim/0.png,Output/0.png\\nim/1.png,Output/1.png\\n```\\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging\\\\_options` argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV.\\n\\nFor example, on Hugging Face\\nSpaces, developers typically don\\'t have access to the underlying ephemeral machine hosting the Gradio\\ndemo. That\\'s why, by default, flagging is turned off in Hugging Face Space. However,\\nyou may want to do something else with the flagged data. We\\'ve made this super easy with the `flagging\\\\_callback` parameter. For example, below we\\'re going to pipe flagged data from our calculator example into a Hugging Face Dataset, e.g. so that we can build a \"crowd-sourced\" dataset:\\n```python\\nimport os\\nHF\\\\_TOKEN = os.getenv(\\'HF\\\\_TOKEN\\')\\nhf\\\\_writer = gr.HuggingFaceDatasetSaver(HF\\\\_TOKEN, \"crowdsourced-calculator-demo\")\\niface = gr.Interface(\\ncalculator,\\n[\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\\n\"number\",\\ndescription=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\\nallow\\\\_flagging=\"manual\",\\nflagging\\\\_options=[\"wrong sign\", \"off by one\", \"other\"],\\nflagging\\\\_callback=hf\\\\_writer\\n)\\niface.launch()\\n```\\nNotice that we define our own\\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\\nthe name of a dataset we\\'d like to save samples to. In addition, we also set `allow\\\\_flagging=\"manual\"`\\nbecause on Hugging Face Spaces, `allow\\\\_flagging` is set to `\"never\"` by default. Here\\'s our demo:\\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo). ![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)\\nWe created the `gradio.HuggingFaceDatasetSaver` class, but you can pass your own custom class as long as it inherits from `FLaggingCallback` defined in [this file](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py). If you create a cool callback, contribute it to the repo! ## Flagging with Blocks\\nWhat about if you are using `gradio.Blocks`? On one hand, you have even more flexibility\\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\\nand assign that using the built-in events in Blocks.\\n\\n- \\\\*\\\\*Files that you explicitly allow via the `allowed\\\\_paths` parameter in `launch()`\\\\*\\\\*. This parameter allows you to pass in a list of additional directories or exact filepaths you\\'d like to allow users to have access to. (By default, this parameter is an empty list). Gradio DOES NOT ALLOW access to:\\n- \\\\*\\\\*Files that you explicitly block via the `blocked\\\\_paths` parameter in `launch()`\\\\*\\\\*. You can pass in a list of additional directories or exact filepaths to the `blocked\\\\_paths` parameter in `launch()`. This parameter takes precedence over the files that Gradio exposes by default or by the `allowed\\\\_paths`. - \\\\*\\\\*Any other paths on the host machine\\\\*\\\\*. Users should NOT be able to access other arbitrary paths on the host. Please make sure you are running the latest version of `gradio` for these security settings to apply.', 'type': 'tool', 'name': 'data_retriever', 'id': '2eb8549f-acea-4662-bb20-8fd59f33edff', 'tool_call_id': 'call_zRd5CLrKnUBdB4T46qf7IKRU', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c32e666af48523f52267', 'page_content': 'Using Flagging\\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\\nTags: FLAGGING, DATA\\n## Introduction\\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \"hard\" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust. Gradio simplifies the collection of this data by including a \\\\*\\\\*Flag\\\\*\\\\* button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`. ## The \\\\*\\\\*Flag\\\\*\\\\* button in `gradio.Interface`\\nFlagging with Gradio\\'s `Interface` is especially easy. By default, underneath the output components, there is a button marked \\\\*\\\\*Flag\\\\*\\\\*. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file. There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail. - `allow\\\\_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`. - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked. - `auto`: users will not see a button to flag, but every sample will be flagged automatically. - `never`: users will not see a button to flag, and no sample will be flagged. - `flagging\\\\_options`: this parameter can be either `None` (default) or a list of strings. - If `None`, then the user simply clicks on the \\\\*\\\\*Flag\\\\*\\\\* button and no additional options are shown. - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled \\\\*\\\\*Flag as Incorrect\\\\*\\\\* and \\\\*\\\\*Flag as Ambiguous\\\\*\\\\* appear. This only applies if `allow\\\\_flagging` is `\"manual\"`. - The chosen option is then logged along with the input and output. - `flagging\\\\_dir`: this parameter takes a string. - It represents what to name the directory where flagged data is stored. - `flagging\\\\_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\\n- Using this parameter allows you to write custom code that gets run when the flag button is clicked\\n- By default, this is set to an instance of `gr.CSVLogger`\\n- One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aac9b3e684812d15f60ff', 'page_content': '- `description`: which accepts text, markdown or HTML and places it right under the title. - `article`: which also accepts text, markdown or HTML and places it below the interface. ![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)\\nIf you\\'re using the `Blocks` API instead, you can insert text, markdown, or HTML anywhere using the `gr.Markdown(...)` or `gr.HTML(...)` components, with descriptive content inside the `Component` constructor. Another useful keyword argument is `label=`, which is present in every `Component`. This modifies the label text at the top of each `Component`. You can also add the `info=` keyword argument to form elements like `Textbox` or `Radio` to provide further information on their usage. ```python\\ngr.Number(label=\\'Age\\', info=\\'In years, must be greater than 0\\')\\n```\\n## Flagging\\nBy default, an `Interface` will have \"Flag\" button. When a user testing your `Interface` sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the `flagging\\\\_dir=` argument to the `Interface` constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:\\n```directory\\n+-- calculator.py\\n+-- flagged/\\n| +-- logs.csv\\n```\\n\\\\_flagged/logs.csv\\\\_\\n```csv\\nnum1,operation,num2,Output\\n5,add,7,12\\n6,subtract,1.5,4.5\\n```\\nWith the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:\\n```directory\\n+-- sepia.py\\n+-- flagged/\\n| +-- logs.csv\\n| +-- im/\\n| | +-- 0.png\\n| | +-- 1.png\\n| +-- Output/\\n| | +-- 0.png\\n| | +-- 1.png\\n```\\n\\\\_flagged/logs.csv\\\\_\\n```csv\\nim,Output\\nim/0.png,Output/0.png\\nim/1.png,Output/1.png\\n```\\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging\\\\_options` argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c32e666af48523f52269', 'page_content': 'For example, on Hugging Face\\nSpaces, developers typically don\\'t have access to the underlying ephemeral machine hosting the Gradio\\ndemo. That\\'s why, by default, flagging is turned off in Hugging Face Space. However,\\nyou may want to do something else with the flagged data. We\\'ve made this super easy with the `flagging\\\\_callback` parameter. For example, below we\\'re going to pipe flagged data from our calculator example into a Hugging Face Dataset, e.g. so that we can build a \"crowd-sourced\" dataset:\\n```python\\nimport os\\nHF\\\\_TOKEN = os.getenv(\\'HF\\\\_TOKEN\\')\\nhf\\\\_writer = gr.HuggingFaceDatasetSaver(HF\\\\_TOKEN, \"crowdsourced-calculator-demo\")\\niface = gr.Interface(\\ncalculator,\\n[\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\\n\"number\",\\ndescription=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\\nallow\\\\_flagging=\"manual\",\\nflagging\\\\_options=[\"wrong sign\", \"off by one\", \"other\"],\\nflagging\\\\_callback=hf\\\\_writer\\n)\\niface.launch()\\n```\\nNotice that we define our own\\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\\nthe name of a dataset we\\'d like to save samples to. In addition, we also set `allow\\\\_flagging=\"manual\"`\\nbecause on Hugging Face Spaces, `allow\\\\_flagging` is set to `\"never\"` by default. Here\\'s our demo:\\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo). ![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)\\nWe created the `gradio.HuggingFaceDatasetSaver` class, but you can pass your own custom class as long as it inherits from `FLaggingCallback` defined in [this file](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py). If you create a cool callback, contribute it to the repo! ## Flagging with Blocks\\nWhat about if you are using `gradio.Blocks`? On one hand, you have even more flexibility\\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\\nand assign that using the built-in events in Blocks.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aba773e684812d15f70f9', 'page_content': \"- \\\\*\\\\*Files that you explicitly allow via the `allowed\\\\_paths` parameter in `launch()`\\\\*\\\\*. This parameter allows you to pass in a list of additional directories or exact filepaths you'd like to allow users to have access to. (By default, this parameter is an empty list). Gradio DOES NOT ALLOW access to:\\n- \\\\*\\\\*Files that you explicitly block via the `blocked\\\\_paths` parameter in `launch()`\\\\*\\\\*. You can pass in a list of additional directories or exact filepaths to the `blocked\\\\_paths` parameter in `launch()`. This parameter takes precedence over the files that Gradio exposes by default or by the `allowed\\\\_paths`. - \\\\*\\\\*Any other paths on the host machine\\\\*\\\\*. Users should NOT be able to access other arbitrary paths on the host. Please make sure you are running the latest version of `gradio` for these security settings to apply.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The default setting for the `allow_flagging` parameter in Gradio\\'s `Interface` is `\"manual\"`.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--776d8ab1-2b32-434f-9592-776f3d12d5c0-0', 'usage_metadata': {'input_tokens': 2359, 'output_tokens': 23, 'total_tokens': 2382, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:34,388 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 41/65 [02:47<01:31,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Where can the full code for the Stable Diffusion demo be found?\\n', 'type': 'human', 'id': '596a25b7-d1d2-45cc-a36e-93be4eda34c6'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_cWVqktzOO0ocCAzwsmO9wKmc', 'function': {'arguments': '{\"query\":\"Stable Diffusion demo full code\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f5849aaf-1952-418d-8ba0-5c1df23027df-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Stable Diffusion demo full code'}, 'id': 'call_cWVqktzOO0ocCAzwsmO9wKmc', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 21, 'total_tokens': 383, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'ote: This is a simplified version of the code needed to create the Stable Diffusion demo.\\n\\nGradio Demo: stable-diffusion\\n### Note: This is a simplified version of the code needed to create the Stable Diffusion demo.\\n\\nCheck out the Stable Diffusion demo for more: https://hf.co/spaces/stabilityai/stable-diffusion/\\n\\nSee full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main', 'type': 'tool', 'name': 'data_retriever', 'id': '3854ddbf-4ef5-4ac5-b160-32d2d13718c9', 'tool_call_id': 'call_cWVqktzOO0ocCAzwsmO9wKmc', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bbccaf3d2ebb270a0a63', 'page_content': 'ote: This is a simplified version of the code needed to create the Stable Diffusion demo.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899be68666af48523f51c90', 'page_content': 'Gradio Demo: stable-diffusion\\n### Note: This is a simplified version of the code needed to create the Stable Diffusion demo.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bfe6666af48523f51e43', 'page_content': 'Check out the Stable Diffusion demo for more: https://hf.co/spaces/stabilityai/stable-diffusion/', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bbccaf3d2ebb270a0a64', 'page_content': 'See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The full code for the Stable Diffusion demo can be found at this link: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--8141110b-0d19-486f-b593-c5192605a871-0', 'usage_metadata': {'input_tokens': 368, 'output_tokens': 33, 'total_tokens': 401, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:38,310 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 42/65 [02:51<01:28,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What transformation does the FNet model use to replace the self-attention layer in a BERT model?\\n', 'type': 'human', 'id': 'c04006b7-f215-481c-a81f-a549ef08b17a'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_5txv6TH0oyl9DSBZK70n5pPh', 'function': {'arguments': '{\"query\":\"FNet model self-attention layer replacement BERT\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--457e755d-1395-4300-bf49-6bcaf130a160-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'FNet model self-attention layer replacement BERT'}, 'id': 'call_5txv6TH0oyl9DSBZK70n5pPh', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 369, 'output_tokens': 25, 'total_tokens': 394, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# FNet\\n## Overview\\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\\npaper is the following:\\n\\\\*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\\noutperform Transformer counterparts.\\\\*\\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f\\\\_net). ## Usage tips\\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\\nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\\nsequence length for fine-tuning and inference. ## Resources\\n- [Text classification task guide](../tasks/sequence\\\\_classification)\\n- [Token classification task guide](../tasks/token\\\\_classification)\\n- [Question answering task guide](../tasks/question\\\\_answering)\\n- [Masked language modeling task guide](../tasks/masked\\\\_language\\\\_modeling)\\n- [Multiple choice task guide](../tasks/multiple\\\\_choice)\\n## FNetConfig\\n[[autodoc]] FNetConfig\\n## FNetTokenizer\\n[[autodoc]] FNetTokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## FNetTokenizerFast\\n[[autodoc]] FNetTokenizerFast\\n## FNetModel\\n[[autodoc]] FNetModel\\n- forward\\n## FNetForPreTraining\\n[[autodoc]] FNetForPreTraining\\n- forward\\n## FNetForMaskedLM\\n[[autodoc]] FNetForMaskedLM\\n- forward\\n## FNetForNextSentencePrediction\\n[[autodoc]] FNetForNextSentencePrediction\\n- forward\\n## FNetForSequenceClassification\\n[[autodoc]] FNetForSequenceClassification\\n- forward\\n## FNetForMultipleChoice\\n[[autodoc]] FNetForMultipleChoice\\n- forward\\n## FNetForTokenClassification\\n[[autodoc]] FNetForTokenClassification\\n- forward\\n## FNetForQuestionAnswering\\n[[autodoc]] FNetForQuestionAnswering\\n- forward\\n\\n!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# FocalNet\\n## Overview\\nThe FocalNet model was proposed in [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. FocalNets completely replace self-attention (used in models like [ViT](vit) and [Swin](swin)) by a focal modulation mechanism for modeling token interactions in vision. The authors claim that FocalNets outperform self-attention based models with similar computational costs on the tasks of image classification, object detection, and segmentation. The abstract from the paper is the following:\\n\\\\*We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its\\ncontent, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show FocalNets outperform the state-of-the-art SA counterparts (e.g., Swin and Focal Transformers) with similar computational costs on the tasks of image classification, object detection, and segmentation. Specifically, FocalNets with tiny and base size achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K in 224 resolution, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224 and 384, respectively. When transferred to downstream tasks, FocalNets exhibit clear superiority. For object detection with Mask R-CNN, FocalNet base trained with 1\\\\times outperforms the Swin counterpart by 2.1 points and already surpasses Swin trained with 3\\\\times schedule (49.0 v.s. 48.5). For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2.4, and beats Swin at multi-scale (50.5 v.s. 49.7). Using large FocalNet and Mask2former, we achieve 58.5 mIoU for ADE20K semantic segmentation, and 57.9 PQ for COCO Panoptic Segmentation. Using huge FocalNet and DINO, we achieved 64.3 and 64.4 mAP on COCO minival and test-dev, respectively, establishing new SoTA on top of much larger attention-based models like Swinv2-G and BEIT-3.\\\\*\\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\\n\\n!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# BERT\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\\n\\n## Overview\\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\\'s a\\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\\n\\\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\\narchitecture modifications.\\\\*\\n\\\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\\\*\\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf).\\n\\nThe original code can be found [here](https://github.com/google-research/bert). ## Usage tips\\n- BERT is a model with absolute position embeddings so it\\'s usually advised to pad the inputs on the right rather than\\nthe left. - BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is\\nefficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. - Corrupts the inputs by using random masking, more precisely, during pretraining, a given percentage of tokens (usually 15%) is masked by:\\n\\\\* a special mask token with probability 0.8\\n\\\\* a random token different from the one masked with probability 0.1\\n\\\\* the same token with probability 0.1\\n- The model must predict the original sentence, but has a second objective: inputs are two sentences A and B (with a separation token in between). With probability 50%, the sentences are consecutive in the corpus, in the remaining 50% they are not related. The model has to predict if the sentences are consecutive or not. ## Resources\\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with BERT.', 'type': 'tool', 'name': 'data_retriever', 'id': 'f8388cf2-7c23-48bd-a246-882ce8b7ea2f', 'tool_call_id': 'call_5txv6TH0oyl9DSBZK70n5pPh', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c123666af48523f51fc0', 'page_content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# FNet\\n## Overview\\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\\npaper is the following:\\n\\\\*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\\noutperform Transformer counterparts.\\\\*\\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f\\\\_net). ## Usage tips\\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\\nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\\nsequence length for fine-tuning and inference. ## Resources\\n- [Text classification task guide](../tasks/sequence\\\\_classification)\\n- [Token classification task guide](../tasks/token\\\\_classification)\\n- [Question answering task guide](../tasks/question\\\\_answering)\\n- [Masked language modeling task guide](../tasks/masked\\\\_language\\\\_modeling)\\n- [Multiple choice task guide](../tasks/multiple\\\\_choice)\\n## FNetConfig\\n[[autodoc]] FNetConfig\\n## FNetTokenizer\\n[[autodoc]] FNetTokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## FNetTokenizerFast\\n[[autodoc]] FNetTokenizerFast\\n## FNetModel\\n[[autodoc]] FNetModel\\n- forward\\n## FNetForPreTraining\\n[[autodoc]] FNetForPreTraining\\n- forward\\n## FNetForMaskedLM\\n[[autodoc]] FNetForMaskedLM\\n- forward\\n## FNetForNextSentencePrediction\\n[[autodoc]] FNetForNextSentencePrediction\\n- forward\\n## FNetForSequenceClassification\\n[[autodoc]] FNetForSequenceClassification\\n- forward\\n## FNetForMultipleChoice\\n[[autodoc]] FNetForMultipleChoice\\n- forward\\n## FNetForTokenClassification\\n[[autodoc]] FNetForTokenClassification\\n- forward\\n## FNetForQuestionAnswering\\n[[autodoc]] FNetForQuestionAnswering\\n- forward', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b4b9af3d2ebb270a01f8', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# FocalNet\\n## Overview\\nThe FocalNet model was proposed in [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. FocalNets completely replace self-attention (used in models like [ViT](vit) and [Swin](swin)) by a focal modulation mechanism for modeling token interactions in vision. The authors claim that FocalNets outperform self-attention based models with similar computational costs on the tasks of image classification, object detection, and segmentation. The abstract from the paper is the following:\\n\\\\*We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its\\ncontent, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show FocalNets outperform the state-of-the-art SA counterparts (e.g., Swin and Focal Transformers) with similar computational costs on the tasks of image classification, object detection, and segmentation. Specifically, FocalNets with tiny and base size achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K in 224 resolution, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224 and 384, respectively. When transferred to downstream tasks, FocalNets exhibit clear superiority. For object detection with Mask R-CNN, FocalNet base trained with 1\\\\times outperforms the Swin counterpart by 2.1 points and already surpasses Swin trained with 3\\\\times schedule (49.0 v.s. 48.5). For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2.4, and beats Swin at multi-scale (50.5 v.s. 49.7). Using large FocalNet and Mask2former, we achieve 58.5 mIoU for ADE20K semantic segmentation, and 57.9 PQ for COCO Panoptic Segmentation. Using huge FocalNet and DINO, we achieved 64.3 and 64.4 mAP on COCO minival and test-dev, respectively, establishing new SoTA on top of much larger attention-based models like Swinv2-G and BEIT-3.\\\\*\\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689abb613e684812d15f71e1', 'page_content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# BERT\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\\n\\n## Overview\\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\\'s a\\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\\n\\\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\\narchitecture modifications.\\\\*\\n\\\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\\\*\\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689abb613e684812d15f71e2', 'page_content': \"The original code can be found [here](https://github.com/google-research/bert). ## Usage tips\\n- BERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\\nthe left. - BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is\\nefficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. - Corrupts the inputs by using random masking, more precisely, during pretraining, a given percentage of tokens (usually 15%) is masked by:\\n\\\\* a special mask token with probability 0.8\\n\\\\* a random token different from the one masked with probability 0.1\\n\\\\* the same token with probability 0.1\\n- The model must predict the original sentence, but has a second objective: inputs are two sentences A and B (with a separation token in between). With probability 50%, the sentences are consecutive in the corpus, in the remaining 50% they are not related. The model has to predict if the sentences are consecutive or not. ## Resources\\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with BERT.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The FNet model replaces the self-attention layer in a BERT model with a Fourier transform, which returns only the real parts of the transform.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--0e1238b9-e28b-45a3-a713-a39f30c4460f-0', 'usage_metadata': {'input_tokens': 2924, 'output_tokens': 30, 'total_tokens': 2954, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:41,676 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 43/65 [02:55<01:21,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': \"What type of test should typically accompany a bug fix in Gradio's testing strategy?\\n\", 'type': 'human', 'id': 'b7f5681c-3f86-4fc5-b36d-c6af64992d7b'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_ktQf8szL4SYHJKVGjJ6noLTF', 'function': {'arguments': '{\"query\":\"Gradio testing strategy bug fix\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--b1cd36e6-834b-45da-965e-8d8a048073ec-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Gradio testing strategy bug fix'}, 'id': 'call_ktQf8szL4SYHJKVGjJ6noLTF', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 21, 'total_tokens': 386, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Test Strategy\\nVery brief, mildly aspirational test strategy document. This isn\\'t where we are but it is where we want to get to. This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Objectives\\nThe purposes of all testing activities on Gradio fit one of the following objectives:\\n1. Ensure that the Gradio library functions as we expect it to. 2. Enable the maintenance team to quickly identify both the presence and source of defects. 3. Prevent regressions, i.e. if we fix something it should stay fixed. 4. Improve the quality of the codebase in order to ease maintenance efforts. 5. Reduce the amount of manual testing required. ## Scope\\nTesting is always a tradeoff. We can\\'t cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas. We should not focus on code coverage but on test coverage following the below criteria:\\n- The documented Gradio API (that\\'s the bit that users interact with via python) should be tested thoroughly. (1)\\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\\n## Types of testing\\nOur tests will broadly fall into one of three categories:\\n- Static Quality checks\\n- Dynamic \\'Code\\' tests\\n- Dynamic Functional tests\\n### Static Quality checks\\nStatic quality checks are generally very fast to run and do not require building the code base.\\n\\ngradio\\n## 4.11.0\\n### Features\\n- [#6842](https://github.com/gradio-app/gradio/pull/6842) [`846d52d`](https://github.com/gradio-app/gradio/commit/846d52d1c92d429077382ce494eea27fd062d9f6) - Fix md highlight. Thanks [@pngwn](https://github.com/pngwn)! - [#6831](https://github.com/gradio-app/gradio/pull/6831) [`f3abde8`](https://github.com/gradio-app/gradio/commit/f3abde80884d96ad69b825020c46486d9dd5cac5) - Add an option to enable header links for markdown. Thanks [@pngwn](https://github.com/pngwn)! - [#6814](https://github.com/gradio-app/gradio/pull/6814) [`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d) - Refactor queue so that there are separate queues for each concurrency id. Thanks [@aliabid94](https://github.com/aliabid94)! - [#6809](https://github.com/gradio-app/gradio/pull/6809) [`1401d99`](https://github.com/gradio-app/gradio/commit/1401d99ade46d87da75b5f5808a3354c49f1d1ea) - Fix `ImageEditor` interaction story. Thanks [@hannahblair](https://github.com/hannahblair)! - [#6803](https://github.com/gradio-app/gradio/pull/6803) [`77c9003`](https://github.com/gradio-app/gradio/commit/77c900311e2ba37b8f849ce088ceb48aa196af18) - Fixes issue 5781: Enables specifying a caching directory for Examples. Thanks [@cswamy](https://github.com/cswamy)! - [#6823](https://github.com/gradio-app/gradio/pull/6823) [`67a2b7f`](https://github.com/gradio-app/gradio/commit/67a2b7f12cb06355fcc41e40d47e8b2ad211d7d1) - Fixed duplicate word (\"this this\"). Thanks [@Cassini-chris](https://github.com/Cassini-chris)!\\n\\ngradio\\n## 4.11.0\\n### Features\\n- [#6842](https://github.com/gradio-app/gradio/pull/6842) [`846d52d`](https://github.com/gradio-app/gradio/commit/846d52d1c92d429077382ce494eea27fd062d9f6) - Fix md highlight. Thanks [@pngwn](https://github.com/pngwn)! - [#6831](https://github.com/gradio-app/gradio/pull/6831) [`f3abde8`](https://github.com/gradio-app/gradio/commit/f3abde80884d96ad69b825020c46486d9dd5cac5) - Add an option to enable header links for markdown. Thanks [@pngwn](https://github.com/pngwn)! - [#6814](https://github.com/gradio-app/gradio/pull/6814) [`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d) - Refactor queue so that there are separate queues for each concurrency id. Thanks [@aliabid94](https://github.com/aliabid94)! - [#6809](https://github.com/gradio-app/gradio/pull/6809) [`1401d99`](https://github.com/gradio-app/gradio/commit/1401d99ade46d87da75b5f5808a3354c49f1d1ea) - Fix `ImageEditor` interaction story. Thanks [@hannahblair](https://github.com/hannahblair)! - [#6803](https://github.com/gradio-app/gradio/pull/6803) [`77c9003`](https://github.com/gradio-app/gradio/commit/77c900311e2ba37b8f849ce088ceb48aa196af18) - Fixes issue 5781: Enables specifying a caching directory for Examples. Thanks [@cswamy](https://github.com/cswamy)! - [#6823](https://github.com/gradio-app/gradio/pull/6823) [`67a2b7f`](https://github.com/gradio-app/gradio/commit/67a2b7f12cb06355fcc41e40d47e8b2ad211d7d1) - Fixed duplicate word (\"this this\"). Thanks [@Cassini-chris](https://github.com/Cassini-chris)!\\n\\ngradio\\\\_test\\n## 0.3.3\\n### Patch Changes\\n- Updated dependencies [[`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d), [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:\\n- @gradio/statustracker@0.4.3\\n- @gradio/atoms@0.4.1\\n## 0.3.2\\n### Patch Changes\\n- Updated dependencies [[`4d1cbbc`](https://github.com/gradio-app/gradio/commit/4d1cbbcf30833ef1de2d2d2710c7492a379a9a00)]:\\n- @gradio/atoms@0.4.0\\n- @gradio/statustracker@0.4.2\\n## 0.3.1\\n### Patch Changes\\n- Updated dependencies []:\\n- @gradio/atoms@0.3.1\\n- @gradio/statustracker@0.4.1\\n## 0.3.0\\n### Features\\n- [#6532](https://github.com/gradio-app/gradio/pull/6532) [`96290d304`](https://github.com/gradio-app/gradio/commit/96290d304a61064b52c10a54b2feeb09ca007542) - tweak deps. Thanks [@pngwn](https://github.com/pngwn)! ## 0.2.3\\n### Patch Changes\\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8)]:\\n- @gradio/atoms@0.3.0\\n- @gradio/statustracker@0.4.0\\n## 0.2.2\\n### Patch Changes\\n- Updated dependencies [[`f816136a0`](https://github.com/gradio-app/gradio/commit/f816136a039fa6011be9c4fb14f573e4050a681a)]:\\n- @gradio/atoms@0.2.2\\n- @gradio/statustracker@0.3.2\\n## 0.2.1\\n### Patch Changes\\n- Updated dependencies [[`3cdeabc68`](https://github.com/gradio-app/gradio/commit/3cdeabc6843000310e1a9e1d17190ecbf3bbc780), [`fad92c29d`](https://github.com/gradio-app/gradio/commit/fad92c29dc1f5cd84341aae417c495b33e01245f)]:\\n- @gradio/atoms@0.2.1\\n- @gradio/statustracker@0.3.1\\n## 0.2.0\\n### Features\\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - fix cc build.', 'type': 'tool', 'name': 'data_retriever', 'id': 'e87f94e1-af86-4504-ba1c-3c99cd7224a1', 'tool_call_id': 'call_ktQf8szL4SYHJKVGjJ6noLTF', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab1bf3e684812d15f66cb', 'page_content': \"Test Strategy\\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to. This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Objectives\\nThe purposes of all testing activities on Gradio fit one of the following objectives:\\n1. Ensure that the Gradio library functions as we expect it to. 2. Enable the maintenance team to quickly identify both the presence and source of defects. 3. Prevent regressions, i.e. if we fix something it should stay fixed. 4. Improve the quality of the codebase in order to ease maintenance efforts. 5. Reduce the amount of manual testing required. ## Scope\\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas. We should not focus on code coverage but on test coverage following the below criteria:\\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\\n## Types of testing\\nOur tests will broadly fall into one of three categories:\\n- Static Quality checks\\n- Dynamic 'Code' tests\\n- Dynamic Functional tests\\n### Static Quality checks\\nStatic quality checks are generally very fast to run and do not require building the code base.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b387af3d2ebb270a0027', 'page_content': 'gradio\\n## 4.11.0\\n### Features\\n- [#6842](https://github.com/gradio-app/gradio/pull/6842) [`846d52d`](https://github.com/gradio-app/gradio/commit/846d52d1c92d429077382ce494eea27fd062d9f6) - Fix md highlight. Thanks [@pngwn](https://github.com/pngwn)! - [#6831](https://github.com/gradio-app/gradio/pull/6831) [`f3abde8`](https://github.com/gradio-app/gradio/commit/f3abde80884d96ad69b825020c46486d9dd5cac5) - Add an option to enable header links for markdown. Thanks [@pngwn](https://github.com/pngwn)! - [#6814](https://github.com/gradio-app/gradio/pull/6814) [`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d) - Refactor queue so that there are separate queues for each concurrency id. Thanks [@aliabid94](https://github.com/aliabid94)! - [#6809](https://github.com/gradio-app/gradio/pull/6809) [`1401d99`](https://github.com/gradio-app/gradio/commit/1401d99ade46d87da75b5f5808a3354c49f1d1ea) - Fix `ImageEditor` interaction story. Thanks [@hannahblair](https://github.com/hannahblair)! - [#6803](https://github.com/gradio-app/gradio/pull/6803) [`77c9003`](https://github.com/gradio-app/gradio/commit/77c900311e2ba37b8f849ce088ceb48aa196af18) - Fixes issue 5781: Enables specifying a caching directory for Examples. Thanks [@cswamy](https://github.com/cswamy)! - [#6823](https://github.com/gradio-app/gradio/pull/6823) [`67a2b7f`](https://github.com/gradio-app/gradio/commit/67a2b7f12cb06355fcc41e40d47e8b2ad211d7d1) - Fixed duplicate word (\"this this\"). Thanks [@Cassini-chris](https://github.com/Cassini-chris)!', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab6a03e684812d15f6c78', 'page_content': 'gradio\\n## 4.11.0\\n### Features\\n- [#6842](https://github.com/gradio-app/gradio/pull/6842) [`846d52d`](https://github.com/gradio-app/gradio/commit/846d52d1c92d429077382ce494eea27fd062d9f6) - Fix md highlight. Thanks [@pngwn](https://github.com/pngwn)! - [#6831](https://github.com/gradio-app/gradio/pull/6831) [`f3abde8`](https://github.com/gradio-app/gradio/commit/f3abde80884d96ad69b825020c46486d9dd5cac5) - Add an option to enable header links for markdown. Thanks [@pngwn](https://github.com/pngwn)! - [#6814](https://github.com/gradio-app/gradio/pull/6814) [`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d) - Refactor queue so that there are separate queues for each concurrency id. Thanks [@aliabid94](https://github.com/aliabid94)! - [#6809](https://github.com/gradio-app/gradio/pull/6809) [`1401d99`](https://github.com/gradio-app/gradio/commit/1401d99ade46d87da75b5f5808a3354c49f1d1ea) - Fix `ImageEditor` interaction story. Thanks [@hannahblair](https://github.com/hannahblair)! - [#6803](https://github.com/gradio-app/gradio/pull/6803) [`77c9003`](https://github.com/gradio-app/gradio/commit/77c900311e2ba37b8f849ce088ceb48aa196af18) - Fixes issue 5781: Enables specifying a caching directory for Examples. Thanks [@cswamy](https://github.com/cswamy)! - [#6823](https://github.com/gradio-app/gradio/pull/6823) [`67a2b7f`](https://github.com/gradio-app/gradio/commit/67a2b7f12cb06355fcc41e40d47e8b2ad211d7d1) - Fixed duplicate word (\"this this\"). Thanks [@Cassini-chris](https://github.com/Cassini-chris)!', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab52a3e684812d15f6aec', 'page_content': 'gradio\\\\_test\\n## 0.3.3\\n### Patch Changes\\n- Updated dependencies [[`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d), [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:\\n- @gradio/statustracker@0.4.3\\n- @gradio/atoms@0.4.1\\n## 0.3.2\\n### Patch Changes\\n- Updated dependencies [[`4d1cbbc`](https://github.com/gradio-app/gradio/commit/4d1cbbcf30833ef1de2d2d2710c7492a379a9a00)]:\\n- @gradio/atoms@0.4.0\\n- @gradio/statustracker@0.4.2\\n## 0.3.1\\n### Patch Changes\\n- Updated dependencies []:\\n- @gradio/atoms@0.3.1\\n- @gradio/statustracker@0.4.1\\n## 0.3.0\\n### Features\\n- [#6532](https://github.com/gradio-app/gradio/pull/6532) [`96290d304`](https://github.com/gradio-app/gradio/commit/96290d304a61064b52c10a54b2feeb09ca007542) - tweak deps. Thanks [@pngwn](https://github.com/pngwn)! ## 0.2.3\\n### Patch Changes\\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8)]:\\n- @gradio/atoms@0.3.0\\n- @gradio/statustracker@0.4.0\\n## 0.2.2\\n### Patch Changes\\n- Updated dependencies [[`f816136a0`](https://github.com/gradio-app/gradio/commit/f816136a039fa6011be9c4fb14f573e4050a681a)]:\\n- @gradio/atoms@0.2.2\\n- @gradio/statustracker@0.3.2\\n## 0.2.1\\n### Patch Changes\\n- Updated dependencies [[`3cdeabc68`](https://github.com/gradio-app/gradio/commit/3cdeabc6843000310e1a9e1d17190ecbf3bbc780), [`fad92c29d`](https://github.com/gradio-app/gradio/commit/fad92c29dc1f5cd84341aae417c495b33e01245f)]:\\n- @gradio/atoms@0.2.1\\n- @gradio/statustracker@0.3.1\\n## 0.2.0\\n### Features\\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - fix cc build.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"In Gradio's testing strategy, bug fixes should typically be accompanied by dynamic 'Code' tests wherever it is reasonably possible.\", 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--2da5bd22-4da7-4be6-bd18-512b90e3b825-0', 'usage_metadata': {'input_tokens': 2591, 'output_tokens': 25, 'total_tokens': 2616, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:45,599 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 44/65 [02:59<01:19,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'How can you force mixed precision training when initializing the Accelerator in ü§ó Accelerate?\\n', 'type': 'human', 'id': 'ec3827ed-36f0-41c5-a1b3-fd8268cf6d6b'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_qJsUREIdVjjVmUHZNtKTcnF2', 'function': {'arguments': '{\"query\":\"force mixed precision training when initializing the Accelerator in ü§ó Accelerate\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--b26f47a5-f368-4860-a1a0-420b8f134aa1-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'force mixed precision training when initializing the Accelerator in ü§ó Accelerate'}, 'id': 'call_qJsUREIdVjjVmUHZNtKTcnF2', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 28, 'total_tokens': 393, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of ü§ó Accelerate is:\\n- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible. - to have the same functions work for any distributed setup, so only have to learn one API. ### How does it work? To see how the library works in practice, let\\'s have a look at each line of code we need to add to a training loop. ```python\\naccelerator = Accelerator()\\n```\\nOn top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script. ```python\\nmodel, optim, data = accelerator.prepare(model, optim, data)\\n```\\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torch.nn.Module`), optimizers (`torch.optim.Optimizer`) and dataloaders (`torch.data.dataloader.DataLoader`). #### Model\\nModel preparation include wrapping it in the proper container (for instance `DistributedDataParallel`) and putting it on the proper device. Like with a regular distributed training, you will need to unwrap your model for saving, or to access its specific methods, which can be done with `accelerator.unwrap\\\\_model(model)`. #### Optimizer\\nThe optimizer is also wrapped in a special container that will perform the necessary operations in the step to make mixed precision work.\\n\\nWhile gradient checkpointing may improve memory efficiency, it slows training by approximately 20%. ## Mixed precision training\\n\\\\*\\\\*Mixed precision training\\\\*\\\\* is a technique that aims to optimize the computational efficiency of training models by\\nutilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point\\nprecision (fp32 or float32) to represent and process variables. However, not all variables require this high precision\\nlevel to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like 16-bit\\nfloating point (fp16 or float16), we can speed up the computations. Because in this approach some computations are performed\\nin half-precision, while some are still in full precision, the approach is called mixed precision training. Most commonly mixed precision training is achieved by using fp16 (float16) data types, however, some GPU architectures\\n(such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. Check\\nout the [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/) to learn more about\\nthe differences between these data types. ### fp16\\nThe main advantage of mixed precision training comes from saving the activations in half precision (fp16). Although the gradients are also computed in half precision they are converted back to full precision for the optimization\\nstep so no memory is saved here. While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes. This is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU). To enable mixed precision training, set the `fp16` flag to `True`:\\n```py\\ntraining\\\\_args = TrainingArguments(per\\\\_device\\\\_train\\\\_batch\\\\_size=4, fp16=True, \\\\*\\\\*default\\\\_args)\\n```\\nIf you prefer to use ü§ó Accelerate, find the ü§ó Accelerate example [further in this guide](#using-accelerate).\\n\\nupercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on which you can run your training: it could be on CPU, GPUs, TPUs. Distributed on one machine with several devices, or several machines (often called nodes) each with multiple devices. On top of that there are new tweaks to make your training faster or more memory efficient, like mixed precision and DeepSpeed. Each of those setups or training tweaks, requires you to change the code of your training loop in one way or another and to learn a new API. All those setups are handled by the Trainer API, and there are several third-party libraries that can also help you with that. The problem with those is that they can feel like a black box and that it might not be easy to implement the tweak to the training loop you need. Accelerate has been designed specifically to let you retain full control over your training loop and be as non-intrusive as possible. With just four lines to add to your training loop (here shown on the code of the training loop from the \"Raw training loop\" video), Accelerate will handle all the setups and training tweaks mentioned on the first slide.\\n\\n## Setup\\nGet started by installing ü§ó Accelerate:\\n```bash\\npip install accelerate\\n```\\nThen import and create an [`~accelerate.Accelerator`] object. The [`~accelerate.Accelerator`] will automatically detect your type of distributed setup and initialize all the necessary components for training. You don\\'t need to explicitly place your model on a device. ```py\\n>>> from accelerate import Accelerator\\n>>> accelerator = Accelerator()\\n```\\n## Prepare to accelerate\\nThe next step is to pass all the relevant training objects to the [`~accelerate.Accelerator.prepare`] method. This includes your training and evaluation DataLoaders, a model and an optimizer:\\n```py\\n>>> train\\\\_dataloader, eval\\\\_dataloader, model, optimizer = accelerator.prepare(\\n... train\\\\_dataloader, eval\\\\_dataloader, model, optimizer\\n... )\\n```\\n## Backward\\nThe last addition is to replace the typical `loss.backward()` in your training loop with ü§ó Accelerate\\'s [`~accelerate.Accelerator.backward`]method:\\n```py\\n>>> for epoch in range(num\\\\_epochs):\\n...', 'type': 'tool', 'name': 'data_retriever', 'id': '2be5f3e8-6455-404f-87ca-7e31318c4d5a', 'tool_call_id': 'call_qJsUREIdVjjVmUHZNtKTcnF2', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa69e3e684812d15f59cd', 'page_content': \"Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of ü§ó Accelerate is:\\n- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible. - to have the same functions work for any distributed setup, so only have to learn one API. ### How does it work? To see how the library works in practice, let's have a look at each line of code we need to add to a training loop. ```python\\naccelerator = Accelerator()\\n```\\nOn top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script. ```python\\nmodel, optim, data = accelerator.prepare(model, optim, data)\\n```\\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torch.nn.Module`), optimizers (`torch.optim.Optimizer`) and dataloaders (`torch.data.dataloader.DataLoader`). #### Model\\nModel preparation include wrapping it in the proper container (for instance `DistributedDataParallel`) and putting it on the proper device. Like with a regular distributed training, you will need to unwrap your model for saving, or to access its specific methods, which can be done with `accelerator.unwrap\\\\_model(model)`. #### Optimizer\\nThe optimizer is also wrapped in a special container that will perform the necessary operations in the step to make mixed precision work.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b910af3d2ebb270a070d', 'page_content': 'While gradient checkpointing may improve memory efficiency, it slows training by approximately 20%. ## Mixed precision training\\n\\\\*\\\\*Mixed precision training\\\\*\\\\* is a technique that aims to optimize the computational efficiency of training models by\\nutilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point\\nprecision (fp32 or float32) to represent and process variables. However, not all variables require this high precision\\nlevel to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like 16-bit\\nfloating point (fp16 or float16), we can speed up the computations. Because in this approach some computations are performed\\nin half-precision, while some are still in full precision, the approach is called mixed precision training. Most commonly mixed precision training is achieved by using fp16 (float16) data types, however, some GPU architectures\\n(such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. Check\\nout the [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/) to learn more about\\nthe differences between these data types. ### fp16\\nThe main advantage of mixed precision training comes from saving the activations in half precision (fp16). Although the gradients are also computed in half precision they are converted back to full precision for the optimization\\nstep so no memory is saved here. While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes. This is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU). To enable mixed precision training, set the `fp16` flag to `True`:\\n```py\\ntraining\\\\_args = TrainingArguments(per\\\\_device\\\\_train\\\\_batch\\\\_size=4, fp16=True, \\\\*\\\\*default\\\\_args)\\n```\\nIf you prefer to use ü§ó Accelerate, find the ü§ó Accelerate example [further in this guide](#using-accelerate).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b3baaf3d2ebb270a00e7', 'page_content': 'upercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on which you can run your training: it could be on CPU, GPUs, TPUs. Distributed on one machine with several devices, or several machines (often called nodes) each with multiple devices. On top of that there are new tweaks to make your training faster or more memory efficient, like mixed precision and DeepSpeed. Each of those setups or training tweaks, requires you to change the code of your training loop in one way or another and to learn a new API. All those setups are handled by the Trainer API, and there are several third-party libraries that can also help you with that. The problem with those is that they can feel like a black box and that it might not be easy to implement the tweak to the training loop you need. Accelerate has been designed specifically to let you retain full control over your training loop and be as non-intrusive as possible. With just four lines to add to your training loop (here shown on the code of the training loop from the \"Raw training loop\" video), Accelerate will handle all the setups and training tweaks mentioned on the first slide.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b4b6af3d2ebb270a01f5', 'page_content': \"## Setup\\nGet started by installing ü§ó Accelerate:\\n```bash\\npip install accelerate\\n```\\nThen import and create an [`~accelerate.Accelerator`] object. The [`~accelerate.Accelerator`] will automatically detect your type of distributed setup and initialize all the necessary components for training. You don't need to explicitly place your model on a device. ```py\\n>>> from accelerate import Accelerator\\n>>> accelerator = Accelerator()\\n```\\n## Prepare to accelerate\\nThe next step is to pass all the relevant training objects to the [`~accelerate.Accelerator.prepare`] method. This includes your training and evaluation DataLoaders, a model and an optimizer:\\n```py\\n>>> train\\\\_dataloader, eval\\\\_dataloader, model, optimizer = accelerator.prepare(\\n... train\\\\_dataloader, eval\\\\_dataloader, model, optimizer\\n... )\\n```\\n## Backward\\nThe last addition is to replace the typical `loss.backward()` in your training loop with ü§ó Accelerate's [`~accelerate.Accelerator.backward`]method:\\n```py\\n>>> for epoch in range(num\\\\_epochs):\\n...\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'You can force mixed precision training when initializing the Accelerator in ü§ó Accelerate by passing the `fp16=True` argument to the `Accelerator()` initialization. Here‚Äôs how you can do it:\\n\\n```python\\naccelerator = Accelerator(fp16=True)\\n``` \\n\\nThis will enable mixed precision training for your setup.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--9365ebe2-9540-496f-8b6d-d1abbcb7ca14-0', 'usage_metadata': {'input_tokens': 1553, 'output_tokens': 63, 'total_tokens': 1616, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:50,548 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 45/65 [03:04<01:22,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of tokenizers in the NLP pipeline?\\n', 'type': 'human', 'id': 'dd860b58-f9f5-4de8-adcd-1c7bd04c7a84'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_nZzi4Pv5OmdxWMai7Cofp9oA', 'function': {'arguments': '{\"query\":\"purpose of tokenizers in NLP pipeline\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--dfc36c74-0096-42aa-9d0c-92e598dd664b-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'purpose of tokenizers in NLP pipeline'}, 'id': 'call_nZzi4Pv5OmdxWMai7Cofp9oA', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 360, 'output_tokens': 22, 'total_tokens': 382, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'p align=\"center\">\\n\\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\\n\\n![Build](https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg)\\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\\n[![Doc](https://docs.rs/tokenizers/badge.svg)](https://docs.rs/tokenizers/)\\n\\nThe core of `tokenizers`, written in Rust. Provides an implementation of today\\'s most used tokenizers, with a focus on performance and\\nversatility. ## What is a Tokenizer\\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`. The various steps of the pipeline are:\\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\\nthe [unicode normalization standards](https://unicode.org/reports/tr15/#Norm\\\\_Forms), such as `NFD` or `NFKC`. More details about how to use the `Normalizers` are available on the\\n[Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\\nsplitting text is simply on whitespace. 3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\\n`BPE` or `WordPiece`.\\n\\nThe tokenization pipeline\\nWhen calling `Tokenizer.encode` or\\n`Tokenizer.encode\\\\_batch`, the input\\ntext(s) go through the following pipeline:\\n- `normalization`\\n- `pre-tokenization`\\n- `model`\\n- `post-processing`\\nWe\\'ll see in details what happens during each of those steps in detail,\\nas well as when you want to `decode ` some token ids, and how the ü§ó Tokenizers library allows you\\nto customize each of those steps to your needs. If you\\'re already\\nfamiliar with those steps and want to learn by seeing some code, jump to\\n`our BERT from scratch example `. For the examples that require a `Tokenizer` we will use the tokenizer we trained in the\\n`quicktour`, which you can load with:\\n\\n{\"path\": \"../../bindings/python/tests/documentation/test\\\\_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START reload\\\\_tokenizer\",\\n\"end-before\": \"END reload\\\\_tokenizer\",\\n\"dedent\": 12}\\n\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline\\\\_reload\\\\_tokenizer\",\\n\"end-before\": \"END pipeline\\\\_reload\\\\_tokenizer\",\\n\"dedent\": 4}\\n\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START reload\\\\_tokenizer\",\\n\"end-before\": \"END reload\\\\_tokenizer\",\\n\"dedent\": 8}\\n## Normalization\\nNormalization is, in a nutshell, a set of operations you apply to a raw\\nstring to make it less random or \"cleaner\".\\n\\nn these few videos, we\\'ll take a look at the tokenizers. In Natural Language Processing, most of the data that we handle consists of raw text. However, machine learning models cannot read and understand text in its raw form they can only work with numbers. The tokenizer\\'s objective will be to translate the text into numbers. There are several possible approaches to this conversion, and the objective is to find the most meaningful representation. We\\'ll take a look at three distinct tokenization algorithms. We compare them one to one, so we recommend you look at the videos in the following order: Word-based, Character-based, and Subword-based.\\n\\nhe fast tokenizers of the Transformers library are fast, but they also implement features that will be super useful for data pre-processing and post-processing. Let\\'s have a look at them! First, let\\'s have a look at the usual output of a tokenizer. We get input IDs that correspond to tokens, but we lose a lot of information in the process. For instance, here the tokenization is the same for the two sentences, even if one has several more spaces than the other. Just having the input IDs is thus not enough if we want to match some tokens with a span of text (something we will need to do when tackling question answering for instance). It\\'s also difficult to know when two tokens belong to the same word or not: it looks easy when you just look at the output of a BERT tokenizer, we just need to look for the ##. But other tokenizers have different ways to tokenize parts of words. For instance RoBERTa adds this special G symbol to mark the tokens at the beginning of a word, and T5 uses this special underscore symbol for the same purpose.', 'type': 'tool', 'name': 'data_retriever', 'id': '673cd249-a123-40cd-b005-4a30aa954993', 'tool_call_id': 'call_nZzi4Pv5OmdxWMai7Cofp9oA', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bb92af3d2ebb270a0a21', 'page_content': 'p align=\"center\">\\n\\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\\n\\n![Build](https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg)\\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\\n[![Doc](https://docs.rs/tokenizers/badge.svg)](https://docs.rs/tokenizers/)\\n\\nThe core of `tokenizers`, written in Rust. Provides an implementation of today\\'s most used tokenizers, with a focus on performance and\\nversatility. ## What is a Tokenizer\\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`. The various steps of the pipeline are:\\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\\nthe [unicode normalization standards](https://unicode.org/reports/tr15/#Norm\\\\_Forms), such as `NFD` or `NFKC`. More details about how to use the `Normalizers` are available on the\\n[Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\\nsplitting text is simply on whitespace. 3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\\n`BPE` or `WordPiece`.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b22baf3d2ebb2709fe6d', 'page_content': 'The tokenization pipeline\\nWhen calling `Tokenizer.encode` or\\n`Tokenizer.encode\\\\_batch`, the input\\ntext(s) go through the following pipeline:\\n- `normalization`\\n- `pre-tokenization`\\n- `model`\\n- `post-processing`\\nWe\\'ll see in details what happens during each of those steps in detail,\\nas well as when you want to `decode ` some token ids, and how the ü§ó Tokenizers library allows you\\nto customize each of those steps to your needs. If you\\'re already\\nfamiliar with those steps and want to learn by seeing some code, jump to\\n`our BERT from scratch example `. For the examples that require a `Tokenizer` we will use the tokenizer we trained in the\\n`quicktour`, which you can load with:\\n\\n{\"path\": \"../../bindings/python/tests/documentation/test\\\\_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START reload\\\\_tokenizer\",\\n\"end-before\": \"END reload\\\\_tokenizer\",\\n\"dedent\": 12}\\n\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline\\\\_reload\\\\_tokenizer\",\\n\"end-before\": \"END pipeline\\\\_reload\\\\_tokenizer\",\\n\"dedent\": 4}\\n\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START reload\\\\_tokenizer\",\\n\"end-before\": \"END reload\\\\_tokenizer\",\\n\"dedent\": 8}\\n## Normalization\\nNormalization is, in a nutshell, a set of operations you apply to a raw\\nstring to make it less random or \"cleaner\".', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b50aaf3d2ebb270a0253', 'page_content': \"n these few videos, we'll take a look at the tokenizers. In Natural Language Processing, most of the data that we handle consists of raw text. However, machine learning models cannot read and understand text in its raw form they can only work with numbers. The tokenizer's objective will be to translate the text into numbers. There are several possible approaches to this conversion, and the objective is to find the most meaningful representation. We'll take a look at three distinct tokenization algorithms. We compare them one to one, so we recommend you look at the videos in the following order: Word-based, Character-based, and Subword-based.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aab5f3e684812d15f5f70', 'page_content': \"he fast tokenizers of the Transformers library are fast, but they also implement features that will be super useful for data pre-processing and post-processing. Let's have a look at them! First, let's have a look at the usual output of a tokenizer. We get input IDs that correspond to tokens, but we lose a lot of information in the process. For instance, here the tokenization is the same for the two sentences, even if one has several more spaces than the other. Just having the input IDs is thus not enough if we want to match some tokens with a span of text (something we will need to do when tackling question answering for instance). It's also difficult to know when two tokens belong to the same word or not: it looks easy when you just look at the output of a BERT tokenizer, we just need to look for the ##. But other tokenizers have different ways to tokenize parts of words. For instance RoBERTa adds this special G symbol to mark the tokens at the beginning of a word, and T5 uses this special underscore symbol for the same purpose.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The purpose of tokenizers in the NLP pipeline is to process raw text as input and output an `Encoding`. They translate text into numbers, which machine learning models can work with, as they cannot read and understand text in its raw form. The tokenization process involves several steps, including normalization, pre-tokenization, and the actual tokenization model, which together help create a meaningful representation of the text for further processing in natural language processing tasks.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--0ac20936-2c08-4709-9873-d1a05b62f0dc-0', 'usage_metadata': {'input_tokens': 1297, 'output_tokens': 89, 'total_tokens': 1386, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:54,703 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 46/65 [03:08<01:18,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of the Safety Checker in the Diffusers library?\\n', 'type': 'human', 'id': 'a45c4d7f-afe5-4e66-af99-4b3ec5031642'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_FAgx1ZScDViKBp8RXWaJLgLQ', 'function': {'arguments': '{\"query\":\"Safety Checker in the Diffusers library\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--4e7dba05-5c66-4663-96e9-3637b7995f68-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Safety Checker in the Diffusers library'}, 'id': 'call_FAgx1ZScDViKBp8RXWaJLgLQ', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 22, 'total_tokens': 384, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'üß® Diffusers Experimental\\nWe are adding experimental code to support novel applications and usages of the Diffusers library.\\n\\n!---\\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n# üß® Diffusers Examples\\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\\nfor a variety of use cases involving training or fine-tuning. \\\\*\\\\*Note\\\\*\\\\*: If you are looking for \\\\*\\\\*official\\\\*\\\\* examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines).\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n\\n![](https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg)\\n\\n# Diffusers\\nü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\\'re looking for a simple inference solution or want to train your own diffusion model, ü§ó Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). The library has three main components:\\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in ü§ó Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve. - Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality. - Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. [Tutorials\\n\\nLearn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you\\'re using ü§ó Diffusers for the first time!](./tutorials/tutorial_overview)\\n[How-to guides\\n\\nPractical guides for helping you load pipelines, models, and schedulers. You\\'ll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.](./using-diffusers/loading_overview)\\n[Conceptual guides\\n\\nUnderstand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.](./conceptual/philosophy)\\n[Reference\\n\\nTechnical descriptions of how ü§ó Diffusers classes and methods work.](./api/models/overview)\\n\\n!---\\nCopyright 2022 - The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![](https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg)\\n\\n[![GitHub](https://img.shields.io/github/license/huggingface/datasets.svg?color=blue)](https://github.com/huggingface/diffusers/blob/main/LICENSE)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/diffusers.svg)](https://github.com/huggingface/diffusers/releases)\\n[![GitHub release](https://static.pepy.tech/badge/diffusers/month)](https://pepy.tech/project/diffusers)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CODE_OF_CONDUCT.md)\\n[![X account](https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&label=Follow%20%40diffuserslib)](https://twitter.com/diffuserslib)\\n\\nü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\\'re looking for a simple inference solution or training your own diffusion models, ü§ó Diffusers is a modular toolbox that supports both.', 'type': 'tool', 'name': 'data_retriever', 'id': '978cea77-d413-491e-a6c6-78072d3ad012', 'tool_call_id': 'call_FAgx1ZScDViKBp8RXWaJLgLQ', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b6efaf3d2ebb270a043c', 'page_content': 'üß® Diffusers Experimental\\nWe are adding experimental code to support novel applications and usages of the Diffusers library.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b29caf3d2ebb2709fedf', 'page_content': '!---\\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n# üß® Diffusers Examples\\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\\nfor a variety of use cases involving training or fine-tuning. \\\\*\\\\*Note\\\\*\\\\*: If you are looking for \\\\*\\\\*official\\\\*\\\\* examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aacf13e684812d15f6180', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n\\n![](https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg)\\n\\n# Diffusers\\nü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\\'re looking for a simple inference solution or want to train your own diffusion model, ü§ó Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). The library has three main components:\\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in ü§ó Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve. - Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality. - Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. [Tutorials\\n\\nLearn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you\\'re using ü§ó Diffusers for the first time!](./tutorials/tutorial_overview)\\n[How-to guides\\n\\nPractical guides for helping you load pipelines, models, and schedulers. You\\'ll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.](./using-diffusers/loading_overview)\\n[Conceptual guides\\n\\nUnderstand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.](./conceptual/philosophy)\\n[Reference\\n\\nTechnical descriptions of how ü§ó Diffusers classes and methods work.](./api/models/overview)', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bf25666af48523f51d80', 'page_content': '!---\\nCopyright 2022 - The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![](https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg)\\n\\n[![GitHub](https://img.shields.io/github/license/huggingface/datasets.svg?color=blue)](https://github.com/huggingface/diffusers/blob/main/LICENSE)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/diffusers.svg)](https://github.com/huggingface/diffusers/releases)\\n[![GitHub release](https://static.pepy.tech/badge/diffusers/month)](https://pepy.tech/project/diffusers)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CODE_OF_CONDUCT.md)\\n[![X account](https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&label=Follow%20%40diffuserslib)](https://twitter.com/diffuserslib)\\n\\nü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\\'re looking for a simple inference solution or training your own diffusion models, ü§ó Diffusers is a modular toolbox that supports both.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"The Safety Checker in the Diffusers library is designed to help ensure that the generated outputs adhere to ethical guidelines and safety standards. It acts as a filter to prevent the generation of harmful or inappropriate content, thereby promoting responsible use of the diffusion models. This feature is part of the library's commitment to safety and ethical considerations in AI-generated content.\", 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--107ba557-42b7-4eb1-9449-6571ccb1beae-0', 'usage_metadata': {'input_tokens': 1527, 'output_tokens': 68, 'total_tokens': 1595, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:28:58,204 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 47/65 [03:11<01:10,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\\n', 'type': 'human', 'id': '8b7da79e-4d2f-4664-be0b-e30e71c65172'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_IurSy1Xtb5VNlqvqRfWiIEuS', 'function': {'arguments': '{\"query\":\"Python class retrieve Discussions and Pull Requests from a repository on Hugging Face Hub\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--5ef14647-3ccf-49d3-88dd-2dfdce303c78-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Python class retrieve Discussions and Pull Requests from a repository on Hugging Face Hub'}, 'id': 'call_IurSy1Xtb5VNlqvqRfWiIEuS', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 370, 'output_tokens': 30, 'total_tokens': 400, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Interact with Discussions and Pull Requests\\nThe `huggingface\\\\_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood. ## Retrieve Discussions and Pull Requests from the Hub\\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\\n```python\\n>>> from huggingface\\\\_hub import get\\\\_repo\\\\_discussions\\n>>> for discussion in get\\\\_repo\\\\_discussions(repo\\\\_id=\"bigscience/bloom\"):\\n... print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is\\\\_pull\\\\_request}\")\\n# 11 - Add Flax weights, pr: True\\n# 10 - Update README.md, pr: True\\n# 9 - Training languages in the model card, pr: True\\n# 8 - Update tokenizer\\\\_config.json, pr: True\\n# 7 - Slurm training script, pr: False\\n[...]\\n```\\n`HfApi.get\\\\_repo\\\\_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\\n```python\\n>>> from huggingface\\\\_hub import get\\\\_repo\\\\_discussions\\n>>> for discussion in get\\\\_repo\\\\_discussions(\\n...\\n\\nrepo\\\\_id=\"bigscience/bloom\",\\n... author=\"ArthurZ\",\\n... discussion\\\\_type=\"pull\\\\_request\",\\n... discussion\\\\_status=\"open\",\\n... ):\\n... print(f\"{discussion.num} - {discussion.title} by {discussion.author}, pr: {discussion.is\\\\_pull\\\\_request}\")\\n# 19 - Add Flax weights by ArthurZ, pr: True\\n```\\n`HfApi.get\\\\_repo\\\\_discussions` returns a [generator](https://docs.python.org/3.7/howto/functional.html#generators) that yields\\n[`Discussion`] objects. To get all the Discussions in a single list, run:\\n```python\\n>>> from huggingface\\\\_hub import get\\\\_repo\\\\_discussions\\n>>> discussions\\\\_list = list(get\\\\_repo\\\\_discussions(repo\\\\_id=\"bert-base-uncased\"))\\n```\\nThe [`Discussion`] object returned by [`HfApi.get\\\\_repo\\\\_discussions`] contains high-level overview of the\\nDiscussion or Pull Request. You can also get more detailed information using [`HfApi.get\\\\_discussion\\\\_details`]:\\n```python\\n>>> from huggingface\\\\_hub import get\\\\_discussion\\\\_details\\n>>> get\\\\_discussion\\\\_details(\\n... repo\\\\_id=\"bigscience/bloom-1b3\",\\n... discussion\\\\_num=2\\n... )\\nDiscussionWithDetails(\\nnum=2,\\nauthor=\\'cakiki\\',\\ntitle=\\'Update VRAM memory for the V100s\\',\\nstatus=\\'open\\',\\nis\\\\_pull\\\\_request=True,\\nevents=[\\nDiscussionComment(type=\\'comment\\', author=\\'cakiki\\', ...),\\nDiscussionCommit(type=\\'commit\\', author=\\'cakiki\\', summary=\\'Update VRAM memory for the V100s\\', oid=\\'1256f9d9a33fa8887e1c1bf0e09b4713da96773a\\', ...),\\n],\\nconflicting\\\\_files=[],\\ntarget\\\\_branch=\\'refs/heads/main\\',\\nmerge\\\\_commit\\\\_oid=None,\\ndiff=\\'diff --git a/README.md b/README.md\\\\nindex a6ae3b9294edf8d0eda0d67c7780a10241242a7e..3a1814f212bc3f0d3cc8f74bdbd316de4ae7b9e3 100644\\\\n--- a/README.md\\\\n+++ b/README.md\\\\n@@ -132,7 +132,7 [...]\\',\\n)\\n```\\n[`HfApi.get\\\\_discussion\\\\_details`] returns a [`DiscussionWithDetails`] object, which is a subclass of [`Discussion`]\\nwith more detailed information about the Discussion or Pull Request. Information includes all the comments, status changes,\\nand renames of the Discussion via [`DiscussionWithDetails.events`]. In case of a Pull Request, you can retrieve the raw git diff with [`DiscussionWithDetails.diff`]. All the commits of the\\nPull Request are listed in [`DiscussionWithDetails.events`]. ## Create and edit a Discussion or Pull Request programmatically\\nThe [`HfApi`] class also offers ways to create and edit Discussions and Pull Requests. You will need an [access token](https://huggingface.co/docs/hub/security-tokens) to create and edit Discussions\\nor Pull Requests. The simplest way to propose changes on a repo on the Hub is via the [`create\\\\_commit`] API: just\\nset the `create\\\\_pr` parameter to `True`. This parameter is also available on other methods that wrap [`create\\\\_commit`]:\\n\\\\* [`upload\\\\_file`]\\n\\\\* [`upload\\\\_folder`]\\n\\\\* [`delete\\\\_file`]\\n\\\\* [`delete\\\\_folder`]\\n\\\\* [`metadata\\\\_update`]\\n```python\\n>>> from huggingface\\\\_hub import metadata\\\\_update\\n>>> metadata\\\\_update(\\n...\\n\\nrepo\\\\_id=\"username/repo\\\\_name\",\\n... metadata={\"tags\": [\"computer-vision\", \"awesome-model\"]},\\n... create\\\\_pr=True,\\n... )\\n```\\nYou can also use [`HfApi.create\\\\_discussion`] (respectively [`HfApi.create\\\\_pull\\\\_request`]) to create a Discussion (respectively a Pull Request) on a repo. Opening a Pull Request this way can be useful if you need to work on changes locally. Pull Requests opened this way will be in `\"draft\"` mode. ```python\\n>>> from huggingface\\\\_hub import create\\\\_discussion, create\\\\_pull\\\\_request\\n>>> create\\\\_discussion(\\n... repo\\\\_id=\"username/repo-name\",\\n... title=\"Hi from the huggingface\\\\_hub library!\",\\n... token=\"\",\\n... )\\nDiscussionWithDetails(...)\\n>>> create\\\\_pull\\\\_request(\\n... repo\\\\_id=\"username/repo-name\",\\n... title=\"Hi from the huggingface\\\\_hub library!\",\\n... token=\"\",\\n... )\\nDiscussionWithDetails(..., is\\\\_pull\\\\_request=True)\\n```\\nManaging Pull Requests and Discussions can be done entirely with the [`HfApi`] class. For example:\\n\\\\* [`comment\\\\_discussion`] to add comments\\n\\\\* [`edit\\\\_discussion\\\\_comment`] to edit comments\\n\\\\* [`rename\\\\_discussion`] to rename a Discussion or Pull Request\\n\\\\* [`change\\\\_discussion\\\\_status`] to open or close a Discussion / Pull Request\\n\\\\* [`merge\\\\_pull\\\\_request`] to merge a Pull Request\\nVisit the [`HfApi`] documentation page for an exhaustive reference of all available methods. ## Push changes to a Pull Request\\n\\\\*Coming soon !\\\\*\\n## See also\\nFor a more detailed reference, visit the [Discussions and Pull Requests](../package\\\\_reference/community) and the [hf\\\\_api](../package\\\\_reference/hf\\\\_api) documentation page.\\n\\n!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Interacting with Discussions and Pull Requests\\nCheck the [`HfApi`] documentation page for the reference of methods enabling\\ninteraction with Pull Requests and Discussions on the Hub. - [`get\\\\_repo\\\\_discussions`]\\n- [`get\\\\_discussion\\\\_details`]\\n- [`create\\\\_discussion`]\\n- [`create\\\\_pull\\\\_request`]\\n- [`rename\\\\_discussion`]\\n- [`comment\\\\_discussion`]\\n- [`edit\\\\_discussion\\\\_comment`]\\n- [`change\\\\_discussion\\\\_status`]\\n- [`merge\\\\_pull\\\\_request`]\\n## Data structures\\n[[autodoc]] Discussion\\n[[autodoc]] DiscussionWithDetails\\n[[autodoc]] DiscussionEvent\\n[[autodoc]] DiscussionComment\\n[[autodoc]] DiscussionStatusChange\\n[[autodoc]] DiscussionCommit\\n[[autodoc]] DiscussionTitleChange', 'type': 'tool', 'name': 'data_retriever', 'id': '6b0fa581-8a6e-4738-8851-d90b7e70d422', 'tool_call_id': 'call_IurSy1Xtb5VNlqvqRfWiIEuS', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aae903e684812d15f6354', 'page_content': '!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Interact with Discussions and Pull Requests\\nThe `huggingface\\\\_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood. ## Retrieve Discussions and Pull Requests from the Hub\\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\\n```python\\n>>> from huggingface\\\\_hub import get\\\\_repo\\\\_discussions\\n>>> for discussion in get\\\\_repo\\\\_discussions(repo\\\\_id=\"bigscience/bloom\"):\\n... print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is\\\\_pull\\\\_request}\")\\n# 11 - Add Flax weights, pr: True\\n# 10 - Update README.md, pr: True\\n# 9 - Training languages in the model card, pr: True\\n# 8 - Update tokenizer\\\\_config.json, pr: True\\n# 7 - Slurm training script, pr: False\\n[...]\\n```\\n`HfApi.get\\\\_repo\\\\_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\\n```python\\n>>> from huggingface\\\\_hub import get\\\\_repo\\\\_discussions\\n>>> for discussion in get\\\\_repo\\\\_discussions(\\n...', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aae903e684812d15f6355', 'page_content': 'repo\\\\_id=\"bigscience/bloom\",\\n... author=\"ArthurZ\",\\n... discussion\\\\_type=\"pull\\\\_request\",\\n... discussion\\\\_status=\"open\",\\n... ):\\n... print(f\"{discussion.num} - {discussion.title} by {discussion.author}, pr: {discussion.is\\\\_pull\\\\_request}\")\\n# 19 - Add Flax weights by ArthurZ, pr: True\\n```\\n`HfApi.get\\\\_repo\\\\_discussions` returns a [generator](https://docs.python.org/3.7/howto/functional.html#generators) that yields\\n[`Discussion`] objects. To get all the Discussions in a single list, run:\\n```python\\n>>> from huggingface\\\\_hub import get\\\\_repo\\\\_discussions\\n>>> discussions\\\\_list = list(get\\\\_repo\\\\_discussions(repo\\\\_id=\"bert-base-uncased\"))\\n```\\nThe [`Discussion`] object returned by [`HfApi.get\\\\_repo\\\\_discussions`] contains high-level overview of the\\nDiscussion or Pull Request. You can also get more detailed information using [`HfApi.get\\\\_discussion\\\\_details`]:\\n```python\\n>>> from huggingface\\\\_hub import get\\\\_discussion\\\\_details\\n>>> get\\\\_discussion\\\\_details(\\n... repo\\\\_id=\"bigscience/bloom-1b3\",\\n... discussion\\\\_num=2\\n... )\\nDiscussionWithDetails(\\nnum=2,\\nauthor=\\'cakiki\\',\\ntitle=\\'Update VRAM memory for the V100s\\',\\nstatus=\\'open\\',\\nis\\\\_pull\\\\_request=True,\\nevents=[\\nDiscussionComment(type=\\'comment\\', author=\\'cakiki\\', ...),\\nDiscussionCommit(type=\\'commit\\', author=\\'cakiki\\', summary=\\'Update VRAM memory for the V100s\\', oid=\\'1256f9d9a33fa8887e1c1bf0e09b4713da96773a\\', ...),\\n],\\nconflicting\\\\_files=[],\\ntarget\\\\_branch=\\'refs/heads/main\\',\\nmerge\\\\_commit\\\\_oid=None,\\ndiff=\\'diff --git a/README.md b/README.md\\\\nindex a6ae3b9294edf8d0eda0d67c7780a10241242a7e..3a1814f212bc3f0d3cc8f74bdbd316de4ae7b9e3 100644\\\\n--- a/README.md\\\\n+++ b/README.md\\\\n@@ -132,7 +132,7 [...]\\',\\n)\\n```\\n[`HfApi.get\\\\_discussion\\\\_details`] returns a [`DiscussionWithDetails`] object, which is a subclass of [`Discussion`]\\nwith more detailed information about the Discussion or Pull Request. Information includes all the comments, status changes,\\nand renames of the Discussion via [`DiscussionWithDetails.events`]. In case of a Pull Request, you can retrieve the raw git diff with [`DiscussionWithDetails.diff`]. All the commits of the\\nPull Request are listed in [`DiscussionWithDetails.events`]. ## Create and edit a Discussion or Pull Request programmatically\\nThe [`HfApi`] class also offers ways to create and edit Discussions and Pull Requests. You will need an [access token](https://huggingface.co/docs/hub/security-tokens) to create and edit Discussions\\nor Pull Requests. The simplest way to propose changes on a repo on the Hub is via the [`create\\\\_commit`] API: just\\nset the `create\\\\_pr` parameter to `True`. This parameter is also available on other methods that wrap [`create\\\\_commit`]:\\n\\\\* [`upload\\\\_file`]\\n\\\\* [`upload\\\\_folder`]\\n\\\\* [`delete\\\\_file`]\\n\\\\* [`delete\\\\_folder`]\\n\\\\* [`metadata\\\\_update`]\\n```python\\n>>> from huggingface\\\\_hub import metadata\\\\_update\\n>>> metadata\\\\_update(\\n...', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aae903e684812d15f6356', 'page_content': 'repo\\\\_id=\"username/repo\\\\_name\",\\n... metadata={\"tags\": [\"computer-vision\", \"awesome-model\"]},\\n... create\\\\_pr=True,\\n... )\\n```\\nYou can also use [`HfApi.create\\\\_discussion`] (respectively [`HfApi.create\\\\_pull\\\\_request`]) to create a Discussion (respectively a Pull Request) on a repo. Opening a Pull Request this way can be useful if you need to work on changes locally. Pull Requests opened this way will be in `\"draft\"` mode. ```python\\n>>> from huggingface\\\\_hub import create\\\\_discussion, create\\\\_pull\\\\_request\\n>>> create\\\\_discussion(\\n... repo\\\\_id=\"username/repo-name\",\\n... title=\"Hi from the huggingface\\\\_hub library!\",\\n... token=\"\",\\n... )\\nDiscussionWithDetails(...)\\n>>> create\\\\_pull\\\\_request(\\n... repo\\\\_id=\"username/repo-name\",\\n... title=\"Hi from the huggingface\\\\_hub library!\",\\n... token=\"\",\\n... )\\nDiscussionWithDetails(..., is\\\\_pull\\\\_request=True)\\n```\\nManaging Pull Requests and Discussions can be done entirely with the [`HfApi`] class. For example:\\n\\\\* [`comment\\\\_discussion`] to add comments\\n\\\\* [`edit\\\\_discussion\\\\_comment`] to edit comments\\n\\\\* [`rename\\\\_discussion`] to rename a Discussion or Pull Request\\n\\\\* [`change\\\\_discussion\\\\_status`] to open or close a Discussion / Pull Request\\n\\\\* [`merge\\\\_pull\\\\_request`] to merge a Pull Request\\nVisit the [`HfApi`] documentation page for an exhaustive reference of all available methods. ## Push changes to a Pull Request\\n\\\\*Coming soon !\\\\*\\n## See also\\nFor a more detailed reference, visit the [Discussions and Pull Requests](../package\\\\_reference/community) and the [hf\\\\_api](../package\\\\_reference/hf\\\\_api) documentation page.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b905af3d2ebb270a0704', 'page_content': '!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Interacting with Discussions and Pull Requests\\nCheck the [`HfApi`] documentation page for the reference of methods enabling\\ninteraction with Pull Requests and Discussions on the Hub. - [`get\\\\_repo\\\\_discussions`]\\n- [`get\\\\_discussion\\\\_details`]\\n- [`create\\\\_discussion`]\\n- [`create\\\\_pull\\\\_request`]\\n- [`rename\\\\_discussion`]\\n- [`comment\\\\_discussion`]\\n- [`edit\\\\_discussion\\\\_comment`]\\n- [`change\\\\_discussion\\\\_status`]\\n- [`merge\\\\_pull\\\\_request`]\\n## Data structures\\n[[autodoc]] Discussion\\n[[autodoc]] DiscussionWithDetails\\n[[autodoc]] DiscussionEvent\\n[[autodoc]] DiscussionComment\\n[[autodoc]] DiscussionStatusChange\\n[[autodoc]] DiscussionCommit\\n[[autodoc]] DiscussionTitleChange', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The `HfApi` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--b7c5b78e-f3bc-40f5-9ece-7b99106fd609-0', 'usage_metadata': {'input_tokens': 2098, 'output_tokens': 26, 'total_tokens': 2124, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:29:01,185 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 48/65 [03:14<01:02,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\\n', 'type': 'human', 'id': 'abc61b5c-3617-4c1a-a31c-9c9f1c551a56'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_P66XtXPzpVeVs9EBdwzOiP9W', 'function': {'arguments': '{\"query\":\"new library introduced by Hugging Face for hosting scikit-learn models\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--c87ddccc-2840-43d0-9a79-c09993d289f1-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'new library introduced by Hugging Face for hosting scikit-learn models'}, 'id': 'call_P66XtXPzpVeVs9EBdwzOiP9W', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 369, 'output_tokens': 29, 'total_tokens': 398, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Integrate your library with the Hub\\nThe Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones already integrated include [spaCy](https://spacy.io/usage/projects#huggingface\\\\_hub), [AllenNLP](https://allennlp.org/), and [timm](https://rwightman.github.io/pytorch-image-models/), among many others. Integration means users can download and upload files to the Hub directly from your library. We hope you will integrate your library and join us in democratizing artificial intelligence for everyone. Integrating the Hub with your library provides many benefits, including:\\n- Free model hosting for you and your users. - Built-in file versioning - even for huge files - made possible by [Git-LFS](https://git-lfs.github.com/). - All public models are powered by the [Inference API](https://huggingface.co/docs/api-inference/index). - In-browser widgets allow users to interact with your hosted models directly. This tutorial will help you integrate the Hub into your library so your users can benefit from all the features offered by the Hub. Before you begin, we recommend you create a [Hugging Face account](https://huggingface.co/join) from which you can manage your repositories and files. If you need help with the integration, feel free to open an [issue](https://github.com/huggingface/huggingface\\\\_hub/issues/new/choose), and we would be more than happy to help you. ## Installation\\n1. Install the `huggingface\\\\_hub` library with pip in your environment:\\n```bash\\npython -m pip install huggingface\\\\_hub\\n```\\n2. Once you have successfully installed the `huggingface\\\\_hub` library, log in to your Hugging Face account:\\n```bash\\nhuggingface-cli login\\n```\\n```bash\\n\\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_| \\\\_| \\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_|\\\\_| \\\\_|\\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_|\\\\_|\\n\\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\n\\\\_|\\\\_|\\\\_|\\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\\\_| \\\\_| \\\\_|\\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_|\\\\_| \\\\_| \\\\_|\\\\_|\\\\_|\\n\\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\n\\\\_| \\\\_| \\\\_|\\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_| \\\\_| \\\\_| \\\\_|\\\\_|\\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_|\\\\_|\\nUsername:\\nPassword:\\n```\\n3. Alternatively, if you prefer working from a Jupyter or Colaboratory notebook, login with `notebook\\\\_login`:\\n```python\\n>>> from huggingface\\\\_hub import notebook\\\\_login\\n>>> notebook\\\\_login()\\n```\\n`notebook\\\\_login` will launch a widget in your notebook from which you can enter your Hugging Face credentials.\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/code_snippet.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/code_snippet-dark.png)\\n\\nFirst, register your library by editing [model-libaries.ts](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries.ts). Then, add a code snippet by updating the [library-ui-elements](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/library-ui-elements.ts) file with instructions for your model. For example, the [Asteroid](https://huggingface.co/asteroid-team) integration includes a brief code snippet for how to load and use an Asteroid model:\\n```typescript\\nconst asteroid = (model: ModelData) =>\\n`from asteroid.models import BaseModel\\nmodel = BaseModel.from\\\\_pretrained(\"${model.id}\")`;\\n```\\nDoing so will also add a tag to your model so users can quickly identify models from your library. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-tags.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-tags-dark.png)\\n\\n## Upload files to the Hub\\nYou might also want to provide a method for creating model repositories and uploading files to the Hub directly from your library. The `huggingface\\\\_hub` library offers two ways to assist you with creating repositories and uploading files:\\n- `create\\\\_repo` creates a repository on the Hub. - `upload\\\\_file` directly uploads files to a repository on the Hub. ### `create\\\\_repo`\\nThe `create\\\\_repo` method creates a repository on the Hub. Use the `name` parameter to provide a name for your repository:\\n```python\\n>>> from huggingface\\\\_hub import create\\\\_repo\\n>>> create\\\\_repo(repo\\\\_id=\"test-model\")\\n\\'https://huggingface.co/lysandre/test-model\\'\\n```\\nWhen you check your Hugging Face account, you should now see a `test-model` repository under your namespace.\\n\\n## Integrating to the Hub. We\\'re partnering with cool open source ML libraries to provide free model hosting and versioning. You can find the existing integrations [here](https://huggingface.co/docs/hub/libraries). The advantages are:\\n- Free model or dataset hosting for libraries and their users. - Built-in file versioning, even with very large files, thanks to a git-based approach. - Hosted inference API for all models publicly available. - In-browser widgets to play with the uploaded models. - Anyone can upload a new model for your library, they just need to add the corresponding tag for the model to be discoverable. - Fast downloads! We use Cloudfront (a CDN) to geo-replicate downloads so they\\'re blazing fast from anywhere on the globe. - Usage stats and more features to come. If you would like to integrate your library, feel free to open an issue to begin the discussion. We wrote a [step-by-step guide](https://huggingface.co/docs/hub/adding-a-library) with ‚ù§Ô∏è showing how to do this integration. ## Contributions (feature requests, bugs, etc.) are super welcome üíôüíöüíõüíúüß°‚ù§Ô∏è\\nEveryone is welcome to contribute, and we value everybody\\'s contribution. Code is not the only way to help the community. Answering questions, helping others, reaching out and improving the documentations are immensely valuable to the community. We wrote a [contribution guide](https://github.com/huggingface/huggingface\\\\_hub/blob/main/CONTRIBUTING.md) to summarize\\nhow to get started to contribute to this repository.\\n\\nThe Hugging Face Hub[[the-hugging-face-hub]]\\nThe [Hugging Face Hub](https://huggingface.co/) ‚Äì- our main website ‚Äì- is a central platform that enables anyone to discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with more than 10,000 publicly available. We\\'ll focus on the models in this chapter, and take a look at the datasets in Chapter 5. The models in the Hub are not limited to ü§ó Transformers or even NLP. There are models from [Flair](https://github.com/flairNLP/flair) and [AllenNLP](https://github.com/allenai/allennlp) for NLP, [Asteroid](https://github.com/asteroid-team/asteroid) and [pyannote](https://github.com/pyannote/pyannote-audio) for speech, and [timm](https://github.com/rwightman/pytorch-image-models) for vision, to name a few. Each of these models is hosted as a Git repository, which allows versioning and reproducibility. Sharing a model on the Hub means opening it up to the community and making it accessible to anyone looking to easily use it, in turn eliminating their need to train a model on their own and simplifying sharing and usage. Additionally, sharing a model on the Hub automatically deploys a hosted Inference API for that model. Anyone in the community is free to test it out directly on the model\\'s page, with custom inputs and appropriate widgets. The best part is that sharing and using any public model on the Hub is completely free! [Paid plans](https://huggingface.co/pricing) also exist if you wish to share models privately. The video below shows how to navigate the Hub. Having a huggingface.co account is required to follow along this part, as we\\'ll be creating and managing repositories on the Hugging Face Hub: [create an account](https://huggingface.co/join)', 'type': 'tool', 'name': 'data_retriever', 'id': 'cef1db78-f325-479e-8943-3fb315b0e47a', 'tool_call_id': 'call_P66XtXPzpVeVs9EBdwzOiP9W', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c0d8666af48523f51f5a', 'page_content': 'Integrate your library with the Hub\\nThe Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones already integrated include [spaCy](https://spacy.io/usage/projects#huggingface\\\\_hub), [AllenNLP](https://allennlp.org/), and [timm](https://rwightman.github.io/pytorch-image-models/), among many others. Integration means users can download and upload files to the Hub directly from your library. We hope you will integrate your library and join us in democratizing artificial intelligence for everyone. Integrating the Hub with your library provides many benefits, including:\\n- Free model hosting for you and your users. - Built-in file versioning - even for huge files - made possible by [Git-LFS](https://git-lfs.github.com/). - All public models are powered by the [Inference API](https://huggingface.co/docs/api-inference/index). - In-browser widgets allow users to interact with your hosted models directly. This tutorial will help you integrate the Hub into your library so your users can benefit from all the features offered by the Hub. Before you begin, we recommend you create a [Hugging Face account](https://huggingface.co/join) from which you can manage your repositories and files. If you need help with the integration, feel free to open an [issue](https://github.com/huggingface/huggingface\\\\_hub/issues/new/choose), and we would be more than happy to help you. ## Installation\\n1. Install the `huggingface\\\\_hub` library with pip in your environment:\\n```bash\\npython -m pip install huggingface\\\\_hub\\n```\\n2. Once you have successfully installed the `huggingface\\\\_hub` library, log in to your Hugging Face account:\\n```bash\\nhuggingface-cli login\\n```\\n```bash\\n\\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_| \\\\_| \\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_|\\\\_| \\\\_|\\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_|\\\\_|\\n\\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\n\\\\_|\\\\_|\\\\_|\\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\\\_| \\\\_| \\\\_|\\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_|\\\\_| \\\\_| \\\\_|\\\\_|\\\\_|\\n\\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\n\\\\_| \\\\_| \\\\_|\\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_| \\\\_| \\\\_| \\\\_|\\\\_|\\\\_| \\\\_| \\\\_| \\\\_| \\\\_|\\\\_|\\\\_| \\\\_|\\\\_|\\\\_|\\\\_|\\nUsername:\\nPassword:\\n```\\n3. Alternatively, if you prefer working from a Jupyter or Colaboratory notebook, login with `notebook\\\\_login`:\\n```python\\n>>> from huggingface\\\\_hub import notebook\\\\_login\\n>>> notebook\\\\_login()\\n```\\n`notebook\\\\_login` will launch a widget in your notebook from which you can enter your Hugging Face credentials.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c0d8666af48523f51f5c', 'page_content': '![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/code_snippet.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/code_snippet-dark.png)\\n\\nFirst, register your library by editing [model-libaries.ts](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries.ts). Then, add a code snippet by updating the [library-ui-elements](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/library-ui-elements.ts) file with instructions for your model. For example, the [Asteroid](https://huggingface.co/asteroid-team) integration includes a brief code snippet for how to load and use an Asteroid model:\\n```typescript\\nconst asteroid = (model: ModelData) =>\\n`from asteroid.models import BaseModel\\nmodel = BaseModel.from\\\\_pretrained(\"${model.id}\")`;\\n```\\nDoing so will also add a tag to your model so users can quickly identify models from your library. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-tags.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-tags-dark.png)\\n\\n## Upload files to the Hub\\nYou might also want to provide a method for creating model repositories and uploading files to the Hub directly from your library. The `huggingface\\\\_hub` library offers two ways to assist you with creating repositories and uploading files:\\n- `create\\\\_repo` creates a repository on the Hub. - `upload\\\\_file` directly uploads files to a repository on the Hub. ### `create\\\\_repo`\\nThe `create\\\\_repo` method creates a repository on the Hub. Use the `name` parameter to provide a name for your repository:\\n```python\\n>>> from huggingface\\\\_hub import create\\\\_repo\\n>>> create\\\\_repo(repo\\\\_id=\"test-model\")\\n\\'https://huggingface.co/lysandre/test-model\\'\\n```\\nWhen you check your Hugging Face account, you should now see a `test-model` repository under your namespace.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bd7baf3d2ebb270a0c45', 'page_content': \"## Integrating to the Hub. We're partnering with cool open source ML libraries to provide free model hosting and versioning. You can find the existing integrations [here](https://huggingface.co/docs/hub/libraries). The advantages are:\\n- Free model or dataset hosting for libraries and their users. - Built-in file versioning, even with very large files, thanks to a git-based approach. - Hosted inference API for all models publicly available. - In-browser widgets to play with the uploaded models. - Anyone can upload a new model for your library, they just need to add the corresponding tag for the model to be discoverable. - Fast downloads! We use Cloudfront (a CDN) to geo-replicate downloads so they're blazing fast from anywhere on the globe. - Usage stats and more features to come. If you would like to integrate your library, feel free to open an issue to begin the discussion. We wrote a [step-by-step guide](https://huggingface.co/docs/hub/adding-a-library) with ‚ù§Ô∏è showing how to do this integration. ## Contributions (feature requests, bugs, etc.) are super welcome üíôüíöüíõüíúüß°‚ù§Ô∏è\\nEveryone is welcome to contribute, and we value everybody's contribution. Code is not the only way to help the community. Answering questions, helping others, reaching out and improving the documentations are immensely valuable to the community. We wrote a [contribution guide](https://github.com/huggingface/huggingface\\\\_hub/blob/main/CONTRIBUTING.md) to summarize\\nhow to get started to contribute to this repository.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899ba7daf3d2ebb270a08ec', 'page_content': \"The Hugging Face Hub[[the-hugging-face-hub]]\\nThe [Hugging Face Hub](https://huggingface.co/) ‚Äì- our main website ‚Äì- is a central platform that enables anyone to discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with more than 10,000 publicly available. We'll focus on the models in this chapter, and take a look at the datasets in Chapter 5. The models in the Hub are not limited to ü§ó Transformers or even NLP. There are models from [Flair](https://github.com/flairNLP/flair) and [AllenNLP](https://github.com/allenai/allennlp) for NLP, [Asteroid](https://github.com/asteroid-team/asteroid) and [pyannote](https://github.com/pyannote/pyannote-audio) for speech, and [timm](https://github.com/rwightman/pytorch-image-models) for vision, to name a few. Each of these models is hosted as a Git repository, which allows versioning and reproducibility. Sharing a model on the Hub means opening it up to the community and making it accessible to anyone looking to easily use it, in turn eliminating their need to train a model on their own and simplifying sharing and usage. Additionally, sharing a model on the Hub automatically deploys a hosted Inference API for that model. Anyone in the community is free to test it out directly on the model's page, with custom inputs and appropriate widgets. The best part is that sharing and using any public model on the Hub is completely free! [Paid plans](https://huggingface.co/pricing) also exist if you wish to share models privately. The video below shows how to navigate the Hub. Having a huggingface.co account is required to follow along this part, as we'll be creating and managing repositories on the Hugging Face Hub: [create an account](https://huggingface.co/join)\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The new library introduced by Hugging Face for hosting scikit-learn models is called `sklearn`.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--142fda2e-7923-4d4b-af8e-a4de77e7478f-0', 'usage_metadata': {'input_tokens': 2255, 'output_tokens': 21, 'total_tokens': 2276, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:29:06,181 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 49/65 [03:19<01:04,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of Textual Inversion?\\n', 'type': 'human', 'id': 'f26dba3d-a4ba-438b-9f9a-a7374ac03b8e'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_BbY38KPQP9mgEToBB8vulM78', 'function': {'arguments': '{\"query\":\"Textual Inversion purpose\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--70acdae4-ca16-41e8-bb38-4e2074c78021-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Textual Inversion purpose'}, 'id': 'call_BbY38KPQP9mgEToBB8vulM78', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 358, 'output_tokens': 20, 'total_tokens': 378, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '# Textual Inversion fine-tuning example\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\\nColab for training\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\\\_textual\\\\_inversion\\\\_training.ipynb)\\nColab for inference\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\\\_conceptualizer\\\\_inference.ipynb)\\n## Running locally with PyTorch\\n### Installing the dependencies\\nBefore running the scripts, make sure to install the library\\'s training dependencies:\\n\\\\*\\\\*Important\\\\*\\\\*\\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\\\*\\\\*installing from source\\\\*\\\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Textual Inversion\\nTextual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder. [`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings. To learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading\\\\_adapters#textual-inversion) loading guide. ## TextualInversionLoaderMixin\\n[[autodoc]] loaders.textual\\\\_inversion.TextualInversionLoaderMixin\\n\\n# Textual Inversion\\n[Textual Inversion](https://hf.co/papers/2208.01618) is a training technique for personalizing image generation models with just a few example images of what you want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide. If you\\'re training on a GPU with limited vRAM, you should try enabling the `gradient\\\\_checkpointing` and `mixed\\\\_precision` parameters in the training command. You can also reduce your memory footprint by using memory-efficient attention with [xFormers](../optimization/xformers). JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn\\'t support gradient checkpointing or xFormers. With the same configuration and setup as PyTorch, the Flax training script should be at least ~70% faster! This guide will explore the [textual\\\\_inversion.py](https://github.com/huggingface/diffusers/blob/main/examples/textual\\\\_inversion/textual\\\\_inversion.py) script to help you become more familiar with it, and how you can adapt it for your own use-case. Before running the script, make sure you install the library from source:\\n```bash\\ngit clone https://github.com/huggingface/diffusers\\ncd diffusers\\npip install . ```\\nNavigate to the example folder with the training script and install the required dependencies for the script you\\'re using:\\n\\n```bash\\ncd examples/textual\\\\_inversion\\npip install -r requirements.txt\\n```\\n\\n```bash\\ncd examples/textual\\\\_inversion\\npip install -r requirements\\\\_flax.txt\\n```\\n\\nü§ó Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It\\'ll automatically configure your training setup based on your hardware and environment. Take a look at the ü§ó Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more. Initialize an ü§ó Accelerate environment:\\n```bash\\naccelerate config\\n```\\nTo setup a default ü§ó Accelerate environment without choosing any configurations:\\n```bash\\naccelerate config default\\n```\\nOr if your environment doesn\\'t support an interactive shell, like a notebook, you can use:\\n```bash\\nfrom accelerate.utils import write\\\\_basic\\\\_config\\nwrite\\\\_basic\\\\_config()\\n```\\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create\\\\_dataset) guide to learn how to create a dataset that works with the training script.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Textual inversion\\n[[open-in-colab]]\\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer). This guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you\\'re interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text\\\\_inversion) training guide. Import the necessary libraries:\\n```py\\nimport torch\\nfrom diffusers import StableDiffusionPipeline\\nfrom diffusers.utils import make\\\\_image\\\\_grid\\n```\\n## Stable Diffusion 1 and 2\\nPick a Stable Diffusion checkpoint and a pre-learned concept from the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer):\\n```py\\npretrained\\\\_model\\\\_name\\\\_or\\\\_path = \"runwayml/stable-diffusion-v1-5\"\\nrepo\\\\_id\\\\_embeds = \"sd-concepts-library/cat-toy\"\\n```\\nNow you can load a pipeline, and pass the pre-learned concept to it:\\n```py\\npipeline = StableDiffusionPipeline.from\\\\_pretrained(\\npretrained\\\\_model\\\\_name\\\\_or\\\\_path, torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n).to(\"cuda\")\\npipeline.load\\\\_textual\\\\_inversion(repo\\\\_id\\\\_embeds)\\n```\\nCreate a prompt with the pre-learned concept by using the special placeholder token ``, and choose the number of samples and rows of images you\\'d like to generate:\\n```py\\nprompt = \"a grafitti in a favela wall with a  on it\"\\nnum\\\\_samples\\\\_per\\\\_row = 2\\nnum\\\\_rows = 2\\n```\\nThen run the pipeline (feel free to adjust the parameters like `num\\\\_inference\\\\_steps` and `guidance\\\\_scale` to see how they affect image quality), save the generated images and visualize them with the helper function you created at the beginning:\\n```py\\nall\\\\_images = []\\nfor \\\\_ in range(num\\\\_rows):\\nimages = pipeline(prompt, num\\\\_images\\\\_per\\\\_prompt=num\\\\_samples\\\\_per\\\\_row, num\\\\_inference\\\\_steps=50, guidance\\\\_scale=7.5).images\\nall\\\\_images.extend(images)\\ngrid = make\\\\_image\\\\_grid(all\\\\_images, num\\\\_rows, num\\\\_samples\\\\_per\\\\_row)\\ngrid\\n```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/textual_inversion_inference.png)\\n\\n## Stable Diffusion XL\\nStable Diffusion XL (SDXL) can also use textual inversion vectors for inference. In contrast to Stable Diffusion 1 and 2, SDXL has two text encoders so you\\'ll need two textual inversion embeddings - one for each text encoder model. Let\\'s download the SDXL textual inversion embeddings and have a closer look at it\\'s structure:\\n```py\\nfrom huggingface\\\\_hub import hf\\\\_hub\\\\_download\\nfrom safetensors.torch import load\\\\_file\\nfile = hf\\\\_hub\\\\_download(\"dn118/unaestheticXL\", filename=\"unaestheticXLv31.safetensors\")\\nstate\\\\_dict = load\\\\_file(file)\\nstate\\\\_dict\\n```\\n```\\n{\\'clip\\\\_g\\': tensor([[ 0.0077, -0.0112, 0.0065, ..., 0.0195, 0.0159, 0.0275],\\n...,\\n[-0.0170, 0.0213, 0.0143, ..., -0.0302, -0.0240, -0.0362]],\\n\\'clip\\\\_l\\': tensor([[ 0.0023, 0.0192, 0.0213, ..., -0.0385, 0.0048, -0.0011],\\n...,\\n[ 0.0475, -0.0508, -0.0145, ..., 0.0070, -0.0089, -0.0163]],\\n```\\nThere are two tensors, `\"clip\\\\_g\"` and `\"clip\\\\_l\"`. `\"clip\\\\_g\"` corresponds to the bigger text encoder in SDXL and refers to\\n`pipe.text\\\\_encoder\\\\_2` and `\"clip\\\\_l\"` refers to `pipe.text\\\\_encoder`. Now you can load each tensor separately by passing them along with the correct text encoder and tokenizer\\nto [`~loaders.TextualInversionLoaderMixin.load\\\\_textual\\\\_inversion`]:\\n```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\npipe = AutoPipelineForText2Image.from\\\\_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", variant=\"fp16\", torch\\\\_dtype=torch.float16)\\npipe.to(\"cuda\")\\npipe.load\\\\_textual\\\\_inversion(state\\\\_dict[\"clip\\\\_g\"], token=\"unaestheticXLv31\", text\\\\_encoder=pipe.text\\\\_encoder\\\\_2, tokenizer=pipe.tokenizer\\\\_2)\\npipe.load\\\\_textual\\\\_inversion(state\\\\_dict[\"clip\\\\_l\"], token=\"unaestheticXLv31\", text\\\\_encoder=pipe.text\\\\_encoder, tokenizer=pipe.tokenizer)\\n# the embedding should be used as a negative embedding, so we pass it as a negative prompt\\ngenerator = torch.Generator().manual\\\\_seed(33)\\nimage = pipe(\"a woman standing in front of a mountain\", negative\\\\_prompt=\"unaestheticXLv31\", generator=generator).images[0]\\nimage\\n```', 'type': 'tool', 'name': 'data_retriever', 'id': 'd084ac73-dc80-43e4-8fbf-bfcd35871d45', 'tool_call_id': 'call_BbY38KPQP9mgEToBB8vulM78', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3403e684812d15f6873', 'page_content': \"# Textual Inversion fine-tuning example\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\\nColab for training\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\\\_textual\\\\_inversion\\\\_training.ipynb)\\nColab for inference\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\\\_conceptualizer\\\\_inference.ipynb)\\n## Running locally with PyTorch\\n### Installing the dependencies\\nBefore running the scripts, make sure to install the library's training dependencies:\\n\\\\*\\\\*Important\\\\*\\\\*\\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\\\*\\\\*installing from source\\\\*\\\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab2a03e684812d15f67cc', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Textual Inversion\\nTextual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder. [`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings. To learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading\\\\_adapters#textual-inversion) loading guide. ## TextualInversionLoaderMixin\\n[[autodoc]] loaders.textual\\\\_inversion.TextualInversionLoaderMixin', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaf9a3e684812d15f6459', 'page_content': \"# Textual Inversion\\n[Textual Inversion](https://hf.co/papers/2208.01618) is a training technique for personalizing image generation models with just a few example images of what you want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide. If you're training on a GPU with limited vRAM, you should try enabling the `gradient\\\\_checkpointing` and `mixed\\\\_precision` parameters in the training command. You can also reduce your memory footprint by using memory-efficient attention with [xFormers](../optimization/xformers). JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn't support gradient checkpointing or xFormers. With the same configuration and setup as PyTorch, the Flax training script should be at least ~70% faster! This guide will explore the [textual\\\\_inversion.py](https://github.com/huggingface/diffusers/blob/main/examples/textual\\\\_inversion/textual\\\\_inversion.py) script to help you become more familiar with it, and how you can adapt it for your own use-case. Before running the script, make sure you install the library from source:\\n```bash\\ngit clone https://github.com/huggingface/diffusers\\ncd diffusers\\npip install . ```\\nNavigate to the example folder with the training script and install the required dependencies for the script you're using:\\n\\n```bash\\ncd examples/textual\\\\_inversion\\npip install -r requirements.txt\\n```\\n\\n```bash\\ncd examples/textual\\\\_inversion\\npip install -r requirements\\\\_flax.txt\\n```\\n\\nü§ó Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the ü§ó Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more. Initialize an ü§ó Accelerate environment:\\n```bash\\naccelerate config\\n```\\nTo setup a default ü§ó Accelerate environment without choosing any configurations:\\n```bash\\naccelerate config default\\n```\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n```bash\\nfrom accelerate.utils import write\\\\_basic\\\\_config\\nwrite\\\\_basic\\\\_config()\\n```\\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create\\\\_dataset) guide to learn how to create a dataset that works with the training script.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bdd2666af48523f51bb4', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Textual inversion\\n[[open-in-colab]]\\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer). This guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you\\'re interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text\\\\_inversion) training guide. Import the necessary libraries:\\n```py\\nimport torch\\nfrom diffusers import StableDiffusionPipeline\\nfrom diffusers.utils import make\\\\_image\\\\_grid\\n```\\n## Stable Diffusion 1 and 2\\nPick a Stable Diffusion checkpoint and a pre-learned concept from the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer):\\n```py\\npretrained\\\\_model\\\\_name\\\\_or\\\\_path = \"runwayml/stable-diffusion-v1-5\"\\nrepo\\\\_id\\\\_embeds = \"sd-concepts-library/cat-toy\"\\n```\\nNow you can load a pipeline, and pass the pre-learned concept to it:\\n```py\\npipeline = StableDiffusionPipeline.from\\\\_pretrained(\\npretrained\\\\_model\\\\_name\\\\_or\\\\_path, torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n).to(\"cuda\")\\npipeline.load\\\\_textual\\\\_inversion(repo\\\\_id\\\\_embeds)\\n```\\nCreate a prompt with the pre-learned concept by using the special placeholder token ``, and choose the number of samples and rows of images you\\'d like to generate:\\n```py\\nprompt = \"a grafitti in a favela wall with a  on it\"\\nnum\\\\_samples\\\\_per\\\\_row = 2\\nnum\\\\_rows = 2\\n```\\nThen run the pipeline (feel free to adjust the parameters like `num\\\\_inference\\\\_steps` and `guidance\\\\_scale` to see how they affect image quality), save the generated images and visualize them with the helper function you created at the beginning:\\n```py\\nall\\\\_images = []\\nfor \\\\_ in range(num\\\\_rows):\\nimages = pipeline(prompt, num\\\\_images\\\\_per\\\\_prompt=num\\\\_samples\\\\_per\\\\_row, num\\\\_inference\\\\_steps=50, guidance\\\\_scale=7.5).images\\nall\\\\_images.extend(images)\\ngrid = make\\\\_image\\\\_grid(all\\\\_images, num\\\\_rows, num\\\\_samples\\\\_per\\\\_row)\\ngrid\\n```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/textual_inversion_inference.png)\\n\\n## Stable Diffusion XL\\nStable Diffusion XL (SDXL) can also use textual inversion vectors for inference. In contrast to Stable Diffusion 1 and 2, SDXL has two text encoders so you\\'ll need two textual inversion embeddings - one for each text encoder model. Let\\'s download the SDXL textual inversion embeddings and have a closer look at it\\'s structure:\\n```py\\nfrom huggingface\\\\_hub import hf\\\\_hub\\\\_download\\nfrom safetensors.torch import load\\\\_file\\nfile = hf\\\\_hub\\\\_download(\"dn118/unaestheticXL\", filename=\"unaestheticXLv31.safetensors\")\\nstate\\\\_dict = load\\\\_file(file)\\nstate\\\\_dict\\n```\\n```\\n{\\'clip\\\\_g\\': tensor([[ 0.0077, -0.0112, 0.0065, ..., 0.0195, 0.0159, 0.0275],\\n...,\\n[-0.0170, 0.0213, 0.0143, ..., -0.0302, -0.0240, -0.0362]],\\n\\'clip\\\\_l\\': tensor([[ 0.0023, 0.0192, 0.0213, ..., -0.0385, 0.0048, -0.0011],\\n...,\\n[ 0.0475, -0.0508, -0.0145, ..., 0.0070, -0.0089, -0.0163]],\\n```\\nThere are two tensors, `\"clip\\\\_g\"` and `\"clip\\\\_l\"`. `\"clip\\\\_g\"` corresponds to the bigger text encoder in SDXL and refers to\\n`pipe.text\\\\_encoder\\\\_2` and `\"clip\\\\_l\"` refers to `pipe.text\\\\_encoder`. Now you can load each tensor separately by passing them along with the correct text encoder and tokenizer\\nto [`~loaders.TextualInversionLoaderMixin.load\\\\_textual\\\\_inversion`]:\\n```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\npipe = AutoPipelineForText2Image.from\\\\_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", variant=\"fp16\", torch\\\\_dtype=torch.float16)\\npipe.to(\"cuda\")\\npipe.load\\\\_textual\\\\_inversion(state\\\\_dict[\"clip\\\\_g\"], token=\"unaestheticXLv31\", text\\\\_encoder=pipe.text\\\\_encoder\\\\_2, tokenizer=pipe.tokenizer\\\\_2)\\npipe.load\\\\_textual\\\\_inversion(state\\\\_dict[\"clip\\\\_l\"], token=\"unaestheticXLv31\", text\\\\_encoder=pipe.text\\\\_encoder, tokenizer=pipe.tokenizer)\\n# the embedding should be used as a negative embedding, so we pass it as a negative prompt\\ngenerator = torch.Generator().manual\\\\_seed(33)\\nimage = pipe(\"a woman standing in front of a mountain\", negative\\\\_prompt=\"unaestheticXLv31\", generator=generator).images[0]\\nimage\\n```', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'Textual Inversion is a training technique for personalizing image generation models by learning new text embeddings from just a few example images. This method allows a model, like Stable Diffusion, to learn a new concept from a small number of sample images, giving users more control over the generated images and enabling them to tailor the model towards specific concepts. The new embeddings are tied to a special word that must be used in the prompt, allowing for personalized image generation based on the provided examples.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f719a65b-03ef-47a5-8f9c-8cc59412bc52-0', 'usage_metadata': {'input_tokens': 2873, 'output_tokens': 97, 'total_tokens': 2970, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:29:10,613 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 50/65 [03:24<01:02,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\\n', 'type': 'human', 'id': 'd2a5b7c4-9086-4a4c-9467-abde897f6223'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_nUQCX8Emj0WFuMvbwDLI1xxE', 'function': {'arguments': '{\"query\":\"recommended multiple of batch size for fp16 data type on A100 GPU\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--dd85832b-5622-48b6-a866-c6301313d51c-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'recommended multiple of batch size for fp16 data type on A100 GPU'}, 'id': 'call_nUQCX8Emj0WFuMvbwDLI1xxE', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 367, 'output_tokens': 29, 'total_tokens': 396, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': \"For instance, for fp16 data type a multiple of 8 is recommended, unless\\nit's an A100 GPU, in which case use multiples of 64. For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup. ## Gradient Accumulation\\nThe \\\\*\\\\*gradient accumulation\\\\*\\\\* method aims to calculate gradients in smaller increments instead of computing them for the\\nentire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward\\nand backward passes through the model and accumulating the gradients during the process. Once a sufficient number of\\ngradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\\nbecomes possible to increase the \\\\*\\\\*effective batch size\\\\*\\\\* beyond the limitations imposed by the GPU's memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can\\nslow down the training process. You can enable gradient accumulation by adding the `gradient\\\\_accumulation\\\\_steps` argument to [`TrainingArguments`]:\\n```py\\ntraining\\\\_args = TrainingArguments(per\\\\_device\\\\_train\\\\_batch\\\\_size=1, gradient\\\\_accumulation\\\\_steps=4, \\\\*\\\\*default\\\\_args)\\n```\\nIn the above example, your effective batch size becomes 4.\\n\\nIf you are not familiar with this data format, please have a look [at the bits layout]( https://en.wikipedia.org/wiki/Bfloat16\\\\_floating-point\\\\_format#bfloat16\\\\_floating-point\\\\_format). The key to BF16 format is that it has the same exponent as FP32 and thus doesn't suffer from overflow FP16 suffers from a lot! With FP16, which has a max numerical range of 64k, you can only multiply small numbers. e.g. you can do `250\\\\*250=62500`, but if you were to try `255\\\\*255=65025` you got yourself an overflow, which is what causes the main problems during training. This means your weights have to remain tiny. A technique called loss scaling can help with this problem, but the limited range of FP16 is still an issue when models become very large. BF16 has no such problem, you can easily do `10\\\\_000\\\\*10\\\\_000=100\\\\_000\\\\_000` and it's no problem. Of course, since BF16 and FP16 have the same size of 2 bytes, one doesn't get a free lunch and one pays with really bad precision when using BF16. However, if you remember the training using stochastic gradient descent and its variations is a sort of stumbling walk, so if you don't get the perfect direction immediately it's no problem, you will correct yourself in the next steps. Regardless of whether one uses BF16 or FP16 there is also a copy of weights which is always in FP32 - this is what gets updated by the optimizer. So the 16-bit formats are only used for the computation, the optimizer updates the FP32 weights with full precision and then casts them into the 16-bit format for the next iteration. All PyTorch components have been updated to ensure that they perform any accumulation in FP32, so no loss happening there. One crucial issue is gradient accumulation, and it's one of the main features of pipeline parallelism as the gradients from each microbatch processing get accumulated. It's crucial to implement gradient accumulation in FP32 to keep the training precise, and this is what `BF16Optimizer` does. Besides other improvements we believe that using BF16 mixed precision training turned a potential nightmare into a relatively smooth process which can be observed from the following lm loss graph:\\n![176B-fail](assets/86\\\\_bloom\\\\_megatron\\\\_deepspeed/176b-lm-loss.png)\\n## Fused CUDA Kernels\\nThe GPU performs two things.\\n\\nTried to allocate 40.00 MiB (GPU 0; 23.65 GiB total capacity; 22.27 GiB already allocated; 20.31 MiB free; 22.76 GiB reserved in total by PyTorch) |\\n| DDP | 1 | 2 | NA | OOM Error RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.65 GiB total capacity; 22.27 GiB already allocated; 20.31 MiB free; 22.76 GiB reserved in total by PyTorch) |\\n| DDP + FP16 | 1 | 1 | NA | OOM Error RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.65 GiB total capacity; 22.27 GiB already allocated; 20.31 MiB free; 22.76 GiB reserved in total by PyTorch) |\\n| FSDP with min\\\\_num\\\\_params = 2K | 5 | 2 | 0.6 | |\\n| FSDP with min\\\\_num\\\\_params = 2K + Offload to CPU | 10 | 1 | 3 | |\\n| FSDP with min\\\\_num\\\\_params = 2K + Offload to CPU | 14 | 2 | 1.16 | |\\nTable 2: Benchmarking FSDP on GPT-2 XL (1.5B) model\\nFrom Table 2, we can observe that DDP (w and w/o fp16) isn‚Äôt even able to run with batch size of 1 and results in CUDA OOM error. FSDP with Zero-Stage 3 is able to be run on 2 GPUs with batch size of 5 (effective batch size =10 (5 X 2)). FSDP with CPU offload can further increase the max batch size to 14 per GPU when using 2 GPUs. \\\\*\\\\*FSDP with CPU offload enables training GPT-2 1.5B model on a single GPU with a batch size of 10\\\\*\\\\*. This enables ML practitioners with minimal compute resources to train such large models, thereby democratizing large model training. ## Capabilities and limitations of the FSDP Integration\\nLet‚Äôs dive into the current support that Accelerate provides for FSDP integration and the known limitations. \\\\*\\\\*Required PyTorch version for FSDP support\\\\*\\\\*: PyTorch Nightly (or 1.12.0 if you read this after it has been released) as the model saving with FSDP activated is only available with recent fixes.\\n\\nIt's extremely important to avoid the hug of death. On this benchmark the initial performance was (on 16xA100 40Go on GCP which is the machine used throughout):\\nRequests/s : 0.3 (throughput)\\nLatency: 350ms/token (latency)\\nThose numbers are not that great. Before getting to work let's estimate\\nthe best we can imagine achieving. The formula for amount of operations is `24Bsh^2 + 4ùêµs^2h24Bsh^2 + 4ùêµs^2h` where `B` is\\nthe batch size, `s` the sequence length, and `h` the hidden dimension. Let's do the math and we are getting `17 TFlop` for a single forward pass. Looking at the [specs](https://www.nvidia.com/en-us/data-center/a100/) of A100 it claims `312 TFLOPS` for a single card. That means a single GPU could potentially run at `17 / 312 = 54ms/token`. We're using 16 of those so `3ms/token` on the overall\\nmachine. Take all these numbers with a big grain of salt, it's never possible to reach those numbers,\\nand real-life performance rarely matches the specs. Also if computation is not your limiting\\nfactor then this is not the lowest you can get. It's just good practice to know how far you are from\\nyour target. In this case, we're 2 orders of magnitude so pretty far. Also, this estimate puts\\nall the flops at the service of latency which means only a single request can go at a time (it's ok since you're maximizing your machine\\nso there's not much else to be done, but we can have higher latency and get throughput back through batching much more easily). # Exploring many routes\\n```\\nNote: Tensor Parallelism (TP) means in this context that each GPU will own\\npart of the weights, so ALL gpus are active all the time and do less work.\", 'type': 'tool', 'name': 'data_retriever', 'id': '8be79bc7-696b-48d0-ba28-f7d89a4b5fe3', 'tool_call_id': 'call_nUQCX8Emj0WFuMvbwDLI1xxE', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b90faf3d2ebb270a0708', 'page_content': \"For instance, for fp16 data type a multiple of 8 is recommended, unless\\nit's an A100 GPU, in which case use multiples of 64. For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup. ## Gradient Accumulation\\nThe \\\\*\\\\*gradient accumulation\\\\*\\\\* method aims to calculate gradients in smaller increments instead of computing them for the\\nentire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward\\nand backward passes through the model and accumulating the gradients during the process. Once a sufficient number of\\ngradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\\nbecomes possible to increase the \\\\*\\\\*effective batch size\\\\*\\\\* beyond the limitations imposed by the GPU's memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can\\nslow down the training process. You can enable gradient accumulation by adding the `gradient\\\\_accumulation\\\\_steps` argument to [`TrainingArguments`]:\\n```py\\ntraining\\\\_args = TrainingArguments(per\\\\_device\\\\_train\\\\_batch\\\\_size=1, gradient\\\\_accumulation\\\\_steps=4, \\\\*\\\\*default\\\\_args)\\n```\\nIn the above example, your effective batch size becomes 4.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c102666af48523f51f98', 'page_content': \"If you are not familiar with this data format, please have a look [at the bits layout]( https://en.wikipedia.org/wiki/Bfloat16\\\\_floating-point\\\\_format#bfloat16\\\\_floating-point\\\\_format). The key to BF16 format is that it has the same exponent as FP32 and thus doesn't suffer from overflow FP16 suffers from a lot! With FP16, which has a max numerical range of 64k, you can only multiply small numbers. e.g. you can do `250\\\\*250=62500`, but if you were to try `255\\\\*255=65025` you got yourself an overflow, which is what causes the main problems during training. This means your weights have to remain tiny. A technique called loss scaling can help with this problem, but the limited range of FP16 is still an issue when models become very large. BF16 has no such problem, you can easily do `10\\\\_000\\\\*10\\\\_000=100\\\\_000\\\\_000` and it's no problem. Of course, since BF16 and FP16 have the same size of 2 bytes, one doesn't get a free lunch and one pays with really bad precision when using BF16. However, if you remember the training using stochastic gradient descent and its variations is a sort of stumbling walk, so if you don't get the perfect direction immediately it's no problem, you will correct yourself in the next steps. Regardless of whether one uses BF16 or FP16 there is also a copy of weights which is always in FP32 - this is what gets updated by the optimizer. So the 16-bit formats are only used for the computation, the optimizer updates the FP32 weights with full precision and then casts them into the 16-bit format for the next iteration. All PyTorch components have been updated to ensure that they perform any accumulation in FP32, so no loss happening there. One crucial issue is gradient accumulation, and it's one of the main features of pipeline parallelism as the gradients from each microbatch processing get accumulated. It's crucial to implement gradient accumulation in FP32 to keep the training precise, and this is what `BF16Optimizer` does. Besides other improvements we believe that using BF16 mixed precision training turned a potential nightmare into a relatively smooth process which can be observed from the following lm loss graph:\\n![176B-fail](assets/86\\\\_bloom\\\\_megatron\\\\_deepspeed/176b-lm-loss.png)\\n## Fused CUDA Kernels\\nThe GPU performs two things.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab14b3e684812d15f6638', 'page_content': 'Tried to allocate 40.00 MiB (GPU 0; 23.65 GiB total capacity; 22.27 GiB already allocated; 20.31 MiB free; 22.76 GiB reserved in total by PyTorch) |\\n| DDP | 1 | 2 | NA | OOM Error RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.65 GiB total capacity; 22.27 GiB already allocated; 20.31 MiB free; 22.76 GiB reserved in total by PyTorch) |\\n| DDP + FP16 | 1 | 1 | NA | OOM Error RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.65 GiB total capacity; 22.27 GiB already allocated; 20.31 MiB free; 22.76 GiB reserved in total by PyTorch) |\\n| FSDP with min\\\\_num\\\\_params = 2K | 5 | 2 | 0.6 | |\\n| FSDP with min\\\\_num\\\\_params = 2K + Offload to CPU | 10 | 1 | 3 | |\\n| FSDP with min\\\\_num\\\\_params = 2K + Offload to CPU | 14 | 2 | 1.16 | |\\nTable 2: Benchmarking FSDP on GPT-2 XL (1.5B) model\\nFrom Table 2, we can observe that DDP (w and w/o fp16) isn‚Äôt even able to run with batch size of 1 and results in CUDA OOM error. FSDP with Zero-Stage 3 is able to be run on 2 GPUs with batch size of 5 (effective batch size =10 (5 X 2)). FSDP with CPU offload can further increase the max batch size to 14 per GPU when using 2 GPUs. \\\\*\\\\*FSDP with CPU offload enables training GPT-2 1.5B model on a single GPU with a batch size of 10\\\\*\\\\*. This enables ML practitioners with minimal compute resources to train such large models, thereby democratizing large model training. ## Capabilities and limitations of the FSDP Integration\\nLet‚Äôs dive into the current support that Accelerate provides for FSDP integration and the known limitations. \\\\*\\\\*Required PyTorch version for FSDP support\\\\*\\\\*: PyTorch Nightly (or 1.12.0 if you read this after it has been released) as the model saving with FSDP activated is only available with recent fixes.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aab873e684812d15f5f96', 'page_content': \"It's extremely important to avoid the hug of death. On this benchmark the initial performance was (on 16xA100 40Go on GCP which is the machine used throughout):\\nRequests/s : 0.3 (throughput)\\nLatency: 350ms/token (latency)\\nThose numbers are not that great. Before getting to work let's estimate\\nthe best we can imagine achieving. The formula for amount of operations is `24Bsh^2 + 4ùêµs^2h24Bsh^2 + 4ùêµs^2h` where `B` is\\nthe batch size, `s` the sequence length, and `h` the hidden dimension. Let's do the math and we are getting `17 TFlop` for a single forward pass. Looking at the [specs](https://www.nvidia.com/en-us/data-center/a100/) of A100 it claims `312 TFLOPS` for a single card. That means a single GPU could potentially run at `17 / 312 = 54ms/token`. We're using 16 of those so `3ms/token` on the overall\\nmachine. Take all these numbers with a big grain of salt, it's never possible to reach those numbers,\\nand real-life performance rarely matches the specs. Also if computation is not your limiting\\nfactor then this is not the lowest you can get. It's just good practice to know how far you are from\\nyour target. In this case, we're 2 orders of magnitude so pretty far. Also, this estimate puts\\nall the flops at the service of latency which means only a single request can go at a time (it's ok since you're maximizing your machine\\nso there's not much else to be done, but we can have higher latency and get throughput back through batching much more easily). # Exploring many routes\\n```\\nNote: Tensor Parallelism (TP) means in this context that each GPU will own\\npart of the weights, so ALL gpus are active all the time and do less work.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'For fp16 data type on an A100 GPU, it is recommended to use multiples of 64.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--a5739838-62a3-4cca-ab84-5b7bf8f61fea-0', 'usage_metadata': {'input_tokens': 2110, 'output_tokens': 21, 'total_tokens': 2131, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:29:16,768 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 51/65 [03:30<01:06,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'How do you run a Gradio Blocks app in reload mode using a Python IDE?\\n', 'type': 'human', 'id': 'd7eb5daa-58d7-4f4f-9dc1-295b906f31f9'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_63TRisrxmbQkXBGrAU5fa5qe', 'function': {'arguments': '{\"query\":\"Gradio Blocks app reload mode Python IDE\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--1b687bfa-aaf9-40f4-9f4d-de9070c88483-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Gradio Blocks app reload mode Python IDE'}, 'id': 'call_63TRisrxmbQkXBGrAU5fa5qe', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 23, 'total_tokens': 388, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Developing Faster with Auto-Reloading\\n\\\\*\\\\*Prerequisite\\\\*\\\\*: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control). This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks. ## Why Auto-Reloading? When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes. To make it faster and more convenient to write your code, we\\'ve made it easier to \"reload\" your Gradio apps instantly when you are developing in a \\\\*\\\\*Python IDE\\\\*\\\\* (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We\\'ve also developed an analogous \"magic command\" that allows you to re-run cells faster if you use \\\\*\\\\*Jupyter Notebooks\\\\*\\\\* (or any similar environment like Colab). This short Guide will cover both of these methods, so no matter how you write Python, you\\'ll leave knowing how to build Gradio apps faster. ## Python IDE Reload üî•\\nIf you are building Gradio Blocks using a Python IDE, your file of code (let\\'s name it `run.py`) might look something like this:\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as demo:\\ngr.Markdown(\"# Greetings from Gradio!\")\\ninp = gr.Textbox(placeholder=\"What is your name?\")\\nout = gr.Textbox()\\ninp.change(fn=lambda x: f\"Welcome, {x}!\",\\ninputs=inp,\\noutputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\ndemo.launch()\\n```\\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\\n\\nÈÄöËøáËá™Âä®ÈáçËΩΩÂÆûÁé∞Êõ¥Âø´ÁöÑÂºÄÂèë\\n\\\\*\\\\*ÂÖàÂÜ≥Êù°‰ª∂\\\\*\\\\*ÔºöÊú¨ÊåáÂçóË¶ÅÊ±ÇÊÇ®‰∫ÜËß£ÂùóÁöÑÁü•ËØÜ„ÄÇËØ∑Á°Æ‰øù[ÂÖàÈòÖËØªÂùóÊåáÂçó](https://gradio.app/quickstart/#blocks-more-flexibility-and-control)„ÄÇ\\nÊú¨ÊåáÂçó‰ªãÁªç‰∫ÜËá™Âä®ÈáçÊñ∞Âä†ËΩΩ„ÄÅÂú® Python IDE ‰∏≠ÈáçÊñ∞Âä†ËΩΩ‰ª•ÂèäÂú® Jupyter Notebooks ‰∏≠‰ΩøÁî® gradio ÁöÑÊñπÊ≥ï„ÄÇ\\n## ‰∏∫‰ªÄ‰πàË¶Å‰ΩøÁî®Ëá™Âä®ÈáçËΩΩÔºü\\nÂΩìÊÇ®ÊûÑÂª∫ Gradio ÊºîÁ§∫Êó∂ÔºåÁâπÂà´ÊòØ‰ΩøÁî® Blocks ÊûÑÂª∫Êó∂ÔºåÊÇ®ÂèØËÉΩ‰ºöÂèëÁé∞ÂèçÂ§çËøêË°å‰ª£Á†Å‰ª•ÊµãËØïÊõ¥ÊîπÂæàÈ∫ªÁÉ¶„ÄÇ\\n‰∏∫‰∫ÜÊõ¥Âø´ÈÄü„ÄÅÊõ¥‰æøÊç∑Âú∞ÁºñÂÜô‰ª£Á†ÅÔºåÊàë‰ª¨Â∑≤ÁªèÁÆÄÂåñ‰∫ÜÂú® \\\\*\\\\*Python IDE\\\\*\\\\*ÔºàÂ¶Ç VS Code„ÄÅSublime Text„ÄÅPyCharm Á≠âÔºâ‰∏≠ÂºÄÂèëÊàñ‰ªéÁªàÁ´ØËøêË°å Python ‰ª£Á†ÅÊó∂‚ÄúÈáçÊñ∞Âä†ËΩΩ‚ÄùGradio Â∫îÁî®ÁöÑÊñπÂºè„ÄÇÊàë‰ª¨ËøòÂºÄÂèë‰∫Ü‰∏Ä‰∏™Á±ª‰ººÁöÑ‚ÄúÈ≠îÊ≥ïÂëΩ‰ª§‚ÄùÔºå‰ΩøÊÇ®ÂèØ‰ª•Êõ¥Âø´ÈÄüÂú∞ÈáçÊñ∞ËøêË°åÂçïÂÖÉÊ†ºÔºåÂ¶ÇÊûúÊÇ®‰ΩøÁî® Jupyter NotebooksÔºàÊàñÁ±ª‰ººÁöÑÁéØÂ¢ÉÔºåÂ¶Ç ColabÔºâÁöÑËØù„ÄÇ\\nËøô‰∏™ÁÆÄÁü≠ÁöÑÊåáÂçóÂ∞ÜÊ∂µÁõñËøô‰∏§ÁßçÊñπÊ≥ïÔºåÊâÄ‰ª•Êó†ËÆ∫ÊÇ®Â¶Ç‰ΩïÁºñÂÜô Python ‰ª£Á†ÅÔºåÊÇ®ÈÉΩÂ∞ÜÁü•ÈÅìÂ¶Ç‰ΩïÊõ¥Âø´Âú∞ÊûÑÂª∫ Gradio Â∫îÁî®Á®ãÂ∫è„ÄÇ\\n## Python IDE ÈáçËΩΩ üî•\\nÂ¶ÇÊûúÊÇ®‰ΩøÁî® Python IDE ÊûÑÂª∫ Gradio BlocksÔºåÈÇ£‰πà‰ª£Á†ÅÊñá‰ª∂ÔºàÂÅáËÆæÂëΩÂêç‰∏∫ `run.py`ÔºâÂèØËÉΩÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as demo:\\ngr.Markdown(\"# Êù•Ëá™GradioÁöÑÈóÆÂÄôÔºÅ\")\\ninp = gr.Textbox(placeholder=\"ÊÇ®Âè´‰ªÄ‰πàÂêçÂ≠óÔºü\")\\nout = gr.Textbox()\\ninp.change(fn=lambda x: f\"Ê¨¢ËøéÔºå{x}ÔºÅ\",\\ninputs=inp,\\noutputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\ndemo.launch()\\n```\\nÈóÆÈ¢òÂú®‰∫éÔºåÊØèÂΩìÊÇ®ÊÉ≥Ë¶ÅÊõ¥ÊîπÂ∏ÉÂ±Ä„ÄÅ‰∫ã‰ª∂ÊàñÁªÑ‰ª∂Êó∂ÔºåÈÉΩÂøÖÈ°ªÈÄöËøáÁºñÂÜô `python run.py` Êù•ÂÖ≥Èó≠ÂíåÈáçÊñ∞ËøêË°åÂ∫îÁî®Á®ãÂ∫è„ÄÇ\\nËÄå‰∏çÊòØËøôÊ†∑ÂÅöÔºåÊÇ®ÂèØ‰ª•ÈÄöËøáÊõ¥Êîπ 1 ‰∏™ÂçïËØçÊù•‰ª•\\\\*\\\\*ÈáçÊñ∞Âä†ËΩΩÊ®°Âºè\\\\*\\\\*ËøêË°å‰ª£Á†ÅÔºöÂ∞Ü `python` Êõ¥Êîπ‰∏∫ `gradio`Ôºö\\nÂú®ÁªàÁ´Ø‰∏≠ËøêË°å `gradio run.py`„ÄÇÂ∞±ÊòØËøôÊ†∑ÔºÅ\\nÁé∞Âú®ÔºåÊÇ®Â∞ÜÁúãÂà∞Á±ª‰ºº‰∫éËøôÊ†∑ÁöÑÂÜÖÂÆπÔºö\\n```bash\\nLaunching in \\\\*reload mode\\\\* on: http://127.0.0.1:7860 (Press CTRL+C to quit)\\nWatching...\\n\\nInstead of doing this, you can run your code in \\\\*\\\\*reload mode\\\\*\\\\* by changing 1 word: `python` to `gradio`:\\nIn the terminal, run `gradio run.py`. That\\'s it! Now, you\\'ll see that after you\\'ll see something like this:\\n```bash\\nWatching: \\'/Users/freddy/sources/gradio/gradio\\', \\'/Users/freddy/sources/gradio/demo/\\'\\nRunning on local URL: http://127.0.0.1:7860\\n```\\nThe important part here is the line that says `Watching...` What\\'s happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically ü•≥\\n‚ö†Ô∏è Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show\\\\_error` in `launch()` will not be reflected in the app. There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as my\\\\_demo:\\ngr.Markdown(\"# Greetings from Gradio!\")\\ninp = gr.Textbox(placeholder=\"What is your name?\")\\nout = gr.Textbox()\\ninp.change(fn=lambda x: f\"Welcome, {x}!\",\\ninputs=inp,\\noutputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\nmy\\\\_demo.launch()\\n```\\nThen you would launch it in reload mode like this: `gradio run.py my\\\\_demo`.\\n\\nGradio Demo: blocks\\\\_scroll\\n```\\n!pip install -q gradio\\n```\\n```\\nimport gradio as gr\\ndemo = gr.Blocks()\\nwith demo:\\ninp = gr.Textbox(placeholder=\"Enter text.\")\\nscroll\\\\_btn = gr.Button(\"Scroll\")\\nno\\\\_scroll\\\\_btn = gr.Button(\"No Scroll\")\\nbig\\\\_block = gr.HTML(\"\"\"\"\"\")\\nout = gr.Textbox()\\nscroll\\\\_btn.click(lambda x: x,\\ninputs=inp,\\noutputs=out,\\nscroll\\\\_to\\\\_output=True)\\nno\\\\_scroll\\\\_btn.click(lambda x: x,\\ninputs=inp,\\noutputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\ndemo.launch()\\n```', 'type': 'tool', 'name': 'data_retriever', 'id': 'c62214a2-46a2-43da-b47a-545b7ce820d5', 'tool_call_id': 'call_63TRisrxmbQkXBGrAU5fa5qe', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c2f2666af48523f52222', 'page_content': 'Developing Faster with Auto-Reloading\\n\\\\*\\\\*Prerequisite\\\\*\\\\*: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control). This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks. ## Why Auto-Reloading? When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes. To make it faster and more convenient to write your code, we\\'ve made it easier to \"reload\" your Gradio apps instantly when you are developing in a \\\\*\\\\*Python IDE\\\\*\\\\* (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We\\'ve also developed an analogous \"magic command\" that allows you to re-run cells faster if you use \\\\*\\\\*Jupyter Notebooks\\\\*\\\\* (or any similar environment like Colab). This short Guide will cover both of these methods, so no matter how you write Python, you\\'ll leave knowing how to build Gradio apps faster. ## Python IDE Reload üî•\\nIf you are building Gradio Blocks using a Python IDE, your file of code (let\\'s name it `run.py`) might look something like this:\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as demo:\\ngr.Markdown(\"# Greetings from Gradio!\")\\ninp = gr.Textbox(placeholder=\"What is your name?\")\\nout = gr.Textbox()\\ninp.change(fn=lambda x: f\"Welcome, {x}!\",\\ninputs=inp,\\noutputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\ndemo.launch()\\n```\\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bf1a666af48523f51d77', 'page_content': 'ÈÄöËøáËá™Âä®ÈáçËΩΩÂÆûÁé∞Êõ¥Âø´ÁöÑÂºÄÂèë\\n\\\\*\\\\*ÂÖàÂÜ≥Êù°‰ª∂\\\\*\\\\*ÔºöÊú¨ÊåáÂçóË¶ÅÊ±ÇÊÇ®‰∫ÜËß£ÂùóÁöÑÁü•ËØÜ„ÄÇËØ∑Á°Æ‰øù[ÂÖàÈòÖËØªÂùóÊåáÂçó](https://gradio.app/quickstart/#blocks-more-flexibility-and-control)„ÄÇ\\nÊú¨ÊåáÂçó‰ªãÁªç‰∫ÜËá™Âä®ÈáçÊñ∞Âä†ËΩΩ„ÄÅÂú® Python IDE ‰∏≠ÈáçÊñ∞Âä†ËΩΩ‰ª•ÂèäÂú® Jupyter Notebooks ‰∏≠‰ΩøÁî® gradio ÁöÑÊñπÊ≥ï„ÄÇ\\n## ‰∏∫‰ªÄ‰πàË¶Å‰ΩøÁî®Ëá™Âä®ÈáçËΩΩÔºü\\nÂΩìÊÇ®ÊûÑÂª∫ Gradio ÊºîÁ§∫Êó∂ÔºåÁâπÂà´ÊòØ‰ΩøÁî® Blocks ÊûÑÂª∫Êó∂ÔºåÊÇ®ÂèØËÉΩ‰ºöÂèëÁé∞ÂèçÂ§çËøêË°å‰ª£Á†Å‰ª•ÊµãËØïÊõ¥ÊîπÂæàÈ∫ªÁÉ¶„ÄÇ\\n‰∏∫‰∫ÜÊõ¥Âø´ÈÄü„ÄÅÊõ¥‰æøÊç∑Âú∞ÁºñÂÜô‰ª£Á†ÅÔºåÊàë‰ª¨Â∑≤ÁªèÁÆÄÂåñ‰∫ÜÂú® \\\\*\\\\*Python IDE\\\\*\\\\*ÔºàÂ¶Ç VS Code„ÄÅSublime Text„ÄÅPyCharm Á≠âÔºâ‰∏≠ÂºÄÂèëÊàñ‰ªéÁªàÁ´ØËøêË°å Python ‰ª£Á†ÅÊó∂‚ÄúÈáçÊñ∞Âä†ËΩΩ‚ÄùGradio Â∫îÁî®ÁöÑÊñπÂºè„ÄÇÊàë‰ª¨ËøòÂºÄÂèë‰∫Ü‰∏Ä‰∏™Á±ª‰ººÁöÑ‚ÄúÈ≠îÊ≥ïÂëΩ‰ª§‚ÄùÔºå‰ΩøÊÇ®ÂèØ‰ª•Êõ¥Âø´ÈÄüÂú∞ÈáçÊñ∞ËøêË°åÂçïÂÖÉÊ†ºÔºåÂ¶ÇÊûúÊÇ®‰ΩøÁî® Jupyter NotebooksÔºàÊàñÁ±ª‰ººÁöÑÁéØÂ¢ÉÔºåÂ¶Ç ColabÔºâÁöÑËØù„ÄÇ\\nËøô‰∏™ÁÆÄÁü≠ÁöÑÊåáÂçóÂ∞ÜÊ∂µÁõñËøô‰∏§ÁßçÊñπÊ≥ïÔºåÊâÄ‰ª•Êó†ËÆ∫ÊÇ®Â¶Ç‰ΩïÁºñÂÜô Python ‰ª£Á†ÅÔºåÊÇ®ÈÉΩÂ∞ÜÁü•ÈÅìÂ¶Ç‰ΩïÊõ¥Âø´Âú∞ÊûÑÂª∫ Gradio Â∫îÁî®Á®ãÂ∫è„ÄÇ\\n## Python IDE ÈáçËΩΩ üî•\\nÂ¶ÇÊûúÊÇ®‰ΩøÁî® Python IDE ÊûÑÂª∫ Gradio BlocksÔºåÈÇ£‰πà‰ª£Á†ÅÊñá‰ª∂ÔºàÂÅáËÆæÂëΩÂêç‰∏∫ `run.py`ÔºâÂèØËÉΩÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as demo:\\ngr.Markdown(\"# Êù•Ëá™GradioÁöÑÈóÆÂÄôÔºÅ\")\\ninp = gr.Textbox(placeholder=\"ÊÇ®Âè´‰ªÄ‰πàÂêçÂ≠óÔºü\")\\nout = gr.Textbox()\\ninp.change(fn=lambda x: f\"Ê¨¢ËøéÔºå{x}ÔºÅ\",\\ninputs=inp,\\noutputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\ndemo.launch()\\n```\\nÈóÆÈ¢òÂú®‰∫éÔºåÊØèÂΩìÊÇ®ÊÉ≥Ë¶ÅÊõ¥ÊîπÂ∏ÉÂ±Ä„ÄÅ‰∫ã‰ª∂ÊàñÁªÑ‰ª∂Êó∂ÔºåÈÉΩÂøÖÈ°ªÈÄöËøáÁºñÂÜô `python run.py` Êù•ÂÖ≥Èó≠ÂíåÈáçÊñ∞ËøêË°åÂ∫îÁî®Á®ãÂ∫è„ÄÇ\\nËÄå‰∏çÊòØËøôÊ†∑ÂÅöÔºåÊÇ®ÂèØ‰ª•ÈÄöËøáÊõ¥Êîπ 1 ‰∏™ÂçïËØçÊù•‰ª•\\\\*\\\\*ÈáçÊñ∞Âä†ËΩΩÊ®°Âºè\\\\*\\\\*ËøêË°å‰ª£Á†ÅÔºöÂ∞Ü `python` Êõ¥Êîπ‰∏∫ `gradio`Ôºö\\nÂú®ÁªàÁ´Ø‰∏≠ËøêË°å `gradio run.py`„ÄÇÂ∞±ÊòØËøôÊ†∑ÔºÅ\\nÁé∞Âú®ÔºåÊÇ®Â∞ÜÁúãÂà∞Á±ª‰ºº‰∫éËøôÊ†∑ÁöÑÂÜÖÂÆπÔºö\\n```bash\\nLaunching in \\\\*reload mode\\\\* on: http://127.0.0.1:7860 (Press CTRL+C to quit)\\nWatching...', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c2f2666af48523f52223', 'page_content': 'Instead of doing this, you can run your code in \\\\*\\\\*reload mode\\\\*\\\\* by changing 1 word: `python` to `gradio`:\\nIn the terminal, run `gradio run.py`. That\\'s it! Now, you\\'ll see that after you\\'ll see something like this:\\n```bash\\nWatching: \\'/Users/freddy/sources/gradio/gradio\\', \\'/Users/freddy/sources/gradio/demo/\\'\\nRunning on local URL: http://127.0.0.1:7860\\n```\\nThe important part here is the line that says `Watching...` What\\'s happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically ü•≥\\n‚ö†Ô∏è Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show\\\\_error` in `launch()` will not be reflected in the app. There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as my\\\\_demo:\\ngr.Markdown(\"# Greetings from Gradio!\")\\ninp = gr.Textbox(placeholder=\"What is your name?\")\\nout = gr.Textbox()\\ninp.change(fn=lambda x: f\"Welcome, {x}!\",\\ninputs=inp,\\noutputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\nmy\\\\_demo.launch()\\n```\\nThen you would launch it in reload mode like this: `gradio run.py my\\\\_demo`.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b51faf3d2ebb270a0269', 'page_content': 'Gradio Demo: blocks\\\\_scroll\\n```\\n!pip install -q gradio\\n```\\n```\\nimport gradio as gr\\ndemo = gr.Blocks()\\nwith demo:\\ninp = gr.Textbox(placeholder=\"Enter text.\")\\nscroll\\\\_btn = gr.Button(\"Scroll\")\\nno\\\\_scroll\\\\_btn = gr.Button(\"No Scroll\")\\nbig\\\\_block = gr.HTML(\"\"\"\"\"\")\\nout = gr.Textbox()\\nscroll\\\\_btn.click(lambda x: x,\\ninputs=inp,\\noutputs=out,\\nscroll\\\\_to\\\\_output=True)\\nno\\\\_scroll\\\\_btn.click(lambda x: x,\\ninputs=inp,\\noutputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\ndemo.launch()\\n```', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'To run a Gradio Blocks app in reload mode using a Python IDE, you need to change the command you use to launch your app. Instead of running your script with `python run.py`, you should run it with `gradio run.py`. \\n\\nThis will enable reload mode, allowing Gradio to automatically observe the directory where your `run.py` file is located. If any changes are made to the file, Gradio will automatically rerun the file for you, making it easier to test your changes without manually restarting the app.\\n\\nFor example, if your code file is named `run.py`, you would execute the following command in your terminal:\\n\\n```bash\\ngradio run.py\\n```\\n\\nThis will start your Gradio app in reload mode.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--ec962ef5-55a0-41cb-b201-99117fa5283b-0', 'usage_metadata': {'input_tokens': 1814, 'output_tokens': 152, 'total_tokens': 1966, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:29:26,458 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 52/65 [03:40<01:21,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'How can you install the Hugging Face Unity API in your Unity project?\\n', 'type': 'human', 'id': '6eb713e9-7a90-42b4-89b2-35b39019a787'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_ZElW7zjvirGlUtudJSwZEjir', 'function': {'arguments': '{\"query\":\"install Hugging Face Unity API in Unity project\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--3ac18dd6-a56b-48be-a6fe-306e071bc15d-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'install Hugging Face Unity API in Unity project'}, 'id': 'call_ZElW7zjvirGlUtudJSwZEjir', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 363, 'output_tokens': 24, 'total_tokens': 387, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '--\\ntitle: \"How to Install and Use the Hugging Face Unity API\"\\nthumbnail: /blog/assets/124\\\\_ml-for-games/unity-api-thumbnail.png\\nauthors:\\n- user: dylanebert\\n---\\n# How to Install and Use the Hugging Face Unity API\\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we\\'ll walk through the steps to install and use the Hugging Face Unity API. ## Installation\\n1. Open your Unity project\\n2. Go to `Window` -> `Package Manager`\\n3. Click `+` and select `Add Package from git URL`\\n4. Enter `https://github.com/huggingface/unity-api.git`\\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif)\\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens). 7. Test the API key by clicking `Test API key` in the API Wizard. 8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API\\\\_URL` field.\\n\\nTo use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard. ## Usage Tips\\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks. 2. Address slow response times or performance issues by changing model endpoints to lower resource models. ## Conclusion\\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!\\n\\n9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png)\\nNow that the API is set up, you can make calls from your scripts to the API. Let\\'s look at an example of performing a Sentence Similarity task:\\n```\\nusing HuggingFace.API;\\n/\\\\* other code \\\\*/\\n// Make a call to the API\\nvoid Query() {\\nstring inputText = \"I\\'m on my way to the forest.\";\\nstring[] candidates = {\\n\"The player is going to the city\",\\n\"The player is going to the wilderness\",\\n\"The player is wandering aimlessly\"\\n};\\nHuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\\n}\\n// If successful, handle the result\\nvoid OnSuccess(float[] result) {\\nforeach(float value in result) {\\nDebug.Log(value);\\n}\\n}\\n// Otherwise, handle the error\\nvoid OnError(string error) {\\nDebug.LogError(error);\\n}\\n/\\\\* other code \\\\*/\\n```\\n## Supported Tasks and Custom Models\\nThe Hugging Face Unity API also currently supports the following tasks:\\n- [Conversation](https://huggingface.co/tasks/conversational)\\n- [Text Generation](https://huggingface.co/tasks/text-generation)\\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\\n- [Text Classification](https://huggingface.co/tasks/text-classification)\\n- [Question Answering](https://huggingface.co/tasks/question-answering)\\n- [Translation](https://huggingface.co/tasks/translation)\\n- [Summarization](https://huggingface.co/tasks/summarization)\\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\\n\\np align=\"center\">\\n\\n![huggingface_hub library logo](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingface_hub.svg)\\n\\n*The official Python client for the Huggingface Hub.*\\n\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc)](https://huggingface.co/docs/huggingface_hub/en/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub/releases)\\n[![PyPi version](https://img.shields.io/pypi/pyversions/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub)\\n[![downloads](https://static.pepy.tech/badge/huggingface_hub/month)](https://pypi.org/project/huggingface-hub)\\n[![Code coverage](https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL)](https://codecov.io/gh/huggingface/huggingface_hub)\\n\\n#### **English** | [Deutsch](https://github.com/huggingface/huggingface_hub/blob/main/README_de.md) | [‡§π‡§ø‡§Ç‡§¶‡•Ä](https://github.com/huggingface/huggingface_hub/blob/main/README_hi.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/huggingface_hub/blob/main/README_ko.md) | [‰∏≠ÊñáÔºàÁÆÄ‰ΩìÔºâ](https://github.com/huggingface/huggingface_hub/blob/main/README_cn.md)\\n\\n---\\n\\\\*\\\\*Documentation\\\\*\\\\*: <https://hf.co/docs/huggingface_hub>\\n\\\\*\\\\*Source Code\\\\*\\\\*: <https://github.com/huggingface/huggingface_hub>\\n---\\n## Welcome to the huggingface\\\\_hub library\\nThe `huggingface\\\\_hub` library allows you to interact with the [Hugging Face Hub](https://huggingface.co/), a platform democratizing open-source Machine Learning for creators and collaborators.', 'type': 'tool', 'name': 'data_retriever', 'id': '0d41ed8a-abfc-4c46-a1d9-83d7133a1fc1', 'tool_call_id': 'call_ZElW7zjvirGlUtudJSwZEjir', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b8a8af3d2ebb270a069c', 'page_content': '--\\ntitle: \"How to Install and Use the Hugging Face Unity API\"\\nthumbnail: /blog/assets/124\\\\_ml-for-games/unity-api-thumbnail.png\\nauthors:\\n- user: dylanebert\\n---\\n# How to Install and Use the Hugging Face Unity API\\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we\\'ll walk through the steps to install and use the Hugging Face Unity API. ## Installation\\n1. Open your Unity project\\n2. Go to `Window` -> `Package Manager`\\n3. Click `+` and select `Add Package from git URL`\\n4. Enter `https://github.com/huggingface/unity-api.git`\\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif)\\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens). 7. Test the API key by clicking `Test API key` in the API Wizard. 8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API\\\\_URL` field.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b8a8af3d2ebb270a069e', 'page_content': 'To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard. ## Usage Tips\\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks. 2. Address slow response times or performance issues by changing model endpoints to lower resource models. ## Conclusion\\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b8a8af3d2ebb270a069d', 'page_content': '9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png)\\nNow that the API is set up, you can make calls from your scripts to the API. Let\\'s look at an example of performing a Sentence Similarity task:\\n```\\nusing HuggingFace.API;\\n/\\\\* other code \\\\*/\\n// Make a call to the API\\nvoid Query() {\\nstring inputText = \"I\\'m on my way to the forest.\";\\nstring[] candidates = {\\n\"The player is going to the city\",\\n\"The player is going to the wilderness\",\\n\"The player is wandering aimlessly\"\\n};\\nHuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\\n}\\n// If successful, handle the result\\nvoid OnSuccess(float[] result) {\\nforeach(float value in result) {\\nDebug.Log(value);\\n}\\n}\\n// Otherwise, handle the error\\nvoid OnError(string error) {\\nDebug.LogError(error);\\n}\\n/\\\\* other code \\\\*/\\n```\\n## Supported Tasks and Custom Models\\nThe Hugging Face Unity API also currently supports the following tasks:\\n- [Conversation](https://huggingface.co/tasks/conversational)\\n- [Text Generation](https://huggingface.co/tasks/text-generation)\\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\\n- [Text Classification](https://huggingface.co/tasks/text-classification)\\n- [Question Answering](https://huggingface.co/tasks/question-answering)\\n- [Translation](https://huggingface.co/tasks/translation)\\n- [Summarization](https://huggingface.co/tasks/summarization)\\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bd7baf3d2ebb270a0c42', 'page_content': 'p align=\"center\">\\n\\n![huggingface_hub library logo](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingface_hub.svg)\\n\\n*The official Python client for the Huggingface Hub.*\\n\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc)](https://huggingface.co/docs/huggingface_hub/en/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub/releases)\\n[![PyPi version](https://img.shields.io/pypi/pyversions/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub)\\n[![downloads](https://static.pepy.tech/badge/huggingface_hub/month)](https://pypi.org/project/huggingface-hub)\\n[![Code coverage](https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL)](https://codecov.io/gh/huggingface/huggingface_hub)\\n\\n#### **English** | [Deutsch](https://github.com/huggingface/huggingface_hub/blob/main/README_de.md) | [‡§π‡§ø‡§Ç‡§¶‡•Ä](https://github.com/huggingface/huggingface_hub/blob/main/README_hi.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/huggingface_hub/blob/main/README_ko.md) | [‰∏≠ÊñáÔºàÁÆÄ‰ΩìÔºâ](https://github.com/huggingface/huggingface_hub/blob/main/README_cn.md)\\n\\n---\\n\\\\*\\\\*Documentation\\\\*\\\\*: <https://hf.co/docs/huggingface_hub>\\n\\\\*\\\\*Source Code\\\\*\\\\*: <https://github.com/huggingface/huggingface_hub>\\n---\\n## Welcome to the huggingface\\\\_hub library\\nThe `huggingface\\\\_hub` library allows you to interact with the [Hugging Face Hub](https://huggingface.co/), a platform democratizing open-source Machine Learning for creators and collaborators.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"To install the Hugging Face Unity API in your Unity project, follow these steps:\\n\\n1. Open your Unity project.\\n2. Go to `Window` -> `Package Manager`.\\n3. Click `+` and select `Add Package from git URL`.\\n4. Enter `https://github.com/huggingface/unity-api.git`.\\n5. Once installed, the Unity API wizard should pop up. If it doesn't, go to `Window` -> `Hugging Face API Wizard`.\\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).\\n7. Test the API key by clicking `Test API key` in the API Wizard.\\n8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the URL from the `API_URL` field.\\n9. Configure advanced settings if desired.\\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard. \\n\\nNow the API is set up, and you can make calls from your scripts to the API.\", 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f9986f54-33dd-4f0c-9b6e-69fe5e716c30-0', 'usage_metadata': {'input_tokens': 1723, 'output_tokens': 266, 'total_tokens': 1989, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:29:30,498 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 53/65 [03:44<01:06,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the pretraining objective of the Wav2Vec2 context network?\\n', 'type': 'human', 'id': '9998ad2e-f2cd-47dd-8734-4a20ecd52491'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_B6xQzGzz4SJWhOL9bHcRh4qz', 'function': {'arguments': '{\"query\":\"Wav2Vec2 pretraining objective\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--588a7952-a7d5-4218-a082-3118539f3335-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Wav2Vec2 pretraining objective'}, 'id': 'call_B6xQzGzz4SJWhOL9bHcRh4qz', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 364, 'output_tokens': 23, 'total_tokens': 387, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Wav2Vec2\\n## Overview\\nThe Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. The abstract from the paper is the following:\\n\\\\*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\\nrecognition with limited amounts of labeled data.\\\\*\\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). ## Usage tips\\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\\nusing [`Wav2Vec2CTCTokenizer`]. ## Resources\\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with Wav2Vec2.\\n\\nNote that many training parameters can be set in the model configuration including\\nthe configuration about the masking distribution (`mask\\\\_time\\\\_length`, `mask\\\\_time\\\\_prob`),\\ndropout (`attention\\\\_dropout`, ...), the trade-off between the contrastive loss and\\nthe diversity loss, etc... Mostly likely you will need to change these parameters depending on your use case. Again, we highly recommend to read the [official paper](https://arxiv.org/abs/2006.11477)\\nto better understand which parameters can be set for pretraining. For this example, we will be using a `\"base\"`-sized model of Wav2Vec2 with robust\\nlayer norm and keep most of the default settings. ```python\\nmodel\\\\_dir=\"./wav2vec2-base-robust\"\\nfrom transformers import Wav2Vec2Config\\nconfig = Wav2Vec2Config.from\\\\_pretrained(\\n\"facebook/wav2vec2-base\",\\nmask\\\\_time\\\\_length=10,\\nmask\\\\_time\\\\_prob=0.05,\\ndiversity\\\\_loss\\\\_weight=0.1,\\nnum\\\\_negatives=100,\\ndo\\\\_stable\\\\_layer\\\\_norm=True,\\nfeat\\\\_extract\\\\_norm=\"layer\",\\n)\\nconfig.save\\\\_pretrained(model\\\\_dir)\\n```\\n### Create a feature extractor configuration\\nBefore we can start the training, we need to define\\na feature extractor that takes care of normalization, etc... Here we can also re-use the feature extractor of [wav2vec2-base-960h](https://huggingface.co/facebook/wav2vec2-base) while making sure that padding is allowed. ```python\\nmodel\\\\_dir=\"./wav2vec2-base-robust\"\\nfrom transformers import Wav2Vec2FeatureExtractor\\nconfig = Wav2Vec2FeatureExtractor.from\\\\_pretrained(\"facebook/wav2vec2-base\", return\\\\_attention\\\\_mask=True)\\nconfig.save\\\\_pretrained(model\\\\_dir)\\n```\\n### Train the model\\nFinally, we can run the example script to train the model:\\n```bash\\n./run\\\\_wav2vec2\\\\_pretrain\\\\_flax.py \\\\\\n--output\\\\_dir=${MODEL\\\\_DIR} \\\\\\n--num\\\\_train\\\\_epochs=\"5\" \\\\\\n--per\\\\_device\\\\_train\\\\_batch\\\\_size=\"32\" \\\\\\n--per\\\\_device\\\\_eval\\\\_batch\\\\_size=\"32\" \\\\\\n--learning\\\\_rate=\"5e-4\" \\\\\\n--weight\\\\_decay=\"0.01\" \\\\\\n--warmup\\\\_steps=\"2000\" \\\\\\n--model\\\\_name\\\\_or\\\\_path=${MODEL\\\\_DIR} \\\\\\n--dataset\\\\_name=\"librispeech\\\\_asr\" \\\\\\n--dataset\\\\_config\\\\_name=\"clean\" \\\\\\n--train\\\\_split\\\\_name=\"train.100\" \\\\\\n--preprocessing\\\\_num\\\\_workers=\"4\" \\\\\\n--max\\\\_duration\\\\_in\\\\_seconds=\"10.0\" \\\\\\n--adam\\\\_beta1=\"0.9\" \\\\\\n--adam\\\\_beta2=\"0.98\" \\\\\\n--pad\\\\_to\\\\_multiple\\\\_of=\"16384\" \\\\\\n--push\\\\_to\\\\_hub\\n```\\nNote that this script is not fully tested yet, so we cannot ensure that\\nthe above script leads to satisfying results.\\n\\nIf you wish to implement your own costumized data loading/sampling, feel free to just make use of the `\"path\"` column instead and disregard the `\"audio\"` column. Training & Evaluation\\n---------------------\\nThe data is processed so that we are ready to start setting up the\\ntraining pipeline. We will make use of ü§ó\\\\\\'s\\n[Trainer](https://huggingface.co/transformers/master/main\\\\_classes/trainer.html?highlight=trainer)\\nfor which we essentially need to do the following:\\n- Define a data collator. In contrast to most NLP models, Wav2Vec2 has\\na much larger input length than output length. \\\\*E.g.\\\\*, a sample of\\ninput length 50000 has an output length of no more than 100. Given\\nthe large input sizes, it is much more efficient to pad the training\\nbatches dynamically meaning that all training samples should only be\\npadded to the longest sample in their batch and not the overall\\nlongest sample. Therefore, fine-tuning Wav2Vec2 requires a special\\npadding data collator, which we will define below\\n- Evaluation metric.\\n\\nWav2Vec2 Contrastive Loss PreTraining examples\\nThe following example showcases how to pretrain a wav2vec2 model using the JAX/Flax backend. Pretraining Wav2Vec2 is rather complex, so it is highly recommended to read the\\n[official paper](https://arxiv.org/abs/2006.11477). JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU. Models written in JAX/Flax are \\\\*\\\\*immutable\\\\*\\\\* and updated in a purely functional\\nway which enables simple and efficient model parallelism. `run\\\\_wav2vec2\\\\_pretrain\\\\_flax.py` is a lightweight example of how to download and preprocess a dataset from the ü§ó Datasets library or use your own files (jsonlines or csv), then pretrain the wav2vec2 architectures above on it. For custom datasets in `jsonlines` format please see: [the Datasets documentation](https://huggingface.co/docs/datasets/loading\\\\_datasets#json-files) and you also will find examples of these below. Let\\'s start by creating a model repository to save the trained model and logs. Here we call the model `\"wav2vec2-base-robust\"`, but you can change the model name as you like. You can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\\nyou are logged in) or via the command line:\\n```\\nhuggingface-cli repo create wav2vec2-base-robust\\n```\\nNext we clone the model repository to add the tokenizer and model files. ```\\ngit clone https://huggingface.co//wav2vec2-base-robust\\n```\\nTo ensure that all tensorboard traces will be uploaded correctly, we need to\\ntrack them. You can run the following command inside your model repo to do so. ```\\ncd wav2vec2-base-robust\\ngit lfs track \"\\\\*tfevents\\\\*\"\\n```\\nGreat, we have set up our model repository. During training, we will automatically\\npush the training logs and model weights to the repo. Next, let\\'s add a symbolic link to the `run\\\\_wav2vec2\\\\_pretrain\\\\_flax`. ```bash\\nexport MODEL\\\\_DIR=\"./wav2vec2-base-robust\"\\nln -s ~/transformers/examples/research\\\\_projects/jax-projects/wav2vec2/run\\\\_wav2vec2\\\\_pretrain\\\\_flax.py ./\\n```\\n### Create the model configuration\\nLet\\'s first create the model configuration and store it in the model repository.', 'type': 'tool', 'name': 'data_retriever', 'id': '74ad1f78-4fe0-464c-b43e-95658ec0c56b', 'tool_call_id': 'call_B6xQzGzz4SJWhOL9bHcRh4qz', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bdea666af48523f51bd4', 'page_content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Wav2Vec2\\n## Overview\\nThe Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. The abstract from the paper is the following:\\n\\\\*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\\nrecognition with limited amounts of labeled data.\\\\*\\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). ## Usage tips\\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\\nusing [`Wav2Vec2CTCTokenizer`]. ## Resources\\nA list of official Hugging Face and community (indicated by üåé) resources to help you get started with Wav2Vec2.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaa3d3e684812d15f5e27', 'page_content': 'Note that many training parameters can be set in the model configuration including\\nthe configuration about the masking distribution (`mask\\\\_time\\\\_length`, `mask\\\\_time\\\\_prob`),\\ndropout (`attention\\\\_dropout`, ...), the trade-off between the contrastive loss and\\nthe diversity loss, etc... Mostly likely you will need to change these parameters depending on your use case. Again, we highly recommend to read the [official paper](https://arxiv.org/abs/2006.11477)\\nto better understand which parameters can be set for pretraining. For this example, we will be using a `\"base\"`-sized model of Wav2Vec2 with robust\\nlayer norm and keep most of the default settings. ```python\\nmodel\\\\_dir=\"./wav2vec2-base-robust\"\\nfrom transformers import Wav2Vec2Config\\nconfig = Wav2Vec2Config.from\\\\_pretrained(\\n\"facebook/wav2vec2-base\",\\nmask\\\\_time\\\\_length=10,\\nmask\\\\_time\\\\_prob=0.05,\\ndiversity\\\\_loss\\\\_weight=0.1,\\nnum\\\\_negatives=100,\\ndo\\\\_stable\\\\_layer\\\\_norm=True,\\nfeat\\\\_extract\\\\_norm=\"layer\",\\n)\\nconfig.save\\\\_pretrained(model\\\\_dir)\\n```\\n### Create a feature extractor configuration\\nBefore we can start the training, we need to define\\na feature extractor that takes care of normalization, etc... Here we can also re-use the feature extractor of [wav2vec2-base-960h](https://huggingface.co/facebook/wav2vec2-base) while making sure that padding is allowed. ```python\\nmodel\\\\_dir=\"./wav2vec2-base-robust\"\\nfrom transformers import Wav2Vec2FeatureExtractor\\nconfig = Wav2Vec2FeatureExtractor.from\\\\_pretrained(\"facebook/wav2vec2-base\", return\\\\_attention\\\\_mask=True)\\nconfig.save\\\\_pretrained(model\\\\_dir)\\n```\\n### Train the model\\nFinally, we can run the example script to train the model:\\n```bash\\n./run\\\\_wav2vec2\\\\_pretrain\\\\_flax.py \\\\\\n--output\\\\_dir=${MODEL\\\\_DIR} \\\\\\n--num\\\\_train\\\\_epochs=\"5\" \\\\\\n--per\\\\_device\\\\_train\\\\_batch\\\\_size=\"32\" \\\\\\n--per\\\\_device\\\\_eval\\\\_batch\\\\_size=\"32\" \\\\\\n--learning\\\\_rate=\"5e-4\" \\\\\\n--weight\\\\_decay=\"0.01\" \\\\\\n--warmup\\\\_steps=\"2000\" \\\\\\n--model\\\\_name\\\\_or\\\\_path=${MODEL\\\\_DIR} \\\\\\n--dataset\\\\_name=\"librispeech\\\\_asr\" \\\\\\n--dataset\\\\_config\\\\_name=\"clean\" \\\\\\n--train\\\\_split\\\\_name=\"train.100\" \\\\\\n--preprocessing\\\\_num\\\\_workers=\"4\" \\\\\\n--max\\\\_duration\\\\_in\\\\_seconds=\"10.0\" \\\\\\n--adam\\\\_beta1=\"0.9\" \\\\\\n--adam\\\\_beta2=\"0.98\" \\\\\\n--pad\\\\_to\\\\_multiple\\\\_of=\"16384\" \\\\\\n--push\\\\_to\\\\_hub\\n```\\nNote that this script is not fully tested yet, so we cannot ensure that\\nthe above script leads to satisfying results.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab4e23e684812d15f6a80', 'page_content': 'If you wish to implement your own costumized data loading/sampling, feel free to just make use of the `\"path\"` column instead and disregard the `\"audio\"` column. Training & Evaluation\\n---------------------\\nThe data is processed so that we are ready to start setting up the\\ntraining pipeline. We will make use of ü§ó\\\\\\'s\\n[Trainer](https://huggingface.co/transformers/master/main\\\\_classes/trainer.html?highlight=trainer)\\nfor which we essentially need to do the following:\\n- Define a data collator. In contrast to most NLP models, Wav2Vec2 has\\na much larger input length than output length. \\\\*E.g.\\\\*, a sample of\\ninput length 50000 has an output length of no more than 100. Given\\nthe large input sizes, it is much more efficient to pad the training\\nbatches dynamically meaning that all training samples should only be\\npadded to the longest sample in their batch and not the overall\\nlongest sample. Therefore, fine-tuning Wav2Vec2 requires a special\\npadding data collator, which we will define below\\n- Evaluation metric.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaa3d3e684812d15f5e26', 'page_content': 'Wav2Vec2 Contrastive Loss PreTraining examples\\nThe following example showcases how to pretrain a wav2vec2 model using the JAX/Flax backend. Pretraining Wav2Vec2 is rather complex, so it is highly recommended to read the\\n[official paper](https://arxiv.org/abs/2006.11477). JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU. Models written in JAX/Flax are \\\\*\\\\*immutable\\\\*\\\\* and updated in a purely functional\\nway which enables simple and efficient model parallelism. `run\\\\_wav2vec2\\\\_pretrain\\\\_flax.py` is a lightweight example of how to download and preprocess a dataset from the ü§ó Datasets library or use your own files (jsonlines or csv), then pretrain the wav2vec2 architectures above on it. For custom datasets in `jsonlines` format please see: [the Datasets documentation](https://huggingface.co/docs/datasets/loading\\\\_datasets#json-files) and you also will find examples of these below. Let\\'s start by creating a model repository to save the trained model and logs. Here we call the model `\"wav2vec2-base-robust\"`, but you can change the model name as you like. You can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\\nyou are logged in) or via the command line:\\n```\\nhuggingface-cli repo create wav2vec2-base-robust\\n```\\nNext we clone the model repository to add the tokenizer and model files. ```\\ngit clone https://huggingface.co//wav2vec2-base-robust\\n```\\nTo ensure that all tensorboard traces will be uploaded correctly, we need to\\ntrack them. You can run the following command inside your model repo to do so. ```\\ncd wav2vec2-base-robust\\ngit lfs track \"\\\\*tfevents\\\\*\"\\n```\\nGreat, we have set up our model repository. During training, we will automatically\\npush the training logs and model weights to the repo. Next, let\\'s add a symbolic link to the `run\\\\_wav2vec2\\\\_pretrain\\\\_flax`. ```bash\\nexport MODEL\\\\_DIR=\"./wav2vec2-base-robust\"\\nln -s ~/transformers/examples/research\\\\_projects/jax-projects/wav2vec2/run\\\\_wav2vec2\\\\_pretrain\\\\_flax.py ./\\n```\\n### Create the model configuration\\nLet\\'s first create the model configuration and store it in the model repository.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The pretraining objective of the Wav2Vec2 context network is to mask the speech input in the latent space and solve a contrastive task defined over a quantization of the latent representations, which are jointly learned. This approach allows the model to learn powerful representations from speech audio alone, followed by fine-tuning on transcribed speech.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--e3275d1e-995e-46c5-8d7d-652c81cade8d-0', 'usage_metadata': {'input_tokens': 2371, 'output_tokens': 68, 'total_tokens': 2439, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:29:33,869 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 54/65 [03:47<00:54,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\\n', 'type': 'human', 'id': '64823c96-fd97-4041-ba79-0e1a52c43e77'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_HCBrm17IS5wVbc2vu8wJvHQ2', 'function': {'arguments': '{\"query\":\"default checkpoint sentiment analysis pipeline Transformers library\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--e3006055-ae29-41e3-aa79-ef97ae00572e-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'default checkpoint sentiment analysis pipeline Transformers library'}, 'id': 'call_HCBrm17IS5wVbc2vu8wJvHQ2', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 364, 'output_tokens': 22, 'total_tokens': 386, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let\\'s look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\\\_tensors option tells the tokenizer to return a TensorFlow tensor.\\n\\nhat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let\\'s look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\\\_tensors option tells the tokenizer to return a PyTorch tensor.\\n\\nWe want Transformers to enable developers, researchers, students, professors, engineers, and anyone\\nelse to build their dream projects. In order to celebrate the 100,000 stars of transformers, we have decided to put the spotlight on the\\ncommunity, and we have created the [awesome-transformers](./awesome-transformers.md) page which lists 100\\nincredible projects built in the vicinity of transformers. If you own or use a project that you believe should be part of the list, please open a PR to add it! ## If you are looking for custom support from the Hugging Face team\\n[![HuggingFace Expert Acceleration Program](https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png)](https://huggingface.co/support)  \\n## Quick tour\\nTo immediately use a model on a given input (text, image, audio, ...), we provide the `pipeline` API. Pipelines group together a pretrained model with the preprocessing that was used during that model\\'s training. Here is how to quickly use a pipeline to classify positive versus negative texts:\\n```python\\n>>> from transformers import pipeline\\n# Allocate a pipeline for sentiment-analysis\\n>>> classifier = pipeline(\\'sentiment-analysis\\')\\n>>> classifier(\\'We are very happy to introduce pipeline to the transformers repository.\\')\\n[{\\'label\\': \\'POSITIVE\\', \\'score\\': 0.9996980428695679}]\\n```\\nThe second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text.\\n\\n!---\\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\\n\\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\\n\\n#### **English** | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Espa√±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [Êó•Êú¨Ë™û](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [–†—É—Å—Å–∫–∏–π](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [–†ortugu√™s](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](https://github.com/huggingface/transformers//blob/main/README_te.md) |\\n\\n### State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\\n\\n### \\n\\nü§ó Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.', 'type': 'tool', 'name': 'data_retriever', 'id': '0e35a0a3-238b-4681-a8f1-b3f16c25b3db', 'tool_call_id': 'call_HCBrm17IS5wVbc2vu8wJvHQ2', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab2d93e684812d15f680c', 'page_content': \"hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\\\_tensors option tells the tokenizer to return a TensorFlow tensor.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aabe43e684812d15f601a', 'page_content': \"hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\\\_tensors option tells the tokenizer to return a PyTorch tensor.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b76daf3d2ebb270a04f1', 'page_content': \"We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\\nelse to build their dream projects. In order to celebrate the 100,000 stars of transformers, we have decided to put the spotlight on the\\ncommunity, and we have created the [awesome-transformers](./awesome-transformers.md) page which lists 100\\nincredible projects built in the vicinity of transformers. If you own or use a project that you believe should be part of the list, please open a PR to add it! ## If you are looking for custom support from the Hugging Face team\\n[![HuggingFace Expert Acceleration Program](https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png)](https://huggingface.co/support)  \\n## Quick tour\\nTo immediately use a model on a given input (text, image, audio, ...), we provide the `pipeline` API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts:\\n```python\\n>>> from transformers import pipeline\\n# Allocate a pipeline for sentiment-analysis\\n>>> classifier = pipeline('sentiment-analysis')\\n>>> classifier('We are very happy to introduce pipeline to the transformers repository.')\\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\\n```\\nThe second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b76daf3d2ebb270a04ed', 'page_content': '!---\\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\\n\\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\\n\\n#### **English** | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Espa√±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [Êó•Êú¨Ë™û](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [–†—É—Å—Å–∫–∏–π](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [–†ortugu√™s](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](https://github.com/huggingface/transformers//blob/main/README_te.md) |\\n\\n### State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\\n\\n### \\n\\nü§ó Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The default checkpoint used by the sentiment analysis pipeline in the Transformers library is `distilbert base uncased finetuned sst2 english`.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--6ec4dc8d-960f-4c95-8cc6-cfa172550f45-0', 'usage_metadata': {'input_tokens': 2098, 'output_tokens': 29, 'total_tokens': 2127, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:29:38,901 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 55/65 [03:52<00:49,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\\n', 'type': 'human', 'id': '3b374270-bc3f-4d5d-82d3-573cc932afab'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_3pUYEx152oV5lGCj0L84ZNae', 'function': {'arguments': '{\"query\":\"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--9ffc0282-06fc-4933-b4a5-eee269d9081c-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'How to use DeepSpeed to train models with billions of parameters on Habana Gaudi'}, 'id': 'call_3pUYEx152oV5lGCj0L84ZNae', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 374, 'output_tokens': 31, 'total_tokens': 405, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Note that while [CUDA graphs](https://developer.nvidia.com/blog/cuda-graphs/) are not currently compatible with model parallelism in DeepSpeed (DeepSpeed v0.8.2, see [here](https://github.com/microsoft/DeepSpeed/blob/v0.8.2/deepspeed/inference/engine.py#L158)), HPU graphs are supported in Habana\\'s DeepSpeed fork. All benchmarks are doing [greedy generation](https://huggingface.co/blog/how-to-generate#greedy-search) of 100 token outputs. The input prompt is:\\n> \"DeepSpeed is a machine learning framework\"\\nwhich consists of 7 tokens with BLOOM\\'s tokenizer. The results for inference latency are displayed in the table below (the unit is \\\\*seconds\\\\*). | Model | Number of devices | Gaudi2 latency (seconds) | A100-80GB latency (seconds) | First-gen Gaudi latency (seconds) |\\n|:-----------:|:-----------------:|:-------------------------:|:-----------------:|:----------------------------------:|\\n| BLOOMZ | 8 | 3.103 | 4.402 | / |\\n| BLOOMZ-7B | 8 | 0.734 | 2.417 | 3.321 |\\n| BLOOMZ-7B | 1 | 0.772 | 2.119 | 2.387 |\\n\\\\*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, leading to a\\\\* x\\\\*1.42 speedup on BLOOMZ with Gaudi2 compared to A100.\\\\*\\nThe Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. \\\\*\\\\*For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB\\\\*\\\\*. Smaller checkpoints present interesting results too. \\\\*\\\\*Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!\\\\*\\\\* It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device. We also ran these models on first-gen Gaudi. While it is slower than Gaudi2, it is interesting from a price perspective as a DL1 instance on AWS costs approximately 13\\\\$ per hour. Latency for BLOOMZ-7B on first-gen Gaudi is 2.387 seconds. Thus, \\\\*\\\\*first-gen Gaudi offers for the 7-billion checkpoint a better price-performance ratio than A100\\\\*\\\\* which costs more than 30\\\\$ per hour! We expect the Habana team will optimize the performance of these models in the upcoming SynapseAI releases. For example, in our last benchmark, we saw that [Gaudi2 performs Stable Diffusion inference 2.2x faster than A100](https://huggingface.co/blog/habana-gaudi-2-benchmark#generating-images-from-text-with-stable-diffusion) and this has since been improved further to 2.37x with the latest optimizations provided by Habana. We will update these numbers as new versions of SynapseAI are released and integrated within Optimum Habana.\\n\\nThe Gaudi runs were performed in \\\\*bfloat16\\\\* precision and the A100 runs in \\\\*fp16\\\\* precision (more information [here](https://huggingface.co/docs/diffusers/optimization/fp16)). All runs were \\\\*single-device\\\\* runs. ### Fine-tuning T5-3B\\nWith 96 GB of memory per device, Gaudi2 enables running much bigger models. For instance, we managed to fine-tune T5-3B (containing 3 billion parameters) with gradient checkpointing being the only applied memory optimization. This is not possible on first-gen Gaudi. [Here](https://huggingface.co/regisss/t5-3b-summarization-gaudi-2/tensorboard?scroll=1#scalars) are the logs of this run where the model was fine-tuned on the CNN DailyMail dataset for text summarization using [this script](https://github.com/huggingface/optimum-habana/tree/main/examples/summarization). The results we achieved are presented in the table below. \\\\*\\\\*Gaudi2 is x2.44 faster than A100 80GB.\\\\*\\\\* We observe that we cannot fit a batch size larger than 1 on Gaudi2 here. This is due to the memory space taken by the graph where operations are accumulated during the first iteration of the run. Habana is working on optimizing the memory footprint in future releases of SynapseAI. We are looking forward to expanding this benchmark using newer versions of Habana\\'s SDK and also using [DeepSpeed](https://www.deepspeed.ai/) to see if the same trend holds. | | First-gen Gaudi | Gaudi2 (BS=1) | A100 (BS=16) |\\n|:-:|:-------:|:--------------:|:------------:|\\n| Throughput (samples/s) | N/A | 19.7 | 8.07 |\\n| Speedup | / | x2.44 | x1.0 |\\n\\\\*BS\\\\* is the batch size per device.\\n\\n--\\ntitle: \"Faster Training and Inference: Habana Gaudi¬Æ2 vs Nvidia A100 80GB\"\\nthumbnail: /blog/assets/habana-gaudi-2-benchmark/thumbnail.png\\nauthors:\\n- user: regisss\\n---\\n# Faster Training and Inference: Habana Gaudi¬Æ-2 vs Nvidia A100 80GB\\nIn this article, you will learn how to use [Habana¬Æ Gaudi¬Æ2](https://habana.ai/training/gaudi2/) to accelerate model training and inference, and train bigger models with ü§ó [Optimum Habana](https://huggingface.co/docs/optimum/habana/index). Then, we present several benchmarks including BERT pre-training, Stable Diffusion inference and T5-3B fine-tuning, to assess the performance differences between first generation Gaudi, Gaudi2 and Nvidia A100 80GB. Spoiler alert - Gaudi2 is about twice faster than Nvidia A100 80GB for both training and inference! [Gaudi2](https://habana.ai/training/gaudi2/) is the second generation AI hardware accelerator designed by Habana Labs. A single server contains 8 accelerator devices with 96GB of memory each (versus 32GB on first generation Gaudi and 80GB on A100 80GB). The Habana SDK, [SynapseAI](https://developer.habana.ai/), is common to both first-gen Gaudi and Gaudi2. That means that ü§ó Optimum Habana, which offers a very user-friendly interface between the ü§ó Transformers and ü§ó Diffusers libraries and SynapseAI, \\\\*\\\\*works the exact same way on Gaudi2 as on first-gen Gaudi!\\\\*\\\\*\\nSo if you already have ready-to-use training or inference workflows for first-gen Gaudi, we encourage you to try them on Gaudi2, as they will work without any single change. ## How to Get Access to Gaudi2? One of the easy, cost-efficient ways that Intel and Habana have made Gaudi2 available is on the Intel Developer Cloud. To start using Gaudi2 there, you should follow the following steps:\\n1.\\n\\n--\\ntitle: \"Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator\"\\nthumbnail: /blog/assets/habana-gaudi-2-bloom/thumbnail.png\\nauthors:\\n- user: regisss\\n---\\n# Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator\\nThis article will show you how to easily deploy large language models with hundreds of billions of parameters like BLOOM on [Habana¬Æ Gaudi¬Æ2](https://habana.ai/training/gaudi2/) using ü§ó [Optimum Habana](https://huggingface.co/docs/optimum/habana/index), which is the bridge between Gaudi2 and the ü§ó Transformers library. As demonstrated in the benchmark presented in this post, this will enable you to \\\\*\\\\*run inference faster than with any GPU currently available on the market\\\\*\\\\*. As models get bigger and bigger, deploying them into production to run inference has become increasingly challenging. Both hardware and software have seen a lot of innovations to address these challenges, so let\\'s dive in to see how to efficiently overcome them! ## BLOOMZ\\n[BLOOM](https://arxiv.org/abs/2211.05100) is a 176-billion-parameter autoregressive model that was trained to complete sequences of text. It can handle 46 different languages and 13 programming languages. Designed and trained as part of the [BigScience](https://bigscience.huggingface.co/) initiative, BLOOM is an open-science project that involved a large number of researchers and engineers all over the world. More recently, another model with the exact same architecture was released: [BLOOMZ](https://arxiv.org/abs/2211.01786), which is a fine-tuned version of BLOOM on several tasks leading to better generalization and zero-shot[^1] capabilities. Such large models raise new challenges in terms of memory and speed for both [training](https://huggingface.co/blog/bloom-megatron-deepspeed) and [inference](https://huggingface.co/blog/bloom-inference-optimization). Even in 16-bit precision, one instance requires 352 GB to fit! You will probably struggle to find any device with so much memory at the moment, but state-of-the-art hardware like Habana Gaudi2 does make it possible to perform inference on BLOOM and BLOOMZ models with low latencies. ## Habana Gaudi2\\n[Gaudi2](https://habana.ai/training/gaudi2/) is the second-generation AI hardware accelerator designed by Habana Labs. A single server contains 8 accelerator devices (called Habana Processing Units, or HPUs) with 96GB of memory each, which provides room to make very large models fit in. However, hosting the model is not very interesting if the computation is slow. Fortunately, Gaudi2 shines on that aspect: it differs from GPUs in that its architecture enables the accelerator to perform General Matrix Multiplication (GeMM) and other operations in parallel, which speeds up deep learning workflows. These features make Gaudi2 a great candidate for LLM training and inference. Habana\\'s SDK, SynapseAI‚Ñ¢, supports PyTorch and DeepSpeed for accelerating LLM training and inference. The [SynapseAI graph compiler](https://docs.habana.ai/en/latest/Gaudi\\\\_Overview/SynapseAI\\\\_Software\\\\_Suite.html#graph-compiler-and-runtime) will optimize the execution of the operations accumulated in the graph (e.g. operator fusion, data layout management, parallelization, pipelining and memory management, and graph-level optimizations). Moreover, support for [HPU graphs](https://docs.habana.ai/en/latest/PyTorch/Inference\\\\_on\\\\_PyTorch/Inference\\\\_Using\\\\_HPU\\\\_Graphs.html) and [DeepSpeed-inference](https://docs.habana.ai/en/latest/PyTorch/DeepSpeed/Inference\\\\_Using\\\\_DeepSpeed.html) have just recently been introduced in SynapseAI, and these are well-suited for latency-sensitive applications as shown in our benchmark below. All these features are integrated into the ü§ó [Optimum Habana](https://github.com/huggingface/optimum-habana) library so that deploying your model on Gaudi is very simple. Check out the quick-start page [here](https://huggingface.co/docs/optimum/habana/quickstart). If you would like to get access to Gaudi2, go to the [Intel Developer Cloud](https://www.intel.com/content/www/us/en/secure/developer/devcloud/cloud-launchpad.html) and follow [this guide](https://huggingface.co/blog/habana-gaudi-2-benchmark#how-to-get-access-to-gaudi2). ## Benchmarks\\nIn this section, we are going to provide an early benchmark of BLOOMZ on Gaudi2, first-generation Gaudi and Nvidia A100 80GB.', 'type': 'tool', 'name': 'data_retriever', 'id': 'b862911a-7311-4b48-bc95-759f028e46a0', 'tool_call_id': 'call_3pUYEx152oV5lGCj0L84ZNae', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab2663e684812d15f677e', 'page_content': 'Note that while [CUDA graphs](https://developer.nvidia.com/blog/cuda-graphs/) are not currently compatible with model parallelism in DeepSpeed (DeepSpeed v0.8.2, see [here](https://github.com/microsoft/DeepSpeed/blob/v0.8.2/deepspeed/inference/engine.py#L158)), HPU graphs are supported in Habana\\'s DeepSpeed fork. All benchmarks are doing [greedy generation](https://huggingface.co/blog/how-to-generate#greedy-search) of 100 token outputs. The input prompt is:\\n> \"DeepSpeed is a machine learning framework\"\\nwhich consists of 7 tokens with BLOOM\\'s tokenizer. The results for inference latency are displayed in the table below (the unit is \\\\*seconds\\\\*). | Model | Number of devices | Gaudi2 latency (seconds) | A100-80GB latency (seconds) | First-gen Gaudi latency (seconds) |\\n|:-----------:|:-----------------:|:-------------------------:|:-----------------:|:----------------------------------:|\\n| BLOOMZ | 8 | 3.103 | 4.402 | / |\\n| BLOOMZ-7B | 8 | 0.734 | 2.417 | 3.321 |\\n| BLOOMZ-7B | 1 | 0.772 | 2.119 | 2.387 |\\n\\\\*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, leading to a\\\\* x\\\\*1.42 speedup on BLOOMZ with Gaudi2 compared to A100.\\\\*\\nThe Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. \\\\*\\\\*For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB\\\\*\\\\*. Smaller checkpoints present interesting results too. \\\\*\\\\*Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!\\\\*\\\\* It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device. We also ran these models on first-gen Gaudi. While it is slower than Gaudi2, it is interesting from a price perspective as a DL1 instance on AWS costs approximately 13\\\\$ per hour. Latency for BLOOMZ-7B on first-gen Gaudi is 2.387 seconds. Thus, \\\\*\\\\*first-gen Gaudi offers for the 7-billion checkpoint a better price-performance ratio than A100\\\\*\\\\* which costs more than 30\\\\$ per hour! We expect the Habana team will optimize the performance of these models in the upcoming SynapseAI releases. For example, in our last benchmark, we saw that [Gaudi2 performs Stable Diffusion inference 2.2x faster than A100](https://huggingface.co/blog/habana-gaudi-2-benchmark#generating-images-from-text-with-stable-diffusion) and this has since been improved further to 2.37x with the latest optimizations provided by Habana. We will update these numbers as new versions of SynapseAI are released and integrated within Optimum Habana.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab9a93e684812d15f7016', 'page_content': \"The Gaudi runs were performed in \\\\*bfloat16\\\\* precision and the A100 runs in \\\\*fp16\\\\* precision (more information [here](https://huggingface.co/docs/diffusers/optimization/fp16)). All runs were \\\\*single-device\\\\* runs. ### Fine-tuning T5-3B\\nWith 96 GB of memory per device, Gaudi2 enables running much bigger models. For instance, we managed to fine-tune T5-3B (containing 3 billion parameters) with gradient checkpointing being the only applied memory optimization. This is not possible on first-gen Gaudi. [Here](https://huggingface.co/regisss/t5-3b-summarization-gaudi-2/tensorboard?scroll=1#scalars) are the logs of this run where the model was fine-tuned on the CNN DailyMail dataset for text summarization using [this script](https://github.com/huggingface/optimum-habana/tree/main/examples/summarization). The results we achieved are presented in the table below. \\\\*\\\\*Gaudi2 is x2.44 faster than A100 80GB.\\\\*\\\\* We observe that we cannot fit a batch size larger than 1 on Gaudi2 here. This is due to the memory space taken by the graph where operations are accumulated during the first iteration of the run. Habana is working on optimizing the memory footprint in future releases of SynapseAI. We are looking forward to expanding this benchmark using newer versions of Habana's SDK and also using [DeepSpeed](https://www.deepspeed.ai/) to see if the same trend holds. | | First-gen Gaudi | Gaudi2 (BS=1) | A100 (BS=16) |\\n|:-:|:-------:|:--------------:|:------------:|\\n| Throughput (samples/s) | N/A | 19.7 | 8.07 |\\n| Speedup | / | x2.44 | x1.0 |\\n\\\\*BS\\\\* is the batch size per device.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab9a93e684812d15f7012', 'page_content': '--\\ntitle: \"Faster Training and Inference: Habana Gaudi¬Æ2 vs Nvidia A100 80GB\"\\nthumbnail: /blog/assets/habana-gaudi-2-benchmark/thumbnail.png\\nauthors:\\n- user: regisss\\n---\\n# Faster Training and Inference: Habana Gaudi¬Æ-2 vs Nvidia A100 80GB\\nIn this article, you will learn how to use [Habana¬Æ Gaudi¬Æ2](https://habana.ai/training/gaudi2/) to accelerate model training and inference, and train bigger models with ü§ó [Optimum Habana](https://huggingface.co/docs/optimum/habana/index). Then, we present several benchmarks including BERT pre-training, Stable Diffusion inference and T5-3B fine-tuning, to assess the performance differences between first generation Gaudi, Gaudi2 and Nvidia A100 80GB. Spoiler alert - Gaudi2 is about twice faster than Nvidia A100 80GB for both training and inference! [Gaudi2](https://habana.ai/training/gaudi2/) is the second generation AI hardware accelerator designed by Habana Labs. A single server contains 8 accelerator devices with 96GB of memory each (versus 32GB on first generation Gaudi and 80GB on A100 80GB). The Habana SDK, [SynapseAI](https://developer.habana.ai/), is common to both first-gen Gaudi and Gaudi2. That means that ü§ó Optimum Habana, which offers a very user-friendly interface between the ü§ó Transformers and ü§ó Diffusers libraries and SynapseAI, \\\\*\\\\*works the exact same way on Gaudi2 as on first-gen Gaudi!\\\\*\\\\*\\nSo if you already have ready-to-use training or inference workflows for first-gen Gaudi, we encourage you to try them on Gaudi2, as they will work without any single change. ## How to Get Access to Gaudi2? One of the easy, cost-efficient ways that Intel and Habana have made Gaudi2 available is on the Intel Developer Cloud. To start using Gaudi2 there, you should follow the following steps:\\n1.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab2663e684812d15f677c', 'page_content': '--\\ntitle: \"Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator\"\\nthumbnail: /blog/assets/habana-gaudi-2-bloom/thumbnail.png\\nauthors:\\n- user: regisss\\n---\\n# Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator\\nThis article will show you how to easily deploy large language models with hundreds of billions of parameters like BLOOM on [Habana¬Æ Gaudi¬Æ2](https://habana.ai/training/gaudi2/) using ü§ó [Optimum Habana](https://huggingface.co/docs/optimum/habana/index), which is the bridge between Gaudi2 and the ü§ó Transformers library. As demonstrated in the benchmark presented in this post, this will enable you to \\\\*\\\\*run inference faster than with any GPU currently available on the market\\\\*\\\\*. As models get bigger and bigger, deploying them into production to run inference has become increasingly challenging. Both hardware and software have seen a lot of innovations to address these challenges, so let\\'s dive in to see how to efficiently overcome them! ## BLOOMZ\\n[BLOOM](https://arxiv.org/abs/2211.05100) is a 176-billion-parameter autoregressive model that was trained to complete sequences of text. It can handle 46 different languages and 13 programming languages. Designed and trained as part of the [BigScience](https://bigscience.huggingface.co/) initiative, BLOOM is an open-science project that involved a large number of researchers and engineers all over the world. More recently, another model with the exact same architecture was released: [BLOOMZ](https://arxiv.org/abs/2211.01786), which is a fine-tuned version of BLOOM on several tasks leading to better generalization and zero-shot[^1] capabilities. Such large models raise new challenges in terms of memory and speed for both [training](https://huggingface.co/blog/bloom-megatron-deepspeed) and [inference](https://huggingface.co/blog/bloom-inference-optimization). Even in 16-bit precision, one instance requires 352 GB to fit! You will probably struggle to find any device with so much memory at the moment, but state-of-the-art hardware like Habana Gaudi2 does make it possible to perform inference on BLOOM and BLOOMZ models with low latencies. ## Habana Gaudi2\\n[Gaudi2](https://habana.ai/training/gaudi2/) is the second-generation AI hardware accelerator designed by Habana Labs. A single server contains 8 accelerator devices (called Habana Processing Units, or HPUs) with 96GB of memory each, which provides room to make very large models fit in. However, hosting the model is not very interesting if the computation is slow. Fortunately, Gaudi2 shines on that aspect: it differs from GPUs in that its architecture enables the accelerator to perform General Matrix Multiplication (GeMM) and other operations in parallel, which speeds up deep learning workflows. These features make Gaudi2 a great candidate for LLM training and inference. Habana\\'s SDK, SynapseAI‚Ñ¢, supports PyTorch and DeepSpeed for accelerating LLM training and inference. The [SynapseAI graph compiler](https://docs.habana.ai/en/latest/Gaudi\\\\_Overview/SynapseAI\\\\_Software\\\\_Suite.html#graph-compiler-and-runtime) will optimize the execution of the operations accumulated in the graph (e.g. operator fusion, data layout management, parallelization, pipelining and memory management, and graph-level optimizations). Moreover, support for [HPU graphs](https://docs.habana.ai/en/latest/PyTorch/Inference\\\\_on\\\\_PyTorch/Inference\\\\_Using\\\\_HPU\\\\_Graphs.html) and [DeepSpeed-inference](https://docs.habana.ai/en/latest/PyTorch/DeepSpeed/Inference\\\\_Using\\\\_DeepSpeed.html) have just recently been introduced in SynapseAI, and these are well-suited for latency-sensitive applications as shown in our benchmark below. All these features are integrated into the ü§ó [Optimum Habana](https://github.com/huggingface/optimum-habana) library so that deploying your model on Gaudi is very simple. Check out the quick-start page [here](https://huggingface.co/docs/optimum/habana/quickstart). If you would like to get access to Gaudi2, go to the [Intel Developer Cloud](https://www.intel.com/content/www/us/en/secure/developer/devcloud/cloud-launchpad.html) and follow [this guide](https://huggingface.co/blog/habana-gaudi-2-benchmark#how-to-get-access-to-gaudi2). ## Benchmarks\\nIn this section, we are going to provide an early benchmark of BLOOMZ on Gaudi2, first-generation Gaudi and Nvidia A100 80GB.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is designed to demonstrate how to leverage DeepSpeed in conjunction with Habana Gaudi hardware to efficiently train large models that contain billions of parameters. It provides guidance on setting up the training environment, utilizing the capabilities of DeepSpeed for optimization, and maximizing the performance of the Habana Gaudi architecture for handling large-scale model training.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--e959c68c-b0b7-416f-903b-64f12b69a813-0', 'usage_metadata': {'input_tokens': 2896, 'output_tokens': 83, 'total_tokens': 2979, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:29:43,540 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 56/65 [03:57<00:43,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What command line module does PyTorch provide to run a script on multiple GPUs?\\n', 'type': 'human', 'id': 'f23a840b-6bb8-47ae-8c77-c71bd9e317af'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_XpB4RoOZt1NZ0X0FBwkLmTkW', 'function': {'arguments': '{\"query\":\"PyTorch command line module for running a script on multiple GPUs\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--3142949e-8976-4f92-a8ab-a36ae2df10ad-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'PyTorch command line module for running a script on multiple GPUs'}, 'id': 'call_XpB4RoOZt1NZ0X0FBwkLmTkW', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 364, 'output_tokens': 27, 'total_tokens': 391, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '\\\\* correct / len(test\\\\_loader.dataset)}\\')\\n```\\nTypically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook. However, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through \\\\*distributed\\\\* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\\n## PyTorch Distributed Data Parallelism\\nAs the name implies, `torch.distributed` is meant to work on \\\\*distributed\\\\* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both. To convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp\\\\_tutorial.html)\\nFirst a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through\\n> Note: for this section of the tutorial it should be assumed these are sent in python script files. Later on a launcher using Accelerate will be discussed that removes this necessity\\n```python\\nimport os\\nimport torch.distributed as dist\\ndef setup(rank, world\\\\_size):\\n\"Sets up the process group and configuration for PyTorch Distributed Data Parallelism\"\\nos.environ[\"MASTER\\\\_ADDR\"] = \\'localhost\\'\\nos.environ[\"MASTER\\\\_PORT\"] = \"12355\"\\n# Initialize the process group\\ndist.init\\\\_process\\\\_group(\"gloo\", rank=rank, world\\\\_size=world\\\\_size)\\ndef cleanup():\\n\"Cleans up the distributed environment\"\\ndist.destroy\\\\_process\\\\_group()\\n```\\nThe last piece of the puzzle is \\\\*how do I send my data and model to another GPU?\\\\*\\nThis is where the `DistributedDataParallel` module comes into play. It will copy your model onto each GPU, and when `loss.backward()` is called the backpropagation is performed and the resulting gradients across all these copies of the model will be averaged/reduced. This ensures each device has the same weights post the optimizer step. Below is an example of our training setup, refactored as a function, with this capability:\\n> Note: Here rank is the overall rank of the current GPU compared to all the other GPUs available, meaning they have a rank of `0 -> n-1`\\n```python\\nfrom torch.nn.parallel import DistributedDataParallel as DDP\\ndef train(model, rank, world\\\\_size):\\nsetup(rank, world\\\\_size)\\nmodel = model.to(rank)\\nddp\\\\_model = DDP(model, device\\\\_ids=[rank])\\noptimizer = optim.AdamW(ddp\\\\_model.parameters(), lr=1e-3)\\n# Train for one epoch\\nmodel.train()\\nfor batch\\\\_idx, (data, target) in enumerate(train\\\\_loader):\\ndata, target = data.to(device), target.to(device)\\noutput = model(data)\\nloss = F.nll\\\\_loss(output, target)\\nloss.backward()\\noptimizer.step()\\noptimizer.zero\\\\_grad()\\ncleanup()\\n```\\nThe optimizer needs to be declared based on the model \\\\*on the specific device\\\\* (so `ddp\\\\_model` and not `model`) for all of the gradients to properly be calculated. Lastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\\n```bash\\ntorchrun --nproc\\\\_per\\\\_node=2 --nnodes=1 example\\\\_script.py\\n```\\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\\n\\n## PyTorch Distributed\\nPyTorch supports [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) which enables data parallelism. To start, create a Python file and import `torch.distributed` and `torch.multiprocessing` to set up the distributed process group and to spawn the processes for inference on each GPU. You should also initialize a [`DiffusionPipeline`]:\\n```py\\nimport torch\\nimport torch.distributed as dist\\nimport torch.multiprocessing as mp\\nfrom diffusers import DiffusionPipeline\\nsd = DiffusionPipeline.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n)\\n```\\nYou\\'ll want to create a function to run inference; [`init\\\\_process\\\\_group`](https://pytorch.org/docs/stable/distributed.html?highlight=init\\\\_process\\\\_group#torch.distributed.init\\\\_process\\\\_group) handles creating a distributed environment with the type of backend to use, the `rank` of the current process, and the `world\\\\_size` or the number of processes participating. If you\\'re running inference in parallel over 2 GPUs, then the `world\\\\_size` is 2. Move the [`DiffusionPipeline`] to `rank` and use `get\\\\_rank` to assign a GPU to each process, where each process handles a different prompt:\\n```py\\ndef run\\\\_inference(rank, world\\\\_size):\\ndist.init\\\\_process\\\\_group(\"nccl\", rank=rank, world\\\\_size=world\\\\_size)\\nsd.to(rank)\\nif torch.distributed.get\\\\_rank() == 0:\\nprompt = \"a dog\"\\nelif torch.distributed.get\\\\_rank() == 1:\\nprompt = \"a cat\"\\nimage = sd(prompt).images[0]\\nimage.save(f\"./{\\'\\\\_\\'.join(prompt)}.png\")\\n```\\nTo run the distributed inference, call [`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn) to run the `run\\\\_inference` function on the number of GPUs defined in `world\\\\_size`:\\n```py\\ndef main():\\nworld\\\\_size = 2\\nmp.spawn(run\\\\_inference, args=(world\\\\_size,), nprocs=world\\\\_size, join=True)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\nmain()\\n```\\nOnce you\\'ve completed the inference script, use the `--nproc\\\\_per\\\\_node` argument to specify the number of GPUs to use and call `torchrun` to run the script:\\n```bash\\ntorchrun run\\\\_distributed.py --nproc\\\\_per\\\\_node=2\\n```\\n\\nJust add the flag `--fp16` to your command launching one of the scripts mentioned above! Using mixed precision training usually results in 2x-speedup for training with the same final results (as shown in\\n[this table](https://github.com/huggingface/transformers/tree/main/examples/text-classification#mixed-precision-training)\\nfor text classification). ## Running on TPUs\\nWhen using Tensorflow, TPUs are supported out of the box as a `tf.distribute.Strategy`. When using PyTorch, we support TPUs thanks to `pytorch/xla`. For more context and information on how to setup your TPU environment refer to Google\\'s documentation and to the\\nvery detailed [pytorch/xla README](https://github.com/pytorch/xla/blob/master/README.md). In this repo, we provide a very simple launcher script named\\n[xla\\\\_spawn.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/xla\\\\_spawn.py) that lets you run our\\nexample scripts on multiple TPU cores without any boilerplate. Just pass a `--num\\\\_cores` flag to this script, then your\\nregular training script with its arguments (this is similar to the `torch.distributed.launch` helper for\\n`torch.distributed`):\\n```bash\\npython xla\\\\_spawn.py --num\\\\_cores num\\\\_tpu\\\\_you\\\\_have \\\\\\npath\\\\_to\\\\_script.py \\\\\\n--all\\\\_arguments\\\\_of\\\\_the\\\\_script\\n```\\nAs an example, here is how you would fine-tune the BERT large model (with whole word masking) on the text\\nclassification MNLI task using the `run\\\\_glue` script, with 8 TPUs (from this folder):\\n```bash\\npython xla\\\\_spawn.py --num\\\\_cores 8 \\\\\\ntext-classification/run\\\\_glue.py \\\\\\n--model\\\\_name\\\\_or\\\\_path bert-large-uncased-whole-word-masking \\\\\\n--task\\\\_name mnli \\\\\\n--do\\\\_train \\\\\\n--do\\\\_eval \\\\\\n--max\\\\_seq\\\\_length 128 \\\\\\n--per\\\\_device\\\\_train\\\\_batch\\\\_size 8 \\\\\\n--learning\\\\_rate 2e-5 \\\\\\n--num\\\\_train\\\\_epochs 3.0 \\\\\\n--output\\\\_dir /tmp/mnli\\\\_output/\\n```\\n## Using Accelerate\\nMost PyTorch example scripts have a version using the [ü§ó Accelerate](https://github.com/huggingface/accelerate) library\\nthat exposes the training loop so it\\'s easy for you to customize or tweak them to your needs.\\n\\n2. All sequences should be padded to be of equal length to avoid extremely slow training. (`finetune\\\\_trainer.py` does this automatically when running on TPU.)\\nWe provide a very simple launcher script named `xla\\\\_spawn.py` that lets you run our example scripts on multiple TPU cores without any boilerplate. Just pass a `--num\\\\_cores` flag to this script, then your regular training script with its arguments (this is similar to the `torch.distributed.launch` helper for `torch.distributed`). `builtin\\\\_trainer/finetune\\\\_tpu.sh` script provides minimal arguments needed for TPU training. The following command fine-tunes `sshleifer/student\\\\_marian\\\\_en\\\\_ro\\\\_6\\\\_3` on TPU V3-8 and should complete one epoch in ~5-6 mins. ```bash\\n./builtin\\\\_trainer/train\\\\_distil\\\\_marian\\\\_enro\\\\_tpu.sh\\n```\\n## Evaluation Commands\\nTo create summaries for each article in dataset, we use `run\\\\_eval.py`, here are a few commands that run eval for different tasks and models. If \\'translation\\' is in your task name, the computed metric will be BLEU. Otherwise, ROUGE will be used. For t5, you need to specify --task translation\\\\_{src}\\\\_to\\\\_{tgt} as follows:\\n```bash\\nexport DATA\\\\_DIR=wmt\\\\_en\\\\_ro\\n./run\\\\_eval.py t5-base \\\\\\n$DATA\\\\_DIR/val.source t5\\\\_val\\\\_generations.txt \\\\\\n--reference\\\\_path $DATA\\\\_DIR/val.target \\\\\\n--score\\\\_path enro\\\\_bleu.json \\\\\\n--task translation\\\\_en\\\\_to\\\\_ro \\\\\\n--n\\\\_obs 100 \\\\\\n--device cuda \\\\\\n--fp16 \\\\\\n--bs 32\\n```\\nThis command works for MBART, although the BLEU score is suspiciously low. ```bash\\nexport DATA\\\\_DIR=wmt\\\\_en\\\\_ro\\n./run\\\\_eval.py facebook/mbart-large-en-ro $DATA\\\\_DIR/val.source mbart\\\\_val\\\\_generations.txt \\\\\\n--reference\\\\_path $DATA\\\\_DIR/val.target \\\\\\n--score\\\\_path enro\\\\_bleu.json \\\\\\n--task translation \\\\\\n--n\\\\_obs 100 \\\\\\n--device cuda \\\\\\n--fp16 \\\\\\n--bs 32\\n```\\nSummarization (xsum will be very similar):\\n```bash\\nexport DATA\\\\_DIR=cnn\\\\_dm\\n./run\\\\_eval.py sshleifer/distilbart-cnn-12-6 $DATA\\\\_DIR/val.source dbart\\\\_val\\\\_generations.txt \\\\\\n--reference\\\\_path $DATA\\\\_DIR/val.target \\\\\\n--score\\\\_path cnn\\\\_rouge.json \\\\\\n--task summarization \\\\\\n--n\\\\_obs 100 \\\\\\nth 56 \\\\\\n--fp16 \\\\\\n--bs 32\\n```\\n### Multi-GPU Evaluation\\nhere is a command to run xsum evaluation on 8 GPUS.', 'type': 'tool', 'name': 'data_retriever', 'id': '343347e5-3e65-4f1f-b87d-66a1e3c68b86', 'tool_call_id': 'call_XpB4RoOZt1NZ0X0FBwkLmTkW', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab8fb3e684812d15f6f44', 'page_content': '\\\\* correct / len(test\\\\_loader.dataset)}\\')\\n```\\nTypically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook. However, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through \\\\*distributed\\\\* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\\n## PyTorch Distributed Data Parallelism\\nAs the name implies, `torch.distributed` is meant to work on \\\\*distributed\\\\* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both. To convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp\\\\_tutorial.html)\\nFirst a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through\\n> Note: for this section of the tutorial it should be assumed these are sent in python script files. Later on a launcher using Accelerate will be discussed that removes this necessity\\n```python\\nimport os\\nimport torch.distributed as dist\\ndef setup(rank, world\\\\_size):\\n\"Sets up the process group and configuration for PyTorch Distributed Data Parallelism\"\\nos.environ[\"MASTER\\\\_ADDR\"] = \\'localhost\\'\\nos.environ[\"MASTER\\\\_PORT\"] = \"12355\"\\n# Initialize the process group\\ndist.init\\\\_process\\\\_group(\"gloo\", rank=rank, world\\\\_size=world\\\\_size)\\ndef cleanup():\\n\"Cleans up the distributed environment\"\\ndist.destroy\\\\_process\\\\_group()\\n```\\nThe last piece of the puzzle is \\\\*how do I send my data and model to another GPU?\\\\*\\nThis is where the `DistributedDataParallel` module comes into play. It will copy your model onto each GPU, and when `loss.backward()` is called the backpropagation is performed and the resulting gradients across all these copies of the model will be averaged/reduced. This ensures each device has the same weights post the optimizer step. Below is an example of our training setup, refactored as a function, with this capability:\\n> Note: Here rank is the overall rank of the current GPU compared to all the other GPUs available, meaning they have a rank of `0 -> n-1`\\n```python\\nfrom torch.nn.parallel import DistributedDataParallel as DDP\\ndef train(model, rank, world\\\\_size):\\nsetup(rank, world\\\\_size)\\nmodel = model.to(rank)\\nddp\\\\_model = DDP(model, device\\\\_ids=[rank])\\noptimizer = optim.AdamW(ddp\\\\_model.parameters(), lr=1e-3)\\n# Train for one epoch\\nmodel.train()\\nfor batch\\\\_idx, (data, target) in enumerate(train\\\\_loader):\\ndata, target = data.to(device), target.to(device)\\noutput = model(data)\\nloss = F.nll\\\\_loss(output, target)\\nloss.backward()\\noptimizer.step()\\noptimizer.zero\\\\_grad()\\ncleanup()\\n```\\nThe optimizer needs to be declared based on the model \\\\*on the specific device\\\\* (so `ddp\\\\_model` and not `model`) for all of the gradients to properly be calculated. Lastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\\n```bash\\ntorchrun --nproc\\\\_per\\\\_node=2 --nnodes=1 example\\\\_script.py\\n```\\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa85f3e684812d15f5be4', 'page_content': '## PyTorch Distributed\\nPyTorch supports [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) which enables data parallelism. To start, create a Python file and import `torch.distributed` and `torch.multiprocessing` to set up the distributed process group and to spawn the processes for inference on each GPU. You should also initialize a [`DiffusionPipeline`]:\\n```py\\nimport torch\\nimport torch.distributed as dist\\nimport torch.multiprocessing as mp\\nfrom diffusers import DiffusionPipeline\\nsd = DiffusionPipeline.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n)\\n```\\nYou\\'ll want to create a function to run inference; [`init\\\\_process\\\\_group`](https://pytorch.org/docs/stable/distributed.html?highlight=init\\\\_process\\\\_group#torch.distributed.init\\\\_process\\\\_group) handles creating a distributed environment with the type of backend to use, the `rank` of the current process, and the `world\\\\_size` or the number of processes participating. If you\\'re running inference in parallel over 2 GPUs, then the `world\\\\_size` is 2. Move the [`DiffusionPipeline`] to `rank` and use `get\\\\_rank` to assign a GPU to each process, where each process handles a different prompt:\\n```py\\ndef run\\\\_inference(rank, world\\\\_size):\\ndist.init\\\\_process\\\\_group(\"nccl\", rank=rank, world\\\\_size=world\\\\_size)\\nsd.to(rank)\\nif torch.distributed.get\\\\_rank() == 0:\\nprompt = \"a dog\"\\nelif torch.distributed.get\\\\_rank() == 1:\\nprompt = \"a cat\"\\nimage = sd(prompt).images[0]\\nimage.save(f\"./{\\'\\\\_\\'.join(prompt)}.png\")\\n```\\nTo run the distributed inference, call [`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn) to run the `run\\\\_inference` function on the number of GPUs defined in `world\\\\_size`:\\n```py\\ndef main():\\nworld\\\\_size = 2\\nmp.spawn(run\\\\_inference, args=(world\\\\_size,), nprocs=world\\\\_size, join=True)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\nmain()\\n```\\nOnce you\\'ve completed the inference script, use the `--nproc\\\\_per\\\\_node` argument to specify the number of GPUs to use and call `torchrun` to run the script:\\n```bash\\ntorchrun run\\\\_distributed.py --nproc\\\\_per\\\\_node=2\\n```', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab7073e684812d15f6d4c', 'page_content': \"Just add the flag `--fp16` to your command launching one of the scripts mentioned above! Using mixed precision training usually results in 2x-speedup for training with the same final results (as shown in\\n[this table](https://github.com/huggingface/transformers/tree/main/examples/text-classification#mixed-precision-training)\\nfor text classification). ## Running on TPUs\\nWhen using Tensorflow, TPUs are supported out of the box as a `tf.distribute.Strategy`. When using PyTorch, we support TPUs thanks to `pytorch/xla`. For more context and information on how to setup your TPU environment refer to Google's documentation and to the\\nvery detailed [pytorch/xla README](https://github.com/pytorch/xla/blob/master/README.md). In this repo, we provide a very simple launcher script named\\n[xla\\\\_spawn.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/xla\\\\_spawn.py) that lets you run our\\nexample scripts on multiple TPU cores without any boilerplate. Just pass a `--num\\\\_cores` flag to this script, then your\\nregular training script with its arguments (this is similar to the `torch.distributed.launch` helper for\\n`torch.distributed`):\\n```bash\\npython xla\\\\_spawn.py --num\\\\_cores num\\\\_tpu\\\\_you\\\\_have \\\\\\npath\\\\_to\\\\_script.py \\\\\\n--all\\\\_arguments\\\\_of\\\\_the\\\\_script\\n```\\nAs an example, here is how you would fine-tune the BERT large model (with whole word masking) on the text\\nclassification MNLI task using the `run\\\\_glue` script, with 8 TPUs (from this folder):\\n```bash\\npython xla\\\\_spawn.py --num\\\\_cores 8 \\\\\\ntext-classification/run\\\\_glue.py \\\\\\n--model\\\\_name\\\\_or\\\\_path bert-large-uncased-whole-word-masking \\\\\\n--task\\\\_name mnli \\\\\\n--do\\\\_train \\\\\\n--do\\\\_eval \\\\\\n--max\\\\_seq\\\\_length 128 \\\\\\n--per\\\\_device\\\\_train\\\\_batch\\\\_size 8 \\\\\\n--learning\\\\_rate 2e-5 \\\\\\n--num\\\\_train\\\\_epochs 3.0 \\\\\\n--output\\\\_dir /tmp/mnli\\\\_output/\\n```\\n## Using Accelerate\\nMost PyTorch example scripts have a version using the [ü§ó Accelerate](https://github.com/huggingface/accelerate) library\\nthat exposes the training loop so it's easy for you to customize or tweak them to your needs.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c0be666af48523f51f46', 'page_content': \"2. All sequences should be padded to be of equal length to avoid extremely slow training. (`finetune\\\\_trainer.py` does this automatically when running on TPU.)\\nWe provide a very simple launcher script named `xla\\\\_spawn.py` that lets you run our example scripts on multiple TPU cores without any boilerplate. Just pass a `--num\\\\_cores` flag to this script, then your regular training script with its arguments (this is similar to the `torch.distributed.launch` helper for `torch.distributed`). `builtin\\\\_trainer/finetune\\\\_tpu.sh` script provides minimal arguments needed for TPU training. The following command fine-tunes `sshleifer/student\\\\_marian\\\\_en\\\\_ro\\\\_6\\\\_3` on TPU V3-8 and should complete one epoch in ~5-6 mins. ```bash\\n./builtin\\\\_trainer/train\\\\_distil\\\\_marian\\\\_enro\\\\_tpu.sh\\n```\\n## Evaluation Commands\\nTo create summaries for each article in dataset, we use `run\\\\_eval.py`, here are a few commands that run eval for different tasks and models. If 'translation' is in your task name, the computed metric will be BLEU. Otherwise, ROUGE will be used. For t5, you need to specify --task translation\\\\_{src}\\\\_to\\\\_{tgt} as follows:\\n```bash\\nexport DATA\\\\_DIR=wmt\\\\_en\\\\_ro\\n./run\\\\_eval.py t5-base \\\\\\n$DATA\\\\_DIR/val.source t5\\\\_val\\\\_generations.txt \\\\\\n--reference\\\\_path $DATA\\\\_DIR/val.target \\\\\\n--score\\\\_path enro\\\\_bleu.json \\\\\\n--task translation\\\\_en\\\\_to\\\\_ro \\\\\\n--n\\\\_obs 100 \\\\\\n--device cuda \\\\\\n--fp16 \\\\\\n--bs 32\\n```\\nThis command works for MBART, although the BLEU score is suspiciously low. ```bash\\nexport DATA\\\\_DIR=wmt\\\\_en\\\\_ro\\n./run\\\\_eval.py facebook/mbart-large-en-ro $DATA\\\\_DIR/val.source mbart\\\\_val\\\\_generations.txt \\\\\\n--reference\\\\_path $DATA\\\\_DIR/val.target \\\\\\n--score\\\\_path enro\\\\_bleu.json \\\\\\n--task translation \\\\\\n--n\\\\_obs 100 \\\\\\n--device cuda \\\\\\n--fp16 \\\\\\n--bs 32\\n```\\nSummarization (xsum will be very similar):\\n```bash\\nexport DATA\\\\_DIR=cnn\\\\_dm\\n./run\\\\_eval.py sshleifer/distilbart-cnn-12-6 $DATA\\\\_DIR/val.source dbart\\\\_val\\\\_generations.txt \\\\\\n--reference\\\\_path $DATA\\\\_DIR/val.target \\\\\\n--score\\\\_path cnn\\\\_rouge.json \\\\\\n--task summarization \\\\\\n--n\\\\_obs 100 \\\\\\nth 56 \\\\\\n--fp16 \\\\\\n--bs 32\\n```\\n### Multi-GPU Evaluation\\nhere is a command to run xsum evaluation on 8 GPUS.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'PyTorch provides the `torchrun` command line module to run a script on multiple GPUs. You can use it by passing in the number of nodes and the script to run, for example:\\n\\n```bash\\ntorchrun --nproc_per_node=2 example_script.py\\n```', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--02f543e5-6556-487d-9429-86e4c35ac44b-0', 'usage_metadata': {'input_tokens': 2942, 'output_tokens': 56, 'total_tokens': 2998, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:29:47,588 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 57/65 [04:01<00:36,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\\n', 'type': 'human', 'id': 'c1208530-b7ef-4873-8a13-64795ce4d455'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_tr9cwCpwHjvpL09z6B9b8xl9', 'function': {'arguments': '{\"query\":\"most popular vision transformer model for image classification site:huggingface.co\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--3471909f-5773-41d6-b767-94838c5dac0e-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'most popular vision transformer model for image classification site:huggingface.co'}, 'id': 'call_tr9cwCpwHjvpL09z6B9b8xl9', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 367, 'output_tokens': 29, 'total_tokens': 396, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '--\\ntitle: \"Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\"\\nthumbnail: /blog/assets/97\\\\_vision\\\\_transformers/thumbnail.png\\nauthors:\\n- user: juliensimon\\n---\\n# Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\\nThis blog post will show how easy it is to fine-tune pre-trained Transformer models for your dataset using the Hugging Face Optimum library on Graphcore Intelligence Processing Units (IPUs). As an example, we will show a step-by-step guide and provide a notebook that takes a large, widely-used chest X-ray dataset and trains a vision transformer (ViT) model. ## Introducing vision transformer (ViT) models\\n\\nIn 2017 a group of Google AI researchers published a paper introducing the transformer model architecture. Characterised by a novel self-attention mechanism, transformers were proposed as a new and efficient group of models for language applications. Indeed, in the last five years, transformers have seen explosive popularity and are now accepted as the de facto standard for natural language processing (NLP). Transformers for language are perhaps most notably represented by the rapidly evolving GPT and BERT model families. Both can run easily and efficiently on Graphcore IPUs as part of the growing [Hugging Face Optimum Graphcore library](/posts/getting-started-with-hugging-face-transformers-for-ipus-with-optimum)). ![transformers_chrono](https://www.graphcore.ai/hs-fs/hubfs/transformers_chrono.png?width=1024&name=transformers_chrono.png)\\n\\nA timeline showing releases of prominent transformer language models (credit: Hugging Face)\\n\\nAn in-depth explainer about the transformer model architecture (with a focus on NLP) can be found [on the Hugging Face website](https://huggingface.co/course/chapter1/4?fw=pt).\\n\\n!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Vision Transformer (ViT)\\n## Overview\\nThe Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\\nat Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\\nUszkoreit, Neil Houlsby.\\n\\n!---\\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\\n\\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\\n\\n#### **English** | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Espa√±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [Êó•Êú¨Ë™û](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [–†—É—Å—Å–∫–∏–π](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [–†ortugu√™s](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](https://github.com/huggingface/transformers//blob/main/README_te.md) |\\n\\n### State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\\n\\n### \\n\\nü§ó Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.\\n\\n!---\\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\\n\\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\\n\\n#### [English](https://github.com/huggingface/transformers/blob/main/README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Espa√±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [Êó•Êú¨Ë™û](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](https://github.com/huggingface/transformers/blob/main/README_hd.md) | **–†—É—Å—Å–∫–∏–π** [‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](https://github.com/huggingface/transformers//blob/main/README_te.md) |\\n\\n### –°–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è JAX, PyTorch –∏ TensorFlow\\n\\n### \\n\\nü§ó Transformers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç—ã—Å—è—á–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–µ–∫—Å—Ç, –∑—Ä–µ–Ω–∏–µ –∏ –∞—É–¥–∏–æ.', 'type': 'tool', 'name': 'data_retriever', 'id': '7480641c-9835-4b8c-a566-f77922fea1e8', 'tool_call_id': 'call_tr9cwCpwHjvpL09z6B9b8xl9', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaca53e684812d15f6106', 'page_content': '--\\ntitle: \"Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\"\\nthumbnail: /blog/assets/97\\\\_vision\\\\_transformers/thumbnail.png\\nauthors:\\n- user: juliensimon\\n---\\n# Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\\nThis blog post will show how easy it is to fine-tune pre-trained Transformer models for your dataset using the Hugging Face Optimum library on Graphcore Intelligence Processing Units (IPUs). As an example, we will show a step-by-step guide and provide a notebook that takes a large, widely-used chest X-ray dataset and trains a vision transformer (ViT) model. ## Introducing vision transformer (ViT) models\\n\\nIn 2017 a group of Google AI researchers published a paper introducing the transformer model architecture. Characterised by a novel self-attention mechanism, transformers were proposed as a new and efficient group of models for language applications. Indeed, in the last five years, transformers have seen explosive popularity and are now accepted as the de facto standard for natural language processing (NLP). Transformers for language are perhaps most notably represented by the rapidly evolving GPT and BERT model families. Both can run easily and efficiently on Graphcore IPUs as part of the growing [Hugging Face Optimum Graphcore library](/posts/getting-started-with-hugging-face-transformers-for-ipus-with-optimum)). ![transformers_chrono](https://www.graphcore.ai/hs-fs/hubfs/transformers_chrono.png?width=1024&name=transformers_chrono.png)\\n\\nA timeline showing releases of prominent transformer language models (credit: Hugging Face)\\n\\nAn in-depth explainer about the transformer model architecture (with a focus on NLP) can be found [on the Hugging Face website](https://huggingface.co/course/chapter1/4?fw=pt).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b6a0af3d2ebb270a03ea', 'page_content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Vision Transformer (ViT)\\n## Overview\\nThe Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\\nat Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\\nUszkoreit, Neil Houlsby.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b76daf3d2ebb270a04ed', 'page_content': '!---\\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\\n\\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\\n\\n#### **English** | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Espa√±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [Êó•Êú¨Ë™û](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [–†—É—Å—Å–∫–∏–π](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [–†ortugu√™s](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](https://github.com/huggingface/transformers//blob/main/README_te.md) |\\n\\n### State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\\n\\n### \\n\\nü§ó Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b1b8af3d2ebb2709fdd5', 'page_content': '!---\\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\\n\\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\\n\\n#### [English](https://github.com/huggingface/transformers/blob/main/README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Espa√±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [Êó•Êú¨Ë™û](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](https://github.com/huggingface/transformers/blob/main/README_hd.md) | **–†—É—Å—Å–∫–∏–π** [‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](https://github.com/huggingface/transformers//blob/main/README_te.md) |\\n\\n### –°–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è JAX, PyTorch –∏ TensorFlow\\n\\n### \\n\\nü§ó Transformers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç—ã—Å—è—á–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–µ–∫—Å—Ç, –∑—Ä–µ–Ω–∏–µ –∏ –∞—É–¥–∏–æ.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The blog post does not specify which vision transformer model is the most popular on the Hugging Face Model Hub for image classification. For detailed information on the most popular models, you may want to explore the Hugging Face Model Hub directly.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--d1f7d9e5-c29d-4f2e-a021-5107d023db88-0', 'usage_metadata': {'input_tokens': 2302, 'output_tokens': 47, 'total_tokens': 2349, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:29:50,560 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 58/65 [04:04<00:28,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the command to upload an ESPnet model to a Hugging Face repository?\\n', 'type': 'human', 'id': 'ff10692a-42bd-4b33-ad09-79ff02ba6210'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_7S1wpTPQIJ6OvBmw7YFpYOCj', 'function': {'arguments': '{\"query\":\"upload ESPnet model to Hugging Face repository\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--aa8c2abb-c567-4a29-8c7f-ab6de75822b8-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'upload ESPnet model to Hugging Face repository'}, 'id': 'call_7S1wpTPQIJ6OvBmw7YFpYOCj', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 24, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Using ESPnet at Hugging Face\\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks. ## Exploring ESPnet in the Hub\\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). All models on the Hub come up with useful features:\\n1. An automatically generated model card with a description, a training configuration, licenses and more. 2. Metadata tags that help for discoverability and contain information such as license, language and datasets. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png)\\n\\n## Using existing models\\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\\\_model\\\\_zoo)). If you\\'re interested in doing inference, different classes for different tasks have a `from\\\\_pretrained` method that allows loading models from the Hub.\\n\\nFor example:\\n\\\\* `Speech2Text` for Automatic Speech Recognition. \\\\* `Text2Speech` for Text to Speech. \\\\* `SeparateSpeech` for Audio Source Separation. Here is an inference example:\\n```py\\nimport soundfile\\nfrom espnet2.bin.tts\\\\_inference import Text2Speech\\ntext2speech = Text2Speech.from\\\\_pretrained(\"model\\\\_name\")\\nspeech = text2speech(\"foobar\")[\"wav\"]\\nsoundfile.write(\"out.wav\", speech.numpy(), text2speech.fs, \"PCM\\\\_16\")\\n```\\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png)\\n\\n## Sharing your models\\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\\\_model\\\\_zoo#register-your-model)). The `run.sh` script allows to upload a given model to a Hugging Face repository. ```bash\\n./run.sh --stage 15 --skip\\\\_upload\\\\_hf false --hf\\\\_repo username/model\\\\_repo\\n```\\n## Additional resources\\n\\\\* ESPnet [docs](https://espnet.github.io/espnet/index.html). \\\\* ESPnet model zoo [repository](https://github.com/espnet/espnet\\\\_model\\\\_zoo).\\n\\nTo share a model with the community, you need an account on [huggingface.co](https://huggingface.co/join). You can also join an existing organization or create a new one. ## Repository features\\nEach repository on the Model Hub behaves like a typical GitHub repository. Our repositories offer versioning, commit history, and the ability to visualize differences. The Model Hub\\'s built-in versioning is based on git and [git-lfs](https://git-lfs.github.com/). In other words, you can treat one model as one repository, enabling greater access control and scalability. Version control allows \\\\*revisions\\\\*, a method for pinning a specific version of a model with a commit hash, tag or branch. As a result, you can load a specific model version with the `revision` parameter:\\n```py\\n>>> model = AutoModel.from\\\\_pretrained(\\n... \"julien-c/EsperBERTo-small\", revision=\"v2.0.1\" # tag name, or branch name, or commit hash\\n... )\\n```\\nFiles are also easily edited in a repository, and you can view the commit history as well as the difference:\\n![vis\\\\_diff](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vis\\\\_diff.png)\\n## Setup\\nBefore sharing a model to the Hub, you will need your Hugging Face credentials. If you have access to a terminal, run the following command in the virtual environment where ü§ó Transformers is installed. This will store your access token in your Hugging Face cache folder (`~/.cache/` by default):\\n```bash\\nhuggingface-cli login\\n```\\nIf you are using a notebook like Jupyter or Colaboratory, make sure you have the [`huggingface\\\\_hub`](https://huggingface.co/docs/hub/adding-a-library) library installed. This library allows you to programmatically interact with the Hub. ```bash\\npip install huggingface\\\\_hub\\n```\\nThen use `notebook\\\\_login` to sign-in to the Hub, and follow the link [here](https://huggingface.co/settings/token) to generate a token to login with:\\n```py\\n>>> from huggingface\\\\_hub import notebook\\\\_login\\n>>> notebook\\\\_login()\\n```\\n## Convert a model for all frameworks\\nTo ensure your model can be used by someone working with a different framework, we recommend you convert and upload your model with both PyTorch and TensorFlow checkpoints.\\n\\nThe Model Hub\\n## What is the Model Hub? The Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing. Download pre-trained models with the [`huggingface\\\\_hub` client library](https://huggingface.co/docs/huggingface\\\\_hub/index), with ü§ó [`Transformers`](https://huggingface.co/docs/transformers/index) for fine-tuning and other usages or with any of the over [15 integrated libraries](./models-libraries). You can even leverage the [Inference API](./models-inference) to use models in production settings. You can refer to the following video for a guide on navigating the Model Hub:\\nTo learn how to upload models to the Hub, you can refer to the [Repositories Getting Started Guide](./repositories-getting-started).', 'type': 'tool', 'name': 'data_retriever', 'id': '5e9159e8-04c9-4146-b371-d547366f866e', 'tool_call_id': 'call_7S1wpTPQIJ6OvBmw7YFpYOCj', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bdcc666af48523f51bb0', 'page_content': \"Using ESPnet at Hugging Face\\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks. ## Exploring ESPnet in the Hub\\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). All models on the Hub come up with useful features:\\n1. An automatically generated model card with a description, a training configuration, licenses and more. 2. Metadata tags that help for discoverability and contain information such as license, language and datasets. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png)\\n\\n## Using existing models\\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\\\_model\\\\_zoo)). If you're interested in doing inference, different classes for different tasks have a `from\\\\_pretrained` method that allows loading models from the Hub.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bdd5666af48523f51bb7', 'page_content': 'For example:\\n\\\\* `Speech2Text` for Automatic Speech Recognition. \\\\* `Text2Speech` for Text to Speech. \\\\* `SeparateSpeech` for Audio Source Separation. Here is an inference example:\\n```py\\nimport soundfile\\nfrom espnet2.bin.tts\\\\_inference import Text2Speech\\ntext2speech = Text2Speech.from\\\\_pretrained(\"model\\\\_name\")\\nspeech = text2speech(\"foobar\")[\"wav\"]\\nsoundfile.write(\"out.wav\", speech.numpy(), text2speech.fs, \"PCM\\\\_16\")\\n```\\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png)\\n\\n## Sharing your models\\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\\\_model\\\\_zoo#register-your-model)). The `run.sh` script allows to upload a given model to a Hugging Face repository. ```bash\\n./run.sh --stage 15 --skip\\\\_upload\\\\_hf false --hf\\\\_repo username/model\\\\_repo\\n```\\n## Additional resources\\n\\\\* ESPnet [docs](https://espnet.github.io/espnet/index.html). \\\\* ESPnet model zoo [repository](https://github.com/espnet/espnet\\\\_model\\\\_zoo).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b1c1af3d2ebb2709fddd', 'page_content': 'To share a model with the community, you need an account on [huggingface.co](https://huggingface.co/join). You can also join an existing organization or create a new one. ## Repository features\\nEach repository on the Model Hub behaves like a typical GitHub repository. Our repositories offer versioning, commit history, and the ability to visualize differences. The Model Hub\\'s built-in versioning is based on git and [git-lfs](https://git-lfs.github.com/). In other words, you can treat one model as one repository, enabling greater access control and scalability. Version control allows \\\\*revisions\\\\*, a method for pinning a specific version of a model with a commit hash, tag or branch. As a result, you can load a specific model version with the `revision` parameter:\\n```py\\n>>> model = AutoModel.from\\\\_pretrained(\\n... \"julien-c/EsperBERTo-small\", revision=\"v2.0.1\" # tag name, or branch name, or commit hash\\n... )\\n```\\nFiles are also easily edited in a repository, and you can view the commit history as well as the difference:\\n![vis\\\\_diff](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vis\\\\_diff.png)\\n## Setup\\nBefore sharing a model to the Hub, you will need your Hugging Face credentials. If you have access to a terminal, run the following command in the virtual environment where ü§ó Transformers is installed. This will store your access token in your Hugging Face cache folder (`~/.cache/` by default):\\n```bash\\nhuggingface-cli login\\n```\\nIf you are using a notebook like Jupyter or Colaboratory, make sure you have the [`huggingface\\\\_hub`](https://huggingface.co/docs/hub/adding-a-library) library installed. This library allows you to programmatically interact with the Hub. ```bash\\npip install huggingface\\\\_hub\\n```\\nThen use `notebook\\\\_login` to sign-in to the Hub, and follow the link [here](https://huggingface.co/settings/token) to generate a token to login with:\\n```py\\n>>> from huggingface\\\\_hub import notebook\\\\_login\\n>>> notebook\\\\_login()\\n```\\n## Convert a model for all frameworks\\nTo ensure your model can be used by someone working with a different framework, we recommend you convert and upload your model with both PyTorch and TensorFlow checkpoints.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bb15af3d2ebb270a0991', 'page_content': 'The Model Hub\\n## What is the Model Hub? The Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing. Download pre-trained models with the [`huggingface\\\\_hub` client library](https://huggingface.co/docs/huggingface\\\\_hub/index), with ü§ó [`Transformers`](https://huggingface.co/docs/transformers/index) for fine-tuning and other usages or with any of the over [15 integrated libraries](./models-libraries). You can even leverage the [Inference API](./models-inference) to use models in production settings. You can refer to the following video for a guide on navigating the Model Hub:\\nTo learn how to upload models to the Hub, you can refer to the [Repositories Getting Started Guide](./repositories-getting-started).', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The command to upload an ESPnet model to a Hugging Face repository is:\\n\\n```bash\\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\\n```', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--0ffa36f5-e95a-4c9e-8834-a38212205368-0', 'usage_metadata': {'input_tokens': 1663, 'output_tokens': 40, 'total_tokens': 1703, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:29:54,696 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 59/65 [04:08<00:24,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\\n', 'type': 'human', 'id': 'bc871f17-3af9-440e-a01e-9238d4f79b12'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_ZADbKtcSR8yjH9SDhFf5GjA9', 'function': {'arguments': '{\"query\":\"custom Python dependencies Inference Endpoints model repository file\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--d787bb0d-b75e-4c96-8bc1-54d469f4d4b9-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'custom Python dependencies Inference Endpoints model repository file'}, 'id': 'call_ZADbKtcSR8yjH9SDhFf5GjA9', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 368, 'output_tokens': 25, 'total_tokens': 393, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Add custom Dependencies\\nInference Endpoints‚Äô base image includes all required libraries to run inference on ü§ó Transformers models, but it also supports custom dependencies. This is useful if you want to:\\n\\\\* [customize your inference pipeline](/docs/inference-endpoints/guides/custom\\\\_handler) and need additional Python dependencies\\n\\\\* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)). To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within. ```bash\\noptimum[onnxruntime]==1.2.3\\nmkl-include\\nmkl\\n```\\nCheck out the `requirements.txt` files in the following model repositories for examples:\\n\\\\* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\\n\\\\* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom\\\\_container) for inference.\\n\\n‚ö°‚ö° Learn more about it by reading the [Inference API documentation](./models-inference). Finally, you can also deploy all those models to dedicated [Inference Endpoints](https://huggingface.co/docs/inference-endpoints).\\n\\nInference Endpoints\\nInference Endpoints provides a secure production solution to easily deploy models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the [Hub](https://huggingface.co/models). This page is a reference for `huggingface\\\\_hub`\\'s integration with Inference Endpoints. For more information about the Inference Endpoints product, check out its [official documentation](https://huggingface.co/docs/inference-endpoints/index). Check out the [related guide](../guides/inference\\\\_endpoints) to learn how to use `huggingface\\\\_hub` to manage your Inference Endpoints programmatically. Inference Endpoints can be fully managed via API. The endpoints are documented with [Swagger](https://api.endpoints.huggingface.cloud/). The [`InferenceEndpoint`] class is a simple wrapper built on top on this API. ## Methods\\nA subset of the Inference Endpoint features are implemented in [`HfApi`]:\\n- [`get\\\\_inference\\\\_endpoint`] and [`list\\\\_inference\\\\_endpoints`] to get information about your Inference Endpoints\\n- [`create\\\\_inference\\\\_endpoint`], [`update\\\\_inference\\\\_endpoint`] and [`delete\\\\_inference\\\\_endpoint`] to deploy and manage Inference Endpoints\\n- [`pause\\\\_inference\\\\_endpoint`] and [`resume\\\\_inference\\\\_endpoint`] to pause and resume an Inference Endpoint\\n- [`scale\\\\_to\\\\_zero\\\\_inference\\\\_endpoint`] to manually scale an Endpoint to 0 replicas\\n## InferenceEndpoint\\nThe main dataclass is [`InferenceEndpoint`].\\n\\n\"my-endpoint-name\",\\n... repository=\"gpt2\",\\n... framework=\"pytorch\",\\n... task=\"text-generation\",\\n... accelerator=\"cpu\",\\n... vendor=\"aws\",\\n... region=\"us-east-1\",\\n... type=\"protected\",\\n... instance\\\\_size=\"medium\",\\n... instance\\\\_type=\"c6i\"\\n... )\\n```\\nIn this example, we created a `protected` Inference Endpoint named `\"my-endpoint-name\"`, to serve [gpt2](https://huggingface.co/gpt2) for `text-generation`. A `protected` Inference Endpoint means your token is required to access the API. We also need to provide additional information to configure the hardware requirements, such as vendor, region, accelerator, instance type, and size. You can check out the list of available resources [here](https://api.endpoints.huggingface.cloud/#/v2%3A%3Aprovider/list\\\\_vendors). Alternatively, you can create an Inference Endpoint manually using the [Web interface](https://ui.endpoints.huggingface.co/new) for convenience. Refer to this [guide](https://huggingface.co/docs/inference-endpoints/guides/advanced) for details on advanced settings and their usage. The value returned by [`create\\\\_inference\\\\_endpoint`] is an [`InferenceEndpoint`] object:\\n```py\\n>>> endpoint\\nInferenceEndpoint(name=\\'my-endpoint-name\\', namespace=\\'Wauplin\\', repository=\\'gpt2\\', status=\\'pending\\', url=None)\\n```\\nIt\\'s a dataclass that holds information about the endpoint.', 'type': 'tool', 'name': 'data_retriever', 'id': '425c9311-e8d3-4153-9460-ef218fb94571', 'tool_call_id': 'call_ZADbKtcSR8yjH9SDhFf5GjA9', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab8d53e684812d15f6f23', 'page_content': 'Add custom Dependencies\\nInference Endpoints‚Äô base image includes all required libraries to run inference on ü§ó Transformers models, but it also supports custom dependencies. This is useful if you want to:\\n\\\\* [customize your inference pipeline](/docs/inference-endpoints/guides/custom\\\\_handler) and need additional Python dependencies\\n\\\\* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)). To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within. ```bash\\noptimum[onnxruntime]==1.2.3\\nmkl-include\\nmkl\\n```\\nCheck out the `requirements.txt` files in the following model repositories for examples:\\n\\\\* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\\n\\\\* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom\\\\_container) for inference.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab8c03e684812d15f6f11', 'page_content': '‚ö°‚ö° Learn more about it by reading the [Inference API documentation](./models-inference). Finally, you can also deploy all those models to dedicated [Inference Endpoints](https://huggingface.co/docs/inference-endpoints).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bc90af3d2ebb270a0b2e', 'page_content': \"Inference Endpoints\\nInference Endpoints provides a secure production solution to easily deploy models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the [Hub](https://huggingface.co/models). This page is a reference for `huggingface\\\\_hub`'s integration with Inference Endpoints. For more information about the Inference Endpoints product, check out its [official documentation](https://huggingface.co/docs/inference-endpoints/index). Check out the [related guide](../guides/inference\\\\_endpoints) to learn how to use `huggingface\\\\_hub` to manage your Inference Endpoints programmatically. Inference Endpoints can be fully managed via API. The endpoints are documented with [Swagger](https://api.endpoints.huggingface.cloud/). The [`InferenceEndpoint`] class is a simple wrapper built on top on this API. ## Methods\\nA subset of the Inference Endpoint features are implemented in [`HfApi`]:\\n- [`get\\\\_inference\\\\_endpoint`] and [`list\\\\_inference\\\\_endpoints`] to get information about your Inference Endpoints\\n- [`create\\\\_inference\\\\_endpoint`], [`update\\\\_inference\\\\_endpoint`] and [`delete\\\\_inference\\\\_endpoint`] to deploy and manage Inference Endpoints\\n- [`pause\\\\_inference\\\\_endpoint`] and [`resume\\\\_inference\\\\_endpoint`] to pause and resume an Inference Endpoint\\n- [`scale\\\\_to\\\\_zero\\\\_inference\\\\_endpoint`] to manually scale an Endpoint to 0 replicas\\n## InferenceEndpoint\\nThe main dataclass is [`InferenceEndpoint`].\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaf223e684812d15f63e2', 'page_content': '\"my-endpoint-name\",\\n... repository=\"gpt2\",\\n... framework=\"pytorch\",\\n... task=\"text-generation\",\\n... accelerator=\"cpu\",\\n... vendor=\"aws\",\\n... region=\"us-east-1\",\\n... type=\"protected\",\\n... instance\\\\_size=\"medium\",\\n... instance\\\\_type=\"c6i\"\\n... )\\n```\\nIn this example, we created a `protected` Inference Endpoint named `\"my-endpoint-name\"`, to serve [gpt2](https://huggingface.co/gpt2) for `text-generation`. A `protected` Inference Endpoint means your token is required to access the API. We also need to provide additional information to configure the hardware requirements, such as vendor, region, accelerator, instance type, and size. You can check out the list of available resources [here](https://api.endpoints.huggingface.cloud/#/v2%3A%3Aprovider/list\\\\_vendors). Alternatively, you can create an Inference Endpoint manually using the [Web interface](https://ui.endpoints.huggingface.co/new) for convenience. Refer to this [guide](https://huggingface.co/docs/inference-endpoints/guides/advanced) for details on advanced settings and their usage. The value returned by [`create\\\\_inference\\\\_endpoint`] is an [`InferenceEndpoint`] object:\\n```py\\n>>> endpoint\\nInferenceEndpoint(name=\\'my-endpoint-name\\', namespace=\\'Wauplin\\', repository=\\'gpt2\\', status=\\'pending\\', url=None)\\n```\\nIt\\'s a dataclass that holds information about the endpoint.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'To install custom Python dependencies for Inference Endpoints, you should add a `requirements.txt` file to your model repository on the Hugging Face Hub. This file should list the Python dependencies you want to install.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--db479ed0-3a8b-471d-bdd8-1b75e77b13d8-0', 'usage_metadata': {'input_tokens': 1359, 'output_tokens': 43, 'total_tokens': 1402, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:29:57,835 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 60/65 [04:11<00:19,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\\n', 'type': 'human', 'id': '3f69493b-978e-4772-acb8-20cba2c9df9f'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_Yb5qeS18OVd57DsSvF4Swjbc', 'function': {'arguments': '{\"query\":\"Textual Inversion Stable Diffusion how many images needed\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--420e6752-3fdc-4c67-bd8a-05d9ed1def5d-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Textual Inversion Stable Diffusion how many images needed'}, 'id': 'call_Yb5qeS18OVd57DsSvF4Swjbc', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 367, 'output_tokens': 26, 'total_tokens': 393, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '# Textual Inversion fine-tuning example\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\\nColab for training\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\\\_textual\\\\_inversion\\\\_training.ipynb)\\nColab for inference\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\\\_conceptualizer\\\\_inference.ipynb)\\n## Running locally with PyTorch\\n### Installing the dependencies\\nBefore running the scripts, make sure to install the library\\'s training dependencies:\\n\\\\*\\\\*Important\\\\*\\\\*\\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\\\*\\\\*installing from source\\\\*\\\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements.\\n\\n# Textual Inversion fine-tuning example\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Training with Intel Extension for PyTorch\\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual\\\\_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual\\\\_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script. The example supports both single node and multi-node distributed training:\\n### Single node training\\n```bash\\nexport MODEL\\\\_NAME=\"CompVis/stable-diffusion-v1-4\"\\nexport DATA\\\\_DIR=\"path-to-dir-containing-dicoo-images\"\\npython textual\\\\_inversion.py \\\\\\n--pretrained\\\\_model\\\\_name\\\\_or\\\\_path=$MODEL\\\\_NAME \\\\\\n--train\\\\_data\\\\_dir=$DATA\\\\_DIR \\\\\\n--learnable\\\\_property=\"object\" \\\\\\n--placeholder\\\\_token=\"\" --initializer\\\\_token=\"toy\" \\\\\\n--seed=7 \\\\\\n--resolution=512 \\\\\\n--train\\\\_batch\\\\_size=1 \\\\\\n--gradient\\\\_accumulation\\\\_steps=1 \\\\\\n--max\\\\_train\\\\_steps=3000 \\\\\\n--learning\\\\_rate=2.5e-03 --scale\\\\_lr \\\\\\n--output\\\\_dir=\"textual\\\\_inversion\\\\_dicoo\"\\n```\\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. You may not get performance speedup without Bfloat16 support. ### Multi-node distributed training\\nBefore running the scripts, make sure to install the library\\'s training dependencies successfully:\\n```bash\\npython -m pip install oneccl\\\\_bind\\\\_pt==1.13 -f https://developer.intel.com/ipex-whl-stable-cpu\\n```\\n```bash\\nexport MODEL\\\\_NAME=\"CompVis/stable-diffusion-v1-4\"\\nexport DATA\\\\_DIR=\"path-to-dir-containing-dicoo-images\"\\noneccl\\\\_bindings\\\\_for\\\\_pytorch\\\\_path=$(python -c \"from oneccl\\\\_bindings\\\\_for\\\\_pytorch import cwd; print(cwd)\")\\nsource $oneccl\\\\_bindings\\\\_for\\\\_pytorch\\\\_path/env/setvars.sh\\npython -m intel\\\\_extension\\\\_for\\\\_pytorch.cpu.launch --distributed \\\\\\n--hostfile hostfile --nnodes 2 --nproc\\\\_per\\\\_node 2 textual\\\\_inversion.py \\\\\\n--pretrained\\\\_model\\\\_name\\\\_or\\\\_path=$MODEL\\\\_NAME \\\\\\n--train\\\\_data\\\\_dir=$DATA\\\\_DIR \\\\\\n--learnable\\\\_property=\"object\" \\\\\\n--placeholder\\\\_token=\"\" --initializer\\\\_token=\"toy\" \\\\\\n--seed=7 \\\\\\n--resolution=512 \\\\\\n--train\\\\_batch\\\\_size=1 \\\\\\n--gradient\\\\_accumulation\\\\_steps=1 \\\\\\n--max\\\\_train\\\\_steps=750 \\\\\\n--learning\\\\_rate=2.5e-03 --scale\\\\_lr \\\\\\n--output\\\\_dir=\"textual\\\\_inversion\\\\_dicoo\"\\n```\\nThe above is a simple distributed training usage on 2 nodes with 2 processes on each node.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Image-to-image\\nThe Stable Diffusion model can also be applied to image-to-image generation by passing a text prompt and an initial image to condition the generation of new images. The [`StableDiffusionImg2ImgPipeline`] uses the diffusion-denoising mechanism proposed in [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations](https://huggingface.co/papers/2108.01073) by Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Stable Diffusion 2\\nStable Diffusion 2 is a text-to-image \\\\_latent diffusion\\\\_ model built upon the work of the original [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release), and it was led by Robin Rombach and Katherine Crowson from [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). \\\\*The Stable Diffusion 2.0 release includes robust text-to-image models trained using a brand new text encoder (OpenCLIP), developed by LAION with support from Stability AI, which greatly improves the quality of the generated images compared to earlier V1 releases. The text-to-image models in this release can generate images with default resolutions of both 512x512 pixels and 768x768 pixels. These models are trained on an aesthetic subset of the [LAION-5B dataset](https://laion.ai/blog/laion-5b/) created by the DeepFloyd team at Stability AI, which is then further filtered to remove adult content using [LAION‚Äôs NSFW filter](https://openreview.net/forum?id=M3Y74vmsMcY).\\\\*\\nFor more details about how Stable Diffusion 2 works and how it differs from the original Stable Diffusion, please refer to the official [announcement post](https://stability.ai/blog/stable-diffusion-v2-release).', 'type': 'tool', 'name': 'data_retriever', 'id': 'e6c6a4b0-8d76-4123-8bc0-929438adcca1', 'tool_call_id': 'call_Yb5qeS18OVd57DsSvF4Swjbc', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3403e684812d15f6873', 'page_content': \"# Textual Inversion fine-tuning example\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\\nColab for training\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\\\_textual\\\\_inversion\\\\_training.ipynb)\\nColab for inference\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\\\_conceptualizer\\\\_inference.ipynb)\\n## Running locally with PyTorch\\n### Installing the dependencies\\nBefore running the scripts, make sure to install the library's training dependencies:\\n\\\\*\\\\*Important\\\\*\\\\*\\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\\\*\\\\*installing from source\\\\*\\\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899ba3baf3d2ebb270a0884', 'page_content': '# Textual Inversion fine-tuning example\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Training with Intel Extension for PyTorch\\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual\\\\_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual\\\\_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script. The example supports both single node and multi-node distributed training:\\n### Single node training\\n```bash\\nexport MODEL\\\\_NAME=\"CompVis/stable-diffusion-v1-4\"\\nexport DATA\\\\_DIR=\"path-to-dir-containing-dicoo-images\"\\npython textual\\\\_inversion.py \\\\\\n--pretrained\\\\_model\\\\_name\\\\_or\\\\_path=$MODEL\\\\_NAME \\\\\\n--train\\\\_data\\\\_dir=$DATA\\\\_DIR \\\\\\n--learnable\\\\_property=\"object\" \\\\\\n--placeholder\\\\_token=\"\" --initializer\\\\_token=\"toy\" \\\\\\n--seed=7 \\\\\\n--resolution=512 \\\\\\n--train\\\\_batch\\\\_size=1 \\\\\\n--gradient\\\\_accumulation\\\\_steps=1 \\\\\\n--max\\\\_train\\\\_steps=3000 \\\\\\n--learning\\\\_rate=2.5e-03 --scale\\\\_lr \\\\\\n--output\\\\_dir=\"textual\\\\_inversion\\\\_dicoo\"\\n```\\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. You may not get performance speedup without Bfloat16 support. ### Multi-node distributed training\\nBefore running the scripts, make sure to install the library\\'s training dependencies successfully:\\n```bash\\npython -m pip install oneccl\\\\_bind\\\\_pt==1.13 -f https://developer.intel.com/ipex-whl-stable-cpu\\n```\\n```bash\\nexport MODEL\\\\_NAME=\"CompVis/stable-diffusion-v1-4\"\\nexport DATA\\\\_DIR=\"path-to-dir-containing-dicoo-images\"\\noneccl\\\\_bindings\\\\_for\\\\_pytorch\\\\_path=$(python -c \"from oneccl\\\\_bindings\\\\_for\\\\_pytorch import cwd; print(cwd)\")\\nsource $oneccl\\\\_bindings\\\\_for\\\\_pytorch\\\\_path/env/setvars.sh\\npython -m intel\\\\_extension\\\\_for\\\\_pytorch.cpu.launch --distributed \\\\\\n--hostfile hostfile --nnodes 2 --nproc\\\\_per\\\\_node 2 textual\\\\_inversion.py \\\\\\n--pretrained\\\\_model\\\\_name\\\\_or\\\\_path=$MODEL\\\\_NAME \\\\\\n--train\\\\_data\\\\_dir=$DATA\\\\_DIR \\\\\\n--learnable\\\\_property=\"object\" \\\\\\n--placeholder\\\\_token=\"\" --initializer\\\\_token=\"toy\" \\\\\\n--seed=7 \\\\\\n--resolution=512 \\\\\\n--train\\\\_batch\\\\_size=1 \\\\\\n--gradient\\\\_accumulation\\\\_steps=1 \\\\\\n--max\\\\_train\\\\_steps=750 \\\\\\n--learning\\\\_rate=2.5e-03 --scale\\\\_lr \\\\\\n--output\\\\_dir=\"textual\\\\_inversion\\\\_dicoo\"\\n```\\nThe above is a simple distributed training usage on 2 nodes with 2 processes on each node.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aabfa3e684812d15f6035', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Image-to-image\\nThe Stable Diffusion model can also be applied to image-to-image generation by passing a text prompt and an initial image to condition the generation of new images. The [`StableDiffusionImg2ImgPipeline`] uses the diffusion-denoising mechanism proposed in [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations](https://huggingface.co/papers/2108.01073) by Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899ba2baf3d2ebb270a0874', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Stable Diffusion 2\\nStable Diffusion 2 is a text-to-image \\\\_latent diffusion\\\\_ model built upon the work of the original [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release), and it was led by Robin Rombach and Katherine Crowson from [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). \\\\*The Stable Diffusion 2.0 release includes robust text-to-image models trained using a brand new text encoder (OpenCLIP), developed by LAION with support from Stability AI, which greatly improves the quality of the generated images compared to earlier V1 releases. The text-to-image models in this release can generate images with default resolutions of both 512x512 pixels and 768x768 pixels. These models are trained on an aesthetic subset of the [LAION-5B dataset](https://laion.ai/blog/laion-5b/) created by the DeepFloyd team at Stability AI, which is then further filtered to remove adult content using [LAION‚Äôs NSFW filter](https://openreview.net/forum?id=M3Y74vmsMcY).\\\\*\\nFor more details about how Stable Diffusion 2 works and how it differs from the original Stable Diffusion, please refer to the official [announcement post](https://stability.ai/blog/stable-diffusion-v2-release).', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'To teach new concepts to Stable Diffusion using Textual Inversion, you need just 3-5 examples.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--6afe49e0-22c1-4abe-99c1-aa5cd15c25d3-0', 'usage_metadata': {'input_tokens': 2021, 'output_tokens': 23, 'total_tokens': 2044, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:30:01,297 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 61/65 [04:14<00:14,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\\n', 'type': 'human', 'id': '05b5dcf6-5d53-48aa-897b-95a95f2e9c84'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_wRkZH7dR65JX6jytR5WMiKZF', 'function': {'arguments': '{\"query\":\"maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--fa87b081-62eb-44ad-877d-2f131b0d6898-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0'}, 'id': 'call_wRkZH7dR65JX6jytR5WMiKZF', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 373, 'output_tokens': 36, 'total_tokens': 409, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future. ## Sharded checkpoints\\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save\\\\_pretrained(save\\\\_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in. You can control the maximum size before sharding with the `max\\\\_shard\\\\_size` parameter, so for the sake of an example, we\\'ll use a normal-size models with a small shard size: let\\'s take a traditional BERT model. ```py\\nfrom transformers import AutoModel\\nmodel = AutoModel.from\\\\_pretrained(\"bert-base-cased\")\\n```\\nIf you save it using [`~PreTrainedModel.save\\\\_pretrained`], you will get a new folder with two files: the config of the model and its weights:\\n```py\\n>>> import os\\n>>> import tempfile\\n>>> with tempfile.TemporaryDirectory() as tmp\\\\_dir:\\n... model.save\\\\_pretrained(tmp\\\\_dir)\\n... print(sorted(os.listdir(tmp\\\\_dir)))\\n[\\'config.json\\', \\'pytorch\\\\_model.bin\\']\\n```\\nNow let\\'s use a maximum shard size of 200MB:\\n```py\\n>>> with tempfile.TemporaryDirectory() as tmp\\\\_dir:\\n... model.save\\\\_pretrained(tmp\\\\_dir, max\\\\_shard\\\\_size=\"200MB\")\\n... print(sorted(os.listdir(tmp\\\\_dir)))\\n[\\'config.json\\', \\'pytorch\\\\_model-00001-of-00003.bin\\', \\'pytorch\\\\_model-00002-of-00003.bin\\', \\'pytorch\\\\_model-00003-of-00003.bin\\', \\'pytorch\\\\_model.bin.index.json\\']\\n```\\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index.\\n\\nA checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from\\\\_pretrained`] method:\\n```py\\n>>> with tempfile.TemporaryDirectory() as tmp\\\\_dir:\\n... model.save\\\\_pretrained(tmp\\\\_dir, max\\\\_shard\\\\_size=\"200MB\")\\n... new\\\\_model = AutoModel.from\\\\_pretrained(tmp\\\\_dir)\\n```\\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard. Behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\\n```py\\n>>> import json\\n>>> with tempfile.TemporaryDirectory() as tmp\\\\_dir:\\n... model.save\\\\_pretrained(tmp\\\\_dir, max\\\\_shard\\\\_size=\"200MB\")\\n... with open(os.path.join(tmp\\\\_dir, \"pytorch\\\\_model.bin.index.json\"), \"r\") as f:\\n... index = json.load(f)\\n>>> print(index.keys())\\ndict\\\\_keys([\\'metadata\\', \\'weight\\\\_map\\'])\\n```\\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\\n```py\\n>>> index[\"metadata\"]\\n{\\'total\\\\_size\\': 433245184}\\n```\\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state\\\\_dict`) to the file it\\'s stored in:\\n```py\\n>>> index[\"weight\\\\_map\"]\\n{\\'embeddings.LayerNorm.bias\\': \\'pytorch\\\\_model-00001-of-00003.bin\\',\\n\\'embeddings.LayerNorm.weight\\': \\'pytorch\\\\_model-00001-of-00003.bin\\',\\n... ```\\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from\\\\_pretrained`] (like you would do `model.load\\\\_state\\\\_dict()` for a full checkpoint) you should use [`~modeling\\\\_utils.load\\\\_sharded\\\\_checkpoint`]:\\n```py\\n>>> from transformers.modeling\\\\_utils import load\\\\_sharded\\\\_checkpoint\\n>>> with tempfile.TemporaryDirectory() as tmp\\\\_dir:\\n... model.save\\\\_pretrained(tmp\\\\_dir, max\\\\_shard\\\\_size=\"200MB\")\\n... load\\\\_sharded\\\\_checkpoint(model, tmp\\\\_dir)\\n```\\n## Low memory loading\\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library. Please read the following guide for more information: [Large model loading using Accelerate](./main\\\\_classes/model#large-model-loading)\\n\\nI was able to fit a batch size (BS) of 16 before hitting Out of Memory (OOM) error. Note, that for simplicity and to make it easier to understand, I have only shown\\nthe command line arguments important for this demonstration. You will find the complete command line at\\n[this post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400). Next, we are going to re-run the benchmark every time adding one of the following:\\n1. `--fp16`\\n2. `--sharded\\\\_ddp` (fairscale)\\n3. `--sharded\\\\_ddp --fp16` (fairscale)\\n4. `--deepspeed` without cpu offloading\\n5. `--deepspeed` with cpu offloading\\nSince the key optimization here is that each technique deploys GPU RAM more efficiently, we will try to continually increase the batch size and expect the training and evaluation to complete faster (while keeping the metrics steady or even improving some, but we won\\'t focus on these here). Remember that training and evaluation stages are very different from each other, because during training model weights are being modified, gradients are being calculated, and optimizer states are stored.\\n\\nere is how to convert a GPT2 model generated outside of `transformers`\\n\\\\* [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)-generated model:\\nUse [convert\\\\_megatron\\\\_gpt2\\\\_checkpoint.py](../megatron\\\\_gpt2/convert\\\\_megatron\\\\_gpt2\\\\_checkpoint.py)\\n\\\\* [big-science fork of Megatron-Deepspeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed/)-generated model:\\nUse the instructions [here](https://github.com/bigscience-workshop/bigscience/tree/aa872e754106f6678e8a9dac8c6962404ba39a6d/train/tr1-13B-base#checkpoint-conversion-and-upload).', 'type': 'tool', 'name': 'data_retriever', 'id': '91b8120f-c101-4d73-93dd-9c3c5b50ce79', 'tool_call_id': 'call_wRkZH7dR65JX6jytR5WMiKZF', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa9b23e684812d15f5d3f', 'page_content': 'In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future. ## Sharded checkpoints\\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save\\\\_pretrained(save\\\\_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in. You can control the maximum size before sharding with the `max\\\\_shard\\\\_size` parameter, so for the sake of an example, we\\'ll use a normal-size models with a small shard size: let\\'s take a traditional BERT model. ```py\\nfrom transformers import AutoModel\\nmodel = AutoModel.from\\\\_pretrained(\"bert-base-cased\")\\n```\\nIf you save it using [`~PreTrainedModel.save\\\\_pretrained`], you will get a new folder with two files: the config of the model and its weights:\\n```py\\n>>> import os\\n>>> import tempfile\\n>>> with tempfile.TemporaryDirectory() as tmp\\\\_dir:\\n... model.save\\\\_pretrained(tmp\\\\_dir)\\n... print(sorted(os.listdir(tmp\\\\_dir)))\\n[\\'config.json\\', \\'pytorch\\\\_model.bin\\']\\n```\\nNow let\\'s use a maximum shard size of 200MB:\\n```py\\n>>> with tempfile.TemporaryDirectory() as tmp\\\\_dir:\\n... model.save\\\\_pretrained(tmp\\\\_dir, max\\\\_shard\\\\_size=\"200MB\")\\n... print(sorted(os.listdir(tmp\\\\_dir)))\\n[\\'config.json\\', \\'pytorch\\\\_model-00001-of-00003.bin\\', \\'pytorch\\\\_model-00002-of-00003.bin\\', \\'pytorch\\\\_model-00003-of-00003.bin\\', \\'pytorch\\\\_model.bin.index.json\\']\\n```\\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa9b23e684812d15f5d40', 'page_content': 'A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from\\\\_pretrained`] method:\\n```py\\n>>> with tempfile.TemporaryDirectory() as tmp\\\\_dir:\\n... model.save\\\\_pretrained(tmp\\\\_dir, max\\\\_shard\\\\_size=\"200MB\")\\n... new\\\\_model = AutoModel.from\\\\_pretrained(tmp\\\\_dir)\\n```\\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard. Behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\\n```py\\n>>> import json\\n>>> with tempfile.TemporaryDirectory() as tmp\\\\_dir:\\n... model.save\\\\_pretrained(tmp\\\\_dir, max\\\\_shard\\\\_size=\"200MB\")\\n... with open(os.path.join(tmp\\\\_dir, \"pytorch\\\\_model.bin.index.json\"), \"r\") as f:\\n... index = json.load(f)\\n>>> print(index.keys())\\ndict\\\\_keys([\\'metadata\\', \\'weight\\\\_map\\'])\\n```\\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\\n```py\\n>>> index[\"metadata\"]\\n{\\'total\\\\_size\\': 433245184}\\n```\\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state\\\\_dict`) to the file it\\'s stored in:\\n```py\\n>>> index[\"weight\\\\_map\"]\\n{\\'embeddings.LayerNorm.bias\\': \\'pytorch\\\\_model-00001-of-00003.bin\\',\\n\\'embeddings.LayerNorm.weight\\': \\'pytorch\\\\_model-00001-of-00003.bin\\',\\n... ```\\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from\\\\_pretrained`] (like you would do `model.load\\\\_state\\\\_dict()` for a full checkpoint) you should use [`~modeling\\\\_utils.load\\\\_sharded\\\\_checkpoint`]:\\n```py\\n>>> from transformers.modeling\\\\_utils import load\\\\_sharded\\\\_checkpoint\\n>>> with tempfile.TemporaryDirectory() as tmp\\\\_dir:\\n... model.save\\\\_pretrained(tmp\\\\_dir, max\\\\_shard\\\\_size=\"200MB\")\\n... load\\\\_sharded\\\\_checkpoint(model, tmp\\\\_dir)\\n```\\n## Low memory loading\\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library. Please read the following guide for more information: [Large model loading using Accelerate](./main\\\\_classes/model#large-model-loading)', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b409af3d2ebb270a014b', 'page_content': \"I was able to fit a batch size (BS) of 16 before hitting Out of Memory (OOM) error. Note, that for simplicity and to make it easier to understand, I have only shown\\nthe command line arguments important for this demonstration. You will find the complete command line at\\n[this post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400). Next, we are going to re-run the benchmark every time adding one of the following:\\n1. `--fp16`\\n2. `--sharded\\\\_ddp` (fairscale)\\n3. `--sharded\\\\_ddp --fp16` (fairscale)\\n4. `--deepspeed` without cpu offloading\\n5. `--deepspeed` with cpu offloading\\nSince the key optimization here is that each technique deploys GPU RAM more efficiently, we will try to continually increase the batch size and expect the training and evaluation to complete faster (while keeping the metrics steady or even improving some, but we won't focus on these here). Remember that training and evaluation stages are very different from each other, because during training model weights are being modified, gradients are being calculated, and optimizer states are stored.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab23d3e684812d15f6753', 'page_content': 'ere is how to convert a GPT2 model generated outside of `transformers`\\n\\\\* [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)-generated model:\\nUse [convert\\\\_megatron\\\\_gpt2\\\\_checkpoint.py](../megatron\\\\_gpt2/convert\\\\_megatron\\\\_gpt2\\\\_checkpoint.py)\\n\\\\* [big-science fork of Megatron-Deepspeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed/)-generated model:\\nUse the instructions [here](https://github.com/bigscience-workshop/bigscience/tree/aa872e754106f6678e8a9dac8c6962404ba39a6d/train/tr1-13B-base#checkpoint-conversion-and-upload).', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'In Transformers version 4.18.0, model checkpoints that take more than 10GB of space are automatically sharded into smaller pieces.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--1fecfd15-e7b9-4220-9226-eb5c0f4ec417-0', 'usage_metadata': {'input_tokens': 1803, 'output_tokens': 29, 'total_tokens': 1832, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:30:05,340 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 62/65 [04:18<00:11,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\\n', 'type': 'human', 'id': '21073c59-100c-4e92-a9ed-5971ded597e5'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_IrMfK6dXfOeJ0SERPOdRMOFl', 'function': {'arguments': '{\"query\":\"Weights and Biases W&B purpose for data scientists and machine learning scientists\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--53128416-43e3-4a95-bd6c-d4d016b5f3eb-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Weights and Biases W&B purpose for data scientists and machine learning scientists'}, 'id': 'call_IrMfK6dXfOeJ0SERPOdRMOFl', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 370, 'output_tokens': 29, 'total_tokens': 399, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'As in the examples above, some common steps that may help decide whether and how to apply ML in a way that minimizes bias-related risk include:\\n\\\\* Investigate:\\n\\\\* Reports of bias in the field pre-ML\\n\\\\* At-risk demographic categories for your specific use case\\n\\\\* Examine:\\n\\\\* The impact of your optimization objective on reinforcing biases\\n\\\\* Alternative objectives that favor diversity and positive long-term impacts\\n### I am curating/picking a dataset for my ML system, how can I address bias? While training datasets are [not the sole source of bias](https://www.cell.com/patterns/fulltext/S2666-3899(21)00061-1) in the ML development cycle, they do play a significant role. Does your [dataset disproportionately associate](https://aclanthology.org/2020.emnlp-main.23/) biographies of women with life events but those of men with achievements? Those \\\\*\\\\*stereotypes\\\\*\\\\* are probably going to show up in your full ML system! Does your voice recognition dataset only feature specific accents? Not a good sign for [the inclusivity of technology](https://www.scientificamerican.com/article/speech-recognition-tech-is-yet-another-example-of-bias/) you build with it in terms of \\\\*\\\\*disparate performance\\\\*\\\\*! Whether you‚Äôre curating a dataset for ML applications or selecting a dataset to train an ML model, finding out, mitigating, and [communicating](https://dl.acm.org/doi/10.1145/3479582) to what extent the data exhibits these phenomena are all necessary steps to reducing bias-related risks. You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it comes from, who are the people represented on the data, and what the curation process was. Several frameworks for this reflection and documentation have been proposed such as [Data Statements for NLP](https://direct.mit.edu/tacl/article/doi/10.1162/tacl\\\\_a\\\\_00041/43452/Data-Statements-for-Natural-Language-Processing) or [Datasheets for Datasets](https://dl.acm.org/doi/10.1145/3458723). The Hugging Face Hub includes a Dataset Card [template](https://github.com/huggingface/datasets/blob/main/templates/README.md) and [guide](https://github.com/huggingface/datasets/blob/main/templates/README\\\\_guide.md#dataset-card-creation-guide) inspired by these works; the section on [considerations for using the data](https://github.com/huggingface/datasets/blob/main/templates/README\\\\_guide.md#considerations-for-using-the-data) is usually a good place to look for information about notable biases if you‚Äôre browsing datasets, or to write a paragraph sharing your insights on the topic if you‚Äôre sharing a new one. And if you‚Äôre looking for more inspiration on what to put there, check out these sections written by Hub users in the [BigLAM organization](https://huggingface.co/biglam) for historical datasets of [legal proceedings](https://huggingface.co/datasets/biglam/old\\\\_bailey\\\\_proceedings#social-impact-of-dataset), [image classification](https://huggingface.co/datasets/biglam/brill\\\\_iconclass#social-impact-of-dataset), and [newspapers](https://huggingface.co/datasets/biglam/bnl\\\\_newspapers1841-1879#social-impact-of-dataset). ![HF Dataset Card guide for the Social Impact and Bias Sections](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img3.png)\\n*[HF Dataset Card guide](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md#social-impact-of-dataset) for the Social Impact and Bias Sections*\\n\\nWhile describing the origin and context of a dataset is always a good starting point to understand the biases at play, [quantitatively measuring phenomena](https://arxiv.org/abs/2212.05129) that encode those biases can be just as helpful.\\n\\n--\\ntitle: \"Let\\'s talk about biases in machine learning! Ethics and Society Newsletter #2\"\\nthumbnail: /blog/assets/122\\\\_ethics\\\\_soc\\\\_2/thumbnail-solstice.png\\nauthors:\\n- user: yjernite\\n---\\n# Machine Learning in development: Let\\'s talk about bias! \\\\_Bias in ML is ubiquitous, and Bias in ML is complex; so complex in fact that no single technical intervention is likely to meaningfully address the problems it engenders. ML models, as sociotechnical systems, amplify social trends that may exacerbate inequities and harmful biases in ways that depend on their deployment context and are constantly evolving.\\\\_\\n\\\\_This means that developing ML systems with care requires vigilance and responding to feedback from those deployment contexts, which in turn we can facilitate by sharing lessons across contexts and developing tools to analyze signs of bias at every level of ML development.\\\\_\\n\\\\_This blog post from the [Ethics and Society regulars @ü§ó](https://huggingface.co/blog/ethics-soc-1) shares some of the lessons we have learned along with tools we have developed to support ourselves and others in our community‚Äôs efforts to better address bias in Machine Learning. The first part is a broader reflection on bias and its context. If you‚Äôve already read it and are coming back specifically for the tools, feel free to jump to the [datasets](#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias) or [models](#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias)\\nsection!\\\\_\\n\\n![Selection of tools developed by HF team members to address bias in ML](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img1.jpg)\\n*Selection of tools developed by ü§ó team members to address bias in ML*\\n\\n\\\\*\\\\*Table of contents:\\\\*\\\\*\\n\\\\* \\\\*\\\\*On Machine Biases\\\\*\\\\*\\n\\\\* [Machine Bias: from ML Systems to Risks](#machine-bias-from-ml-systems-to-personal-and-social-risks)\\n\\\\* [Putting Bias in Context](#putting-bias-in-context)\\n\\\\* \\\\*\\\\*Tools and Recommendations\\\\*\\\\*\\n\\\\* [Addressing Bias throughout ML Development](#addressing-bias-throughout-the-ml-development-cycle)\\n\\\\* [Task Definition](#i-am-defining-the-task-of-my-ml-system-how-can-i-address-bias)\\n\\\\* [Dataset Curation](#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias)\\n\\\\* [Model Training](#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias)\\n\\\\* [Overview of ü§ó Bias Tools](#conclusion-and-overview-of-bias-analysis-and-documentation-tools-from-)\\n## \\\\_Machine Bias:\\\\_ from ML Systems to Personal and Social Risks\\nML systems allow us to automate complex tasks at a scale never seen before as they are deployed in more sectors and use cases.\\n\\nWe hope you will find it a useful resource to guide concrete considerations of the social impact of your work and can leverage the tools referenced here to help mitigate these issues when they arise. ## Putting Bias in Context\\nThe first and maybe most important concept to consider when dealing with machine bias is \\\\*\\\\*context\\\\*\\\\*. In their foundational work on [bias in NLP](https://aclanthology.org/2020.acl-main.485.pdf), Su Lin Blodgett et al. point out that: \\\\_‚Äú[T]he majority of [academic works on machine bias] fail to engage critically with what constitutes ‚Äúbias‚Äù in the first place‚Äù\\\\_, including by building their work on top of \\\\_‚Äúunstated assumptions about what kinds of system behaviors are harmful, in what ways, to whom, and why‚Äù\\\\_. This may not come as much of a surprise given the ML research community‚Äôs [focus on the value of ‚Äúgeneralization‚Äù](https://dl.acm.org/doi/10.1145/3531146.3533083) ‚Äî the most cited motivation for work in the field after ‚Äúperformance‚Äù. However, while tools for bias assessment that apply to a wide range of settings are valuable to \\\\*\\\\*enable a broader analysis of common trends\\\\*\\\\* in model behaviors, their ability to target the mechanisms that lead to discrimination in \\\\*\\\\*concrete use cases is inherently limited\\\\*\\\\*. Using them to guide specific decisions within the ML development cycle usually requires an extra step or two to take the system‚Äôs specific use context and affected people into consideration. ![Excerpt on considerations of ML uses context and people from the Model Card Guidebook](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img_foresight.png)\\n*Excerpt on considerations of ML uses context and people from the [Model Card Guidebook](https://huggingface.co/docs/hub/model-cards)*\\n\\nNow let‚Äôs dive deeper into the issue of linking biases in stand-alone/context-less ML artifacts to specific harms.\\n\\nin real time via an API or in batch jobs that run once a day. Memory footprint refers to the size of the model weights and how much hardware memory they occupy. If a model is too large to fit on a single GPU or CPU, then it has to be split over multiple ones, which can be more or less difficult depending on the model architecture and the deployment method. When doing online model evaluation, there is often a trade-off to be done between inference speed and accuracy or precision, whereas this is less the case for offline evaluation. ## Limitations and bias\\nAll models and all metrics have their limitations and biases, which depend on the way in which they were trained, the data that was used, and their intended uses. It is important to measure and communicate these limitations clearly to prevent misuse and unintended impacts, for instance via [model cards](https://huggingface.co/course/chapter4/4?fw=pt) which document the training and evaluation process. Measuring biases can be done by evaluating models on datasets such as [Wino Bias](https://huggingface.co/datasets/wino\\\\_bias) or [MD Gender Bias](https://huggingface.co/datasets/md\\\\_gender\\\\_bias), and by doing [Interactive Error Analyis](https://huggingface.co/spaces/nazneen/error-analysis) to try to identify which subsets of the evaluation dataset a model performs poorly on. We are currently working on additional measurements that can be used to quantify different dimensions of bias in both models and datasets -- stay tuned for more documentation on this topic!', 'type': 'tool', 'name': 'data_retriever', 'id': 'ce70ee95-f81c-415b-b8a9-c41606487a70', 'tool_call_id': 'call_IrMfK6dXfOeJ0SERPOdRMOFl', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c1e1666af48523f520aa', 'page_content': 'As in the examples above, some common steps that may help decide whether and how to apply ML in a way that minimizes bias-related risk include:\\n\\\\* Investigate:\\n\\\\* Reports of bias in the field pre-ML\\n\\\\* At-risk demographic categories for your specific use case\\n\\\\* Examine:\\n\\\\* The impact of your optimization objective on reinforcing biases\\n\\\\* Alternative objectives that favor diversity and positive long-term impacts\\n### I am curating/picking a dataset for my ML system, how can I address bias? While training datasets are [not the sole source of bias](https://www.cell.com/patterns/fulltext/S2666-3899(21)00061-1) in the ML development cycle, they do play a significant role. Does your [dataset disproportionately associate](https://aclanthology.org/2020.emnlp-main.23/) biographies of women with life events but those of men with achievements? Those \\\\*\\\\*stereotypes\\\\*\\\\* are probably going to show up in your full ML system! Does your voice recognition dataset only feature specific accents? Not a good sign for [the inclusivity of technology](https://www.scientificamerican.com/article/speech-recognition-tech-is-yet-another-example-of-bias/) you build with it in terms of \\\\*\\\\*disparate performance\\\\*\\\\*! Whether you‚Äôre curating a dataset for ML applications or selecting a dataset to train an ML model, finding out, mitigating, and [communicating](https://dl.acm.org/doi/10.1145/3479582) to what extent the data exhibits these phenomena are all necessary steps to reducing bias-related risks. You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it comes from, who are the people represented on the data, and what the curation process was. Several frameworks for this reflection and documentation have been proposed such as [Data Statements for NLP](https://direct.mit.edu/tacl/article/doi/10.1162/tacl\\\\_a\\\\_00041/43452/Data-Statements-for-Natural-Language-Processing) or [Datasheets for Datasets](https://dl.acm.org/doi/10.1145/3458723). The Hugging Face Hub includes a Dataset Card [template](https://github.com/huggingface/datasets/blob/main/templates/README.md) and [guide](https://github.com/huggingface/datasets/blob/main/templates/README\\\\_guide.md#dataset-card-creation-guide) inspired by these works; the section on [considerations for using the data](https://github.com/huggingface/datasets/blob/main/templates/README\\\\_guide.md#considerations-for-using-the-data) is usually a good place to look for information about notable biases if you‚Äôre browsing datasets, or to write a paragraph sharing your insights on the topic if you‚Äôre sharing a new one. And if you‚Äôre looking for more inspiration on what to put there, check out these sections written by Hub users in the [BigLAM organization](https://huggingface.co/biglam) for historical datasets of [legal proceedings](https://huggingface.co/datasets/biglam/old\\\\_bailey\\\\_proceedings#social-impact-of-dataset), [image classification](https://huggingface.co/datasets/biglam/brill\\\\_iconclass#social-impact-of-dataset), and [newspapers](https://huggingface.co/datasets/biglam/bnl\\\\_newspapers1841-1879#social-impact-of-dataset). ![HF Dataset Card guide for the Social Impact and Bias Sections](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img3.png)\\n*[HF Dataset Card guide](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md#social-impact-of-dataset) for the Social Impact and Bias Sections*\\n\\nWhile describing the origin and context of a dataset is always a good starting point to understand the biases at play, [quantitatively measuring phenomena](https://arxiv.org/abs/2212.05129) that encode those biases can be just as helpful.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c1de666af48523f52095', 'page_content': '--\\ntitle: \"Let\\'s talk about biases in machine learning! Ethics and Society Newsletter #2\"\\nthumbnail: /blog/assets/122\\\\_ethics\\\\_soc\\\\_2/thumbnail-solstice.png\\nauthors:\\n- user: yjernite\\n---\\n# Machine Learning in development: Let\\'s talk about bias! \\\\_Bias in ML is ubiquitous, and Bias in ML is complex; so complex in fact that no single technical intervention is likely to meaningfully address the problems it engenders. ML models, as sociotechnical systems, amplify social trends that may exacerbate inequities and harmful biases in ways that depend on their deployment context and are constantly evolving.\\\\_\\n\\\\_This means that developing ML systems with care requires vigilance and responding to feedback from those deployment contexts, which in turn we can facilitate by sharing lessons across contexts and developing tools to analyze signs of bias at every level of ML development.\\\\_\\n\\\\_This blog post from the [Ethics and Society regulars @ü§ó](https://huggingface.co/blog/ethics-soc-1) shares some of the lessons we have learned along with tools we have developed to support ourselves and others in our community‚Äôs efforts to better address bias in Machine Learning. The first part is a broader reflection on bias and its context. If you‚Äôve already read it and are coming back specifically for the tools, feel free to jump to the [datasets](#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias) or [models](#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias)\\nsection!\\\\_\\n\\n![Selection of tools developed by HF team members to address bias in ML](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img1.jpg)\\n*Selection of tools developed by ü§ó team members to address bias in ML*\\n\\n\\\\*\\\\*Table of contents:\\\\*\\\\*\\n\\\\* \\\\*\\\\*On Machine Biases\\\\*\\\\*\\n\\\\* [Machine Bias: from ML Systems to Risks](#machine-bias-from-ml-systems-to-personal-and-social-risks)\\n\\\\* [Putting Bias in Context](#putting-bias-in-context)\\n\\\\* \\\\*\\\\*Tools and Recommendations\\\\*\\\\*\\n\\\\* [Addressing Bias throughout ML Development](#addressing-bias-throughout-the-ml-development-cycle)\\n\\\\* [Task Definition](#i-am-defining-the-task-of-my-ml-system-how-can-i-address-bias)\\n\\\\* [Dataset Curation](#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias)\\n\\\\* [Model Training](#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias)\\n\\\\* [Overview of ü§ó Bias Tools](#conclusion-and-overview-of-bias-analysis-and-documentation-tools-from-)\\n## \\\\_Machine Bias:\\\\_ from ML Systems to Personal and Social Risks\\nML systems allow us to automate complex tasks at a scale never seen before as they are deployed in more sectors and use cases.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c1de666af48523f5209a', 'page_content': 'We hope you will find it a useful resource to guide concrete considerations of the social impact of your work and can leverage the tools referenced here to help mitigate these issues when they arise. ## Putting Bias in Context\\nThe first and maybe most important concept to consider when dealing with machine bias is \\\\*\\\\*context\\\\*\\\\*. In their foundational work on [bias in NLP](https://aclanthology.org/2020.acl-main.485.pdf), Su Lin Blodgett et al. point out that: \\\\_‚Äú[T]he majority of [academic works on machine bias] fail to engage critically with what constitutes ‚Äúbias‚Äù in the first place‚Äù\\\\_, including by building their work on top of \\\\_‚Äúunstated assumptions about what kinds of system behaviors are harmful, in what ways, to whom, and why‚Äù\\\\_. This may not come as much of a surprise given the ML research community‚Äôs [focus on the value of ‚Äúgeneralization‚Äù](https://dl.acm.org/doi/10.1145/3531146.3533083) ‚Äî the most cited motivation for work in the field after ‚Äúperformance‚Äù. However, while tools for bias assessment that apply to a wide range of settings are valuable to \\\\*\\\\*enable a broader analysis of common trends\\\\*\\\\* in model behaviors, their ability to target the mechanisms that lead to discrimination in \\\\*\\\\*concrete use cases is inherently limited\\\\*\\\\*. Using them to guide specific decisions within the ML development cycle usually requires an extra step or two to take the system‚Äôs specific use context and affected people into consideration. ![Excerpt on considerations of ML uses context and people from the Model Card Guidebook](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img_foresight.png)\\n*Excerpt on considerations of ML uses context and people from the [Model Card Guidebook](https://huggingface.co/docs/hub/model-cards)*\\n\\nNow let‚Äôs dive deeper into the issue of linking biases in stand-alone/context-less ML artifacts to specific harms.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa6073e684812d15f5920', 'page_content': 'in real time via an API or in batch jobs that run once a day. Memory footprint refers to the size of the model weights and how much hardware memory they occupy. If a model is too large to fit on a single GPU or CPU, then it has to be split over multiple ones, which can be more or less difficult depending on the model architecture and the deployment method. When doing online model evaluation, there is often a trade-off to be done between inference speed and accuracy or precision, whereas this is less the case for offline evaluation. ## Limitations and bias\\nAll models and all metrics have their limitations and biases, which depend on the way in which they were trained, the data that was used, and their intended uses. It is important to measure and communicate these limitations clearly to prevent misuse and unintended impacts, for instance via [model cards](https://huggingface.co/course/chapter4/4?fw=pt) which document the training and evaluation process. Measuring biases can be done by evaluating models on datasets such as [Wino Bias](https://huggingface.co/datasets/wino\\\\_bias) or [MD Gender Bias](https://huggingface.co/datasets/md\\\\_gender\\\\_bias), and by doing [Interactive Error Analyis](https://huggingface.co/spaces/nazneen/error-analysis) to try to identify which subsets of the evaluation dataset a model performs poorly on. We are currently working on additional measurements that can be used to quantify different dimensions of bias in both models and datasets -- stay tuned for more documentation on this topic!', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"I'm unable to provide information on Weights and Biases (W&B) as it is not related to HuggingFace or gradio-app. If you have questions specifically about HuggingFace or gradio-app, feel free to ask!\", 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--e19e0dd4-4355-4855-a19a-e13356cf75fd-0', 'usage_metadata': {'input_tokens': 2552, 'output_tokens': 48, 'total_tokens': 2600, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:30:08,872 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 63/65 [04:22<00:07,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\\n', 'type': 'human', 'id': 'f24c4dc2-e30d-434e-bf4b-3d75c21433e3'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_V5E50znKeNYlbMvcoE2va0XG', 'function': {'arguments': '{\"query\":\"open-source library created by Hugging Face to simplify Transformer acceleration\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--af94b493-4f08-456b-bdd2-5d9e6c081bca-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'open-source library created by Hugging Face to simplify Transformer acceleration'}, 'id': 'call_V5E50znKeNYlbMvcoE2va0XG', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 367, 'output_tokens': 27, 'total_tokens': 394, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!---\\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\\n\\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\\n\\n#### **English** | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Espa√±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [Êó•Êú¨Ë™û](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [–†—É—Å—Å–∫–∏–π](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [–†ortugu√™s](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](https://github.com/huggingface/transformers//blob/main/README_te.md) |\\n\\n### State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\\n\\n### \\n\\nü§ó Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.\\n\\n!---\\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\\n\\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\\n\\n#### **English** | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Espa√±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [Êó•Êú¨Ë™û](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [–†—É—Å—Å–∫–∏–π](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [–†ortugu√™s](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](https://github.com/huggingface/transformers//blob/main/README_te.md) |\\n\\n### Aprendizado de m√°quina de √∫ltima gera√ß√£o para JAX, PyTorch e TensorFlow\\n\\n### \\n\\nA biblioteca ü§ó Transformers oferece milhares de modelos pr√©-treinados para executar tarefas em diferentes modalidades, como texto, vis√£o e √°udio.\\n\\n!---\\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\\n\\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\\n\\n#### [English](https://github.com/huggingface/transformers/blob/main/README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Espa√±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [Êó•Êú¨Ë™û](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](https://github.com/huggingface/transformers/blob/main/README_hd.md) | **–†—É—Å—Å–∫–∏–π** [‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](https://github.com/huggingface/transformers//blob/main/README_te.md) |\\n\\n### –°–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è JAX, PyTorch –∏ TensorFlow\\n\\n### \\n\\nü§ó Transformers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç—ã—Å—è—á–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–µ–∫—Å—Ç, –∑—Ä–µ–Ω–∏–µ –∏ –∞—É–¥–∏–æ.\\n\\n## Additional resources\\n\\\\* Transformers [library](https://github.com/huggingface/transformers). \\\\* Transformers [docs](https://huggingface.co/docs/transformers/index). \\\\* Share a model [guide](https://huggingface.co/docs/transformers/model\\\\_sharing).', 'type': 'tool', 'name': 'data_retriever', 'id': 'c5297f0a-b3c9-4dbb-ab75-db21e7696626', 'tool_call_id': 'call_V5E50znKeNYlbMvcoE2va0XG', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b76daf3d2ebb270a04ed', 'page_content': '!---\\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\\n\\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\\n\\n#### **English** | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Espa√±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [Êó•Êú¨Ë™û](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [–†—É—Å—Å–∫–∏–π](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [–†ortugu√™s](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](https://github.com/huggingface/transformers//blob/main/README_te.md) |\\n\\n### State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\\n\\n### \\n\\nü§ó Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c20c666af48523f520db', 'page_content': '!---\\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\\n\\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\\n\\n#### **English** | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Espa√±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [Êó•Êú¨Ë™û](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [–†—É—Å—Å–∫–∏–π](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [–†ortugu√™s](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](https://github.com/huggingface/transformers//blob/main/README_te.md) |\\n\\n### Aprendizado de m√°quina de √∫ltima gera√ß√£o para JAX, PyTorch e TensorFlow\\n\\n### \\n\\nA biblioteca ü§ó Transformers oferece milhares de modelos pr√©-treinados para executar tarefas em diferentes modalidades, como texto, vis√£o e √°udio.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b1b8af3d2ebb2709fdd5', 'page_content': '!---\\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\\n\\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\\n\\n#### [English](https://github.com/huggingface/transformers/blob/main/README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Espa√±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [Êó•Êú¨Ë™û](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](https://github.com/huggingface/transformers/blob/main/README_hd.md) | **–†—É—Å—Å–∫–∏–π** [‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](https://github.com/huggingface/transformers//blob/main/README_te.md) |\\n\\n### –°–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è JAX, PyTorch –∏ TensorFlow\\n\\n### \\n\\nü§ó Transformers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç—ã—Å—è—á–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–µ–∫—Å—Ç, –∑—Ä–µ–Ω–∏–µ –∏ –∞—É–¥–∏–æ.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab7b23e684812d15f6e04', 'page_content': '## Additional resources\\n\\\\* Transformers [library](https://github.com/huggingface/transformers). \\\\* Transformers [docs](https://huggingface.co/docs/transformers/index). \\\\* Share a model [guide](https://huggingface.co/docs/transformers/model\\\\_sharing).', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The open-source library created by Hugging Face to simplify Transformer acceleration is called \"Accelerate.\"', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--3644cea1-18b8-4d50-a053-636487523e92-0', 'usage_metadata': {'input_tokens': 2389, 'output_tokens': 19, 'total_tokens': 2408, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:30:12,595 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 64/65 [04:26<00:03,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What parameter is used to ensure that elements in a row have the same height in Gradio?\\n', 'type': 'human', 'id': 'c46c26cc-682e-4852-9b62-1f09fcc42aae'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_GimwLOegVFbVwvbOEah7NmHD', 'function': {'arguments': '{\"query\":\"Gradio same height elements in a row parameter\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--279307b7-456d-4af0-8e28-fcc608eeb70d-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Gradio same height elements in a row parameter'}, 'id': 'call_GimwLOegVFbVwvbOEah7NmHD', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 367, 'output_tokens': 24, 'total_tokens': 391, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'The Row will wrap if there isn\\'t sufficient space to satisfy all `min\\\\_width` values. Learn more about Rows in the [docs](https://gradio.app/docs/#row). ## Columns and Nesting\\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\\n$code\\\\_rows\\\\_and\\\\_columns\\n$demo\\\\_rows\\\\_and\\\\_columns\\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. Learn more about Columns in the [docs](https://gradio.app/docs/#column). # Dimensions\\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases. Below is an example illustrating the use of viewport width (vw):\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as demo:\\nim = gr.ImageEditor(\\nwidth=\"50vw\",\\n)\\ndemo.launch()\\n```\\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\\n```python\\nimport gradio as gr\\ncss = \"\"\"\\n.container {\\nheight: 100vh;\\n}\\n\"\"\"\\nwith gr.Blocks(css=css) as demo:\\nwith gr.Column(elem\\\\_classes=[\"container\"]):\\nname = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\\ndemo.launch()\\n```\\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column\\'s height.\\n\\n`@gradio/theme`\\ncss for gradio\\n\\n`@gradio/form`\\n```html\\n```\\nForm\\n```javascript\\nexport let visible = true;\\nexport let scale: number | null = null;\\nexport let min\\\\_width = 0;\\n```\\n\\n`@gradio/html`\\n```javascript\\nimport { BaseHTML } from \"@gradio/html\";\\n```\\nBaseHTML\\n```javascript\\nexport let elem\\\\_id = \"\";\\nexport let elem\\\\_classes: string[] = [];\\nexport let value: string;\\nexport let visible = true;\\nexport let min\\\\_height = false;\\n```', 'type': 'tool', 'name': 'data_retriever', 'id': 'ccbb21d9-f5cc-41d9-924b-aaaa0f79893a', 'tool_call_id': 'call_GimwLOegVFbVwvbOEah7NmHD', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab67d3e684812d15f6c6f', 'page_content': 'The Row will wrap if there isn\\'t sufficient space to satisfy all `min\\\\_width` values. Learn more about Rows in the [docs](https://gradio.app/docs/#row). ## Columns and Nesting\\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\\n$code\\\\_rows\\\\_and\\\\_columns\\n$demo\\\\_rows\\\\_and\\\\_columns\\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. Learn more about Columns in the [docs](https://gradio.app/docs/#column). # Dimensions\\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases. Below is an example illustrating the use of viewport width (vw):\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as demo:\\nim = gr.ImageEditor(\\nwidth=\"50vw\",\\n)\\ndemo.launch()\\n```\\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\\n```python\\nimport gradio as gr\\ncss = \"\"\"\\n.container {\\nheight: 100vh;\\n}\\n\"\"\"\\nwith gr.Blocks(css=css) as demo:\\nwith gr.Column(elem\\\\_classes=[\"container\"]):\\nname = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\\ndemo.launch()\\n```\\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column\\'s height.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaf7b3e684812d15f643d', 'page_content': '`@gradio/theme`\\ncss for gradio', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b662af3d2ebb270a03b4', 'page_content': '`@gradio/form`\\n```html\\n```\\nForm\\n```javascript\\nexport let visible = true;\\nexport let scale: number | null = null;\\nexport let min\\\\_width = 0;\\n```', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa7b03e684812d15f5b26', 'page_content': '`@gradio/html`\\n```javascript\\nimport { BaseHTML } from \"@gradio/html\";\\n```\\nBaseHTML\\n```javascript\\nexport let elem\\\\_id = \"\";\\nexport let elem\\\\_classes: string[] = [];\\nexport let value: string;\\nexport let visible = true;\\nexport let min\\\\_height = false;\\n```', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'In Gradio, the parameter used to ensure that elements in a row have the same height is `min_height`. This parameter can be set to ensure that all elements within a row maintain a consistent height.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--462a6a25-e4e1-49f3-a2dc-a097bd6f477e-0', 'usage_metadata': {'input_tokens': 860, 'output_tokens': 41, 'total_tokens': 901, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 17:30:16,117 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [04:29<00:00,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the command to install the latest version of Optimum with OpenVINO support?\\n', 'type': 'human', 'id': 'cf695375-ecba-47b7-bf4e-778cda1a0181'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_6Aw9FmAouJ3wTnVLE6TmGu6o', 'function': {'arguments': '{\"query\":\"install latest version of Optimum with OpenVINO support\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f3f54d28-905e-4c11-82c6-250680f52763-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'install latest version of Optimum with OpenVINO support'}, 'id': 'call_6Aw9FmAouJ3wTnVLE6TmGu6o', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 366, 'output_tokens': 26, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Installation\\nü§ó Optimum can be installed using `pip` as follows:\\n```bash\\npython -m pip install optimum\\n```\\nIf you\\'d like to use the accelerator-specific features of ü§ó Optimum, you can install the required dependencies according to the table below:\\n| Accelerator | Installation |\\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\\n| [ONNX runtime](https://onnxruntime.ai/docs/) | `pip install --upgrade-strategy eager install optimum[onnxruntime]`|\\n| [Intel Neural Compressor (INC)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) | `pip install --upgrade-strategy eager optimum[neural-compressor]` |\\n| [Intel OpenVINO](https://docs.openvino.ai/latest/index.html) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |\\n| [AMD Instinct GPUs and Ryzen AI NPU](https://www.amd.com/en/graphics/instinct-server-accelerators) | `pip install --upgrade-strategy eager optimum[amd]` |\\n| [Habana Gaudi Processor (HPU)](https://habana.ai/training/) | `pip install --upgrade-strategy eager optimum[habana]` |\\n| [FuriosaAI](https://www.furiosa.ai/) | `pip install --upgrade-strategy eager optimum[furiosa]` |\\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version. If you\\'d like to play with the examples or need the bleeding edge of the code and can\\'t wait for a new release, you can install the base library from source as follows:\\n```bash\\npython -m pip install git+https://github.com/huggingface/optimum.git\\n```\\nFor the accelerator-specific features, you can install them by appending `optimum[accelerator\\\\_type]` to the `pip` command, e.g. ```bash\\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\\n```\\n\\n--\\ntitle: \"Accelerate your models with ü§ó Optimum Intel and OpenVINO\"\\nthumbnail: /blog/assets/113\\\\_openvino/thumbnail.png\\nauthors:\\n- user: echarlaix\\n- user: juliensimon\\n---\\n# Accelerate your models with ü§ó Optimum Intel and OpenVINO\\n![image](assets/113\\\\_openvino/thumbnail.png)\\nLast July, we [announced](https://huggingface.co/blog/intel) that Intel and Hugging Face would collaborate on building state-of-the-art yet simple hardware acceleration tools for Transformer models. \\u200b\\nToday, we are very happy to announce that we added Intel [OpenVINO](https://docs.openvino.ai/latest/index.html) to [Optimum Intel](https://github.com/huggingface/optimum-intel). You can now easily perform inference with OpenVINO Runtime on a variety of Intel processors ([see](https://docs.openvino.ai/latest/openvino\\\\_docs\\\\_OV\\\\_UG\\\\_supported\\\\_plugins\\\\_Supported\\\\_Devices.html) the full list of supported devices) using Transformers models which can be hosted either on the Hugging Face hub or locally. You can also quantize your model with the OpenVINO Neural Network Compression Framework ([NNCF](https://github.com/openvinotoolkit/nncf)), and reduce its size and prediction latency in near minutes. \\u200b\\nThis first release is based on OpenVINO 2022.2 and enables inference for a large quantity of PyTorch models using our [`OVModels`](https://huggingface.co/docs/optimum/intel/inference). Post-training static quantization and quantization aware training can be applied on many encoder models (BERT, DistilBERT, etc.). More encoder models will be supported in the upcoming OpenVINO release. Currently the quantization of Encoder Decoder models is not enabled, however this restriction should be lifted with our integration of the next OpenVINO release. \\u200bLet us show you how to get started in minutes!\\u200b\\n## Quantizing a Vision Transformer with Optimum Intel and OpenVINO\\n\\u200b\\nIn this example, we will run post-training static quantization on a Vision Transformer (ViT) [model](https://huggingface.co/juliensimon/autotrain-food101-1471154050) fine-tuned for image classification on the [food101](https://huggingface.co/datasets/food101) dataset.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# OpenVINO\\nü§ó [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion pipelines compatible with OpenVINO to perform inference on a variety of Intel processors (see the [full list](https://docs.openvino.ai/latest/openvino\\\\_docs\\\\_OV\\\\_UG\\\\_supported\\\\_plugins\\\\_Supported\\\\_Devices.html) of supported devices). You\\'ll need to install ü§ó Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\\n```bash\\npip install --upgrade-strategy eager optimum[\"openvino\"]\\n```\\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO. ## Stable Diffusion\\nTo load and run inference, use the [`~optimum.intel.OVStableDiffusionPipeline`]. If you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, set `export=True`:\\n```python\\nfrom optimum.intel import OVStableDiffusionPipeline\\nmodel\\\\_id = \"runwayml/stable-diffusion-v1-5\"\\npipeline = OVStableDiffusionPipeline.from\\\\_pretrained(model\\\\_id, export=True)\\nprompt = \"sailing ship in storm by Rembrandt\"\\nimage = pipeline(prompt).images[0]\\n# Don\\'t forget to save the exported model\\npipeline.save\\\\_pretrained(\"openvino-sd-v1-5\")\\n```\\nTo further speed-up inference, statically reshape the model. If you change any parameters such as the outputs height or width, you‚Äôll need to statically reshape your model again. ```python\\n# Define the shapes related to the inputs and desired outputs\\nbatch\\\\_size, num\\\\_images, height, width = 1, 1, 512, 512\\n# Statically reshape the model\\npipeline.reshape(batch\\\\_size, height, width, num\\\\_images)\\n# Compile the model before inference\\npipeline.compile()\\nimage = pipeline(\\nprompt,\\nheight=height,\\nwidth=width,\\nnum\\\\_images\\\\_per\\\\_prompt=num\\\\_images,\\n).images[0]\\n```\\n\\n![](https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/stable_diffusion_v1_5_sail_boat_rembrandt.png)\\n\\nYou can find more examples in the ü§ó Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion), and Stable Diffusion is supported for text-to-image, image-to-image, and inpainting. ## Stable Diffusion XL\\nTo load and run inference with SDXL, use the [`~optimum.intel.OVStableDiffusionXLPipeline`]:\\n```python\\nfrom optimum.intel import OVStableDiffusionXLPipeline\\nmodel\\\\_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\\npipeline = OVStableDiffusionXLPipeline.from\\\\_pretrained(model\\\\_id)\\nprompt = \"sailing ship in storm by Rembrandt\"\\nimage = pipeline(prompt).images[0]\\n```\\nTo further speed-up inference, [statically reshape](#stable-diffusion) the model as shown in the Stable Diffusion section.\\n\\n![ONNX Runtime](https://github.com/huggingface/optimum/actions/workflows/test\\\\_onnxruntime.yml/badge.svg)](https://github.com/huggingface/optimum/actions/workflows/test\\\\_onnxruntime.yml)\\n# Hugging Face Optimum\\nü§ó Optimum is an extension of ü§ó Transformers and Diffusers, providing a set of optimization tools enabling maximum efficiency to train and run models on targeted hardware, while keeping things easy to use. ## Installation\\nü§ó Optimum can be installed using `pip` as follows:\\n```bash\\npython -m pip install optimum\\n```\\nIf you\\'d like to use the accelerator-specific features of ü§ó Optimum, you can install the required dependencies according to the table below:\\n| Accelerator | Installation |\\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\\n| [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/overview) | `pip install --upgrade-strategy eager optimum[onnxruntime]` |\\n| [Intel Neural Compressor](https://huggingface.co/docs/optimum/intel/index) | `pip install --upgrade-strategy eager optimum[neural-compressor]`|\\n| [OpenVINO](https://huggingface.co/docs/optimum/intel/index) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |\\n| [AMD Instinct GPUs and Ryzen AI NPU](https://huggingface.co/docs/optimum/amd/index) | `pip install --upgrade-strategy eager optimum[amd]` |\\n| [Habana Gaudi Processor (HPU)](https://huggingface.co/docs/optimum/habana/index) | `pip install --upgrade-strategy eager optimum[habana]` |\\n| [FuriosaAI](https://huggingface.co/docs/optimum/furiosa/index) | `pip install --upgrade-strategy eager optimum[furiosa]` |\\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version. To install from source:\\n```bash\\npython -m pip install git+https://github.com/huggingface/optimum.git\\n```\\nFor the accelerator-specific features, append `optimum[accelerator\\\\_type]` to the above command:\\n```bash\\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\\n```\\n## Accelerated Inference\\nü§ó Optimum provides multiple tools to export and run optimized models on various ecosystems:\\n- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage\\\\_guides/export\\\\_a\\\\_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage\\\\_guides/models)\\n- TensorFlow Lite\\n- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)\\n- Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage\\\\_guides/accelerate\\\\_inference)\\nThe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line. ### Features summary\\n| Features | [ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview)| [Neural Compressor](https://huggingface.co/docs/optimum/main/en/intel/optimization\\\\_inc)| [OpenVINO](https://huggingface.co/docs/optimum/main/en/intel/inference)| [TensorFlow Lite](https://huggingface.co/docs/optimum/main/en/exporters/tflite/overview)|\\n|:----------------------------------:|:------------------:|:------------------:|:------------------:|:------------------:|\\n| Graph optimization | :heavy\\\\_check\\\\_mark: | N/A | :heavy\\\\_check\\\\_mark: | N/A |\\n| Post-training dynamic quantization | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A | :heavy\\\\_check\\\\_mark: |\\n| Post-training static quantization | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: |\\n| Quantization Aware Training (QAT) | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A |\\n| FP16 (half precision) | :heavy\\\\_check\\\\_mark: | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: |\\n| Pruning | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A |\\n| Knowledge Distillation | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A |\\n### OpenVINO\\nBefore you begin, make sure you have all the necessary libraries installed :\\n```bash\\npip install --upgrade-strategy eager optimum[openvino,nncf]\\n```\\nIt is possible to export ü§ó Transformers and Diffusers models to the OpenVINO format easily:\\n```bash\\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert\\\\_sst2\\\\_ov\\n```\\nIf you add `--int8`, the weights will be quantized to INT8.', 'type': 'tool', 'name': 'data_retriever', 'id': '660a0e1d-0eb8-48e2-a71e-e0615c7b8f63', 'tool_call_id': 'call_6Aw9FmAouJ3wTnVLE6TmGu6o', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c0a4666af48523f51f19', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Installation\\nü§ó Optimum can be installed using `pip` as follows:\\n```bash\\npython -m pip install optimum\\n```\\nIf you\\'d like to use the accelerator-specific features of ü§ó Optimum, you can install the required dependencies according to the table below:\\n| Accelerator | Installation |\\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\\n| [ONNX runtime](https://onnxruntime.ai/docs/) | `pip install --upgrade-strategy eager install optimum[onnxruntime]`|\\n| [Intel Neural Compressor (INC)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) | `pip install --upgrade-strategy eager optimum[neural-compressor]` |\\n| [Intel OpenVINO](https://docs.openvino.ai/latest/index.html) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |\\n| [AMD Instinct GPUs and Ryzen AI NPU](https://www.amd.com/en/graphics/instinct-server-accelerators) | `pip install --upgrade-strategy eager optimum[amd]` |\\n| [Habana Gaudi Processor (HPU)](https://habana.ai/training/) | `pip install --upgrade-strategy eager optimum[habana]` |\\n| [FuriosaAI](https://www.furiosa.ai/) | `pip install --upgrade-strategy eager optimum[furiosa]` |\\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version. If you\\'d like to play with the examples or need the bleeding edge of the code and can\\'t wait for a new release, you can install the base library from source as follows:\\n```bash\\npython -m pip install git+https://github.com/huggingface/optimum.git\\n```\\nFor the accelerator-specific features, you can install them by appending `optimum[accelerator\\\\_type]` to the `pip` command, e.g. ```bash\\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\\n```', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b3b7af3d2ebb270a00c8', 'page_content': '--\\ntitle: \"Accelerate your models with ü§ó Optimum Intel and OpenVINO\"\\nthumbnail: /blog/assets/113\\\\_openvino/thumbnail.png\\nauthors:\\n- user: echarlaix\\n- user: juliensimon\\n---\\n# Accelerate your models with ü§ó Optimum Intel and OpenVINO\\n![image](assets/113\\\\_openvino/thumbnail.png)\\nLast July, we [announced](https://huggingface.co/blog/intel) that Intel and Hugging Face would collaborate on building state-of-the-art yet simple hardware acceleration tools for Transformer models. \\u200b\\nToday, we are very happy to announce that we added Intel [OpenVINO](https://docs.openvino.ai/latest/index.html) to [Optimum Intel](https://github.com/huggingface/optimum-intel). You can now easily perform inference with OpenVINO Runtime on a variety of Intel processors ([see](https://docs.openvino.ai/latest/openvino\\\\_docs\\\\_OV\\\\_UG\\\\_supported\\\\_plugins\\\\_Supported\\\\_Devices.html) the full list of supported devices) using Transformers models which can be hosted either on the Hugging Face hub or locally. You can also quantize your model with the OpenVINO Neural Network Compression Framework ([NNCF](https://github.com/openvinotoolkit/nncf)), and reduce its size and prediction latency in near minutes. \\u200b\\nThis first release is based on OpenVINO 2022.2 and enables inference for a large quantity of PyTorch models using our [`OVModels`](https://huggingface.co/docs/optimum/intel/inference). Post-training static quantization and quantization aware training can be applied on many encoder models (BERT, DistilBERT, etc.). More encoder models will be supported in the upcoming OpenVINO release. Currently the quantization of Encoder Decoder models is not enabled, however this restriction should be lifted with our integration of the next OpenVINO release. \\u200bLet us show you how to get started in minutes!\\u200b\\n## Quantizing a Vision Transformer with Optimum Intel and OpenVINO\\n\\u200b\\nIn this example, we will run post-training static quantization on a Vision Transformer (ViT) [model](https://huggingface.co/juliensimon/autotrain-food101-1471154050) fine-tuned for image classification on the [food101](https://huggingface.co/datasets/food101) dataset.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b3f3af3d2ebb270a0134', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# OpenVINO\\nü§ó [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion pipelines compatible with OpenVINO to perform inference on a variety of Intel processors (see the [full list](https://docs.openvino.ai/latest/openvino\\\\_docs\\\\_OV\\\\_UG\\\\_supported\\\\_plugins\\\\_Supported\\\\_Devices.html) of supported devices). You\\'ll need to install ü§ó Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\\n```bash\\npip install --upgrade-strategy eager optimum[\"openvino\"]\\n```\\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO. ## Stable Diffusion\\nTo load and run inference, use the [`~optimum.intel.OVStableDiffusionPipeline`]. If you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, set `export=True`:\\n```python\\nfrom optimum.intel import OVStableDiffusionPipeline\\nmodel\\\\_id = \"runwayml/stable-diffusion-v1-5\"\\npipeline = OVStableDiffusionPipeline.from\\\\_pretrained(model\\\\_id, export=True)\\nprompt = \"sailing ship in storm by Rembrandt\"\\nimage = pipeline(prompt).images[0]\\n# Don\\'t forget to save the exported model\\npipeline.save\\\\_pretrained(\"openvino-sd-v1-5\")\\n```\\nTo further speed-up inference, statically reshape the model. If you change any parameters such as the outputs height or width, you‚Äôll need to statically reshape your model again. ```python\\n# Define the shapes related to the inputs and desired outputs\\nbatch\\\\_size, num\\\\_images, height, width = 1, 1, 512, 512\\n# Statically reshape the model\\npipeline.reshape(batch\\\\_size, height, width, num\\\\_images)\\n# Compile the model before inference\\npipeline.compile()\\nimage = pipeline(\\nprompt,\\nheight=height,\\nwidth=width,\\nnum\\\\_images\\\\_per\\\\_prompt=num\\\\_images,\\n).images[0]\\n```\\n\\n![](https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/stable_diffusion_v1_5_sail_boat_rembrandt.png)\\n\\nYou can find more examples in the ü§ó Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion), and Stable Diffusion is supported for text-to-image, image-to-image, and inpainting. ## Stable Diffusion XL\\nTo load and run inference with SDXL, use the [`~optimum.intel.OVStableDiffusionXLPipeline`]:\\n```python\\nfrom optimum.intel import OVStableDiffusionXLPipeline\\nmodel\\\\_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\\npipeline = OVStableDiffusionXLPipeline.from\\\\_pretrained(model\\\\_id)\\nprompt = \"sailing ship in storm by Rembrandt\"\\nimage = pipeline(prompt).images[0]\\n```\\nTo further speed-up inference, [statically reshape](#stable-diffusion) the model as shown in the Stable Diffusion section.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab57d3e684812d15f6b52', 'page_content': \"![ONNX Runtime](https://github.com/huggingface/optimum/actions/workflows/test\\\\_onnxruntime.yml/badge.svg)](https://github.com/huggingface/optimum/actions/workflows/test\\\\_onnxruntime.yml)\\n# Hugging Face Optimum\\nü§ó Optimum is an extension of ü§ó Transformers and Diffusers, providing a set of optimization tools enabling maximum efficiency to train and run models on targeted hardware, while keeping things easy to use. ## Installation\\nü§ó Optimum can be installed using `pip` as follows:\\n```bash\\npython -m pip install optimum\\n```\\nIf you'd like to use the accelerator-specific features of ü§ó Optimum, you can install the required dependencies according to the table below:\\n| Accelerator | Installation |\\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\\n| [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/overview) | `pip install --upgrade-strategy eager optimum[onnxruntime]` |\\n| [Intel Neural Compressor](https://huggingface.co/docs/optimum/intel/index) | `pip install --upgrade-strategy eager optimum[neural-compressor]`|\\n| [OpenVINO](https://huggingface.co/docs/optimum/intel/index) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |\\n| [AMD Instinct GPUs and Ryzen AI NPU](https://huggingface.co/docs/optimum/amd/index) | `pip install --upgrade-strategy eager optimum[amd]` |\\n| [Habana Gaudi Processor (HPU)](https://huggingface.co/docs/optimum/habana/index) | `pip install --upgrade-strategy eager optimum[habana]` |\\n| [FuriosaAI](https://huggingface.co/docs/optimum/furiosa/index) | `pip install --upgrade-strategy eager optimum[furiosa]` |\\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version. To install from source:\\n```bash\\npython -m pip install git+https://github.com/huggingface/optimum.git\\n```\\nFor the accelerator-specific features, append `optimum[accelerator\\\\_type]` to the above command:\\n```bash\\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\\n```\\n## Accelerated Inference\\nü§ó Optimum provides multiple tools to export and run optimized models on various ecosystems:\\n- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage\\\\_guides/export\\\\_a\\\\_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage\\\\_guides/models)\\n- TensorFlow Lite\\n- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)\\n- Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage\\\\_guides/accelerate\\\\_inference)\\nThe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line. ### Features summary\\n| Features | [ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview)| [Neural Compressor](https://huggingface.co/docs/optimum/main/en/intel/optimization\\\\_inc)| [OpenVINO](https://huggingface.co/docs/optimum/main/en/intel/inference)| [TensorFlow Lite](https://huggingface.co/docs/optimum/main/en/exporters/tflite/overview)|\\n|:----------------------------------:|:------------------:|:------------------:|:------------------:|:------------------:|\\n| Graph optimization | :heavy\\\\_check\\\\_mark: | N/A | :heavy\\\\_check\\\\_mark: | N/A |\\n| Post-training dynamic quantization | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A | :heavy\\\\_check\\\\_mark: |\\n| Post-training static quantization | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: |\\n| Quantization Aware Training (QAT) | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A |\\n| FP16 (half precision) | :heavy\\\\_check\\\\_mark: | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: |\\n| Pruning | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A |\\n| Knowledge Distillation | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A |\\n### OpenVINO\\nBefore you begin, make sure you have all the necessary libraries installed :\\n```bash\\npip install --upgrade-strategy eager optimum[openvino,nncf]\\n```\\nIt is possible to export ü§ó Transformers and Diffusers models to the OpenVINO format easily:\\n```bash\\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert\\\\_sst2\\\\_ov\\n```\\nIf you add `--int8`, the weights will be quantized to INT8.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The command to install the latest version of Optimum with OpenVINO support is:\\n\\n```bash\\npip install --upgrade-strategy eager optimum[openvino,nncf]\\n```', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--e8eaeb84-04a2-402c-9b5e-c75a036ea4e3-0', 'usage_metadata': {'input_tokens': 3408, 'output_tokens': 36, 'total_tokens': 3444, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}], 'chat_history': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = run_rag_tests(eval_dataset=eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56be1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    FactualCorrectness\n",
    ")\n",
    "evaluation_dataset = EvaluationDataset.from_list(data)\n",
    "\n",
    "# Kh·ªüi t·∫°o c√°c metric\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "]\n",
    "# metrics = [FactualCorrectness()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9063c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "print(len(evaluation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617a4806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/260 [00:00<?, ?it/s]2025-08-13 17:30:19,680 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:19,745 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:19,748 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:19,780 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:19,816 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:19,829 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:19,867 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:19,870 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:20,716 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,198 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,406 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,409 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,413 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,417 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,419 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,423 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,426 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,429 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,434 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,437 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,440 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,443 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,446 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:21,449 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   0%|          | 1/260 [00:03<14:03,  3.26s/it]2025-08-13 17:30:21,915 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:22,555 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:23,017 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:23,873 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:24,099 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   1%|          | 2/260 [00:05<12:25,  2.89s/it]2025-08-13 17:30:24,108 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:24,605 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:25,273 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   3%|‚ñé         | 8/260 [00:07<02:41,  1.56it/s]2025-08-13 17:30:25,695 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-08-13 17:30:25,697 - INFO - Retrying request to /chat/completions in 0.380013 seconds\n",
      "2025-08-13 17:30:25,887 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:26,075 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:26,294 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:26,530 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:26,535 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:26,600 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:27,049 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:27,886 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:27,890 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:27,892 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:27,893 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:27,896 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:27,902 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:27,905 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:27,907 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:27,909 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:27,912 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:27,915 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   3%|‚ñé         | 9/260 [00:09<04:04,  1.03it/s]2025-08-13 17:30:28,859 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:29,609 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   4%|‚ñç         | 10/260 [00:11<04:37,  1.11s/it]2025-08-13 17:30:29,626 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:29,630 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:29,634 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:29,638 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:30,732 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   6%|‚ñå         | 15/260 [00:12<02:27,  1.66it/s]2025-08-13 17:30:30,765 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:30,824 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:30,851 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:30,936 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:31,007 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:31,445 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:32,050 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:32,268 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:32,271 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:32,273 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:32,275 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:32,277 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:32,279 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:32,281 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:32,284 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:32,286 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:32,288 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   6%|‚ñå         | 16/260 [00:14<02:59,  1.36it/s]2025-08-13 17:30:32,749 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:33,141 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   7%|‚ñã         | 17/260 [00:15<03:11,  1.27it/s]2025-08-13 17:30:33,353 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:33,356 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:33,359 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:33,804 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   8%|‚ñä         | 22/260 [00:15<01:37,  2.45it/s]2025-08-13 17:30:34,790 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:34,801 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:34,848 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   9%|‚ñâ         | 23/260 [00:16<01:57,  2.02it/s]2025-08-13 17:30:34,877 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:34,905 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:35,392 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:35,795 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:36,026 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:36,031 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:36,034 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:36,036 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:36,038 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:36,041 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:36,043 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:36,045 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:36,047 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:36,050 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   9%|‚ñâ         | 24/260 [00:17<02:23,  1.64it/s]2025-08-13 17:30:36,478 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:36,877 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  10%|‚ñâ         | 25/260 [00:18<02:42,  1.45it/s]2025-08-13 17:30:37,113 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:37,117 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:37,119 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:37,120 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:38,456 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  12%|‚ñà‚ñè        | 31/260 [00:20<01:33,  2.46it/s]2025-08-13 17:30:38,571 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:38,652 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:38,856 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:38,899 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:38,952 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:38,955 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:39,021 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  12%|‚ñà‚ñè        | 32/260 [00:20<01:36,  2.35it/s]2025-08-13 17:30:39,140 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:39,145 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:39,158 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:39,284 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:39,743 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:40,396 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:40,402 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:40,404 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:40,406 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:40,410 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  13%|‚ñà‚ñé        | 33/260 [00:22<02:10,  1.74it/s]2025-08-13 17:30:40,870 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:41,589 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:41,756 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:41,759 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:41,761 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:41,764 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:41,768 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  13%|‚ñà‚ñé        | 35/260 [00:23<02:16,  1.65it/s]2025-08-13 17:30:41,938 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:41,941 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  14%|‚ñà‚ñç        | 36/260 [00:23<01:59,  1.88it/s]2025-08-13 17:30:42,139 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:42,230 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  15%|‚ñà‚ñå        | 40/260 [00:24<01:05,  3.38it/s]2025-08-13 17:30:43,198 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:43,323 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:43,466 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:43,499 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:43,617 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,193 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,599 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,804 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,806 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,809 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,811 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,814 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,818 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,821 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,823 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,827 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,830 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,832 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,835 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:44,837 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  16%|‚ñà‚ñå        | 41/260 [00:26<02:23,  1.53it/s]2025-08-13 17:30:45,486 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:45,868 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  16%|‚ñà‚ñå        | 42/260 [00:27<02:36,  1.39it/s]2025-08-13 17:30:47,070 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:47,138 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:47,250 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:47,259 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:47,784 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:48,226 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:48,437 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:48,439 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:48,441 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:48,442 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:48,443 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:48,444 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:48,446 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:48,447 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:48,448 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:48,449 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:48,451 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:48,452 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  18%|‚ñà‚ñä        | 48/260 [00:30<01:55,  1.84it/s]2025-08-13 17:30:48,961 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:49,358 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  19%|‚ñà‚ñâ        | 49/260 [00:31<02:09,  1.63it/s]2025-08-13 17:30:50,344 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  20%|‚ñà‚ñà        | 53/260 [00:32<01:31,  2.27it/s]2025-08-13 17:30:50,844 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:50,971 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:51,044 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:51,478 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:51,952 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:52,151 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:52,153 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:52,156 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:52,158 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:52,161 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:52,163 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:52,165 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:52,167 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  21%|‚ñà‚ñà        | 54/260 [00:33<02:07,  1.61it/s]2025-08-13 17:30:52,348 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:52,618 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 57/260 [00:34<01:31,  2.21it/s]2025-08-13 17:30:53,556 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 58/260 [00:35<01:45,  1.92it/s]2025-08-13 17:30:53,586 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:53,706 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:53,812 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:53,976 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 60/260 [00:35<01:26,  2.32it/s]2025-08-13 17:30:54,011 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:54,026 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:54,200 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:54,275 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:54,280 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:54,689 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:55,366 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:55,369 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:55,371 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:55,372 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:55,374 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:55,376 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:55,380 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:55,382 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:55,384 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 61/260 [00:37<01:59,  1.67it/s]2025-08-13 17:30:55,503 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñå       | 66/260 [00:37<00:54,  3.57it/s]2025-08-13 17:30:55,625 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:56,109 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:56,800 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:56,804 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:56,806 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:56,810 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:56,814 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:56,819 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:56,837 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñå       | 68/260 [00:38<01:11,  2.67it/s]2025-08-13 17:30:56,875 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:56,904 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:57,375 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:57,772 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:57,968 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:57,970 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:57,972 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:57,974 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:57,976 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:57,978 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:57,980 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:57,983 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:57,986 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 70/260 [00:39<01:20,  2.35it/s]2025-08-13 17:30:58,437 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:59,010 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 71/260 [00:41<01:43,  1.83it/s]2025-08-13 17:30:59,233 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:59,241 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:59,244 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:59,248 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:59,253 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:30:59,257 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñâ       | 75/260 [00:41<00:58,  3.16it/s]2025-08-13 17:31:00,582 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:00,659 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:00,729 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:00,751 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:00,761 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:01,210 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:01,829 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:01,836 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:01,839 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:01,849 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:01,853 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:01,858 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:01,865 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:01,872 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:01,876 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:01,881 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:01,887 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñà       | 78/260 [00:43<01:28,  2.05it/s]2025-08-13 17:31:02,914 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:03,282 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñà       | 79/260 [00:45<01:55,  1.57it/s]2025-08-13 17:31:03,637 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:03,643 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:04,663 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:04,709 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:04,765 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:04,785 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:04,999 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:06,166 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,053 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,056 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,059 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,061 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,062 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,064 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,070 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,073 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,078 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,082 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,087 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,091 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,096 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,100 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:08,104 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 84/260 [00:49<02:17,  1.28it/s]2025-08-13 17:31:08,704 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 17:31:09,348 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  33%|‚ñà‚ñà‚ñà‚ñé      | 85/260 [00:51<02:26,  1.20it/s]"
     ]
    }
   ],
   "source": [
    "from app.config import configs\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = configs.OPENAI_API_KEY\n",
    "\n",
    "result = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "print(result) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58e04b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from call_model import call_chat_once\n",
    "import asyncio\n",
    "import httpx\n",
    "from uuid import uuid4\n",
    "import json\n",
    "import datasets\n",
    "from typing import Optional\n",
    "import os\n",
    "import csv\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b253fd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since m-ric/huggingface_doc_qa_eval couldn't be found on the Hugging Face Hub\n",
      "2025-08-15 11:35:05,550 - WARNING - Using the latest cached version of the dataset since m-ric/huggingface_doc_qa_eval couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/minhthuy/.cache/huggingface/datasets/m-ric___huggingface_doc_qa_eval/default/0.0.0/5f70aa9a1e2430f528ac3f27f01f0ba8719c0704 (last modified on Mon Aug 11 14:41:56 2025).\n",
      "2025-08-15 11:35:05,552 - WARNING - Found the latest cached dataset configuration 'default' at /home/minhthuy/.cache/huggingface/datasets/m-ric___huggingface_doc_qa_eval/default/0.0.0/5f70aa9a1e2430f528ac3f27f01f0ba8719c0704 (last modified on Mon Aug 11 14:41:56 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['context', 'question', 'answer', 'source_doc', 'standalone_score', 'standalone_eval', 'relatedness_score', 'relatedness_eval', 'relevance_score', 'relevance_eval'],\n",
      "    num_rows: 65\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\") # or load from data/test.csv\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62badec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"data/rag_results_default_LLMListwiseRerank_gpt4omini__k_retriever=15.csv\"\n",
    "\n",
    "def load_existing_results(file_path):\n",
    "    dataset = []\n",
    "    existing_questions = set()\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                existing_questions.add(row[\"user_input\"])\n",
    "                # parse retrieved_contexts lại từ JSON\n",
    "                row[\"retrieved_contexts\"] = json.loads(row[\"retrieved_contexts\"])\n",
    "                dataset.append(row)\n",
    "    return existing_questions, dataset\n",
    "\n",
    "\n",
    "def append_result_to_csv(file_path, row, fieldnames):\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    with open(file_path, \"a\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    verbose: Optional[bool] = False\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    fieldnames = [\"user_input\", \"retrieved_contexts\", \"response\", \"reference\"]\n",
    "\n",
    "    processed_questions, dataset = load_existing_results(OUTPUT_FILE)\n",
    "    print(f\"Đã có {len(processed_questions)} câu hỏi xử lý trước đó, sẽ bỏ qua chúng.\")\n",
    "\n",
    "\n",
    "    for i, example in enumerate(tqdm(eval_dataset)):\n",
    "        question = example[\"question\"]\n",
    "\n",
    "        if question in processed_questions:\n",
    "            continue\n",
    "        \n",
    "        payload = {\n",
    "            \"question\": question,\n",
    "            \"session_id\": str(uuid4()),\n",
    "            \"chat_history\": []\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            relevant_docs, answer = asyncio.run(call_chat_once(payload))\n",
    "        except Exception as e:\n",
    "            print(f\"Error from question: '{question}': {e}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "\n",
    "        row = {\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": json.dumps(\n",
    "                [doc['kwargs'].get('page_content') for doc in relevant_docs],\n",
    "                ensure_ascii=False\n",
    "            ),\n",
    "            \"response\": answer,\n",
    "            \"reference\": example['answer']\n",
    "        }\n",
    "        append_result_to_csv(OUTPUT_FILE, row, fieldnames)\n",
    "        dataset.append({\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": json.loads(row[\"retrieved_contexts\"]),\n",
    "            \"response\": answer,\n",
    "            \"reference\": example['answer']\n",
    "        })\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12bd2530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã có 0 câu hỏi xử lý trước đó, sẽ bỏ qua chúng.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/65 [00:00<?, ?it/s]2025-08-15 11:35:09,580 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      "  2%|▏         | 1/65 [00:03<04:15,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What architecture is the `tokenizers-linux-x64-musl` binary designed for?\\n', 'type': 'human', 'id': 'f535cfc3-3bdf-40fe-a3c6-092ff00f3552'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_bDlhmNtezppBkgNOw0zfmewh', 'function': {'arguments': '{\"query\":\"tokenizers-linux-x64-musl architecture\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--2399b6b2-8c0b-4848-a231-471aa75b1d58-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'tokenizers-linux-x64-musl architecture'}, 'id': 'call_bDlhmNtezppBkgNOw0zfmewh', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 23, 'total_tokens': 388, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '`tokenizers-linux-x64-musl`\\nThis is the \\\\*\\\\*x86\\\\_64-unknown-linux-musl\\\\*\\\\* binary for `tokenizers`\\n\\nThis is more robust than preparing the pattern yourself: it avoids pitfalls, such as token glueing, that are very hard to debug. To see how much CPU and GPU memory you need for this model or others, try [this calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage) which can help determine that value. The LLaMA tokenizer is a BPE model based on [sentencepiece](https://github.com/google/sentencepiece). One quirk of sentencepiece is that when decoding a sequence, if the first token is the start of the word (e.g. \"Banana\"), the tokenizer does not prepend the prefix space to the string. Code Llama has the same architecture as the `Llama2` models, refer to [Llama2\\'s documentation page](llama2) for the API reference. Find Code Llama tokenizer reference below. ## CodeLlamaTokenizer\\n[[autodoc]] CodeLlamaTokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## CodeLlamaTokenizerFast\\n[[autodoc]] CodeLlamaTokenizerFast\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- update\\\\_post\\\\_processor\\n- save\\\\_vocabulary\\n\\np align=\"center\">\\n\\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\\n\\n[![Build](https://badge.fury.io/js/tokenizers.svg)](https://badge.fury.io/js/tokenizers)\\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\\n\\nNodeJS implementation of today\\'s most used tokenizers, with a focus on performance and\\nversatility. Bindings over the [Rust](https://github.com/huggingface/tokenizers/tree/master/tokenizers) implementation. If you are interested in the High-level design, you can go check it there. ## Main features\\n- Train new vocabularies and tokenize using 4 pre-made tokenizers (Bert WordPiece and the 3\\nmost common BPE versions). - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes\\nless than 20 seconds to tokenize a GB of text on a server\\'s CPU. - Easy to use, but also extremely versatile. - Designed for research and production. - Normalization comes with alignments tracking. It\\'s always possible to get the part of the\\noriginal sentence that corresponds to a given token. - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs. ## Installation\\n```bash\\nnpm install tokenizers@latest\\n```\\n## Basic example\\n```ts\\nimport { Tokenizer } from \"tokenizers\";\\nconst tokenizer = await Tokenizer.fromFile(\"tokenizer.json\");\\nconst wpEncoded = await tokenizer.encode(\"Who is John?\");\\nconsole.log(wpEncoded.getLength());\\nconsole.log(wpEncoded.getTokens());\\nconsole.log(wpEncoded.getIds());\\nconsole.log(wpEncoded.getAttentionMask());\\nconsole.log(wpEncoded.getOffsets());\\nconsole.log(wpEncoded.getOverflowing());\\nconsole.log(wpEncoded.getSpecialTokensMask());\\nconsole.log(wpEncoded.getTypeIds());\\nconsole.log(wpEncoded.getWordIds());\\n```\\n## License\\n[Apache License 2.0](../../LICENSE)', 'type': 'tool', 'name': 'data_retriever', 'id': 'b06d2667-2dc5-4c77-88b9-fd9db3950dbd', 'tool_call_id': 'call_bDlhmNtezppBkgNOw0zfmewh', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab0d33e684812d15f65a4', 'page_content': '`tokenizers-linux-x64-musl`\\nThis is the \\\\*\\\\*x86\\\\_64-unknown-linux-musl\\\\*\\\\* binary for `tokenizers`', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bbfdaf3d2ebb270a0a9d', 'page_content': 'This is more robust than preparing the pattern yourself: it avoids pitfalls, such as token glueing, that are very hard to debug. To see how much CPU and GPU memory you need for this model or others, try [this calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage) which can help determine that value. The LLaMA tokenizer is a BPE model based on [sentencepiece](https://github.com/google/sentencepiece). One quirk of sentencepiece is that when decoding a sequence, if the first token is the start of the word (e.g. \"Banana\"), the tokenizer does not prepend the prefix space to the string. Code Llama has the same architecture as the `Llama2` models, refer to [Llama2\\'s documentation page](llama2) for the API reference. Find Code Llama tokenizer reference below. ## CodeLlamaTokenizer\\n[[autodoc]] CodeLlamaTokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## CodeLlamaTokenizerFast\\n[[autodoc]] CodeLlamaTokenizerFast\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- update\\\\_post\\\\_processor\\n- save\\\\_vocabulary', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aab4e3e684812d15f5f62', 'page_content': 'p align=\"center\">\\n\\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\\n\\n[![Build](https://badge.fury.io/js/tokenizers.svg)](https://badge.fury.io/js/tokenizers)\\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\\n\\nNodeJS implementation of today\\'s most used tokenizers, with a focus on performance and\\nversatility. Bindings over the [Rust](https://github.com/huggingface/tokenizers/tree/master/tokenizers) implementation. If you are interested in the High-level design, you can go check it there. ## Main features\\n- Train new vocabularies and tokenize using 4 pre-made tokenizers (Bert WordPiece and the 3\\nmost common BPE versions). - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes\\nless than 20 seconds to tokenize a GB of text on a server\\'s CPU. - Easy to use, but also extremely versatile. - Designed for research and production. - Normalization comes with alignments tracking. It\\'s always possible to get the part of the\\noriginal sentence that corresponds to a given token. - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs. ## Installation\\n```bash\\nnpm install tokenizers@latest\\n```\\n## Basic example\\n```ts\\nimport { Tokenizer } from \"tokenizers\";\\nconst tokenizer = await Tokenizer.fromFile(\"tokenizer.json\");\\nconst wpEncoded = await tokenizer.encode(\"Who is John?\");\\nconsole.log(wpEncoded.getLength());\\nconsole.log(wpEncoded.getTokens());\\nconsole.log(wpEncoded.getIds());\\nconsole.log(wpEncoded.getAttentionMask());\\nconsole.log(wpEncoded.getOffsets());\\nconsole.log(wpEncoded.getOverflowing());\\nconsole.log(wpEncoded.getSpecialTokensMask());\\nconsole.log(wpEncoded.getTypeIds());\\nconsole.log(wpEncoded.getWordIds());\\n```\\n## License\\n[Apache License 2.0](../../LICENSE)', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--4fb51a2d-4ad7-46fd-b31b-b165ccf0b749-0', 'usage_metadata': {'input_tokens': 1080, 'output_tokens': 28, 'total_tokens': 1108, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:35:14,788 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      "  3%|▎         | 2/65 [00:09<04:56,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of the BLIP-Diffusion model?\\n', 'type': 'human', 'id': '00a88708-a0d3-47fb-ac68-0a511277bb18'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_oYS7Uoe3W31nKJupiyz8vX55', 'function': {'arguments': '{\"query\":\"BLIP-Diffusion model purpose\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--92541e2a-f2b5-4712-9e43-2b8272db375a-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'BLIP-Diffusion model purpose'}, 'id': 'call_oYS7Uoe3W31nKJupiyz8vX55', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 361, 'output_tokens': 22, 'total_tokens': 383, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# BLIP-Diffusion\\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. The abstract from the paper is:\\n\\\\*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).\\\\*\\nThe original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization. `BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## BlipDiffusionPipeline\\n[[autodoc]] BlipDiffusionPipeline\\n- all\\n- \\\\_\\\\_call\\\\_\\\\_\\n## BlipDiffusionControlNetPipeline\\n[[autodoc]] BlipDiffusionControlNetPipeline\\n- all\\n- \\\\_\\\\_call\\\\_\\\\_\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# BLIP\\n## Overview\\nThe BLIP model was proposed in [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. BLIP is a model that is able to perform various multi-modal tasks including:\\n- Visual Question Answering\\n- Image-Text retrieval (Image-text matching)\\n- Image Captioning\\nThe abstract from the paper is the following:\\n\\\\*Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# MultiDiffusion\\n[MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation](https://huggingface.co/papers/2302.08113) is by Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. The abstract from the paper is:\\n\\\\*Recent advances in text-to-image generation with diffusion models present transformative capabilities in image quality. However, user controllability of the generated image, and fast adaptation to new tasks still remains an open challenge, currently mostly addressed by costly and long re-training and fine-tuning or ad-hoc adaptations to specific image generation tasks. In this work, we present MultiDiffusion, a unified framework that enables versatile and controllable image generation, using a pre-trained text-to-image diffusion model, without any further training or finetuning. At the center of our approach is a new generation process, based on an optimization task that binds together multiple diffusion generation processes with a shared set of parameters or constraints. We show that MultiDiffusion can be readily applied to generate high quality and diverse images that adhere to user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals, ranging from tight segmentation masks to bounding boxes.\\\\*\\nYou can find additional information about MultiDiffusion on the [project page](https://multidiffusion.github.io/), [original codebase](https://github.com/omerbt/MultiDiffusion), and try it out in a [demo](https://huggingface.co/spaces/weizmannscience/MultiDiffusion).', 'type': 'tool', 'name': 'data_retriever', 'id': '9b640656-f908-45d1-9717-fcf43fbc4cdd', 'tool_call_id': 'call_oYS7Uoe3W31nKJupiyz8vX55', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab90a3e684812d15f6f5c', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# BLIP-Diffusion\\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. The abstract from the paper is:\\n\\\\*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).\\\\*\\nThe original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization. `BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## BlipDiffusionPipeline\\n[[autodoc]] BlipDiffusionPipeline\\n- all\\n- \\\\_\\\\_call\\\\_\\\\_\\n## BlipDiffusionControlNetPipeline\\n[[autodoc]] BlipDiffusionControlNetPipeline\\n- all\\n- \\\\_\\\\_call\\\\_\\\\_', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aad263e684812d15f61b2', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# BLIP\\n## Overview\\nThe BLIP model was proposed in [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. BLIP is a model that is able to perform various multi-modal tasks including:\\n- Visual Question Answering\\n- Image-Text retrieval (Image-text matching)\\n- Image Captioning\\nThe abstract from the paper is the following:\\n\\\\*Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b274af3d2ebb2709feba', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# MultiDiffusion\\n[MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation](https://huggingface.co/papers/2302.08113) is by Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. The abstract from the paper is:\\n\\\\*Recent advances in text-to-image generation with diffusion models present transformative capabilities in image quality. However, user controllability of the generated image, and fast adaptation to new tasks still remains an open challenge, currently mostly addressed by costly and long re-training and fine-tuning or ad-hoc adaptations to specific image generation tasks. In this work, we present MultiDiffusion, a unified framework that enables versatile and controllable image generation, using a pre-trained text-to-image diffusion model, without any further training or finetuning. At the center of our approach is a new generation process, based on an optimization task that binds together multiple diffusion generation processes with a shared set of parameters or constraints. We show that MultiDiffusion can be readily applied to generate high quality and diverse images that adhere to user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals, ranging from tight segmentation masks to bounding boxes.\\\\*\\nYou can find additional information about MultiDiffusion on the [project page](https://multidiffusion.github.io/), [original codebase](https://github.com/omerbt/MultiDiffusion), and try it out in a [demo](https://huggingface.co/spaces/weizmannscience/MultiDiffusion).', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation in text-to-image generation tasks. It creates novel renditions of an input subject based on text prompts while overcoming limitations such as lengthy fine-tuning and difficulties in preserving subject fidelity. BLIP-Diffusion introduces a new multimodal encoder pre-trained to provide subject representation, allowing for efficient fine-tuning and flexible combinations with existing techniques for novel subject-driven generation and editing applications.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--94166431-c9fa-4c9b-b267-3442ae5cd3e4-0', 'usage_metadata': {'input_tokens': 1692, 'output_tokens': 100, 'total_tokens': 1792, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:35:19,656 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      "  5%|▍         | 3/65 [00:14<04:56,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'How can a user claim authorship of a paper on the Hugging Face Hub?\\n', 'type': 'human', 'id': '9c8e0778-0c17-4185-8f21-b182e85b3e15'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_4FZM3vXe14ilRMNhEYgLqNPv', 'function': {'arguments': '{\"query\":\"claim authorship of a paper on the Hugging Face Hub\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--3067043c-1a32-4a3c-b901-071fd417a9e6-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'claim authorship of a paper on the Hugging Face Hub'}, 'id': 'call_4FZM3vXe14ilRMNhEYgLqNPv', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 27, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Paper Pages\\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png)\\n\\n## Linking a Paper to a model, dataset or Space\\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository\\'s tags. Clicking on the arxiv tag will let you:\\n\\\\* Visit the Paper page. \\\\* Filter for other models or datasets on the Hub that cite the same paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\\n\\n## Claiming authorship to a Paper\\nThe Hub will attempt to automatically match paper to users based on their email. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png)\\n\\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\".\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\\n\\nRead more about paper pages [here](./paper-pages).\\n\\n| Partially handled by `huggingface\\\\_hub`. |', 'type': 'tool', 'name': 'data_retriever', 'id': '34618a2c-5b25-4498-a2bd-8af283723d24', 'tool_call_id': 'call_4FZM3vXe14ilRMNhEYgLqNPv', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab6613e684812d15f6c54', 'page_content': 'Paper Pages\\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png)\\n\\n## Linking a Paper to a model, dataset or Space\\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository\\'s tags. Clicking on the arxiv tag will let you:\\n\\\\* Visit the Paper page. \\\\* Filter for other models or datasets on the Hub that cite the same paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\\n\\n## Claiming authorship to a Paper\\nThe Hub will attempt to automatically match paper to users based on their email. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png)\\n\\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\".', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa60b3e684812d15f5924', 'page_content': '![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\\n\\nRead more about paper pages [here](./paper-pages).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaeb93e684812d15f637d', 'page_content': '| Partially handled by `huggingface\\\\_hub`. |', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'To claim authorship of a paper on the Hugging Face Hub, a user can follow these steps:\\n\\n1. Go to the corresponding Paper page.\\n2. Click on their name displayed on the Paper page.\\n3. Select the option to \"claim authorship.\"\\n\\nThis process allows users to link their paper to their account if it is not already connected.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--ba74b6c1-0ffd-48a2-824d-3132a5ad4332-0', 'usage_metadata': {'input_tokens': 707, 'output_tokens': 71, 'total_tokens': 778, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:35:23,394 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      "  6%|▌         | 4/65 [00:17<04:26,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of the /healthcheck endpoint in the Datasets server API?\\n', 'type': 'human', 'id': '65a8cacc-bd34-4bf9-b488-4dc8a6f47d39'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_xujlsvyy4Gnhbpq8nz4q22FJ', 'function': {'arguments': '{\"query\":\"/healthcheck endpoint in the Datasets server API\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--a090f962-b7ea-4e0b-a45c-dfdaf219e540-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': '/healthcheck endpoint in the Datasets server API'}, 'id': 'call_xujlsvyy4Gnhbpq8nz4q22FJ', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 25, 'total_tokens': 390, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Datasets server API - rows endpoint\\n> /rows endpoint\\n## Configuration\\nThe service can be configured using environment variables. They are grouped by scope. ### API service\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\\nSee https://huggingface.co/docs/datasets-server\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /rows: get a slice of rows of a dataset split\\n\\nDatasets server admin machine\\n> Admin endpoints\\n## Configuration\\nThe worker can be configured using environment variables. They are grouped by scope. ### Admin service\\nSet environment variables to configure the application (`ADMIN\\\\_` prefix):\\n- `ADMIN\\\\_HF\\\\_ORGANIZATION`: the huggingface organization from which the authenticated user must be part of in order to access the protected routes, eg. \"huggingface\". If empty, the authentication is disabled. Defaults to None. - `ADMIN\\\\_CACHE\\\\_REPORTS\\\\_NUM\\\\_RESULTS`: the number of results in /cache-reports/... endpoints. Defaults to `100`. - `ADMIN\\\\_CACHE\\\\_REPORTS\\\\_WITH\\\\_CONTENT\\\\_NUM\\\\_RESULTS`: the number of results in /cache-reports-with-content/... endpoints. Defaults to `100`. - `ADMIN\\\\_HF\\\\_TIMEOUT\\\\_SECONDS`: the timeout in seconds for the requests to the Hugging Face Hub. Defaults to `0.2` (200 ms). - `ADMIN\\\\_HF\\\\_WHOAMI\\\\_PATH`: the path of the external whoami service, on the hub (see `HF\\\\_ENDPOINT`), eg. \"/api/whoami-v2\". Defaults to `/api/whoami-v2`. - `ADMIN\\\\_MAX\\\\_AGE`: number of seconds to set in the `max-age` header on technical endpoints. Defaults to `10` (10 seconds). ### Uvicorn\\nThe following environment variables are used to configure the Uvicorn server (`ADMIN\\\\_UVICORN\\\\_` prefix):\\n- `ADMIN\\\\_UVICORN\\\\_HOSTNAME`: the hostname. Defaults to `\"localhost\"`. - `ADMIN\\\\_UVICORN\\\\_NUM\\\\_WORKERS`: the number of uvicorn workers. Defaults to `2`. - `ADMIN\\\\_UVICORN\\\\_PORT`: the port. Defaults to `8000`. ### Prometheus\\n- `PROMETHEUS\\\\_MULTIPROC\\\\_DIR`: the directory where the uvicorn workers share their prometheus metrics. See https://github.com/prometheus/client\\\\_python#multiprocess-mode-eg-gunicorn. Defaults to empty, in which case every worker manages its own metrics, and the /metrics endpoint returns the metrics of a random worker. ### Common\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\\nThe admin service provides endpoints:\\n- `/healthcheck`\\n- `/metrics`: give info about the cache and the queue\\n- `/cache-reports{processing\\\\_step}`: give detailed reports on the content of the cache for a processing step\\n- `/cache-reports-with-content{processing\\\\_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\\n- `/force-refresh{processing\\\\_step}`: force refresh cache entries for the processing step. It\\'s a POST endpoint. Pass the requested parameters, depending on the processing step\\'s input type:\\n- `dataset`: `?dataset={dataset}`\\n- `config`: `?dataset={dataset}&config={config}`\\n- `split`: `?dataset={dataset}&config={config}&split={split}`\\n- `/recreate-dataset`: deletes all the cache entries related to a specific dataset, then run all the steps in order. It\\'s a POST endpoint.\\n\\nDatasets server SSE API\\n> Server-sent events API for the Datasets server. It\\'s used to update the Hub\\'s backend cache. ## Configuration\\nThe service can be configured using environment variables. They are grouped by scope. ### Common\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\\nSee https://huggingface.co/docs/datasets-server\\n- /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub\\\\_cache` field is null for deleted entries, or when the response is an error. The `num\\\\_rows` value is `0` if it could not be determined. ```typescript\\n{\\ndataset: string;\\nhub\\\\_cache: null | {\\npreview: boolean;\\nviewer: boolean;\\npartial: boolean;\\nnum\\\\_rows: int;\\n};\\n}\\n```', 'type': 'tool', 'name': 'data_retriever', 'id': '2d3b428e-9d89-40fb-a728-b3822a4578be', 'tool_call_id': 'call_xujlsvyy4Gnhbpq8nz4q22FJ', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab5983e684812d15f6b67', 'page_content': 'Datasets server API - rows endpoint\\n> /rows endpoint\\n## Configuration\\nThe service can be configured using environment variables. They are grouped by scope. ### API service\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\\nSee https://huggingface.co/docs/datasets-server\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /rows: get a slice of rows of a dataset split', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bad3af3d2ebb270a0943', 'page_content': 'Datasets server admin machine\\n> Admin endpoints\\n## Configuration\\nThe worker can be configured using environment variables. They are grouped by scope. ### Admin service\\nSet environment variables to configure the application (`ADMIN\\\\_` prefix):\\n- `ADMIN\\\\_HF\\\\_ORGANIZATION`: the huggingface organization from which the authenticated user must be part of in order to access the protected routes, eg. \"huggingface\". If empty, the authentication is disabled. Defaults to None. - `ADMIN\\\\_CACHE\\\\_REPORTS\\\\_NUM\\\\_RESULTS`: the number of results in /cache-reports/... endpoints. Defaults to `100`. - `ADMIN\\\\_CACHE\\\\_REPORTS\\\\_WITH\\\\_CONTENT\\\\_NUM\\\\_RESULTS`: the number of results in /cache-reports-with-content/... endpoints. Defaults to `100`. - `ADMIN\\\\_HF\\\\_TIMEOUT\\\\_SECONDS`: the timeout in seconds for the requests to the Hugging Face Hub. Defaults to `0.2` (200 ms). - `ADMIN\\\\_HF\\\\_WHOAMI\\\\_PATH`: the path of the external whoami service, on the hub (see `HF\\\\_ENDPOINT`), eg. \"/api/whoami-v2\". Defaults to `/api/whoami-v2`. - `ADMIN\\\\_MAX\\\\_AGE`: number of seconds to set in the `max-age` header on technical endpoints. Defaults to `10` (10 seconds). ### Uvicorn\\nThe following environment variables are used to configure the Uvicorn server (`ADMIN\\\\_UVICORN\\\\_` prefix):\\n- `ADMIN\\\\_UVICORN\\\\_HOSTNAME`: the hostname. Defaults to `\"localhost\"`. - `ADMIN\\\\_UVICORN\\\\_NUM\\\\_WORKERS`: the number of uvicorn workers. Defaults to `2`. - `ADMIN\\\\_UVICORN\\\\_PORT`: the port. Defaults to `8000`. ### Prometheus\\n- `PROMETHEUS\\\\_MULTIPROC\\\\_DIR`: the directory where the uvicorn workers share their prometheus metrics. See https://github.com/prometheus/client\\\\_python#multiprocess-mode-eg-gunicorn. Defaults to empty, in which case every worker manages its own metrics, and the /metrics endpoint returns the metrics of a random worker. ### Common\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\\nThe admin service provides endpoints:\\n- `/healthcheck`\\n- `/metrics`: give info about the cache and the queue\\n- `/cache-reports{processing\\\\_step}`: give detailed reports on the content of the cache for a processing step\\n- `/cache-reports-with-content{processing\\\\_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\\n- `/force-refresh{processing\\\\_step}`: force refresh cache entries for the processing step. It\\'s a POST endpoint. Pass the requested parameters, depending on the processing step\\'s input type:\\n- `dataset`: `?dataset={dataset}`\\n- `config`: `?dataset={dataset}&config={config}`\\n- `split`: `?dataset={dataset}&config={config}&split={split}`\\n- `/recreate-dataset`: deletes all the cache entries related to a specific dataset, then run all the steps in order. It\\'s a POST endpoint.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c14e666af48523f51fe9', 'page_content': \"Datasets server SSE API\\n> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache. ## Configuration\\nThe service can be configured using environment variables. They are grouped by scope. ### Common\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\\nSee https://huggingface.co/docs/datasets-server\\n- /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub\\\\_cache` field is null for deleted entries, or when the response is an error. The `num\\\\_rows` value is `0` if it could not be determined. ```typescript\\n{\\ndataset: string;\\nhub\\\\_cache: null | {\\npreview: boolean;\\nviewer: boolean;\\npartial: boolean;\\nnum\\\\_rows: int;\\n};\\n}\\n```\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the app is running.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--76bcaa3b-a116-43c9-9162-f60d13b9ee68-0', 'usage_metadata': {'input_tokens': 1444, 'output_tokens': 23, 'total_tokens': 1467, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:35:28,217 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      "  8%|▊         | 5/65 [00:22<04:31,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the default context window size for Local Attention in the LongT5 model?\\n', 'type': 'human', 'id': '8490300e-b7d6-4925-b493-cbe38446415b'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_wept2uhai9jsZhWRWACwiVa9', 'function': {'arguments': '{\"query\":\"default context window size for Local Attention in LongT5 model\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--8aebee2e-3c14-41a8-b628-acaffb830d91-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'default context window size for Local Attention in LongT5 model'}, 'id': 'call_wept2uhai9jsZhWRWACwiVa9', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 27, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# LongT5\\n## Overview\\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It\\'s an\\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\\nTransient-Global attention. The abstract from the paper is the following:\\n\\\\*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\\\em Transient Global}\\n(TGlobal), which mimics ETC\\'s local/global attention mechanism, but without requiring additional side-inputs. We are\\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\\nquestion answering tasks.\\\\*\\nThis model was contributed by [stancld](https://huggingface.co/stancld). The original code can be found [here](https://github.com/google-research/longt5). ## Usage tips\\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\\nencoder \\\\*self-attention\\\\* layer with efficient either \\\\*local\\\\* attention or \\\\*transient-global\\\\* (\\\\*tglobal\\\\*) attention. - Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\\ninspired by the pre-training of [`PegasusForConditionalGeneration`]. - LongT5 model is designed to work efficiently and very well on long-range \\\\*sequence-to-sequence\\\\* tasks where the\\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens. - For \\\\*Local Attention\\\\*, the sparse sliding-window local attention operation allows a given token to attend only `r`\\ntokens to the left and right of it (with `r=127` by default). \\\\*Local Attention\\\\* does not introduce any new parameters\\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l\\\\*r)`. - \\\\*Transient Global Attention\\\\* is an extension of the \\\\*Local Attention\\\\*. It, furthermore, allows each input token to\\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\\nalso every global token like in the case of standard global attention (\\\\*transient\\\\* represents the fact the global tokens\\nare constructed dynamically within each attention operation). As a consequence, \\\\*TGlobal\\\\* attention introduces\\na few new parameters -- global relative position biases and a layer normalization for global token\\'s embedding.\\n\\nWith our sliding window approach, however, there is overlap in\\nthe tokens we pass to the model at each iteration. We don\\'t want the log-likelihood for the tokens we\\'re just treating\\nas context to be included in our loss, so we can set these targets to `-100` so that they are ignored. The following\\nis an example of how we could do this with a stride of `512`. This means that the model will have at least 512 tokens\\nfor context when calculating the conditional likelihood of any one token (provided there are 512 preceding tokens\\navailable to condition on). ```python\\nimport torch\\nfrom tqdm import tqdm\\nmax\\\\_length = model.config.n\\\\_positions\\nstride = 512\\nseq\\\\_len = encodings.input\\\\_ids.size(1)\\nnlls = []\\nprev\\\\_end\\\\_loc = 0\\nfor begin\\\\_loc in tqdm(range(0, seq\\\\_len, stride)):\\nend\\\\_loc = min(begin\\\\_loc + max\\\\_length, seq\\\\_len)\\ntrg\\\\_len = end\\\\_loc - prev\\\\_end\\\\_loc # may be different from stride on last loop\\ninput\\\\_ids = encodings.input\\\\_ids[:, begin\\\\_loc:end\\\\_loc].to(device)\\ntarget\\\\_ids = input\\\\_ids.clone()\\ntarget\\\\_ids[:, :-trg\\\\_len] = -100\\nwith torch.no\\\\_grad():\\noutputs = model(input\\\\_ids, labels=target\\\\_ids)\\n# loss is calculated using CrossEntropyLoss which averages over valid labels\\n# N.B. the model only calculates loss over trg\\\\_len - 1 labels, because it internally shifts the labels\\n# to the left by 1. neg\\\\_log\\\\_likelihood = outputs.loss\\nnlls.append(neg\\\\_log\\\\_likelihood)\\nprev\\\\_end\\\\_loc = end\\\\_loc\\nif end\\\\_loc == seq\\\\_len:\\nbreak\\nppl = torch.exp(torch.stack(nlls).mean())\\n```\\nRunning this with the stride length equal to the max input length is equivalent to the suboptimal, non-sliding-window\\nstrategy we discussed above. The smaller the stride, the more context the model will have in making each prediction,\\nand the better the reported perplexity will typically be. When we run the above with `stride = 1024`, i.e. no overlap, the resulting PPL is `19.44`, which is about the same\\nas the `19.93` reported in the GPT-2 paper. By using `stride = 512` and thereby employing our striding window\\nstrategy, this jumps down to `16.45`. This is not only a more favorable score, but is calculated in a way that is\\ncloser to the true autoregressive decomposition of a sequence likelihood.\\n\\n![](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/mixtral-7b-inference-large-seqlen.png)\\n\\n### Sliding window Attention\\nThe current implementation supports the sliding window attention mechanism and memory efficient cache management. To enable sliding window attention, just make sure to have a `flash-attn` version that is compatible with sliding window attention (`>=2.3.0`). The Flash Attention-2 model uses also a more memory efficient cache slicing mechanism - as recommended per the official implementation of Mistral model that use rolling cache mechanism we keep the cache size fixed (`self.config.sliding\\\\_window`), support batched generation only for `padding\\\\_side=\"left\"` and use the absolute position of the current token to compute the positional embedding. ## The Mistral Team\\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed. ## MixtralConfig\\n[[autodoc]] MixtralConfig\\n## MixtralModel\\n[[autodoc]] MixtralModel\\n- forward\\n## MixtralForCausalLM\\n[[autodoc]] MixtralForCausalLM\\n- forward\\n## MixtralForSequenceClassification\\n[[autodoc]] MixtralForSequenceClassification\\n- forward', 'type': 'tool', 'name': 'data_retriever', 'id': 'c717acc2-7241-45b6-82e3-6e278bae4581', 'tool_call_id': 'call_wept2uhai9jsZhWRWACwiVa9', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa9783e684812d15f5d02', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# LongT5\\n## Overview\\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It\\'s an\\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\\nTransient-Global attention. The abstract from the paper is the following:\\n\\\\*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\\\em Transient Global}\\n(TGlobal), which mimics ETC\\'s local/global attention mechanism, but without requiring additional side-inputs. We are\\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\\nquestion answering tasks.\\\\*\\nThis model was contributed by [stancld](https://huggingface.co/stancld). The original code can be found [here](https://github.com/google-research/longt5). ## Usage tips\\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\\nencoder \\\\*self-attention\\\\* layer with efficient either \\\\*local\\\\* attention or \\\\*transient-global\\\\* (\\\\*tglobal\\\\*) attention. - Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\\ninspired by the pre-training of [`PegasusForConditionalGeneration`]. - LongT5 model is designed to work efficiently and very well on long-range \\\\*sequence-to-sequence\\\\* tasks where the\\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens. - For \\\\*Local Attention\\\\*, the sparse sliding-window local attention operation allows a given token to attend only `r`\\ntokens to the left and right of it (with `r=127` by default). \\\\*Local Attention\\\\* does not introduce any new parameters\\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l\\\\*r)`. - \\\\*Transient Global Attention\\\\* is an extension of the \\\\*Local Attention\\\\*. It, furthermore, allows each input token to\\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\\nalso every global token like in the case of standard global attention (\\\\*transient\\\\* represents the fact the global tokens\\nare constructed dynamically within each attention operation). As a consequence, \\\\*TGlobal\\\\* attention introduces\\na few new parameters -- global relative position biases and a layer normalization for global token\\'s embedding.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899be11666af48523f51c0b', 'page_content': \"With our sliding window approach, however, there is overlap in\\nthe tokens we pass to the model at each iteration. We don't want the log-likelihood for the tokens we're just treating\\nas context to be included in our loss, so we can set these targets to `-100` so that they are ignored. The following\\nis an example of how we could do this with a stride of `512`. This means that the model will have at least 512 tokens\\nfor context when calculating the conditional likelihood of any one token (provided there are 512 preceding tokens\\navailable to condition on). ```python\\nimport torch\\nfrom tqdm import tqdm\\nmax\\\\_length = model.config.n\\\\_positions\\nstride = 512\\nseq\\\\_len = encodings.input\\\\_ids.size(1)\\nnlls = []\\nprev\\\\_end\\\\_loc = 0\\nfor begin\\\\_loc in tqdm(range(0, seq\\\\_len, stride)):\\nend\\\\_loc = min(begin\\\\_loc + max\\\\_length, seq\\\\_len)\\ntrg\\\\_len = end\\\\_loc - prev\\\\_end\\\\_loc # may be different from stride on last loop\\ninput\\\\_ids = encodings.input\\\\_ids[:, begin\\\\_loc:end\\\\_loc].to(device)\\ntarget\\\\_ids = input\\\\_ids.clone()\\ntarget\\\\_ids[:, :-trg\\\\_len] = -100\\nwith torch.no\\\\_grad():\\noutputs = model(input\\\\_ids, labels=target\\\\_ids)\\n# loss is calculated using CrossEntropyLoss which averages over valid labels\\n# N.B. the model only calculates loss over trg\\\\_len - 1 labels, because it internally shifts the labels\\n# to the left by 1. neg\\\\_log\\\\_likelihood = outputs.loss\\nnlls.append(neg\\\\_log\\\\_likelihood)\\nprev\\\\_end\\\\_loc = end\\\\_loc\\nif end\\\\_loc == seq\\\\_len:\\nbreak\\nppl = torch.exp(torch.stack(nlls).mean())\\n```\\nRunning this with the stride length equal to the max input length is equivalent to the suboptimal, non-sliding-window\\nstrategy we discussed above. The smaller the stride, the more context the model will have in making each prediction,\\nand the better the reported perplexity will typically be. When we run the above with `stride = 1024`, i.e. no overlap, the resulting PPL is `19.44`, which is about the same\\nas the `19.93` reported in the GPT-2 paper. By using `stride = 512` and thereby employing our striding window\\nstrategy, this jumps down to `16.45`. This is not only a more favorable score, but is calculated in a way that is\\ncloser to the true autoregressive decomposition of a sequence likelihood.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa63b3e684812d15f5962', 'page_content': '![](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/mixtral-7b-inference-large-seqlen.png)\\n\\n### Sliding window Attention\\nThe current implementation supports the sliding window attention mechanism and memory efficient cache management. To enable sliding window attention, just make sure to have a `flash-attn` version that is compatible with sliding window attention (`>=2.3.0`). The Flash Attention-2 model uses also a more memory efficient cache slicing mechanism - as recommended per the official implementation of Mistral model that use rolling cache mechanism we keep the cache size fixed (`self.config.sliding\\\\_window`), support batched generation only for `padding\\\\_side=\"left\"` and use the absolute position of the current token to compute the positional embedding. ## The Mistral Team\\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed. ## MixtralConfig\\n[[autodoc]] MixtralConfig\\n## MixtralModel\\n[[autodoc]] MixtralModel\\n- forward\\n## MixtralForCausalLM\\n[[autodoc]] MixtralForCausalLM\\n- forward\\n## MixtralForSequenceClassification\\n[[autodoc]] MixtralForSequenceClassification\\n- forward', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The default context window size for Local Attention in the LongT5 model is `r=127`, which means a given token can attend to 127 tokens to the left and 127 tokens to the right of it.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--ee4c3555-88dc-410b-8807-7ae952cdcfd9-0', 'usage_metadata': {'input_tokens': 2159, 'output_tokens': 44, 'total_tokens': 2203, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:35:31,638 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      "  9%|▉         | 6/65 [00:26<04:05,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What method is used to load a checkpoint for a task using `AutoPipeline`?\\n', 'type': 'human', 'id': '97ab859b-c0e5-4abd-b14c-3f95e073e31d'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_2JmUw0e5fJmbTEnVfXhegSkf', 'function': {'arguments': '{\"query\":\"load a checkpoint for a task using AutoPipeline\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--aabac081-66fc-4af1-8dfd-5c0d59196aeb-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'load a checkpoint for a task using AutoPipeline'}, 'id': 'call_2JmUw0e5fJmbTEnVfXhegSkf', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 24, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'The `AutoPipeline` class is designed to simplify the variety of pipelines in 🤗 Diffusers. It is a generic, \\\\*task-first\\\\* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name. Take a look at the [AutoPipeline](../api/pipelines/auto\\\\_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting. This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights. ## Choose an AutoPipeline for your task\\nStart by picking a checkpoint. For example, if you\\'re interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\\n```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\npipeline = AutoPipelineForText2Image.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n).to(\"cuda\")\\nprompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"\\nimage = pipeline(prompt, num\\\\_inference\\\\_steps=25).images[0]\\nimage\\n```\\n\\n![generated image of peasant fighting dragon in wood cutting style](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png)\\n\\nUnder the hood, [`AutoPipelineForText2Image`]:\\n1. automatically detects a `\"stable-diffusion\"` class from the [`model\\\\_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model\\\\_index.json) file\\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\"stable-diffusion\"` class name\\nLikewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\"stable-diffusion\"` checkpoint from the `model\\\\_index.json` file and it\\'ll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image:\\n```py\\nfrom diffusers import AutoPipelineForImage2Image\\nimport torch\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\npipeline = AutoPipelineForImage2Image.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\",\\ntorch\\\\_dtype=torch.float16,\\nuse\\\\_safetensors=True,\\n).to(\"cuda\")\\nprompt = \"a portrait of a dog wearing a pearl earring\"\\nurl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665\\\\_Girl\\\\_with\\\\_a\\\\_Pearl\\\\_Earring.jpg/800px-1665\\\\_Girl\\\\_with\\\\_a\\\\_Pearl\\\\_Earring.jpg\"\\nresponse = requests.get(url)\\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\\nimage.thumbnail((768, 768))\\nimage = pipeline(prompt, image, num\\\\_inference\\\\_steps=200, strength=0.75, guidance\\\\_scale=10.5).images[0]\\nimage\\n```\\n\\n![generated image of a vermeer portrait of a dog wearing a pearl earring](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png)\\n\\nAnd if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way:\\n```py\\nfrom diffusers import AutoPipelineForInpainting\\nfrom diffusers.utils import load\\\\_image\\nimport torch\\npipeline = AutoPipelineForInpainting.from\\\\_pretrained(\\n\"stabilityai/stable-diffusion-xl-base-1.0\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n).to(\"cuda\")\\nimg\\\\_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\\\_examples/overture-creations-5sI6fQgYIuo.png\"\\nmask\\\\_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\\\_examples/overture-creations-5sI6fQgYIuo\\\\_mask.png\"\\ninit\\\\_image = load\\\\_image(img\\\\_url).convert(\"RGB\")\\nmask\\\\_image = load\\\\_image(mask\\\\_url).convert(\"RGB\")\\nprompt = \"A majestic tiger sitting on a bench\"\\nimage = pipeline(prompt, image=init\\\\_image, mask\\\\_image=mask\\\\_image, num\\\\_inference\\\\_steps=50, strength=0.80).images[0]\\nimage\\n```\\n\\n![generated image of a tiger sitting on a bench](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png)\\n\\nIf you try to load an unsupported checkpoint, it\\'ll throw an error:\\n```py\\nfrom diffusers import AutoPipelineForImage2Image\\nimport torch\\npipeline = AutoPipelineForImage2Image.from\\\\_pretrained(\\n\"openai/shap-e-img2img\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n)\\n\"ValueError: AutoPipeline can\\'t find a pipeline linked to ShapEImg2ImgPipeline for None\"\\n```\\n## Use multiple pipelines\\nFor some workflows or if you\\'re loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Overview\\n🧨 Diffusers offers many pipelines, models, and schedulers for generative tasks. To make loading these components as simple as possible, we provide a single and unified method - `from\\\\_pretrained()` - that loads any of these components from either the Hugging Face [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) or your local machine. Whenever you load a pipeline or model, the latest files are automatically downloaded and cached so you can quickly reuse them next time without redownloading the files. This section will show you everything you need to know about loading pipelines, how to load different components in a pipeline, how to load checkpoint variants, and how to load community pipelines. You\\'ll also learn how to load schedulers and compare the speed and quality trade-offs of using different schedulers. Finally, you\\'ll see how to convert and load KerasCV checkpoints so you can use them in PyTorch with 🧨 Diffusers.\\n\\nWhen using\\n`DiffusionPipeline.from\\\\_pretrained(...)`, checkpoints are loaded into\\ntheir default pipeline. The default pipeline for the IF is the\\ntext-to-image [`IFPipeline`](https://huggingface.co/docs/diffusers/v0.16.0/en/api/pipelines/if#diffusers.IFPipeline). When loading checkpoints\\nwith a non-default pipeline, the pipeline must be explicitly specified. ``` python\\nfrom diffusers import IFImg2ImgPipeline\\npipe = IFImg2ImgPipeline.from\\\\_pretrained(\\n\"DeepFloyd/IF-I-XL-v1.0\",\\ntext\\\\_encoder=text\\\\_encoder,\\nunet=None,\\ndevice\\\\_map=\"auto\"\\n)\\n```\\nLet\\\\\\'s turn our salesman into an anime character. ``` python\\nprompt = \"anime style\"\\n```\\nAs before, we create the text embeddings with T5\\n``` python\\nprompt\\\\_embeds, negative\\\\_embeds = pipe.encode\\\\_prompt(prompt)\\n```\\nand free GPU and CPU memory. First, remove the Python pointers\\n``` python\\ndel text\\\\_encoder\\ndel pipe\\n```\\nand then free the memory\\n``` python\\nflush()\\n```\\n### 2.2 Stage 1: The main diffusion process\\nNext, we only load the stage 1 UNet weights into the pipeline object,\\njust like we did in the previous section. ``` python\\npipe = IFImg2ImgPipeline.from\\\\_pretrained(\\n\"DeepFloyd/IF-I-XL-v1.0\",\\ntext\\\\_encoder=None,\\nvariant=\"fp16\",\\ntorch\\\\_dtype=torch.float16,\\ndevice\\\\_map=\"auto\"\\n)\\n```\\nThe image variation pipeline requires both the original image and the\\nprompt embeddings.', 'type': 'tool', 'name': 'data_retriever', 'id': '5f6d045c-5736-4687-a2eb-bb84f89979df', 'tool_call_id': 'call_2JmUw0e5fJmbTEnVfXhegSkf', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b3c6af3d2ebb270a00fe', 'page_content': 'The `AutoPipeline` class is designed to simplify the variety of pipelines in 🤗 Diffusers. It is a generic, \\\\*task-first\\\\* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name. Take a look at the [AutoPipeline](../api/pipelines/auto\\\\_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting. This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights. ## Choose an AutoPipeline for your task\\nStart by picking a checkpoint. For example, if you\\'re interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\\n```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\npipeline = AutoPipelineForText2Image.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n).to(\"cuda\")\\nprompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"\\nimage = pipeline(prompt, num\\\\_inference\\\\_steps=25).images[0]\\nimage\\n```\\n\\n![generated image of peasant fighting dragon in wood cutting style](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png)\\n\\nUnder the hood, [`AutoPipelineForText2Image`]:\\n1. automatically detects a `\"stable-diffusion\"` class from the [`model\\\\_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model\\\\_index.json) file\\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\"stable-diffusion\"` class name\\nLikewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\"stable-diffusion\"` checkpoint from the `model\\\\_index.json` file and it\\'ll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image:\\n```py\\nfrom diffusers import AutoPipelineForImage2Image\\nimport torch\\nimport requests\\nfrom PIL import Image\\nfrom io import BytesIO\\npipeline = AutoPipelineForImage2Image.from\\\\_pretrained(\\n\"runwayml/stable-diffusion-v1-5\",\\ntorch\\\\_dtype=torch.float16,\\nuse\\\\_safetensors=True,\\n).to(\"cuda\")\\nprompt = \"a portrait of a dog wearing a pearl earring\"\\nurl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665\\\\_Girl\\\\_with\\\\_a\\\\_Pearl\\\\_Earring.jpg/800px-1665\\\\_Girl\\\\_with\\\\_a\\\\_Pearl\\\\_Earring.jpg\"\\nresponse = requests.get(url)\\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\\nimage.thumbnail((768, 768))\\nimage = pipeline(prompt, image, num\\\\_inference\\\\_steps=200, strength=0.75, guidance\\\\_scale=10.5).images[0]\\nimage\\n```\\n\\n![generated image of a vermeer portrait of a dog wearing a pearl earring](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png)\\n\\nAnd if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way:\\n```py\\nfrom diffusers import AutoPipelineForInpainting\\nfrom diffusers.utils import load\\\\_image\\nimport torch\\npipeline = AutoPipelineForInpainting.from\\\\_pretrained(\\n\"stabilityai/stable-diffusion-xl-base-1.0\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n).to(\"cuda\")\\nimg\\\\_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\\\_examples/overture-creations-5sI6fQgYIuo.png\"\\nmask\\\\_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\\\_examples/overture-creations-5sI6fQgYIuo\\\\_mask.png\"\\ninit\\\\_image = load\\\\_image(img\\\\_url).convert(\"RGB\")\\nmask\\\\_image = load\\\\_image(mask\\\\_url).convert(\"RGB\")\\nprompt = \"A majestic tiger sitting on a bench\"\\nimage = pipeline(prompt, image=init\\\\_image, mask\\\\_image=mask\\\\_image, num\\\\_inference\\\\_steps=50, strength=0.80).images[0]\\nimage\\n```\\n\\n![generated image of a tiger sitting on a bench](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png)\\n\\nIf you try to load an unsupported checkpoint, it\\'ll throw an error:\\n```py\\nfrom diffusers import AutoPipelineForImage2Image\\nimport torch\\npipeline = AutoPipelineForImage2Image.from\\\\_pretrained(\\n\"openai/shap-e-img2img\", torch\\\\_dtype=torch.float16, use\\\\_safetensors=True\\n)\\n\"ValueError: AutoPipeline can\\'t find a pipeline linked to ShapEImg2ImgPipeline for None\"\\n```\\n## Use multiple pipelines\\nFor some workflows or if you\\'re loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b62faf3d2ebb270a0381', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Overview\\n🧨 Diffusers offers many pipelines, models, and schedulers for generative tasks. To make loading these components as simple as possible, we provide a single and unified method - `from\\\\_pretrained()` - that loads any of these components from either the Hugging Face [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) or your local machine. Whenever you load a pipeline or model, the latest files are automatically downloaded and cached so you can quickly reuse them next time without redownloading the files. This section will show you everything you need to know about loading pipelines, how to load different components in a pipeline, how to load checkpoint variants, and how to load community pipelines. You\\'ll also learn how to load schedulers and compare the speed and quality trade-offs of using different schedulers. Finally, you\\'ll see how to convert and load KerasCV checkpoints so you can use them in PyTorch with 🧨 Diffusers.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bee4666af48523f51d35', 'page_content': 'When using\\n`DiffusionPipeline.from\\\\_pretrained(...)`, checkpoints are loaded into\\ntheir default pipeline. The default pipeline for the IF is the\\ntext-to-image [`IFPipeline`](https://huggingface.co/docs/diffusers/v0.16.0/en/api/pipelines/if#diffusers.IFPipeline). When loading checkpoints\\nwith a non-default pipeline, the pipeline must be explicitly specified. ``` python\\nfrom diffusers import IFImg2ImgPipeline\\npipe = IFImg2ImgPipeline.from\\\\_pretrained(\\n\"DeepFloyd/IF-I-XL-v1.0\",\\ntext\\\\_encoder=text\\\\_encoder,\\nunet=None,\\ndevice\\\\_map=\"auto\"\\n)\\n```\\nLet\\\\\\'s turn our salesman into an anime character. ``` python\\nprompt = \"anime style\"\\n```\\nAs before, we create the text embeddings with T5\\n``` python\\nprompt\\\\_embeds, negative\\\\_embeds = pipe.encode\\\\_prompt(prompt)\\n```\\nand free GPU and CPU memory. First, remove the Python pointers\\n``` python\\ndel text\\\\_encoder\\ndel pipe\\n```\\nand then free the memory\\n``` python\\nflush()\\n```\\n### 2.2 Stage 1: The main diffusion process\\nNext, we only load the stage 1 UNet weights into the pipeline object,\\njust like we did in the previous section. ``` python\\npipe = IFImg2ImgPipeline.from\\\\_pretrained(\\n\"DeepFloyd/IF-I-XL-v1.0\",\\ntext\\\\_encoder=None,\\nvariant=\"fp16\",\\ntorch\\\\_dtype=torch.float16,\\ndevice\\\\_map=\"auto\"\\n)\\n```\\nThe image variation pipeline requires both the original image and the\\nprompt embeddings.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically detects the correct pipeline class to use for the specified task and loads the corresponding checkpoint.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--2474f6ef-6a5d-4043-8034-b2eecb8c5043-0', 'usage_metadata': {'input_tokens': 2245, 'output_tokens': 42, 'total_tokens': 2287, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:35:37,989 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 11%|█         | 7/65 [00:32<04:42,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of Diffusers library?\\n', 'type': 'human', 'id': 'be91e168-a555-45a7-9470-4a2b579c5b4f'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_7t8R5i4RZfFhY1h107TQFSed', 'function': {'arguments': '{\"query\":\"Diffusers library purpose\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--3493840e-e5a7-4449-aae0-a8684ffedc93-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Diffusers library purpose'}, 'id': 'call_7t8R5i4RZfFhY1h107TQFSed', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 357, 'output_tokens': 19, 'total_tokens': 376, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# 🧨 Diffusers’ Ethical Guidelines\\n## Preamble\\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training. Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users’ contributions, and usage of the Diffusers library. The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Philosophy\\n🧨 Diffusers provides \\\\*\\\\*state-of-the-art\\\\*\\\\* pretrained diffusion models across multiple modalities. Its purpose is to serve as a \\\\*\\\\*modular toolbox\\\\*\\\\* for both inference and training. We aim at building a library that stands the test of time and therefore take API design very seriously. In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch\\'s Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let\\'s go over the most important ones:\\n## Usability over Performance\\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library. - Diffusers aims to be a \\\\*\\\\*light-weight\\\\*\\\\* package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages. - Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired. ## Simple over easy\\nAs PyTorch states, \\\\*\\\\*explicit is better than implicit\\\\*\\\\* and \\\\*\\\\*simple is better than complex\\\\*\\\\*.\\n\\n--\\ntitle: Swift 🧨Diffusers - Fast Stable Diffusion for Mac\\nthumbnail: /blog/assets/fast-mac-diffusers/thumbnail.png\\nauthors:\\n- user: pcuenq\\n- user: reach-vb\\n---\\n# Swift 🧨Diffusers: Fast Stable Diffusion for Mac\\nTransform your text into stunning images with ease using Diffusers for Mac, a native app powered by state-of-the-art diffusion models. It leverages a bouquet of SoTA Text-to-Image models contributed by the community to the Hugging Face Hub, and converted to Core ML for blazingly fast performance. Our latest version, 1.1, is now available on the [Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) with significant performance upgrades and user-friendly interface tweaks. It\\'s a solid foundation for future feature updates. Plus, the app is fully open source with a permissive [license](https://github.com/huggingface/swift-coreml-diffusers/blob/main/LICENSE), so you can build on it too! Check out our GitHub repository at https://github.com/huggingface/swift-coreml-diffusers for more information. ![Screenshot showing Diffusers for Mac UI](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-mac-diffusers/UI.png)\\n## What exactly is 🧨Diffusers for Mac anyway? The Diffusers app ([App Store](https://apps.apple.com/app/diffusers/id1666309574), [source code](https://github.com/huggingface/swift-coreml-diffusers)) is the Mac counterpart to our [🧨`diffusers` library](https://github.com/huggingface/diffusers). This library is written in Python with PyTorch, and uses a modular design to train and run diffusion models.', 'type': 'tool', 'name': 'data_retriever', 'id': 'c76172ba-9647-403a-88fd-78af707ccf1b', 'tool_call_id': 'call_7t8R5i4RZfFhY1h107TQFSed', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bb05af3d2ebb270a097e', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# 🧨 Diffusers’ Ethical Guidelines\\n## Preamble\\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training. Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users’ contributions, and usage of the Diffusers library. The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa5253e684812d15f5806', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Philosophy\\n🧨 Diffusers provides \\\\*\\\\*state-of-the-art\\\\*\\\\* pretrained diffusion models across multiple modalities. Its purpose is to serve as a \\\\*\\\\*modular toolbox\\\\*\\\\* for both inference and training. We aim at building a library that stands the test of time and therefore take API design very seriously. In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch\\'s Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let\\'s go over the most important ones:\\n## Usability over Performance\\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library. - Diffusers aims to be a \\\\*\\\\*light-weight\\\\*\\\\* package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages. - Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired. ## Simple over easy\\nAs PyTorch states, \\\\*\\\\*explicit is better than implicit\\\\*\\\\* and \\\\*\\\\*simple is better than complex\\\\*\\\\*.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab6b43e684812d15f6c8f', 'page_content': \"--\\ntitle: Swift 🧨Diffusers - Fast Stable Diffusion for Mac\\nthumbnail: /blog/assets/fast-mac-diffusers/thumbnail.png\\nauthors:\\n- user: pcuenq\\n- user: reach-vb\\n---\\n# Swift 🧨Diffusers: Fast Stable Diffusion for Mac\\nTransform your text into stunning images with ease using Diffusers for Mac, a native app powered by state-of-the-art diffusion models. It leverages a bouquet of SoTA Text-to-Image models contributed by the community to the Hugging Face Hub, and converted to Core ML for blazingly fast performance. Our latest version, 1.1, is now available on the [Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) with significant performance upgrades and user-friendly interface tweaks. It's a solid foundation for future feature updates. Plus, the app is fully open source with a permissive [license](https://github.com/huggingface/swift-coreml-diffusers/blob/main/LICENSE), so you can build on it too! Check out our GitHub repository at https://github.com/huggingface/swift-coreml-diffusers for more information. ![Screenshot showing Diffusers for Mac UI](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-mac-diffusers/UI.png)\\n## What exactly is 🧨Diffusers for Mac anyway? The Diffusers app ([App Store](https://apps.apple.com/app/diffusers/id1666309574), [source code](https://github.com/huggingface/swift-coreml-diffusers)) is the Mac counterpart to our [🧨`diffusers` library](https://github.com/huggingface/diffusers). This library is written in Python with PyTorch, and uses a modular design to train and run diffusion models.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The purpose of the Diffusers library is to provide state-of-the-art pretrained diffusion models across multiple modalities and to serve as a modular toolbox for both inference and training. It aims to build a library that stands the test of time, focusing on usability and simplicity in its design.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--7c27d4a3-7f7a-47df-8f99-9dcb63d0eb63-0', 'usage_metadata': {'input_tokens': 1431, 'output_tokens': 55, 'total_tokens': 1486, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:35:41,849 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 12%|█▏        | 8/65 [00:36<04:19,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What method does the EulerAncestralDiscreteScheduler use for sampling?\\n', 'type': 'human', 'id': '85e9aa5a-86a3-4fbd-8038-7bcedc1efd45'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_5kvYyIXCIrTWUiJwyFvlq8wQ', 'function': {'arguments': '{\"query\":\"EulerAncestralDiscreteScheduler sampling method\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--d2feb267-9ead-48b0-afcf-e667e956aa36-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'EulerAncestralDiscreteScheduler sampling method'}, 'id': 'call_5kvYyIXCIrTWUiJwyFvlq8wQ', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 23, 'total_tokens': 385, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# EulerAncestralDiscreteScheduler\\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\\\_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerAncestralDiscreteScheduler\\n[[autodoc]] EulerAncestralDiscreteScheduler\\n## EulerAncestralDiscreteSchedulerOutput\\n[[autodoc]] schedulers.scheduling\\\\_euler\\\\_ancestral\\\\_discrete.EulerAncestralDiscreteSchedulerOutput\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Schedulers\\n🤗 Diffusers provides many scheduler functions for the diffusion process. A scheduler takes a model\\'s output (the sample which the diffusion process is iterating on) and a timestep to return a denoised sample. The timestep is important because it dictates where in the diffusion process the step is; data is generated by iterating forward \\\\*n\\\\* timesteps and inference occurs by propagating backward through the timesteps. Based on the timestep, a scheduler may be \\\\*discrete\\\\* in which case the timestep is an `int` or \\\\*continuous\\\\* in which case the timestep is a `float`. Depending on the context, a scheduler defines how to iteratively add noise to an image or how to update a sample based on a model\\'s output:\\n- during \\\\*training\\\\*, a scheduler adds noise (there are different algorithms for how to add noise) to a sample to train a diffusion model\\n- during \\\\*inference\\\\*, a scheduler defines how to update a sample based on a pretrained model\\'s output\\nMany schedulers are implemented from the [k-diffusion](https://github.com/crowsonkb/k-diffusion) library by [Katherine Crowson](https://github.com/crowsonkb/), and they\\'re also widely used in A1111. To help you map the schedulers from k-diffusion and A1111 to the schedulers in 🤗 Diffusers, take a look at the table below:\\n| A1111/k-diffusion | 🤗 Diffusers | Usage |\\n|---------------------|-------------------------------------|---------------------------------------------------------------------------------------------------------------|\\n| DPM++ 2M | [`DPMSolverMultistepScheduler`] | |\\n| DPM++ 2M Karras | [`DPMSolverMultistepScheduler`] | init with `use\\\\_karras\\\\_sigmas=True` |\\n| DPM++ 2M SDE | [`DPMSolverMultistepScheduler`] | init with `algorithm\\\\_type=\"sde-dpmsolver++\"` |\\n| DPM++ 2M SDE Karras | [`DPMSolverMultistepScheduler`] | init with `use\\\\_karras\\\\_sigmas=True` and `algorithm\\\\_type=\"sde-dpmsolver++\"` |\\n| DPM++ 2S a | N/A | very similar to `DPMSolverSinglestepScheduler` |\\n| DPM++ 2S a Karras | N/A | very similar to `DPMSolverSinglestepScheduler(use\\\\_karras\\\\_sigmas=True, ...)` |\\n| DPM++ SDE | [`DPMSolverSinglestepScheduler`] | |\\n| DPM++ SDE Karras | [`DPMSolverSinglestepScheduler`] | init with `use\\\\_karras\\\\_sigmas=True` |\\n| DPM2 | [`KDPM2DiscreteScheduler`] | |\\n| DPM2 Karras | [`KDPM2DiscreteScheduler`] | init with `use\\\\_karras\\\\_sigmas=True` |\\n| DPM2 a | [`KDPM2AncestralDiscreteScheduler`] | |\\n| DPM2 a Karras | [`KDPM2AncestralDiscreteScheduler`] | init with `use\\\\_karras\\\\_sigmas=True` |\\n| DPM adaptive | N/A | |\\n| DPM fast | N/A | |\\n| Euler | [`EulerDiscreteScheduler`] | |\\n| Euler a | [`EulerAncestralDiscreteScheduler`] | |\\n| Heun | [`HeunDiscreteScheduler`] | |\\n| LMS | [`LMSDiscreteScheduler`] | |\\n| LMS Karras | [`LMSDiscreteScheduler`] | init with `use\\\\_karras\\\\_sigmas=True` |\\n| N/A | [`DEISMultistepScheduler`] | |\\n| N/A | [`UniPCMultistepScheduler`] | |\\nAll schedulers are built from the base [`SchedulerMixin`] class which implements low level utilities shared by all schedulers. ## SchedulerMixin\\n[[autodoc]] SchedulerMixin\\n## SchedulerOutput\\n[[autodoc]] schedulers.scheduling\\\\_utils.SchedulerOutput\\n## KarrasDiffusionSchedulers\\n[`KarrasDiffusionSchedulers`] are a broad generalization of schedulers in 🤗 Diffusers.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# KDPM2AncestralDiscreteScheduler\\nThe `KDPM2DiscreteScheduler` with ancestral sampling is inspired by the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper, and the scheduler is ported from and created by [Katherine Crowson](https://github.com/crowsonkb/). The original codebase can be found at [crowsonkb/k-diffusion](https://github.com/crowsonkb/k-diffusion). ## KDPM2AncestralDiscreteScheduler\\n[[autodoc]] KDPM2AncestralDiscreteScheduler\\n## SchedulerOutput\\n[[autodoc]] schedulers.scheduling\\\\_utils.SchedulerOutput', 'type': 'tool', 'name': 'data_retriever', 'id': 'c5af41c8-9e27-441b-8f69-d87c80ca38ef', 'tool_call_id': 'call_5kvYyIXCIrTWUiJwyFvlq8wQ', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab8943e684812d15f6edd', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# EulerAncestralDiscreteScheduler\\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\\\_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerAncestralDiscreteScheduler\\n[[autodoc]] EulerAncestralDiscreteScheduler\\n## EulerAncestralDiscreteSchedulerOutput\\n[[autodoc]] schedulers.scheduling\\\\_euler\\\\_ancestral\\\\_discrete.EulerAncestralDiscreteSchedulerOutput', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3143e684812d15f6847', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Schedulers\\n🤗 Diffusers provides many scheduler functions for the diffusion process. A scheduler takes a model\\'s output (the sample which the diffusion process is iterating on) and a timestep to return a denoised sample. The timestep is important because it dictates where in the diffusion process the step is; data is generated by iterating forward \\\\*n\\\\* timesteps and inference occurs by propagating backward through the timesteps. Based on the timestep, a scheduler may be \\\\*discrete\\\\* in which case the timestep is an `int` or \\\\*continuous\\\\* in which case the timestep is a `float`. Depending on the context, a scheduler defines how to iteratively add noise to an image or how to update a sample based on a model\\'s output:\\n- during \\\\*training\\\\*, a scheduler adds noise (there are different algorithms for how to add noise) to a sample to train a diffusion model\\n- during \\\\*inference\\\\*, a scheduler defines how to update a sample based on a pretrained model\\'s output\\nMany schedulers are implemented from the [k-diffusion](https://github.com/crowsonkb/k-diffusion) library by [Katherine Crowson](https://github.com/crowsonkb/), and they\\'re also widely used in A1111. To help you map the schedulers from k-diffusion and A1111 to the schedulers in 🤗 Diffusers, take a look at the table below:\\n| A1111/k-diffusion | 🤗 Diffusers | Usage |\\n|---------------------|-------------------------------------|---------------------------------------------------------------------------------------------------------------|\\n| DPM++ 2M | [`DPMSolverMultistepScheduler`] | |\\n| DPM++ 2M Karras | [`DPMSolverMultistepScheduler`] | init with `use\\\\_karras\\\\_sigmas=True` |\\n| DPM++ 2M SDE | [`DPMSolverMultistepScheduler`] | init with `algorithm\\\\_type=\"sde-dpmsolver++\"` |\\n| DPM++ 2M SDE Karras | [`DPMSolverMultistepScheduler`] | init with `use\\\\_karras\\\\_sigmas=True` and `algorithm\\\\_type=\"sde-dpmsolver++\"` |\\n| DPM++ 2S a | N/A | very similar to `DPMSolverSinglestepScheduler` |\\n| DPM++ 2S a Karras | N/A | very similar to `DPMSolverSinglestepScheduler(use\\\\_karras\\\\_sigmas=True, ...)` |\\n| DPM++ SDE | [`DPMSolverSinglestepScheduler`] | |\\n| DPM++ SDE Karras | [`DPMSolverSinglestepScheduler`] | init with `use\\\\_karras\\\\_sigmas=True` |\\n| DPM2 | [`KDPM2DiscreteScheduler`] | |\\n| DPM2 Karras | [`KDPM2DiscreteScheduler`] | init with `use\\\\_karras\\\\_sigmas=True` |\\n| DPM2 a | [`KDPM2AncestralDiscreteScheduler`] | |\\n| DPM2 a Karras | [`KDPM2AncestralDiscreteScheduler`] | init with `use\\\\_karras\\\\_sigmas=True` |\\n| DPM adaptive | N/A | |\\n| DPM fast | N/A | |\\n| Euler | [`EulerDiscreteScheduler`] | |\\n| Euler a | [`EulerAncestralDiscreteScheduler`] | |\\n| Heun | [`HeunDiscreteScheduler`] | |\\n| LMS | [`LMSDiscreteScheduler`] | |\\n| LMS Karras | [`LMSDiscreteScheduler`] | init with `use\\\\_karras\\\\_sigmas=True` |\\n| N/A | [`DEISMultistepScheduler`] | |\\n| N/A | [`UniPCMultistepScheduler`] | |\\nAll schedulers are built from the base [`SchedulerMixin`] class which implements low level utilities shared by all schedulers. ## SchedulerMixin\\n[[autodoc]] SchedulerMixin\\n## SchedulerOutput\\n[[autodoc]] schedulers.scheduling\\\\_utils.SchedulerOutput\\n## KarrasDiffusionSchedulers\\n[`KarrasDiffusionSchedulers`] are a broad generalization of schedulers in 🤗 Diffusers.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bc7aaf3d2ebb270a0b17', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# KDPM2AncestralDiscreteScheduler\\nThe `KDPM2DiscreteScheduler` with ancestral sampling is inspired by the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper, and the scheduler is ported from and created by [Katherine Crowson](https://github.com/crowsonkb/). The original codebase can be found at [crowsonkb/k-diffusion](https://github.com/crowsonkb/k-diffusion). ## KDPM2AncestralDiscreteScheduler\\n[[autodoc]] KDPM2AncestralDiscreteScheduler\\n## SchedulerOutput\\n[[autodoc]] schedulers.scheduling\\\\_utils.SchedulerOutput', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The `EulerAncestralDiscreteScheduler` uses ancestral sampling with Euler method steps for sampling.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--a0026dd6-d53e-4b25-81d9-7f7409d4c3cd-0', 'usage_metadata': {'input_tokens': 1864, 'output_tokens': 19, 'total_tokens': 1883, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:35:45,262 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 14%|█▍        | 9/65 [00:39<03:54,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\\n', 'type': 'human', 'id': '1a13ce52-5dcd-4764-b2d9-0040c709d1bd'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_bxxggKwOHJuDJczChARi9Pvz', 'function': {'arguments': '{\"query\":\"large multimodal model based on Flamingo\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--6f340306-c143-43e5-92bb-5a43440fdf2e-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'large multimodal model based on Flamingo'}, 'id': 'call_bxxggKwOHJuDJczChARi9Pvz', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 371, 'output_tokens': 23, 'total_tokens': 394, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Image tasks with IDEFICS\\n[[open-in-colab]]\\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach\\nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. For instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can\\nsolve image-text tasks with a large multimodal model called IDEFICS. [IDEFICS](../model\\\\_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198),\\na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image\\nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content,\\ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b)\\nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the 🤗 Hub.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# FLAN-UL2\\n## Overview\\nFlan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same configuration as the [UL2](ul2) model released earlier last year. It was fine tuned using the \"Flan\" prompt tuning and dataset collection. Similar to `Flan-T5`, one can directly use FLAN-UL2 weights without finetuning the model:\\nAccording to the original blog here are the notable improvements:\\n- The original UL2 model was only trained with receptive field of 512, which made it non-ideal for N-shot prompting where N is large. - The Flan-UL2 checkpoint uses a receptive field of 2048 which makes it more usable for few-shot in-context learning.\\n\\n!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# FLAVA\\n## Overview\\nThe FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022. The paper aims at creating a single unified foundation model which can work across vision, language\\nas well as vision-and-language multimodal tasks. The abstract from the paper is the following:\\n\\\\*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety\\nof downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal\\n(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising\\ndirection would be to use a single holistic universal model, as a \"foundation\", that targets all modalities\\nat once -- a true vision and language foundation model should be good at vision tasks, language tasks, and\\ncross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate\\nimpressive performance on a wide range of 35 tasks spanning these target modalities.\\\\*\\nThis model was contributed by [aps](https://huggingface.co/aps). The original code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava). ## FlavaConfig\\n[[autodoc]] FlavaConfig\\n## FlavaTextConfig\\n[[autodoc]] FlavaTextConfig\\n## FlavaImageConfig\\n[[autodoc]] FlavaImageConfig\\n## FlavaMultimodalConfig\\n[[autodoc]] FlavaMultimodalConfig\\n## FlavaImageCodebookConfig\\n[[autodoc]] FlavaImageCodebookConfig\\n## FlavaProcessor\\n[[autodoc]] FlavaProcessor\\n## FlavaFeatureExtractor\\n[[autodoc]] FlavaFeatureExtractor\\n## FlavaImageProcessor\\n[[autodoc]] FlavaImageProcessor\\n- preprocess\\n## FlavaForPreTraining\\n[[autodoc]] FlavaForPreTraining\\n- forward\\n## FlavaModel\\n[[autodoc]] FlavaModel\\n- forward\\n- get\\\\_text\\\\_features\\n- get\\\\_image\\\\_features\\n## FlavaImageCodebook\\n[[autodoc]] FlavaImageCodebook\\n- forward\\n- get\\\\_codebook\\\\_indices\\n- get\\\\_codebook\\\\_probs\\n## FlavaTextModel\\n[[autodoc]] FlavaTextModel\\n- forward\\n## FlavaImageModel\\n[[autodoc]] FlavaImageModel\\n- forward\\n## FlavaMultimodalModel\\n[[autodoc]] FlavaMultimodalModel\\n- forward', 'type': 'tool', 'name': 'data_retriever', 'id': 'a4635d97-0e14-4984-9d85-de863d2472ef', 'tool_call_id': 'call_bxxggKwOHJuDJczChARi9Pvz', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b757af3d2ebb270a04d6', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Image tasks with IDEFICS\\n[[open-in-colab]]\\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach\\nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. For instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can\\nsolve image-text tasks with a large multimodal model called IDEFICS. [IDEFICS](../model\\\\_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198),\\na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image\\nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content,\\ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b)\\nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the 🤗 Hub.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa7a93e684812d15f5b15', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# FLAN-UL2\\n## Overview\\nFlan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same configuration as the [UL2](ul2) model released earlier last year. It was fine tuned using the \"Flan\" prompt tuning and dataset collection. Similar to `Flan-T5`, one can directly use FLAN-UL2 weights without finetuning the model:\\nAccording to the original blog here are the notable improvements:\\n- The original UL2 model was only trained with receptive field of 512, which made it non-ideal for N-shot prompting where N is large. - The Flan-UL2 checkpoint uses a receptive field of 2048 which makes it more usable for few-shot in-context learning.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa55f3e684812d15f5859', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# FLAVA\\n## Overview\\nThe FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022. The paper aims at creating a single unified foundation model which can work across vision, language\\nas well as vision-and-language multimodal tasks. The abstract from the paper is the following:\\n\\\\*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety\\nof downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal\\n(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising\\ndirection would be to use a single holistic universal model, as a \"foundation\", that targets all modalities\\nat once -- a true vision and language foundation model should be good at vision tasks, language tasks, and\\ncross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate\\nimpressive performance on a wide range of 35 tasks spanning these target modalities.\\\\*\\nThis model was contributed by [aps](https://huggingface.co/aps). The original code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava). ## FlavaConfig\\n[[autodoc]] FlavaConfig\\n## FlavaTextConfig\\n[[autodoc]] FlavaTextConfig\\n## FlavaImageConfig\\n[[autodoc]] FlavaImageConfig\\n## FlavaMultimodalConfig\\n[[autodoc]] FlavaMultimodalConfig\\n## FlavaImageCodebookConfig\\n[[autodoc]] FlavaImageCodebookConfig\\n## FlavaProcessor\\n[[autodoc]] FlavaProcessor\\n## FlavaFeatureExtractor\\n[[autodoc]] FlavaFeatureExtractor\\n## FlavaImageProcessor\\n[[autodoc]] FlavaImageProcessor\\n- preprocess\\n## FlavaForPreTraining\\n[[autodoc]] FlavaForPreTraining\\n- forward\\n## FlavaModel\\n[[autodoc]] FlavaModel\\n- forward\\n- get\\\\_text\\\\_features\\n- get\\\\_image\\\\_features\\n## FlavaImageCodebook\\n[[autodoc]] FlavaImageCodebook\\n- forward\\n- get\\\\_codebook\\\\_indices\\n- get\\\\_codebook\\\\_probs\\n## FlavaTextModel\\n[[autodoc]] FlavaTextModel\\n- forward\\n## FlavaImageModel\\n[[autodoc]] FlavaImageModel\\n- forward\\n## FlavaMultimodalModel\\n[[autodoc]] FlavaMultimodalModel\\n- forward', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The large multimodal model that can solve image-text tasks and is based on Flamingo is called IDEFICS.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--3350b110-5a3a-4394-906a-4c90c039de3e-0', 'usage_metadata': {'input_tokens': 1824, 'output_tokens': 23, 'total_tokens': 1847, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:35:50,580 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 15%|█▌        | 10/65 [00:44<04:09,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of the `gradio.Blocks` API?\\n', 'type': 'human', 'id': '3c40377b-c98c-430c-9c04-534103054a27'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_cRH0Csquvh7e0g9uSm9dHs9b', 'function': {'arguments': '{\"query\":\"gradio.Blocks API purpose\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--02e1b550-32de-407c-b297-924930f147e5-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'gradio.Blocks API purpose'}, 'id': 'call_cRH0Csquvh7e0g9uSm9dHs9b', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 21, 'total_tokens': 383, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our \\\\*\\\\*newly developed\\\\*\\\\* low-level API called `gradio.Blocks`. Now, what\\'s the difference between `Interface` and `Blocks`? - ⚡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs. - 🧱 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \"building blocks\"). ### Why Blocks 🧱?[[why-blocks-]]\\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\\n- Group together related demos as multiple tabs in one web application\\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\\n- Change a component\\'s properties (for example, the choices in a dropdown) or its visibility based on user input\\nWe will explore all of these concepts below. ### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook. ```py\\nimport gradio as gr\\ndef flip\\\\_text(x):\\nreturn x[::-1]\\ndemo = gr.Blocks()\\nwith demo:\\ngr.Markdown(\\n\"\"\"\\n# Flip Text! Start typing below to see the output. \"\"\"\\n)\\ninput = gr.Textbox(placeholder=\"Flip this text\")\\noutput = gr.Textbox()\\ninput.change(fn=flip\\\\_text, inputs=input, outputs=output)\\ndemo.launch()\\n```\\nThis simple example above introduces 4 concepts that underlie Blocks:\\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.\\n\\n### An Overview of Gradio\\nSo far, we\\'ve been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do? #### Chatbots with `gr.ChatInterface`\\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you\\'re interested in creating a chatbot, you can jump straight [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast). #### Custom Demos with `gr.Blocks`\\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction — still all in Python. You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners). #### The Gradio Python & JavaScript Ecosystem\\nThat\\'s the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\\n\\\\* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio\\\\_client`): query any Gradio app programmatically in Python. \\\\* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript. \\\\* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \\\\* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications — for free!\\n\\nAll components are now keyboard navigable and include better colors to be usable by a wider audience. ![](https://media0.giphy.com/media/Kv1bAN7MX3ya5krkEU/giphy.gif)\\n\\\\*\\\\*3. Server Side Events\\\\*\\\\*:\\nGradio\\'s built-in queuing system is now the default for every Gradio app. We now use Server Side Events instead of Websockets for the queue. SSE means everything is served over HTTP and has better device support and better scaling than websockets. ![](https://i.imgur.com/ewUIuUc.png)\\n\\\\*\\\\*4. Custom Share Servers\\\\*\\\\*:\\nGradio share links can now run on custom domains. You can now set up your own server to serve Gradio share links. To get started, [read our guide here](https://github.com/huggingface/frp/). ![](https://i.imgur.com/VFWVsqn.png)\\n5. We now support adding arbitrary JS to your apps using the `js` parameter in Blocks, and arbitrary modifications to the  of your app using the `head` parameter in Blocks\\n6. We no longer expose a user\\'s working directory by default when you release a Gradio app.', 'type': 'tool', 'name': 'data_retriever', 'id': '64e3ca4a-3192-4b70-8265-1c6321026fc9', 'tool_call_id': 'call_cRH0Csquvh7e0g9uSm9dHs9b', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa7693e684812d15f5ace', 'page_content': 'Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our \\\\*\\\\*newly developed\\\\*\\\\* low-level API called `gradio.Blocks`. Now, what\\'s the difference between `Interface` and `Blocks`? - ⚡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs. - 🧱 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \"building blocks\"). ### Why Blocks 🧱?[[why-blocks-]]\\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\\n- Group together related demos as multiple tabs in one web application\\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\\n- Change a component\\'s properties (for example, the choices in a dropdown) or its visibility based on user input\\nWe will explore all of these concepts below. ### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook. ```py\\nimport gradio as gr\\ndef flip\\\\_text(x):\\nreturn x[::-1]\\ndemo = gr.Blocks()\\nwith demo:\\ngr.Markdown(\\n\"\"\"\\n# Flip Text! Start typing below to see the output. \"\"\"\\n)\\ninput = gr.Textbox(placeholder=\"Flip this text\")\\noutput = gr.Textbox()\\ninput.change(fn=flip\\\\_text, inputs=input, outputs=output)\\ndemo.launch()\\n```\\nThis simple example above introduces 4 concepts that underlie Blocks:\\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c2d5666af48523f52207', 'page_content': \"### An Overview of Gradio\\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do? #### Chatbots with `gr.ChatInterface`\\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast). #### Custom Demos with `gr.Blocks`\\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction — still all in Python. You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners). #### The Gradio Python & JavaScript Ecosystem\\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\\n\\\\* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio\\\\_client`): query any Gradio app programmatically in Python. \\\\* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript. \\\\* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \\\\* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications — for free!\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b39baf3d2ebb270a0050', 'page_content': \"All components are now keyboard navigable and include better colors to be usable by a wider audience. ![](https://media0.giphy.com/media/Kv1bAN7MX3ya5krkEU/giphy.gif)\\n\\\\*\\\\*3. Server Side Events\\\\*\\\\*:\\nGradio's built-in queuing system is now the default for every Gradio app. We now use Server Side Events instead of Websockets for the queue. SSE means everything is served over HTTP and has better device support and better scaling than websockets. ![](https://i.imgur.com/ewUIuUc.png)\\n\\\\*\\\\*4. Custom Share Servers\\\\*\\\\*:\\nGradio share links can now run on custom domains. You can now set up your own server to serve Gradio share links. To get started, [read our guide here](https://github.com/huggingface/frp/). ![](https://i.imgur.com/VFWVsqn.png)\\n5. We now support adding arbitrary JS to your apps using the `js` parameter in Blocks, and arbitrary modifications to the  of your app using the `head` parameter in Blocks\\n6. We no longer expose a user's working directory by default when you release a Gradio app.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"The `gradio.Blocks` API is a low-level API that allows you to have full control over the data flows and layout of your application. It provides flexibility to build complex, multi-step applications by enabling you to:\\n\\n- Group related demos as multiple tabs in one web application.\\n- Change the layout of your demo, specifying where the inputs and outputs are located.\\n- Create multi-step interfaces, where the output of one model becomes the input to the next model, or have more flexible data flows.\\n- Modify a component's properties (e.g., choices in a dropdown) or its visibility based on user input.\\n\\nOverall, `gradio.Blocks` allows for more custom and complex applications compared to the high-level `Interface` class.\", 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--7a8b86d8-1523-4e98-acb6-55373534572e-0', 'usage_metadata': {'input_tokens': 1590, 'output_tokens': 149, 'total_tokens': 1739, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:35:56,410 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 17%|█▋        | 11/65 [00:50<04:26,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\\n', 'type': 'human', 'id': '6074ec8e-b78a-4524-b2e0-3d4f738ae728'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_DjPSL7mFZZnd6py9rgHqcFKG', 'function': {'arguments': '{\"query\":\"Hierarchical Text-Conditional Image Generation with CLIP Latents\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--81ecbdf0-4ab5-4684-9c68-cfebcd05e586-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Hierarchical Text-Conditional Image Generation with CLIP Latents'}, 'id': 'call_DjPSL7mFZZnd6py9rgHqcFKG', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 376, 'output_tokens': 27, 'total_tokens': 403, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# unCLIP\\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in 🤗 Diffusers comes from kakaobrain\\'s [karlo](https://github.com/kakaobrain/karlo). The abstract from the paper is following:\\n\\\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\\\*\\nYou can find lucidrains\\' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Prior Transformer\\nThe Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process. The abstract from the paper is:\\n\\\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\\\*\\n## PriorTransformer\\n[[autodoc]] PriorTransformer\\n## PriorTransformerOutput\\n[[autodoc]] models.prior\\\\_transformer.PriorTransformerOutput\\n\\nWhile CLIPSeg is trained on simple object descriptions (e.g., “a car”), its CLIP backbone is pre-trained on engineered text templates (e.g., “an image of a car”, “a photo of a car”) and kept frozen during training. Once the inputs are preprocessed, we can perform inference to get a binary segmentation map of shape (height, width) for each text query. ```py\\nimport torch\\nwith torch.no\\\\_grad():\\noutputs = model(\\\\*\\\\*inputs)\\nlogits = outputs.logits\\nprint(logits.shape)\\n>>> torch.Size([3, 352, 352])\\n```\\nLet’s visualize the results to see how well CLIPSeg performed (code is adapted from [this post](https://huggingface.co/blog/clipseg-zero-shot)). ```py\\nimport matplotlib.pyplot as plt\\nlogits = logits.unsqueeze(1)\\n\\\\_, ax = plt.subplots(1, len(texts) + 1, figsize=(3\\\\*(len(texts) + 1), 12))\\n[a.axis(\\'off\\') for a in ax.flatten()]\\nax[0].imshow(image)\\n[ax[i+1].imshow(torch.sigmoid(logits[i][0])) for i in range(len(texts))];\\n[ax[i+1].text(0, -15, prompt) for i, prompt in enumerate(texts)]\\n```\\n\\n![CLIPSeg results](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/clipseg_result.png)\\n\\nAmazing, isn’t it?', 'type': 'tool', 'name': 'data_retriever', 'id': '4310f156-7e39-45e2-9fd3-6ad9a0ee36aa', 'tool_call_id': 'call_DjPSL7mFZZnd6py9rgHqcFKG', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b493af3d2ebb270a01d4', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# unCLIP\\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in 🤗 Diffusers comes from kakaobrain\\'s [karlo](https://github.com/kakaobrain/karlo). The abstract from the paper is following:\\n\\\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\\\*\\nYou can find lucidrains\\' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c0ca666af48523f51f51', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Prior Transformer\\nThe Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process. The abstract from the paper is:\\n\\\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\\\*\\n## PriorTransformer\\n[[autodoc]] PriorTransformer\\n## PriorTransformerOutput\\n[[autodoc]] models.prior\\\\_transformer.PriorTransformerOutput', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b596af3d2ebb270a02e5', 'page_content': \"While CLIPSeg is trained on simple object descriptions (e.g., “a car”), its CLIP backbone is pre-trained on engineered text templates (e.g., “an image of a car”, “a photo of a car”) and kept frozen during training. Once the inputs are preprocessed, we can perform inference to get a binary segmentation map of shape (height, width) for each text query. ```py\\nimport torch\\nwith torch.no\\\\_grad():\\noutputs = model(\\\\*\\\\*inputs)\\nlogits = outputs.logits\\nprint(logits.shape)\\n>>> torch.Size([3, 352, 352])\\n```\\nLet’s visualize the results to see how well CLIPSeg performed (code is adapted from [this post](https://huggingface.co/blog/clipseg-zero-shot)). ```py\\nimport matplotlib.pyplot as plt\\nlogits = logits.unsqueeze(1)\\n\\\\_, ax = plt.subplots(1, len(texts) + 1, figsize=(3\\\\*(len(texts) + 1), 12))\\n[a.axis('off') for a in ax.flatten()]\\nax[0].imshow(image)\\n[ax[i+1].imshow(torch.sigmoid(logits[i][0])) for i in range(len(texts))];\\n[ax[i+1].text(0, -15, prompt) for i, prompt in enumerate(texts)]\\n```\\n\\n![CLIPSeg results](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/clipseg_result.png)\\n\\nAmazing, isn’t it?\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" is to leverage robust representations of images learned by contrastive models like CLIP for image generation. The model consists of a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. This approach aims to improve image diversity with minimal loss in photorealism and caption similarity, allowing for variations of an image that preserve its semantics and style while varying non-essential details. Additionally, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--354c853b-208d-44b2-b7e6-87d70408cbbd-0', 'usage_metadata': {'input_tokens': 1545, 'output_tokens': 134, 'total_tokens': 1679, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:36:00,081 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 18%|█▊        | 12/65 [00:54<04:01,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What command is used to install the requirements for a research project using 🤗 Transformers?\\n', 'type': 'human', 'id': '61a9b6dd-e98d-40fe-8c66-65e61aaed882'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_VcsvPTYk2WFCrVPSItuOzgR4', 'function': {'arguments': '{\"query\":\"install requirements for research project using 🤗 Transformers\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--b4bf40e2-e521-43bf-b446-a056b51e2163-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'install requirements for research project using 🤗 Transformers'}, 'id': 'call_VcsvPTYk2WFCrVPSItuOzgR4', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 24, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!---\\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n# Research projects\\nThis folder contains various research projects using 🤗 Transformers. They are not maintained and require a specific\\nversion of 🤗 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\\n```\\npip install -r requirements.txt\\n```\\ninside the folder of your choice. If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder.\\n\\now to add BigBird to 🤗 Transformers? =====================================\\nMentor: [Patrick](https://github.com/patrickvonplaten)\\nBegin: 12.02.2020\\nEstimated End: 19.03.2020\\nContributor: [Vasudev](https://github.com/thevasudevgupta)\\nAdding a new model is often difficult and requires an in-depth knowledge\\nof the 🤗 Transformers library and ideally also of the model\\'s original\\nrepository. At Hugging Face, we are trying to empower the community more\\nand more to add models independently. The following sections explain in detail how to add BigBird\\nto Transformers. You will work closely with Patrick to\\nintegrate BigBird into Transformers. By doing so, you will both gain a\\ntheoretical and deep practical understanding of BigBird. But more importantly, you will have made a major\\nopen-source contribution to Transformers. Along the way, you will:\\n- get insights into open-source best practices\\n- understand the design principles of one of the most popular NLP\\nlibraries\\n- learn how to do efficiently test large NLP models\\n- learn how to integrate Python utilities like `black`, `ruff`,\\n`make fix-copies` into a library to always ensure clean and readable\\ncode\\nTo start, let\\'s try to get a general overview of the Transformers\\nlibrary.\\n\\n\\\\* [TensorFlow 2.0](https://www.tensorflow.org/install/pip) installation instructions. \\\\* [Flax](https://flax.readthedocs.io/en/latest/) installation instructions. ## Install with pip\\nYou should install 🤗 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you\\'re unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies. Start by creating a virtual environment in your project directory:\\n```bash\\npython -m venv .env\\n```\\nActivate the virtual environment. On Linux and MacOs:\\n```bash\\nsource .env/bin/activate\\n```\\nActivate Virtual environment on Windows\\n```bash\\n.env/Scripts/activate\\n```\\nNow you\\'re ready to install 🤗 Transformers with the following command:\\n```bash\\npip install transformers\\n```\\nFor CPU-support only, you can conveniently install 🤗 Transformers and a deep learning library in one line. For example, install 🤗 Transformers and PyTorch with:\\n```bash\\npip install \\'transformers[torch]\\'\\n```\\n🤗 Transformers and TensorFlow 2.0:\\n```bash\\npip install \\'transformers[tf-cpu]\\'\\n```\\nM1 / ARM Users\\nYou will need to install the following before installing TensorFLow 2.0\\n```\\nbrew install cmake\\nbrew install pkg-config\\n```\\n🤗 Transformers and Flax:\\n```bash\\npip install \\'transformers[flax]\\'\\n```\\nFinally, check if 🤗 Transformers has been properly installed by running the following command. It will download a pretrained model:\\n```bash\\npython -c \"from transformers import pipeline; print(pipeline(\\'sentiment-analysis\\')(\\'we love you\\'))\"\\n```\\nThen print out the label and score:\\n```bash\\n[{\\'label\\': \\'POSITIVE\\', \\'score\\': 0.9998704791069031}]\\n```\\n## Install from source\\nInstall 🤗 Transformers from source with the following command:\\n```bash\\npip install git+https://github.com/huggingface/transformers\\n```\\nThis command installs the bleeding edge `main` version rather than the latest `stable` version.', 'type': 'tool', 'name': 'data_retriever', 'id': '996c563e-f447-4df3-9f9f-9e2e97078f86', 'tool_call_id': 'call_VcsvPTYk2WFCrVPSItuOzgR4', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b91aaf3d2ebb270a0725', 'page_content': '!---\\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n# Research projects\\nThis folder contains various research projects using 🤗 Transformers. They are not maintained and require a specific\\nversion of 🤗 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\\n```\\npip install -r requirements.txt\\n```\\ninside the folder of your choice. If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aba453e684812d15f70ae', 'page_content': \"ow to add BigBird to 🤗 Transformers? =====================================\\nMentor: [Patrick](https://github.com/patrickvonplaten)\\nBegin: 12.02.2020\\nEstimated End: 19.03.2020\\nContributor: [Vasudev](https://github.com/thevasudevgupta)\\nAdding a new model is often difficult and requires an in-depth knowledge\\nof the 🤗 Transformers library and ideally also of the model's original\\nrepository. At Hugging Face, we are trying to empower the community more\\nand more to add models independently. The following sections explain in detail how to add BigBird\\nto Transformers. You will work closely with Patrick to\\nintegrate BigBird into Transformers. By doing so, you will both gain a\\ntheoretical and deep practical understanding of BigBird. But more importantly, you will have made a major\\nopen-source contribution to Transformers. Along the way, you will:\\n- get insights into open-source best practices\\n- understand the design principles of one of the most popular NLP\\nlibraries\\n- learn how to do efficiently test large NLP models\\n- learn how to integrate Python utilities like `black`, `ruff`,\\n`make fix-copies` into a library to always ensure clean and readable\\ncode\\nTo start, let's try to get a general overview of the Transformers\\nlibrary.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa6af3e684812d15f59df', 'page_content': '\\\\* [TensorFlow 2.0](https://www.tensorflow.org/install/pip) installation instructions. \\\\* [Flax](https://flax.readthedocs.io/en/latest/) installation instructions. ## Install with pip\\nYou should install 🤗 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you\\'re unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies. Start by creating a virtual environment in your project directory:\\n```bash\\npython -m venv .env\\n```\\nActivate the virtual environment. On Linux and MacOs:\\n```bash\\nsource .env/bin/activate\\n```\\nActivate Virtual environment on Windows\\n```bash\\n.env/Scripts/activate\\n```\\nNow you\\'re ready to install 🤗 Transformers with the following command:\\n```bash\\npip install transformers\\n```\\nFor CPU-support only, you can conveniently install 🤗 Transformers and a deep learning library in one line. For example, install 🤗 Transformers and PyTorch with:\\n```bash\\npip install \\'transformers[torch]\\'\\n```\\n🤗 Transformers and TensorFlow 2.0:\\n```bash\\npip install \\'transformers[tf-cpu]\\'\\n```\\nM1 / ARM Users\\nYou will need to install the following before installing TensorFLow 2.0\\n```\\nbrew install cmake\\nbrew install pkg-config\\n```\\n🤗 Transformers and Flax:\\n```bash\\npip install \\'transformers[flax]\\'\\n```\\nFinally, check if 🤗 Transformers has been properly installed by running the following command. It will download a pretrained model:\\n```bash\\npython -c \"from transformers import pipeline; print(pipeline(\\'sentiment-analysis\\')(\\'we love you\\'))\"\\n```\\nThen print out the label and score:\\n```bash\\n[{\\'label\\': \\'POSITIVE\\', \\'score\\': 0.9998704791069031}]\\n```\\n## Install from source\\nInstall 🤗 Transformers from source with the following command:\\n```bash\\npip install git+https://github.com/huggingface/transformers\\n```\\nThis command installs the bleeding edge `main` version rather than the latest `stable` version.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The command used to install the requirements for a research project using 🤗 Transformers is:\\n\\n```\\npip install -r requirements.txt\\n``` \\n\\nYou should run this command inside the folder of your choice.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--4d27748f-0ce6-4501-b6cf-06095cf39435-0', 'usage_metadata': {'input_tokens': 1278, 'output_tokens': 40, 'total_tokens': 1318, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:36:04,318 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 20%|██        | 13/65 [00:58<03:51,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What task does the `roberta-large-mnli` checkpoint perform?\\n', 'type': 'human', 'id': '68fef62c-d153-4505-9d73-d00e09efde1a'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_fWulDvJt5HIVf1BwCu55zQPv', 'function': {'arguments': '{\"query\":\"roberta-large-mnli checkpoint task\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--4bcc7b79-196b-4987-8018-0e863e7c9404-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'roberta-large-mnli checkpoint task'}, 'id': 'call_fWulDvJt5HIVf1BwCu55zQPv', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 363, 'output_tokens': 23, 'total_tokens': 386, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!-- DISABLE-FRONTMATTER-SECTIONS -->\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\nThis chapter covered a lot of ground! Don\\'t worry if you didn\\'t grasp all the details; the next chapters will help you understand how things work under the hood. First, though, let\\'s test what you learned in this chapter! ### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform? roberta-large-mnli page.\"\\n},\\n{\\ntext: \"Text classification\",\\nexplain: \"More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) — a task also called *natural language inference*.\",\\ncorrect: true\\n},\\n{\\ntext: \"Text generation\",\\nexplain: \"Look again on the [roberta-large-mnli page](\\\\\"https://huggingface.co/roberta-large-mnli\\\\\").\"\\n}\\n]}\\n/>\\n### 2. What will the following code return? ```py\\nfrom transformers import pipeline\\nner = pipeline(\"ner\", grouped\\\\_entities=True)\\nner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\\n```\\nsentiment-analysis pipeline.\"\\n},\\n{\\ntext: \"It will return a generated text completing this sentence.\",\\nexplain: \"This is incorrect — it would be a `text-generation` pipeline.\",\\n},\\n{\\ntext: \"It will return the words representing persons, organizations or locations.\",\\nexplain: \"Furthermore, with `grouped_entities=True`, it will group together the words belonging to the same entity, like \\\\\"Hugging Face\\\\\".\",\\ncorrect: true\\n}\\n]}\\n/>\\n### 3.\\n\\n- [Question answering task guide](../tasks/question\\\\_answering)\\n\\\\*\\\\*Multiple choice\\\\*\\\\*\\n- [`RobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice.ipynb). - [`TFRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice-tf.ipynb). - [Multiple choice task guide](../tasks/multiple\\\\_choice)\\n## RobertaConfig\\n[[autodoc]] RobertaConfig\\n## RobertaTokenizer\\n[[autodoc]] RobertaTokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## RobertaTokenizerFast\\n[[autodoc]] RobertaTokenizerFast\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n\\n## RobertaModel\\n[[autodoc]] RobertaModel\\n- forward\\n## RobertaForCausalLM\\n[[autodoc]] RobertaForCausalLM\\n- forward\\n## RobertaForMaskedLM\\n[[autodoc]] RobertaForMaskedLM\\n- forward\\n## RobertaForSequenceClassification\\n[[autodoc]] RobertaForSequenceClassification\\n- forward\\n## RobertaForMultipleChoice\\n[[autodoc]] RobertaForMultipleChoice\\n- forward\\n## RobertaForTokenClassification\\n[[autodoc]] RobertaForTokenClassification\\n- forward\\n## RobertaForQuestionAnswering\\n[[autodoc]] RobertaForQuestionAnswering\\n- forward\\n\\n## TFRobertaModel\\n[[autodoc]] TFRobertaModel\\n- call\\n## TFRobertaForCausalLM\\n[[autodoc]] TFRobertaForCausalLM\\n- call\\n## TFRobertaForMaskedLM\\n[[autodoc]] TFRobertaForMaskedLM\\n- call\\n## TFRobertaForSequenceClassification\\n[[autodoc]] TFRobertaForSequenceClassification\\n- call\\n## TFRobertaForMultipleChoice\\n[[autodoc]] TFRobertaForMultipleChoice\\n- call\\n## TFRobertaForTokenClassification\\n[[autodoc]] TFRobertaForTokenClassification\\n- call\\n## TFRobertaForQuestionAnswering\\n[[autodoc]] TFRobertaForQuestionAnswering\\n- call\\n\\n## FlaxRobertaModel\\n[[autodoc]] FlaxRobertaModel\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForCausalLM\\n[[autodoc]] FlaxRobertaForCausalLM\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForMaskedLM\\n[[autodoc]] FlaxRobertaForMaskedLM\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForSequenceClassification\\n[[autodoc]] FlaxRobertaForSequenceClassification\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForMultipleChoice\\n[[autodoc]] FlaxRobertaForMultipleChoice\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForTokenClassification\\n[[autodoc]] FlaxRobertaForTokenClassification\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForQuestionAnswering\\n[[autodoc]] FlaxRobertaForQuestionAnswering\\n- \\\\_\\\\_call\\\\_\\\\_\\n\\n- [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the 🤗 Hugging Face Course. - [Masked language modeling task guide](../tasks/masked\\\\_language\\\\_modeling)\\n- A blog on [Accelerated Inference with Optimum and Transformers Pipelines](https://huggingface.co/blog/optimum-inference) with RoBERTa for question answering. - [`RobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question\\\\_answering.ipynb). - [`TFRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question\\\\_answering-tf.ipynb). - [`FlaxRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering). - [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the 🤗 Hugging Face Course.', 'type': 'tool', 'name': 'data_retriever', 'id': '2113b9f6-7ca6-45b1-9676-9cf7bd4587e2', 'tool_call_id': 'call_fWulDvJt5HIVf1BwCu55zQPv', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab30a3e684812d15f683f', 'page_content': '!-- DISABLE-FRONTMATTER-SECTIONS -->\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\nThis chapter covered a lot of ground! Don\\'t worry if you didn\\'t grasp all the details; the next chapters will help you understand how things work under the hood. First, though, let\\'s test what you learned in this chapter! ### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform? roberta-large-mnli page.\"\\n},\\n{\\ntext: \"Text classification\",\\nexplain: \"More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) — a task also called *natural language inference*.\",\\ncorrect: true\\n},\\n{\\ntext: \"Text generation\",\\nexplain: \"Look again on the [roberta-large-mnli page](\\\\\"https://huggingface.co/roberta-large-mnli\\\\\").\"\\n}\\n]}\\n/>\\n### 2. What will the following code return? ```py\\nfrom transformers import pipeline\\nner = pipeline(\"ner\", grouped\\\\_entities=True)\\nner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\\n```\\nsentiment-analysis pipeline.\"\\n},\\n{\\ntext: \"It will return a generated text completing this sentence.\",\\nexplain: \"This is incorrect — it would be a `text-generation` pipeline.\",\\n},\\n{\\ntext: \"It will return the words representing persons, organizations or locations.\",\\nexplain: \"Furthermore, with `grouped_entities=True`, it will group together the words belonging to the same entity, like \\\\\"Hugging Face\\\\\".\",\\ncorrect: true\\n}\\n]}\\n/>\\n### 3.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bc8daf3d2ebb270a0b2c', 'page_content': '- [Question answering task guide](../tasks/question\\\\_answering)\\n\\\\*\\\\*Multiple choice\\\\*\\\\*\\n- [`RobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice.ipynb). - [`TFRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\\\_choice-tf.ipynb). - [Multiple choice task guide](../tasks/multiple\\\\_choice)\\n## RobertaConfig\\n[[autodoc]] RobertaConfig\\n## RobertaTokenizer\\n[[autodoc]] RobertaTokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## RobertaTokenizerFast\\n[[autodoc]] RobertaTokenizerFast\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n\\n## RobertaModel\\n[[autodoc]] RobertaModel\\n- forward\\n## RobertaForCausalLM\\n[[autodoc]] RobertaForCausalLM\\n- forward\\n## RobertaForMaskedLM\\n[[autodoc]] RobertaForMaskedLM\\n- forward\\n## RobertaForSequenceClassification\\n[[autodoc]] RobertaForSequenceClassification\\n- forward\\n## RobertaForMultipleChoice\\n[[autodoc]] RobertaForMultipleChoice\\n- forward\\n## RobertaForTokenClassification\\n[[autodoc]] RobertaForTokenClassification\\n- forward\\n## RobertaForQuestionAnswering\\n[[autodoc]] RobertaForQuestionAnswering\\n- forward\\n\\n## TFRobertaModel\\n[[autodoc]] TFRobertaModel\\n- call\\n## TFRobertaForCausalLM\\n[[autodoc]] TFRobertaForCausalLM\\n- call\\n## TFRobertaForMaskedLM\\n[[autodoc]] TFRobertaForMaskedLM\\n- call\\n## TFRobertaForSequenceClassification\\n[[autodoc]] TFRobertaForSequenceClassification\\n- call\\n## TFRobertaForMultipleChoice\\n[[autodoc]] TFRobertaForMultipleChoice\\n- call\\n## TFRobertaForTokenClassification\\n[[autodoc]] TFRobertaForTokenClassification\\n- call\\n## TFRobertaForQuestionAnswering\\n[[autodoc]] TFRobertaForQuestionAnswering\\n- call\\n\\n## FlaxRobertaModel\\n[[autodoc]] FlaxRobertaModel\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForCausalLM\\n[[autodoc]] FlaxRobertaForCausalLM\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForMaskedLM\\n[[autodoc]] FlaxRobertaForMaskedLM\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForSequenceClassification\\n[[autodoc]] FlaxRobertaForSequenceClassification\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForMultipleChoice\\n[[autodoc]] FlaxRobertaForMultipleChoice\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForTokenClassification\\n[[autodoc]] FlaxRobertaForTokenClassification\\n- \\\\_\\\\_call\\\\_\\\\_\\n## FlaxRobertaForQuestionAnswering\\n[[autodoc]] FlaxRobertaForQuestionAnswering\\n- \\\\_\\\\_call\\\\_\\\\_', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bc8daf3d2ebb270a0b2b', 'page_content': '- [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the 🤗 Hugging Face Course. - [Masked language modeling task guide](../tasks/masked\\\\_language\\\\_modeling)\\n- A blog on [Accelerated Inference with Optimum and Transformers Pipelines](https://huggingface.co/blog/optimum-inference) with RoBERTa for question answering. - [`RobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question\\\\_answering.ipynb). - [`TFRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question\\\\_answering-tf.ipynb). - [`FlaxRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering). - [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the 🤗 Hugging Face Course.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The `roberta-large-mnli` checkpoint performs text classification, specifically classifying if two sentences are logically linked across three labels: contradiction, neutral, and entailment. This task is also known as natural language inference.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--3814fcdd-f770-4d93-bd64-c7e57834f98d-0', 'usage_metadata': {'input_tokens': 1757, 'output_tokens': 45, 'total_tokens': 1802, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:36:07,695 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 22%|██▏       | 14/65 [01:02<03:30,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What service is replacing the Paid tier of the Inference API at Hugging Face?\\n', 'type': 'human', 'id': 'e54145c8-63bb-46b1-8ff1-0ee2b6f50f7c'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_CZ4eQbHcL0FVSEvORvuPSM1H', 'function': {'arguments': '{\"query\":\"Paid tier of the Inference API replacement service Hugging Face\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--1c985bfd-f5ac-4058-b442-c2e14a381fa4-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Paid tier of the Inference API replacement service Hugging Face'}, 'id': 'call_CZ4eQbHcL0FVSEvORvuPSM1H', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 27, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': \"--\\ntitle: Introducing our new pricing\\nthumbnail: /blog/assets/114\\\\_pricing-update/thumbnail.png\\nauthors:\\n- user: sbrandeis\\n- user: pierric\\n---\\n# Introducing our new pricing\\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently. First of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints). Along with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations). Your billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records. \\\\*\\\\*TL;DR\\\\*\\\\*: \\\\*\\\\*At HF we monetize by providing simple access to compute for AI\\\\*\\\\*, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub. [Read more](https://huggingface.co/docs/hub/billing) about our pricing and billing system. If you have any questions, feel free to reach out. We welcome your feedback 🔥\\n\\nInference Endpoints\\nInference Endpoints provides a secure production solution to easily deploy models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the [Hub](https://huggingface.co/models). This page is a reference for `huggingface\\\\_hub`'s integration with Inference Endpoints. For more information about the Inference Endpoints product, check out its [official documentation](https://huggingface.co/docs/inference-endpoints/index). Check out the [related guide](../guides/inference\\\\_endpoints) to learn how to use `huggingface\\\\_hub` to manage your Inference Endpoints programmatically. Inference Endpoints can be fully managed via API. The endpoints are documented with [Swagger](https://api.endpoints.huggingface.cloud/). The [`InferenceEndpoint`] class is a simple wrapper built on top on this API. ## Methods\\nA subset of the Inference Endpoint features are implemented in [`HfApi`]:\\n- [`get\\\\_inference\\\\_endpoint`] and [`list\\\\_inference\\\\_endpoints`] to get information about your Inference Endpoints\\n- [`create\\\\_inference\\\\_endpoint`], [`update\\\\_inference\\\\_endpoint`] and [`delete\\\\_inference\\\\_endpoint`] to deploy and manage Inference Endpoints\\n- [`pause\\\\_inference\\\\_endpoint`] and [`resume\\\\_inference\\\\_endpoint`] to pause and resume an Inference Endpoint\\n- [`scale\\\\_to\\\\_zero\\\\_inference\\\\_endpoint`] to manually scale an Endpoint to 0 replicas\\n## InferenceEndpoint\\nThe main dataclass is [`InferenceEndpoint`].\\n\\nAccess 🤗 Inference Endpoints\\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account. You can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method. There are two pricing plans:\\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. - There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested! After you've added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying!\", 'type': 'tool', 'name': 'data_retriever', 'id': 'e269bb74-6248-40a7-ba72-b248b7eee773', 'tool_call_id': 'call_CZ4eQbHcL0FVSEvORvuPSM1H', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b5d3af3d2ebb270a0321', 'page_content': \"--\\ntitle: Introducing our new pricing\\nthumbnail: /blog/assets/114\\\\_pricing-update/thumbnail.png\\nauthors:\\n- user: sbrandeis\\n- user: pierric\\n---\\n# Introducing our new pricing\\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently. First of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints). Along with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations). Your billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records. \\\\*\\\\*TL;DR\\\\*\\\\*: \\\\*\\\\*At HF we monetize by providing simple access to compute for AI\\\\*\\\\*, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub. [Read more](https://huggingface.co/docs/hub/billing) about our pricing and billing system. If you have any questions, feel free to reach out. We welcome your feedback 🔥\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bc90af3d2ebb270a0b2e', 'page_content': \"Inference Endpoints\\nInference Endpoints provides a secure production solution to easily deploy models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the [Hub](https://huggingface.co/models). This page is a reference for `huggingface\\\\_hub`'s integration with Inference Endpoints. For more information about the Inference Endpoints product, check out its [official documentation](https://huggingface.co/docs/inference-endpoints/index). Check out the [related guide](../guides/inference\\\\_endpoints) to learn how to use `huggingface\\\\_hub` to manage your Inference Endpoints programmatically. Inference Endpoints can be fully managed via API. The endpoints are documented with [Swagger](https://api.endpoints.huggingface.cloud/). The [`InferenceEndpoint`] class is a simple wrapper built on top on this API. ## Methods\\nA subset of the Inference Endpoint features are implemented in [`HfApi`]:\\n- [`get\\\\_inference\\\\_endpoint`] and [`list\\\\_inference\\\\_endpoints`] to get information about your Inference Endpoints\\n- [`create\\\\_inference\\\\_endpoint`], [`update\\\\_inference\\\\_endpoint`] and [`delete\\\\_inference\\\\_endpoint`] to deploy and manage Inference Endpoints\\n- [`pause\\\\_inference\\\\_endpoint`] and [`resume\\\\_inference\\\\_endpoint`] to pause and resume an Inference Endpoint\\n- [`scale\\\\_to\\\\_zero\\\\_inference\\\\_endpoint`] to manually scale an Endpoint to 0 replicas\\n## InferenceEndpoint\\nThe main dataclass is [`InferenceEndpoint`].\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aab343e684812d15f5f3c', 'page_content': \"Access 🤗 Inference Endpoints\\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account. You can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method. There are two pricing plans:\\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. - There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested! After you've added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying!\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The Paid tier of the Inference API service is being replaced by the new solution called **Inference Endpoints**.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--c8fef7e8-a6bc-437d-848d-fc79d8edd005-0', 'usage_metadata': {'input_tokens': 1288, 'output_tokens': 23, 'total_tokens': 1311, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:36:11,982 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 23%|██▎       | 15/65 [01:06<03:28,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\\n', 'type': 'human', 'id': '4348cb03-ed72-4a79-9e57-65dffdcfa400'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_pcGhJQEsWSGd0aoSAO6kRZys', 'function': {'arguments': '{\"query\":\"SqueezeBERT architectural feature instead of fully-connected layers for Q, K, V, and FFN layers\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--aa7df7b1-67fd-4737-9e18-bae68f997bf6-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'SqueezeBERT architectural feature instead of fully-connected layers for Q, K, V, and FFN layers'}, 'id': 'call_pcGhJQEsWSGd0aoSAO6kRZys', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 375, 'output_tokens': 37, 'total_tokens': 412, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# SqueezeBERT\\n## Overview\\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It\\'s a\\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\\ninstead of fully-connected layers for the Q, K, V and FFN layers.\\n\\nIn the original model it is possible to use an average pooling layer with a fixed 7x7 window and stride 1 instead of global pooling. For inputs that are larger than the recommended image size, this gives a pooled output that is larger than 1x1. The Hugging Face implementation does not support this. - The original TensorFlow checkpoints include quantized models. We do not support these models as they include additional \"FakeQuantization\" operations to unquantize the weights. - It\\'s common to extract the output from the expansion layers at indices 10 and 13, as well as the output from the final 1x1 convolution layer, for downstream purposes. Using `output\\\\_hidden\\\\_states=True` returns the output from all intermediate layers. There is currently no way to limit this to specific layers. - The DeepLabV3+ segmentation head does not use the final convolution layer from the backbone, but this layer gets computed anyway. There is currently no way to tell [`MobileNetV2Model`] up to which layer it should run.\\n\\nEfficientNet (Knapsack Pruned)\\n\\\\*\\\\*EfficientNet\\\\*\\\\* is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a \\\\*compound coefficient\\\\*. Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use \\\\\\\\( 2^N \\\\\\\\) times more computational resources, then we can simply increase the network depth by \\\\\\\\( \\\\alpha ^ N \\\\\\\\), width by \\\\\\\\( \\\\beta ^ N \\\\\\\\), and image size by \\\\\\\\( \\\\gamma ^ N \\\\\\\\), where \\\\\\\\( \\\\alpha, \\\\beta, \\\\gamma \\\\\\\\) are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\\\\\\\( \\\\phi \\\\\\\\) to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of [MobileNetV2](https://paperswithcode.com/method/mobilenetv2), in addition to [squeeze-and-excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block).', 'type': 'tool', 'name': 'data_retriever', 'id': '0cadf97f-9d72-42af-91fc-dbc823c2da2f', 'tool_call_id': 'call_pcGhJQEsWSGd0aoSAO6kRZys', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaa5a3e684812d15f5e3f', 'page_content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# SqueezeBERT\\n## Overview\\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It\\'s a\\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\\ninstead of fully-connected layers for the Q, K, V and FFN layers.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aadeb3e684812d15f62a1', 'page_content': 'In the original model it is possible to use an average pooling layer with a fixed 7x7 window and stride 1 instead of global pooling. For inputs that are larger than the recommended image size, this gives a pooled output that is larger than 1x1. The Hugging Face implementation does not support this. - The original TensorFlow checkpoints include quantized models. We do not support these models as they include additional \"FakeQuantization\" operations to unquantize the weights. - It\\'s common to extract the output from the expansion layers at indices 10 and 13, as well as the output from the final 1x1 convolution layer, for downstream purposes. Using `output\\\\_hidden\\\\_states=True` returns the output from all intermediate layers. There is currently no way to limit this to specific layers. - The DeepLabV3+ segmentation head does not use the final convolution layer from the backbone, but this layer gets computed anyway. There is currently no way to tell [`MobileNetV2Model`] up to which layer it should run.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b681af3d2ebb270a03ce', 'page_content': 'EfficientNet (Knapsack Pruned)\\n\\\\*\\\\*EfficientNet\\\\*\\\\* is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a \\\\*compound coefficient\\\\*. Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use \\\\\\\\( 2^N \\\\\\\\) times more computational resources, then we can simply increase the network depth by \\\\\\\\( \\\\alpha ^ N \\\\\\\\), width by \\\\\\\\( \\\\beta ^ N \\\\\\\\), and image size by \\\\\\\\( \\\\gamma ^ N \\\\\\\\), where \\\\\\\\( \\\\alpha, \\\\beta, \\\\gamma \\\\\\\\) are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\\\\\\\( \\\\phi \\\\\\\\) to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of [MobileNetV2](https://paperswithcode.com/method/mobilenetv2), in addition to [squeeze-and-excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block).', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f4040b09-f11d-41da-9838-e8b3652aac3f-0', 'usage_metadata': {'input_tokens': 1107, 'output_tokens': 26, 'total_tokens': 1133, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:36:15,607 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 25%|██▍       | 16/65 [01:10<03:16,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': \"What type of license is the HuggingFace Team's software distributed under?\\n\", 'type': 'human', 'id': 'dbf9cc46-c68d-408d-9542-c657367c8470'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_PEUxMhL48gltaLclBDmg4lBd', 'function': {'arguments': '{\"query\":\"HuggingFace software license\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--00374c1e-7a99-40b1-b521-21c5f9f272ca-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'HuggingFace software license'}, 'id': 'call_PEUxMhL48gltaLclBDmg4lBd', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 363, 'output_tokens': 21, 'total_tokens': 384, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Under construction\\n\\nThe ability to use Hugging Face’s comprehensive hub of foundation models means that users can pick the models that best align with their business needs without having to invest in the resources required to train them. This integration is a significant step forward in making foundation models more accessible to enterprises around the world. \\\\_If you’re interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!\\\\_\\n\\nSecurity\\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you\\'ll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\\n- [User Access Tokens](./security-tokens)\\n- [Git over SSH](./security-git-ssh)\\n- [Signing commits with GPG](./security-gpg)\\n- [Single Sign-On (SSO)](./security-sso)\\n- [Malware Scanning](./security-malware)\\n- [Pickle Scanning](./security-pickle)\\n- [Secrets Scanning](./security-secrets)', 'type': 'tool', 'name': 'data_retriever', 'id': '24f4920f-ae3b-40d8-9495-068a77e6f0fa', 'tool_call_id': 'call_PEUxMhL48gltaLclBDmg4lBd', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b9e6af3d2ebb270a082f', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Under construction', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab37e3e684812d15f68c2', 'page_content': 'The ability to use Hugging Face’s comprehensive hub of foundation models means that users can pick the models that best align with their business needs without having to invest in the resources required to train them. This integration is a significant step forward in making foundation models more accessible to enterprises around the world. \\\\_If you’re interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!\\\\_', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aae003e684812d15f62c7', 'page_content': \"Security\\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\\n- [User Access Tokens](./security-tokens)\\n- [Git over SSH](./security-git-ssh)\\n- [Signing commits with GPG](./security-gpg)\\n- [Single Sign-On (SSO)](./security-sso)\\n- [Malware Scanning](./security-malware)\\n- [Pickle Scanning](./security-pickle)\\n- [Secrets Scanning](./security-secrets)\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"The HuggingFace Team's software is distributed under the Apache License, Version 2.0.\", 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--992ea6a9-1d22-4c45-9505-cc226affc056-0', 'usage_metadata': {'input_tokens': 835, 'output_tokens': 20, 'total_tokens': 855, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:36:20,791 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 26%|██▌       | 17/65 [01:15<03:29,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\\n', 'type': 'human', 'id': '1d860e8f-bc92-4057-b466-11ca11a37c64'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_y9YIjx98Y0Gk05nczTRFI5oh', 'function': {'arguments': '{\"query\":\"ALBERT model parameter-reduction techniques\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--7bd0e3fb-8b75-4907-ba81-8afb990e0261-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'ALBERT model parameter-reduction techniques'}, 'id': 'call_y9YIjx98Y0Gk05nczTRFI5oh', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 372, 'output_tokens': 23, 'total_tokens': 395, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'It presents two parameter-reduction techniques to lower memory consumption and increase the training\\nspeed of BERT:\\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\\n\\\\*Increasing model size when pretraining natural language representations often results in improved performance on\\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\\\*\\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\\n- ALBERT is a model with absolute position embeddings so it\\'s usually advised to pad the inputs on the right rather\\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it\\'s more logical to have H >> E. Also, the embedding matrix is large since it\\'s V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A.\\n\\nHowever, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model\\\\_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model\\\\_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model\\\\_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix. ### Decoder[[nlp-decoder]]\\n[GPT-2](model\\\\_doc/gpt2) is a decoder-only Transformer that predicts the next word in the sequence.\\n\\n!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# ALBERT\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-albert-blueviolet)](https://huggingface.co/models?filter=albert)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/albert-base-v2)\\n\\n## Overview\\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\\nRadu Soricut.', 'type': 'tool', 'name': 'data_retriever', 'id': '040b21b8-dd39-4e0e-9f40-ec928884f323', 'tool_call_id': 'call_y9YIjx98Y0Gk05nczTRFI5oh', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3d33e684812d15f692e', 'page_content': \"It presents two parameter-reduction techniques to lower memory consumption and increase the training\\nspeed of BERT:\\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\\n\\\\*Increasing model size when pretraining natural language representations often results in improved performance on\\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\\\*\\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b99baf3d2ebb270a07d0', 'page_content': 'However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model\\\\_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model\\\\_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model\\\\_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix. ### Decoder[[nlp-decoder]]\\n[GPT-2](model\\\\_doc/gpt2) is a decoder-only Transformer that predicts the next word in the sequence.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3d33e684812d15f692d', 'page_content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# ALBERT\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-albert-blueviolet)](https://huggingface.co/models?filter=albert)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/albert-base-v2)\\n\\n## Overview\\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\\nRadu Soricut.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:\\n\\n1. Splitting the embedding matrix into two smaller matrices.\\n2. Using repeating layers split among groups.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f6790cea-c7cd-4df1-8a58-1ab6119752cf-0', 'usage_metadata': {'input_tokens': 1347, 'output_tokens': 44, 'total_tokens': 1391, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:36:24,985 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 28%|██▊       | 18/65 [01:19<03:22,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What are the three main steps for fine-tuning a model with the 🤗 Datasets library?\\n', 'type': 'human', 'id': 'a30b6418-3429-4a19-9fe5-8f0014c350c5'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_ioMpixaEVESzUSP0aeyTSWfV', 'function': {'arguments': '{\"query\":\"three main steps for fine-tuning a model with the 🤗 Datasets library\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--06b7f04f-c5e1-40a7-8f39-8c409bdebc13-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'three main steps for fine-tuning a model with the 🤗 Datasets library'}, 'id': 'call_ioMpixaEVESzUSP0aeyTSWfV', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 368, 'output_tokens': 31, 'total_tokens': 399, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Introduction[[introduction]]\\nIn [Chapter 3](/course/chapter3) you got your first taste of the 🤗 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\\n1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics. But this is just scratching the surface of what 🤗 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we\\'ll find answers to the following questions:\\n\\\\* What do you do when your dataset is not on the Hub? \\\\* How can you slice and dice a dataset? (And what if you \\\\_really\\\\_ need to use Pandas?)\\n\\\\* What do you do when your dataset is huge and will melt your laptop\\'s RAM? \\\\* What the heck are \"memory mapping\" and Apache Arrow? \\\\* How can you create your own dataset and push it to the Hub? The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let\\'s get started!\\n\\nTo verify that all libraries are correctly installed, you can run the following command in a Python shell. It verifies that both `transformers` and `datasets` have been correclty installed. ```python\\nfrom transformers import AutoModelForCTC, AutoProcessor\\nfrom datasets import load\\\\_dataset\\ndummy\\\\_dataset = load\\\\_dataset(\"common\\\\_voice\", \"ab\", split=\"test\")\\nmodel = AutoModelForCTC.from\\\\_pretrained(\"hf-internal-testing/tiny-random-wav2vec2\")\\nmodel.to(\"cuda\")\\nprocessor = AutoProcessor.from\\\\_pretrained(\"hf-internal-testing/tiny-random-wav2vec2\")\\ninput\\\\_values = processor(dummy\\\\_dataset[0][\"audio\"][\"array\"], return\\\\_tensors=\"pt\", sampling\\\\_rate=16\\\\_000).input\\\\_values\\ninput\\\\_values = input\\\\_values.to(\"cuda\")\\nlogits = model(input\\\\_values).logits\\nassert logits.shape[-1] == 32\\n```\\n## How to finetune an acoustic model\\nIn this section, we show you how to fine-tune a pre-trained [XLS-R Model](https://huggingface.co/docs/transformers/model\\\\_doc/xls\\\\_r) on the [Common Voice 7 dataset](https://huggingface.co/datasets/mozilla-foundation/common\\\\_voice\\\\_7\\\\_0).\\n\\nConsiderations for model evaluation\\nDeveloping an ML model is rarely a one-shot deal: it often involves multiple stages of defining the model architecture and tuning hyper-parameters before converging on a final set. Responsible model evaluation is a key part of this process, and 🤗 Evaluate is here to help! Here are some things to keep in mind when evaluating your model using the 🤗 Evaluate library:\\n## Properly splitting your data\\nGood evaluation generally requires three splits of your dataset:\\n- \\\\*\\\\*train\\\\*\\\\*: this is used for training your model. - \\\\*\\\\*validation\\\\*\\\\*: this is used for validating the model hyperparameters. - \\\\*\\\\*test\\\\*\\\\*: this is used for evaluating your model. Many of the datasets on the 🤗 Hub are separated into 2 splits: `train` and `validation`; others are split into 3 splits (`train`, `validation` and `test`) -- make sure to use the right split for the right purpose! Some datasets on the 🤗 Hub are already separated into these three splits. However, there are also many that only have a train/validation or only train split. If the dataset you\\'re using doesn\\'t have a predefined train-test split, it is up to you to define which part of the dataset you want to use for training your model and which you want to use for hyperparameter tuning or final evaluation. Training and evaluating on the same split can misrepresent your results! If you overfit on your training data the evaluation results on that split will look great but the model will perform poorly on new data. Depending on the size of the dataset, you can keep anywhere from 10-30% for evaluation and the rest for training, while aiming to set up the test set to reflect the production data as close as possible. Check out [this thread](https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090) for a more in-depth discussion of dataset splitting! ## The impact of class imbalance\\nWhile many academic datasets, such as the [IMDb dataset](https://huggingface.co/datasets/imdb) of movie reviews, are perfectly balanced, most real-world datasets are not. In machine learning a \\\\*balanced dataset\\\\* corresponds to a datasets where all labels are represented equally. In the case of the IMDb dataset this means that there are as many positive as negative reviews in the dataset. In an imbalanced dataset this is not the case: in fraud detection for example there are usually many more non-fraud cases than fraud cases in the dataset. Having an imbalanced dataset can skew the results of your metrics. Imagine a dataset with 99 \"non-fraud\" cases and 1 \"fraud\" case. A simple model that always predicts \"non-fraud\" cases would give yield a 99% accuracy which might sound good at first until you realize that you will never catch a fraud case.', 'type': 'tool', 'name': 'data_retriever', 'id': '528a1e8a-6640-416c-a951-abf8faa18367', 'tool_call_id': 'call_ioMpixaEVESzUSP0aeyTSWfV', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899ba48af3d2ebb270a089c', 'page_content': 'Introduction[[introduction]]\\nIn [Chapter 3](/course/chapter3) you got your first taste of the 🤗 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\\n1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics. But this is just scratching the surface of what 🤗 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we\\'ll find answers to the following questions:\\n\\\\* What do you do when your dataset is not on the Hub? \\\\* How can you slice and dice a dataset? (And what if you \\\\_really\\\\_ need to use Pandas?)\\n\\\\* What do you do when your dataset is huge and will melt your laptop\\'s RAM? \\\\* What the heck are \"memory mapping\" and Apache Arrow? \\\\* How can you create your own dataset and push it to the Hub? The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let\\'s get started!', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaad03e684812d15f5ec7', 'page_content': 'To verify that all libraries are correctly installed, you can run the following command in a Python shell. It verifies that both `transformers` and `datasets` have been correclty installed. ```python\\nfrom transformers import AutoModelForCTC, AutoProcessor\\nfrom datasets import load\\\\_dataset\\ndummy\\\\_dataset = load\\\\_dataset(\"common\\\\_voice\", \"ab\", split=\"test\")\\nmodel = AutoModelForCTC.from\\\\_pretrained(\"hf-internal-testing/tiny-random-wav2vec2\")\\nmodel.to(\"cuda\")\\nprocessor = AutoProcessor.from\\\\_pretrained(\"hf-internal-testing/tiny-random-wav2vec2\")\\ninput\\\\_values = processor(dummy\\\\_dataset[0][\"audio\"][\"array\"], return\\\\_tensors=\"pt\", sampling\\\\_rate=16\\\\_000).input\\\\_values\\ninput\\\\_values = input\\\\_values.to(\"cuda\")\\nlogits = model(input\\\\_values).logits\\nassert logits.shape[-1] == 32\\n```\\n## How to finetune an acoustic model\\nIn this section, we show you how to fine-tune a pre-trained [XLS-R Model](https://huggingface.co/docs/transformers/model\\\\_doc/xls\\\\_r) on the [Common Voice 7 dataset](https://huggingface.co/datasets/mozilla-foundation/common\\\\_voice\\\\_7\\\\_0).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa6073e684812d15f591d', 'page_content': 'Considerations for model evaluation\\nDeveloping an ML model is rarely a one-shot deal: it often involves multiple stages of defining the model architecture and tuning hyper-parameters before converging on a final set. Responsible model evaluation is a key part of this process, and 🤗 Evaluate is here to help! Here are some things to keep in mind when evaluating your model using the 🤗 Evaluate library:\\n## Properly splitting your data\\nGood evaluation generally requires three splits of your dataset:\\n- \\\\*\\\\*train\\\\*\\\\*: this is used for training your model. - \\\\*\\\\*validation\\\\*\\\\*: this is used for validating the model hyperparameters. - \\\\*\\\\*test\\\\*\\\\*: this is used for evaluating your model. Many of the datasets on the 🤗 Hub are separated into 2 splits: `train` and `validation`; others are split into 3 splits (`train`, `validation` and `test`) -- make sure to use the right split for the right purpose! Some datasets on the 🤗 Hub are already separated into these three splits. However, there are also many that only have a train/validation or only train split. If the dataset you\\'re using doesn\\'t have a predefined train-test split, it is up to you to define which part of the dataset you want to use for training your model and which you want to use for hyperparameter tuning or final evaluation. Training and evaluating on the same split can misrepresent your results! If you overfit on your training data the evaluation results on that split will look great but the model will perform poorly on new data. Depending on the size of the dataset, you can keep anywhere from 10-30% for evaluation and the rest for training, while aiming to set up the test set to reflect the production data as close as possible. Check out [this thread](https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090) for a more in-depth discussion of dataset splitting! ## The impact of class imbalance\\nWhile many academic datasets, such as the [IMDb dataset](https://huggingface.co/datasets/imdb) of movie reviews, are perfectly balanced, most real-world datasets are not. In machine learning a \\\\*balanced dataset\\\\* corresponds to a datasets where all labels are represented equally. In the case of the IMDb dataset this means that there are as many positive as negative reviews in the dataset. In an imbalanced dataset this is not the case: in fraud detection for example there are usually many more non-fraud cases than fraud cases in the dataset. Having an imbalanced dataset can skew the results of your metrics. Imagine a dataset with 99 \"non-fraud\" cases and 1 \"fraud\" case. A simple model that always predicts \"non-fraud\" cases would give yield a 99% accuracy which might sound good at first until you realize that you will never catch a fraud case.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The three main steps for fine-tuning a model with the 🤗 Datasets library are:\\n\\n1. Load a dataset from the Hugging Face Hub.\\n2. Preprocess the data with `Dataset.map()`.\\n3. Load and compute metrics.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--8bbb362a-c777-471e-b658-82a784926a8e-0', 'usage_metadata': {'input_tokens': 1448, 'output_tokens': 50, 'total_tokens': 1498, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:36:29,016 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 29%|██▉       | 19/65 [01:23<03:14,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\\n', 'type': 'human', 'id': 'db8e1345-c325-4c60-8e67-95d6f6305925'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_ZB1L3g0hN9nH92xJTVX9Xoom', 'function': {'arguments': '{\"query\":\"maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--83d7d5ed-dc14-4c90-9006-ea4296335761-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers'}, 'id': 'call_ZB1L3g0hN9nH92xJTVX9Xoom', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 366, 'output_tokens': 29, 'total_tokens': 395, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark. Figure 4. Latency (Batch=1, Physical Cores=2)\\n\\n---\\n## Conclusion\\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput. Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens. The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial)\\n## Resources\\n\\\\* [Hugging Face Infinity](https://huggingface.co/infinity)\\n\\\\* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\\n\\\\* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i)\\n\\\\* [DistilBERT](https://huggingface.co/docs/transformers/model\\\\_doc/distilbert)\\n\\\\* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\\n\\\\* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\\n\\\\* [🤗 Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)\\n\\nYou can test it and get a feeling for the performance of Infinity. Below you will find a `curl` command on how to send a request to the hosted endpoint. The API returns a `x-compute-time` HTTP Header, which contains the duration of the end-to-end pipeline. ```bash\\ncurl --request POST `-i` \\\\\\n--url https://infinity.huggingface.co/cpu/distilbert-base-uncased-emotion \\\\\\n--header \\'Content-Type: application/json\\' \\\\\\n--data \\'{\"inputs\":\"I like you. I love you\"}\\'\\n```\\n### Throughput\\nBelow you can find the throughput comparison for running infinity on 2 physical cores with batch size 1, compared with vanilla transformers. Figure 3. Throughput: Infinity vs Transformers\\n\\n| Sequence Length | Infinity | Transformers | improvement |\\n|-----------------|-------------|--------------|-------------|\\n| 8 | 248 req/sec | 49 req/sec | +506% |\\n| 16 | 212 req/sec | 50 req/sec | +424% |\\n| 32 | 150 req/sec | 40 req/sec | +375% |\\n| 64 | 97 req/sec | 28 req/sec | +346% |\\n| 128 | 55 req/sec | 18 req/sec | +305% |\\n| 256 | 27 req/sec | 9 req/sec | +300% |\\n| 384 | 17 req/sec | 5 req/sec | +340% |\\n| 512 | 12 req/sec | 4 req/sec | +300% |\\n### Latency\\nBelow, you can find the latency results for an experiment running Hugging Face Infinity on 2 Physical Cores with Batch Size 1.\\n\\n!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Overview\\n🤗 Optimum provides an API called BetterTransformer, a fast path of standard PyTorch Transformer APIs to benefit from interesting speedups on CPU & GPU through sparsity and fused kernels as Flash Attention. For now, BetterTransformer supports the fastpath from the native [`nn.TransformerEncoderLayer`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) as well as Flash Attention and Memory-Efficient Attention from [`torch.nn.functional.scaled\\\\_dot\\\\_product\\\\_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled\\\\_dot\\\\_product\\\\_attention.html).', 'type': 'tool', 'name': 'data_retriever', 'id': '346c1b8a-e94f-4d5c-9cd1-8dae6bad828c', 'tool_call_id': 'call_ZB1L3g0hN9nH92xJTVX9Xoom', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab7f83e684812d15f6e43', 'page_content': 'It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark. Figure 4. Latency (Batch=1, Physical Cores=2)\\n\\n---\\n## Conclusion\\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput. Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens. The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial)\\n## Resources\\n\\\\* [Hugging Face Infinity](https://huggingface.co/infinity)\\n\\\\* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\\n\\\\* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i)\\n\\\\* [DistilBERT](https://huggingface.co/docs/transformers/model\\\\_doc/distilbert)\\n\\\\* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\\n\\\\* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\\n\\\\* [🤗 Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab7f83e684812d15f6e42', 'page_content': 'You can test it and get a feeling for the performance of Infinity. Below you will find a `curl` command on how to send a request to the hosted endpoint. The API returns a `x-compute-time` HTTP Header, which contains the duration of the end-to-end pipeline. ```bash\\ncurl --request POST `-i` \\\\\\n--url https://infinity.huggingface.co/cpu/distilbert-base-uncased-emotion \\\\\\n--header \\'Content-Type: application/json\\' \\\\\\n--data \\'{\"inputs\":\"I like you. I love you\"}\\'\\n```\\n### Throughput\\nBelow you can find the throughput comparison for running infinity on 2 physical cores with batch size 1, compared with vanilla transformers. Figure 3. Throughput: Infinity vs Transformers\\n\\n| Sequence Length | Infinity | Transformers | improvement |\\n|-----------------|-------------|--------------|-------------|\\n| 8 | 248 req/sec | 49 req/sec | +506% |\\n| 16 | 212 req/sec | 50 req/sec | +424% |\\n| 32 | 150 req/sec | 40 req/sec | +375% |\\n| 64 | 97 req/sec | 28 req/sec | +346% |\\n| 128 | 55 req/sec | 18 req/sec | +305% |\\n| 256 | 27 req/sec | 9 req/sec | +300% |\\n| 384 | 17 req/sec | 5 req/sec | +340% |\\n| 512 | 12 req/sec | 4 req/sec | +300% |\\n### Latency\\nBelow, you can find the latency results for an experiment running Hugging Face Infinity on 2 Physical Cores with Batch Size 1.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa9493e684812d15f5cda', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Overview\\n🤗 Optimum provides an API called BetterTransformer, a fast path of standard PyTorch Transformer APIs to benefit from interesting speedups on CPU & GPU through sparsity and fused kernels as Flash Attention. For now, BetterTransformer supports the fastpath from the native [`nn.TransformerEncoderLayer`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) as well as Flash Attention and Memory-Efficient Attention from [`torch.nn.functional.scaled\\\\_dot\\\\_product\\\\_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled\\\\_dot\\\\_product\\\\_attention.html).', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--cf13f531-1473-4268-950a-f7f90b8ce33a-0', 'usage_metadata': {'input_tokens': 1344, 'output_tokens': 19, 'total_tokens': 1363, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:36:33,814 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 31%|███       | 20/65 [01:28<03:17,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the command to upload a spaCy pipeline to the Hugging Face Hub?\\n', 'type': 'human', 'id': '9a33ec71-77de-4074-85d7-e0dd528219fc'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_ABK51mlf8RnFuoi1caZGhqJn', 'function': {'arguments': '{\"query\":\"upload spaCy pipeline to Hugging Face Hub\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--9ca62ebc-36cd-4ebf-9fde-fcad9fa5e497-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'upload spaCy pipeline to Hugging Face Hub'}, 'id': 'call_ABK51mlf8RnFuoi1caZGhqJn', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 24, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Defaults to `hub` in the current working directory. |\\n| `--verbose`, `-V` | bool | Output additional info for debugging, e.g. the full generated hub metadata. |\\nYou can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`. ```bash\\nhuggingface-cli login\\npython -m spacy package ./en\\\\_ner\\\\_fashion ./output --build wheel\\ncd ./output/en\\\\_ner\\\\_fashion-0.0.0/dist\\npython -m spacy huggingface-hub push en\\\\_ner\\\\_fashion-0.0.0-py3-none-any.whl\\n```\\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card. The command will output two things:\\n\\\\* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en\\\\_core\\\\_web\\\\_sm\\n\\\\* And how to install the pipeline directly from the Hub! ### From a Python script\\nYou can use the `push` function from Python. It returns a dictionary containing the `\"url\"` and \"`whl\\\\_url`\" of the published model and the wheel file, which you can later install with `pip install`. ```py\\nfrom spacy\\\\_huggingface\\\\_hub import push\\nresult = push(\"./en\\\\_ner\\\\_fashion-0.0.0-py3-none-any.whl\")\\nprint(result[\"url\"])\\n```\\n## Additional resources\\n\\\\* spacy-huggingface-hub [library](https://github.com/explosion/spacy-huggingface-hub).\\n\\n--\\ntitle: \"Welcome spaCy to the Hugging Face Hub\"\\nthumbnail: /blog/assets/23\\\\_spacy/thumbnail.png\\nauthors:\\n- user: osanseviero\\n- user: ines\\n---\\n# Welcome spaCy to the Hugging Face Hub\\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text. Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You\\'ll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production! ### Finding models\\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy. ### Widgets\\nThis integration includes support for NER widgets, so all models with a NER component will have this out of the box! Coming soon there will be support for text classification and POS. [`spacy/en_core_web_sm`](/spacy/en_core_web_sm)\\n\\nHosted inference API\\n\\nToken Classification\\n\\nCompute\\n\\nThis model is currently loaded and running on the Inference API. JSON Output \\nMaximize\\n\\n### Using existing models\\nAll models from the Hub can be directly installed using `pip install`. ```bash\\npip install https://huggingface.co/spacy/en\\\\_core\\\\_web\\\\_sm/resolve/main/en\\\\_core\\\\_web\\\\_sm-any-py3-none-any.whl\\n```\\n```python\\n# Using spacy.load(). import spacy\\nnlp = spacy.load(\"en\\\\_core\\\\_web\\\\_sm\")\\n# Importing as module. import en\\\\_core\\\\_web\\\\_sm\\nnlp = en\\\\_core\\\\_web\\\\_sm.load()\\n```\\nWhen you open a repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model!\\n\\n\\\\* Launch [blog post](https://huggingface.co/blog/spacy)\\n\\\\* spaCy v 3.1 [Announcement](https://explosion.ai/blog/spacy-v3-1#huggingface-hub)\\n\\\\* spaCy [documentation](https://spacy.io/universe/project/spacy-huggingface-hub/)', 'type': 'tool', 'name': 'data_retriever', 'id': '65136e1a-9467-4d50-bd8c-63d012b24ddd', 'tool_call_id': 'call_ABK51mlf8RnFuoi1caZGhqJn', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b629af3d2ebb270a037b', 'page_content': 'Defaults to `hub` in the current working directory. |\\n| `--verbose`, `-V` | bool | Output additional info for debugging, e.g. the full generated hub metadata. |\\nYou can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`. ```bash\\nhuggingface-cli login\\npython -m spacy package ./en\\\\_ner\\\\_fashion ./output --build wheel\\ncd ./output/en\\\\_ner\\\\_fashion-0.0.0/dist\\npython -m spacy huggingface-hub push en\\\\_ner\\\\_fashion-0.0.0-py3-none-any.whl\\n```\\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card. The command will output two things:\\n\\\\* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en\\\\_core\\\\_web\\\\_sm\\n\\\\* And how to install the pipeline directly from the Hub! ### From a Python script\\nYou can use the `push` function from Python. It returns a dictionary containing the `\"url\"` and \"`whl\\\\_url`\" of the published model and the wheel file, which you can later install with `pip install`. ```py\\nfrom spacy\\\\_huggingface\\\\_hub import push\\nresult = push(\"./en\\\\_ner\\\\_fashion-0.0.0-py3-none-any.whl\")\\nprint(result[\"url\"])\\n```\\n## Additional resources\\n\\\\* spacy-huggingface-hub [library](https://github.com/explosion/spacy-huggingface-hub).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aab923e684812d15f5fa9', 'page_content': '--\\ntitle: \"Welcome spaCy to the Hugging Face Hub\"\\nthumbnail: /blog/assets/23\\\\_spacy/thumbnail.png\\nauthors:\\n- user: osanseviero\\n- user: ines\\n---\\n# Welcome spaCy to the Hugging Face Hub\\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text. Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You\\'ll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production! ### Finding models\\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy. ### Widgets\\nThis integration includes support for NER widgets, so all models with a NER component will have this out of the box! Coming soon there will be support for text classification and POS. [`spacy/en_core_web_sm`](/spacy/en_core_web_sm)\\n\\nHosted inference API\\n\\nToken Classification\\n\\nCompute\\n\\nThis model is currently loaded and running on the Inference API. JSON Output \\nMaximize\\n\\n### Using existing models\\nAll models from the Hub can be directly installed using `pip install`. ```bash\\npip install https://huggingface.co/spacy/en\\\\_core\\\\_web\\\\_sm/resolve/main/en\\\\_core\\\\_web\\\\_sm-any-py3-none-any.whl\\n```\\n```python\\n# Using spacy.load(). import spacy\\nnlp = spacy.load(\"en\\\\_core\\\\_web\\\\_sm\")\\n# Importing as module. import en\\\\_core\\\\_web\\\\_sm\\nnlp = en\\\\_core\\\\_web\\\\_sm.load()\\n```\\nWhen you open a repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model!', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b629af3d2ebb270a037c', 'page_content': '\\\\* Launch [blog post](https://huggingface.co/blog/spacy)\\n\\\\* spaCy v 3.1 [Announcement](https://explosion.ai/blog/spacy-v3-1#huggingface-hub)\\n\\\\* spaCy [documentation](https://spacy.io/universe/project/spacy-huggingface-hub/)', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The command to upload a spaCy pipeline to the Hugging Face Hub is as follows:\\n\\n```bash\\nhuggingface-cli login\\npython -m spacy package ./en_ner_fashion ./output --build wheel\\ncd ./output/en_ner_fashion-0.0.0/dist\\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\\n```\\n\\nThis will package your spaCy pipeline and push it to the Hugging Face Hub.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--58530302-7d01-4277-afb3-797a8bce0b04-0', 'usage_metadata': {'input_tokens': 1349, 'output_tokens': 109, 'total_tokens': 1458, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:36:37,743 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 32%|███▏      | 21/65 [01:32<03:07,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': \"What is the time and memory complexity of the Nyströmformer's approximation of self-attention?\\n\", 'type': 'human', 'id': 'f889575b-573a-47e3-9268-7761c5389669'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_DJui7qJQHZPoN471Kl3gjjM5', 'function': {'arguments': '{\"query\":\"Nyströmformer time and memory complexity\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--8b5eb834-6a5c-4602-a9ea-ca992cb6945b-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Nyströmformer time and memory complexity'}, 'id': 'call_DJui7qJQHZPoN471Kl3gjjM5', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 367, 'output_tokens': 22, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '--\\ntitle: \"Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\"\\nthumbnail: /blog/assets/86\\\\_nystromformer/thumbnail.png\\nauthors:\\n- user: asi\\nguest: true\\n---\\n# Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\\n## Introduction\\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\\\\\(O(n^2)\\\\\\\\) (where \\\\\\\\(n\\\\\\\\) is the length of the input sequence), making it expensive to train on long input sequences. The [Nyströmformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\\\\\(O(n)\\\\\\\\) complexity. Nyströmformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nyström method and how it can be adapted to approximate self-attention. ## Nyström method for matrix approximation\\nAt the heart of Nyströmformer is the Nyström method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let\\'s consider a matrix \\\\\\\\(P^{n \\\\times n}\\\\\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nyström method. We start by sampling \\\\\\\\(m\\\\\\\\) rows and columns from \\\\\\\\(P\\\\\\\\). We can then arrange the sampled rows and columns as follows:\\n\\nRepresenting P as a block matrix\\n\\nWe now have four submatrices: \\\\\\\\(A\\\\_P, B\\\\_P, F\\\\_P,\\\\\\\\) and \\\\\\\\(C\\\\_P\\\\\\\\), with sizes \\\\\\\\(m \\\\times m, m \\\\times (n - m), (n - m) \\\\times m\\\\\\\\) and\\n\\\\\\\\((n - m) \\\\times (n - m)\\\\\\\\) respectively. The \\\\\\\\(m\\\\\\\\) sampled columns are contained in \\\\\\\\(A\\\\_P\\\\\\\\) and \\\\\\\\(F\\\\_P\\\\\\\\), whereas the \\\\\\\\(m\\\\\\\\) sampled rows are contained in \\\\\\\\(A\\\\_P\\\\\\\\) and \\\\\\\\(B\\\\_P\\\\\\\\). So, the entries of \\\\\\\\(A\\\\_P, B\\\\_P,\\\\\\\\) and \\\\\\\\(F\\\\_P\\\\\\\\) are known to us, and we will estimate \\\\\\\\(C\\\\_P\\\\\\\\). According to the Nyström method, \\\\\\\\(C\\\\_P\\\\\\\\) is given by:\\n$$C\\\\_P = F\\\\_P A\\\\_P^+ B\\\\_P$$\\nHere, \\\\\\\\(+\\\\\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). Thus, the Nyström approximation of \\\\\\\\(P, \\\\hat{P}\\\\\\\\) can be written as:\\n\\nNyström approximation of P\\n\\nAs shown in the second line, \\\\\\\\(\\\\hat{P}\\\\\\\\) can be expressed as a product of three matrices.\\n\\nThe abstract from the paper is the following:\\n\\\\*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\\ncommunity. To address this limitation, we propose Nyströmformer -- a model that exhibits favorable scalability as a\\nfunction of sequence length. Our idea is based on adapting the Nyström method to approximate standard self-attention\\nwith O(n) complexity. The scalability of Nyströmformer enables application to longer sequences with thousands of\\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\\nsequence length, and find that our Nyströmformer performs comparably, or in a few cases, even slightly better, than\\nstandard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nyströmformer performs\\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.\\\\*\\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/Nystromformer). ## Resources\\n- [Text classification task guide](../tasks/sequence\\\\_classification)\\n- [Token classification task guide](../tasks/token\\\\_classification)\\n- [Question answering task guide](../tasks/question\\\\_answering)\\n- [Masked language modeling task guide](../tasks/masked\\\\_language\\\\_modeling)\\n- [Multiple choice task guide](../tasks/multiple\\\\_choice)\\n## NystromformerConfig\\n[[autodoc]] NystromformerConfig\\n## NystromformerModel\\n[[autodoc]] NystromformerModel\\n- forward\\n## NystromformerForMaskedLM\\n[[autodoc]] NystromformerForMaskedLM\\n- forward\\n## NystromformerForSequenceClassification\\n[[autodoc]] NystromformerForSequenceClassification\\n- forward\\n## NystromformerForMultipleChoice\\n[[autodoc]] NystromformerForMultipleChoice\\n- forward\\n## NystromformerForTokenClassification\\n[[autodoc]] NystromformerForTokenClassification\\n- forward\\n## NystromformerForQuestionAnswering\\n[[autodoc]] NystromformerForQuestionAnswering\\n- forward\\n\\nOnly at longer sequence lengths of 4096, a slight decrease in memory usage can be seen. Let\\'s see what happens to the memory peak usage if we increase the size of the feed forward layer by a factor of 4 and reduce the number of attention heads also by a factor of 4 so that the feed forward layer becomes the memory bottleneck. ```\\nconfig\\\\_no\\\\_chunk = ReformerConfig.from\\\\_pretrained(\"google/reformer-enwik8\", chunk\\\\_size\\\\_feed\\\\_forward=0, num\\\\_attention\\\\_{h}eads=2, feed\\\\_forward\\\\_size=16384) # no chuck\\nconfig\\\\_chunk = ReformerConfig.from\\\\_pretrained(\"google/reformer-enwik8\", chunk\\\\_size\\\\_feed\\\\_forward=1, num\\\\_attention\\\\_{h}eads=2, feed\\\\_forward\\\\_size=16384) # feed forward chunk\\nbenchmark\\\\_args = PyTorchBenchmarkArguments(sequence\\\\_lengths=[1024, 2048, 4096], batch\\\\_sizes=[8], models=[\"Reformer-No-Chunk\", \"Reformer-Chunk\"], no\\\\_speed=True, no\\\\_env\\\\_print=True)\\nbenchmark = PyTorchBenchmark(configs=[config\\\\_no\\\\_chunk, config\\\\_chunk], args=benchmark\\\\_args)\\nresult = benchmark.run()\\n```\\n1 / 2\\n2 / 2\\n==================== INFERENCE - MEMORY - RESULT ====================\\n--------------------------------------------------------------------------------\\nModel Name Batch Size Seq Length Memory in MB\\n--------------------------------------------------------------------------------\\nReformer-No-Chunk 8 1024 3743\\nReformer-No-Chunk 8 2048 5539\\nReformer-No-Chunk 8 4096 9087\\nReformer-Chunk 8 1024 2973\\nReformer-Chunk 8 2048 3999\\nReformer-Chunk 8 4096 6011\\n--------------------------------------------------------------------------------\\nNow a clear decrease in peak memory usage can be seen for longer input sequences.', 'type': 'tool', 'name': 'data_retriever', 'id': '08fad077-3347-4710-a7c6-ea75b7d65bd9', 'tool_call_id': 'call_DJui7qJQHZPoN471Kl3gjjM5', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aacb93e684812d15f6128', 'page_content': '--\\ntitle: \"Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\"\\nthumbnail: /blog/assets/86\\\\_nystromformer/thumbnail.png\\nauthors:\\n- user: asi\\nguest: true\\n---\\n# Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\\n## Introduction\\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\\\\\(O(n^2)\\\\\\\\) (where \\\\\\\\(n\\\\\\\\) is the length of the input sequence), making it expensive to train on long input sequences. The [Nyströmformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\\\\\(O(n)\\\\\\\\) complexity. Nyströmformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nyström method and how it can be adapted to approximate self-attention. ## Nyström method for matrix approximation\\nAt the heart of Nyströmformer is the Nyström method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let\\'s consider a matrix \\\\\\\\(P^{n \\\\times n}\\\\\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nyström method. We start by sampling \\\\\\\\(m\\\\\\\\) rows and columns from \\\\\\\\(P\\\\\\\\). We can then arrange the sampled rows and columns as follows:\\n\\nRepresenting P as a block matrix\\n\\nWe now have four submatrices: \\\\\\\\(A\\\\_P, B\\\\_P, F\\\\_P,\\\\\\\\) and \\\\\\\\(C\\\\_P\\\\\\\\), with sizes \\\\\\\\(m \\\\times m, m \\\\times (n - m), (n - m) \\\\times m\\\\\\\\) and\\n\\\\\\\\((n - m) \\\\times (n - m)\\\\\\\\) respectively. The \\\\\\\\(m\\\\\\\\) sampled columns are contained in \\\\\\\\(A\\\\_P\\\\\\\\) and \\\\\\\\(F\\\\_P\\\\\\\\), whereas the \\\\\\\\(m\\\\\\\\) sampled rows are contained in \\\\\\\\(A\\\\_P\\\\\\\\) and \\\\\\\\(B\\\\_P\\\\\\\\). So, the entries of \\\\\\\\(A\\\\_P, B\\\\_P,\\\\\\\\) and \\\\\\\\(F\\\\_P\\\\\\\\) are known to us, and we will estimate \\\\\\\\(C\\\\_P\\\\\\\\). According to the Nyström method, \\\\\\\\(C\\\\_P\\\\\\\\) is given by:\\n$$C\\\\_P = F\\\\_P A\\\\_P^+ B\\\\_P$$\\nHere, \\\\\\\\(+\\\\\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). Thus, the Nyström approximation of \\\\\\\\(P, \\\\hat{P}\\\\\\\\) can be written as:\\n\\nNyström approximation of P\\n\\nAs shown in the second line, \\\\\\\\(\\\\hat{P}\\\\\\\\) can be expressed as a product of three matrices.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab5e03e684812d15f6bcf', 'page_content': 'The abstract from the paper is the following:\\n\\\\*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\\ncommunity. To address this limitation, we propose Nyströmformer -- a model that exhibits favorable scalability as a\\nfunction of sequence length. Our idea is based on adapting the Nyström method to approximate standard self-attention\\nwith O(n) complexity. The scalability of Nyströmformer enables application to longer sequences with thousands of\\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\\nsequence length, and find that our Nyströmformer performs comparably, or in a few cases, even slightly better, than\\nstandard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nyströmformer performs\\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.\\\\*\\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/Nystromformer). ## Resources\\n- [Text classification task guide](../tasks/sequence\\\\_classification)\\n- [Token classification task guide](../tasks/token\\\\_classification)\\n- [Question answering task guide](../tasks/question\\\\_answering)\\n- [Masked language modeling task guide](../tasks/masked\\\\_language\\\\_modeling)\\n- [Multiple choice task guide](../tasks/multiple\\\\_choice)\\n## NystromformerConfig\\n[[autodoc]] NystromformerConfig\\n## NystromformerModel\\n[[autodoc]] NystromformerModel\\n- forward\\n## NystromformerForMaskedLM\\n[[autodoc]] NystromformerForMaskedLM\\n- forward\\n## NystromformerForSequenceClassification\\n[[autodoc]] NystromformerForSequenceClassification\\n- forward\\n## NystromformerForMultipleChoice\\n[[autodoc]] NystromformerForMultipleChoice\\n- forward\\n## NystromformerForTokenClassification\\n[[autodoc]] NystromformerForTokenClassification\\n- forward\\n## NystromformerForQuestionAnswering\\n[[autodoc]] NystromformerForQuestionAnswering\\n- forward', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab5b13e684812d15f6b8e', 'page_content': 'Only at longer sequence lengths of 4096, a slight decrease in memory usage can be seen. Let\\'s see what happens to the memory peak usage if we increase the size of the feed forward layer by a factor of 4 and reduce the number of attention heads also by a factor of 4 so that the feed forward layer becomes the memory bottleneck. ```\\nconfig\\\\_no\\\\_chunk = ReformerConfig.from\\\\_pretrained(\"google/reformer-enwik8\", chunk\\\\_size\\\\_feed\\\\_forward=0, num\\\\_attention\\\\_{h}eads=2, feed\\\\_forward\\\\_size=16384) # no chuck\\nconfig\\\\_chunk = ReformerConfig.from\\\\_pretrained(\"google/reformer-enwik8\", chunk\\\\_size\\\\_feed\\\\_forward=1, num\\\\_attention\\\\_{h}eads=2, feed\\\\_forward\\\\_size=16384) # feed forward chunk\\nbenchmark\\\\_args = PyTorchBenchmarkArguments(sequence\\\\_lengths=[1024, 2048, 4096], batch\\\\_sizes=[8], models=[\"Reformer-No-Chunk\", \"Reformer-Chunk\"], no\\\\_speed=True, no\\\\_env\\\\_print=True)\\nbenchmark = PyTorchBenchmark(configs=[config\\\\_no\\\\_chunk, config\\\\_chunk], args=benchmark\\\\_args)\\nresult = benchmark.run()\\n```\\n1 / 2\\n2 / 2\\n==================== INFERENCE - MEMORY - RESULT ====================\\n--------------------------------------------------------------------------------\\nModel Name Batch Size Seq Length Memory in MB\\n--------------------------------------------------------------------------------\\nReformer-No-Chunk 8 1024 3743\\nReformer-No-Chunk 8 2048 5539\\nReformer-No-Chunk 8 4096 9087\\nReformer-Chunk 8 1024 2973\\nReformer-Chunk 8 2048 3999\\nReformer-Chunk 8 4096 6011\\n--------------------------------------------------------------------------------\\nNow a clear decrease in peak memory usage can be seen for longer input sequences.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"The Nyströmformer's approximation of self-attention has a time and memory complexity of \\\\(O(n)\\\\), where \\\\(n\\\\) is the length of the input sequence.\", 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--8626572c-b5d1-4fe7-8515-8948bd6b601d-0', 'usage_metadata': {'input_tokens': 1928, 'output_tokens': 35, 'total_tokens': 1963, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:36:41,942 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 34%|███▍      | 22/65 [01:36<03:02,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the goal of the Named Entity Recognition task in token classification?\\n', 'type': 'human', 'id': 'b784f8f4-2cb7-42e3-9d2d-f3d4be98c2dc'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_ofoWm9709NQcNfsPjcEPbmQ1', 'function': {'arguments': '{\"query\":\"goal of Named Entity Recognition in token classification\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--341050e1-516c-42e3-a32a-51c23c3855ae-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'goal of Named Entity Recognition in token classification'}, 'id': 'call_ofoWm9709NQcNfsPjcEPbmQ1', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 23, 'total_tokens': 385, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'p align=\"center\"> ![FSNER LOGO](http://sayef.tech:8082/uploads/FSNER-LOGO-2.png) \\n\\nImplemented by  [sayef](https://huggingface.co/sayef) . ## Overview\\nThe FSNER model was proposed in [Example-Based Named Entity Recognition](https://arxiv.org/abs/2008.10570) by Morteza Ziyadi, Yuting Sun, Abhishek Goswami, Jade Huang, Weizhu Chen. To identify entity spans in a new domain, it uses a train-free few-shot learning approach inspired by question-answering. ## Abstract\\n----\\n> We present a novel approach to named entity recognition (NER) in the presence of scarce data that we call example-based NER. Our train-free few-shot learning approach takes inspiration from question-answering to identify entity spans in a new and unseen domain. In comparison with the current state-of-the-art, the proposed method performs significantly better, especially when using a low number of support examples. ## Model Training Details\\n-----\\n| identifier | epochs | datasets |\\n| ---------- |:----------:| :-----:|\\n| [sayef/fsner-bert-base-uncased](https://huggingface.co/sayef/fsner-bert-base-uncased) | 10 | ontonotes5, conll2003, wnut2017, and fin (Alvarado et al.). |\\n## Installation and Example Usage\\n------\\nYou can use the FSNER model in 3 ways:\\n1. Install directly from PyPI: `pip install fsner` and import the model as shown in the code example below\\nor\\n2. Install from source: `python setup.py install` and import the model as shown in the code example below\\nor\\n3. Clone repo and change directory to `src` and import the model as shown in the code example below\\n```python\\nfrom fsner import FSNERModel, FSNERTokenizerUtils\\nmodel = FSNERModel(\"sayef/fsner-bert-base-uncased\")\\ntokenizer = FSNERTokenizerUtils(\"sayef/fsner-bert-base-uncased\")\\n# size of query and supports must be the same.\\n\\nNamed-Entity Recognition\\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert\\\\_ner\\\\_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial\\\\_Analyst\\\\_AI\\nTags: NER, TEXT, HIGHLIGHT\\n## Introduction\\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \"token\") into different categories, such as names of people or names of locations, or different parts of speech. For example, given the sentence:\\n> Does Chicago have any Pakistani restaurants? A named-entity recognition algorithm may identify:\\n- \"Chicago\" as a \\\\*\\\\*location\\\\*\\\\*\\n- \"Pakistani\" as an \\\\*\\\\*ethnicity\\\\*\\\\*\\nand so on. Using `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team. Here is an example of a demo that you\\'ll be able to build:\\n$demo\\\\_ner\\\\_pipeline\\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!\\n\\note: the following transcripts are associated with Merve Noyan\\'s videos in the Hugging Face Tasks playlist: https://www.youtube.com/playlist?list=PLo2EIpI\\\\_JMQtyEr-sLJSy5\\\\_SnLCb4vtQf\\nToken Classification video\\nWelcome to the Hugging Face tasks series! In this video we’ll take a look at the token classification task. Token classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging. Let’s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity. Another token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech. Token classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score.', 'type': 'tool', 'name': 'data_retriever', 'id': 'd8c4b5ad-8906-4632-a0a1-71b5125ee088', 'tool_call_id': 'call_ofoWm9709NQcNfsPjcEPbmQ1', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab1613e684812d15f6651', 'page_content': 'p align=\"center\"> ![FSNER LOGO](http://sayef.tech:8082/uploads/FSNER-LOGO-2.png) \\n\\nImplemented by  [sayef](https://huggingface.co/sayef) . ## Overview\\nThe FSNER model was proposed in [Example-Based Named Entity Recognition](https://arxiv.org/abs/2008.10570) by Morteza Ziyadi, Yuting Sun, Abhishek Goswami, Jade Huang, Weizhu Chen. To identify entity spans in a new domain, it uses a train-free few-shot learning approach inspired by question-answering. ## Abstract\\n----\\n> We present a novel approach to named entity recognition (NER) in the presence of scarce data that we call example-based NER. Our train-free few-shot learning approach takes inspiration from question-answering to identify entity spans in a new and unseen domain. In comparison with the current state-of-the-art, the proposed method performs significantly better, especially when using a low number of support examples. ## Model Training Details\\n-----\\n| identifier | epochs | datasets |\\n| ---------- |:----------:| :-----:|\\n| [sayef/fsner-bert-base-uncased](https://huggingface.co/sayef/fsner-bert-base-uncased) | 10 | ontonotes5, conll2003, wnut2017, and fin (Alvarado et al.). |\\n## Installation and Example Usage\\n------\\nYou can use the FSNER model in 3 ways:\\n1. Install directly from PyPI: `pip install fsner` and import the model as shown in the code example below\\nor\\n2. Install from source: `python setup.py install` and import the model as shown in the code example below\\nor\\n3. Clone repo and change directory to `src` and import the model as shown in the code example below\\n```python\\nfrom fsner import FSNERModel, FSNERTokenizerUtils\\nmodel = FSNERModel(\"sayef/fsner-bert-base-uncased\")\\ntokenizer = FSNERTokenizerUtils(\"sayef/fsner-bert-base-uncased\")\\n# size of query and supports must be the same.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c2c7666af48523f521f7', 'page_content': 'Named-Entity Recognition\\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert\\\\_ner\\\\_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial\\\\_Analyst\\\\_AI\\nTags: NER, TEXT, HIGHLIGHT\\n## Introduction\\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \"token\") into different categories, such as names of people or names of locations, or different parts of speech. For example, given the sentence:\\n> Does Chicago have any Pakistani restaurants? A named-entity recognition algorithm may identify:\\n- \"Chicago\" as a \\\\*\\\\*location\\\\*\\\\*\\n- \"Pakistani\" as an \\\\*\\\\*ethnicity\\\\*\\\\*\\nand so on. Using `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team. Here is an example of a demo that you\\'ll be able to build:\\n$demo\\\\_ner\\\\_pipeline\\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab0ac3e684812d15f6577', 'page_content': \"ote: the following transcripts are associated with Merve Noyan's videos in the Hugging Face Tasks playlist: https://www.youtube.com/playlist?list=PLo2EIpI\\\\_JMQtyEr-sLJSy5\\\\_SnLCb4vtQf\\nToken Classification video\\nWelcome to the Hugging Face tasks series! In this video we’ll take a look at the token classification task. Token classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging. Let’s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity. Another token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech. Token classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The goal of the Named Entity Recognition (NER) task in token classification is to find the entities in a piece of text, such as names of people, locations, or organizations. This task is formulated as labeling each token with one class for each entity, and another class for tokens that have no entity.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--5bfe8164-2a19-4aef-9412-d727e1c2ccae-0', 'usage_metadata': {'input_tokens': 1333, 'output_tokens': 61, 'total_tokens': 1394, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:36:45,414 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 35%|███▌      | 23/65 [01:39<02:48,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the resolution of images used by the CLIPSeg model?\\n', 'type': 'human', 'id': '555ef9fa-ea7d-4b79-9c58-4eb4e87967c4'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_wB76r317NrQqvYyCJFETG2dq', 'function': {'arguments': '{\"query\":\"CLIPSeg model image resolution\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--8aea932e-1b35-4ae4-ad26-3e177a8fb727-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'CLIPSeg model image resolution'}, 'id': 'call_wB76r317NrQqvYyCJFETG2dq', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 21, 'total_tokens': 383, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm\\\\_source=hf&utm\\\\_medium=blog&utm\\\\_campaign=clipseg). Before we describe how to do that, let’s first take a look at how CLIPSeg works. ## CLIP: the magic model behind CLIPSeg\\n[CLIP](https://huggingface.co/docs/transformers/main/en/model\\\\_doc/clip), which stands for \\\\*\\\\*C\\\\*\\\\*ontrastive \\\\*\\\\*L\\\\*\\\\*anguage–\\\\*\\\\*I\\\\*\\\\*mage \\\\*\\\\*P\\\\*\\\\*re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract \\\\*representation\\\\* of your input. This abstract representation, also called an \\\\*embedding\\\\*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together).\\n\\n!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# CLIPSeg\\n## Overview\\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke\\nand Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero- and one-shot image segmentation.\\n\\nIt allows a computer to not only know what is in an image (classification), where objects are in the image (detection), but also what the outlines of those objects are. Knowing the outlines of objects is essential in fields such as robotics and autonomous driving. For example, a robot has to know the shape of an object to grab it correctly. Segmentation can also be combined with [image inpainting](https://t.co/5q8YHSOfx7) to allow users to describe which part of the image they want to replace. One limitation of most image segmentation models is that they only work with a fixed list of categories. For example, you cannot simply use a segmentation model trained on oranges to segment apples. To teach the segmentation model an additional category, you have to label data of the new category and train a new model, which can be costly and time-consuming. But what if there was a model that can already segment almost any kind of object, without any further training? That’s exactly what [CLIPSeg](https://arxiv.org/abs/2112.10003), a zero-shot segmentation model, achieves. Currently, CLIPSeg still has its limitations.', 'type': 'tool', 'name': 'data_retriever', 'id': '36440296-7361-4fdb-a071-7357bedfd86d', 'tool_call_id': 'call_wB76r317NrQqvYyCJFETG2dq', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab0393e684812d15f6503', 'page_content': 'For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm\\\\_source=hf&utm\\\\_medium=blog&utm\\\\_campaign=clipseg). Before we describe how to do that, let’s first take a look at how CLIPSeg works. ## CLIP: the magic model behind CLIPSeg\\n[CLIP](https://huggingface.co/docs/transformers/main/en/model\\\\_doc/clip), which stands for \\\\*\\\\*C\\\\*\\\\*ontrastive \\\\*\\\\*L\\\\*\\\\*anguage–\\\\*\\\\*I\\\\*\\\\*mage \\\\*\\\\*P\\\\*\\\\*re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract \\\\*representation\\\\* of your input. This abstract representation, also called an \\\\*embedding\\\\*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689abb503e684812d15f71d1', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# CLIPSeg\\n## Overview\\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke\\nand Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero- and one-shot image segmentation.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab0393e684812d15f6502', 'page_content': 'It allows a computer to not only know what is in an image (classification), where objects are in the image (detection), but also what the outlines of those objects are. Knowing the outlines of objects is essential in fields such as robotics and autonomous driving. For example, a robot has to know the shape of an object to grab it correctly. Segmentation can also be combined with [image inpainting](https://t.co/5q8YHSOfx7) to allow users to describe which part of the image they want to replace. One limitation of most image segmentation models is that they only work with a fixed list of categories. For example, you cannot simply use a segmentation model trained on oranges to segment apples. To teach the segmentation model an additional category, you have to label data of the new category and train a new model, which can be costly and time-consuming. But what if there was a model that can already segment almost any kind of object, without any further training? That’s exactly what [CLIPSeg](https://arxiv.org/abs/2112.10003), a zero-shot segmentation model, achieves. Currently, CLIPSeg still has its limitations.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The CLIPSeg model uses images of 352 x 352 pixels, which results in relatively low-resolution outputs.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--20ff7a36-c5fc-4cb7-be31-0d547ceffce8-0', 'usage_metadata': {'input_tokens': 1133, 'output_tokens': 23, 'total_tokens': 1156, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:36:50,455 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 37%|███▋      | 24/65 [01:44<02:57,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What can you use Gradio for?\\n', 'type': 'human', 'id': '24910029-f6b8-43b9-81c8-842512f3201e'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_Loqy7RnXchCO2ThY9qApsfju', 'function': {'arguments': '{\"query\":\"Gradio use cases\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--0c988598-2066-41bb-ac6c-c4e8006f2b7f-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Gradio use cases'}, 'id': 'call_Loqy7RnXchCO2ThY9qApsfju', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 356, 'output_tokens': 19, 'total_tokens': 375, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Key Features\\nLet\\'s go through some of the key features of Gradio. This guide is intended to be a high-level overview of various things that you should be aware of as you build your demo. Where appropriate, we link to more detailed guides on specific topics. 1. [Components](#components)\\n2. [Queuing](#queuing)\\n3. [Streaming outputs](#streaming-outputs)\\n4. [Streaming inputs](#streaming-inputs)\\n5. [Alert modals](#alert-modals)\\n6. [Styling](#styling)\\n7. [Progress bars](#progress-bars)\\n8. [Batch functions](#batch-functions)\\n## Components\\nGradio includes more than 30 pre-built components (as well as many user-built \\\\_custom components\\\\_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on. Each component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[\"webcam\", \"upload\"]`). \\\\*\\\\*Static and Interactive Components\\\\*\\\\*\\nEvery component has a \\\\_static\\\\_ version that is designed to \\\\*display\\\\* data, and most components also have an \\\\_interactive\\\\_ version designed to let users input or modify the data. Typically, you don\\'t need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports. \\\\*\\\\*Preprocessing and Postprocessing\\\\*\\\\*\\nWhen a component is used as an input, Gradio automatically handles the \\\\_preprocessing\\\\_ needed to convert the data from a type sent by the user\\'s browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array). Similarly, when a component is used as an output, Gradio automatically handles the \\\\_postprocessing\\\\_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user\\'s browser (a gallery of images). Consider an example demo with three input components (`gr.Textbox`, `gr.Number`, and `gr.Image`) and two outputs (`gr.Number` and `gr.Gallery`) that serve as a UI for your image-to-image generation model. Below is a diagram of what our preprocessing will send to the model and what our postprocessing will require from it. ![](https://github.com/gradio-app/gradio/blob/main/guides/assets/dataflow.svg?raw=true)\\nIn this image, the following preprocessing steps happen to send the data from the browser to your function:\\n\\\\* The text in the textbox is converted to a Python `str` (essentially no preprocessing)\\n\\\\* The number in the number input in converted to a Python `float` (essentially no preprocessing)\\n\\\\* Most importantly, ihe image supplied by the user is converted to a `numpy.array` representation of the RGB values in the image\\nImages are converted to NumPy arrays because they are a common format for machine learning workflows. You can control the \\\\_preprocessing\\\\_ using the component\\'s parameters when constructing the component. For example, if you instantiate the `Image` component with the following parameters, it will preprocess the image to the `PIL` format instead:\\n```py\\nimg = gr.Image(type=\"pil\")\\n```\\nPostprocessing is even simpler! Gradio automatically recognizes the format of the returned data (e.g. does the user\\'s function return a `numpy` array or a `str` filepath for the `gr.Image` component?) and postprocesses it appropriately into a format that can be displayed by the browser. So in the image above, the following postprocessing steps happen to send the data returned from a user\\'s function to the browser:\\n\\\\* The `float` is displayed as a number and displayed directly to the user\\n\\\\* The list of string filepaths (`list[str]`) is interpreted as a list of image filepaths and displayed as a gallery in the browser\\nTake a look at the [Docs](https://gradio.app/docs) to see all the parameters for each Gradio component.\\n\\n### An Overview of Gradio\\nSo far, we\\'ve been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do? #### Chatbots with `gr.ChatInterface`\\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you\\'re interested in creating a chatbot, you can jump straight [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast). #### Custom Demos with `gr.Blocks`\\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction — still all in Python. You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners). #### The Gradio Python & JavaScript Ecosystem\\nThat\\'s the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\\n\\\\* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio\\\\_client`): query any Gradio app programmatically in Python. \\\\* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript. \\\\* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \\\\* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications — for free!\\n\\nGradio, check![[gradio-check]]\\nThis wraps up the chapter on building cool ML demos with Gradio - we hope you enjoyed it! To recap, in this chapter we learned:\\n- How to create Gradio demos with the high-level `Interface` API, and how to configure different input and output modalities. - Different ways to share Gradio demos, through temporary links and hosting on [Hugging Face Spaces](https://huggingface.co/spaces). - How to integrate Gradio demos with models and Spaces on the Hugging Face Hub. - Advanced features like storing state in a demo or providing authentication. - How to have full control of the data flow and layout of your demo with Gradio Blocks. If you\\'d like to test your understanding of the concepts covered in this chapter, check out the quiz in the next section! ## Where to next?[[where-to-next]]\\nIf you want to learn more about Gradio you can\\n- Take a look at [Demos](https://github.com/gradio-app/gradio/tree/main/demo) in the repo, there are quite a lot of examples there. - See the [Guides](https://gradio.app/guides/) page, where you can find guides about cool and advanced features.', 'type': 'tool', 'name': 'data_retriever', 'id': '5e7fad26-6709-4967-a632-edea92dd75d7', 'tool_call_id': 'call_Loqy7RnXchCO2ThY9qApsfju', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab1723e684812d15f665b', 'page_content': 'Key Features\\nLet\\'s go through some of the key features of Gradio. This guide is intended to be a high-level overview of various things that you should be aware of as you build your demo. Where appropriate, we link to more detailed guides on specific topics. 1. [Components](#components)\\n2. [Queuing](#queuing)\\n3. [Streaming outputs](#streaming-outputs)\\n4. [Streaming inputs](#streaming-inputs)\\n5. [Alert modals](#alert-modals)\\n6. [Styling](#styling)\\n7. [Progress bars](#progress-bars)\\n8. [Batch functions](#batch-functions)\\n## Components\\nGradio includes more than 30 pre-built components (as well as many user-built \\\\_custom components\\\\_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on. Each component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[\"webcam\", \"upload\"]`). \\\\*\\\\*Static and Interactive Components\\\\*\\\\*\\nEvery component has a \\\\_static\\\\_ version that is designed to \\\\*display\\\\* data, and most components also have an \\\\_interactive\\\\_ version designed to let users input or modify the data. Typically, you don\\'t need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports. \\\\*\\\\*Preprocessing and Postprocessing\\\\*\\\\*\\nWhen a component is used as an input, Gradio automatically handles the \\\\_preprocessing\\\\_ needed to convert the data from a type sent by the user\\'s browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array). Similarly, when a component is used as an output, Gradio automatically handles the \\\\_postprocessing\\\\_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user\\'s browser (a gallery of images). Consider an example demo with three input components (`gr.Textbox`, `gr.Number`, and `gr.Image`) and two outputs (`gr.Number` and `gr.Gallery`) that serve as a UI for your image-to-image generation model. Below is a diagram of what our preprocessing will send to the model and what our postprocessing will require from it. ![](https://github.com/gradio-app/gradio/blob/main/guides/assets/dataflow.svg?raw=true)\\nIn this image, the following preprocessing steps happen to send the data from the browser to your function:\\n\\\\* The text in the textbox is converted to a Python `str` (essentially no preprocessing)\\n\\\\* The number in the number input in converted to a Python `float` (essentially no preprocessing)\\n\\\\* Most importantly, ihe image supplied by the user is converted to a `numpy.array` representation of the RGB values in the image\\nImages are converted to NumPy arrays because they are a common format for machine learning workflows. You can control the \\\\_preprocessing\\\\_ using the component\\'s parameters when constructing the component. For example, if you instantiate the `Image` component with the following parameters, it will preprocess the image to the `PIL` format instead:\\n```py\\nimg = gr.Image(type=\"pil\")\\n```\\nPostprocessing is even simpler! Gradio automatically recognizes the format of the returned data (e.g. does the user\\'s function return a `numpy` array or a `str` filepath for the `gr.Image` component?) and postprocesses it appropriately into a format that can be displayed by the browser. So in the image above, the following postprocessing steps happen to send the data returned from a user\\'s function to the browser:\\n\\\\* The `float` is displayed as a number and displayed directly to the user\\n\\\\* The list of string filepaths (`list[str]`) is interpreted as a list of image filepaths and displayed as a gallery in the browser\\nTake a look at the [Docs](https://gradio.app/docs) to see all the parameters for each Gradio component.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c2d5666af48523f52207', 'page_content': \"### An Overview of Gradio\\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do? #### Chatbots with `gr.ChatInterface`\\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast). #### Custom Demos with `gr.Blocks`\\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction — still all in Python. You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners). #### The Gradio Python & JavaScript Ecosystem\\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\\n\\\\* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio\\\\_client`): query any Gradio app programmatically in Python. \\\\* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript. \\\\* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \\\\* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications — for free!\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa93a3e684812d15f5cce', 'page_content': \"Gradio, check![[gradio-check]]\\nThis wraps up the chapter on building cool ML demos with Gradio - we hope you enjoyed it! To recap, in this chapter we learned:\\n- How to create Gradio demos with the high-level `Interface` API, and how to configure different input and output modalities. - Different ways to share Gradio demos, through temporary links and hosting on [Hugging Face Spaces](https://huggingface.co/spaces). - How to integrate Gradio demos with models and Spaces on the Hugging Face Hub. - Advanced features like storing state in a demo or providing authentication. - How to have full control of the data flow and layout of your demo with Gradio Blocks. If you'd like to test your understanding of the concepts covered in this chapter, check out the quiz in the next section! ## Where to next?[[where-to-next]]\\nIf you want to learn more about Gradio you can\\n- Take a look at [Demos](https://github.com/gradio-app/gradio/tree/main/demo) in the repo, there are quite a lot of examples there. - See the [Guides](https://gradio.app/guides/) page, where you can find guides about cool and advanced features.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'Gradio can be used to build machine learning applications and demos quickly and easily. It provides a high-level class called `Interface` that allows you to create demos with various input and output modalities. You can also create chatbots using the `gr.ChatInterface` class, and design custom web apps with flexible layouts and data flows using the `gr.Blocks` class. Additionally, Gradio is part of a larger ecosystem that includes Python and JavaScript libraries for querying Gradio apps programmatically, as well as hosting applications on Hugging Face Spaces. Overall, Gradio is a versatile tool for developing and sharing machine learning models and applications.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--17856014-4c62-4942-9588-c2957c2b14ce-0', 'usage_metadata': {'input_tokens': 2088, 'output_tokens': 128, 'total_tokens': 2216, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:36:56,556 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 38%|███▊      | 25/65 [01:50<03:14,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What TensorFlow API function is used to load a saved tensor file?\\n', 'type': 'human', 'id': '749fd6e7-2bc8-4964-8946-5e94881d54ff'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_alTFD6Th96yNHvR4hYCrqspq', 'function': {'arguments': '{\"query\":\"TensorFlow API function to load a saved tensor file\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--a865485f-27bf-4447-9be9-be590dbee395-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'TensorFlow API function to load a saved tensor file'}, 'id': 'call_alTFD6Th96yNHvR4hYCrqspq', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 25, 'total_tokens': 387, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Tensorflow API\\n[[autodoc]] safetensors.tensorflow.load\\\\_file\\n[[autodoc]] safetensors.tensorflow.load\\n[[autodoc]] safetensors.tensorflow.save\\\\_file\\n[[autodoc]] safetensors.tensorflow.save\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Load safetensors\\n[[open-in-colab]]\\n[safetensors](https://github.com/huggingface/safetensors) is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or \\\\*pickled\\\\* into a `.bin` file with Python\\'s [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights. This guide will show you how you load `.safetensor` files, and how to convert Stable Diffusion model weights stored in other formats to `.safetensor`. Before you start, make sure you have safetensors installed:\\n```py\\n# uncomment to install the necessary libraries in Colab\\n#!pip install safetensors\\n```\\nIf you look at the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main) repository, you\\'ll see weights inside the `text\\\\_encoder`, `unet` and `vae` subfolders are stored in the `.safetensors` format. By default, 🤗 Diffusers automatically loads these `.safetensors` files from their subfolders if they\\'re available in the model repository. For more explicit control, you can optionally set `use\\\\_safetensors=True` (if `safetensors` is not installed, you\\'ll get an error message asking you to install it):\\n```py\\nfrom diffusers import DiffusionPipeline\\npipeline = DiffusionPipeline.from\\\\_pretrained(\"runwayml/stable-diffusion-v1-5\", use\\\\_safetensors=True)\\n```\\nHowever, model weights are not necessarily stored in separate subfolders like in the example above. Sometimes, all the weights are stored in a single `.safetensors` file. In this case, if the weights are Stable Diffusion weights, you can load the file directly with the [`~diffusers.loaders.FromSingleFileMixin.from\\\\_single\\\\_file`] method:\\n```py\\nfrom diffusers import StableDiffusionPipeline\\npipeline = StableDiffusionPipeline.from\\\\_single\\\\_file(\\n\"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\"\\n)\\n```\\n## Convert to safetensors\\nNot all weights on the Hub are available in the `.safetensors` format, and you may encounter weights stored as `.bin`.\\n\\n!-- DISABLE-FRONTMATTER-SECTIONS -->\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-logo-light.svg)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-logo-dark.svg)\\n\\n# Safetensors\\nSafetensors is a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy). Safetensors is really [fast 🚀](./speed). ## Installation\\nwith pip:\\n```\\npip install safetensors\\n```\\nwith conda:\\n```\\nconda install -c huggingface safetensors\\n```\\n## Usage\\n### Load tensors\\n```python\\nfrom safetensors import safe\\\\_open\\ntensors = {}\\nwith safe\\\\_open(\"model.safetensors\", framework=\"pt\", device=0) as f:\\nfor k in f.keys():\\ntensors[k] = f.get\\\\_tensor(k)\\n```\\nLoading only part of the tensors (interesting when running on multiple GPU)\\n```python\\nfrom safetensors import safe\\\\_open\\ntensors = {}\\nwith safe\\\\_open(\"model.safetensors\", framework=\"pt\", device=0) as f:\\ntensor\\\\_slice = f.get\\\\_slice(\"embedding\")\\nvocab\\\\_size, hidden\\\\_dim = tensor\\\\_slice.get\\\\_shape()\\ntensor = tensor\\\\_slice[:, :hidden\\\\_dim]\\n```\\n### Save tensors\\n```python\\nimport torch\\nfrom safetensors.torch import save\\\\_file\\ntensors = {\\n\"embedding\": torch.zeros((2, 2)),\\n\"attention\": torch.zeros((2, 3))\\n}\\nsave\\\\_file(tensors, \"model.safetensors\")\\n```\\n## Format\\nLet\\'s say you have safetensors file named `model.safetensors`, then `model.safetensors` will have the following internal format:\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-format.svg)\\n\\n## Featured Projects\\nSafetensors is being used widely at leading AI enterprises, such as [Hugging Face](https://huggingface.co/), [EleutherAI](https://www.eleuther.ai/), and [StabilityAI](https://stability.ai/). Here is a non-exhaustive list of projects that are using safetensors:\\n\\\\* [huggingface/transformers](https://github.com/huggingface/transformers)\\n\\\\* [AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)\\n\\\\* [Llama-cpp](https://github.com/ggerganov/llama.cpp/blob/e6a46b0ed1884c77267dc70693183e3b7164e0e0/convert.py#L537)\\n\\\\* [microsoft/TaskMatrix](https://github.com/microsoft/TaskMatrix)\\n\\\\* [hpcaitech/ColossalAI](https://github.com/hpcaitech/ColossalAI)\\n\\\\* [huggingface/pytorch-image-models](https://github.com/huggingface/pytorch-image-models)\\n\\\\* [CivitAI](https://civitai.com/)\\n\\\\* [huggingface/diffusers](https://github.com/huggingface/diffusers)\\n\\\\* [coreylowman/dfdx](https://github.com/coreylowman/dfdx)\\n\\\\* [invoke-ai/InvokeAI](https://github.com/invoke-ai/InvokeAI)\\n\\\\* [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n\\\\* [Sanster/lama-cleaner](https://github.com/Sanster/lama-cleaner)\\n\\\\* [PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\\n\\\\* [AIGC-Audio/AudioGPT](https://github.com/AIGC-Audio/AudioGPT)\\n\\\\* [brycedrennan/imaginAIry](https://github.com/brycedrennan/imaginAIry)\\n\\\\* [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)\\n\\\\* [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE)\\n\\\\* [alvarobartt/safejax](https://github.com/alvarobartt/safejax)\\n\\\\* [MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)\\n\\\\* [LaurentMazare/tch-rs](https://github.com/LaurentMazare/tch-rs)\\n\\\\* [chainyo/tensorshare](https://github.com/chainyo/tensorshare)', 'type': 'tool', 'name': 'data_retriever', 'id': 'e6fcff91-b0c0-48a0-a3ee-bd79a76ec79a', 'tool_call_id': 'call_alTFD6Th96yNHvR4hYCrqspq', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab5fc3e684812d15f6bea', 'page_content': 'Tensorflow API\\n[[autodoc]] safetensors.tensorflow.load\\\\_file\\n[[autodoc]] safetensors.tensorflow.load\\n[[autodoc]] safetensors.tensorflow.save\\\\_file\\n[[autodoc]] safetensors.tensorflow.save', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c21b666af48523f52102', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Load safetensors\\n[[open-in-colab]]\\n[safetensors](https://github.com/huggingface/safetensors) is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or \\\\*pickled\\\\* into a `.bin` file with Python\\'s [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights. This guide will show you how you load `.safetensor` files, and how to convert Stable Diffusion model weights stored in other formats to `.safetensor`. Before you start, make sure you have safetensors installed:\\n```py\\n# uncomment to install the necessary libraries in Colab\\n#!pip install safetensors\\n```\\nIf you look at the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main) repository, you\\'ll see weights inside the `text\\\\_encoder`, `unet` and `vae` subfolders are stored in the `.safetensors` format. By default, 🤗 Diffusers automatically loads these `.safetensors` files from their subfolders if they\\'re available in the model repository. For more explicit control, you can optionally set `use\\\\_safetensors=True` (if `safetensors` is not installed, you\\'ll get an error message asking you to install it):\\n```py\\nfrom diffusers import DiffusionPipeline\\npipeline = DiffusionPipeline.from\\\\_pretrained(\"runwayml/stable-diffusion-v1-5\", use\\\\_safetensors=True)\\n```\\nHowever, model weights are not necessarily stored in separate subfolders like in the example above. Sometimes, all the weights are stored in a single `.safetensors` file. In this case, if the weights are Stable Diffusion weights, you can load the file directly with the [`~diffusers.loaders.FromSingleFileMixin.from\\\\_single\\\\_file`] method:\\n```py\\nfrom diffusers import StableDiffusionPipeline\\npipeline = StableDiffusionPipeline.from\\\\_single\\\\_file(\\n\"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\"\\n)\\n```\\n## Convert to safetensors\\nNot all weights on the Hub are available in the `.safetensors` format, and you may encounter weights stored as `.bin`.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab6cb3e684812d15f6cc0', 'page_content': '!-- DISABLE-FRONTMATTER-SECTIONS -->\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-logo-light.svg)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-logo-dark.svg)\\n\\n# Safetensors\\nSafetensors is a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy). Safetensors is really [fast 🚀](./speed). ## Installation\\nwith pip:\\n```\\npip install safetensors\\n```\\nwith conda:\\n```\\nconda install -c huggingface safetensors\\n```\\n## Usage\\n### Load tensors\\n```python\\nfrom safetensors import safe\\\\_open\\ntensors = {}\\nwith safe\\\\_open(\"model.safetensors\", framework=\"pt\", device=0) as f:\\nfor k in f.keys():\\ntensors[k] = f.get\\\\_tensor(k)\\n```\\nLoading only part of the tensors (interesting when running on multiple GPU)\\n```python\\nfrom safetensors import safe\\\\_open\\ntensors = {}\\nwith safe\\\\_open(\"model.safetensors\", framework=\"pt\", device=0) as f:\\ntensor\\\\_slice = f.get\\\\_slice(\"embedding\")\\nvocab\\\\_size, hidden\\\\_dim = tensor\\\\_slice.get\\\\_shape()\\ntensor = tensor\\\\_slice[:, :hidden\\\\_dim]\\n```\\n### Save tensors\\n```python\\nimport torch\\nfrom safetensors.torch import save\\\\_file\\ntensors = {\\n\"embedding\": torch.zeros((2, 2)),\\n\"attention\": torch.zeros((2, 3))\\n}\\nsave\\\\_file(tensors, \"model.safetensors\")\\n```\\n## Format\\nLet\\'s say you have safetensors file named `model.safetensors`, then `model.safetensors` will have the following internal format:\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-format.svg)\\n\\n## Featured Projects\\nSafetensors is being used widely at leading AI enterprises, such as [Hugging Face](https://huggingface.co/), [EleutherAI](https://www.eleuther.ai/), and [StabilityAI](https://stability.ai/). Here is a non-exhaustive list of projects that are using safetensors:\\n\\\\* [huggingface/transformers](https://github.com/huggingface/transformers)\\n\\\\* [AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)\\n\\\\* [Llama-cpp](https://github.com/ggerganov/llama.cpp/blob/e6a46b0ed1884c77267dc70693183e3b7164e0e0/convert.py#L537)\\n\\\\* [microsoft/TaskMatrix](https://github.com/microsoft/TaskMatrix)\\n\\\\* [hpcaitech/ColossalAI](https://github.com/hpcaitech/ColossalAI)\\n\\\\* [huggingface/pytorch-image-models](https://github.com/huggingface/pytorch-image-models)\\n\\\\* [CivitAI](https://civitai.com/)\\n\\\\* [huggingface/diffusers](https://github.com/huggingface/diffusers)\\n\\\\* [coreylowman/dfdx](https://github.com/coreylowman/dfdx)\\n\\\\* [invoke-ai/InvokeAI](https://github.com/invoke-ai/InvokeAI)\\n\\\\* [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n\\\\* [Sanster/lama-cleaner](https://github.com/Sanster/lama-cleaner)\\n\\\\* [PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\\n\\\\* [AIGC-Audio/AudioGPT](https://github.com/AIGC-Audio/AudioGPT)\\n\\\\* [brycedrennan/imaginAIry](https://github.com/brycedrennan/imaginAIry)\\n\\\\* [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)\\n\\\\* [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE)\\n\\\\* [alvarobartt/safejax](https://github.com/alvarobartt/safejax)\\n\\\\* [MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)\\n\\\\* [LaurentMazare/tch-rs](https://github.com/LaurentMazare/tch-rs)\\n\\\\* [chainyo/tensorshare](https://github.com/chainyo/tensorshare)', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--dc215cc3-47e1-4ea9-ba12-e9aecc6cea0a-0', 'usage_metadata': {'input_tokens': 2048, 'output_tokens': 22, 'total_tokens': 2070, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:37:02,668 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 40%|████      | 26/65 [01:57<03:24,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Where can you access the logs of your Endpoints in Hugging Face Endpoints?\\n', 'type': 'human', 'id': '8226bc93-3928-42fb-bfc6-a3a79b4206cf'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_JRmJSvjQMTxEWhp0Evn0Z7bK', 'function': {'arguments': '{\"query\":\"Hugging Face Endpoints logs access\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--88e9166f-1dcd-4acc-bddd-5b1dd7b17afb-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Hugging Face Endpoints logs access'}, 'id': 'call_JRmJSvjQMTxEWhp0Evn0Z7bK', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 23, 'total_tokens': 388, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': \"Access and read Logs\\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the “Logs” tab of your Endpoint. You will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference. ![select logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png)\\nThe Container Logs are only available when your Endpoint is in the “Running” state. \\\\_Note: If your Endpoint creation is in the “Failed” state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc.\\\\_\\n\\\\*\\\\*Build Logs:\\\\*\\\\*\\n![build logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png)\\n\\\\*\\\\*Container Logs:\\\\*\\\\*\\n![container logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png)\\n\\nAccess and view Metrics\\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the “Analytics” tab of your Endpoints. ![metric dashboard](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png)\\n## Access Metrics via API\\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api\\\\_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.\\n\\nOnce I'm done testing, I delete the endpoints that I've created to avoid unwanted charges. I also delete the VPC Endpoint in the AWS console. Hugging Face customers are already using Inference Endpoints. For example, [Phamily](https://phamily.com/), the #1 in-house chronic care management & proactive care platform, [told us](https://www.youtube.com/watch?v=20C9X5OYO2Q) that Inference Endpoints is helping them simplify and accelerate HIPAA-compliant Transformer deployments. ### Now it's your turn! Thanks to Inference Endpoints, you can deploy production-grade, scalable, secure endpoints in minutes, in just a few clicks. Why don't you [give it a try](https://ui.endpoints.huggingface.co/new)? We have plenty of ideas to make the service even better, and we'd love to hear your feedback in the [Hugging Face forum](https://discuss.huggingface.co/). Thank you for reading and have fun with Inference Endpoints!\", 'type': 'tool', 'name': 'data_retriever', 'id': '49897a63-26a3-4e32-81a3-e384fd26ef16', 'tool_call_id': 'call_JRmJSvjQMTxEWhp0Evn0Z7bK', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b190af3d2ebb2709fdbc', 'page_content': 'Access and read Logs\\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the “Logs” tab of your Endpoint. You will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference. ![select logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png)\\nThe Container Logs are only available when your Endpoint is in the “Running” state. \\\\_Note: If your Endpoint creation is in the “Failed” state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc.\\\\_\\n\\\\*\\\\*Build Logs:\\\\*\\\\*\\n![build logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png)\\n\\\\*\\\\*Container Logs:\\\\*\\\\*\\n![container logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png)', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b643af3d2ebb270a0394', 'page_content': 'Access and view Metrics\\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the “Analytics” tab of your Endpoints. ![metric dashboard](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png)\\n## Access Metrics via API\\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api\\\\_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b65daf3d2ebb270a03af', 'page_content': \"Once I'm done testing, I delete the endpoints that I've created to avoid unwanted charges. I also delete the VPC Endpoint in the AWS console. Hugging Face customers are already using Inference Endpoints. For example, [Phamily](https://phamily.com/), the #1 in-house chronic care management & proactive care platform, [told us](https://www.youtube.com/watch?v=20C9X5OYO2Q) that Inference Endpoints is helping them simplify and accelerate HIPAA-compliant Transformer deployments. ### Now it's your turn! Thanks to Inference Endpoints, you can deploy production-grade, scalable, secure endpoints in minutes, in just a few clicks. Why don't you [give it a try](https://ui.endpoints.huggingface.co/new)? We have plenty of ideas to make the service even better, and we'd love to hear your feedback in the [Hugging Face forum](https://discuss.huggingface.co/). Thank you for reading and have fun with Inference Endpoints!\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'You can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the “Logs” tab of your Endpoint. You will have access to the build logs of your Image artifacts as well as the Container Logs during inference. The Container Logs are only available when your Endpoint is in the “Running” state. If your Endpoint creation is in the “Failed” state, you can check the Build Logs to see the reason for the failure.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f8ab0bcc-04b9-46ae-96d9-61fbd517ca2d-0', 'usage_metadata': {'input_tokens': 827, 'output_tokens': 92, 'total_tokens': 919, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:37:06,631 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 42%|████▏     | 27/65 [02:01<03:04,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the latest task added to Hugging Face AutoTrain for Computer Vision?\\n', 'type': 'human', 'id': '641cd9aa-4da6-4f6e-9f69-322ac6138a97'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_BB0MbGX3mHnBKzdrxaivzD01', 'function': {'arguments': '{\"query\":\"latest task added to Hugging Face AutoTrain for Computer Vision\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--629a2479-3721-473f-b299-07babf4455eb-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'latest task added to Hugging Face AutoTrain for Computer Vision'}, 'id': 'call_BB0MbGX3mHnBKzdrxaivzD01', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 364, 'output_tokens': 27, 'total_tokens': 391, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '--\\ntitle: Image Classification with AutoTrain\\nthumbnail: /blog/assets/105\\\\_autotrain-image-classification/thumbnail.png\\nauthors:\\n- user: nimaboscarino\\n---\\n# Image Classification with AutoTrain\\nSo you’ve heard all about the cool things that are happening in the machine learning world, and you want to join in. There’s just one problem – you don’t know how to code! 😱 Or maybe you’re a seasoned software engineer who wants to add some ML to your side-project, but you don’t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. That’s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature we’ve just added, we’re making “no-code” machine learning better than ever. Best of all, you can create your first project for ✨ free! ✨\\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with \\\\*\\\\*zero\\\\*\\\\* configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there\\'s even a good chance that you\\'ll end up with a model that performs better than a model that\\'s been hand-trained by an engineer 🤯 We’ve been expanding the number of tasks that we support, and we’re proud to announce that \\\\*\\\\*you can now use AutoTrain for Computer Vision\\\\*\\\\*!\\n\\nFor Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage. AutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate 🤗 Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details. ## The technical philosophy\\nIn this section, we wanted to share our philosophy behind adding support for Computer Vision in 🤗 Transformers so that the community is aware of the design choices specific to this area. Even though Transformers started with NLP, we support multiple modalities today, for example – vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:\\n- Easy model download with a single line of code with `from\\\\_pretrained()`\\n- Easy model upload with `push\\\\_to\\\\_hub()`\\n- Support for loading huge checkpoints with efficient checkpoint sharding techniques\\n- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))\\n- Initialization from model configurations\\n- Support for both PyTorch and TensorFlow (non-exhaustive)\\n- and many more\\nUnlike tokenizers, we have preprocessors (such as [this](https://huggingface.co/docs/transformers/model\\\\_doc/vit#transformers.ViTImageProcessor)) that take care of preparing data for the vision models. We have worked hard to ensure the user experience of using a vision model still feels easy and similar:\\n```py\\nfrom transformers import ViTImageProcessor, ViTForImageClassification\\nimport torch\\nfrom datasets import load\\\\_dataset\\ndataset = load\\\\_dataset(\"huggingface/cats-image\")\\nimage = dataset[\"test\"][\"image\"][0]\\nimage\\\\_processor  = ViTImageProcessor.from\\\\_pretrained(\"google/vit-base-patch16-224\")\\nmodel = ViTForImageClassification.from\\\\_pretrained(\"google/vit-base-patch16-224\")\\ninputs = image\\\\_processor(image, return\\\\_tensors=\"pt\")\\nwith torch.no\\\\_grad():\\nlogits = model(\\\\*\\\\*inputs).logits\\n# model predicts one of the 1000 ImageNet classes\\npredicted\\\\_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted\\\\_label])\\n# Egyptian cat\\n```\\nEven for a difficult task like object detection, the user experience doesn’t change very much:\\n```py\\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage\\\\_processor = AutoImageProcessor.from\\\\_pretrained(\"microsoft/conditional-detr-resnet-50\")\\nmodel = AutoModelForObjectDetection.from\\\\_pretrained(\"microsoft/conditional-detr-resnet-50\")\\ninputs = image\\\\_processor(images=image, return\\\\_tensors=\"pt\")\\noutputs = model(\\\\*\\\\*inputs)\\n# convert outputs (bounding boxes and class logits) to COCO API\\ntarget\\\\_sizes = torch.tensor([image.size[::-1]])\\nresults = image\\\\_processor.post\\\\_process\\\\_object\\\\_detection(\\noutputs, threshold=0.5, target\\\\_sizes=target\\\\_sizes\\n)[0]\\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\\nbox = [round(i, 2) for i in box.tolist()]\\nprint(\\nf\"Detected {model.config.id2label[label.item()]} with confidence \"\\nf\"{round(score.item(), 3)} at location {box}\"\\n)\\n```\\nLeads to:\\n```bash\\nDetected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\\nDetected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\\nDetected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\\nDetected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\\nDetected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\\n```\\n## Zero-shot models for vision\\nThere’s been a surge of models that reformulate core vision tasks like segmentation and detection in interesting ways and introduce even more flexibility.\\n\\nTasks\\n## What\\'s a task? Tasks, or pipeline types, describe the \"shape\" of each model\\'s API (inputs and outputs) and are used to determine which Inference API and widget we want to display for any given model. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/tasks.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/tasks-dark.png)\\n\\nThis classification is relatively coarse-grained (you can always add more fine-grained task names in your model tags), so \\\\*\\\\*you should rarely have to create a new task\\\\*\\\\*. If you want to add support for a new task, this document explains the required steps. ## Overview\\nHaving a new task integrated into the Hub means that:\\n\\\\* Users can search for all models – and datasets – of a given task. \\\\* The Inference API supports the task. \\\\* Users can try out models directly with the widget. 🏆\\nNote that you don\\'t need to implement all the steps by yourself. Adding a new task is a community effort, and multiple people can contribute. 🧑\\u200d🤝\\u200d🧑\\nTo begin the process, open a new issue in the [huggingface\\\\_hub](https://github.com/huggingface/huggingface\\\\_hub/issues) repository.', 'type': 'tool', 'name': 'data_retriever', 'id': '8f909771-ea7b-454b-8957-eafc0b2b0f2b', 'tool_call_id': 'call_BB0MbGX3mHnBKzdrxaivzD01', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab93f3e684812d15f6f88', 'page_content': \"--\\ntitle: Image Classification with AutoTrain\\nthumbnail: /blog/assets/105\\\\_autotrain-image-classification/thumbnail.png\\nauthors:\\n- user: nimaboscarino\\n---\\n# Image Classification with AutoTrain\\nSo you’ve heard all about the cool things that are happening in the machine learning world, and you want to join in. There’s just one problem – you don’t know how to code! 😱 Or maybe you’re a seasoned software engineer who wants to add some ML to your side-project, but you don’t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. That’s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature we’ve just added, we’re making “no-code” machine learning better than ever. Best of all, you can create your first project for ✨ free! ✨\\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with \\\\*\\\\*zero\\\\*\\\\* configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer 🤯 We’ve been expanding the number of tasks that we support, and we’re proud to announce that \\\\*\\\\*you can now use AutoTrain for Computer Vision\\\\*\\\\*!\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bf06666af48523f51d63', 'page_content': 'For Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage. AutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate 🤗 Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details. ## The technical philosophy\\nIn this section, we wanted to share our philosophy behind adding support for Computer Vision in 🤗 Transformers so that the community is aware of the design choices specific to this area. Even though Transformers started with NLP, we support multiple modalities today, for example – vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:\\n- Easy model download with a single line of code with `from\\\\_pretrained()`\\n- Easy model upload with `push\\\\_to\\\\_hub()`\\n- Support for loading huge checkpoints with efficient checkpoint sharding techniques\\n- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))\\n- Initialization from model configurations\\n- Support for both PyTorch and TensorFlow (non-exhaustive)\\n- and many more\\nUnlike tokenizers, we have preprocessors (such as [this](https://huggingface.co/docs/transformers/model\\\\_doc/vit#transformers.ViTImageProcessor)) that take care of preparing data for the vision models. We have worked hard to ensure the user experience of using a vision model still feels easy and similar:\\n```py\\nfrom transformers import ViTImageProcessor, ViTForImageClassification\\nimport torch\\nfrom datasets import load\\\\_dataset\\ndataset = load\\\\_dataset(\"huggingface/cats-image\")\\nimage = dataset[\"test\"][\"image\"][0]\\nimage\\\\_processor  = ViTImageProcessor.from\\\\_pretrained(\"google/vit-base-patch16-224\")\\nmodel = ViTForImageClassification.from\\\\_pretrained(\"google/vit-base-patch16-224\")\\ninputs = image\\\\_processor(image, return\\\\_tensors=\"pt\")\\nwith torch.no\\\\_grad():\\nlogits = model(\\\\*\\\\*inputs).logits\\n# model predicts one of the 1000 ImageNet classes\\npredicted\\\\_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted\\\\_label])\\n# Egyptian cat\\n```\\nEven for a difficult task like object detection, the user experience doesn’t change very much:\\n```py\\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage\\\\_processor = AutoImageProcessor.from\\\\_pretrained(\"microsoft/conditional-detr-resnet-50\")\\nmodel = AutoModelForObjectDetection.from\\\\_pretrained(\"microsoft/conditional-detr-resnet-50\")\\ninputs = image\\\\_processor(images=image, return\\\\_tensors=\"pt\")\\noutputs = model(\\\\*\\\\*inputs)\\n# convert outputs (bounding boxes and class logits) to COCO API\\ntarget\\\\_sizes = torch.tensor([image.size[::-1]])\\nresults = image\\\\_processor.post\\\\_process\\\\_object\\\\_detection(\\noutputs, threshold=0.5, target\\\\_sizes=target\\\\_sizes\\n)[0]\\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\\nbox = [round(i, 2) for i in box.tolist()]\\nprint(\\nf\"Detected {model.config.id2label[label.item()]} with confidence \"\\nf\"{round(score.item(), 3)} at location {box}\"\\n)\\n```\\nLeads to:\\n```bash\\nDetected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\\nDetected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\\nDetected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\\nDetected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\\nDetected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\\n```\\n## Zero-shot models for vision\\nThere’s been a surge of models that reformulate core vision tasks like segmentation and detection in interesting ways and introduce even more flexibility.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab95a3e684812d15f6fa1', 'page_content': 'Tasks\\n## What\\'s a task? Tasks, or pipeline types, describe the \"shape\" of each model\\'s API (inputs and outputs) and are used to determine which Inference API and widget we want to display for any given model. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/tasks.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/tasks-dark.png)\\n\\nThis classification is relatively coarse-grained (you can always add more fine-grained task names in your model tags), so \\\\*\\\\*you should rarely have to create a new task\\\\*\\\\*. If you want to add support for a new task, this document explains the required steps. ## Overview\\nHaving a new task integrated into the Hub means that:\\n\\\\* Users can search for all models – and datasets – of a given task. \\\\* The Inference API supports the task. \\\\* Users can try out models directly with the widget. 🏆\\nNote that you don\\'t need to implement all the steps by yourself. Adding a new task is a community effort, and multiple people can contribute. 🧑\\u200d🤝\\u200d🧑\\nTo begin the process, open a new issue in the [huggingface\\\\_hub](https://github.com/huggingface/huggingface\\\\_hub/issues) repository.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The latest task added to Hugging Face AutoTrain for Computer Vision is **image classification**.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--009a4daa-c63b-43e1-886d-4f441fbcf634-0', 'usage_metadata': {'input_tokens': 1979, 'output_tokens': 19, 'total_tokens': 1998, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:37:10,051 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 43%|████▎     | 28/65 [02:04<02:43,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the default repository type created by the `create_repo` function on Hugging Face Hub?\\n', 'type': 'human', 'id': 'fad9dc11-eacf-4204-ae98-b7ff31e9ad64'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_ptu1EQ6oregNX6Vl1Hrfz6NO', 'function': {'arguments': '{\"query\":\"default repository type created by the create_repo function on Hugging Face Hub\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--683fcbd2-93fa-4e14-8043-029f931f6a97-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'default repository type created by the create_repo function on Hugging Face Hub'}, 'id': 'call_ptu1EQ6oregNX6Vl1Hrfz6NO', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 368, 'output_tokens': 29, 'total_tokens': 397, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'It\\'s a model repo by default. Parameters:\\n- `type`: Type of repo (dataset or space; model by default). - `name`: Name of repo. - `organization`: Name of organization (optional). Payload:\\n```js\\npayload = {\\n\"type\": \"model\",\\n\"name\": \"name\",\\n\"organization\": \"organization\",\\n}\\n```\\nThis is equivalent to `huggingface\\\\_hub.delete\\\\_repo()`. ### PUT /api/repos/{repo\\\\_type}/{repo\\\\_id}/settings\\nUpdate repo visibility. Payload:\\n```js\\npayload = {\\n\"private\": \"private\",\\n}\\n```\\nThis is equivalent to `huggingface\\\\_hub.update\\\\_repo\\\\_visibility()`. ### POST /api/repos/move\\nMove a repository (rename within the same namespace or transfer from user to organization). Parameters:\\n- `fromRepo`: repo to rename. - `toRepo`: new name of the repo. - `type`: Type of repo (dataset or space; model by default). Payload:\\n```js\\npayload = {\\n\"fromRepo\" : \"namespace/repo\\\\_name\",\\n\"toRepo\" : \"namespace2/repo\\\\_name2\",\\n\"type\": \"model\",\\n}\\n```\\nThis is equivalent to `huggingface\\\\_hub.move\\\\_repo()`. ## User API\\nThe following endpoint gets information about a user. ### GET /api/whoami-v2\\nGet username and organizations the user belongs to. Payload:\\n```js\\nheaders = { \"authorization\" : \"Bearer $token\" }\\n```\\nThis is equivalent to `huggingface\\\\_hub.whoami()`.\\n\\nFirstly, there are a few methods to manage repository creation, deletion, and others:\\n```python no-format\\nfrom huggingface\\\\_hub import (\\n# User management\\nlogin,\\nlogout,\\nwhoami,\\n# Repository creation and management\\ncreate\\\\_repo,\\ndelete\\\\_repo,\\nupdate\\\\_repo\\\\_visibility,\\n# And some methods to retrieve/change information about the content\\nlist\\\\_models,\\nlist\\\\_datasets,\\nlist\\\\_metrics,\\nlist\\\\_repo\\\\_files,\\nupload\\\\_file,\\ndelete\\\\_file,\\n)\\n```\\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them. The `create\\\\_repo` method can be used to create a new repository on the hub:\\n```py\\nfrom huggingface\\\\_hub import create\\\\_repo\\ncreate\\\\_repo(\"dummy-model\")\\n```\\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\\n```py\\nfrom huggingface\\\\_hub import create\\\\_repo\\ncreate\\\\_repo(\"dummy-model\", organization=\"huggingface\")\\n```\\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization.\\n\\n!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# 创建和管理存储库\\nHugging Face Hub是一组 Git 存储库。[Git](https://git-scm.com/)是软件开发中广泛使用的工具，可以在协作工作时轻松对项目进行版本控制。本指南将向您展示如何与 Hub 上的存储库进行交互，特别关注以下内容：\\n- 创建和删除存储库\\n- 管理分支和标签\\n- 重命名您的存储库\\n- 更新您的存储库可见性\\n- 管理存储库的本地副本\\n如果您习惯于使用类似于GitLab/GitHub/Bitbucket等平台，您可能首先想到使用 `git`命令行工具来克隆存储库（`git clone`）、提交更改（`git add` , ` git commit`）并推送它们（`git push`）。在使用 Hugging Face Hub 时，这是有效的。然而，软件工程和机器学习并不具有相同的要求和工作流程。模型存储库可能会维护大量模型权重文件以适应不同的框架和工具，因此克隆存储库会导致您维护大量占用空间的本地文件夹。因此，使用我们的自定义HTTP方法可能更有效。您可以阅读我们的[git与HTTP相比较](../concepts/git\\\\_vs\\\\_http)解释页面以获取更多详细信息\\n如果你想在Hub上创建和管理一个仓库，你的计算机必须处于登录状态。如果尚未登录，请参考[此部分](../quick-start#login)。在本指南的其余部分，我们将假设你的计算机已登录\\n## 仓库创建和删除\\n第一步是了解如何创建和删除仓库。你只能管理你拥有的仓库（在你的用户名命名空间下）或者你具有写入权限的组织中的仓库\\n### 创建一个仓库\\n使用 [`create\\\\_repo`] 创建一个空仓库，并通过 `repo\\\\_id`参数为其命名 `repo\\\\_id`是你的命名空间，后面跟着仓库名称：`username\\\\_or\\\\_org/repo\\\\_name`\\n运行以下代码，以创建仓库：\\n```py\\n>>> from huggingface\\\\_hub import create\\\\_repo\\n>>> create\\\\_repo(\"lysandre/test-model\")\\n\\'https://huggingface.co/lysandre/test-model\\'\\n```\\n默认情况下，[`create\\\\_repo`] 会创建一个模型仓库。但是你可以使用 `repo\\\\_type`参数来指定其他仓库类型。例如，如果你想创建一个数据集仓库\\n请运行以下代码：\\n```py\\n>>> from huggingface\\\\_hub import create\\\\_repo\\n>>> create\\\\_repo(\"lysandre/test-dataset\", repo\\\\_type=\"dataset\")\\n\\'https://huggingface.co/datasets/lysandre/test-dataset\\'\\n```\\n创建仓库时，你可以使用 `private`参数设置仓库的可见性\\n请运行以下代码\\n```py\\n>>> from huggingface\\\\_hub import create\\\\_repo\\n>>> create\\\\_repo(\"lysandre/test-private\", private=True)\\n```\\n如果你想在以后更改仓库的可见性，你可以使用[`update\\\\_repo\\\\_visibility`] 函数\\n### 删除一个仓库\\n使用 [`delete\\\\_repo`] 删除一个仓库。确保你确实想要删除仓库，因为这是一个不可逆转的过程！做完上述过程后，指定你想要删除的仓库的 `repo\\\\_id`\\n请运行以下代码：\\n```py\\n>>> delete\\\\_repo(repo\\\\_id=\"lysandre/my-corrupted-dataset\", repo\\\\_type=\"dataset\")\\n```\\n### 克隆一个仓库（仅适用于 Spaces）\\n在某些情况下，你可能想要复制别人的仓库并根据自己的用例进行调整。对于 Spaces，你可以使用 [`duplicate\\\\_space`] 方法来实现。它将复制整个仓库。\\n你仍然需要配置自己的设置（硬件和密钥）。查看我们的[管理你的Space指南](./manage-spaces)以获取更多详细信息。\\n请运行以下代码：\\n```py\\n>>> from huggingface\\\\_hub import duplicate\\\\_space\\n>>> duplicate\\\\_space(\"multimodalart/dreambooth-training\", private=False)\\nRepoUrl(\\'https://huggingface.co/spaces/nateraw/dreambooth-training\\',...)\\n```\\n## 上传和下载文件\\n既然您已经创建了您的存储库，您现在也可以推送更改至其中并从中下载文件\\n这两个主题有它们自己的指南。请[上传指南](./upload) 和[下载指南](./download)来学习如何使用您的存储库。\\n## 分支和标签\\nGit存储库通常使用分支来存储同一存储库的不同版本。标签也可以用于标记存储库的特定状态，例如，在发布版本这个情况下。更一般地说，分支和标签被称为[git引用](https://git-scm.com/book/en/v2/Git-Internals-Git-References). ### 创建分支和标签\\n你可以使用[`create\\\\_branch`]和[`create\\\\_tag`]来创建新的分支和标签:\\n请运行以下代码：\\n```py\\n>>> from huggingface\\\\_hub import create\\\\_branch, create\\\\_tag\\n# Create a branch on a Space repo from `main` branch\\n>>> create\\\\_branch(\"Matthijs/speecht5-tts-demo\", repo\\\\_type=\"space\", branch=\"handle-dog-speaker\")\\n# Create a tag on a Dataset repo from `v0.1-release` branch\\n>>> create\\\\_branch(\"bigcode/the-stack\", repo\\\\_type=\"dataset\", revision=\"v0.1-release\", tag=\"v0.1.1\", tag\\\\_message=\"Bump release version.\")\\n```\\n同时,你可以以相同的方式使用 [`delete\\\\_branch`] 和 [`delete\\\\_tag`] 函数来删除分支或标签\\n### 列出所有的分支和标签\\n你还可以使用 [`list\\\\_repo\\\\_refs`] 列出存储库中的现有 Git 引用\\n请运行以下代码：\\n```py\\n>>> from huggingface\\\\_hub import list\\\\_repo\\\\_refs\\n>>> api.list\\\\_repo\\\\_refs(\"bigcode/the-stack\", repo\\\\_type=\"dataset\")\\nGitRefs(\\nbranches=[\\nGitRefInfo(name=\\'main\\', ref=\\'refs/heads/main\\', target\\\\_commit=\\'18edc1591d9ce72aa82f56c4431b3c969b210ae3\\'),\\nGitRefInfo(name=\\'v1.1.a1\\', ref=\\'refs/heads/v1.1.a1\\', target\\\\_commit=\\'f9826b862d1567f3822d3d25649b0d6d22ace714\\')\\n],\\nconverts=[],\\ntags=[\\nGitRefInfo(name=\\'v1.0\\', ref=\\'refs/tags/v1.0\\', target\\\\_commit=\\'c37a8cd1e382064d8aced5e05543c5f7753834da\\')\\n]\\n)\\n```\\n## 修改存储库设置\\n存储库具有一些可配置的设置。大多数情况下，您通常会在浏览器中的存储库设置页面上手动配置这些设置。要配置存储库，您必须具有对其的写访问权限（拥有它或属于组织）。在本节中，我们将看到您还可以使用 `huggingface\\\\_hub` 在编程方式上配置的设置。\\n一些设置是特定于 Spaces（硬件、环境变量等）的。要配置这些设置，请参考我们的[管理Spaces](../guides/manage-spaces)指南。\\n### 更新可见性\\n一个存储库可以是公共的或私有的。私有存储库仅对您或存储库所在组织的成员可见。\\n请运行以下代码将存储库更改为私有：\\n```py\\n>>> from huggingface\\\\_hub import update\\\\_repo\\\\_visibility\\n>>> update\\\\_repo\\\\_visibility(repo\\\\_id=repo\\\\_id, private=True)\\n```\\n### 重命名您的存储库\\n您可以使用 [`move\\\\_repo`] 在 Hub 上重命名您的存储库。使用这种方法，您还可以将存储库从一个用户移动到一个组织。在这样做时，有一些[限制](https://hf.co/docs/hub/repositories-settings#renaming-or-transferring-a-repo)需要注意。例如，您不能将存储库转移到另一个用户。\\n请运行以下代码：\\n```py\\n>>> from huggingface\\\\_hub import move\\\\_repo\\n>>> move\\\\_repo(from\\\\_id=\"Wauplin/cool-model\", to\\\\_id=\"huggingface/cool-model\")\\n```\\n## 管理存储库的本地副本\\n上述所有操作都可以通过HTTP请求完成。然而，在某些情况下，您可能希望在本地拥有存储库的副本，并使用您熟悉的Git命令与之交互。\\n[`Repository`] 类允许您使用类似于Git命令的函数与Hub上的文件和存储库进行交互。它是对Git和Git-LFS方法的包装，以使用您已经了解和喜爱的Git命令。在开始之前，请确保已安装Git-LFS（请参阅[此处](https://git-lfs.github.com/)获取安装说明）。\\n### 使用本地存储库\\n使用本地存储库路径实例化一个 [`Repository`] 对象：\\n请运行以下代码：\\n```py\\n>>> from huggingface\\\\_hub import Repository\\n>>> repo = Repository(local\\\\_dir=\"//\")\\n```\\n### 克隆\\n`clone\\\\_from`参数将一个存储库从Hugging Face存储库ID克隆到由 `local\\\\_dir`参数指定的本地目录：\\n请运行以下代码：\\n```py\\n>>> from huggingface\\\\_hub import Repository\\n>>> repo = Repository(local\\\\_dir=\"w2v2\", clone\\\\_from=\"facebook/wav2vec2-large-960h-lv60\")\\n```\\n`clone\\\\_from`还可以使用URL克隆存储库：\\n请运行以下代码：\\n```py\\n>>> repo = Repository(local\\\\_dir=\"huggingface-hub\", clone\\\\_from=\"https://huggingface.co/facebook/wav2vec2-large-960h-lv60\")\\n```\\n你可以将`clone\\\\_from`参数与[`create\\\\_repo`]结合使用，以创建并克隆一个存储库：\\n请运行以下代码：\\n```py\\n>>> repo\\\\_url = create\\\\_repo(repo\\\\_id=\"repo\\\\_name\")\\n>>> repo = Repository(local\\\\_dir=\"repo\\\\_local\\\\_path\", clone\\\\_from=repo\\\\_url)\\n```\\n当你克隆一个存储库时，通过在克隆时指定`git\\\\_user`和`git\\\\_email`参数，你还可以为克隆的存储库配置Git用户名和电子邮件。当用户提交到该存储库时，Git将知道提交的作者是谁。\\n请运行以下代码：\\n```py\\n>>> repo = Repository(\\n...', 'type': 'tool', 'name': 'data_retriever', 'id': '566b0055-3349-455b-b2e2-edbf5228d92f', 'tool_call_id': 'call_ptu1EQ6oregNX6Vl1Hrfz6NO', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aae813e684812d15f6346', 'page_content': 'It\\'s a model repo by default. Parameters:\\n- `type`: Type of repo (dataset or space; model by default). - `name`: Name of repo. - `organization`: Name of organization (optional). Payload:\\n```js\\npayload = {\\n\"type\": \"model\",\\n\"name\": \"name\",\\n\"organization\": \"organization\",\\n}\\n```\\nThis is equivalent to `huggingface\\\\_hub.delete\\\\_repo()`. ### PUT /api/repos/{repo\\\\_type}/{repo\\\\_id}/settings\\nUpdate repo visibility. Payload:\\n```js\\npayload = {\\n\"private\": \"private\",\\n}\\n```\\nThis is equivalent to `huggingface\\\\_hub.update\\\\_repo\\\\_visibility()`. ### POST /api/repos/move\\nMove a repository (rename within the same namespace or transfer from user to organization). Parameters:\\n- `fromRepo`: repo to rename. - `toRepo`: new name of the repo. - `type`: Type of repo (dataset or space; model by default). Payload:\\n```js\\npayload = {\\n\"fromRepo\" : \"namespace/repo\\\\_name\",\\n\"toRepo\" : \"namespace2/repo\\\\_name2\",\\n\"type\": \"model\",\\n}\\n```\\nThis is equivalent to `huggingface\\\\_hub.move\\\\_repo()`. ## User API\\nThe following endpoint gets information about a user. ### GET /api/whoami-v2\\nGet username and organizations the user belongs to. Payload:\\n```js\\nheaders = { \"authorization\" : \"Bearer $token\" }\\n```\\nThis is equivalent to `huggingface\\\\_hub.whoami()`.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b46caf3d2ebb270a01b3', 'page_content': 'Firstly, there are a few methods to manage repository creation, deletion, and others:\\n```python no-format\\nfrom huggingface\\\\_hub import (\\n# User management\\nlogin,\\nlogout,\\nwhoami,\\n# Repository creation and management\\ncreate\\\\_repo,\\ndelete\\\\_repo,\\nupdate\\\\_repo\\\\_visibility,\\n# And some methods to retrieve/change information about the content\\nlist\\\\_models,\\nlist\\\\_datasets,\\nlist\\\\_metrics,\\nlist\\\\_repo\\\\_files,\\nupload\\\\_file,\\ndelete\\\\_file,\\n)\\n```\\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them. The `create\\\\_repo` method can be used to create a new repository on the hub:\\n```py\\nfrom huggingface\\\\_hub import create\\\\_repo\\ncreate\\\\_repo(\"dummy-model\")\\n```\\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\\n```py\\nfrom huggingface\\\\_hub import create\\\\_repo\\ncreate\\\\_repo(\"dummy-model\", organization=\"huggingface\")\\n```\\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab4213e684812d15f6986', 'page_content': '!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# 创建和管理存储库\\nHugging Face Hub是一组 Git 存储库。[Git](https://git-scm.com/)是软件开发中广泛使用的工具，可以在协作工作时轻松对项目进行版本控制。本指南将向您展示如何与 Hub 上的存储库进行交互，特别关注以下内容：\\n- 创建和删除存储库\\n- 管理分支和标签\\n- 重命名您的存储库\\n- 更新您的存储库可见性\\n- 管理存储库的本地副本\\n如果您习惯于使用类似于GitLab/GitHub/Bitbucket等平台，您可能首先想到使用 `git`命令行工具来克隆存储库（`git clone`）、提交更改（`git add` , ` git commit`）并推送它们（`git push`）。在使用 Hugging Face Hub 时，这是有效的。然而，软件工程和机器学习并不具有相同的要求和工作流程。模型存储库可能会维护大量模型权重文件以适应不同的框架和工具，因此克隆存储库会导致您维护大量占用空间的本地文件夹。因此，使用我们的自定义HTTP方法可能更有效。您可以阅读我们的[git与HTTP相比较](../concepts/git\\\\_vs\\\\_http)解释页面以获取更多详细信息\\n如果你想在Hub上创建和管理一个仓库，你的计算机必须处于登录状态。如果尚未登录，请参考[此部分](../quick-start#login)。在本指南的其余部分，我们将假设你的计算机已登录\\n## 仓库创建和删除\\n第一步是了解如何创建和删除仓库。你只能管理你拥有的仓库（在你的用户名命名空间下）或者你具有写入权限的组织中的仓库\\n### 创建一个仓库\\n使用 [`create\\\\_repo`] 创建一个空仓库，并通过 `repo\\\\_id`参数为其命名 `repo\\\\_id`是你的命名空间，后面跟着仓库名称：`username\\\\_or\\\\_org/repo\\\\_name`\\n运行以下代码，以创建仓库：\\n```py\\n>>> from huggingface\\\\_hub import create\\\\_repo\\n>>> create\\\\_repo(\"lysandre/test-model\")\\n\\'https://huggingface.co/lysandre/test-model\\'\\n```\\n默认情况下，[`create\\\\_repo`] 会创建一个模型仓库。但是你可以使用 `repo\\\\_type`参数来指定其他仓库类型。例如，如果你想创建一个数据集仓库\\n请运行以下代码：\\n```py\\n>>> from huggingface\\\\_hub import create\\\\_repo\\n>>> create\\\\_repo(\"lysandre/test-dataset\", repo\\\\_type=\"dataset\")\\n\\'https://huggingface.co/datasets/lysandre/test-dataset\\'\\n```\\n创建仓库时，你可以使用 `private`参数设置仓库的可见性\\n请运行以下代码\\n```py\\n>>> from huggingface\\\\_hub import create\\\\_repo\\n>>> create\\\\_repo(\"lysandre/test-private\", private=True)\\n```\\n如果你想在以后更改仓库的可见性，你可以使用[`update\\\\_repo\\\\_visibility`] 函数\\n### 删除一个仓库\\n使用 [`delete\\\\_repo`] 删除一个仓库。确保你确实想要删除仓库，因为这是一个不可逆转的过程！做完上述过程后，指定你想要删除的仓库的 `repo\\\\_id`\\n请运行以下代码：\\n```py\\n>>> delete\\\\_repo(repo\\\\_id=\"lysandre/my-corrupted-dataset\", repo\\\\_type=\"dataset\")\\n```\\n### 克隆一个仓库（仅适用于 Spaces）\\n在某些情况下，你可能想要复制别人的仓库并根据自己的用例进行调整。对于 Spaces，你可以使用 [`duplicate\\\\_space`] 方法来实现。它将复制整个仓库。\\n你仍然需要配置自己的设置（硬件和密钥）。查看我们的[管理你的Space指南](./manage-spaces)以获取更多详细信息。\\n请运行以下代码：\\n```py\\n>>> from huggingface\\\\_hub import duplicate\\\\_space\\n>>> duplicate\\\\_space(\"multimodalart/dreambooth-training\", private=False)\\nRepoUrl(\\'https://huggingface.co/spaces/nateraw/dreambooth-training\\',...)\\n```\\n## 上传和下载文件\\n既然您已经创建了您的存储库，您现在也可以推送更改至其中并从中下载文件\\n这两个主题有它们自己的指南。请[上传指南](./upload) 和[下载指南](./download)来学习如何使用您的存储库。\\n## 分支和标签\\nGit存储库通常使用分支来存储同一存储库的不同版本。标签也可以用于标记存储库的特定状态，例如，在发布版本这个情况下。更一般地说，分支和标签被称为[git引用](https://git-scm.com/book/en/v2/Git-Internals-Git-References). ### 创建分支和标签\\n你可以使用[`create\\\\_branch`]和[`create\\\\_tag`]来创建新的分支和标签:\\n请运行以下代码：\\n```py\\n>>> from huggingface\\\\_hub import create\\\\_branch, create\\\\_tag\\n# Create a branch on a Space repo from `main` branch\\n>>> create\\\\_branch(\"Matthijs/speecht5-tts-demo\", repo\\\\_type=\"space\", branch=\"handle-dog-speaker\")\\n# Create a tag on a Dataset repo from `v0.1-release` branch\\n>>> create\\\\_branch(\"bigcode/the-stack\", repo\\\\_type=\"dataset\", revision=\"v0.1-release\", tag=\"v0.1.1\", tag\\\\_message=\"Bump release version.\")\\n```\\n同时,你可以以相同的方式使用 [`delete\\\\_branch`] 和 [`delete\\\\_tag`] 函数来删除分支或标签\\n### 列出所有的分支和标签\\n你还可以使用 [`list\\\\_repo\\\\_refs`] 列出存储库中的现有 Git 引用\\n请运行以下代码：\\n```py\\n>>> from huggingface\\\\_hub import list\\\\_repo\\\\_refs\\n>>> api.list\\\\_repo\\\\_refs(\"bigcode/the-stack\", repo\\\\_type=\"dataset\")\\nGitRefs(\\nbranches=[\\nGitRefInfo(name=\\'main\\', ref=\\'refs/heads/main\\', target\\\\_commit=\\'18edc1591d9ce72aa82f56c4431b3c969b210ae3\\'),\\nGitRefInfo(name=\\'v1.1.a1\\', ref=\\'refs/heads/v1.1.a1\\', target\\\\_commit=\\'f9826b862d1567f3822d3d25649b0d6d22ace714\\')\\n],\\nconverts=[],\\ntags=[\\nGitRefInfo(name=\\'v1.0\\', ref=\\'refs/tags/v1.0\\', target\\\\_commit=\\'c37a8cd1e382064d8aced5e05543c5f7753834da\\')\\n]\\n)\\n```\\n## 修改存储库设置\\n存储库具有一些可配置的设置。大多数情况下，您通常会在浏览器中的存储库设置页面上手动配置这些设置。要配置存储库，您必须具有对其的写访问权限（拥有它或属于组织）。在本节中，我们将看到您还可以使用 `huggingface\\\\_hub` 在编程方式上配置的设置。\\n一些设置是特定于 Spaces（硬件、环境变量等）的。要配置这些设置，请参考我们的[管理Spaces](../guides/manage-spaces)指南。\\n### 更新可见性\\n一个存储库可以是公共的或私有的。私有存储库仅对您或存储库所在组织的成员可见。\\n请运行以下代码将存储库更改为私有：\\n```py\\n>>> from huggingface\\\\_hub import update\\\\_repo\\\\_visibility\\n>>> update\\\\_repo\\\\_visibility(repo\\\\_id=repo\\\\_id, private=True)\\n```\\n### 重命名您的存储库\\n您可以使用 [`move\\\\_repo`] 在 Hub 上重命名您的存储库。使用这种方法，您还可以将存储库从一个用户移动到一个组织。在这样做时，有一些[限制](https://hf.co/docs/hub/repositories-settings#renaming-or-transferring-a-repo)需要注意。例如，您不能将存储库转移到另一个用户。\\n请运行以下代码：\\n```py\\n>>> from huggingface\\\\_hub import move\\\\_repo\\n>>> move\\\\_repo(from\\\\_id=\"Wauplin/cool-model\", to\\\\_id=\"huggingface/cool-model\")\\n```\\n## 管理存储库的本地副本\\n上述所有操作都可以通过HTTP请求完成。然而，在某些情况下，您可能希望在本地拥有存储库的副本，并使用您熟悉的Git命令与之交互。\\n[`Repository`] 类允许您使用类似于Git命令的函数与Hub上的文件和存储库进行交互。它是对Git和Git-LFS方法的包装，以使用您已经了解和喜爱的Git命令。在开始之前，请确保已安装Git-LFS（请参阅[此处](https://git-lfs.github.com/)获取安装说明）。\\n### 使用本地存储库\\n使用本地存储库路径实例化一个 [`Repository`] 对象：\\n请运行以下代码：\\n```py\\n>>> from huggingface\\\\_hub import Repository\\n>>> repo = Repository(local\\\\_dir=\"//\")\\n```\\n### 克隆\\n`clone\\\\_from`参数将一个存储库从Hugging Face存储库ID克隆到由 `local\\\\_dir`参数指定的本地目录：\\n请运行以下代码：\\n```py\\n>>> from huggingface\\\\_hub import Repository\\n>>> repo = Repository(local\\\\_dir=\"w2v2\", clone\\\\_from=\"facebook/wav2vec2-large-960h-lv60\")\\n```\\n`clone\\\\_from`还可以使用URL克隆存储库：\\n请运行以下代码：\\n```py\\n>>> repo = Repository(local\\\\_dir=\"huggingface-hub\", clone\\\\_from=\"https://huggingface.co/facebook/wav2vec2-large-960h-lv60\")\\n```\\n你可以将`clone\\\\_from`参数与[`create\\\\_repo`]结合使用，以创建并克隆一个存储库：\\n请运行以下代码：\\n```py\\n>>> repo\\\\_url = create\\\\_repo(repo\\\\_id=\"repo\\\\_name\")\\n>>> repo = Repository(local\\\\_dir=\"repo\\\\_local\\\\_path\", clone\\\\_from=repo\\\\_url)\\n```\\n当你克隆一个存储库时，通过在克隆时指定`git\\\\_user`和`git\\\\_email`参数，你还可以为克隆的存储库配置Git用户名和电子邮件。当用户提交到该存储库时，Git将知道提交的作者是谁。\\n请运行以下代码：\\n```py\\n>>> repo = Repository(\\n...', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--b914fd82-e4fd-4172-89dc-971f88fac047-0', 'usage_metadata': {'input_tokens': 3357, 'output_tokens': 22, 'total_tokens': 3379, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:37:13,800 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 45%|████▍     | 29/65 [02:08<02:31,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'How many splits does the \"duorc\" dataset have?\\n', 'type': 'human', 'id': '00d2381a-f198-498e-b922-744794eb4f7e'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_t6m04xXhLrCPl75Th5pBUgfw', 'function': {'arguments': '{\"query\":\"duorc dataset splits\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--ed529e35-3784-4f16-968a-06ddb3be8e58-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'duorc dataset splits'}, 'id': 'call_t6m04xXhLrCPl75Th5pBUgfw', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 360, 'output_tokens': 19, 'total_tokens': 379, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'List splits and configurations\\nDatasets typically have splits and may also have configurations. A \\\\_split\\\\_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A \\\\_configuration\\\\_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you\\'re interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load\\\\_hub)! This guide shows you how to use Datasets Server\\'s `/splits` endpoint to retrieve a dataset\\'s splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\\nThe `/splits` endpoint accepts the dataset name as its query parameter:\\n\\n```python\\nimport requests\\nheaders = {\"Authorization\": f\"Bearer {API\\\\_TOKEN}\"}\\nAPI\\\\_URL = \"https://datasets-server.huggingface.co/splits?dataset=duorc\"\\ndef query():\\nresponse = requests.get(API\\\\_URL, headers=headers)\\nreturn response.json()\\ndata = query()\\n```\\n\\n```js\\nimport fetch from \"node-fetch\";\\nasync function query(data) {\\nconst response = await fetch(\\n\"https://datasets-server.huggingface.co/splits?dataset=duorc\",\\n{\\nheaders: { Authorization: `Bearer ${API\\\\_TOKEN}` },\\nmethod: \"GET\"\\n}\\n);\\nconst result = await response.json();\\nreturn result;\\n}\\nquery().then((response) => {\\nconsole.log(JSON.stringify(response));\\n});\\n```\\n\\n```curl\\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\\\\n-X GET \\\\\\n-H \"Authorization: Bearer ${API\\\\_TOKEN}\"\\n```\\nThe endpoint response is a JSON containing a list of the dataset\\'s splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\\n```json\\n{\\n\"splits\": [\\n{ \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\\n{ \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"validation\" },\\n{ \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"test\" },\\n{ \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"train\" },\\n{ \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"validation\" },\\n{ \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"test\" }\\n],\\n\"pending\": [],\\n\"failed\": []\\n}\\n```\\n\\nIts structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package\\\\_reference/main\\\\_classes#datasets.DatasetInfo) object of the `datasets` library. ```json\\n{\\n\"dataset\\\\_info\": {\\n\"description\": \"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\\\n\",\\n\"citation\": \"@inproceedings{DuoRC,\\\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\\\nyear = {2018}\\\\n}\\\\n\",\\n\"homepage\": \"https://duorc.github.io/\",\\n\"license\": \"https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\",\\n\"features\": {\\n\"plot\\\\_id\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"plot\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"title\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"question\\\\_id\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"question\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"answers\": {\\n\"feature\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"\\\\_type\": \"Sequence\"\\n},\\n\"no\\\\_answer\": {\\n\"dtype\": \"bool\",\\n\"\\\\_type\": \"Value\"\\n}\\n},\\n\"builder\\\\_name\": \"duorc\",\\n\"config\\\\_name\": \"SelfRC\",\\n\"version\": {\\n\"version\\\\_str\": \"1.0.0\",\\n\"major\": 1,\\n\"minor\": 0,\\n\"patch\": 0\\n},\\n\"splits\": {\\n\"train\": {\\n\"name\": \"train\",\\n\"num\\\\_bytes\": 239852729,\\n\"num\\\\_examples\": 60721,\\n\"dataset\\\\_name\": \"duorc\"\\n},\\n\"validation\": {\\n\"name\": \"validation\",\\n\"num\\\\_bytes\": 51662519,\\n\"num\\\\_examples\": 12961,\\n\"dataset\\\\_name\": \"duorc\"\\n},\\n\"test\": {\\n\"name\": \"test\",\\n\"num\\\\_bytes\": 49142710,\\n\"num\\\\_examples\": 12559,\\n\"dataset\\\\_name\": \"duorc\"\\n}\\n},\\n\"download\\\\_checksums\": {\\n\"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\\\_train.json\": {\\n\"num\\\\_bytes\": 24388192,\\n\"checksum\": null\\n},\\n\"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\\\_dev.json\": {\\n\"num\\\\_bytes\": 5051240,\\n\"checksum\": null\\n},\\n\"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\\\_test.json\": {\\n\"num\\\\_bytes\": 5023228,\\n\"checksum\": null\\n}\\n},\\n\"download\\\\_size\": 34462660,\\n\"dataset\\\\_size\": 340657958,\\n\"size\\\\_in\\\\_bytes\": 375120618\\n}\\n}\\n```\\n\\nIt provides the number of rows, the number of colums (where applicable) and the size in bytes for the different forms of the data: original files, size in memory (RAM) and auto-converted parquet files. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has 187.213 rows along all its configurations and splits, for a total of 97MB. ```json\\n{\\n\"size\": {\\n\"dataset\": {\\n\"dataset\": \"duorc\",\\n\"num\\\\_bytes\\\\_original\\\\_files\": 97383710,\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 58710973,\\n\"num\\\\_bytes\\\\_memory\": 1059067116,\\n\"num\\\\_rows\": 187213\\n},\\n\"configs\": [\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"num\\\\_bytes\\\\_original\\\\_files\": 62921050,\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 37709127,\\n\"num\\\\_bytes\\\\_memory\": 718409158,\\n\"num\\\\_rows\": 100972,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"num\\\\_bytes\\\\_original\\\\_files\": 34462660,\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 21001846,\\n\"num\\\\_bytes\\\\_memory\": 340657958,\\n\"num\\\\_rows\": 86241,\\n\"num\\\\_columns\": 7\\n}\\n],\\n\"splits\": [\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"split\": \"train\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 26005668,\\n\"num\\\\_bytes\\\\_memory\": 496682909,\\n\"num\\\\_rows\": 69524,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"split\": \"validation\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 5566868,\\n\"num\\\\_bytes\\\\_memory\": 106510489,\\n\"num\\\\_rows\": 15591,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"split\": \"test\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 6136591,\\n\"num\\\\_bytes\\\\_memory\": 115215760,\\n\"num\\\\_rows\": 15857,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"split\": \"train\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 14851720,\\n\"num\\\\_bytes\\\\_memory\": 239852729,\\n\"num\\\\_rows\": 60721,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"split\": \"validation\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 3114390,\\n\"num\\\\_bytes\\\\_memory\": 51662519,\\n\"num\\\\_rows\": 12961,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"split\": \"test\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 3035736,\\n\"num\\\\_bytes\\\\_memory\": 49142710,\\n\"num\\\\_rows\": 12559,\\n\"num\\\\_columns\": 7\\n}\\n]\\n},\\n\"pending\": [],\\n\"failed\": [],\\n\"partial\": false\\n}\\n```\\nIf the size has `partial: true` it means that the actual size of the dataset couldn\\'t been determined because it\\'s too big. In that case the number of rows and bytes can be inferior to the actual numbers.', 'type': 'tool', 'name': 'data_retriever', 'id': '60c28427-b0d5-4e87-9594-48eb274cce07', 'tool_call_id': 'call_t6m04xXhLrCPl75Th5pBUgfw', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab9bc3e684812d15f7024', 'page_content': 'List splits and configurations\\nDatasets typically have splits and may also have configurations. A \\\\_split\\\\_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A \\\\_configuration\\\\_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you\\'re interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load\\\\_hub)! This guide shows you how to use Datasets Server\\'s `/splits` endpoint to retrieve a dataset\\'s splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\\nThe `/splits` endpoint accepts the dataset name as its query parameter:\\n\\n```python\\nimport requests\\nheaders = {\"Authorization\": f\"Bearer {API\\\\_TOKEN}\"}\\nAPI\\\\_URL = \"https://datasets-server.huggingface.co/splits?dataset=duorc\"\\ndef query():\\nresponse = requests.get(API\\\\_URL, headers=headers)\\nreturn response.json()\\ndata = query()\\n```\\n\\n```js\\nimport fetch from \"node-fetch\";\\nasync function query(data) {\\nconst response = await fetch(\\n\"https://datasets-server.huggingface.co/splits?dataset=duorc\",\\n{\\nheaders: { Authorization: `Bearer ${API\\\\_TOKEN}` },\\nmethod: \"GET\"\\n}\\n);\\nconst result = await response.json();\\nreturn result;\\n}\\nquery().then((response) => {\\nconsole.log(JSON.stringify(response));\\n});\\n```\\n\\n```curl\\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\\\\n-X GET \\\\\\n-H \"Authorization: Bearer ${API\\\\_TOKEN}\"\\n```\\nThe endpoint response is a JSON containing a list of the dataset\\'s splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\\n```json\\n{\\n\"splits\": [\\n{ \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\\n{ \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"validation\" },\\n{ \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"test\" },\\n{ \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"train\" },\\n{ \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"validation\" },\\n{ \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"test\" }\\n],\\n\"pending\": [],\\n\"failed\": []\\n}\\n```', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bca7af3d2ebb270a0b44', 'page_content': 'Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package\\\\_reference/main\\\\_classes#datasets.DatasetInfo) object of the `datasets` library. ```json\\n{\\n\"dataset\\\\_info\": {\\n\"description\": \"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\\\n\",\\n\"citation\": \"@inproceedings{DuoRC,\\\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\\\nyear = {2018}\\\\n}\\\\n\",\\n\"homepage\": \"https://duorc.github.io/\",\\n\"license\": \"https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\",\\n\"features\": {\\n\"plot\\\\_id\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"plot\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"title\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"question\\\\_id\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"question\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"answers\": {\\n\"feature\": {\\n\"dtype\": \"string\",\\n\"\\\\_type\": \"Value\"\\n},\\n\"\\\\_type\": \"Sequence\"\\n},\\n\"no\\\\_answer\": {\\n\"dtype\": \"bool\",\\n\"\\\\_type\": \"Value\"\\n}\\n},\\n\"builder\\\\_name\": \"duorc\",\\n\"config\\\\_name\": \"SelfRC\",\\n\"version\": {\\n\"version\\\\_str\": \"1.0.0\",\\n\"major\": 1,\\n\"minor\": 0,\\n\"patch\": 0\\n},\\n\"splits\": {\\n\"train\": {\\n\"name\": \"train\",\\n\"num\\\\_bytes\": 239852729,\\n\"num\\\\_examples\": 60721,\\n\"dataset\\\\_name\": \"duorc\"\\n},\\n\"validation\": {\\n\"name\": \"validation\",\\n\"num\\\\_bytes\": 51662519,\\n\"num\\\\_examples\": 12961,\\n\"dataset\\\\_name\": \"duorc\"\\n},\\n\"test\": {\\n\"name\": \"test\",\\n\"num\\\\_bytes\": 49142710,\\n\"num\\\\_examples\": 12559,\\n\"dataset\\\\_name\": \"duorc\"\\n}\\n},\\n\"download\\\\_checksums\": {\\n\"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\\\_train.json\": {\\n\"num\\\\_bytes\": 24388192,\\n\"checksum\": null\\n},\\n\"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\\\_dev.json\": {\\n\"num\\\\_bytes\": 5051240,\\n\"checksum\": null\\n},\\n\"https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\\\_test.json\": {\\n\"num\\\\_bytes\": 5023228,\\n\"checksum\": null\\n}\\n},\\n\"download\\\\_size\": 34462660,\\n\"dataset\\\\_size\": 340657958,\\n\"size\\\\_in\\\\_bytes\": 375120618\\n}\\n}\\n```', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab34b3e684812d15f687c', 'page_content': 'It provides the number of rows, the number of colums (where applicable) and the size in bytes for the different forms of the data: original files, size in memory (RAM) and auto-converted parquet files. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has 187.213 rows along all its configurations and splits, for a total of 97MB. ```json\\n{\\n\"size\": {\\n\"dataset\": {\\n\"dataset\": \"duorc\",\\n\"num\\\\_bytes\\\\_original\\\\_files\": 97383710,\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 58710973,\\n\"num\\\\_bytes\\\\_memory\": 1059067116,\\n\"num\\\\_rows\": 187213\\n},\\n\"configs\": [\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"num\\\\_bytes\\\\_original\\\\_files\": 62921050,\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 37709127,\\n\"num\\\\_bytes\\\\_memory\": 718409158,\\n\"num\\\\_rows\": 100972,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"num\\\\_bytes\\\\_original\\\\_files\": 34462660,\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 21001846,\\n\"num\\\\_bytes\\\\_memory\": 340657958,\\n\"num\\\\_rows\": 86241,\\n\"num\\\\_columns\": 7\\n}\\n],\\n\"splits\": [\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"split\": \"train\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 26005668,\\n\"num\\\\_bytes\\\\_memory\": 496682909,\\n\"num\\\\_rows\": 69524,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"split\": \"validation\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 5566868,\\n\"num\\\\_bytes\\\\_memory\": 106510489,\\n\"num\\\\_rows\": 15591,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"ParaphraseRC\",\\n\"split\": \"test\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 6136591,\\n\"num\\\\_bytes\\\\_memory\": 115215760,\\n\"num\\\\_rows\": 15857,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"split\": \"train\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 14851720,\\n\"num\\\\_bytes\\\\_memory\": 239852729,\\n\"num\\\\_rows\": 60721,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"split\": \"validation\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 3114390,\\n\"num\\\\_bytes\\\\_memory\": 51662519,\\n\"num\\\\_rows\": 12961,\\n\"num\\\\_columns\": 7\\n},\\n{\\n\"dataset\": \"duorc\",\\n\"config\": \"SelfRC\",\\n\"split\": \"test\",\\n\"num\\\\_bytes\\\\_parquet\\\\_files\": 3035736,\\n\"num\\\\_bytes\\\\_memory\": 49142710,\\n\"num\\\\_rows\": 12559,\\n\"num\\\\_columns\": 7\\n}\\n]\\n},\\n\"pending\": [],\\n\"failed\": [],\\n\"partial\": false\\n}\\n```\\nIf the size has `partial: true` it means that the actual size of the dataset couldn\\'t been determined because it\\'s too big. In that case the number of rows and bytes can be inferior to the actual numbers.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The \"duorc\" dataset has six splits.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--09629b57-5427-4bc1-b6ba-61e6fd1607ad-0', 'usage_metadata': {'input_tokens': 2532, 'output_tokens': 10, 'total_tokens': 2542, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:37:19,494 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 46%|████▌     | 30/65 [02:13<02:43,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\\n', 'type': 'human', 'id': '668a7e7d-ff87-4bf4-9b78-3f225cbd839e'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_kJMTbbq05gWoBJpXxITewpw1', 'function': {'arguments': '{\"query\":\"Fully Sharded Data Parallel (FSDP) purpose in distributed training\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--08ff39b3-cdca-4e01-a3ee-da3f141fa5c0-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Fully Sharded Data Parallel (FSDP) purpose in distributed training'}, 'id': 'call_kJMTbbq05gWoBJpXxITewpw1', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 367, 'output_tokens': 29, 'total_tokens': 396, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Fully Sharded Data Parallel\\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model\\'s parameters, gradients and optimizer states across the number of available GPUs (also called workers or \\\\*rank\\\\*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class. Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer. ```bash\\npip install accelerate\\n```\\n## FSDP configuration\\nTo start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package\\\\_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`. ```bash\\naccelerate config\\n```\\nWhen you run `accelerate config`, you\\'ll be prompted with a series of options to configure your training environment.\\n\\n--\\ntitle: \"Fine-tuning Llama 2 70B using PyTorch FSDP\"\\nthumbnail: /blog/assets/160\\\\_fsdp\\\\_llama/thumbnail.jpg\\nauthors:\\n- user: smangrul\\n- user: sgugger\\n- user: lewtun\\n- user: philschmid\\n---\\n# Fine-tuning Llama 2 70B using PyTorch FSDP\\n## Introduction\\nIn this blog post, we will look at how to fine-tune Llama 2 70B using PyTorch FSDP and related best practices. We will be leveraging Hugging Face Transformers, Accelerate and TRL. We will also learn how to use Accelerate with SLURM. Fully Sharded Data Parallelism (FSDP) is a paradigm in which the optimizer states, gradients and parameters are sharded across devices. During the forward pass, each FSDP unit performs an \\\\_all-gather operation\\\\_ to get the complete weights, computation is performed followed by discarding the shards from other devices. After the forward pass, the loss is computed followed by the backward pass. In the backward pass, each FSDP unit performs an all-gather operation to get the complete weights, with computation performed to get the local gradients. These local gradients are averaged and sharded across the devices via a \\\\_reduce-scatter operation\\\\_ so that each device can update the parameters of its shard. For more information on what PyTorch FSDP is, please refer to this blog post: [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface.co/blog/pytorch-fsdp). ![FSDP Workflow](./assets/62\\\\_pytorch\\\\_fsdp/FSDP\\\\_workflow.png)\\n(Source: [link](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/))\\n## Hardware Used\\nNumber of nodes: 2. Minimum required is 1. Number of GPUs per node: 8\\nGPU type: A100\\nGPU memory: 80GB\\nintra-node connection: NVLink\\nRAM per node: 1TB\\nCPU cores per node: 96\\ninter-node connection: Elastic Fabric Adapter\\n## Challenges with fine-tuning LLaMa 70B\\nWe encountered three main challenges when trying to fine-tune LLaMa 70B with FSDP:\\n1. FSDP wraps the model after loading the pre-trained model. If each process/rank within a node loads the Llama-70B model, it would require 70\\\\\\\\*4\\\\\\\\*8 GB ~ 2TB of CPU RAM, where 4 is the number of bytes per parameter and 8 is the number of GPUs on each node.\\n\\n--\\ntitle: \"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\"\\nthumbnail: /blog/assets/62\\\\_pytorch\\\\_fsdp/fsdp-thumbnail.png\\nauthors:\\n- user: smangrul\\n- user: sgugger\\n---\\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\\nIn this post we will look at how we can leverage \\\\*\\\\*[Accelerate](https://github.com/huggingface/accelerate)\\\\*\\\\* Library for training large models which enables users to leverage the latest features of \\\\*\\\\*[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)\\\\*\\\\*. # Motivation 🤗\\n\\\\*\\\\*With the ever increasing scale, size and parameters of the Machine Learning (ML) models, ML practitioners are finding it difficult to train or even load such large models on their hardware.\\\\*\\\\* On one hand, it has been found that large models learn quickly (data and compute efficient) and are significantly more performant when compared to smaller models [1]; on the other hand, it becomes prohibitive to train such models on most of the available hardware.', 'type': 'tool', 'name': 'data_retriever', 'id': '98c30f53-1055-44ae-9ef3-34bb173804fc', 'tool_call_id': 'call_kJMTbbq05gWoBJpXxITewpw1', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab6403e684812d15f6c35', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Fully Sharded Data Parallel\\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model\\'s parameters, gradients and optimizer states across the number of available GPUs (also called workers or \\\\*rank\\\\*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class. Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer. ```bash\\npip install accelerate\\n```\\n## FSDP configuration\\nTo start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package\\\\_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`. ```bash\\naccelerate config\\n```\\nWhen you run `accelerate config`, you\\'ll be prompted with a series of options to configure your training environment.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aed50e275123dbb256a02', 'page_content': '--\\ntitle: \"Fine-tuning Llama 2 70B using PyTorch FSDP\"\\nthumbnail: /blog/assets/160\\\\_fsdp\\\\_llama/thumbnail.jpg\\nauthors:\\n- user: smangrul\\n- user: sgugger\\n- user: lewtun\\n- user: philschmid\\n---\\n# Fine-tuning Llama 2 70B using PyTorch FSDP\\n## Introduction\\nIn this blog post, we will look at how to fine-tune Llama 2 70B using PyTorch FSDP and related best practices. We will be leveraging Hugging Face Transformers, Accelerate and TRL. We will also learn how to use Accelerate with SLURM. Fully Sharded Data Parallelism (FSDP) is a paradigm in which the optimizer states, gradients and parameters are sharded across devices. During the forward pass, each FSDP unit performs an \\\\_all-gather operation\\\\_ to get the complete weights, computation is performed followed by discarding the shards from other devices. After the forward pass, the loss is computed followed by the backward pass. In the backward pass, each FSDP unit performs an all-gather operation to get the complete weights, with computation performed to get the local gradients. These local gradients are averaged and sharded across the devices via a \\\\_reduce-scatter operation\\\\_ so that each device can update the parameters of its shard. For more information on what PyTorch FSDP is, please refer to this blog post: [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface.co/blog/pytorch-fsdp). ![FSDP Workflow](./assets/62\\\\_pytorch\\\\_fsdp/FSDP\\\\_workflow.png)\\n(Source: [link](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/))\\n## Hardware Used\\nNumber of nodes: 2. Minimum required is 1. Number of GPUs per node: 8\\nGPU type: A100\\nGPU memory: 80GB\\nintra-node connection: NVLink\\nRAM per node: 1TB\\nCPU cores per node: 96\\ninter-node connection: Elastic Fabric Adapter\\n## Challenges with fine-tuning LLaMa 70B\\nWe encountered three main challenges when trying to fine-tune LLaMa 70B with FSDP:\\n1. FSDP wraps the model after loading the pre-trained model. If each process/rank within a node loads the Llama-70B model, it would require 70\\\\\\\\*4\\\\\\\\*8 GB ~ 2TB of CPU RAM, where 4 is the number of bytes per parameter and 8 is the number of GPUs on each node.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab14a3e684812d15f6633', 'page_content': '--\\ntitle: \"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\"\\nthumbnail: /blog/assets/62\\\\_pytorch\\\\_fsdp/fsdp-thumbnail.png\\nauthors:\\n- user: smangrul\\n- user: sgugger\\n---\\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\\nIn this post we will look at how we can leverage \\\\*\\\\*[Accelerate](https://github.com/huggingface/accelerate)\\\\*\\\\* Library for training large models which enables users to leverage the latest features of \\\\*\\\\*[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)\\\\*\\\\*. # Motivation 🤗\\n\\\\*\\\\*With the ever increasing scale, size and parameters of the Machine Learning (ML) models, ML practitioners are finding it difficult to train or even load such large models on their hardware.\\\\*\\\\* On one hand, it has been found that large models learn quickly (data and compute efficient) and are significantly more performant when compared to smaller models [1]; on the other hand, it becomes prohibitive to train such models on most of the available hardware.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"Fully Sharded Data Parallel (FSDP) is a data parallel method that shards a model's parameters, gradients, and optimizer states across the available GPUs (also called workers or ranks). The purpose of FSDP in distributed training is to reduce memory usage by not replicating the entire model on each GPU, which improves GPU memory efficiency. This allows practitioners to train much larger models on fewer GPUs compared to traditional methods like DistributedDataParallel (DDP). FSDP is integrated with the Accelerate library, making it easier to manage training in distributed environments.\", 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--cd711959-0251-4530-9715-0f9271342ff2-0', 'usage_metadata': {'input_tokens': 1581, 'output_tokens': 113, 'total_tokens': 1694, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:37:23,650 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 48%|████▊     | 31/65 [02:18<02:33,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What file format is used to save and store PyTorch model weights more securely than `.bin` files?\\n', 'type': 'human', 'id': 'dcea9e6e-f67d-4908-bf6c-03b44a60ded5'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_TJ9WCBHZ0mYoo8qWhOHcOP8C', 'function': {'arguments': '{\"query\":\"PyTorch model weights file format more secure than .bin\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--6163cc2d-7fe2-456a-91b7-9ebeae836a07-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'PyTorch model weights file format more secure than .bin'}, 'id': 'call_TJ9WCBHZ0mYoo8qWhOHcOP8C', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 369, 'output_tokens': 26, 'total_tokens': 395, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': \"We are also taking steps to make sure the existing PyTorch files are not malicious, but the best we can do is flag suspicious-looking files. Of course, there are other file formats out there, but\\nnone seemed to meet the full set of [ideal requirements](https://github.com/huggingface/safetensors#yet-another-format-) our team identified. In addition to being safe, `safetensors` allows lazy loading and generally faster loads (around 100x faster on CPU). Lazy loading means loading only part of a tensor in an efficient manner. This particular feature enables arbitrary sharding with efficient inference libraries, such as [text-generation-inference](https://github.com/huggingface/text-generation-inference), to load LLMs (such as LLaMA, StarCoder, etc.) on various types of hardware\\nwith maximum efficiency. Because it loads so fast and is framework agnostic, we can even use the format\\nto load models from the same file in PyTorch or TensorFlow. ## The security audit\\nSince `safetensors` main asset is providing safety guarantees, we wanted to make sure\\nit actually delivered. That's why Hugging Face, EleutherAI, and Stability AI teamed up to get an external\\nsecurity audit to confirm it. Important findings:\\n- No critical security flaw leading to arbitrary code execution was found. - Some imprecisions in the spec format were detected and fixed. - Some missing validation allowed [polyglot files](https://en.wikipedia.org/wiki/Polyglot\\\\_(computing)), which was fixed. - Lots of improvements to the test suite were proposed and implemented. In the name of openness and transparency, all companies agreed to make the report\\nfully public. [Full report](https://huggingface.co/datasets/safetensors/trail\\\\_of\\\\_bits\\\\_audit\\\\_repot/resolve/main/SOW-TrailofBits-EleutherAI\\\\_HuggingFace-v1.2.pdf)\\nOne import thing to note is that the library is written in Rust.\\n\\nConvert weights to safetensors\\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`. The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository. For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights. Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.\\n\\nOn CPU, if the file is already in cache, then it can\\ntruly be zero-copy, whereas on GPU there is not such disk cache, so a copy is always required\\nbut you can bypass allocating all the tensors on CPU at any given point. SafeTensors is not zero-copy for the header. The choice of JSON is pretty arbitrary, but since deserialization is <<< of the time required to load the actual tensor data and is readable I went that way, (also space is <<< to the tensor data). - Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the\\nmoment. - Order: 'C' or row-major. This seems to have won. We can add that information later if needed. - Stride: No striding, all tensors need to be packed before being serialized. I have yet to see a case where it seems useful to have a strided tensor stored in serialized format. ### Benefits\\nSince we can invent a new format we can propose additional benefits:\\n- Prevent DOS attacks: We can craft the format in such a way that it's almost\\nimpossible to use malicious files to DOS attack a user. Currently, there's a limit\\non the size of the header of 100MB to prevent parsing extremely large JSON. Also when reading the file, there's a guarantee that addresses in the file\\ndo not overlap in any way, meaning when you're loading a file you should never\\nexceed the size of the file in memory\\n- Faster load: PyTorch seems to be the fastest file to load out in the major\\nML formats. However, it does seem to have an extra copy on CPU, which we\\ncan bypass in this lib by using `torch.UntypedStorage.from\\\\_file`. Currently, CPU loading times are extremely fast with this lib compared to pickle. GPU loading times are as fast or faster than PyTorch equivalent. Loading first on CPU with memmapping with torch, and then moving all tensors to GPU seems\\nto be faster too somehow (similar behavior in torch pickle)\\n- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to\\nload only part of the tensors on the various models. For\\n[BLOOM](https://huggingface.co/bigscience/bloom) using this format enabled\\nto load the model on 8 GPUs from 10mn with regular PyTorch weights down to 45s.\", 'type': 'tool', 'name': 'data_retriever', 'id': 'ed17208e-27eb-4d55-97eb-8ec2ac47a656', 'tool_call_id': 'call_TJ9WCBHZ0mYoo8qWhOHcOP8C', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab2a73e684812d15f67d2', 'page_content': \"We are also taking steps to make sure the existing PyTorch files are not malicious, but the best we can do is flag suspicious-looking files. Of course, there are other file formats out there, but\\nnone seemed to meet the full set of [ideal requirements](https://github.com/huggingface/safetensors#yet-another-format-) our team identified. In addition to being safe, `safetensors` allows lazy loading and generally faster loads (around 100x faster on CPU). Lazy loading means loading only part of a tensor in an efficient manner. This particular feature enables arbitrary sharding with efficient inference libraries, such as [text-generation-inference](https://github.com/huggingface/text-generation-inference), to load LLMs (such as LLaMA, StarCoder, etc.) on various types of hardware\\nwith maximum efficiency. Because it loads so fast and is framework agnostic, we can even use the format\\nto load models from the same file in PyTorch or TensorFlow. ## The security audit\\nSince `safetensors` main asset is providing safety guarantees, we wanted to make sure\\nit actually delivered. That's why Hugging Face, EleutherAI, and Stability AI teamed up to get an external\\nsecurity audit to confirm it. Important findings:\\n- No critical security flaw leading to arbitrary code execution was found. - Some imprecisions in the spec format were detected and fixed. - Some missing validation allowed [polyglot files](https://en.wikipedia.org/wiki/Polyglot\\\\_(computing)), which was fixed. - Lots of improvements to the test suite were proposed and implemented. In the name of openness and transparency, all companies agreed to make the report\\nfully public. [Full report](https://huggingface.co/datasets/safetensors/trail\\\\_of\\\\_bits\\\\_audit\\\\_repot/resolve/main/SOW-TrailofBits-EleutherAI\\\\_HuggingFace-v1.2.pdf)\\nOne import thing to note is that the library is written in Rust.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c0ba666af48523f51f40', 'page_content': \"Convert weights to safetensors\\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`. The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository. For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights. Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689abaea3e684812d15f7171', 'page_content': \"On CPU, if the file is already in cache, then it can\\ntruly be zero-copy, whereas on GPU there is not such disk cache, so a copy is always required\\nbut you can bypass allocating all the tensors on CPU at any given point. SafeTensors is not zero-copy for the header. The choice of JSON is pretty arbitrary, but since deserialization is <<< of the time required to load the actual tensor data and is readable I went that way, (also space is <<< to the tensor data). - Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the\\nmoment. - Order: 'C' or row-major. This seems to have won. We can add that information later if needed. - Stride: No striding, all tensors need to be packed before being serialized. I have yet to see a case where it seems useful to have a strided tensor stored in serialized format. ### Benefits\\nSince we can invent a new format we can propose additional benefits:\\n- Prevent DOS attacks: We can craft the format in such a way that it's almost\\nimpossible to use malicious files to DOS attack a user. Currently, there's a limit\\non the size of the header of 100MB to prevent parsing extremely large JSON. Also when reading the file, there's a guarantee that addresses in the file\\ndo not overlap in any way, meaning when you're loading a file you should never\\nexceed the size of the file in memory\\n- Faster load: PyTorch seems to be the fastest file to load out in the major\\nML formats. However, it does seem to have an extra copy on CPU, which we\\ncan bypass in this lib by using `torch.UntypedStorage.from\\\\_file`. Currently, CPU loading times are extremely fast with this lib compared to pickle. GPU loading times are as fast or faster than PyTorch equivalent. Loading first on CPU with memmapping with torch, and then moving all tensors to GPU seems\\nto be faster too somehow (similar behavior in torch pickle)\\n- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to\\nload only part of the tensors on the various models. For\\n[BLOOM](https://huggingface.co/bigscience/bloom) using this format enabled\\nto load the model on 8 GPUs from 10mn with regular PyTorch weights down to 45s.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The file format used to save and store PyTorch model weights more securely than `.bin` files is `.safetensors`.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--34b2a57e-73ea-48f1-acbb-04073d62afaf-0', 'usage_metadata': {'input_tokens': 1445, 'output_tokens': 25, 'total_tokens': 1470, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:37:27,171 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 49%|████▉     | 32/65 [02:21<02:19,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What type of security certification does Hugging Face have?\\n', 'type': 'human', 'id': 'f4fa7648-5b96-4ebb-8b9b-cb90f3e4415f'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_hgmq32sNmcdzcpfY9xWFPwYt', 'function': {'arguments': '{\"query\":\"Hugging Face security certification\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--a7dbd131-4660-4eef-a15a-a609fcbe3488-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Hugging Face security certification'}, 'id': 'call_hgmq32sNmcdzcpfY9xWFPwYt', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 359, 'output_tokens': 21, 'total_tokens': 380, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': \"Security\\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\\n- [User Access Tokens](./security-tokens)\\n- [Git over SSH](./security-git-ssh)\\n- [Signing commits with GPG](./security-gpg)\\n- [Single Sign-On (SSO)](./security-sso)\\n- [Malware Scanning](./security-malware)\\n- [Pickle Scanning](./security-pickle)\\n- [Secrets Scanning](./security-secrets)\\n\\n| Version | Supported |\\n|---------|--------------------|\\n| 1.x.x | :white\\\\_check\\\\_mark: |\\n| 2.x.x | :white\\\\_check\\\\_mark: |\\n## Reporting a Vulnerability\\nTo report a security vulnerability, please contact: security@huggingface.co\\n\\nSingle Sign-On (SSO)\\nThe Hugging Face Hub gives you the ability to implement mandatory Single Sign-On (SSO) for your organization. We support both SAML 2.0 and OpenID Connect (OIDC) protocols. This feature is part of the [Enterprise Hub](https://huggingface.co/enterprise). ## How does it work? When Single Sign-On is enabled, the members of your organization must authenticate through your Identity Provider (IdP) to access any content under the organization's namespace. Public content will still be available to users who are not members of the organization. \\\\*\\\\*We use email addresses to identify SSO users. Make sure that your organizational email address (e.g. your company email) has been added to [your user account](https://huggingface.co/settings/account).\\\\*\\\\*\\nWhen users log in, they will be prompted to complete the Single Sign-On authentication flow with a banner similar to the following:\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-sso-prompt.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-sso-prompt-dark.png)\\n\\nSingle Sign-On only applies to your organization. Members may belong to other organizations on Hugging Face. We support [role mapping](#role-mapping): you can automatically assign [roles](./organizations-security#access-control-in-organizations) to organization members based on attributes provided by your Identity Provider. ### Supported Identity Providers\\nYou can easily integrate Hugging Face Hub with a variety of Identity Providers, such as Okta, OneLogin or Azure Active Directory (Azure AD). Hugging Face Hub can work with any OIDC-compliant or SAML Identity Provider. ## How to configure OIDC/SAML provider in the Hub\\nWe have some guides available to help with configuring based on your chosen SSO provider, or to take inspiration from:\\n- [How to configure OIDC with Okta in the Hub](./security-sso-okta-oidc)\\n- [How to configure SAML with Okta in the Hub](./security-sso-okta-saml)\\n- [How to configure SAML with Azure in the Hub](./security-sso-azure-saml)\\n### Users Management\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-settings-users.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-settings-users-dark.png)\\n\\n#### Session Timeout\\nThis value sets the duration of the session for members of your organization.\", 'type': 'tool', 'name': 'data_retriever', 'id': '4812f753-99f6-47ea-8d9d-f5e30c3bff86', 'tool_call_id': 'call_hgmq32sNmcdzcpfY9xWFPwYt', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aae003e684812d15f62c7', 'page_content': \"Security\\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\\n- [User Access Tokens](./security-tokens)\\n- [Git over SSH](./security-git-ssh)\\n- [Signing commits with GPG](./security-gpg)\\n- [Single Sign-On (SSO)](./security-sso)\\n- [Malware Scanning](./security-malware)\\n- [Pickle Scanning](./security-pickle)\\n- [Secrets Scanning](./security-secrets)\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa6a53e684812d15f59d6', 'page_content': '| Version | Supported |\\n|---------|--------------------|\\n| 1.x.x | :white\\\\_check\\\\_mark: |\\n| 2.x.x | :white\\\\_check\\\\_mark: |\\n## Reporting a Vulnerability\\nTo report a security vulnerability, please contact: security@huggingface.co', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab0e63e684812d15f65b2', 'page_content': \"Single Sign-On (SSO)\\nThe Hugging Face Hub gives you the ability to implement mandatory Single Sign-On (SSO) for your organization. We support both SAML 2.0 and OpenID Connect (OIDC) protocols. This feature is part of the [Enterprise Hub](https://huggingface.co/enterprise). ## How does it work? When Single Sign-On is enabled, the members of your organization must authenticate through your Identity Provider (IdP) to access any content under the organization's namespace. Public content will still be available to users who are not members of the organization. \\\\*\\\\*We use email addresses to identify SSO users. Make sure that your organizational email address (e.g. your company email) has been added to [your user account](https://huggingface.co/settings/account).\\\\*\\\\*\\nWhen users log in, they will be prompted to complete the Single Sign-On authentication flow with a banner similar to the following:\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-sso-prompt.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-sso-prompt-dark.png)\\n\\nSingle Sign-On only applies to your organization. Members may belong to other organizations on Hugging Face. We support [role mapping](#role-mapping): you can automatically assign [roles](./organizations-security#access-control-in-organizations) to organization members based on attributes provided by your Identity Provider. ### Supported Identity Providers\\nYou can easily integrate Hugging Face Hub with a variety of Identity Providers, such as Okta, OneLogin or Azure Active Directory (Azure AD). Hugging Face Hub can work with any OIDC-compliant or SAML Identity Provider. ## How to configure OIDC/SAML provider in the Hub\\nWe have some guides available to help with configuring based on your chosen SSO provider, or to take inspiration from:\\n- [How to configure OIDC with Okta in the Hub](./security-sso-okta-oidc)\\n- [How to configure SAML with Okta in the Hub](./security-sso-okta-saml)\\n- [How to configure SAML with Azure in the Hub](./security-sso-azure-saml)\\n### Users Management\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-settings-users.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-settings-users-dark.png)\\n\\n#### Session Timeout\\nThis value sets the duration of the session for members of your organization.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--7e8d2ccc-fdf4-430f-b1fd-cff715c7be5a-0', 'usage_metadata': {'input_tokens': 1215, 'output_tokens': 30, 'total_tokens': 1245, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:37:32,131 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 51%|█████     | 33/65 [02:26<02:21,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What do RAG models combine to generate outputs?\\n', 'type': 'human', 'id': '2634ccda-4201-4271-85af-6b9304a139c3'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_VIwtetygkjTdsjV4LPQsJeZP', 'function': {'arguments': '{\"query\":\"RAG models combine to generate outputs\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--1d2e8ced-3d12-45c0-b723-8fd61bf3a8a8-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'RAG models combine to generate outputs'}, 'id': 'call_VIwtetygkjTdsjV4LPQsJeZP', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 358, 'output_tokens': 22, 'total_tokens': 380, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# RAG\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-rag-blueviolet)](https://huggingface.co/models?filter=rag)\\n\\n## Overview\\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and\\nsequence-to-sequence models.\\n\\nThis information retrieval step allows [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) to make use of multiple sources of knowledge -- those that are baked into the model parameters and the information that is contained in the contextual passages, allowing it to outperform other state-of-the-art models in tasks like question answering. You can try it for yourself using this [demo provided by Huggingface](https://huggingface.co/rag/)! ### Scaling up fine-tuning\\nThis retrieval of contextual documents is crucial for RAG\\'s state-of-the-art results but introduces an extra layer of complexity. When scaling up the training process via a data-parallel training routine, a naive implementation of the document lookup can become a bottleneck for training. Further, the \\\\*\\\\*document index\\\\*\\\\* used in the retrieval component is often quite large, making it infeasible for each training worker to load its own replicated copy of the index. The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) communication package for the document retrieval portion.\\n\\nIf this path already exists, the script will use saved predictions to calculate metrics. Add `--recalculate` parameter to force the script to perform inference from scratch. An example e2e evaluation run could look as follows:\\n```bash\\npython examples/research\\\\_projects/rag/eval\\\\_rag.py \\\\\\n--model\\\\_name\\\\_or\\\\_path facebook/rag-sequence-nq \\\\\\n--model\\\\_type rag\\\\_sequence \\\\\\n--evaluation\\\\_set path/to/test.source \\\\\\n--gold\\\\_data\\\\_path path/to/gold\\\\_data \\\\\\n--predictions\\\\_path path/to/e2e\\\\_preds.txt \\\\\\n--eval\\\\_mode e2e \\\\\\n--gold\\\\_data\\\\_mode qa \\\\\\n--n\\\\_docs 5 \\\\ # You can experiment with retrieving different number of documents at evaluation time\\n--print\\\\_predictions \\\\\\n--recalculate \\\\ # adding this parameter will force recalculating predictions even if predictions\\\\_path already exists\\n```\\n# Use your own knowledge source\\nBy default, RAG uses the English Wikipedia as a knowledge source, known as the \\'wiki\\\\_dpr\\' dataset. With `use\\\\_custom\\\\_knowledge\\\\_dataset.py` you can build your own knowledge source, \\\\*e.g.\\\\* for RAG. For instance, if documents are serialized as tab-separated csv files with the columns \"title\" and \"text\", one can use `use\\\\_own\\\\_knowledge\\\\_dataset.py` as follows:\\n```bash\\npython examples/research\\\\_projects/rag/use\\\\_own\\\\_knowledge\\\\_dataset.py \\\\\\n--csv\\\\_path path/to/my\\\\_csv \\\\\\n--output\\\\_dir path/to/my\\\\_knowledge\\\\_dataset \\\\\\n```\\nThe created outputs in `path/to/my\\\\_knowledge\\\\_dataset` can then be used to finetune RAG as follows:\\n```bash\\npython examples/research\\\\_projects/rag/finetune\\\\_rag.py \\\\\\n--data\\\\_dir $DATA\\\\_DIR \\\\\\n--output\\\\_dir $OUTPUT\\\\_DIR \\\\\\n--model\\\\_name\\\\_or\\\\_path $MODEL\\\\_NAME\\\\_OR\\\\_PATH \\\\\\n--model\\\\_type rag\\\\_sequence \\\\\\n--fp16 \\\\\\n--gpus 8\\n--index\\\\_name custom\\n--passages\\\\_path path/to/data/my\\\\_knowledge\\\\_dataset\\n--index\\\\_path path/to/my\\\\_knowledge\\\\_dataset\\\\_hnsw\\\\_index.faiss\\n```', 'type': 'tool', 'name': 'data_retriever', 'id': 'ab4bc62b-ce43-4d1b-8608-c66ebd1132b3', 'tool_call_id': 'call_VIwtetygkjTdsjV4LPQsJeZP', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaac53e684812d15f5ebc', 'page_content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# RAG\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-rag-blueviolet)](https://huggingface.co/models?filter=rag)\\n\\n## Overview\\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and\\nsequence-to-sequence models.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b5feaf3d2ebb270a034d', 'page_content': \"This information retrieval step allows [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) to make use of multiple sources of knowledge -- those that are baked into the model parameters and the information that is contained in the contextual passages, allowing it to outperform other state-of-the-art models in tasks like question answering. You can try it for yourself using this [demo provided by Huggingface](https://huggingface.co/rag/)! ### Scaling up fine-tuning\\nThis retrieval of contextual documents is crucial for RAG's state-of-the-art results but introduces an extra layer of complexity. When scaling up the training process via a data-parallel training routine, a naive implementation of the document lookup can become a bottleneck for training. Further, the \\\\*\\\\*document index\\\\*\\\\* used in the retrieval component is often quite large, making it infeasible for each training worker to load its own replicated copy of the index. The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) communication package for the document retrieval portion.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab2c93e684812d15f67fa', 'page_content': 'If this path already exists, the script will use saved predictions to calculate metrics. Add `--recalculate` parameter to force the script to perform inference from scratch. An example e2e evaluation run could look as follows:\\n```bash\\npython examples/research\\\\_projects/rag/eval\\\\_rag.py \\\\\\n--model\\\\_name\\\\_or\\\\_path facebook/rag-sequence-nq \\\\\\n--model\\\\_type rag\\\\_sequence \\\\\\n--evaluation\\\\_set path/to/test.source \\\\\\n--gold\\\\_data\\\\_path path/to/gold\\\\_data \\\\\\n--predictions\\\\_path path/to/e2e\\\\_preds.txt \\\\\\n--eval\\\\_mode e2e \\\\\\n--gold\\\\_data\\\\_mode qa \\\\\\n--n\\\\_docs 5 \\\\ # You can experiment with retrieving different number of documents at evaluation time\\n--print\\\\_predictions \\\\\\n--recalculate \\\\ # adding this parameter will force recalculating predictions even if predictions\\\\_path already exists\\n```\\n# Use your own knowledge source\\nBy default, RAG uses the English Wikipedia as a knowledge source, known as the \\'wiki\\\\_dpr\\' dataset. With `use\\\\_custom\\\\_knowledge\\\\_dataset.py` you can build your own knowledge source, \\\\*e.g.\\\\* for RAG. For instance, if documents are serialized as tab-separated csv files with the columns \"title\" and \"text\", one can use `use\\\\_own\\\\_knowledge\\\\_dataset.py` as follows:\\n```bash\\npython examples/research\\\\_projects/rag/use\\\\_own\\\\_knowledge\\\\_dataset.py \\\\\\n--csv\\\\_path path/to/my\\\\_csv \\\\\\n--output\\\\_dir path/to/my\\\\_knowledge\\\\_dataset \\\\\\n```\\nThe created outputs in `path/to/my\\\\_knowledge\\\\_dataset` can then be used to finetune RAG as follows:\\n```bash\\npython examples/research\\\\_projects/rag/finetune\\\\_rag.py \\\\\\n--data\\\\_dir $DATA\\\\_DIR \\\\\\n--output\\\\_dir $OUTPUT\\\\_DIR \\\\\\n--model\\\\_name\\\\_or\\\\_path $MODEL\\\\_NAME\\\\_OR\\\\_PATH \\\\\\n--model\\\\_type rag\\\\_sequence \\\\\\n--fp16 \\\\\\n--gpus 8\\n--index\\\\_name custom\\n--passages\\\\_path path/to/data/my\\\\_knowledge\\\\_dataset\\n--index\\\\_path path/to/my\\\\_knowledge\\\\_dataset\\\\_hnsw\\\\_index.faiss\\n```', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs. This allows RAG to utilize multiple sources of knowledge, including information baked into the model parameters and contextual passages, enabling it to outperform other state-of-the-art models in tasks like question answering.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--ca05d4f9-f6f8-4216-b979-d4826dbad8ee-0', 'usage_metadata': {'input_tokens': 1276, 'output_tokens': 62, 'total_tokens': 1338, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:37:35,867 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 52%|█████▏    | 34/65 [02:30<02:11,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\\n', 'type': 'human', 'id': '962ed44e-7959-4cd8-a78d-a780942331b8'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_TEhVGqNnzQYFchBcQeKdvKsQ', 'function': {'arguments': '{\"query\":\"MarkupLMFeatureExtractor library for HTML and XML extraction\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--379aab33-5762-44f6-a956-ff926aaf1600-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'MarkupLMFeatureExtractor library for HTML and XML extraction'}, 'id': 'call_TEhVGqNnzQYFchBcQeKdvKsQ', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 366, 'output_tokens': 25, 'total_tokens': 391, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# MarkupLM\\n## Overview\\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\\nperformance, similar to [LayoutLM](layoutlm). The model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\\nstate-of-the-art results on 2 important benchmarks:\\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\\n- [SWDE](https://www.researchgate.net/publication/221299838\\\\_From\\\\_one\\\\_tree\\\\_to\\\\_a\\\\_forest\\\\_a\\\\_unified\\\\_solution\\\\_for\\\\_structured\\\\_web\\\\_data\\\\_extraction), a dataset\\nfor information extraction from web pages (basically named-entity recogntion on web pages)\\nThe abstract from the paper is the following:\\n\\\\*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\\ntasks. The pre-trained model and code will be publicly available.\\\\*\\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm). ## Usage tips\\n- In addition to `input\\\\_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath\\\\_tags\\\\_seq` and `xpath\\\\_subs\\\\_seq`. These are the XPATH tags and subscripts respectively for each token in the input sequence. - One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info. ![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg)\\n MarkupLM architecture. Taken from the [original paper.](https://arxiv.org/abs/2110.08518) \\n## Usage: MarkupLMProcessor\\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\\ntoken-level inputs of the model (`input\\\\_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\\nif you only want to handle one of the two tasks. ```python\\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\\nfeature\\\\_extractor = MarkupLMFeatureExtractor()\\ntokenizer = MarkupLMTokenizerFast.from\\\\_pretrained(\"microsoft/markuplm-base\")\\nprocessor = MarkupLMProcessor(feature\\\\_extractor, tokenizer)\\n```\\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\\nand it will create the inputs expected by the model. Internally, the processor first uses\\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\\nto token-level `input\\\\_ids`, `attention\\\\_mask`, `token\\\\_type\\\\_ids`, `xpath\\\\_subs\\\\_seq`, `xpath\\\\_tags\\\\_seq`. Optionally, one can provide node labels to the processor, which are turned into token-level `labels`. [`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].\\n\\nIn total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs). \\\\*\\\\*Use case 1: web page classification (training, inference) + token classification (inference), parse\\\\_html = True\\\\*\\\\*\\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML. ```python\\n>>> from transformers import MarkupLMProcessor\\n>>> processor = MarkupLMProcessor.from\\\\_pretrained(\"microsoft/markuplm-base\")\\n>>> html\\\\_string = \"\"\"\\n... ... ... ... Hello world\\n... ... ... # Welcome\\n\\n... Here is my website. ... ... \"\"\"\\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\\n>>> encoding = processor(html\\\\_string, return\\\\_tensors=\"pt\")\\n>>> print(encoding.keys())\\ndict\\\\_keys([\\'input\\\\_ids\\', \\'token\\\\_type\\\\_ids\\', \\'attention\\\\_mask\\', \\'xpath\\\\_tags\\\\_seq\\', \\'xpath\\\\_subs\\\\_seq\\'])\\n```\\n\\\\*\\\\*Use case 2: web page classification (training, inference) + token classification (inference), parse\\\\_html=False\\\\*\\\\*\\nIn case one already has obtained all nodes and xpaths, one doesn\\'t need the feature extractor. In that case, one should\\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse\\\\_html` to `False`. ```python\\n>>> from transformers import MarkupLMProcessor\\n>>> processor = MarkupLMProcessor.from\\\\_pretrained(\"microsoft/markuplm-base\")\\n>>> processor.parse\\\\_html = False\\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return\\\\_tensors=\"pt\")\\n>>> print(encoding.keys())\\ndict\\\\_keys([\\'input\\\\_ids\\', \\'token\\\\_type\\\\_ids\\', \\'attention\\\\_mask\\', \\'xpath\\\\_tags\\\\_seq\\', \\'xpath\\\\_subs\\\\_seq\\'])\\n```\\n\\\\*\\\\*Use case 3: token classification (training), parse\\\\_html=False\\\\*\\\\*\\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\\ncorresponding node labels in order to train a model.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Utilities for `FeatureExtractors`\\nThis page lists all the utility functions that can be used by the audio [`FeatureExtractor`] in order to compute special features from a raw audio using common algorithms such as \\\\*Short Time Fourier Transform\\\\* or \\\\*log mel spectrogram\\\\*. Most of those are only useful if you are studying the code of the audio processors in the library. ## Audio Transformations\\n[[autodoc]] audio\\\\_utils.hertz\\\\_to\\\\_mel\\n[[autodoc]] audio\\\\_utils.mel\\\\_to\\\\_hertz\\n[[autodoc]] audio\\\\_utils.mel\\\\_filter\\\\_bank\\n[[autodoc]] audio\\\\_utils.optimal\\\\_fft\\\\_length\\n[[autodoc]] audio\\\\_utils.window\\\\_function\\n[[autodoc]] audio\\\\_utils.spectrogram\\n[[autodoc]] audio\\\\_utils.power\\\\_to\\\\_db\\n[[autodoc]] audio\\\\_utils.amplitude\\\\_to\\\\_db', 'type': 'tool', 'name': 'data_retriever', 'id': '925aa99b-db76-46b7-85e4-2f40f3b13eaf', 'tool_call_id': 'call_TEhVGqNnzQYFchBcQeKdvKsQ', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899ba72af3d2ebb270a08cd', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# MarkupLM\\n## Overview\\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\\nperformance, similar to [LayoutLM](layoutlm). The model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\\nstate-of-the-art results on 2 important benchmarks:\\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\\n- [SWDE](https://www.researchgate.net/publication/221299838\\\\_From\\\\_one\\\\_tree\\\\_to\\\\_a\\\\_forest\\\\_a\\\\_unified\\\\_solution\\\\_for\\\\_structured\\\\_web\\\\_data\\\\_extraction), a dataset\\nfor information extraction from web pages (basically named-entity recogntion on web pages)\\nThe abstract from the paper is the following:\\n\\\\*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\\ntasks. The pre-trained model and code will be publicly available.\\\\*\\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm). ## Usage tips\\n- In addition to `input\\\\_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath\\\\_tags\\\\_seq` and `xpath\\\\_subs\\\\_seq`. These are the XPATH tags and subscripts respectively for each token in the input sequence. - One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info. ![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg)\\n MarkupLM architecture. Taken from the [original paper.](https://arxiv.org/abs/2110.08518) \\n## Usage: MarkupLMProcessor\\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\\ntoken-level inputs of the model (`input\\\\_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\\nif you only want to handle one of the two tasks. ```python\\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\\nfeature\\\\_extractor = MarkupLMFeatureExtractor()\\ntokenizer = MarkupLMTokenizerFast.from\\\\_pretrained(\"microsoft/markuplm-base\")\\nprocessor = MarkupLMProcessor(feature\\\\_extractor, tokenizer)\\n```\\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\\nand it will create the inputs expected by the model. Internally, the processor first uses\\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\\nto token-level `input\\\\_ids`, `attention\\\\_mask`, `token\\\\_type\\\\_ids`, `xpath\\\\_subs\\\\_seq`, `xpath\\\\_tags\\\\_seq`. Optionally, one can provide node labels to the processor, which are turned into token-level `labels`. [`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899ba72af3d2ebb270a08d2', 'page_content': 'In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs). \\\\*\\\\*Use case 1: web page classification (training, inference) + token classification (inference), parse\\\\_html = True\\\\*\\\\*\\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML. ```python\\n>>> from transformers import MarkupLMProcessor\\n>>> processor = MarkupLMProcessor.from\\\\_pretrained(\"microsoft/markuplm-base\")\\n>>> html\\\\_string = \"\"\"\\n... ... ... ... Hello world\\n... ... ... # Welcome\\n\\n... Here is my website. ... ... \"\"\"\\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\\n>>> encoding = processor(html\\\\_string, return\\\\_tensors=\"pt\")\\n>>> print(encoding.keys())\\ndict\\\\_keys([\\'input\\\\_ids\\', \\'token\\\\_type\\\\_ids\\', \\'attention\\\\_mask\\', \\'xpath\\\\_tags\\\\_seq\\', \\'xpath\\\\_subs\\\\_seq\\'])\\n```\\n\\\\*\\\\*Use case 2: web page classification (training, inference) + token classification (inference), parse\\\\_html=False\\\\*\\\\*\\nIn case one already has obtained all nodes and xpaths, one doesn\\'t need the feature extractor. In that case, one should\\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse\\\\_html` to `False`. ```python\\n>>> from transformers import MarkupLMProcessor\\n>>> processor = MarkupLMProcessor.from\\\\_pretrained(\"microsoft/markuplm-base\")\\n>>> processor.parse\\\\_html = False\\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return\\\\_tensors=\"pt\")\\n>>> print(encoding.keys())\\ndict\\\\_keys([\\'input\\\\_ids\\', \\'token\\\\_type\\\\_ids\\', \\'attention\\\\_mask\\', \\'xpath\\\\_tags\\\\_seq\\', \\'xpath\\\\_subs\\\\_seq\\'])\\n```\\n\\\\*\\\\*Use case 3: token classification (training), parse\\\\_html=False\\\\*\\\\*\\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\\ncorresponding node labels in order to train a model.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bb9aaf3d2ebb270a0a2b', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Utilities for `FeatureExtractors`\\nThis page lists all the utility functions that can be used by the audio [`FeatureExtractor`] in order to compute special features from a raw audio using common algorithms such as \\\\*Short Time Fourier Transform\\\\* or \\\\*log mel spectrogram\\\\*. Most of those are only useful if you are studying the code of the audio processors in the library. ## Audio Transformations\\n[[autodoc]] audio\\\\_utils.hertz\\\\_to\\\\_mel\\n[[autodoc]] audio\\\\_utils.mel\\\\_to\\\\_hertz\\n[[autodoc]] audio\\\\_utils.mel\\\\_filter\\\\_bank\\n[[autodoc]] audio\\\\_utils.optimal\\\\_fft\\\\_length\\n[[autodoc]] audio\\\\_utils.window\\\\_function\\n[[autodoc]] audio\\\\_utils.spectrogram\\n[[autodoc]] audio\\\\_utils.power\\\\_to\\\\_db\\n[[autodoc]] audio\\\\_utils.amplitude\\\\_to\\\\_db', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'MarkupLMFeatureExtractor uses Beautiful Soup, a Python library for pulling data out of HTML and XML files, to extract data from HTML and XML documents.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--19ae9978-9c6f-4667-81d1-76197225ad5a-0', 'usage_metadata': {'input_tokens': 2460, 'output_tokens': 30, 'total_tokens': 2490, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:37:40,360 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 54%|█████▍    | 35/65 [02:34<02:09,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the file size limit for syncing to HF Spaces without using Git-LFS?\\n', 'type': 'human', 'id': '0723c3dd-cac8-4278-9462-fd68b81580ae'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_22eZVqWHoxnNkqJQ419Vxv8m', 'function': {'arguments': '{\"query\":\"file size limit for syncing to HF Spaces without using Git-LFS\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--535ede4f-2243-48da-b358-1800d9236cf4-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'file size limit for syncing to HF Spaces without using Git-LFS'}, 'id': 'call_22eZVqWHoxnNkqJQ419Vxv8m', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 28, 'total_tokens': 393, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Managing Spaces with CircleCI Workflows\\nYou can keep your app in sync with your GitHub repository with a \\\\*\\\\*CircleCI workflow\\\\*\\\\*. [CircleCI](https://circleci.com) is a continuous integration and continuous delivery (CI/CD) platform that helps automate the software development process. A [CircleCI workflow](https://circleci.com/docs/workflows/) is a set of automated tasks defined in a configuration file, orchestrated by CircleCI, to streamline the process of building, testing, and deploying software applications. \\\\*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don\\'t want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\\\\*\\nFirst, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository. ```bash\\ngit remote add space https://huggingface.co/spaces/HF\\\\_USERNAME/SPACE\\\\_NAME\\n```\\nThen force push to sync everything for the first time:\\n```bash\\ngit push --force space main\\n```\\nNext, set up a [CircleCI workflow](https://circleci.com/docs/workflows/) to push your `main` git branch to Spaces. In the example below:\\n\\\\* Replace `HF\\\\_USERNAME` with your username and `SPACE\\\\_NAME` with your Space name. \\\\* [Create a context in CircleCI](https://circleci.com/docs/contexts/) and add an env variable into it called \\\\*HF\\\\_PERSONAL\\\\_TOKEN\\\\* (you can give it any name, use the key you create in place of HF\\\\_PERSONAL\\\\_TOKEN) and the value as your Hugging Face API token. You can find your Hugging Face API token under \\\\*\\\\*API Tokens\\\\*\\\\* on [your Hugging Face profile](https://huggingface.co/settings/tokens). ```yaml\\nversion: 2.1\\nworkflows:\\nmain:\\njobs:\\n- sync-to-huggingface:\\ncontext:\\n- HuggingFace\\nfilters:\\nbranches:\\nonly:\\n- main\\njobs:\\nsync-to-huggingface:\\ndocker:\\n- image: alpine\\nresource\\\\_class: small\\nsteps:\\n- run:\\nname: install git\\ncommand: apk update && apk add openssh-client git\\n- checkout\\n- run:\\nname: push to Huggingface hub\\ncommand: |\\ngit config user.email \"\"\\ngit config user.name \"\"\\ngit push -f https://HF\\\\_USERNAME:${HF\\\\_PERSONAL\\\\_TOKEN}@huggingface.co/spaces/HF\\\\_USERNAME/SPACE\\\\_NAME main\\n```\\n\\nThere are a few reasons for this:\\n- Uploading and downloading smaller files is much easier both for you and the other users. Connection issues can always\\nhappen when streaming data and smaller files avoid resuming from the beginning in case of errors. - Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\\nleading to a slower download speed. In all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size. - \\\\*\\\\*Number of commits\\\\*\\\\*: There is no hard limit for the total number of commits on your repo history. However, from\\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\\nwrites. If your repo\\'s history gets very large, it is always possible to squash all the commits to get a\\nfresh start using `huggingface\\\\_hub`\\'s [`super\\\\_squash\\\\_history`](https://huggingface.co/docs/huggingface\\\\_hub/main/en/package\\\\_reference/hf\\\\_api#huggingface\\\\_hub.HfApi.super\\\\_squash\\\\_history).\\n\\nBe aware that this is a non-revertible operation. - \\\\*\\\\*Number of operations per commit\\\\*\\\\*: Once again, there is no hard limit here. When a commit is uploaded on the Hub, each\\ngit operation (addition or delete) is checked by the server. When a hundred LFS files are committed at once,\\neach file is checked individually to ensure it\\'s been correctly uploaded. When pushing data through HTTP,\\na timeout of 60s is set on the request, meaning that if the process takes more time, an error is raised. However, it can\\nhappen (in rare cases) that even if the timeout is raised client-side, the process is still\\ncompleted server-side. This can be checked manually by browsing the repo on the Hub. To prevent this timeout, we recommend\\nadding around 50-100 files per commit.', 'type': 'tool', 'name': 'data_retriever', 'id': '40b689e4-24b4-4c2b-bb54-3a56e6f93c0c', 'tool_call_id': 'call_22eZVqWHoxnNkqJQ419Vxv8m', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3573e684812d15f688f', 'page_content': 'Managing Spaces with CircleCI Workflows\\nYou can keep your app in sync with your GitHub repository with a \\\\*\\\\*CircleCI workflow\\\\*\\\\*. [CircleCI](https://circleci.com) is a continuous integration and continuous delivery (CI/CD) platform that helps automate the software development process. A [CircleCI workflow](https://circleci.com/docs/workflows/) is a set of automated tasks defined in a configuration file, orchestrated by CircleCI, to streamline the process of building, testing, and deploying software applications. \\\\*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don\\'t want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\\\\*\\nFirst, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository. ```bash\\ngit remote add space https://huggingface.co/spaces/HF\\\\_USERNAME/SPACE\\\\_NAME\\n```\\nThen force push to sync everything for the first time:\\n```bash\\ngit push --force space main\\n```\\nNext, set up a [CircleCI workflow](https://circleci.com/docs/workflows/) to push your `main` git branch to Spaces. In the example below:\\n\\\\* Replace `HF\\\\_USERNAME` with your username and `SPACE\\\\_NAME` with your Space name. \\\\* [Create a context in CircleCI](https://circleci.com/docs/contexts/) and add an env variable into it called \\\\*HF\\\\_PERSONAL\\\\_TOKEN\\\\* (you can give it any name, use the key you create in place of HF\\\\_PERSONAL\\\\_TOKEN) and the value as your Hugging Face API token. You can find your Hugging Face API token under \\\\*\\\\*API Tokens\\\\*\\\\* on [your Hugging Face profile](https://huggingface.co/settings/tokens). ```yaml\\nversion: 2.1\\nworkflows:\\nmain:\\njobs:\\n- sync-to-huggingface:\\ncontext:\\n- HuggingFace\\nfilters:\\nbranches:\\nonly:\\n- main\\njobs:\\nsync-to-huggingface:\\ndocker:\\n- image: alpine\\nresource\\\\_class: small\\nsteps:\\n- run:\\nname: install git\\ncommand: apk update && apk add openssh-client git\\n- checkout\\n- run:\\nname: push to Huggingface hub\\ncommand: |\\ngit config user.email \"\"\\ngit config user.name \"\"\\ngit push -f https://HF\\\\_USERNAME:${HF\\\\_PERSONAL\\\\_TOKEN}@huggingface.co/spaces/HF\\\\_USERNAME/SPACE\\\\_NAME main\\n```', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b41eaf3d2ebb270a0163', 'page_content': \"There are a few reasons for this:\\n- Uploading and downloading smaller files is much easier both for you and the other users. Connection issues can always\\nhappen when streaming data and smaller files avoid resuming from the beginning in case of errors. - Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\\nleading to a slower download speed. In all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size. - \\\\*\\\\*Number of commits\\\\*\\\\*: There is no hard limit for the total number of commits on your repo history. However, from\\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a\\nfresh start using `huggingface\\\\_hub`'s [`super\\\\_squash\\\\_history`](https://huggingface.co/docs/huggingface\\\\_hub/main/en/package\\\\_reference/hf\\\\_api#huggingface\\\\_hub.HfApi.super\\\\_squash\\\\_history).\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b41eaf3d2ebb270a0164', 'page_content': \"Be aware that this is a non-revertible operation. - \\\\*\\\\*Number of operations per commit\\\\*\\\\*: Once again, there is no hard limit here. When a commit is uploaded on the Hub, each\\ngit operation (addition or delete) is checked by the server. When a hundred LFS files are committed at once,\\neach file is checked individually to ensure it's been correctly uploaded. When pushing data through HTTP,\\na timeout of 60s is set on the request, meaning that if the process takes more time, an error is raised. However, it can\\nhappen (in rare cases) that even if the timeout is raised client-side, the process is still\\ncompleted server-side. This can be checked manually by browsing the repo on the Hub. To prevent this timeout, we recommend\\nadding around 50-100 files per commit.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. If your files are larger than this, you will need to use Git-LFS or review your files and check your history.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--bbb440c4-cb26-41de-9724-6528caf2ceb1-0', 'usage_metadata': {'input_tokens': 1343, 'output_tokens': 44, 'total_tokens': 1387, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:37:43,989 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 55%|█████▌    | 36/65 [02:38<01:58,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the title of the paper introducing the ByT5 model?\\n', 'type': 'human', 'id': 'e29060f8-4f32-4946-bb06-640ee6219143'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_UVBrODGlcsM1HE8hX8W0S9we', 'function': {'arguments': '{\"query\":\"ByT5 model paper title\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--e870f027-5ccc-46ae-83c1-1248a54aaa0e-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'ByT5 model paper title'}, 'id': 'call_UVBrODGlcsM1HE8hX8W0S9we', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 21, 'total_tokens': 383, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# ByT5\\n## Overview\\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\\nKale, Adam Roberts, Colin Raffel.\\n\\nThe abstract from the paper is the following:\\n\\\\*Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from\\nthe model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they\\ncan process text in any language out of the box, they are more robust to noise, and they minimize technical debt by\\nremoving complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token\\nsequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of\\noperating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with\\nminimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count,\\ntraining FLOPs, and inference speed, and show that byte-level models are competitive with their token-level\\ncounterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on\\ntasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of\\npre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our\\nexperiments.\\\\*\\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\\nfound [here](https://github.com/google-research/byt5). ByT5\\'s architecture is based on the T5v1.1 model, refer to [T5v1.1\\'s documentation page](t5v1.1) for the API reference. They\\nonly differ in how inputs should be prepared for the model, see the code examples below. Since ByT5 was pre-trained unsupervisedly, there\\'s no real advantage to using a task prefix during single-task\\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix. ## Usage example\\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\\n```python\\n>>> from transformers import T5ForConditionalGeneration\\n>>> import torch\\n>>> model = T5ForConditionalGeneration.from\\\\_pretrained(\"google/byt5-small\")\\n>>> num\\\\_special\\\\_tokens = 3\\n>>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5. >>> # => Need to shift utf-8 character encodings by 3 before passing ids to model. >>> input\\\\_ids = torch.tensor([list(\"Life is like a box of chocolates.\".encode(\"utf-8\"))]) + num\\\\_special\\\\_tokens\\n>>> labels = torch.tensor([list(\"La vie est comme une boîte de chocolat.\".encode(\"utf-8\"))]) + num\\\\_special\\\\_tokens\\n>>> loss = model(input\\\\_ids, labels=labels).loss\\n>>> loss.item()\\n2.66\\n```\\nFor batched inference and training it is however recommended to make use of the tokenizer:\\n```python\\n>>> from transformers import T5ForConditionalGeneration, AutoTokenizer\\n>>> model = T5ForConditionalGeneration.from\\\\_pretrained(\"google/byt5-small\")\\n>>> tokenizer = AutoTokenizer.from\\\\_pretrained(\"google/byt5-small\")\\n>>> model\\\\_inputs = tokenizer(\\n... [\"Life is like a box of chocolates.\", \"Today is Monday.\"], padding=\"longest\", return\\\\_tensors=\"pt\"\\n...', 'type': 'tool', 'name': 'data_retriever', 'id': '51ed8b8d-87aa-4cfa-b58f-834b79147bff', 'tool_call_id': 'call_UVBrODGlcsM1HE8hX8W0S9we', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa9f53e684812d15f5da3', 'page_content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# ByT5\\n## Overview\\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\\nKale, Adam Roberts, Colin Raffel.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa9f53e684812d15f5da6', 'page_content': 'The abstract from the paper is the following:\\n\\\\*Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from\\nthe model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they\\ncan process text in any language out of the box, they are more robust to noise, and they minimize technical debt by\\nremoving complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token\\nsequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of\\noperating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with\\nminimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count,\\ntraining FLOPs, and inference speed, and show that byte-level models are competitive with their token-level\\ncounterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on\\ntasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of\\npre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our\\nexperiments.\\\\*\\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\\nfound [here](https://github.com/google-research/byt5). ByT5\\'s architecture is based on the T5v1.1 model, refer to [T5v1.1\\'s documentation page](t5v1.1) for the API reference. They\\nonly differ in how inputs should be prepared for the model, see the code examples below. Since ByT5 was pre-trained unsupervisedly, there\\'s no real advantage to using a task prefix during single-task\\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix. ## Usage example\\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\\n```python\\n>>> from transformers import T5ForConditionalGeneration\\n>>> import torch\\n>>> model = T5ForConditionalGeneration.from\\\\_pretrained(\"google/byt5-small\")\\n>>> num\\\\_special\\\\_tokens = 3\\n>>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5. >>> # => Need to shift utf-8 character encodings by 3 before passing ids to model. >>> input\\\\_ids = torch.tensor([list(\"Life is like a box of chocolates.\".encode(\"utf-8\"))]) + num\\\\_special\\\\_tokens\\n>>> labels = torch.tensor([list(\"La vie est comme une boîte de chocolat.\".encode(\"utf-8\"))]) + num\\\\_special\\\\_tokens\\n>>> loss = model(input\\\\_ids, labels=labels).loss\\n>>> loss.item()\\n2.66\\n```\\nFor batched inference and training it is however recommended to make use of the tokenizer:\\n```python\\n>>> from transformers import T5ForConditionalGeneration, AutoTokenizer\\n>>> model = T5ForConditionalGeneration.from\\\\_pretrained(\"google/byt5-small\")\\n>>> tokenizer = AutoTokenizer.from\\\\_pretrained(\"google/byt5-small\")\\n>>> model\\\\_inputs = tokenizer(\\n... [\"Life is like a box of chocolates.\", \"Today is Monday.\"], padding=\"longest\", return\\\\_tensors=\"pt\"\\n...', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models.\"', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--076aa3c0-5806-40e8-bff1-7a4a2b7cb1ab-0', 'usage_metadata': {'input_tokens': 1274, 'output_tokens': 30, 'total_tokens': 1304, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:37:47,654 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 57%|█████▋    | 37/65 [02:42<01:51,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the dimension of the feature vector for the base BERT model?\\n', 'type': 'human', 'id': '8be5e232-1d89-4d7b-8e94-f9f7d0b4c73c'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_s4S9lLuy2U0vQYC9DvLnlWe0', 'function': {'arguments': '{\"query\":\"dimension of the feature vector for the base BERT model\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--9847123c-a71b-4064-894d-16521956f1e8-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'dimension of the feature vector for the base BERT model'}, 'id': 'call_s4S9lLuy2U0vQYC9DvLnlWe0', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 363, 'output_tokens': 26, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'n this video, we\\'ll study the encoder architecture. An example of a popular encoder-only architecture is BERT, which is the most popular model of its kind. Let\\'s first start by understanding how it works. We\\'ll use a small example, using three words. We use these as inputs, and pass them through the encoder. We retrieve a numerical representation of each word. Here, for example, the encoder converts the three words “Welcome to NYC” in these three sequences of numbers. The encoder outputs exactly one sequence of numbers per input word. This numerical representation can also be called a \"Feature vector\", or \"Feature tensor\". Let\\'s dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \"to\", isn\\'t the representation of only the \"to\" word. It also takes into account the words around it, which we call the “context”.As in, it looks to the left context, the word on the left of the one we\\'re studying (here the word \"Welcome\") and the context on the right (here the word \"NYC\") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \"meaning\" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence.\\n\\n!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# BERT\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\\n\\n## Overview\\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\\'s a\\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\\n\\\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\\narchitecture modifications.\\\\*\\n\\\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\\\*\\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf).\\n\\n---\\n## 2. How does BERT Work? BERT works by leveraging the following:\\n### 2.1 Large amounts of training data\\nA massive dataset of 3.3 Billion words has contributed to BERT’s continued success. BERT was specifically trained on Wikipedia (\\\\~2.5B words) and Google’s BooksCorpus (\\\\~800M words). These large informational datasets contributed to BERT’s deep knowledge not only of the English language but also of our world! 🚀\\nTraining on a dataset this large takes a long time. BERT’s training was made possible thanks to the novel Transformer architecture and sped up by using TPUs (Tensor Processing Units - Google’s custom circuit built specifically for large ML models). —64 TPUs trained BERT over the course of 4 days. \\\\*\\\\*Note:\\\\*\\\\* Demand for smaller BERT models is increasing in order to use BERT within smaller computational environments (like cell phones and personal computers). [23 smaller BERT models were released in March 2020](https://github.com/google-research/bert). [DistilBERT](https://huggingface.co/docs/transformers/model\\\\_doc/distilbert) offers a lighter version of BERT; runs 60% faster while maintaining over 95% of BERT’s performance. ### 2.2 What is a Masked Language Model? MLM enables/enforces bidirectional learning from text by masking (hiding) a word in a sentence and forcing BERT to bidirectionally use the words on either side of the covered word to predict the masked word.', 'type': 'tool', 'name': 'data_retriever', 'id': '8c7a2c81-b8f6-44b6-94db-1c9fd3967c2b', 'tool_call_id': 'call_s4S9lLuy2U0vQYC9DvLnlWe0', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899ba2eaf3d2ebb270a0877', 'page_content': 'n this video, we\\'ll study the encoder architecture. An example of a popular encoder-only architecture is BERT, which is the most popular model of its kind. Let\\'s first start by understanding how it works. We\\'ll use a small example, using three words. We use these as inputs, and pass them through the encoder. We retrieve a numerical representation of each word. Here, for example, the encoder converts the three words “Welcome to NYC” in these three sequences of numbers. The encoder outputs exactly one sequence of numbers per input word. This numerical representation can also be called a \"Feature vector\", or \"Feature tensor\". Let\\'s dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \"to\", isn\\'t the representation of only the \"to\" word. It also takes into account the words around it, which we call the “context”.As in, it looks to the left context, the word on the left of the one we\\'re studying (here the word \"Welcome\") and the context on the right (here the word \"NYC\") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \"meaning\" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689abb613e684812d15f71e1', 'page_content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# BERT\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\\n\\n## Overview\\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\\'s a\\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\\n\\\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\\narchitecture modifications.\\\\*\\n\\\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\\\*\\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bf77666af48523f51dc8', 'page_content': '---\\n## 2. How does BERT Work? BERT works by leveraging the following:\\n### 2.1 Large amounts of training data\\nA massive dataset of 3.3 Billion words has contributed to BERT’s continued success. BERT was specifically trained on Wikipedia (\\\\~2.5B words) and Google’s BooksCorpus (\\\\~800M words). These large informational datasets contributed to BERT’s deep knowledge not only of the English language but also of our world! 🚀\\nTraining on a dataset this large takes a long time. BERT’s training was made possible thanks to the novel Transformer architecture and sped up by using TPUs (Tensor Processing Units - Google’s custom circuit built specifically for large ML models). —64 TPUs trained BERT over the course of 4 days. \\\\*\\\\*Note:\\\\*\\\\* Demand for smaller BERT models is increasing in order to use BERT within smaller computational environments (like cell phones and personal computers). [23 smaller BERT models were released in March 2020](https://github.com/google-research/bert). [DistilBERT](https://huggingface.co/docs/transformers/model\\\\_doc/distilbert) offers a lighter version of BERT; runs 60% faster while maintaining over 95% of BERT’s performance. ### 2.2 What is a Masked Language Model? MLM enables/enforces bidirectional learning from text by masking (hiding) a word in a sentence and forcing BERT to bidirectionally use the words on either side of the covered word to predict the masked word.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The dimension of the feature vector for the base BERT model is 768.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--4ee27563-24da-40e9-bc54-8b1ac77ed797-0', 'usage_metadata': {'input_tokens': 1571, 'output_tokens': 16, 'total_tokens': 1587, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:37:51,651 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 58%|█████▊    | 38/65 [02:46<01:47,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What special identifier does the WordPiece Model use for continuing subwords?\\n', 'type': 'human', 'id': '7ba0b07d-bf0d-4fd9-9375-697d77dc14de'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_6xvEThiukEmSuWVM2GEE27gF', 'function': {'arguments': '{\"query\":\"WordPiece Model special identifier for continuing subwords\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--48d31a99-7b15-42b5-821e-6ec62bbf98f8-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'WordPiece Model special identifier for continuing subwords'}, 'id': 'call_6xvEThiukEmSuWVM2GEE27gF', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 24, 'total_tokens': 386, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'WordPiece tokenization[[wordpiece-tokenization]]\\nWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It\\'s very similar to BPE in terms of the training, but the actual tokenization is done differently. 💡 This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm. ## Training algorithm[[training-algorithm]]\\n⚠️ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate. Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this:\\n```\\nw ##o ##r ##d\\n```\\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix. Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\\n$$\\\\mathrm{score} = (\\\\mathrm{freq\\\\\\\\_of\\\\\\\\_pair}) / (\\\\mathrm{freq\\\\\\\\_of\\\\\\\\_first\\\\\\\\_element} \\\\times \\\\mathrm{freq\\\\\\\\_of\\\\\\\\_second\\\\\\\\_element})$$\\nBy dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary.\\n\\nFor instance,\\nthe word `\"bug\"` would be tokenized to `[\"b\", \"ug\"]` but `\"mug\"` would be tokenized as `[\"\", \"ug\"]` since\\nthe symbol `\"m\"` is not in the base vocabulary. In general, single letters such as `\"m\"` are not replaced by the\\n`\"\"` symbol because the training data usually includes at least one occurrence of each letter, but it is likely\\nto happen for very special characters like emojis. As mentioned earlier, the vocabulary size, \\\\*i.e.\\\\* the base vocabulary size + the number of merges, is a hyperparameter\\nto choose. For instance [GPT](model\\\\_doc/gpt) has a vocabulary size of 40,478 since they have 478 base characters\\nand chose to stop training after 40,000 merges. #### Byte-level BPE\\nA base vocabulary that includes all possible base characters can be quite large if \\\\*e.g.\\\\* all unicode characters are\\nconsidered as base characters. To have a better base vocabulary, [GPT-2](https://cdn.openai.com/better-language-models/language\\\\_models\\\\_are\\\\_unsupervised\\\\_multitask\\\\_learners.pdf) uses bytes\\nas the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that\\nevery base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2\\'s\\ntokenizer can tokenize every text without the need for the  symbol. [GPT-2](model\\\\_doc/gpt) has a vocabulary\\nsize of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned\\nwith 50,000 merges. ### WordPiece\\nWordPiece is the subword tokenization algorithm used for [BERT](model\\\\_doc/bert), [DistilBERT](model\\\\_doc/distilbert), and [Electra](model\\\\_doc/electra). The algorithm was outlined in [Japanese and Korean\\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary. So what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is\\nequivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by\\nits second symbol is the greatest among all symbol pairs. \\\\*E.g.\\\\* `\"u\"`, followed by `\"g\"` would have only been\\nmerged if the probability of `\"ug\"` divided by `\"u\"`, `\"g\"` would have been greater than for any other symbol\\npair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it \\\\_loses\\\\_ by merging two symbols\\nto ensure it\\'s \\\\_worth it\\\\_. ### Unigram\\nUnigram is a subword tokenization algorithm introduced in [Subword Regularization: Improving Neural Network Translation\\nModels with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). In contrast to BPE or\\nWordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each\\nsymbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and\\nthe most common substrings. Unigram is not used directly for any of the models in the transformers, but it\\'s used in\\nconjunction with [SentencePiece](#sentencepiece). At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training\\ndata given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm\\ncomputes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then\\nremoves p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, \\\\*i.e.\\\\* those\\nsymbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has\\nreached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized. Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of\\ntokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:\\n```\\n[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"],\\n```\\n`\"hugs\"` could be tokenized both as `[\"hug\", \"s\"]`, `[\"h\", \"ug\", \"s\"]` or `[\"h\", \"u\", \"g\", \"s\"]`. So which one\\nto choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that\\nthe probability of each possible tokenization can be computed after training.\\n\\nWe can see that the words `[\"i\", \"have\", \"a\", \"new\"]` are present in the tokenizer\\'s vocabulary, but the word `\"gpu\"` is not. Consequently, the\\ntokenizer splits `\"gpu\"` into known subwords: `[\"gp\" and \"##u\"]`. `\"##\"` means that the rest of the token should\\nbe attached to the previous one, without space (for decoding or reversal of the tokenization). As another example, [`~transformers.XLNetTokenizer`] tokenizes our previously exemplary text as follows:\\n```py\\n>>> from transformers import XLNetTokenizer\\n>>> tokenizer = XLNetTokenizer.from\\\\_pretrained(\"xlnet-base-cased\")\\n>>> tokenizer.tokenize(\"Don\\'t you love 🤗 Transformers? We sure do.\")\\n[\"▁Don\", \"\\'\", \"t\", \"▁you\", \"▁love\", \"▁\", \"🤗\", \"▁\", \"Transform\", \"ers\", \"?\", \"▁We\", \"▁sure\", \"▁do\", \".\"]\\n```\\nWe\\'ll get back to the meaning of those `\"▁\"` when we look at [SentencePiece](#sentencepiece). As one can see,\\nthe rare word `\"Transformers\"` has been split into the more frequent subwords `\"Transform\"` and `\"ers\"`. Let\\'s now look at how the different subword tokenization algorithms work. Note that all of those tokenization\\nalgorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained\\non. ### Byte-Pair Encoding (BPE)\\nByte-Pair Encoding (BPE) was introduced in [Neural Machine Translation of Rare Words with Subword Units (Sennrich et\\nal., 2015)](https://arxiv.org/abs/1508.07909).', 'type': 'tool', 'name': 'data_retriever', 'id': 'e20b26a1-8c22-4f4d-b4a9-4bc5a09ae2d8', 'tool_call_id': 'call_6xvEThiukEmSuWVM2GEE27gF', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaf263e684812d15f63e9', 'page_content': 'WordPiece tokenization[[wordpiece-tokenization]]\\nWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It\\'s very similar to BPE in terms of the training, but the actual tokenization is done differently. 💡 This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm. ## Training algorithm[[training-algorithm]]\\n⚠️ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate. Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this:\\n```\\nw ##o ##r ##d\\n```\\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix. Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\\n$$\\\\mathrm{score} = (\\\\mathrm{freq\\\\\\\\_of\\\\\\\\_pair}) / (\\\\mathrm{freq\\\\\\\\_of\\\\\\\\_first\\\\\\\\_element} \\\\times \\\\mathrm{freq\\\\\\\\_of\\\\\\\\_second\\\\\\\\_element})$$\\nBy dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa7e73e684812d15f5b64', 'page_content': 'For instance,\\nthe word `\"bug\"` would be tokenized to `[\"b\", \"ug\"]` but `\"mug\"` would be tokenized as `[\"\", \"ug\"]` since\\nthe symbol `\"m\"` is not in the base vocabulary. In general, single letters such as `\"m\"` are not replaced by the\\n`\"\"` symbol because the training data usually includes at least one occurrence of each letter, but it is likely\\nto happen for very special characters like emojis. As mentioned earlier, the vocabulary size, \\\\*i.e.\\\\* the base vocabulary size + the number of merges, is a hyperparameter\\nto choose. For instance [GPT](model\\\\_doc/gpt) has a vocabulary size of 40,478 since they have 478 base characters\\nand chose to stop training after 40,000 merges. #### Byte-level BPE\\nA base vocabulary that includes all possible base characters can be quite large if \\\\*e.g.\\\\* all unicode characters are\\nconsidered as base characters. To have a better base vocabulary, [GPT-2](https://cdn.openai.com/better-language-models/language\\\\_models\\\\_are\\\\_unsupervised\\\\_multitask\\\\_learners.pdf) uses bytes\\nas the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that\\nevery base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2\\'s\\ntokenizer can tokenize every text without the need for the  symbol. [GPT-2](model\\\\_doc/gpt) has a vocabulary\\nsize of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned\\nwith 50,000 merges. ### WordPiece\\nWordPiece is the subword tokenization algorithm used for [BERT](model\\\\_doc/bert), [DistilBERT](model\\\\_doc/distilbert), and [Electra](model\\\\_doc/electra). The algorithm was outlined in [Japanese and Korean\\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary. So what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is\\nequivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by\\nits second symbol is the greatest among all symbol pairs. \\\\*E.g.\\\\* `\"u\"`, followed by `\"g\"` would have only been\\nmerged if the probability of `\"ug\"` divided by `\"u\"`, `\"g\"` would have been greater than for any other symbol\\npair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it \\\\_loses\\\\_ by merging two symbols\\nto ensure it\\'s \\\\_worth it\\\\_. ### Unigram\\nUnigram is a subword tokenization algorithm introduced in [Subword Regularization: Improving Neural Network Translation\\nModels with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). In contrast to BPE or\\nWordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each\\nsymbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and\\nthe most common substrings. Unigram is not used directly for any of the models in the transformers, but it\\'s used in\\nconjunction with [SentencePiece](#sentencepiece). At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training\\ndata given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm\\ncomputes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then\\nremoves p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, \\\\*i.e.\\\\* those\\nsymbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has\\nreached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized. Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of\\ntokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:\\n```\\n[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"],\\n```\\n`\"hugs\"` could be tokenized both as `[\"hug\", \"s\"]`, `[\"h\", \"ug\", \"s\"]` or `[\"h\", \"u\", \"g\", \"s\"]`. So which one\\nto choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that\\nthe probability of each possible tokenization can be computed after training.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa7e73e684812d15f5b61', 'page_content': 'We can see that the words `[\"i\", \"have\", \"a\", \"new\"]` are present in the tokenizer\\'s vocabulary, but the word `\"gpu\"` is not. Consequently, the\\ntokenizer splits `\"gpu\"` into known subwords: `[\"gp\" and \"##u\"]`. `\"##\"` means that the rest of the token should\\nbe attached to the previous one, without space (for decoding or reversal of the tokenization). As another example, [`~transformers.XLNetTokenizer`] tokenizes our previously exemplary text as follows:\\n```py\\n>>> from transformers import XLNetTokenizer\\n>>> tokenizer = XLNetTokenizer.from\\\\_pretrained(\"xlnet-base-cased\")\\n>>> tokenizer.tokenize(\"Don\\'t you love 🤗 Transformers? We sure do.\")\\n[\"▁Don\", \"\\'\", \"t\", \"▁you\", \"▁love\", \"▁\", \"🤗\", \"▁\", \"Transform\", \"ers\", \"?\", \"▁We\", \"▁sure\", \"▁do\", \".\"]\\n```\\nWe\\'ll get back to the meaning of those `\"▁\"` when we look at [SentencePiece](#sentencepiece). As one can see,\\nthe rare word `\"Transformers\"` has been split into the more frequent subwords `\"Transform\"` and `\"ers\"`. Let\\'s now look at how the different subword tokenization algorithms work. Note that all of those tokenization\\nalgorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained\\non. ### Byte-Pair Encoding (BPE)\\nByte-Pair Encoding (BPE) was introduced in [Neural Machine Translation of Rare Words with Subword Units (Sennrich et\\nal., 2015)](https://arxiv.org/abs/1508.07909).', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The WordPiece Model uses the prefix `##` as a special identifier for continuing subwords. For example, a word like `\"word\"` would be tokenized as `[\"w\", \"##o\", \"##r\", \"##d\"]`.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--84e115c3-5183-41d5-9b14-877bff908f15-0', 'usage_metadata': {'input_tokens': 2217, 'output_tokens': 50, 'total_tokens': 2267, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:37:57,148 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 60%|██████    | 39/65 [02:51<01:55,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of the 🧨 Diffusers tutorials?\\n', 'type': 'human', 'id': '2505a3e4-8177-4f0f-987f-bf486b224cfa'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_EIAlRRAXrSov6L3VM19Ha2lh', 'function': {'arguments': '{\"query\":\"Diffusers tutorials purpose\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--8aa3eb6b-7250-40e4-ae2a-11c6f17b3cc5-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Diffusers tutorials purpose'}, 'id': 'call_EIAlRRAXrSov6L3VM19Ha2lh', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 361, 'output_tokens': 19, 'total_tokens': 380, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Overview\\nWelcome to 🧨 Diffusers! If you\\'re new to diffusion models and generative AI, and want to learn more, then you\\'ve come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how 🧨 Diffusers is meant to be used. You\\'ll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you\\'ll learn how to train your own diffusion model to generate what you want. After completing the tutorials, you\\'ll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications. Feel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers! Let\\'s start diffusing!\\n\\n!---\\nCopyright 2022 - The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![](https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg)\\n\\n[![GitHub](https://img.shields.io/github/license/huggingface/datasets.svg?color=blue)](https://github.com/huggingface/diffusers/blob/main/LICENSE)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/diffusers.svg)](https://github.com/huggingface/diffusers/releases)\\n[![GitHub release](https://static.pepy.tech/badge/diffusers/month)](https://pepy.tech/project/diffusers)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CODE_OF_CONDUCT.md)\\n[![X account](https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&label=Follow%20%40diffuserslib)](https://twitter.com/diffuserslib)\\n\\n🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\\'re looking for a simple inference solution or training your own diffusion models, 🤗 Diffusers is a modular toolbox that supports both.\\n\\nOur library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). 🤗 Diffusers offers three core components:\\n- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code. - Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality. - Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. ## Installation\\nWe recommend installing 🤗 Diffusers in a virtual environment from PyPI or Conda. For more details about installing [PyTorch](https://pytorch.org/get-started/locally/) and [Flax](https://flax.readthedocs.io/en/latest/#installation), please refer to their official documentation. ### PyTorch\\nWith `pip` (official package):\\n```bash\\npip install --upgrade diffusers[torch]\\n```\\nWith `conda` (maintained by the community):\\n```sh\\nconda install -c conda-forge diffusers\\n```\\n### Flax\\nWith `pip` (official package):\\n```bash\\npip install --upgrade diffusers[flax]\\n```\\n### Apple Silicon (M1/M2) support\\nPlease refer to the [How to use Stable Diffusion in Apple Silicon](https://huggingface.co/docs/diffusers/optimization/mps) guide. ## Quickstart\\nGenerating outputs is super easy with 🤗 Diffusers. To generate an image from text, use the `from\\\\_pretrained` method to load any pretrained diffusion model (browse the [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) for 16000+ checkpoints):\\n```python\\nfrom diffusers import DiffusionPipeline\\nimport torch\\npipeline = DiffusionPipeline.from\\\\_pretrained(\"runwayml/stable-diffusion-v1-5\", torch\\\\_dtype=torch.float16)\\npipeline.to(\"cuda\")\\npipeline(\"An image of a squirrel in Picasso style\").images[0]\\n```\\nYou can also dig into the models and schedulers toolbox to build your own diffusion system:\\n```python\\nfrom diffusers import DDPMScheduler, UNet2DModel\\nfrom PIL import Image\\nimport torch\\nscheduler = DDPMScheduler.from\\\\_pretrained(\"google/ddpm-cat-256\")\\nmodel = UNet2DModel.from\\\\_pretrained(\"google/ddpm-cat-256\").to(\"cuda\")\\nscheduler.set\\\\_timesteps(50)\\nsample\\\\_size = model.config.sample\\\\_size\\nnoise = torch.randn((1, 3, sample\\\\_size, sample\\\\_size), device=\"cuda\")\\ninput = noise\\nfor t in scheduler.timesteps:\\nwith torch.no\\\\_grad():\\nnoisy\\\\_residual = model(input, t).sample\\nprev\\\\_noisy\\\\_sample = scheduler.step(noisy\\\\_residual, t, input).prev\\\\_sample\\ninput = prev\\\\_noisy\\\\_sample\\nimage = (input / 2 + 0.5).clamp(0, 1)\\nimage = image.cpu().permute(0, 2, 3, 1).numpy()[0]\\nimage = Image.fromarray((image \\\\* 255).round().astype(\"uint8\"))\\nimage\\n```\\nCheck out the [Quickstart](https://huggingface.co/docs/diffusers/quicktour) to launch your diffusion journey today! ## How to navigate the documentation\\n| \\\\*\\\\*Documentation\\\\*\\\\* | \\\\*\\\\*What can I learn?\\\\*\\\\* |\\n|---------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| [Tutorial](https://huggingface.co/docs/diffusers/tutorials/tutorial\\\\_overview) | A basic crash course for learning how to use the library\\'s most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model. |\\n| [Loading](https://huggingface.co/docs/diffusers/using-diffusers/loading\\\\_overview) | Guides for how to load and configure all the components (pipelines, models, and schedulers) of the library, as well as how to use different schedulers. |\\n| [Pipelines for inference](https://huggingface.co/docs/diffusers/using-diffusers/pipeline\\\\_overview) | Guides for how to use pipelines for different inference tasks, batched generation, controlling generated outputs and randomness, and how to contribute a pipeline to the library.', 'type': 'tool', 'name': 'data_retriever', 'id': 'd1d2ff96-1369-436f-8078-49941aa826ec', 'tool_call_id': 'call_EIAlRRAXrSov6L3VM19Ha2lh', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa5443e684812d15f5836', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Overview\\nWelcome to 🧨 Diffusers! If you\\'re new to diffusion models and generative AI, and want to learn more, then you\\'ve come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how 🧨 Diffusers is meant to be used. You\\'ll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you\\'ll learn how to train your own diffusion model to generate what you want. After completing the tutorials, you\\'ll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications. Feel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers! Let\\'s start diffusing!', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bf25666af48523f51d80', 'page_content': '!---\\nCopyright 2022 - The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n\\n![](https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg)\\n\\n[![GitHub](https://img.shields.io/github/license/huggingface/datasets.svg?color=blue)](https://github.com/huggingface/diffusers/blob/main/LICENSE)\\n[![GitHub release](https://img.shields.io/github/release/huggingface/diffusers.svg)](https://github.com/huggingface/diffusers/releases)\\n[![GitHub release](https://static.pepy.tech/badge/diffusers/month)](https://pepy.tech/project/diffusers)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CODE_OF_CONDUCT.md)\\n[![X account](https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&label=Follow%20%40diffuserslib)](https://twitter.com/diffuserslib)\\n\\n🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\\'re looking for a simple inference solution or training your own diffusion models, 🤗 Diffusers is a modular toolbox that supports both.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bf25666af48523f51d81', 'page_content': 'Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). 🤗 Diffusers offers three core components:\\n- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code. - Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality. - Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. ## Installation\\nWe recommend installing 🤗 Diffusers in a virtual environment from PyPI or Conda. For more details about installing [PyTorch](https://pytorch.org/get-started/locally/) and [Flax](https://flax.readthedocs.io/en/latest/#installation), please refer to their official documentation. ### PyTorch\\nWith `pip` (official package):\\n```bash\\npip install --upgrade diffusers[torch]\\n```\\nWith `conda` (maintained by the community):\\n```sh\\nconda install -c conda-forge diffusers\\n```\\n### Flax\\nWith `pip` (official package):\\n```bash\\npip install --upgrade diffusers[flax]\\n```\\n### Apple Silicon (M1/M2) support\\nPlease refer to the [How to use Stable Diffusion in Apple Silicon](https://huggingface.co/docs/diffusers/optimization/mps) guide. ## Quickstart\\nGenerating outputs is super easy with 🤗 Diffusers. To generate an image from text, use the `from\\\\_pretrained` method to load any pretrained diffusion model (browse the [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) for 16000+ checkpoints):\\n```python\\nfrom diffusers import DiffusionPipeline\\nimport torch\\npipeline = DiffusionPipeline.from\\\\_pretrained(\"runwayml/stable-diffusion-v1-5\", torch\\\\_dtype=torch.float16)\\npipeline.to(\"cuda\")\\npipeline(\"An image of a squirrel in Picasso style\").images[0]\\n```\\nYou can also dig into the models and schedulers toolbox to build your own diffusion system:\\n```python\\nfrom diffusers import DDPMScheduler, UNet2DModel\\nfrom PIL import Image\\nimport torch\\nscheduler = DDPMScheduler.from\\\\_pretrained(\"google/ddpm-cat-256\")\\nmodel = UNet2DModel.from\\\\_pretrained(\"google/ddpm-cat-256\").to(\"cuda\")\\nscheduler.set\\\\_timesteps(50)\\nsample\\\\_size = model.config.sample\\\\_size\\nnoise = torch.randn((1, 3, sample\\\\_size, sample\\\\_size), device=\"cuda\")\\ninput = noise\\nfor t in scheduler.timesteps:\\nwith torch.no\\\\_grad():\\nnoisy\\\\_residual = model(input, t).sample\\nprev\\\\_noisy\\\\_sample = scheduler.step(noisy\\\\_residual, t, input).prev\\\\_sample\\ninput = prev\\\\_noisy\\\\_sample\\nimage = (input / 2 + 0.5).clamp(0, 1)\\nimage = image.cpu().permute(0, 2, 3, 1).numpy()[0]\\nimage = Image.fromarray((image \\\\* 255).round().astype(\"uint8\"))\\nimage\\n```\\nCheck out the [Quickstart](https://huggingface.co/docs/diffusers/quicktour) to launch your diffusion journey today! ## How to navigate the documentation\\n| \\\\*\\\\*Documentation\\\\*\\\\* | \\\\*\\\\*What can I learn?\\\\*\\\\* |\\n|---------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| [Tutorial](https://huggingface.co/docs/diffusers/tutorials/tutorial\\\\_overview) | A basic crash course for learning how to use the library\\'s most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model. |\\n| [Loading](https://huggingface.co/docs/diffusers/using-diffusers/loading\\\\_overview) | Guides for how to load and configure all the components (pipelines, models, and schedulers) of the library, as well as how to use different schedulers. |\\n| [Pipelines for inference](https://huggingface.co/docs/diffusers/using-diffusers/pipeline\\\\_overview) | Guides for how to use pipelines for different inference tasks, batched generation, controlling generated outputs and randomness, and how to contribute a pipeline to the library.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The purpose of the 🧨 Diffusers tutorials is to provide a gentle introduction to diffusion models and help users understand the library fundamentals. These beginner-friendly tutorials are designed to teach users how to use a pipeline for inference to rapidly generate outputs, as well as how to deconstruct that pipeline to understand how to use the library as a modular toolbox for building their own diffusion systems. After completing the tutorials, users will gain the necessary skills to explore the library on their own and apply it to their own projects and applications.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--a6bf4e54-7239-443c-ad28-8403c26ccf73-0', 'usage_metadata': {'input_tokens': 2103, 'output_tokens': 103, 'total_tokens': 2206, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:38:01,698 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 62%|██████▏   | 40/65 [02:56<01:51,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': \"What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\\n\", 'type': 'human', 'id': '13a65cb9-e689-4ba1-b3b0-266737a02598'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_dhnSACIdLaAmgcO4MRICmCPW', 'function': {'arguments': '{\"query\":\"default setting for allow_flagging parameter in Gradio Interface\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--1175b6d4-4c1c-4c49-8dc2-c59b8a9c161c-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'default setting for allow_flagging parameter in Gradio Interface'}, 'id': 'call_dhnSACIdLaAmgcO4MRICmCPW', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 369, 'output_tokens': 26, 'total_tokens': 395, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Using Flagging\\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\\nTags: FLAGGING, DATA\\n## Introduction\\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \"hard\" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust. Gradio simplifies the collection of this data by including a \\\\*\\\\*Flag\\\\*\\\\* button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`. ## The \\\\*\\\\*Flag\\\\*\\\\* button in `gradio.Interface`\\nFlagging with Gradio\\'s `Interface` is especially easy. By default, underneath the output components, there is a button marked \\\\*\\\\*Flag\\\\*\\\\*. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file. There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail. - `allow\\\\_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`. - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked. - `auto`: users will not see a button to flag, but every sample will be flagged automatically. - `never`: users will not see a button to flag, and no sample will be flagged. - `flagging\\\\_options`: this parameter can be either `None` (default) or a list of strings. - If `None`, then the user simply clicks on the \\\\*\\\\*Flag\\\\*\\\\* button and no additional options are shown. - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled \\\\*\\\\*Flag as Incorrect\\\\*\\\\* and \\\\*\\\\*Flag as Ambiguous\\\\*\\\\* appear. This only applies if `allow\\\\_flagging` is `\"manual\"`. - The chosen option is then logged along with the input and output. - `flagging\\\\_dir`: this parameter takes a string. - It represents what to name the directory where flagged data is stored. - `flagging\\\\_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\\n- Using this parameter allows you to write custom code that gets run when the flag button is clicked\\n- By default, this is set to an instance of `gr.CSVLogger`\\n- One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset.\\n\\n- `description`: which accepts text, markdown or HTML and places it right under the title. - `article`: which also accepts text, markdown or HTML and places it below the interface. ![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)\\nIf you\\'re using the `Blocks` API instead, you can insert text, markdown, or HTML anywhere using the `gr.Markdown(...)` or `gr.HTML(...)` components, with descriptive content inside the `Component` constructor. Another useful keyword argument is `label=`, which is present in every `Component`. This modifies the label text at the top of each `Component`. You can also add the `info=` keyword argument to form elements like `Textbox` or `Radio` to provide further information on their usage. ```python\\ngr.Number(label=\\'Age\\', info=\\'In years, must be greater than 0\\')\\n```\\n## Flagging\\nBy default, an `Interface` will have \"Flag\" button. When a user testing your `Interface` sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the `flagging\\\\_dir=` argument to the `Interface` constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:\\n```directory\\n+-- calculator.py\\n+-- flagged/\\n| +-- logs.csv\\n```\\n\\\\_flagged/logs.csv\\\\_\\n```csv\\nnum1,operation,num2,Output\\n5,add,7,12\\n6,subtract,1.5,4.5\\n```\\nWith the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:\\n```directory\\n+-- sepia.py\\n+-- flagged/\\n| +-- logs.csv\\n| +-- im/\\n| | +-- 0.png\\n| | +-- 1.png\\n| +-- Output/\\n| | +-- 0.png\\n| | +-- 1.png\\n```\\n\\\\_flagged/logs.csv\\\\_\\n```csv\\nim,Output\\nim/0.png,Output/0.png\\nim/1.png,Output/1.png\\n```\\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging\\\\_options` argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV.\\n\\n(See more below.)\\n## What happens to flagged data? Within the directory provided by the `flagging\\\\_dir` argument, a CSV file will log the flagged data. Here\\'s an example: The code below creates the calculator interface embedded below it:\\n```python\\nimport gradio as gr\\ndef calculator(num1, operation, num2):\\nif operation == \"add\":\\nreturn num1 + num2\\nelif operation == \"subtract\":\\nreturn num1 - num2\\nelif operation == \"multiply\":\\nreturn num1 \\\\* num2\\nelif operation == \"divide\":\\nreturn num1 / num2\\niface = gr.Interface(\\ncalculator,\\n[\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\\n\"number\",\\nallow\\\\_flagging=\"manual\"\\n)\\niface.launch()\\n```\\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged. ```directory\\n+-- flagged/\\n| +-- logs.csv\\n```\\n\\\\_flagged/logs.csv\\\\_\\n```csv\\nnum1,operation,num2,Output,timestamp\\n5,add,7,12,2022-01-31 11:40:51.093412\\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\\n```\\nIf the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure. ```directory\\n+-- flagged/\\n| +-- logs.csv\\n| +-- image/\\n| | +-- 0.png\\n| | +-- 1.png\\n| +-- Output/\\n| | +-- 0.png\\n| | +-- 1.png\\n```\\n\\\\_flagged/logs.csv\\\\_\\n```csv\\nim,Output timestamp\\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\\n```\\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging\\\\_options` argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV. If we go back to the calculator example, the following code will create the interface embedded below it. ```python\\niface = gr.Interface(\\ncalculator,\\n[\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\\n\"number\",\\nallow\\\\_flagging=\"manual\",\\nflagging\\\\_options=[\"wrong sign\", \"off by one\", \"other\"]\\n)\\niface.launch()\\n```\\nWhen users click the flag button, the csv file will now include a column indicating the selected option. \\\\_flagged/logs.csv\\\\_\\n```csv\\nnum1,operation,num2,Output,flag,timestamp\\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\\n```\\n## The HuggingFaceDatasetSaver Callback\\nSometimes, saving the data to a local CSV file doesn\\'t make sense.', 'type': 'tool', 'name': 'data_retriever', 'id': '1d06faa3-4108-4ab8-903d-0f5a370c361e', 'tool_call_id': 'call_dhnSACIdLaAmgcO4MRICmCPW', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c32e666af48523f52267', 'page_content': 'Using Flagging\\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\\nTags: FLAGGING, DATA\\n## Introduction\\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \"hard\" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust. Gradio simplifies the collection of this data by including a \\\\*\\\\*Flag\\\\*\\\\* button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`. ## The \\\\*\\\\*Flag\\\\*\\\\* button in `gradio.Interface`\\nFlagging with Gradio\\'s `Interface` is especially easy. By default, underneath the output components, there is a button marked \\\\*\\\\*Flag\\\\*\\\\*. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file. There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail. - `allow\\\\_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`. - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked. - `auto`: users will not see a button to flag, but every sample will be flagged automatically. - `never`: users will not see a button to flag, and no sample will be flagged. - `flagging\\\\_options`: this parameter can be either `None` (default) or a list of strings. - If `None`, then the user simply clicks on the \\\\*\\\\*Flag\\\\*\\\\* button and no additional options are shown. - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled \\\\*\\\\*Flag as Incorrect\\\\*\\\\* and \\\\*\\\\*Flag as Ambiguous\\\\*\\\\* appear. This only applies if `allow\\\\_flagging` is `\"manual\"`. - The chosen option is then logged along with the input and output. - `flagging\\\\_dir`: this parameter takes a string. - It represents what to name the directory where flagged data is stored. - `flagging\\\\_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\\n- Using this parameter allows you to write custom code that gets run when the flag button is clicked\\n- By default, this is set to an instance of `gr.CSVLogger`\\n- One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aac9b3e684812d15f60ff', 'page_content': '- `description`: which accepts text, markdown or HTML and places it right under the title. - `article`: which also accepts text, markdown or HTML and places it below the interface. ![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)\\nIf you\\'re using the `Blocks` API instead, you can insert text, markdown, or HTML anywhere using the `gr.Markdown(...)` or `gr.HTML(...)` components, with descriptive content inside the `Component` constructor. Another useful keyword argument is `label=`, which is present in every `Component`. This modifies the label text at the top of each `Component`. You can also add the `info=` keyword argument to form elements like `Textbox` or `Radio` to provide further information on their usage. ```python\\ngr.Number(label=\\'Age\\', info=\\'In years, must be greater than 0\\')\\n```\\n## Flagging\\nBy default, an `Interface` will have \"Flag\" button. When a user testing your `Interface` sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the `flagging\\\\_dir=` argument to the `Interface` constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:\\n```directory\\n+-- calculator.py\\n+-- flagged/\\n| +-- logs.csv\\n```\\n\\\\_flagged/logs.csv\\\\_\\n```csv\\nnum1,operation,num2,Output\\n5,add,7,12\\n6,subtract,1.5,4.5\\n```\\nWith the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:\\n```directory\\n+-- sepia.py\\n+-- flagged/\\n| +-- logs.csv\\n| +-- im/\\n| | +-- 0.png\\n| | +-- 1.png\\n| +-- Output/\\n| | +-- 0.png\\n| | +-- 1.png\\n```\\n\\\\_flagged/logs.csv\\\\_\\n```csv\\nim,Output\\nim/0.png,Output/0.png\\nim/1.png,Output/1.png\\n```\\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging\\\\_options` argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c32e666af48523f52268', 'page_content': '(See more below.)\\n## What happens to flagged data? Within the directory provided by the `flagging\\\\_dir` argument, a CSV file will log the flagged data. Here\\'s an example: The code below creates the calculator interface embedded below it:\\n```python\\nimport gradio as gr\\ndef calculator(num1, operation, num2):\\nif operation == \"add\":\\nreturn num1 + num2\\nelif operation == \"subtract\":\\nreturn num1 - num2\\nelif operation == \"multiply\":\\nreturn num1 \\\\* num2\\nelif operation == \"divide\":\\nreturn num1 / num2\\niface = gr.Interface(\\ncalculator,\\n[\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\\n\"number\",\\nallow\\\\_flagging=\"manual\"\\n)\\niface.launch()\\n```\\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged. ```directory\\n+-- flagged/\\n| +-- logs.csv\\n```\\n\\\\_flagged/logs.csv\\\\_\\n```csv\\nnum1,operation,num2,Output,timestamp\\n5,add,7,12,2022-01-31 11:40:51.093412\\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\\n```\\nIf the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure. ```directory\\n+-- flagged/\\n| +-- logs.csv\\n| +-- image/\\n| | +-- 0.png\\n| | +-- 1.png\\n| +-- Output/\\n| | +-- 0.png\\n| | +-- 1.png\\n```\\n\\\\_flagged/logs.csv\\\\_\\n```csv\\nim,Output timestamp\\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\\n```\\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging\\\\_options` argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV. If we go back to the calculator example, the following code will create the interface embedded below it. ```python\\niface = gr.Interface(\\ncalculator,\\n[\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\\n\"number\",\\nallow\\\\_flagging=\"manual\",\\nflagging\\\\_options=[\"wrong sign\", \"off by one\", \"other\"]\\n)\\niface.launch()\\n```\\nWhen users click the flag button, the csv file will now include a column indicating the selected option. \\\\_flagged/logs.csv\\\\_\\n```csv\\nnum1,operation,num2,Output,flag,timestamp\\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\\n```\\n## The HuggingFaceDatasetSaver Callback\\nSometimes, saving the data to a local CSV file doesn\\'t make sense.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The default setting for the `allow_flagging` parameter in Gradio\\'s `Interface` is `\"manual\"`.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--b723110a-d67f-4405-8af1-94b3389e125d-0', 'usage_metadata': {'input_tokens': 2351, 'output_tokens': 23, 'total_tokens': 2374, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:38:05,952 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 63%|██████▎   | 41/65 [03:00<01:45,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Where can the full code for the Stable Diffusion demo be found?\\n', 'type': 'human', 'id': 'ea95ecb8-572a-46cf-9e9c-b03baa8111c4'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_mAmLanLjSx9pDEFFCX8DQkqq', 'function': {'arguments': '{\"query\":\"Stable Diffusion demo full code\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--bd3411c9-e8cc-4a47-80c6-b49da24de28a-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Stable Diffusion demo full code'}, 'id': 'call_mAmLanLjSx9pDEFFCX8DQkqq', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 21, 'total_tokens': 383, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '--\\ntitle: 🧨 Stable Diffusion in JAX / Flax ! thumbnail: /blog/assets/108\\\\_stable\\\\_diffusion\\\\_jax/thumbnail.png\\nauthors:\\n- user: pcuenq\\n- user: patrickvonplaten\\n---\\n# 🧨 Stable Diffusion in JAX / Flax ! [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_jax_how_to.ipynb)\\n# \\\\*\\\\*Stable Diffusion in JAX / Flax\\\\*\\\\* 🚀\\n🤗 Hugging Face [Diffusers](https://github.com/huggingface/diffusers) supports Flax since version `0.5.1`! This allows for super fast inference on Google TPUs, such as those available in Colab, Kaggle or Google Cloud Platform. This post shows how to run inference using JAX / Flax. If you want more details about how Stable Diffusion works or want to run it in GPU, please refer to [this Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\\\_diffusion.ipynb). If you want to follow along, click the button above to open this post as a Colab notebook. First, make sure you are using a TPU backend. If you are running this notebook in Colab, select `Runtime` in the menu above, then select the option \"Change runtime type\" and then select `TPU` under the `Hardware accelerator` setting. Note that JAX is not exclusive to TPUs, but it shines on that hardware because each TPU server has 8 TPU accelerators working in parallel. ## Setup\\n``` python\\nimport jax\\nnum\\\\_devices = jax.device\\\\_count()\\ndevice\\\\_type = jax.devices()[0].device\\\\_kind\\nprint(f\"Found {num\\\\_devices} JAX devices of type {device\\\\_type}.\")\\nassert \"TPU\" in device\\\\_type, \"Available device is not a TPU, please select TPU from Edit > Notebook settings > Hardware accelerator\"\\n```\\n\\\\*Output\\\\*:\\n```bash\\nFound 8 JAX devices of type TPU v2.\\n\\nFree solutions include many [spaces](https://huggingface.co/spaces) here on 🤗 Hugging Face, such as the [Stable Diffusion 2.1 Demo](https://huggingface.co/spaces/stabilityai/stable-diffusion) or the [camemduru webui](https://huggingface.co/spaces/camenduru/webui). You can find a list of additional online services [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services). You can even use 🤗 [Diffusers](https://huggingface.co/docs/diffusers/index) to write your own free solution! You can find a simple code example to get started [here](https://colab.research.google.com/drive/1HebngGyjKj7nLdXfj6Qi0N1nh7WvD74z?usp=sharing). \\\\*Note:\\\\* Parts of this series will use advanced features such as image2image, which may not be available on all online services. ### Generating Concept Art \\nLet\\'s generate some concept art. The steps are simple:\\n1. Type what you want. 2. Click generate. ![Stable Diffusion Demo Space](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/sd-demo.png)\\nBut, how do you get the results you actually want?\\n\\nCheck out the Stable Diffusion demo for more: https://hf.co/spaces/stabilityai/stable-diffusion/', 'type': 'tool', 'name': 'data_retriever', 'id': '2a4061ca-5dbd-4df1-82ea-42f0735e3b09', 'tool_call_id': 'call_mAmLanLjSx9pDEFFCX8DQkqq', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c26a666af48523f52182', 'page_content': '--\\ntitle: 🧨 Stable Diffusion in JAX / Flax ! thumbnail: /blog/assets/108\\\\_stable\\\\_diffusion\\\\_jax/thumbnail.png\\nauthors:\\n- user: pcuenq\\n- user: patrickvonplaten\\n---\\n# 🧨 Stable Diffusion in JAX / Flax ! [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_jax_how_to.ipynb)\\n# \\\\*\\\\*Stable Diffusion in JAX / Flax\\\\*\\\\* 🚀\\n🤗 Hugging Face [Diffusers](https://github.com/huggingface/diffusers) supports Flax since version `0.5.1`! This allows for super fast inference on Google TPUs, such as those available in Colab, Kaggle or Google Cloud Platform. This post shows how to run inference using JAX / Flax. If you want more details about how Stable Diffusion works or want to run it in GPU, please refer to [this Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\\\_diffusion.ipynb). If you want to follow along, click the button above to open this post as a Colab notebook. First, make sure you are using a TPU backend. If you are running this notebook in Colab, select `Runtime` in the menu above, then select the option \"Change runtime type\" and then select `TPU` under the `Hardware accelerator` setting. Note that JAX is not exclusive to TPUs, but it shines on that hardware because each TPU server has 8 TPU accelerators working in parallel. ## Setup\\n``` python\\nimport jax\\nnum\\\\_devices = jax.device\\\\_count()\\ndevice\\\\_type = jax.devices()[0].device\\\\_kind\\nprint(f\"Found {num\\\\_devices} JAX devices of type {device\\\\_type}.\")\\nassert \"TPU\" in device\\\\_type, \"Available device is not a TPU, please select TPU from Edit > Notebook settings > Hardware accelerator\"\\n```\\n\\\\*Output\\\\*:\\n```bash\\nFound 8 JAX devices of type TPU v2.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab8f33e684812d15f6f3c', 'page_content': \"Free solutions include many [spaces](https://huggingface.co/spaces) here on 🤗 Hugging Face, such as the [Stable Diffusion 2.1 Demo](https://huggingface.co/spaces/stabilityai/stable-diffusion) or the [camemduru webui](https://huggingface.co/spaces/camenduru/webui). You can find a list of additional online services [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services). You can even use 🤗 [Diffusers](https://huggingface.co/docs/diffusers/index) to write your own free solution! You can find a simple code example to get started [here](https://colab.research.google.com/drive/1HebngGyjKj7nLdXfj6Qi0N1nh7WvD74z?usp=sharing). \\\\*Note:\\\\* Parts of this series will use advanced features such as image2image, which may not be available on all online services. ### Generating Concept Art \\nLet's generate some concept art. The steps are simple:\\n1. Type what you want. 2. Click generate. ![Stable Diffusion Demo Space](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/sd-demo.png)\\nBut, how do you get the results you actually want?\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bfe6666af48523f51e43', 'page_content': 'Check out the Stable Diffusion demo for more: https://hf.co/spaces/stabilityai/stable-diffusion/', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The full code for the Stable Diffusion demo can be found in the [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--2c6f40dc-5ede-48f2-bedf-9248c22f50de-0', 'usage_metadata': {'input_tokens': 1101, 'output_tokens': 47, 'total_tokens': 1148, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:38:10,522 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 65%|██████▍   | 42/65 [03:04<01:42,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What transformation does the FNet model use to replace the self-attention layer in a BERT model?\\n', 'type': 'human', 'id': '9cd115bf-ddb7-4a86-a370-c5a968597d71'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_9RxNEWyX1xUVUL8eCR5yXxnb', 'function': {'arguments': '{\"query\":\"FNet model self-attention layer replacement BERT\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--a5dff2e0-e32a-450f-9c21-c7e77d12c143-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'FNet model self-attention layer replacement BERT'}, 'id': 'call_9RxNEWyX1xUVUL8eCR5yXxnb', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 369, 'output_tokens': 25, 'total_tokens': 394, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# FNet\\n## Overview\\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\\npaper is the following:\\n\\\\*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\\noutperform Transformer counterparts.\\\\*\\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f\\\\_net). ## Usage tips\\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\\nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\\nsequence length for fine-tuning and inference. ## Resources\\n- [Text classification task guide](../tasks/sequence\\\\_classification)\\n- [Token classification task guide](../tasks/token\\\\_classification)\\n- [Question answering task guide](../tasks/question\\\\_answering)\\n- [Masked language modeling task guide](../tasks/masked\\\\_language\\\\_modeling)\\n- [Multiple choice task guide](../tasks/multiple\\\\_choice)\\n## FNetConfig\\n[[autodoc]] FNetConfig\\n## FNetTokenizer\\n[[autodoc]] FNetTokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## FNetTokenizerFast\\n[[autodoc]] FNetTokenizerFast\\n## FNetModel\\n[[autodoc]] FNetModel\\n- forward\\n## FNetForPreTraining\\n[[autodoc]] FNetForPreTraining\\n- forward\\n## FNetForMaskedLM\\n[[autodoc]] FNetForMaskedLM\\n- forward\\n## FNetForNextSentencePrediction\\n[[autodoc]] FNetForNextSentencePrediction\\n- forward\\n## FNetForSequenceClassification\\n[[autodoc]] FNetForSequenceClassification\\n- forward\\n## FNetForMultipleChoice\\n[[autodoc]] FNetForMultipleChoice\\n- forward\\n## FNetForTokenClassification\\n[[autodoc]] FNetForTokenClassification\\n- forward\\n## FNetForQuestionAnswering\\n[[autodoc]] FNetForQuestionAnswering\\n- forward\\n\\n!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# DeBERTa\\n## Overview\\nThe DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google\\'s\\nBERT model released in 2018 and Facebook\\'s RoBERTa model released in 2019. It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in\\nRoBERTa. The abstract from the paper is the following:\\n\\\\*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\\nlanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\\ndisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\\ndisentangled attention mechanism, where each word is represented using two vectors that encode its content and\\nposition, respectively, and the attention weights among words are computed using disentangled matrices on their\\ncontents and relative positions.\\n\\n!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# BERT\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\\n\\n## Overview\\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\\'s a\\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\\n\\\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\\narchitecture modifications.\\\\*\\n\\\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\\\*\\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf).', 'type': 'tool', 'name': 'data_retriever', 'id': '5dbaa1c0-cb0a-4db4-ad19-159792a101ee', 'tool_call_id': 'call_9RxNEWyX1xUVUL8eCR5yXxnb', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c123666af48523f51fc0', 'page_content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# FNet\\n## Overview\\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\\npaper is the following:\\n\\\\*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\\noutperform Transformer counterparts.\\\\*\\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f\\\\_net). ## Usage tips\\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\\nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\\nsequence length for fine-tuning and inference. ## Resources\\n- [Text classification task guide](../tasks/sequence\\\\_classification)\\n- [Token classification task guide](../tasks/token\\\\_classification)\\n- [Question answering task guide](../tasks/question\\\\_answering)\\n- [Masked language modeling task guide](../tasks/masked\\\\_language\\\\_modeling)\\n- [Multiple choice task guide](../tasks/multiple\\\\_choice)\\n## FNetConfig\\n[[autodoc]] FNetConfig\\n## FNetTokenizer\\n[[autodoc]] FNetTokenizer\\n- build\\\\_inputs\\\\_with\\\\_special\\\\_tokens\\n- get\\\\_special\\\\_tokens\\\\_mask\\n- create\\\\_token\\\\_type\\\\_ids\\\\_from\\\\_sequences\\n- save\\\\_vocabulary\\n## FNetTokenizerFast\\n[[autodoc]] FNetTokenizerFast\\n## FNetModel\\n[[autodoc]] FNetModel\\n- forward\\n## FNetForPreTraining\\n[[autodoc]] FNetForPreTraining\\n- forward\\n## FNetForMaskedLM\\n[[autodoc]] FNetForMaskedLM\\n- forward\\n## FNetForNextSentencePrediction\\n[[autodoc]] FNetForNextSentencePrediction\\n- forward\\n## FNetForSequenceClassification\\n[[autodoc]] FNetForSequenceClassification\\n- forward\\n## FNetForMultipleChoice\\n[[autodoc]] FNetForMultipleChoice\\n- forward\\n## FNetForTokenClassification\\n[[autodoc]] FNetForTokenClassification\\n- forward\\n## FNetForQuestionAnswering\\n[[autodoc]] FNetForQuestionAnswering\\n- forward', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab5453e684812d15f6b0d', 'page_content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# DeBERTa\\n## Overview\\nThe DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google\\'s\\nBERT model released in 2018 and Facebook\\'s RoBERTa model released in 2019. It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in\\nRoBERTa. The abstract from the paper is the following:\\n\\\\*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\\nlanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\\ndisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\\ndisentangled attention mechanism, where each word is represented using two vectors that encode its content and\\nposition, respectively, and the attention weights among words are computed using disentangled matrices on their\\ncontents and relative positions.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689abb613e684812d15f71e1', 'page_content': '!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# BERT\\n\\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\\n\\n## Overview\\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\\'s a\\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\\n\\\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\\narchitecture modifications.\\\\*\\n\\\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\\\*\\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf).', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The FNet model replaces the self-attention layer in a BERT model with a Fourier transform, which returns only the real parts of the transform.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--4752aa7a-f920-4816-8c6b-5ba5e2946bf2-0', 'usage_metadata': {'input_tokens': 2306, 'output_tokens': 30, 'total_tokens': 2336, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:38:16,074 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 66%|██████▌   | 43/65 [03:10<01:45,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': \"What type of test should typically accompany a bug fix in Gradio's testing strategy?\\n\", 'type': 'human', 'id': 'bb990688-6cf0-4381-80e8-c19d05ce975a'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_yUoySy1uTNT0yoBzVlFpM0x0', 'function': {'arguments': '{\"query\":\"Gradio testing strategy bug fix\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--a91d8a85-20c7-418b-a0cf-364c6b5722d6-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Gradio testing strategy bug fix'}, 'id': 'call_yUoySy1uTNT0yoBzVlFpM0x0', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 21, 'total_tokens': 386, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': \"Test Strategy\\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to. This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Objectives\\nThe purposes of all testing activities on Gradio fit one of the following objectives:\\n1. Ensure that the Gradio library functions as we expect it to. 2. Enable the maintenance team to quickly identify both the presence and source of defects. 3. Prevent regressions, i.e. if we fix something it should stay fixed. 4. Improve the quality of the codebase in order to ease maintenance efforts. 5. Reduce the amount of manual testing required. ## Scope\\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas. We should not focus on code coverage but on test coverage following the below criteria:\\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\\n## Types of testing\\nOur tests will broadly fall into one of three categories:\\n- Static Quality checks\\n- Dynamic 'Code' tests\\n- Dynamic Functional tests\\n### Static Quality checks\\nStatic quality checks are generally very fast to run and do not require building the code base.\\n\\nTests in this category could be browser-based end-to-end tests, accessibility tests, or performance tests. They are sometimes called acceptance tests. ## Testing tools\\nWe currently use the following tools:\\n### Static quality checks\\n- Python type-checking (python)\\n- Black linting (python)\\n- ruff formatting (python)\\n- prettier formatting (javascript/svelte)\\n- TypeScript type-checking (javascript/svelte)\\n- eslint linting (javascript/svelte) [in progress]\\n### Dynamic code tests\\n- pytest (python unit and integration tests)\\n- vitest (node-based unit and integration tests)\\n- playwright (browser-based unit and integration tests)\\n### Functional/acceptance tests\\n- playwright (full end to end testing)\\n- chromatic (visual testing) [in progress]\\n- Accessibility testing [to do]\\n## Supported environments and versions\\nAll operating systems refer to the current runner variants supported by GitHub actions. All unspecified version segments (`x`) refer to latest. | Software | Version(s) | Operating System(s) |\\n| -------- | --------------------- | --------------------------------- |\\n| Python | `3.8.x` | `ubuntu-latest`, `windows-latest` |\\n| Node | `18.x.x` | `ubuntu-latest` |\\n| Browser | `playwright-chrome-x` | `ubuntu-latest` |\\n## Test execution\\nTests need to be executed in a number of environments and at different stages of the development cycle in order to be useful. The requirements for tests are as follows:\\n- \\\\*\\\\*Locally\\\\*\\\\*: it is important that developers can easily run most tests locally to ensure a passing suite before making a PR. There are some exceptions to this, certain tests may require access to secret values which we cannot make available to all possible contributors for practical security reasons. It is reasonable that it isn't possible to run these tests but they should be disabled by default when running locally. - \\\\*\\\\*CI\\\\*\\\\* - It is \\\\_critical\\\\_ that all tests run successfully in CI with no exceptions. Not every test is required to pass to satisfy CI checks for practical reasons but it is required that all tests should run in CI and notify us if something unexpected happens in order for the development team to take appropriate action. For instructions on how to write and run tests see the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Managing defects\\nAs we formalise our testing strategy and bring / keep our test up to standard, it is important that we have some principles on managing defects as they occur/ are reported. For now we can have one very simple rule:\\n- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should \\\\_typically\\\\_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule.\\n\\nThanks [@abidlabs](https://github.com/abidlabs)! - [#6079](https://github.com/gradio-app/gradio/pull/6079) [`3b2d9eaa3`](https://github.com/gradio-app/gradio/commit/3b2d9eaa3e84de3e4a0799e4585a94510d665f26) - fix cc build. Thanks [@pngwn](https://github.com/pngwn)! ### Fixes\\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Pending events behavior. Thanks [@pngwn](https://github.com/pngwn)! - [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Reinstate types that were removed in error in #5832. Thanks [@pngwn](https://github.com/pngwn)! - [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Fixes: slider bar are too thin on FireFox. Thanks [@pngwn](https://github.com/pngwn)! - [#6146](https://github.com/gradio-app/gradio/pull/6146) [`40a171ea6`](https://github.com/gradio-app/gradio/commit/40a171ea60c74afa9519d6cb159def16ce68e1ca) - Fix image double change bug. Thanks [@pngwn](https://github.com/pngwn)! - [#6148](https://github.com/gradio-app/gradio/pull/6148) [`0000a1916`](https://github.com/gradio-app/gradio/commit/0000a191688c5480c977c80acdd0c9023865d57e) - fix dropdown arrow size. Thanks [@pngwn](https://github.com/pngwn)! - [#6067](https://github.com/gradio-app/gradio/pull/6067) [`bf38e5f06`](https://github.com/gradio-app/gradio/commit/bf38e5f06a7039be913614901c308794fea83ae0) - remove dupe component. Thanks [@pngwn](https://github.com/pngwn)! - [#6065](https://github.com/gradio-app/gradio/pull/6065) [`7d07001e8`](https://github.com/gradio-app/gradio/commit/7d07001e8e7ca9cbd2251632667b3a043de49f49) - fix storybook. Thanks [@pngwn](https://github.com/pngwn)! - [#5826](https://github.com/gradio-app/gradio/pull/5826) [`ce036c5d4`](https://github.com/gradio-app/gradio/commit/ce036c5d47e741e29812654bcc641ea6be876504) - Pending events behavior. Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)! - [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)! - [#6042](https://github.com/gradio-app/gradio/pull/6042) [`e27997fe6`](https://github.com/gradio-app/gradio/commit/e27997fe6c2bcfebc7015fc26100cee9625eb13a) - Fix `root` when user is unauthenticated so that login page appears correctly. Thanks [@abidlabs](https://github.com/abidlabs)! - [#6076](https://github.com/gradio-app/gradio/pull/6076) [`f3f98f923`](https://github.com/gradio-app/gradio/commit/f3f98f923c9db506284b8440e18a3ac7ddd8398b) - Lite error handler. Thanks [@whitphx](https://github.com/whitphx)! - [#5984](https://github.com/gradio-app/gradio/pull/5984) [`66549d8d2`](https://github.com/gradio-app/gradio/commit/66549d8d256b1845c8c5efa0384695b36cb46eab) - Fixes: slider bar are too thin on FireFox. Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)! ## 3.45.0-beta.13\\n### Features\\n- [#5964](https://github.com/gradio-app/gradio/pull/5964) [`5fbda0bd2`](https://github.com/gradio-app/gradio/commit/5fbda0bd2b2bbb2282249b8875d54acf87cd7e84) - Wasm release. Thanks [@pngwn](https://github.com/pngwn)! ## 3.45.0-beta.12\\n### Features\\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-app/gradio/commit/85ba6de136a45b3e92c74e410bb27e3cbe7138d7) - V4: Some misc fixes.\", 'type': 'tool', 'name': 'data_retriever', 'id': '382e2b6d-7a84-414e-b678-3df080949ad9', 'tool_call_id': 'call_yUoySy1uTNT0yoBzVlFpM0x0', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab1bf3e684812d15f66cb', 'page_content': \"Test Strategy\\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to. This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Objectives\\nThe purposes of all testing activities on Gradio fit one of the following objectives:\\n1. Ensure that the Gradio library functions as we expect it to. 2. Enable the maintenance team to quickly identify both the presence and source of defects. 3. Prevent regressions, i.e. if we fix something it should stay fixed. 4. Improve the quality of the codebase in order to ease maintenance efforts. 5. Reduce the amount of manual testing required. ## Scope\\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas. We should not focus on code coverage but on test coverage following the below criteria:\\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\\n## Types of testing\\nOur tests will broadly fall into one of three categories:\\n- Static Quality checks\\n- Dynamic 'Code' tests\\n- Dynamic Functional tests\\n### Static Quality checks\\nStatic quality checks are generally very fast to run and do not require building the code base.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab1c03e684812d15f66cd', 'page_content': \"Tests in this category could be browser-based end-to-end tests, accessibility tests, or performance tests. They are sometimes called acceptance tests. ## Testing tools\\nWe currently use the following tools:\\n### Static quality checks\\n- Python type-checking (python)\\n- Black linting (python)\\n- ruff formatting (python)\\n- prettier formatting (javascript/svelte)\\n- TypeScript type-checking (javascript/svelte)\\n- eslint linting (javascript/svelte) [in progress]\\n### Dynamic code tests\\n- pytest (python unit and integration tests)\\n- vitest (node-based unit and integration tests)\\n- playwright (browser-based unit and integration tests)\\n### Functional/acceptance tests\\n- playwright (full end to end testing)\\n- chromatic (visual testing) [in progress]\\n- Accessibility testing [to do]\\n## Supported environments and versions\\nAll operating systems refer to the current runner variants supported by GitHub actions. All unspecified version segments (`x`) refer to latest. | Software | Version(s) | Operating System(s) |\\n| -------- | --------------------- | --------------------------------- |\\n| Python | `3.8.x` | `ubuntu-latest`, `windows-latest` |\\n| Node | `18.x.x` | `ubuntu-latest` |\\n| Browser | `playwright-chrome-x` | `ubuntu-latest` |\\n## Test execution\\nTests need to be executed in a number of environments and at different stages of the development cycle in order to be useful. The requirements for tests are as follows:\\n- \\\\*\\\\*Locally\\\\*\\\\*: it is important that developers can easily run most tests locally to ensure a passing suite before making a PR. There are some exceptions to this, certain tests may require access to secret values which we cannot make available to all possible contributors for practical security reasons. It is reasonable that it isn't possible to run these tests but they should be disabled by default when running locally. - \\\\*\\\\*CI\\\\*\\\\* - It is \\\\_critical\\\\_ that all tests run successfully in CI with no exceptions. Not every test is required to pass to satisfy CI checks for practical reasons but it is required that all tests should run in CI and notify us if something unexpected happens in order for the development team to take appropriate action. For instructions on how to write and run tests see the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Managing defects\\nAs we formalise our testing strategy and bring / keep our test up to standard, it is important that we have some principles on managing defects as they occur/ are reported. For now we can have one very simple rule:\\n- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should \\\\_typically\\\\_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab6cb3e684812d15f6cc1', 'page_content': 'Thanks [@abidlabs](https://github.com/abidlabs)! - [#6079](https://github.com/gradio-app/gradio/pull/6079) [`3b2d9eaa3`](https://github.com/gradio-app/gradio/commit/3b2d9eaa3e84de3e4a0799e4585a94510d665f26) - fix cc build. Thanks [@pngwn](https://github.com/pngwn)! ### Fixes\\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Pending events behavior. Thanks [@pngwn](https://github.com/pngwn)! - [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Reinstate types that were removed in error in #5832. Thanks [@pngwn](https://github.com/pngwn)! - [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Fixes: slider bar are too thin on FireFox. Thanks [@pngwn](https://github.com/pngwn)! - [#6146](https://github.com/gradio-app/gradio/pull/6146) [`40a171ea6`](https://github.com/gradio-app/gradio/commit/40a171ea60c74afa9519d6cb159def16ce68e1ca) - Fix image double change bug. Thanks [@pngwn](https://github.com/pngwn)! - [#6148](https://github.com/gradio-app/gradio/pull/6148) [`0000a1916`](https://github.com/gradio-app/gradio/commit/0000a191688c5480c977c80acdd0c9023865d57e) - fix dropdown arrow size. Thanks [@pngwn](https://github.com/pngwn)! - [#6067](https://github.com/gradio-app/gradio/pull/6067) [`bf38e5f06`](https://github.com/gradio-app/gradio/commit/bf38e5f06a7039be913614901c308794fea83ae0) - remove dupe component. Thanks [@pngwn](https://github.com/pngwn)! - [#6065](https://github.com/gradio-app/gradio/pull/6065) [`7d07001e8`](https://github.com/gradio-app/gradio/commit/7d07001e8e7ca9cbd2251632667b3a043de49f49) - fix storybook. Thanks [@pngwn](https://github.com/pngwn)! - [#5826](https://github.com/gradio-app/gradio/pull/5826) [`ce036c5d4`](https://github.com/gradio-app/gradio/commit/ce036c5d47e741e29812654bcc641ea6be876504) - Pending events behavior. Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)! - [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)! - [#6042](https://github.com/gradio-app/gradio/pull/6042) [`e27997fe6`](https://github.com/gradio-app/gradio/commit/e27997fe6c2bcfebc7015fc26100cee9625eb13a) - Fix `root` when user is unauthenticated so that login page appears correctly. Thanks [@abidlabs](https://github.com/abidlabs)! - [#6076](https://github.com/gradio-app/gradio/pull/6076) [`f3f98f923`](https://github.com/gradio-app/gradio/commit/f3f98f923c9db506284b8440e18a3ac7ddd8398b) - Lite error handler. Thanks [@whitphx](https://github.com/whitphx)! - [#5984](https://github.com/gradio-app/gradio/pull/5984) [`66549d8d2`](https://github.com/gradio-app/gradio/commit/66549d8d256b1845c8c5efa0384695b36cb46eab) - Fixes: slider bar are too thin on FireFox. Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)! ## 3.45.0-beta.13\\n### Features\\n- [#5964](https://github.com/gradio-app/gradio/pull/5964) [`5fbda0bd2`](https://github.com/gradio-app/gradio/commit/5fbda0bd2b2bbb2282249b8875d54acf87cd7e84) - Wasm release. Thanks [@pngwn](https://github.com/pngwn)! ## 3.45.0-beta.12\\n### Features\\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-app/gradio/commit/85ba6de136a45b3e92c74e410bb27e3cbe7138d7) - V4: Some misc fixes.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"Every bug fix in Gradio's testing strategy should typically be accompanied by a dynamic code test that failed before the fix and passes afterwards. This test could also be a linting rule or a new type if that is appropriate.\", 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--fc0e1132-5338-40ce-8cf5-819cd0763270-0', 'usage_metadata': {'input_tokens': 2681, 'output_tokens': 45, 'total_tokens': 2726, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:38:20,399 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 68%|██████▊   | 44/65 [03:14<01:37,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?\\n', 'type': 'human', 'id': '15b10391-0f8b-4370-b67a-fb178206e66c'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_0id4PsQgxgNBsk83tkrSRcTa', 'function': {'arguments': '{\"query\":\"force mixed precision training when initializing the Accelerator in 🤗 Accelerate\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--3d29a385-56d1-4e87-8972-4e57ca8a0b7e-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'force mixed precision training when initializing the Accelerator in 🤗 Accelerate'}, 'id': 'call_0id4PsQgxgNBsk83tkrSRcTa', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 28, 'total_tokens': 393, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of 🤗 Accelerate is:\\n- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible. - to have the same functions work for any distributed setup, so only have to learn one API. ### How does it work? To see how the library works in practice, let\\'s have a look at each line of code we need to add to a training loop. ```python\\naccelerator = Accelerator()\\n```\\nOn top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script. ```python\\nmodel, optim, data = accelerator.prepare(model, optim, data)\\n```\\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torch.nn.Module`), optimizers (`torch.optim.Optimizer`) and dataloaders (`torch.data.dataloader.DataLoader`). #### Model\\nModel preparation include wrapping it in the proper container (for instance `DistributedDataParallel`) and putting it on the proper device. Like with a regular distributed training, you will need to unwrap your model for saving, or to access its specific methods, which can be done with `accelerator.unwrap\\\\_model(model)`. #### Optimizer\\nThe optimizer is also wrapped in a special container that will perform the necessary operations in the step to make mixed precision work.\\n\\nAlternatively, use 🤗 Accelerate to gain full control over the training loop. Find the 🤗 Accelerate example\\n[further down in this guide](#using-accelerate). While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can\\nresult in a more pronounced training slowdown. Consider the following example. Let\\'s say, the `per\\\\_device\\\\_train\\\\_batch\\\\_size=4`\\nwithout gradient accumulation hits the GPU\\'s limit. If you would like to train with batches of size 64, do not set the\\n`per\\\\_device\\\\_train\\\\_batch\\\\_size` to 1 and `gradient\\\\_accumulation\\\\_steps` to 64. Instead, keep `per\\\\_device\\\\_train\\\\_batch\\\\_size=4`\\nand set `gradient\\\\_accumulation\\\\_steps=16`. This results in the same effective batch size while making better use of\\nthe available GPU resources. For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\\nand [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957). ## Gradient Checkpointing\\nSome large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used.\\n\\n# Initialize accelerator and tensorboard logging\\n... accelerator = Accelerator(\\n... mixed\\\\_precision=config.mixed\\\\_precision,\\n... gradient\\\\_accumulation\\\\_steps=config.gradient\\\\_accumulation\\\\_steps,\\n... log\\\\_with=\"tensorboard\",\\n... project\\\\_dir=os.path.join(config.output\\\\_dir, \"logs\"),\\n... )\\n... if accelerator.is\\\\_main\\\\_process:\\n... if config.output\\\\_dir is not None:\\n... os.makedirs(config.output\\\\_dir, exist\\\\_ok=True)\\n... if config.push\\\\_to\\\\_hub:\\n... repo\\\\_id = create\\\\_repo(\\n... repo\\\\_id=config.hub\\\\_model\\\\_id or Path(config.output\\\\_dir).name, exist\\\\_ok=True\\n... ).repo\\\\_id\\n... accelerator.init\\\\_trackers(\"train\\\\_example\")\\n... # Prepare everything\\n... # There is no specific order to remember, you just need to unpack the\\n... # objects in the same order you gave them to the prepare method. ... model, optimizer, train\\\\_dataloader, lr\\\\_scheduler = accelerator.prepare(\\n... model, optimizer, train\\\\_dataloader, lr\\\\_scheduler\\n... )\\n... global\\\\_step = 0\\n... # Now you train the model\\n... for epoch in range(config.num\\\\_epochs):\\n... progress\\\\_bar = tqdm(total=len(train\\\\_dataloader), disable=not accelerator.is\\\\_local\\\\_main\\\\_process)\\n... progress\\\\_bar.set\\\\_description(f\"Epoch {epoch}\")\\n...', 'type': 'tool', 'name': 'data_retriever', 'id': '7182d5df-7cd5-4ceb-b97c-1d61c0bce0d3', 'tool_call_id': 'call_0id4PsQgxgNBsk83tkrSRcTa', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa69e3e684812d15f59cd', 'page_content': \"Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of 🤗 Accelerate is:\\n- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible. - to have the same functions work for any distributed setup, so only have to learn one API. ### How does it work? To see how the library works in practice, let's have a look at each line of code we need to add to a training loop. ```python\\naccelerator = Accelerator()\\n```\\nOn top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script. ```python\\nmodel, optim, data = accelerator.prepare(model, optim, data)\\n```\\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torch.nn.Module`), optimizers (`torch.optim.Optimizer`) and dataloaders (`torch.data.dataloader.DataLoader`). #### Model\\nModel preparation include wrapping it in the proper container (for instance `DistributedDataParallel`) and putting it on the proper device. Like with a regular distributed training, you will need to unwrap your model for saving, or to access its specific methods, which can be done with `accelerator.unwrap\\\\_model(model)`. #### Optimizer\\nThe optimizer is also wrapped in a special container that will perform the necessary operations in the step to make mixed precision work.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b90faf3d2ebb270a070a', 'page_content': \"Alternatively, use 🤗 Accelerate to gain full control over the training loop. Find the 🤗 Accelerate example\\n[further down in this guide](#using-accelerate). While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can\\nresult in a more pronounced training slowdown. Consider the following example. Let's say, the `per\\\\_device\\\\_train\\\\_batch\\\\_size=4`\\nwithout gradient accumulation hits the GPU's limit. If you would like to train with batches of size 64, do not set the\\n`per\\\\_device\\\\_train\\\\_batch\\\\_size` to 1 and `gradient\\\\_accumulation\\\\_steps` to 64. Instead, keep `per\\\\_device\\\\_train\\\\_batch\\\\_size=4`\\nand set `gradient\\\\_accumulation\\\\_steps=16`. This results in the same effective batch size while making better use of\\nthe available GPU resources. For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\\nand [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957). ## Gradient Checkpointing\\nSome large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899ba6faf3d2ebb270a08cc', 'page_content': '# Initialize accelerator and tensorboard logging\\n... accelerator = Accelerator(\\n... mixed\\\\_precision=config.mixed\\\\_precision,\\n... gradient\\\\_accumulation\\\\_steps=config.gradient\\\\_accumulation\\\\_steps,\\n... log\\\\_with=\"tensorboard\",\\n... project\\\\_dir=os.path.join(config.output\\\\_dir, \"logs\"),\\n... )\\n... if accelerator.is\\\\_main\\\\_process:\\n... if config.output\\\\_dir is not None:\\n... os.makedirs(config.output\\\\_dir, exist\\\\_ok=True)\\n... if config.push\\\\_to\\\\_hub:\\n... repo\\\\_id = create\\\\_repo(\\n... repo\\\\_id=config.hub\\\\_model\\\\_id or Path(config.output\\\\_dir).name, exist\\\\_ok=True\\n... ).repo\\\\_id\\n... accelerator.init\\\\_trackers(\"train\\\\_example\")\\n... # Prepare everything\\n... # There is no specific order to remember, you just need to unpack the\\n... # objects in the same order you gave them to the prepare method. ... model, optimizer, train\\\\_dataloader, lr\\\\_scheduler = accelerator.prepare(\\n... model, optimizer, train\\\\_dataloader, lr\\\\_scheduler\\n... )\\n... global\\\\_step = 0\\n... # Now you train the model\\n... for epoch in range(config.num\\\\_epochs):\\n... progress\\\\_bar = tqdm(total=len(train\\\\_dataloader), disable=not accelerator.is\\\\_local\\\\_main\\\\_process)\\n... progress\\\\_bar.set\\\\_description(f\"Epoch {epoch}\")\\n...', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'You can force mixed precision training when initializing the Accelerator in 🤗 Accelerate by passing the argument `fp16=True` to the `Accelerator()` initialization. Here is an example:\\n\\n```python\\naccelerator = Accelerator(fp16=True)\\n```', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--09c1c0f4-ec20-4d84-b289-6ae8dfa09f4a-0', 'usage_metadata': {'input_tokens': 1278, 'output_tokens': 49, 'total_tokens': 1327, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:38:25,839 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 69%|██████▉   | 45/65 [03:20<01:37,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of tokenizers in the NLP pipeline?\\n', 'type': 'human', 'id': '9b02f93b-3c31-4261-838d-8cbfd926d475'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_JMXvz6U4M6HCl45huRbCfZlZ', 'function': {'arguments': '{\"query\":\"purpose of tokenizers in NLP pipeline\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--db02b566-e55a-4605-94e6-703c19b36946-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'purpose of tokenizers in NLP pipeline'}, 'id': 'call_JMXvz6U4M6HCl45huRbCfZlZ', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 360, 'output_tokens': 22, 'total_tokens': 382, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'p align=\"center\">\\n\\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\\n\\n![Build](https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg)\\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\\n[![Doc](https://docs.rs/tokenizers/badge.svg)](https://docs.rs/tokenizers/)\\n\\nThe core of `tokenizers`, written in Rust. Provides an implementation of today\\'s most used tokenizers, with a focus on performance and\\nversatility. ## What is a Tokenizer\\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`. The various steps of the pipeline are:\\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\\nthe [unicode normalization standards](https://unicode.org/reports/tr15/#Norm\\\\_Forms), such as `NFD` or `NFKC`. More details about how to use the `Normalizers` are available on the\\n[Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\\nsplitting text is simply on whitespace. 3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\\n`BPE` or `WordPiece`.\\n\\nFrameworkSwitchCourse {fw} />\\n# Tokenizers[[tokenizers]]\\n{#if fw === \\'pt\\'}\\n{:else}\\n{/if}\\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we\\'ll explore exactly what happens in the tokenization pipeline. In NLP tasks, the data that is generally processed is raw text. Here\\'s an example of such text:\\n```\\nJim Henson was a puppeteer\\n```\\nHowever, models can only process numbers, so we need to find a way to convert the raw text to numbers. That\\'s what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation — that is, the one that makes the most sense to the model — and, if possible, the smallest representation. Let\\'s take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization. ## Word-based[[word-based]]\\nThe first type of tokenizer that comes to mind is \\\\_word-based\\\\_. It\\'s generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:\\n\\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg)\\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg)\\n\\nThere are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python\\'s `split()` function:\\n```py\\ntokenized\\\\_text = \"Jim Henson was a puppeteer\".split()\\nprint(tokenized\\\\_text)\\n```\\n```python out\\n[\\'Jim\\', \\'Henson\\', \\'was\\', \\'a\\', \\'puppeteer\\']\\n```\\nThere are also variations of word tokenizers that have extra rules for punctuation.\\n\\nn these few videos, we\\'ll take a look at the tokenizers. In Natural Language Processing, most of the data that we handle consists of raw text. However, machine learning models cannot read and understand text in its raw form they can only work with numbers. The tokenizer\\'s objective will be to translate the text into numbers. There are several possible approaches to this conversion, and the objective is to find the most meaningful representation. We\\'ll take a look at three distinct tokenization algorithms. We compare them one to one, so we recommend you look at the videos in the following order: Word-based, Character-based, and Subword-based.', 'type': 'tool', 'name': 'data_retriever', 'id': 'a55f7000-a3d4-4af6-a49e-66572a4b4072', 'tool_call_id': 'call_JMXvz6U4M6HCl45huRbCfZlZ', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bb92af3d2ebb270a0a21', 'page_content': 'p align=\"center\">\\n\\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\\n\\n![Build](https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg)\\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\\n[![Doc](https://docs.rs/tokenizers/badge.svg)](https://docs.rs/tokenizers/)\\n\\nThe core of `tokenizers`, written in Rust. Provides an implementation of today\\'s most used tokenizers, with a focus on performance and\\nversatility. ## What is a Tokenizer\\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`. The various steps of the pipeline are:\\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\\nthe [unicode normalization standards](https://unicode.org/reports/tr15/#Norm\\\\_Forms), such as `NFD` or `NFKC`. More details about how to use the `Normalizers` are available on the\\n[Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\\nsplitting text is simply on whitespace. 3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\\n`BPE` or `WordPiece`.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab8b53e684812d15f6f00', 'page_content': 'FrameworkSwitchCourse {fw} />\\n# Tokenizers[[tokenizers]]\\n{#if fw === \\'pt\\'}\\n{:else}\\n{/if}\\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we\\'ll explore exactly what happens in the tokenization pipeline. In NLP tasks, the data that is generally processed is raw text. Here\\'s an example of such text:\\n```\\nJim Henson was a puppeteer\\n```\\nHowever, models can only process numbers, so we need to find a way to convert the raw text to numbers. That\\'s what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation — that is, the one that makes the most sense to the model — and, if possible, the smallest representation. Let\\'s take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization. ## Word-based[[word-based]]\\nThe first type of tokenizer that comes to mind is \\\\_word-based\\\\_. It\\'s generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:\\n\\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg)\\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg)\\n\\nThere are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python\\'s `split()` function:\\n```py\\ntokenized\\\\_text = \"Jim Henson was a puppeteer\".split()\\nprint(tokenized\\\\_text)\\n```\\n```python out\\n[\\'Jim\\', \\'Henson\\', \\'was\\', \\'a\\', \\'puppeteer\\']\\n```\\nThere are also variations of word tokenizers that have extra rules for punctuation.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b50aaf3d2ebb270a0253', 'page_content': \"n these few videos, we'll take a look at the tokenizers. In Natural Language Processing, most of the data that we handle consists of raw text. However, machine learning models cannot read and understand text in its raw form they can only work with numbers. The tokenizer's objective will be to translate the text into numbers. There are several possible approaches to this conversion, and the objective is to find the most meaningful representation. We'll take a look at three distinct tokenization algorithms. We compare them one to one, so we recommend you look at the videos in the following order: Word-based, Character-based, and Subword-based.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The purpose of tokenizers in the NLP pipeline is to translate text into data that can be processed by the model. Since models can only process numbers, tokenizers convert raw text inputs into numerical data. This conversion is essential for enabling models to understand and work with the text, as they require a numerical representation of the input data. Tokenizers aim to find the most meaningful representation of the text, ideally the smallest representation that makes sense to the model.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--de3c4bbf-d638-454e-8694-d20e773d2a16-0', 'usage_metadata': {'input_tokens': 1210, 'output_tokens': 90, 'total_tokens': 1300, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:38:31,931 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 71%|███████   | 46/65 [03:26<01:39,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of the Safety Checker in the Diffusers library?\\n', 'type': 'human', 'id': 'fb0569c5-c9da-45cb-aab2-96e3374409d9'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_d5QpjiA3L6OFKvA4TJ4a3uGo', 'function': {'arguments': '{\"query\":\"Safety Checker Diffusers library\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--7cc91530-2576-418b-8e03-78ebcb72db49-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Safety Checker Diffusers library'}, 'id': 'call_d5QpjiA3L6OFKvA4TJ4a3uGo', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 362, 'output_tokens': 20, 'total_tokens': 382, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'We will keep tracking risks and adapt the following guidelines based on the community\\'s responsiveness and valuable feedback. ## Scope\\nThe Diffusers community will apply the following ethical guidelines to the project’s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. ## Ethical guidelines\\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - \\\\*\\\\*Transparency\\\\*\\\\*: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions. - \\\\*\\\\*Consistency\\\\*\\\\*: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - \\\\*\\\\*Simplicity\\\\*\\\\*: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project’s goals lean and coherent. - \\\\*\\\\*Accessibility\\\\*\\\\*: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community. - \\\\*\\\\*Reproducibility\\\\*\\\\*: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library. - \\\\*\\\\*Responsibility\\\\*\\\\*: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology\\'s potential risks and dangers. ## Examples of implementations: Safety features and Mechanisms\\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# 🧨 Diffusers’ Ethical Guidelines\\n## Preamble\\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training. Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users’ contributions, and usage of the Diffusers library. The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\\n\\nThis forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\\nis very simple thanks to Diffusers\\' ability to separate single components of the diffusion pipeline. ## Tweakable, contributor-friendly over abstraction\\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don\\'t repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t\\\\_repeat\\\\_yourself). In short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers. Functions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable. \\\\*\\\\*However\\\\*\\\\*, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions. - Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions. - Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel. At Hugging Face, we call this design the \\\\*\\\\*single-file policy\\\\*\\\\* which means that almost all of the code of a certain class should be written in a single, self-contained file.', 'type': 'tool', 'name': 'data_retriever', 'id': '6be950fb-581d-4684-a761-c0104fe7a8d5', 'tool_call_id': 'call_d5QpjiA3L6OFKvA4TJ4a3uGo', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bb05af3d2ebb270a097f', 'page_content': \"We will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback. ## Scope\\nThe Diffusers community will apply the following ethical guidelines to the project’s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. ## Ethical guidelines\\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - \\\\*\\\\*Transparency\\\\*\\\\*: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions. - \\\\*\\\\*Consistency\\\\*\\\\*: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - \\\\*\\\\*Simplicity\\\\*\\\\*: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project’s goals lean and coherent. - \\\\*\\\\*Accessibility\\\\*\\\\*: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community. - \\\\*\\\\*Reproducibility\\\\*\\\\*: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library. - \\\\*\\\\*Responsibility\\\\*\\\\*: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers. ## Examples of implementations: Safety features and Mechanisms\\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bb05af3d2ebb270a097e', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# 🧨 Diffusers’ Ethical Guidelines\\n## Preamble\\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training. Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users’ contributions, and usage of the Diffusers library. The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa5253e684812d15f580a', 'page_content': \"This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\\nis very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline. ## Tweakable, contributor-friendly over abstraction\\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t\\\\_repeat\\\\_yourself). In short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers. Functions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable. \\\\*\\\\*However\\\\*\\\\*, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions. - Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions. - Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel. At Hugging Face, we call this design the \\\\*\\\\*single-file policy\\\\*\\\\* which means that almost all of the code of a certain class should be written in a single, self-contained file.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"The Safety Checker in the Diffusers library is designed to help mitigate potential ethical and social risks associated with diffusion technology. It serves as a tool to ensure that the generated content adheres to safety standards, preventing the creation of harmful or inappropriate outputs. This aligns with the ethical guidelines of the Diffusers project, which emphasize responsibility and the anticipation of risks related to the technology's use.\", 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f9ee5287-450e-4e34-9724-15b736ce19bb-0', 'usage_metadata': {'input_tokens': 1384, 'output_tokens': 76, 'total_tokens': 1460, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:38:35,994 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 72%|███████▏  | 47/65 [03:30<01:28,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\\n', 'type': 'human', 'id': 'd71a491c-5bec-4359-8f52-b84f817d3e70'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_diq5eXNL2dp2NqejhMbuH1Sq', 'function': {'arguments': '{\"query\":\"Python class retrieve Discussions and Pull Requests from a repository Hugging Face Hub\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--2330a29f-d4ef-4676-b377-11992f1285a6-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Python class retrieve Discussions and Pull Requests from a repository Hugging Face Hub'}, 'id': 'call_diq5eXNL2dp2NqejhMbuH1Sq', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 370, 'output_tokens': 29, 'total_tokens': 399, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Interact with Discussions and Pull Requests\\nThe `huggingface\\\\_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood. ## Retrieve Discussions and Pull Requests from the Hub\\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\\n```python\\n>>> from huggingface\\\\_hub import get\\\\_repo\\\\_discussions\\n>>> for discussion in get\\\\_repo\\\\_discussions(repo\\\\_id=\"bigscience/bloom\"):\\n... print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is\\\\_pull\\\\_request}\")\\n# 11 - Add Flax weights, pr: True\\n# 10 - Update README.md, pr: True\\n# 9 - Training languages in the model card, pr: True\\n# 8 - Update tokenizer\\\\_config.json, pr: True\\n# 7 - Slurm training script, pr: False\\n[...]\\n```\\n`HfApi.get\\\\_repo\\\\_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\\n```python\\n>>> from huggingface\\\\_hub import get\\\\_repo\\\\_discussions\\n>>> for discussion in get\\\\_repo\\\\_discussions(\\n...\\n\\n-->\\n# Git 与 HTTP 范式\\n`huggingface\\\\_hub`库是用于与Hugging Face Hub进行交互的库，Hugging Face Hub是一组基于Git的存储库（模型、数据集或Spaces）。使用 `huggingface\\\\_hub`有两种主要方式来访问Hub。\\n第一种方法，即所谓的“基于git”的方法，由[`Repository`]类驱动。这种方法使用了一个包装器，它在 `git`命令的基础上增加了专门与Hub交互的额外函数。第二种选择，称为“基于HTTP”的方法，涉及使用[`HfApi`]客户端进行HTTP请求。让我们来看一看每种方法的优缺点。\\n## 存储库：基于历史的 Git 方法\\n最初，`huggingface\\\\_hub`主要围绕 [`Repository`] 类构建。它为常见的 `git` 命令（如 `\"git add\"`、`\"git commit\"`、`\"git push\"`、`\"git tag\"`、`\"git checkout\"` 等）提供了 Python 包装器\\n该库还可以帮助设置凭据和跟踪大型文件，这些文件通常在机器学习存储库中使用。此外，该库允许您在后台执行其方法，使其在训练期间上传数据很有用。\\n使用 [`Repository`] 的最大优点是它允许你在本地机器上维护整个存储库的本地副本。这也可能是一个缺点，因为它需要你不断更新和维护这个本地副本。这类似于传统软件开发中，每个开发人员都维护自己的本地副本，并在开发功能时推送更改。但是，在机器学习的上下文中，这可能并不总是必要的，因为用户可能只需要下载推理所需的权重，或将权重从一种格式转换为另一种格式，而无需克隆整个存储库。\\n## HfApi: 一个功能强大且方便的HTTP客户端\\n`HfApi` 被开发为本地 git 存储库的替代方案，因为本地 git 存储库在处理大型模型或数据集时可能会很麻烦。`HfApi` 提供与基于 git 的方法相同的功能，例如下载和推送文件以及创建分支和标签，但无需本地文件夹来保持同步。\\n`HfApi`除了提供 `git` 已经提供的功能外，还提供其他功能，例如：\\n\\\\* 管理存储库\\n\\\\* 使用缓存下载文件以进行有效的重复使用\\n\\\\* 在 Hub 中搜索存储库和元数据\\n\\\\* 访问社区功能，如讨论、PR和评论\\n\\\\* 配置Spaces\\n## 我应该使用什么？以及何时使用？\\n总的来说，在大多数情况下，`HTTP 方法`是使用 huggingface\\\\_hub 的推荐方法。但是，在以下几种情况下，维护本地 git 克隆（使用 `Repository`）可能更有益：\\n如果您在本地机器上训练模型，使用传统的 git 工作流程并定期推送更新可能更有效。`Repository` 被优化为此类情况，因为它能够在后台运行。\\n如果您需要手动编辑大型文件，`git `是最佳选择，因为它只会将文件的差异发送到服务器。使用 `HfAPI` 客户端，每次编辑都会上传整个文件。请记住，大多数大型文件是二进制文件，因此无法从 git 差异中受益。\\n并非所有 git 命令都通过 [`HfApi`] 提供。有些可能永远不会被实现，但我们一直在努力改进并缩小差距。如果您没有看到您的用例被覆盖。\\n请在[Github](https://github.com/huggingface/huggingface\\\\_hub)打开一个 issue！我们欢迎反馈，以帮助我们与我们的用户一起构建 🤗 生态系统。\\n\\nPull requests and Discussions\\nHub Pull requests and Discussions allow users to do community contributions to repositories. Pull requests and discussions work the same for all the repo types. At a high level, the aim is to build a simpler version of other git hosts\\' (like GitHub\\'s) PRs and Issues:\\n- no forks are involved: contributors push to a special `ref` branch directly on the source repo. - there\\'s no hard distinction between discussions and PRs: they are essentially the same so they are displayed in the same lists. - they are streamlined for ML (i.e. models/datasets/spaces repos), not arbitrary repos. \\\\_Note, Pull Requests and discussions can be enabled or disabled from the [repository settings](./repositories-settings#disabling-discussions-pull-requests)\\\\_\\n## List\\nBy going to the community tab in any repository, you can see all Discussions and Pull requests. You can also filter to only see the ones that are open. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list-dark.png)\\n\\n## View\\nThe Discussion page allows you to see the comments from different users. If it\\'s a Pull Request, you can see all the changes by going to the Files changed tab. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-view.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-view-dark.png)\\n\\n## Editing a Discussion / Pull request title\\nIf you opened a PR or discussion, are the author of the repository, or have write access to it, you can edit the discussion title by clicking on the pencil button. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-edit-title.PNG)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-edit-title-dark.PNG)\\n\\n## Pin a Discussion / Pull Request\\nIf you have write access to a repository, you can pin discussions and Pull Requests. Pinned discussions appear at the top of all the discussions. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pin.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pin-dark.png)\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pinned.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pinned-dark.png)\\n\\n## Lock a Discussion / Pull Request\\nIf you have write access to a repository, you can lock discussions or Pull Requests. Once a discussion is locked, previous comments are still visible and users won\\'t be able to add new comments. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-lock.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-lock-dark.png)\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-locked.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-locked-dark.png)\\n\\n## Comment edition and moderation\\nIf you wrote a comment or have write access to the repository, you can edit the content of the comment from the contextual menu in the top-right corner of the comment box. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-dark.png)\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-edit.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-edit-dark.png)\\n\\nOnce the comment has been edited, a new link will appear above the comment. This link shows the edit history. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-link.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-link-dark.png)\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-history.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-history-dark.png)\\n\\nYou can also hide a comment. Hiding a comment is irreversible, and nobody will be able to see its content nor edit it anymore. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-hidden.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-hidden-dark.png)\\n\\nRead also [moderation](./moderation) to see how to report an abusive comment. ## Can I use Markdown and LaTeX in my comments and discussions? Yes! You can use Markdown to add formatting to your comments. Additionally, you can utilize LaTeX for mathematical typesetting, your formulas will be rendered with [KaTeX](https://katex.org/) before being parsed in Markdown.', 'type': 'tool', 'name': 'data_retriever', 'id': '6769d019-7c7e-4646-b991-1e9f52c6e751', 'tool_call_id': 'call_diq5eXNL2dp2NqejhMbuH1Sq', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aae903e684812d15f6354', 'page_content': '!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Interact with Discussions and Pull Requests\\nThe `huggingface\\\\_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood. ## Retrieve Discussions and Pull Requests from the Hub\\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\\n```python\\n>>> from huggingface\\\\_hub import get\\\\_repo\\\\_discussions\\n>>> for discussion in get\\\\_repo\\\\_discussions(repo\\\\_id=\"bigscience/bloom\"):\\n... print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is\\\\_pull\\\\_request}\")\\n# 11 - Add Flax weights, pr: True\\n# 10 - Update README.md, pr: True\\n# 9 - Training languages in the model card, pr: True\\n# 8 - Update tokenizer\\\\_config.json, pr: True\\n# 7 - Slurm training script, pr: False\\n[...]\\n```\\n`HfApi.get\\\\_repo\\\\_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\\n```python\\n>>> from huggingface\\\\_hub import get\\\\_repo\\\\_discussions\\n>>> for discussion in get\\\\_repo\\\\_discussions(\\n...', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab7593e684812d15f6dbc', 'page_content': '-->\\n# Git 与 HTTP 范式\\n`huggingface\\\\_hub`库是用于与Hugging Face Hub进行交互的库，Hugging Face Hub是一组基于Git的存储库（模型、数据集或Spaces）。使用 `huggingface\\\\_hub`有两种主要方式来访问Hub。\\n第一种方法，即所谓的“基于git”的方法，由[`Repository`]类驱动。这种方法使用了一个包装器，它在 `git`命令的基础上增加了专门与Hub交互的额外函数。第二种选择，称为“基于HTTP”的方法，涉及使用[`HfApi`]客户端进行HTTP请求。让我们来看一看每种方法的优缺点。\\n## 存储库：基于历史的 Git 方法\\n最初，`huggingface\\\\_hub`主要围绕 [`Repository`] 类构建。它为常见的 `git` 命令（如 `\"git add\"`、`\"git commit\"`、`\"git push\"`、`\"git tag\"`、`\"git checkout\"` 等）提供了 Python 包装器\\n该库还可以帮助设置凭据和跟踪大型文件，这些文件通常在机器学习存储库中使用。此外，该库允许您在后台执行其方法，使其在训练期间上传数据很有用。\\n使用 [`Repository`] 的最大优点是它允许你在本地机器上维护整个存储库的本地副本。这也可能是一个缺点，因为它需要你不断更新和维护这个本地副本。这类似于传统软件开发中，每个开发人员都维护自己的本地副本，并在开发功能时推送更改。但是，在机器学习的上下文中，这可能并不总是必要的，因为用户可能只需要下载推理所需的权重，或将权重从一种格式转换为另一种格式，而无需克隆整个存储库。\\n## HfApi: 一个功能强大且方便的HTTP客户端\\n`HfApi` 被开发为本地 git 存储库的替代方案，因为本地 git 存储库在处理大型模型或数据集时可能会很麻烦。`HfApi` 提供与基于 git 的方法相同的功能，例如下载和推送文件以及创建分支和标签，但无需本地文件夹来保持同步。\\n`HfApi`除了提供 `git` 已经提供的功能外，还提供其他功能，例如：\\n\\\\* 管理存储库\\n\\\\* 使用缓存下载文件以进行有效的重复使用\\n\\\\* 在 Hub 中搜索存储库和元数据\\n\\\\* 访问社区功能，如讨论、PR和评论\\n\\\\* 配置Spaces\\n## 我应该使用什么？以及何时使用？\\n总的来说，在大多数情况下，`HTTP 方法`是使用 huggingface\\\\_hub 的推荐方法。但是，在以下几种情况下，维护本地 git 克隆（使用 `Repository`）可能更有益：\\n如果您在本地机器上训练模型，使用传统的 git 工作流程并定期推送更新可能更有效。`Repository` 被优化为此类情况，因为它能够在后台运行。\\n如果您需要手动编辑大型文件，`git `是最佳选择，因为它只会将文件的差异发送到服务器。使用 `HfAPI` 客户端，每次编辑都会上传整个文件。请记住，大多数大型文件是二进制文件，因此无法从 git 差异中受益。\\n并非所有 git 命令都通过 [`HfApi`] 提供。有些可能永远不会被实现，但我们一直在努力改进并缩小差距。如果您没有看到您的用例被覆盖。\\n请在[Github](https://github.com/huggingface/huggingface\\\\_hub)打开一个 issue！我们欢迎反馈，以帮助我们与我们的用户一起构建 🤗 生态系统。', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c2aa666af48523f521de', 'page_content': \"Pull requests and Discussions\\nHub Pull requests and Discussions allow users to do community contributions to repositories. Pull requests and discussions work the same for all the repo types. At a high level, the aim is to build a simpler version of other git hosts' (like GitHub's) PRs and Issues:\\n- no forks are involved: contributors push to a special `ref` branch directly on the source repo. - there's no hard distinction between discussions and PRs: they are essentially the same so they are displayed in the same lists. - they are streamlined for ML (i.e. models/datasets/spaces repos), not arbitrary repos. \\\\_Note, Pull Requests and discussions can be enabled or disabled from the [repository settings](./repositories-settings#disabling-discussions-pull-requests)\\\\_\\n## List\\nBy going to the community tab in any repository, you can see all Discussions and Pull requests. You can also filter to only see the ones that are open. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list-dark.png)\\n\\n## View\\nThe Discussion page allows you to see the comments from different users. If it's a Pull Request, you can see all the changes by going to the Files changed tab. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-view.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-view-dark.png)\\n\\n## Editing a Discussion / Pull request title\\nIf you opened a PR or discussion, are the author of the repository, or have write access to it, you can edit the discussion title by clicking on the pencil button. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-edit-title.PNG)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-edit-title-dark.PNG)\\n\\n## Pin a Discussion / Pull Request\\nIf you have write access to a repository, you can pin discussions and Pull Requests. Pinned discussions appear at the top of all the discussions. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pin.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pin-dark.png)\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pinned.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pinned-dark.png)\\n\\n## Lock a Discussion / Pull Request\\nIf you have write access to a repository, you can lock discussions or Pull Requests. Once a discussion is locked, previous comments are still visible and users won't be able to add new comments. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-lock.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-lock-dark.png)\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-locked.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-locked-dark.png)\\n\\n## Comment edition and moderation\\nIf you wrote a comment or have write access to the repository, you can edit the content of the comment from the contextual menu in the top-right corner of the comment box. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-dark.png)\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-edit.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-edit-dark.png)\\n\\nOnce the comment has been edited, a new link will appear above the comment. This link shows the edit history. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-link.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-link-dark.png)\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-history.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-history-dark.png)\\n\\nYou can also hide a comment. Hiding a comment is irreversible, and nobody will be able to see its content nor edit it anymore. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-hidden.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-hidden-dark.png)\\n\\nRead also [moderation](./moderation) to see how to report an abusive comment. ## Can I use Markdown and LaTeX in my comments and discussions? Yes! You can use Markdown to add formatting to your comments. Additionally, you can utilize LaTeX for mathematical typesetting, your formulas will be rendered with [KaTeX](https://katex.org/) before being parsed in Markdown.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The `HfApi` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--a9a35d55-74e4-4c3b-8edc-a2cdd9bd6d8e-0', 'usage_metadata': {'input_tokens': 2698, 'output_tokens': 26, 'total_tokens': 2724, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:38:40,100 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 74%|███████▍  | 48/65 [03:34<01:19,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\\n', 'type': 'human', 'id': '57284a94-6089-49ab-a0c1-648d4333b2cd'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_uHzZYgnYLXP5QNSLjOxKe1rs', 'function': {'arguments': '{\"query\":\"new library introduced by Hugging Face for hosting scikit-learn models\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--a10ca4cf-5054-4f23-8e96-320f6082d662-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'new library introduced by Hugging Face for hosting scikit-learn models'}, 'id': 'call_uHzZYgnYLXP5QNSLjOxKe1rs', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 369, 'output_tokens': 29, 'total_tokens': 398, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '--\\ntitle: \"Introducing Skops\"\\nthumbnail: /blog/assets/94\\\\_skops/introducing\\\\_skops.png\\nauthors:\\n- user: merve\\n- user: adrin\\n- user: BenjaminB\\n---\\n# Introducing Skops\\n## Introducing Skops\\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others. Let\\'s go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production. ```python\\n# let\\'s import the libraries first\\nimport sklearn\\nfrom sklearn.datasets import load\\\\_breast\\\\_cancer\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.model\\\\_selection import train\\\\_test\\\\_split\\n# Load the data and split\\nX, y = load\\\\_breast\\\\_cancer(as\\\\_frame=True, return\\\\_X\\\\_y=True)\\nX\\\\_train, X\\\\_test, y\\\\_train, y\\\\_test = train\\\\_test\\\\_split(\\nX, y, test\\\\_size=0.3, random\\\\_state=42\\n)\\n# Train the model\\nmodel = DecisionTreeClassifier().fit(X\\\\_train, y\\\\_train)\\n```\\nYou can use any model filename and serialization method, like `pickle` or `joblib`.', 'type': 'tool', 'name': 'data_retriever', 'id': '4e1c3ecb-a58c-4ab6-96e5-50e356cf10d0', 'tool_call_id': 'call_uHzZYgnYLXP5QNSLjOxKe1rs', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab4483e684812d15f69b0', 'page_content': '--\\ntitle: \"Introducing Skops\"\\nthumbnail: /blog/assets/94\\\\_skops/introducing\\\\_skops.png\\nauthors:\\n- user: merve\\n- user: adrin\\n- user: BenjaminB\\n---\\n# Introducing Skops\\n## Introducing Skops\\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others. Let\\'s go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production. ```python\\n# let\\'s import the libraries first\\nimport sklearn\\nfrom sklearn.datasets import load\\\\_breast\\\\_cancer\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.model\\\\_selection import train\\\\_test\\\\_split\\n# Load the data and split\\nX, y = load\\\\_breast\\\\_cancer(as\\\\_frame=True, return\\\\_X\\\\_y=True)\\nX\\\\_train, X\\\\_test, y\\\\_train, y\\\\_test = train\\\\_test\\\\_split(\\nX, y, test\\\\_size=0.3, random\\\\_state=42\\n)\\n# Train the model\\nmodel = DecisionTreeClassifier().fit(X\\\\_train, y\\\\_train)\\n```\\nYou can use any model filename and serialization method, like `pickle` or `joblib`.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The new library introduced by Hugging Face for hosting scikit-learn models is called Skops.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--1e06ae39-b3d6-47ad-8171-0330c9a9cca6-0', 'usage_metadata': {'input_tokens': 626, 'output_tokens': 20, 'total_tokens': 646, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:38:44,646 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 75%|███████▌  | 49/65 [03:39<01:13,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of Textual Inversion?\\n', 'type': 'human', 'id': 'd4de14b6-25f9-4b00-9d51-108faec5802e'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_DyuAn1c9O9QhhTPp7ZO1IKVp', 'function': {'arguments': '{\"query\":\"Textual Inversion\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--ab2f285a-4f90-43c3-9842-213f7810afd3-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Textual Inversion'}, 'id': 'call_DyuAn1c9O9QhhTPp7ZO1IKVp', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 358, 'output_tokens': 19, 'total_tokens': 377, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '# Textual Inversion fine-tuning example\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\\nColab for training\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\\\_textual\\\\_inversion\\\\_training.ipynb)\\nColab for inference\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\\\_conceptualizer\\\\_inference.ipynb)\\n## Running locally with PyTorch\\n### Installing the dependencies\\nBefore running the scripts, make sure to install the library\\'s training dependencies:\\n\\\\*\\\\*Important\\\\*\\\\*\\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\\\*\\\\*installing from source\\\\*\\\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements.\\n\\n!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Textual Inversion\\nTextual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder. [`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings. To learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading\\\\_adapters#textual-inversion) loading guide. ## TextualInversionLoaderMixin\\n[[autodoc]] loaders.textual\\\\_inversion.TextualInversionLoaderMixin\\n\\n# Textual Inversion\\n[Textual Inversion](https://hf.co/papers/2208.01618) is a training technique for personalizing image generation models with just a few example images of what you want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide. If you\\'re training on a GPU with limited vRAM, you should try enabling the `gradient\\\\_checkpointing` and `mixed\\\\_precision` parameters in the training command. You can also reduce your memory footprint by using memory-efficient attention with [xFormers](../optimization/xformers). JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn\\'t support gradient checkpointing or xFormers. With the same configuration and setup as PyTorch, the Flax training script should be at least ~70% faster! This guide will explore the [textual\\\\_inversion.py](https://github.com/huggingface/diffusers/blob/main/examples/textual\\\\_inversion/textual\\\\_inversion.py) script to help you become more familiar with it, and how you can adapt it for your own use-case. Before running the script, make sure you install the library from source:\\n```bash\\ngit clone https://github.com/huggingface/diffusers\\ncd diffusers\\npip install . ```\\nNavigate to the example folder with the training script and install the required dependencies for the script you\\'re using:\\n\\n```bash\\ncd examples/textual\\\\_inversion\\npip install -r requirements.txt\\n```\\n\\n```bash\\ncd examples/textual\\\\_inversion\\npip install -r requirements\\\\_flax.txt\\n```\\n\\n🤗 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It\\'ll automatically configure your training setup based on your hardware and environment. Take a look at the 🤗 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more. Initialize an 🤗 Accelerate environment:\\n```bash\\naccelerate config\\n```\\nTo setup a default 🤗 Accelerate environment without choosing any configurations:\\n```bash\\naccelerate config default\\n```\\nOr if your environment doesn\\'t support an interactive shell, like a notebook, you can use:\\n```bash\\nfrom accelerate.utils import write\\\\_basic\\\\_config\\nwrite\\\\_basic\\\\_config()\\n```\\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create\\\\_dataset) guide to learn how to create a dataset that works with the training script.', 'type': 'tool', 'name': 'data_retriever', 'id': '4da34a91-48f1-4571-b29a-7bda7d805afd', 'tool_call_id': 'call_DyuAn1c9O9QhhTPp7ZO1IKVp', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3403e684812d15f6873', 'page_content': \"# Textual Inversion fine-tuning example\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\\nColab for training\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\\\_textual\\\\_inversion\\\\_training.ipynb)\\nColab for inference\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\\\_conceptualizer\\\\_inference.ipynb)\\n## Running locally with PyTorch\\n### Installing the dependencies\\nBefore running the scripts, make sure to install the library's training dependencies:\\n\\\\*\\\\*Important\\\\*\\\\*\\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\\\*\\\\*installing from source\\\\*\\\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab2a03e684812d15f67cc', 'page_content': '!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Textual Inversion\\nTextual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder. [`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings. To learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading\\\\_adapters#textual-inversion) loading guide. ## TextualInversionLoaderMixin\\n[[autodoc]] loaders.textual\\\\_inversion.TextualInversionLoaderMixin', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaf9a3e684812d15f6459', 'page_content': \"# Textual Inversion\\n[Textual Inversion](https://hf.co/papers/2208.01618) is a training technique for personalizing image generation models with just a few example images of what you want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide. If you're training on a GPU with limited vRAM, you should try enabling the `gradient\\\\_checkpointing` and `mixed\\\\_precision` parameters in the training command. You can also reduce your memory footprint by using memory-efficient attention with [xFormers](../optimization/xformers). JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn't support gradient checkpointing or xFormers. With the same configuration and setup as PyTorch, the Flax training script should be at least ~70% faster! This guide will explore the [textual\\\\_inversion.py](https://github.com/huggingface/diffusers/blob/main/examples/textual\\\\_inversion/textual\\\\_inversion.py) script to help you become more familiar with it, and how you can adapt it for your own use-case. Before running the script, make sure you install the library from source:\\n```bash\\ngit clone https://github.com/huggingface/diffusers\\ncd diffusers\\npip install . ```\\nNavigate to the example folder with the training script and install the required dependencies for the script you're using:\\n\\n```bash\\ncd examples/textual\\\\_inversion\\npip install -r requirements.txt\\n```\\n\\n```bash\\ncd examples/textual\\\\_inversion\\npip install -r requirements\\\\_flax.txt\\n```\\n\\n🤗 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the 🤗 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more. Initialize an 🤗 Accelerate environment:\\n```bash\\naccelerate config\\n```\\nTo setup a default 🤗 Accelerate environment without choosing any configurations:\\n```bash\\naccelerate config default\\n```\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n```bash\\nfrom accelerate.utils import write\\\\_basic\\\\_config\\nwrite\\\\_basic\\\\_config()\\n```\\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create\\\\_dataset) guide to learn how to create a dataset that works with the training script.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The purpose of Textual Inversion is to personalize image generation models by learning new text embeddings from a few example images. This technique allows the model to generate images that align more closely with the specific characteristics of the provided examples, enabling users to customize the output based on their preferences. The new embeddings are tied to a special word that must be used in the prompt, making it a flexible method for adapting models to individual needs.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--167a3de1-b760-4f01-aed7-faa21899d2de-0', 'usage_metadata': {'input_tokens': 1416, 'output_tokens': 85, 'total_tokens': 1501, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:38:50,240 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 77%|███████▋  | 50/65 [03:44<01:13,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\\n', 'type': 'human', 'id': '7938c0e0-3651-4acb-a476-de1a38f38a15'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_ks8vUuJzZkMWYbDpqMTKJwkC', 'function': {'arguments': '{\"query\":\"recommended multiple of batch size for fp16 data type on A100 GPU\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--8343282c-cf0b-4cc3-975d-4ab1d84dafd3-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'recommended multiple of batch size for fp16 data type on A100 GPU'}, 'id': 'call_ks8vUuJzZkMWYbDpqMTKJwkC', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 367, 'output_tokens': 29, 'total_tokens': 396, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': \"For instance, for fp16 data type a multiple of 8 is recommended, unless\\nit's an A100 GPU, in which case use multiples of 64. For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup. ## Gradient Accumulation\\nThe \\\\*\\\\*gradient accumulation\\\\*\\\\* method aims to calculate gradients in smaller increments instead of computing them for the\\nentire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward\\nand backward passes through the model and accumulating the gradients during the process. Once a sufficient number of\\ngradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\\nbecomes possible to increase the \\\\*\\\\*effective batch size\\\\*\\\\* beyond the limitations imposed by the GPU's memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can\\nslow down the training process. You can enable gradient accumulation by adding the `gradient\\\\_accumulation\\\\_steps` argument to [`TrainingArguments`]:\\n```py\\ntraining\\\\_args = TrainingArguments(per\\\\_device\\\\_train\\\\_batch\\\\_size=1, gradient\\\\_accumulation\\\\_steps=4, \\\\*\\\\*default\\\\_args)\\n```\\nIn the above example, your effective batch size becomes 4.\\n\\nIt was run on a single NVIDIA A100-SXM4-80GB GPU with a prompt length of 512. The model we used was `meta-llama/Llama-2-13b-hf`. with batch size = 1:\\n|quantization |act\\\\_order|bits|group\\\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\\n|fp16|None |None|None |None |26.0 |36.958 |27.058 |29152.98 |\\n|gptq |False |4 |128 |exllama|36.2 |33.711 |29.663 |10484.34 |\\n|bitsandbytes|None |4|None |None |37.64 |52.00 |19.23 |11018.36 |\\nwith batch size = 16:\\n|quantization |act\\\\_order|bits|group\\\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\\n|fp16|None |None|None |None |26.0 |69.94 |228.76 |53986.51 |\\n|gptq |False |4 |128 |exllama|36.2 |95.41 |167.68 |34777.04 |\\n|bitsandbytes|None |4|None |None |37.64 |113.98 |140.38 |35532.37 |\\nFrom the benchmark, we can see that bitsandbyes and GPTQ are equivalent, with GPTQ being slightly faster for large batch size.\\n\\nThe impact of `fp16` on latency is less marked with `batch\\\\_size = 1`, but here it is of enormous interest as it can reduce latency by almost half, and almost double throughput! # Concluding remarks\\nThis blog post showcased a few simple optimization tricks bundled in the 🤗 ecosystem. Using anyone of these techniques, or a combination of all three, can greatly improve Bark inference speed and memory footprint. \\\\* You can use the large version of Bark without any performance degradation and a footprint of just 2GB instead of 5GB, 15% faster, \\\\*\\\\*using 🤗 Better Transformer and CPU offload\\\\*\\\\*. \\\\* Do you prefer high throughput? \\\\*\\\\*Batch by 8 with 🤗 Better Transformer and half-precision\\\\*\\\\*. \\\\* You can get the best of both worlds by using \\\\*\\\\*fp16, 🤗 Better Transformer and CPU offload\\\\*\\\\*!\", 'type': 'tool', 'name': 'data_retriever', 'id': '3a2c96f3-cb0f-443e-b824-2e58ef48616b', 'tool_call_id': 'call_ks8vUuJzZkMWYbDpqMTKJwkC', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b90faf3d2ebb270a0708', 'page_content': \"For instance, for fp16 data type a multiple of 8 is recommended, unless\\nit's an A100 GPU, in which case use multiples of 64. For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup. ## Gradient Accumulation\\nThe \\\\*\\\\*gradient accumulation\\\\*\\\\* method aims to calculate gradients in smaller increments instead of computing them for the\\nentire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward\\nand backward passes through the model and accumulating the gradients during the process. Once a sufficient number of\\ngradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\\nbecomes possible to increase the \\\\*\\\\*effective batch size\\\\*\\\\* beyond the limitations imposed by the GPU's memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can\\nslow down the training process. You can enable gradient accumulation by adding the `gradient\\\\_accumulation\\\\_steps` argument to [`TrainingArguments`]:\\n```py\\ntraining\\\\_args = TrainingArguments(per\\\\_device\\\\_train\\\\_batch\\\\_size=1, gradient\\\\_accumulation\\\\_steps=4, \\\\*\\\\*default\\\\_args)\\n```\\nIn the above example, your effective batch size becomes 4.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3993e684812d15f68e1', 'page_content': 'It was run on a single NVIDIA A100-SXM4-80GB GPU with a prompt length of 512. The model we used was `meta-llama/Llama-2-13b-hf`. with batch size = 1:\\n|quantization |act\\\\_order|bits|group\\\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\\n|fp16|None |None|None |None |26.0 |36.958 |27.058 |29152.98 |\\n|gptq |False |4 |128 |exllama|36.2 |33.711 |29.663 |10484.34 |\\n|bitsandbytes|None |4|None |None |37.64 |52.00 |19.23 |11018.36 |\\nwith batch size = 16:\\n|quantization |act\\\\_order|bits|group\\\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\\n|fp16|None |None|None |None |26.0 |69.94 |228.76 |53986.51 |\\n|gptq |False |4 |128 |exllama|36.2 |95.41 |167.68 |34777.04 |\\n|bitsandbytes|None |4|None |None |37.64 |113.98 |140.38 |35532.37 |\\nFrom the benchmark, we can see that bitsandbyes and GPTQ are equivalent, with GPTQ being slightly faster for large batch size.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa78a3e684812d15f5af9', 'page_content': 'The impact of `fp16` on latency is less marked with `batch\\\\_size = 1`, but here it is of enormous interest as it can reduce latency by almost half, and almost double throughput! # Concluding remarks\\nThis blog post showcased a few simple optimization tricks bundled in the 🤗 ecosystem. Using anyone of these techniques, or a combination of all three, can greatly improve Bark inference speed and memory footprint. \\\\* You can use the large version of Bark without any performance degradation and a footprint of just 2GB instead of 5GB, 15% faster, \\\\*\\\\*using 🤗 Better Transformer and CPU offload\\\\*\\\\*. \\\\* Do you prefer high throughput? \\\\*\\\\*Batch by 8 with 🤗 Better Transformer and half-precision\\\\*\\\\*. \\\\* You can get the best of both worlds by using \\\\*\\\\*fp16, 🤗 Better Transformer and CPU offload\\\\*\\\\*!', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'For fp16 data type on an A100 GPU, it is recommended to use multiples of 64.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--9db8f03e-886a-49fa-9ea2-704fd9d0b10b-0', 'usage_metadata': {'input_tokens': 1185, 'output_tokens': 21, 'total_tokens': 1206, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:38:57,490 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 78%|███████▊  | 51/65 [03:51<01:18,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'How do you run a Gradio Blocks app in reload mode using a Python IDE?\\n', 'type': 'human', 'id': 'a8c5546f-b4d9-4e9e-bfd2-77466793de34'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_kucVLZL9o8FIN5i5OyY7GRtn', 'function': {'arguments': '{\"query\":\"Gradio Blocks app reload mode Python IDE\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--4437937b-18e3-4da6-8335-a993e0278375-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Gradio Blocks app reload mode Python IDE'}, 'id': 'call_kucVLZL9o8FIN5i5OyY7GRtn', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 23, 'total_tokens': 388, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Developing Faster with Auto-Reloading\\n\\\\*\\\\*Prerequisite\\\\*\\\\*: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control). This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks. ## Why Auto-Reloading? When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes. To make it faster and more convenient to write your code, we\\'ve made it easier to \"reload\" your Gradio apps instantly when you are developing in a \\\\*\\\\*Python IDE\\\\*\\\\* (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We\\'ve also developed an analogous \"magic command\" that allows you to re-run cells faster if you use \\\\*\\\\*Jupyter Notebooks\\\\*\\\\* (or any similar environment like Colab). This short Guide will cover both of these methods, so no matter how you write Python, you\\'ll leave knowing how to build Gradio apps faster. ## Python IDE Reload 🔥\\nIf you are building Gradio Blocks using a Python IDE, your file of code (let\\'s name it `run.py`) might look something like this:\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as demo:\\ngr.Markdown(\"# Greetings from Gradio!\")\\ninp = gr.Textbox(placeholder=\"What is your name?\")\\nout = gr.Textbox()\\ninp.change(fn=lambda x: f\"Welcome, {x}!\",\\ninputs=inp,\\noutputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\ndemo.launch()\\n```\\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\\n\\nInstead of doing this, you can run your code in \\\\*\\\\*reload mode\\\\*\\\\* by changing 1 word: `python` to `gradio`:\\nIn the terminal, run `gradio run.py`. That\\'s it! Now, you\\'ll see that after you\\'ll see something like this:\\n```bash\\nWatching: \\'/Users/freddy/sources/gradio/gradio\\', \\'/Users/freddy/sources/gradio/demo/\\'\\nRunning on local URL: http://127.0.0.1:7860\\n```\\nThe important part here is the line that says `Watching...` What\\'s happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically 🥳\\n⚠️ Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show\\\\_error` in `launch()` will not be reflected in the app. There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as my\\\\_demo:\\ngr.Markdown(\"# Greetings from Gradio!\")\\ninp = gr.Textbox(placeholder=\"What is your name?\")\\nout = gr.Textbox()\\ninp.change(fn=lambda x: f\"Welcome, {x}!\",\\ninputs=inp,\\noutputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\nmy\\\\_demo.launch()\\n```\\nThen you would launch it in reload mode like this: `gradio run.py my\\\\_demo`.\\n\\nWARNING: The --reload flag should not be used in production on Windows. ```\\n这里最重要的一行是 `正在观察 ...`。这里发生的情况是 Gradio 将观察 `run.py` 文件所在的目录，如果文件发生更改，它将自动为您重新运行文件。因此，您只需专注于编写代码，Gradio 演示将自动刷新 🥳\\n⚠️ 警告：`gradio` 命令不会检测传递给 `launch()` 方法的参数，因为在重新加载模式下从未调用 `launch()` 方法。例如，设置 `launch()` 中的 `auth` 或 `show\\\\_error` 不会在应用程序中反映出来。\\n当您使用重新加载模式时，请记住一件重要的事情：Gradio 专门查找名为 `demo` 的 Gradio Blocks/Interface 演示。如果您将演示命名为其他名称，您需要在代码中的第二个参数中传入演示的 FastAPI 应用程序的名称。对于 Gradio 演示，可以使用 `.app` 属性访问 FastAPI 应用程序。因此，如果您的 `run.py` 文件如下所示：\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as my\\\\_demo:\\ngr.Markdown(\"# 来自Gradio的问候！\")\\ninp = gr.Textbox(placeholder=\"您叫什么名字？\")\\nout = gr.Textbox()\\ninp.change(fn=lambda x: f\"欢迎，{x}！\",\\ninputs=inp,\\noutputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\nmy\\\\_demo.launch()\\n```\\n那么您可以这样启动它：`gradio run.py my\\\\_demo.app`。\\nGradio默认使用UTF-8编码格式。对于\\\\*\\\\*重新加载模式\\\\*\\\\*，如果你的脚本使用的是除UTF-8以外的编码（如GBK）：\\n1. 在Python脚本的编码声明处指定你想要的编码格式，如：`# -\\\\*- coding: gbk -\\\\*-`\\n2. 确保你的代码编辑器识别到该格式。\\n3. 执行：`gradio run.py --encoding gbk`\\n🔥 如果您的应用程序接受命令行参数，您也可以传递它们。下面是一个例子：\\n```python\\nimport gradio as gr\\nimport argparse\\nparser = argparse.ArgumentParser()\\nparser.add\\\\_argument(\"--name\", type=str, default=\"User\")\\nargs, unknown = parser.parse\\\\_known\\\\_args()\\nwith gr.Blocks() as demo:\\ngr.Markdown(f\"# 欢迎 {args.name}！\")\\ninp = gr.Textbox()\\nout = gr.Textbox()\\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\ndemo.launch()\\n```\\n您可以像这样运行它：`gradio run.py --name Gretel`\\n作为一个小提示，只要更改了 `run.py` 源代码或 Gradio 源代码，自动重新加载就会发生。这意味着如果您决定[为 Gradio 做贡献](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md)，这将非常有用 ✅\\n## Jupyter Notebook 魔法命令🔮\\n如果您使用 Jupyter Notebooks（或 Colab Notebooks 等）进行开发，我们也为您提供了一个解决方案！\\n我们开发了一个 \\\\*\\\\*magic command 魔法命令\\\\*\\\\*，可以为您创建和运行一个 Blocks 演示。要使用此功能，在笔记本顶部加载 gradio 扩展：\\n`%load\\\\_ext gradio`\\n然后，在您正在开发 Gradio 演示的单元格中，只需在顶部写入魔法命令\\\\*\\\\*`%%blocks`\\\\*\\\\*，然后像平常一样编写布局和组件：\\n```py\\n%%blocks\\nimport gradio as gr\\ngr.Markdown(\"# 来自Gradio的问候！\")\\ninp = gr.Textbox(placeholder=\"您叫什么名字？\")\\nout = gr.Textbox()\\ninp.change(fn=lambda x: f\"欢迎，{x}！\",\\ninputs=inp,\\noutputs=out)\\n```\\n请注意：\\n- 您不需要放置样板代码 `with gr.Blocks() as demo:` 和 `demo.launch()` — Gradio 会自动为您完成！\\n- 每次重新运行单元格时，Gradio 都将在相同的端口上重新启动您的应用程序，并使用相同的底层网络服务器。这意味着您将比正常重新运行单元格更快地看到变化。\\n下面是在 Jupyter Notebook 中的示例：\\n![](https://i.ibb.co/nrszFws/Blocks.gif)\\n🪄这在 colab 笔记本中也适用！[这是一个 colab 笔记本](https://colab.research.google.com/drive/1jUlX1w7JqckRHVE-nbDyMPyZ7fYD8488?authuser=1#scrollTo=zxHYjbCTTz\\\\_5)，您可以在其中看到 Blocks 魔法效果。尝试进行一些更改并重新运行带有 Gradio 代码的单元格！\\nNotebook Magic 现在是作者构建 Gradio 演示的首选方式。无论您如何编写 Python 代码，我们都希望这两种方法都能为您提供更好的 Gradio 开发体验。\\n---\\n## 下一步\\n既然您已经了解了如何使用 Gradio 快速开发，请开始构建自己的应用程序吧！\\n如果你正在寻找灵感，请尝试浏览其他人用 Gradio 构建的演示，[浏览 Hugging Face Spaces](http://hf.space/) 🤗', 'type': 'tool', 'name': 'data_retriever', 'id': '033182b8-3dca-47b7-93ec-29262e38ddc2', 'tool_call_id': 'call_kucVLZL9o8FIN5i5OyY7GRtn', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c2f2666af48523f52222', 'page_content': 'Developing Faster with Auto-Reloading\\n\\\\*\\\\*Prerequisite\\\\*\\\\*: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control). This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks. ## Why Auto-Reloading? When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes. To make it faster and more convenient to write your code, we\\'ve made it easier to \"reload\" your Gradio apps instantly when you are developing in a \\\\*\\\\*Python IDE\\\\*\\\\* (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We\\'ve also developed an analogous \"magic command\" that allows you to re-run cells faster if you use \\\\*\\\\*Jupyter Notebooks\\\\*\\\\* (or any similar environment like Colab). This short Guide will cover both of these methods, so no matter how you write Python, you\\'ll leave knowing how to build Gradio apps faster. ## Python IDE Reload 🔥\\nIf you are building Gradio Blocks using a Python IDE, your file of code (let\\'s name it `run.py`) might look something like this:\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as demo:\\ngr.Markdown(\"# Greetings from Gradio!\")\\ninp = gr.Textbox(placeholder=\"What is your name?\")\\nout = gr.Textbox()\\ninp.change(fn=lambda x: f\"Welcome, {x}!\",\\ninputs=inp,\\noutputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\ndemo.launch()\\n```\\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c2f2666af48523f52223', 'page_content': 'Instead of doing this, you can run your code in \\\\*\\\\*reload mode\\\\*\\\\* by changing 1 word: `python` to `gradio`:\\nIn the terminal, run `gradio run.py`. That\\'s it! Now, you\\'ll see that after you\\'ll see something like this:\\n```bash\\nWatching: \\'/Users/freddy/sources/gradio/gradio\\', \\'/Users/freddy/sources/gradio/demo/\\'\\nRunning on local URL: http://127.0.0.1:7860\\n```\\nThe important part here is the line that says `Watching...` What\\'s happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically 🥳\\n⚠️ Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show\\\\_error` in `launch()` will not be reflected in the app. There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as my\\\\_demo:\\ngr.Markdown(\"# Greetings from Gradio!\")\\ninp = gr.Textbox(placeholder=\"What is your name?\")\\nout = gr.Textbox()\\ninp.change(fn=lambda x: f\"Welcome, {x}!\",\\ninputs=inp,\\noutputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\nmy\\\\_demo.launch()\\n```\\nThen you would launch it in reload mode like this: `gradio run.py my\\\\_demo`.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bf1a666af48523f51d78', 'page_content': 'WARNING: The --reload flag should not be used in production on Windows. ```\\n这里最重要的一行是 `正在观察 ...`。这里发生的情况是 Gradio 将观察 `run.py` 文件所在的目录，如果文件发生更改，它将自动为您重新运行文件。因此，您只需专注于编写代码，Gradio 演示将自动刷新 🥳\\n⚠️ 警告：`gradio` 命令不会检测传递给 `launch()` 方法的参数，因为在重新加载模式下从未调用 `launch()` 方法。例如，设置 `launch()` 中的 `auth` 或 `show\\\\_error` 不会在应用程序中反映出来。\\n当您使用重新加载模式时，请记住一件重要的事情：Gradio 专门查找名为 `demo` 的 Gradio Blocks/Interface 演示。如果您将演示命名为其他名称，您需要在代码中的第二个参数中传入演示的 FastAPI 应用程序的名称。对于 Gradio 演示，可以使用 `.app` 属性访问 FastAPI 应用程序。因此，如果您的 `run.py` 文件如下所示：\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as my\\\\_demo:\\ngr.Markdown(\"# 来自Gradio的问候！\")\\ninp = gr.Textbox(placeholder=\"您叫什么名字？\")\\nout = gr.Textbox()\\ninp.change(fn=lambda x: f\"欢迎，{x}！\",\\ninputs=inp,\\noutputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\nmy\\\\_demo.launch()\\n```\\n那么您可以这样启动它：`gradio run.py my\\\\_demo.app`。\\nGradio默认使用UTF-8编码格式。对于\\\\*\\\\*重新加载模式\\\\*\\\\*，如果你的脚本使用的是除UTF-8以外的编码（如GBK）：\\n1. 在Python脚本的编码声明处指定你想要的编码格式，如：`# -\\\\*- coding: gbk -\\\\*-`\\n2. 确保你的代码编辑器识别到该格式。\\n3. 执行：`gradio run.py --encoding gbk`\\n🔥 如果您的应用程序接受命令行参数，您也可以传递它们。下面是一个例子：\\n```python\\nimport gradio as gr\\nimport argparse\\nparser = argparse.ArgumentParser()\\nparser.add\\\\_argument(\"--name\", type=str, default=\"User\")\\nargs, unknown = parser.parse\\\\_known\\\\_args()\\nwith gr.Blocks() as demo:\\ngr.Markdown(f\"# 欢迎 {args.name}！\")\\ninp = gr.Textbox()\\nout = gr.Textbox()\\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\ndemo.launch()\\n```\\n您可以像这样运行它：`gradio run.py --name Gretel`\\n作为一个小提示，只要更改了 `run.py` 源代码或 Gradio 源代码，自动重新加载就会发生。这意味着如果您决定[为 Gradio 做贡献](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md)，这将非常有用 ✅\\n## Jupyter Notebook 魔法命令🔮\\n如果您使用 Jupyter Notebooks（或 Colab Notebooks 等）进行开发，我们也为您提供了一个解决方案！\\n我们开发了一个 \\\\*\\\\*magic command 魔法命令\\\\*\\\\*，可以为您创建和运行一个 Blocks 演示。要使用此功能，在笔记本顶部加载 gradio 扩展：\\n`%load\\\\_ext gradio`\\n然后，在您正在开发 Gradio 演示的单元格中，只需在顶部写入魔法命令\\\\*\\\\*`%%blocks`\\\\*\\\\*，然后像平常一样编写布局和组件：\\n```py\\n%%blocks\\nimport gradio as gr\\ngr.Markdown(\"# 来自Gradio的问候！\")\\ninp = gr.Textbox(placeholder=\"您叫什么名字？\")\\nout = gr.Textbox()\\ninp.change(fn=lambda x: f\"欢迎，{x}！\",\\ninputs=inp,\\noutputs=out)\\n```\\n请注意：\\n- 您不需要放置样板代码 `with gr.Blocks() as demo:` 和 `demo.launch()` — Gradio 会自动为您完成！\\n- 每次重新运行单元格时，Gradio 都将在相同的端口上重新启动您的应用程序，并使用相同的底层网络服务器。这意味着您将比正常重新运行单元格更快地看到变化。\\n下面是在 Jupyter Notebook 中的示例：\\n![](https://i.ibb.co/nrszFws/Blocks.gif)\\n🪄这在 colab 笔记本中也适用！[这是一个 colab 笔记本](https://colab.research.google.com/drive/1jUlX1w7JqckRHVE-nbDyMPyZ7fYD8488?authuser=1#scrollTo=zxHYjbCTTz\\\\_5)，您可以在其中看到 Blocks 魔法效果。尝试进行一些更改并重新运行带有 Gradio 代码的单元格！\\nNotebook Magic 现在是作者构建 Gradio 演示的首选方式。无论您如何编写 Python 代码，我们都希望这两种方法都能为您提供更好的 Gradio 开发体验。\\n---\\n## 下一步\\n既然您已经了解了如何使用 Gradio 快速开发，请开始构建自己的应用程序吧！\\n如果你正在寻找灵感，请尝试浏览其他人用 Gradio 构建的演示，[浏览 Hugging Face Spaces](http://hf.space/) 🤗', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'To run a Gradio Blocks app in reload mode using a Python IDE, you need to change the command you use to launch your app. Instead of running your script with `python run.py`, you should use the command `gradio run.py`. \\n\\nHere’s a brief overview of the steps:\\n\\n1. Make sure your code is structured properly. For example, your `run.py` file might look like this:\\n\\n   ```python\\n   import gradio as gr\\n   with gr.Blocks() as demo:\\n       gr.Markdown(\"# Greetings from Gradio!\")\\n       inp = gr.Textbox(placeholder=\"What is your name?\")\\n       out = gr.Textbox()\\n       inp.change(fn=lambda x: f\"Welcome, {x}!\", inputs=inp, outputs=out)\\n   if __name__ == \"__main__\":\\n       demo.launch()\\n   ```\\n\\n2. In the terminal, run the command:\\n\\n   ```\\n   gradio run.py\\n   ```\\n\\n3. You will see output indicating that Gradio is watching for changes in your code. If you make any changes to your `run.py` file, Gradio will automatically rerun the file for you, allowing you to see your updates without needing to manually restart the app.\\n\\nNote: If your demo is named something other than `demo`, you will need to specify the name of your demo as the second parameter when running the command, like this: `gradio run.py my_demo`.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--90198846-8a05-4d27-bfee-60f8315952b6-0', 'usage_metadata': {'input_tokens': 2362, 'output_tokens': 294, 'total_tokens': 2656, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:39:07,939 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 80%|████████  | 52/65 [04:02<01:31,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'How can you install the Hugging Face Unity API in your Unity project?\\n', 'type': 'human', 'id': 'ab90958c-8230-44da-a6b1-168ea2f33d85'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_aaK6Q5pvvWkHf0A5FviqaC5U', 'function': {'arguments': '{\"query\":\"install Hugging Face Unity API in Unity project\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--94b50186-fe87-4c72-91b3-37fc54e4aaa2-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'install Hugging Face Unity API in Unity project'}, 'id': 'call_aaK6Q5pvvWkHf0A5FviqaC5U', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 363, 'output_tokens': 24, 'total_tokens': 387, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '--\\ntitle: \"How to Install and Use the Hugging Face Unity API\"\\nthumbnail: /blog/assets/124\\\\_ml-for-games/unity-api-thumbnail.png\\nauthors:\\n- user: dylanebert\\n---\\n# How to Install and Use the Hugging Face Unity API\\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we\\'ll walk through the steps to install and use the Hugging Face Unity API. ## Installation\\n1. Open your Unity project\\n2. Go to `Window` -> `Package Manager`\\n3. Click `+` and select `Add Package from git URL`\\n4. Enter `https://github.com/huggingface/unity-api.git`\\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif)\\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens). 7. Test the API key by clicking `Test API key` in the API Wizard. 8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API\\\\_URL` field.\\n\\n9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png)\\nNow that the API is set up, you can make calls from your scripts to the API. Let\\'s look at an example of performing a Sentence Similarity task:\\n```\\nusing HuggingFace.API;\\n/\\\\* other code \\\\*/\\n// Make a call to the API\\nvoid Query() {\\nstring inputText = \"I\\'m on my way to the forest.\";\\nstring[] candidates = {\\n\"The player is going to the city\",\\n\"The player is going to the wilderness\",\\n\"The player is wandering aimlessly\"\\n};\\nHuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\\n}\\n// If successful, handle the result\\nvoid OnSuccess(float[] result) {\\nforeach(float value in result) {\\nDebug.Log(value);\\n}\\n}\\n// Otherwise, handle the error\\nvoid OnError(string error) {\\nDebug.LogError(error);\\n}\\n/\\\\* other code \\\\*/\\n```\\n## Supported Tasks and Custom Models\\nThe Hugging Face Unity API also currently supports the following tasks:\\n- [Conversation](https://huggingface.co/tasks/conversational)\\n- [Text Generation](https://huggingface.co/tasks/text-generation)\\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\\n- [Text Classification](https://huggingface.co/tasks/text-classification)\\n- [Question Answering](https://huggingface.co/tasks/question-answering)\\n- [Translation](https://huggingface.co/tasks/translation)\\n- [Summarization](https://huggingface.co/tasks/summarization)\\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\\n\\n### 4. Speech Recognition\\nNext, we\\'ll want to use the Hugging Face Unity API to run speech recognition on our encoded audio. To do so, we\\'ll create a `SendRecording()` method:\\n```\\nusing HuggingFace.API;\\nprivate void SendRecording() {\\nHuggingFaceAPI.AutomaticSpeechRecognition(bytes, response => {\\ntext.color = Color.white;\\ntext.text = response;\\n}, error => {\\ntext.color = Color.red;\\ntext.text = error;\\n});\\n}\\n```\\nThis will send the encoded audio to the API, displaying the response in white if successful, otherwise the error message in red. Don\\'t forget to call `SendRecording()` at the end of the `StopRecording()` method:\\n```\\nprivate void StopRecording() {\\n/\\\\* other code \\\\*/\\nSendRecording();\\n}\\n```\\n### 5. Final Touches\\nFinally, let\\'s improve the UX of this demo a bit using button interactability and status messages. The Start and Stop buttons should only be interactable when appropriate, i.e. when a recording is ready to be started/stopped. Then, set the response text to a simple status message while recording or waiting for the API. The finished script should look something like this:\\n```\\nusing System.IO;\\nusing HuggingFace.API;\\nusing TMPro;\\nusing UnityEngine;\\nusing UnityEngine.UI;\\npublic class SpeechRecognitionTest : MonoBehaviour {\\n[SerializeField] private Button startButton;\\n[SerializeField] private Button stopButton;\\n[SerializeField] private TextMeshProUGUI text;\\nprivate AudioClip clip;\\nprivate byte[] bytes;\\nprivate bool recording;\\nprivate void Start() {\\nstartButton.onClick.AddListener(StartRecording);\\nstopButton.onClick.AddListener(StopRecording);\\nstopButton.interactable = false;\\n}\\nprivate void Update() {\\nif (recording && Microphone.GetPosition(null) >= clip.samples) {\\nStopRecording();\\n}\\n}\\nprivate void StartRecording() {\\ntext.color = Color.white;\\ntext.text = \"Recording...\";\\nstartButton.interactable = false;\\nstopButton.interactable = true;\\nclip = Microphone.Start(null, false, 10, 44100);\\nrecording = true;\\n}\\nprivate void StopRecording() {\\nvar position = Microphone.GetPosition(null);\\nMicrophone.End(null);\\nvar samples = new float[position \\\\* clip.channels];\\nclip.GetData(samples, 0);\\nbytes = EncodeAsWAV(samples, clip.frequency, clip.channels);\\nrecording = false;\\nSendRecording();\\n}\\nprivate void SendRecording() {\\ntext.color = Color.yellow;\\ntext.text = \"Sending...\";\\nstopButton.interactable = false;\\nHuggingFaceAPI.AutomaticSpeechRecognition(bytes, response => {\\ntext.color = Color.white;\\ntext.text = response;\\nstartButton.interactable = true;\\n}, error => {\\ntext.color = Color.red;\\ntext.text = error;\\nstartButton.interactable = true;\\n});\\n}\\nprivate byte[] EncodeAsWAV(float[] samples, int frequency, int channels) {\\nusing (var memoryStream = new MemoryStream(44 + samples.Length \\\\* 2)) {\\nusing (var writer = new BinaryWriter(memoryStream)) {\\nwriter.Write(\"RIFF\".ToCharArray());\\nwriter.Write(36 + samples.Length \\\\* 2);\\nwriter.Write(\"WAVE\".ToCharArray());\\nwriter.Write(\"fmt \".ToCharArray());\\nwriter.Write(16);\\nwriter.Write((ushort)1);\\nwriter.Write((ushort)channels);\\nwriter.Write(frequency);\\nwriter.Write(frequency \\\\* channels \\\\* 2);\\nwriter.Write((ushort)(channels \\\\* 2));\\nwriter.Write((ushort)16);\\nwriter.Write(\"data\".ToCharArray());\\nwriter.Write(samples.Length \\\\* 2);\\nforeach (var sample in samples) {\\nwriter.Write((short)(sample \\\\* short.MaxValue));\\n}\\n}\\nreturn memoryStream.ToArray();\\n}\\n}\\n}\\n```\\nCongratulations, you can now use state-of-the-art Speech Recognition in Unity! If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!', 'type': 'tool', 'name': 'data_retriever', 'id': 'f2a669ce-1df6-4f08-9881-7bcaab45bcb7', 'tool_call_id': 'call_aaK6Q5pvvWkHf0A5FviqaC5U', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b8a8af3d2ebb270a069c', 'page_content': '--\\ntitle: \"How to Install and Use the Hugging Face Unity API\"\\nthumbnail: /blog/assets/124\\\\_ml-for-games/unity-api-thumbnail.png\\nauthors:\\n- user: dylanebert\\n---\\n# How to Install and Use the Hugging Face Unity API\\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we\\'ll walk through the steps to install and use the Hugging Face Unity API. ## Installation\\n1. Open your Unity project\\n2. Go to `Window` -> `Package Manager`\\n3. Click `+` and select `Add Package from git URL`\\n4. Enter `https://github.com/huggingface/unity-api.git`\\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif)\\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens). 7. Test the API key by clicking `Test API key` in the API Wizard. 8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API\\\\_URL` field.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b8a8af3d2ebb270a069d', 'page_content': '9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png)\\nNow that the API is set up, you can make calls from your scripts to the API. Let\\'s look at an example of performing a Sentence Similarity task:\\n```\\nusing HuggingFace.API;\\n/\\\\* other code \\\\*/\\n// Make a call to the API\\nvoid Query() {\\nstring inputText = \"I\\'m on my way to the forest.\";\\nstring[] candidates = {\\n\"The player is going to the city\",\\n\"The player is going to the wilderness\",\\n\"The player is wandering aimlessly\"\\n};\\nHuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\\n}\\n// If successful, handle the result\\nvoid OnSuccess(float[] result) {\\nforeach(float value in result) {\\nDebug.Log(value);\\n}\\n}\\n// Otherwise, handle the error\\nvoid OnError(string error) {\\nDebug.LogError(error);\\n}\\n/\\\\* other code \\\\*/\\n```\\n## Supported Tasks and Custom Models\\nThe Hugging Face Unity API also currently supports the following tasks:\\n- [Conversation](https://huggingface.co/tasks/conversational)\\n- [Text Generation](https://huggingface.co/tasks/text-generation)\\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\\n- [Text Classification](https://huggingface.co/tasks/text-classification)\\n- [Question Answering](https://huggingface.co/tasks/question-answering)\\n- [Translation](https://huggingface.co/tasks/translation)\\n- [Summarization](https://huggingface.co/tasks/summarization)\\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa5ef3e684812d15f590a', 'page_content': '### 4. Speech Recognition\\nNext, we\\'ll want to use the Hugging Face Unity API to run speech recognition on our encoded audio. To do so, we\\'ll create a `SendRecording()` method:\\n```\\nusing HuggingFace.API;\\nprivate void SendRecording() {\\nHuggingFaceAPI.AutomaticSpeechRecognition(bytes, response => {\\ntext.color = Color.white;\\ntext.text = response;\\n}, error => {\\ntext.color = Color.red;\\ntext.text = error;\\n});\\n}\\n```\\nThis will send the encoded audio to the API, displaying the response in white if successful, otherwise the error message in red. Don\\'t forget to call `SendRecording()` at the end of the `StopRecording()` method:\\n```\\nprivate void StopRecording() {\\n/\\\\* other code \\\\*/\\nSendRecording();\\n}\\n```\\n### 5. Final Touches\\nFinally, let\\'s improve the UX of this demo a bit using button interactability and status messages. The Start and Stop buttons should only be interactable when appropriate, i.e. when a recording is ready to be started/stopped. Then, set the response text to a simple status message while recording or waiting for the API. The finished script should look something like this:\\n```\\nusing System.IO;\\nusing HuggingFace.API;\\nusing TMPro;\\nusing UnityEngine;\\nusing UnityEngine.UI;\\npublic class SpeechRecognitionTest : MonoBehaviour {\\n[SerializeField] private Button startButton;\\n[SerializeField] private Button stopButton;\\n[SerializeField] private TextMeshProUGUI text;\\nprivate AudioClip clip;\\nprivate byte[] bytes;\\nprivate bool recording;\\nprivate void Start() {\\nstartButton.onClick.AddListener(StartRecording);\\nstopButton.onClick.AddListener(StopRecording);\\nstopButton.interactable = false;\\n}\\nprivate void Update() {\\nif (recording && Microphone.GetPosition(null) >= clip.samples) {\\nStopRecording();\\n}\\n}\\nprivate void StartRecording() {\\ntext.color = Color.white;\\ntext.text = \"Recording...\";\\nstartButton.interactable = false;\\nstopButton.interactable = true;\\nclip = Microphone.Start(null, false, 10, 44100);\\nrecording = true;\\n}\\nprivate void StopRecording() {\\nvar position = Microphone.GetPosition(null);\\nMicrophone.End(null);\\nvar samples = new float[position \\\\* clip.channels];\\nclip.GetData(samples, 0);\\nbytes = EncodeAsWAV(samples, clip.frequency, clip.channels);\\nrecording = false;\\nSendRecording();\\n}\\nprivate void SendRecording() {\\ntext.color = Color.yellow;\\ntext.text = \"Sending...\";\\nstopButton.interactable = false;\\nHuggingFaceAPI.AutomaticSpeechRecognition(bytes, response => {\\ntext.color = Color.white;\\ntext.text = response;\\nstartButton.interactable = true;\\n}, error => {\\ntext.color = Color.red;\\ntext.text = error;\\nstartButton.interactable = true;\\n});\\n}\\nprivate byte[] EncodeAsWAV(float[] samples, int frequency, int channels) {\\nusing (var memoryStream = new MemoryStream(44 + samples.Length \\\\* 2)) {\\nusing (var writer = new BinaryWriter(memoryStream)) {\\nwriter.Write(\"RIFF\".ToCharArray());\\nwriter.Write(36 + samples.Length \\\\* 2);\\nwriter.Write(\"WAVE\".ToCharArray());\\nwriter.Write(\"fmt \".ToCharArray());\\nwriter.Write(16);\\nwriter.Write((ushort)1);\\nwriter.Write((ushort)channels);\\nwriter.Write(frequency);\\nwriter.Write(frequency \\\\* channels \\\\* 2);\\nwriter.Write((ushort)(channels \\\\* 2));\\nwriter.Write((ushort)16);\\nwriter.Write(\"data\".ToCharArray());\\nwriter.Write(samples.Length \\\\* 2);\\nforeach (var sample in samples) {\\nwriter.Write((short)(sample \\\\* short.MaxValue));\\n}\\n}\\nreturn memoryStream.ToArray();\\n}\\n}\\n}\\n```\\nCongratulations, you can now use state-of-the-art Speech Recognition in Unity! If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'To install the Hugging Face Unity API in your Unity project, follow these steps:\\n\\n1. Open your Unity project.\\n2. Go to `Window` -> `Package Manager`.\\n3. Click `+` and select `Add Package from git URL`.\\n4. Enter `https://github.com/huggingface/unity-api.git`.\\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.\\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).\\n7. Test the API key by clicking `Test API key` in the API Wizard.\\n8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.\\n9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`.\\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--9f5b0320-0976-44a0-96ed-1e1c03df1e71-0', 'usage_metadata': {'input_tokens': 1905, 'output_tokens': 269, 'total_tokens': 2174, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:39:12,592 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 82%|████████▏ | 53/65 [04:07<01:16,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the pretraining objective of the Wav2Vec2 context network?\\n', 'type': 'human', 'id': 'e9a9b4f4-e16f-4c6e-8c25-5cac2361a9f0'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_hoAqboZ2HObijU1OTiVHMW3N', 'function': {'arguments': '{\"query\":\"Wav2Vec2 pretraining objective\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--eefe95b5-1518-4205-8a4e-443223176df1-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Wav2Vec2 pretraining objective'}, 'id': 'call_hoAqboZ2HObijU1OTiVHMW3N', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 364, 'output_tokens': 23, 'total_tokens': 387, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Wav2Vec2 Contrastive Loss PreTraining examples\\nThe following example showcases how to pretrain a wav2vec2 model using the JAX/Flax backend. Pretraining Wav2Vec2 is rather complex, so it is highly recommended to read the\\n[official paper](https://arxiv.org/abs/2006.11477). JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU. Models written in JAX/Flax are \\\\*\\\\*immutable\\\\*\\\\* and updated in a purely functional\\nway which enables simple and efficient model parallelism. `run\\\\_wav2vec2\\\\_pretrain\\\\_flax.py` is a lightweight example of how to download and preprocess a dataset from the 🤗 Datasets library or use your own files (jsonlines or csv), then pretrain the wav2vec2 architectures above on it. For custom datasets in `jsonlines` format please see: [the Datasets documentation](https://huggingface.co/docs/datasets/loading\\\\_datasets#json-files) and you also will find examples of these below. Let\\'s start by creating a model repository to save the trained model and logs. Here we call the model `\"wav2vec2-base-robust\"`, but you can change the model name as you like. You can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\\nyou are logged in) or via the command line:\\n```\\nhuggingface-cli repo create wav2vec2-base-robust\\n```\\nNext we clone the model repository to add the tokenizer and model files. ```\\ngit clone https://huggingface.co//wav2vec2-base-robust\\n```\\nTo ensure that all tensorboard traces will be uploaded correctly, we need to\\ntrack them. You can run the following command inside your model repo to do so. ```\\ncd wav2vec2-base-robust\\ngit lfs track \"\\\\*tfevents\\\\*\"\\n```\\nGreat, we have set up our model repository. During training, we will automatically\\npush the training logs and model weights to the repo. Next, let\\'s add a symbolic link to the `run\\\\_wav2vec2\\\\_pretrain\\\\_flax`. ```bash\\nexport MODEL\\\\_DIR=\"./wav2vec2-base-robust\"\\nln -s ~/transformers/examples/research\\\\_projects/jax-projects/wav2vec2/run\\\\_wav2vec2\\\\_pretrain\\\\_flax.py ./\\n```\\n### Create the model configuration\\nLet\\'s first create the model configuration and store it in the model repository.\\n\\n!---\\nCopyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n# Speech Recognition Pre-Training\\n## Wav2Vec2 Speech Pre-Training\\nThe script [`run\\\\_speech\\\\_wav2vec2\\\\_pretraining\\\\_no\\\\_trainer.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run\\\\_wav2vec2\\\\_pretraining\\\\_no\\\\_trainer.py) can be used to pre-train a [Wav2Vec2](https://huggingface.co/transformers/model\\\\_doc/wav2vec2.html?highlight=wav2vec2) model from scratch. In the script [`run\\\\_speech\\\\_wav2vec2\\\\_pretraining\\\\_no\\\\_trainer`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run\\\\_wav2vec2\\\\_pretraining\\\\_no\\\\_trainer.py), a Wav2Vec2 model is pre-trained on audio data alone using [Wav2Vec2\\'s contrastive loss objective](https://arxiv.org/abs/2006.11477). The following examples show how to fine-tune a `\"base\"`-sized Wav2Vec2 model as well as a `\"large\"`-sized Wav2Vec2 model using [`accelerate`](https://github.com/huggingface/accelerate). ---\\n\\\\*\\\\*NOTE 1\\\\*\\\\*\\nWav2Vec2\\'s pre-training is known to be quite unstable. It is advised to do a couple of test runs with a smaller dataset,\\n\\\\*i.e.\\\\* `--dataset\\\\_config\\\\_names clean clean`, `--dataset\\\\_split\\\\_names validation test`\\nto find good hyper-parameters for `learning\\\\_rate`, `batch\\\\_size`, `num\\\\_warmup\\\\_steps`,\\nand the optimizer. A good metric to observe during training is the gradient norm which should ideally be between 0.5 and 2. ---\\n---\\n\\\\*\\\\*NOTE 2\\\\*\\\\*\\nWhen training a model on large datasets it is recommended to run the data preprocessing\\nin a first run in a \\\\*\\\\*non-distributed\\\\*\\\\* mode via `--preprocessing\\\\_only` so that\\nwhen running the model in \\\\*\\\\*distributed\\\\*\\\\* mode in a second step the preprocessed data\\ncan easily be loaded on each distributed device. ---\\n### Demo\\nIn this demo run we pre-train a `\"base-sized\"` Wav2Vec2 model simply only on the validation\\nand test data of [librispeech\\\\_asr](https://huggingface.co/datasets/librispeech\\\\_asr). The demo is run on two Titan RTX (24 GB RAM each). In case you have less RAM available\\nper device, consider reducing `--batch\\\\_size` and/or the `--max\\\\_duration\\\\_in\\\\_seconds`. ```bash\\naccelerate launch run\\\\_wav2vec2\\\\_pretraining\\\\_no\\\\_trainer.py \\\\\\n--dataset\\\\_name=\"librispeech\\\\_asr\" \\\\\\n--dataset\\\\_config\\\\_names clean clean \\\\\\n--dataset\\\\_split\\\\_names validation test \\\\\\n--model\\\\_name\\\\_or\\\\_path=\"patrickvonplaten/wav2vec2-base-v2\" \\\\\\n--output\\\\_dir=\"./wav2vec2-pretrained-demo\" \\\\\\n--max\\\\_train\\\\_steps=\"20000\" \\\\\\n--num\\\\_warmup\\\\_steps=\"32000\" \\\\\\n--gradient\\\\_accumulation\\\\_steps=\"8\" \\\\\\n--learning\\\\_rate=\"0.005\" \\\\\\n--weight\\\\_decay=\"0.01\" \\\\\\n--max\\\\_duration\\\\_in\\\\_seconds=\"20.0\" \\\\\\n--min\\\\_duration\\\\_in\\\\_seconds=\"2.0\" \\\\\\n--logging\\\\_steps=\"1\" \\\\\\n--saving\\\\_steps=\"10000\" \\\\\\n--per\\\\_device\\\\_train\\\\_batch\\\\_size=\"8\" \\\\\\n--per\\\\_device\\\\_eval\\\\_batch\\\\_size=\"8\" \\\\\\n--adam\\\\_beta1=\"0.9\" \\\\\\n--adam\\\\_beta2=\"0.98\" \\\\\\n--adam\\\\_epsilon=\"1e-06\" \\\\\\n--gradient\\\\_checkpointing \\\\\\n--mask\\\\_time\\\\_prob=\"0.65\" \\\\\\n--mask\\\\_time\\\\_length=\"10\"\\n```\\nThe results of this run can be seen [here](https://wandb.ai/patrickvonplaten/wav2vec2-pretrained-demo/reports/Wav2Vec2-PreTraining-Demo-Run--VmlldzoxMDk3MjAw?accessToken=oa05s1y57lizo2ocxy3k01g6db1u4pt8m6ur2n8nl4cb0ug02ms2cw313kb8ruch). ### Base\\nTo pre-train `\"base-sized\"` Wav2Vec2 model, \\\\*e.g.\\\\* [facebook/wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base)\\non [librispeech\\\\_asr](https://huggingface.co/datasets/librispeech\\\\_asr), the following command can be run:\\n```bash\\naccelerate launch run\\\\_wav2vec2\\\\_pretraining\\\\_no\\\\_trainer.py \\\\\\n--dataset\\\\_name=librispeech\\\\_asr \\\\\\n--dataset\\\\_config\\\\_names clean clean other \\\\\\n--dataset\\\\_split\\\\_names train.100 train.360 train.500 \\\\\\n--model\\\\_name\\\\_or\\\\_path=\"patrickvonplaten/wav2vec2-base-v2\" \\\\\\n--output\\\\_dir=\"./wav2vec2-pretrained-demo\" \\\\\\n--max\\\\_train\\\\_steps=\"200000\" \\\\\\n--num\\\\_warmup\\\\_steps=\"32000\" \\\\\\n--gradient\\\\_accumulation\\\\_steps=\"4\" \\\\\\n--learning\\\\_rate=\"0.001\" \\\\\\n--weight\\\\_decay=\"0.01\" \\\\\\n--max\\\\_duration\\\\_in\\\\_seconds=\"20.0\" \\\\\\n--min\\\\_duration\\\\_in\\\\_seconds=\"2.0\" \\\\\\n--logging\\\\_steps=\"1\" \\\\\\n--saving\\\\_steps=\"10000\" \\\\\\n--per\\\\_device\\\\_train\\\\_batch\\\\_size=\"8\" \\\\\\n--per\\\\_device\\\\_eval\\\\_batch\\\\_size=\"8\" \\\\\\n--adam\\\\_beta1=\"0.9\" \\\\\\n--adam\\\\_beta2=\"0.98\" \\\\\\n--adam\\\\_epsilon=\"1e-06\" \\\\\\n--gradient\\\\_checkpointing \\\\\\n--mask\\\\_time\\\\_prob=\"0.65\" \\\\\\n--mask\\\\_time\\\\_length=\"10\"\\n```\\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 4 days. In case you have more than 8 GPUs available for a higher effective `batch\\\\_size`,\\nit is recommended to increase the `learning\\\\_rate` to `0.005` for faster convergence. The results of this run can be seen [here](https://wandb.ai/patrickvonplaten/test/reports/Wav2Vec2-Base--VmlldzoxMTUyODQ0?accessToken=rg6e8u9yizx964k8q47zctq1m4afpvtn1i3qi9exgdmzip6xwkfzvagfajpzj55n) and the checkpoint pretrained for 85,000 steps can be accessed [here](https://huggingface.co/patrickvonplaten/wav2vec2-base-repro-960h-libri-85k-steps)\\n### Large\\nTo pre-train `\"large-sized\"` Wav2Vec2 model, \\\\*e.g.\\\\* [facebook/wav2vec2-large-lv60](https://huggingface.co/facebook/wav2vec2-large-lv60),\\non [librispeech\\\\_asr](https://huggingface.co/datasets/librispeech\\\\_asr), the following command can be run:\\n```bash\\naccelerate launch run\\\\_wav2vec2\\\\_pretraining\\\\_no\\\\_trainer.py \\\\\\n--dataset\\\\_name=librispeech\\\\_asr \\\\\\n--dataset\\\\_config\\\\_names clean clean other \\\\\\n--dataset\\\\_split\\\\_names train.100 train.360 train.500 \\\\\\n--output\\\\_dir=./test \\\\\\n--max\\\\_train\\\\_steps=200000 \\\\\\n--num\\\\_warmup\\\\_steps=32000 \\\\\\n--gradient\\\\_accumulation\\\\_steps=8 \\\\\\n--learning\\\\_rate=0.001 \\\\\\n--weight\\\\_decay=0.01 \\\\\\n--max\\\\_duration\\\\_in\\\\_seconds=20.0 \\\\\\n--min\\\\_duration\\\\_in\\\\_seconds=2.0 \\\\\\n--model\\\\_name\\\\_or\\\\_path=./\\n--logging\\\\_steps=1 \\\\\\n--saving\\\\_steps=10000 \\\\\\n--per\\\\_device\\\\_train\\\\_batch\\\\_size=2 \\\\\\n--per\\\\_device\\\\_eval\\\\_batch\\\\_size=4 \\\\\\n--adam\\\\_beta1=0.9 \\\\\\n--adam\\\\_beta2=0.98 \\\\\\n--adam\\\\_epsilon=1e-06 \\\\\\n--gradient\\\\_checkpointing \\\\\\n--mask\\\\_time\\\\_prob=0.65 \\\\\\n--mask\\\\_time\\\\_length=10\\n```\\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 7 days. In case you have more than 8 GPUs available for a higher effective `batch\\\\_size`,\\nit is recommended to increase the `learning\\\\_rate` to `0.005` for faster convergence.\\n\\n!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Wav2Vec2\\n## Overview\\nThe Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. The abstract from the paper is the following:\\n\\\\*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\\nrecognition with limited amounts of labeled data.\\\\*\\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). ## Usage tips\\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\\nusing [`Wav2Vec2CTCTokenizer`]. ## Resources\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Wav2Vec2.', 'type': 'tool', 'name': 'data_retriever', 'id': '0ca5ec0e-828c-47dd-8f61-80666d87a0d6', 'tool_call_id': 'call_hoAqboZ2HObijU1OTiVHMW3N', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaa3d3e684812d15f5e26', 'page_content': 'Wav2Vec2 Contrastive Loss PreTraining examples\\nThe following example showcases how to pretrain a wav2vec2 model using the JAX/Flax backend. Pretraining Wav2Vec2 is rather complex, so it is highly recommended to read the\\n[official paper](https://arxiv.org/abs/2006.11477). JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU. Models written in JAX/Flax are \\\\*\\\\*immutable\\\\*\\\\* and updated in a purely functional\\nway which enables simple and efficient model parallelism. `run\\\\_wav2vec2\\\\_pretrain\\\\_flax.py` is a lightweight example of how to download and preprocess a dataset from the 🤗 Datasets library or use your own files (jsonlines or csv), then pretrain the wav2vec2 architectures above on it. For custom datasets in `jsonlines` format please see: [the Datasets documentation](https://huggingface.co/docs/datasets/loading\\\\_datasets#json-files) and you also will find examples of these below. Let\\'s start by creating a model repository to save the trained model and logs. Here we call the model `\"wav2vec2-base-robust\"`, but you can change the model name as you like. You can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\\nyou are logged in) or via the command line:\\n```\\nhuggingface-cli repo create wav2vec2-base-robust\\n```\\nNext we clone the model repository to add the tokenizer and model files. ```\\ngit clone https://huggingface.co//wav2vec2-base-robust\\n```\\nTo ensure that all tensorboard traces will be uploaded correctly, we need to\\ntrack them. You can run the following command inside your model repo to do so. ```\\ncd wav2vec2-base-robust\\ngit lfs track \"\\\\*tfevents\\\\*\"\\n```\\nGreat, we have set up our model repository. During training, we will automatically\\npush the training logs and model weights to the repo. Next, let\\'s add a symbolic link to the `run\\\\_wav2vec2\\\\_pretrain\\\\_flax`. ```bash\\nexport MODEL\\\\_DIR=\"./wav2vec2-base-robust\"\\nln -s ~/transformers/examples/research\\\\_projects/jax-projects/wav2vec2/run\\\\_wav2vec2\\\\_pretrain\\\\_flax.py ./\\n```\\n### Create the model configuration\\nLet\\'s first create the model configuration and store it in the model repository.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab1563e684812d15f6645', 'page_content': '!---\\nCopyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\\nlimitations under the License. -->\\n# Speech Recognition Pre-Training\\n## Wav2Vec2 Speech Pre-Training\\nThe script [`run\\\\_speech\\\\_wav2vec2\\\\_pretraining\\\\_no\\\\_trainer.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run\\\\_wav2vec2\\\\_pretraining\\\\_no\\\\_trainer.py) can be used to pre-train a [Wav2Vec2](https://huggingface.co/transformers/model\\\\_doc/wav2vec2.html?highlight=wav2vec2) model from scratch. In the script [`run\\\\_speech\\\\_wav2vec2\\\\_pretraining\\\\_no\\\\_trainer`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run\\\\_wav2vec2\\\\_pretraining\\\\_no\\\\_trainer.py), a Wav2Vec2 model is pre-trained on audio data alone using [Wav2Vec2\\'s contrastive loss objective](https://arxiv.org/abs/2006.11477). The following examples show how to fine-tune a `\"base\"`-sized Wav2Vec2 model as well as a `\"large\"`-sized Wav2Vec2 model using [`accelerate`](https://github.com/huggingface/accelerate). ---\\n\\\\*\\\\*NOTE 1\\\\*\\\\*\\nWav2Vec2\\'s pre-training is known to be quite unstable. It is advised to do a couple of test runs with a smaller dataset,\\n\\\\*i.e.\\\\* `--dataset\\\\_config\\\\_names clean clean`, `--dataset\\\\_split\\\\_names validation test`\\nto find good hyper-parameters for `learning\\\\_rate`, `batch\\\\_size`, `num\\\\_warmup\\\\_steps`,\\nand the optimizer. A good metric to observe during training is the gradient norm which should ideally be between 0.5 and 2. ---\\n---\\n\\\\*\\\\*NOTE 2\\\\*\\\\*\\nWhen training a model on large datasets it is recommended to run the data preprocessing\\nin a first run in a \\\\*\\\\*non-distributed\\\\*\\\\* mode via `--preprocessing\\\\_only` so that\\nwhen running the model in \\\\*\\\\*distributed\\\\*\\\\* mode in a second step the preprocessed data\\ncan easily be loaded on each distributed device. ---\\n### Demo\\nIn this demo run we pre-train a `\"base-sized\"` Wav2Vec2 model simply only on the validation\\nand test data of [librispeech\\\\_asr](https://huggingface.co/datasets/librispeech\\\\_asr). The demo is run on two Titan RTX (24 GB RAM each). In case you have less RAM available\\nper device, consider reducing `--batch\\\\_size` and/or the `--max\\\\_duration\\\\_in\\\\_seconds`. ```bash\\naccelerate launch run\\\\_wav2vec2\\\\_pretraining\\\\_no\\\\_trainer.py \\\\\\n--dataset\\\\_name=\"librispeech\\\\_asr\" \\\\\\n--dataset\\\\_config\\\\_names clean clean \\\\\\n--dataset\\\\_split\\\\_names validation test \\\\\\n--model\\\\_name\\\\_or\\\\_path=\"patrickvonplaten/wav2vec2-base-v2\" \\\\\\n--output\\\\_dir=\"./wav2vec2-pretrained-demo\" \\\\\\n--max\\\\_train\\\\_steps=\"20000\" \\\\\\n--num\\\\_warmup\\\\_steps=\"32000\" \\\\\\n--gradient\\\\_accumulation\\\\_steps=\"8\" \\\\\\n--learning\\\\_rate=\"0.005\" \\\\\\n--weight\\\\_decay=\"0.01\" \\\\\\n--max\\\\_duration\\\\_in\\\\_seconds=\"20.0\" \\\\\\n--min\\\\_duration\\\\_in\\\\_seconds=\"2.0\" \\\\\\n--logging\\\\_steps=\"1\" \\\\\\n--saving\\\\_steps=\"10000\" \\\\\\n--per\\\\_device\\\\_train\\\\_batch\\\\_size=\"8\" \\\\\\n--per\\\\_device\\\\_eval\\\\_batch\\\\_size=\"8\" \\\\\\n--adam\\\\_beta1=\"0.9\" \\\\\\n--adam\\\\_beta2=\"0.98\" \\\\\\n--adam\\\\_epsilon=\"1e-06\" \\\\\\n--gradient\\\\_checkpointing \\\\\\n--mask\\\\_time\\\\_prob=\"0.65\" \\\\\\n--mask\\\\_time\\\\_length=\"10\"\\n```\\nThe results of this run can be seen [here](https://wandb.ai/patrickvonplaten/wav2vec2-pretrained-demo/reports/Wav2Vec2-PreTraining-Demo-Run--VmlldzoxMDk3MjAw?accessToken=oa05s1y57lizo2ocxy3k01g6db1u4pt8m6ur2n8nl4cb0ug02ms2cw313kb8ruch). ### Base\\nTo pre-train `\"base-sized\"` Wav2Vec2 model, \\\\*e.g.\\\\* [facebook/wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base)\\non [librispeech\\\\_asr](https://huggingface.co/datasets/librispeech\\\\_asr), the following command can be run:\\n```bash\\naccelerate launch run\\\\_wav2vec2\\\\_pretraining\\\\_no\\\\_trainer.py \\\\\\n--dataset\\\\_name=librispeech\\\\_asr \\\\\\n--dataset\\\\_config\\\\_names clean clean other \\\\\\n--dataset\\\\_split\\\\_names train.100 train.360 train.500 \\\\\\n--model\\\\_name\\\\_or\\\\_path=\"patrickvonplaten/wav2vec2-base-v2\" \\\\\\n--output\\\\_dir=\"./wav2vec2-pretrained-demo\" \\\\\\n--max\\\\_train\\\\_steps=\"200000\" \\\\\\n--num\\\\_warmup\\\\_steps=\"32000\" \\\\\\n--gradient\\\\_accumulation\\\\_steps=\"4\" \\\\\\n--learning\\\\_rate=\"0.001\" \\\\\\n--weight\\\\_decay=\"0.01\" \\\\\\n--max\\\\_duration\\\\_in\\\\_seconds=\"20.0\" \\\\\\n--min\\\\_duration\\\\_in\\\\_seconds=\"2.0\" \\\\\\n--logging\\\\_steps=\"1\" \\\\\\n--saving\\\\_steps=\"10000\" \\\\\\n--per\\\\_device\\\\_train\\\\_batch\\\\_size=\"8\" \\\\\\n--per\\\\_device\\\\_eval\\\\_batch\\\\_size=\"8\" \\\\\\n--adam\\\\_beta1=\"0.9\" \\\\\\n--adam\\\\_beta2=\"0.98\" \\\\\\n--adam\\\\_epsilon=\"1e-06\" \\\\\\n--gradient\\\\_checkpointing \\\\\\n--mask\\\\_time\\\\_prob=\"0.65\" \\\\\\n--mask\\\\_time\\\\_length=\"10\"\\n```\\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 4 days. In case you have more than 8 GPUs available for a higher effective `batch\\\\_size`,\\nit is recommended to increase the `learning\\\\_rate` to `0.005` for faster convergence. The results of this run can be seen [here](https://wandb.ai/patrickvonplaten/test/reports/Wav2Vec2-Base--VmlldzoxMTUyODQ0?accessToken=rg6e8u9yizx964k8q47zctq1m4afpvtn1i3qi9exgdmzip6xwkfzvagfajpzj55n) and the checkpoint pretrained for 85,000 steps can be accessed [here](https://huggingface.co/patrickvonplaten/wav2vec2-base-repro-960h-libri-85k-steps)\\n### Large\\nTo pre-train `\"large-sized\"` Wav2Vec2 model, \\\\*e.g.\\\\* [facebook/wav2vec2-large-lv60](https://huggingface.co/facebook/wav2vec2-large-lv60),\\non [librispeech\\\\_asr](https://huggingface.co/datasets/librispeech\\\\_asr), the following command can be run:\\n```bash\\naccelerate launch run\\\\_wav2vec2\\\\_pretraining\\\\_no\\\\_trainer.py \\\\\\n--dataset\\\\_name=librispeech\\\\_asr \\\\\\n--dataset\\\\_config\\\\_names clean clean other \\\\\\n--dataset\\\\_split\\\\_names train.100 train.360 train.500 \\\\\\n--output\\\\_dir=./test \\\\\\n--max\\\\_train\\\\_steps=200000 \\\\\\n--num\\\\_warmup\\\\_steps=32000 \\\\\\n--gradient\\\\_accumulation\\\\_steps=8 \\\\\\n--learning\\\\_rate=0.001 \\\\\\n--weight\\\\_decay=0.01 \\\\\\n--max\\\\_duration\\\\_in\\\\_seconds=20.0 \\\\\\n--min\\\\_duration\\\\_in\\\\_seconds=2.0 \\\\\\n--model\\\\_name\\\\_or\\\\_path=./\\n--logging\\\\_steps=1 \\\\\\n--saving\\\\_steps=10000 \\\\\\n--per\\\\_device\\\\_train\\\\_batch\\\\_size=2 \\\\\\n--per\\\\_device\\\\_eval\\\\_batch\\\\_size=4 \\\\\\n--adam\\\\_beta1=0.9 \\\\\\n--adam\\\\_beta2=0.98 \\\\\\n--adam\\\\_epsilon=1e-06 \\\\\\n--gradient\\\\_checkpointing \\\\\\n--mask\\\\_time\\\\_prob=0.65 \\\\\\n--mask\\\\_time\\\\_length=10\\n```\\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 7 days. In case you have more than 8 GPUs available for a higher effective `batch\\\\_size`,\\nit is recommended to increase the `learning\\\\_rate` to `0.005` for faster convergence.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bdea666af48523f51bd4', 'page_content': '!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer. -->\\n# Wav2Vec2\\n## Overview\\nThe Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. The abstract from the paper is the following:\\n\\\\*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\\nrecognition with limited amounts of labeled data.\\\\*\\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). ## Usage tips\\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\\nusing [`Wav2Vec2CTCTokenizer`]. ## Resources\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Wav2Vec2.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The pretraining objective of the Wav2Vec2 context network is to solve a contrastive task defined over a quantization of the latent representations, which are jointly learned. This approach allows the model to learn powerful representations from speech audio alone, followed by fine-tuning on transcribed speech.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f415e9f8-ca9f-4c97-be5b-986d105f3ca9-0', 'usage_metadata': {'input_tokens': 3631, 'output_tokens': 59, 'total_tokens': 3690, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:39:17,773 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 83%|████████▎ | 54/65 [04:12<01:05,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\\n', 'type': 'human', 'id': '554c4570-dc88-4eb2-81d2-db5ba05e5006'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_qXBNTZLvKFZJoiegqHR89QOM', 'function': {'arguments': '{\"query\":\"default checkpoint sentiment analysis pipeline Transformers library\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--9b2df6c1-9489-4e87-993a-2ed94625836c-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'default checkpoint sentiment analysis pipeline Transformers library'}, 'id': 'call_qXBNTZLvKFZJoiegqHR89QOM', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 364, 'output_tokens': 22, 'total_tokens': 386, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let\\'s look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\\\_tensors option tells the tokenizer to return a TensorFlow tensor.\\n\\nTransformers, what can they do?[[transformers-what-can-they-do]]\\nIn this section, we will look at what Transformer models can do and use our first tool from the 🤗 Transformers library: the `pipeline()` function. 👀 See that *Open in Colab* button on the top right? Click on it to open a Google Colab notebook with all the code samples of this section. This button will be present in any section containing code examples. If you want to run the examples locally, we recommend taking a look at the [setup](/course/chapter0). ## Transformers are everywhere![[transformers-are-everywhere]]\\nTransformer models are used to solve all kinds of NLP tasks, like the ones mentioned in the previous section. Here are some of the companies and organizations using Hugging Face and Transformer models, who also contribute back to the community by sharing their models:\\n![Companies using Hugging Face](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG)\\nThe [🤗 Transformers library](https://github.com/huggingface/transformers) provides the functionality to create and use those shared models. The [Model Hub](https://huggingface.co/models) contains thousands of pretrained models that anyone can download and use. You can also upload your own models to the Hub! ⚠️ The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want! [Create a huggingface.co](https://huggingface.co/join) account to benefit from all available features! Before diving into how Transformer models work under the hood, let\\'s look at a few examples of how they can be used to solve some interesting NLP problems. ## Working with pipelines[[working-with-pipelines]]\\nThe most basic object in the 🤗 Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:\\n```python\\nfrom transformers import pipeline\\nclassifier = pipeline(\"sentiment-analysis\")\\nclassifier(\"I\\'ve been waiting for a HuggingFace course my whole life.\")\\n```\\n```python out\\n[{\\'label\\': \\'POSITIVE\\', \\'score\\': 0.9598047137260437}]\\n```\\nWe can even pass several sentences! ```python\\nclassifier(\\n[\"I\\'ve been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\\n)\\n```\\n```python out\\n[{\\'label\\': \\'POSITIVE\\', \\'score\\': 0.9598047137260437},\\n{\\'label\\': \\'NEGATIVE\\', \\'score\\': 0.9994558095932007}]\\n```\\nBy default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English.\\n\\nhe pipeline function. The pipeline function is the most high-level API of the Transformers library. It regroups together all the steps to go from raw texts to usable predictions. The model used is at the core of a pipeline, but the pipeline also include all the necessary pre-processing (since the model does not expect texts, but numbers) as well as some post-processing to make the output of the model human-readable. Let\\'s look at a first example with the sentiment analysis pipeline. This pipeline performs text classification on a given input, and determines if it\\'s positive or negative. Here, it attributed the positive label on the given text, with a confidence of 95%. You can pass multiple texts to the same pipeline, which will be processed and passed through the model together, as a batch. The output is a list of individual results, in the same order as the input texts. Here we find the same label and score for the first text, and the second text is judged positive with a confidence of 99.99%. The zero-shot classification pipeline is a more general text-classification pipeline: it allows you to provide the labels you want.', 'type': 'tool', 'name': 'data_retriever', 'id': 'd291414d-e869-494f-b497-af9873f754dd', 'tool_call_id': 'call_qXBNTZLvKFZJoiegqHR89QOM', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab2d93e684812d15f680c', 'page_content': \"hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\\\_tensors option tells the tokenizer to return a TensorFlow tensor.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c248666af48523f52159', 'page_content': 'Transformers, what can they do?[[transformers-what-can-they-do]]\\nIn this section, we will look at what Transformer models can do and use our first tool from the 🤗 Transformers library: the `pipeline()` function. 👀 See that *Open in Colab* button on the top right? Click on it to open a Google Colab notebook with all the code samples of this section. This button will be present in any section containing code examples. If you want to run the examples locally, we recommend taking a look at the [setup](/course/chapter0). ## Transformers are everywhere![[transformers-are-everywhere]]\\nTransformer models are used to solve all kinds of NLP tasks, like the ones mentioned in the previous section. Here are some of the companies and organizations using Hugging Face and Transformer models, who also contribute back to the community by sharing their models:\\n![Companies using Hugging Face](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG)\\nThe [🤗 Transformers library](https://github.com/huggingface/transformers) provides the functionality to create and use those shared models. The [Model Hub](https://huggingface.co/models) contains thousands of pretrained models that anyone can download and use. You can also upload your own models to the Hub! ⚠️ The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want! [Create a huggingface.co](https://huggingface.co/join) account to benefit from all available features! Before diving into how Transformer models work under the hood, let\\'s look at a few examples of how they can be used to solve some interesting NLP problems. ## Working with pipelines[[working-with-pipelines]]\\nThe most basic object in the 🤗 Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:\\n```python\\nfrom transformers import pipeline\\nclassifier = pipeline(\"sentiment-analysis\")\\nclassifier(\"I\\'ve been waiting for a HuggingFace course my whole life.\")\\n```\\n```python out\\n[{\\'label\\': \\'POSITIVE\\', \\'score\\': 0.9598047137260437}]\\n```\\nWe can even pass several sentences! ```python\\nclassifier(\\n[\"I\\'ve been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\\n)\\n```\\n```python out\\n[{\\'label\\': \\'POSITIVE\\', \\'score\\': 0.9598047137260437},\\n{\\'label\\': \\'NEGATIVE\\', \\'score\\': 0.9994558095932007}]\\n```\\nBy default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab1da3e684812d15f66e5', 'page_content': \"he pipeline function. The pipeline function is the most high-level API of the Transformers library. It regroups together all the steps to go from raw texts to usable predictions. The model used is at the core of a pipeline, but the pipeline also include all the necessary pre-processing (since the model does not expect texts, but numbers) as well as some post-processing to make the output of the model human-readable. Let's look at a first example with the sentiment analysis pipeline. This pipeline performs text classification on a given input, and determines if it's positive or negative. Here, it attributed the positive label on the given text, with a confidence of 95%. You can pass multiple texts to the same pipeline, which will be processed and passed through the model together, as a batch. The output is a list of individual results, in the same order as the input texts. Here we find the same label and score for the first text, and the second text is judged positive with a confidence of 99.99%. The zero-shot classification pipeline is a more general text-classification pipeline: it allows you to provide the labels you want.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert base uncased finetuned sst2 english.\"', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--7dae8680-f77b-4d2d-b3c3-45ad0bdbb1d0-0', 'usage_metadata': {'input_tokens': 1497, 'output_tokens': 29, 'total_tokens': 1526, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:39:23,114 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 85%|████████▍ | 55/65 [04:17<00:57,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\\n', 'type': 'human', 'id': '7efc8ab4-15df-4332-966c-72de0c46a5a9'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_GMI3G36f4zlxRMedKdhNH3vy', 'function': {'arguments': '{\"query\":\"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--a981498c-9058-4ef3-bc6b-f746e28b34ed-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'How to use DeepSpeed to train models with billions of parameters on Habana Gaudi'}, 'id': 'call_GMI3G36f4zlxRMedKdhNH3vy', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 374, 'output_tokens': 31, 'total_tokens': 405, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Although these devices have quite a lot of memory, the model is so large that a single device is not enough to contain a single instance of BLOOMZ. To solve this issue, we are going to use [DeepSpeed](https://www.deepspeed.ai/), which is a deep learning optimization library that enables many memory and speed improvements to accelerate the model and make it fit the device. In particular, we rely here on [DeepSpeed-inference](https://arxiv.org/abs/2207.00032): it introduces several features such as [model (or pipeline) parallelism](https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism) to make the most of the available devices. For Gaudi2, we use [Habana\\'s DeepSpeed fork](https://github.com/HabanaAI/deepspeed) that adds support for HPUs. ### Latency\\nWe measured latencies (batch of one sample) for two different sizes of BLOOMZ, both with multi-billion parameters:\\n- [176 billion](https://huggingface.co/bigscience/bloomz) parameters\\n- [7 billion](https://huggingface.co/bigscience/bloomz-7b1) parameters\\nRuns were performed with DeepSpeed-inference in 16-bit precision with 8 devices and using a [key-value cache](https://huggingface.co/docs/transformers/v4.27.1/en/model\\\\_doc/bloom#transformers.BloomForCausalLM.forward.use\\\\_cache).\\n\\nNote that while [CUDA graphs](https://developer.nvidia.com/blog/cuda-graphs/) are not currently compatible with model parallelism in DeepSpeed (DeepSpeed v0.8.2, see [here](https://github.com/microsoft/DeepSpeed/blob/v0.8.2/deepspeed/inference/engine.py#L158)), HPU graphs are supported in Habana\\'s DeepSpeed fork. All benchmarks are doing [greedy generation](https://huggingface.co/blog/how-to-generate#greedy-search) of 100 token outputs. The input prompt is:\\n> \"DeepSpeed is a machine learning framework\"\\nwhich consists of 7 tokens with BLOOM\\'s tokenizer. The results for inference latency are displayed in the table below (the unit is \\\\*seconds\\\\*). | Model | Number of devices | Gaudi2 latency (seconds) | A100-80GB latency (seconds) | First-gen Gaudi latency (seconds) |\\n|:-----------:|:-----------------:|:-------------------------:|:-----------------:|:----------------------------------:|\\n| BLOOMZ | 8 | 3.103 | 4.402 | / |\\n| BLOOMZ-7B | 8 | 0.734 | 2.417 | 3.321 |\\n| BLOOMZ-7B | 1 | 0.772 | 2.119 | 2.387 |\\n\\\\*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, leading to a\\\\* x\\\\*1.42 speedup on BLOOMZ with Gaudi2 compared to A100.\\\\*\\nThe Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. \\\\*\\\\*For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB\\\\*\\\\*. Smaller checkpoints present interesting results too. \\\\*\\\\*Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!\\\\*\\\\* It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device. We also ran these models on first-gen Gaudi. While it is slower than Gaudi2, it is interesting from a price perspective as a DL1 instance on AWS costs approximately 13\\\\$ per hour. Latency for BLOOMZ-7B on first-gen Gaudi is 2.387 seconds. Thus, \\\\*\\\\*first-gen Gaudi offers for the 7-billion checkpoint a better price-performance ratio than A100\\\\*\\\\* which costs more than 30\\\\$ per hour! We expect the Habana team will optimize the performance of these models in the upcoming SynapseAI releases. For example, in our last benchmark, we saw that [Gaudi2 performs Stable Diffusion inference 2.2x faster than A100](https://huggingface.co/blog/habana-gaudi-2-benchmark#generating-images-from-text-with-stable-diffusion) and this has since been improved further to 2.37x with the latest optimizations provided by Habana. We will update these numbers as new versions of SynapseAI are released and integrated within Optimum Habana.\\n\\n--\\ntitle: \"Accelerate Large Model Training using DeepSpeed\"\\nthumbnail: /blog/assets/83\\\\_accelerate\\\\_deepspeed/deepspeed-thumbnail.png\\nauthors:\\n- user: smangrul\\n- user: sgugger\\n---\\n# Accelerate Large Model Training using DeepSpeed\\nIn this post we will look at how we can leverage the \\\\*\\\\*[Accelerate](https://github.com/huggingface/accelerate)\\\\*\\\\* library for training large models which enables users to leverage the ZeRO features of \\\\*\\\\*[DeeSpeed](https://www.deepspeed.ai)\\\\*\\\\*. # Motivation 🤗\\n\\\\*\\\\*Tired of Out of Memory (OOM) errors while trying to train large models? We\\'ve got you covered. Large models are very performant [1] but difficult to train with the available hardware. To get the most of the available hardware for training large models one can leverage Data Parallelism using ZeRO - Zero Redundancy Optimizer [2]\\\\*\\\\*. Below is a short description of Data Parallelism using ZeRO with diagram from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)\\n![ZeRO Data Parallelism](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero.png)\\n(Source: [link](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/))\\na.', 'type': 'tool', 'name': 'data_retriever', 'id': '5898dccf-e97e-4399-99fa-e073b7825c12', 'tool_call_id': 'call_GMI3G36f4zlxRMedKdhNH3vy', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab2663e684812d15f677d', 'page_content': \"Although these devices have quite a lot of memory, the model is so large that a single device is not enough to contain a single instance of BLOOMZ. To solve this issue, we are going to use [DeepSpeed](https://www.deepspeed.ai/), which is a deep learning optimization library that enables many memory and speed improvements to accelerate the model and make it fit the device. In particular, we rely here on [DeepSpeed-inference](https://arxiv.org/abs/2207.00032): it introduces several features such as [model (or pipeline) parallelism](https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism) to make the most of the available devices. For Gaudi2, we use [Habana's DeepSpeed fork](https://github.com/HabanaAI/deepspeed) that adds support for HPUs. ### Latency\\nWe measured latencies (batch of one sample) for two different sizes of BLOOMZ, both with multi-billion parameters:\\n- [176 billion](https://huggingface.co/bigscience/bloomz) parameters\\n- [7 billion](https://huggingface.co/bigscience/bloomz-7b1) parameters\\nRuns were performed with DeepSpeed-inference in 16-bit precision with 8 devices and using a [key-value cache](https://huggingface.co/docs/transformers/v4.27.1/en/model\\\\_doc/bloom#transformers.BloomForCausalLM.forward.use\\\\_cache).\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab2663e684812d15f677e', 'page_content': 'Note that while [CUDA graphs](https://developer.nvidia.com/blog/cuda-graphs/) are not currently compatible with model parallelism in DeepSpeed (DeepSpeed v0.8.2, see [here](https://github.com/microsoft/DeepSpeed/blob/v0.8.2/deepspeed/inference/engine.py#L158)), HPU graphs are supported in Habana\\'s DeepSpeed fork. All benchmarks are doing [greedy generation](https://huggingface.co/blog/how-to-generate#greedy-search) of 100 token outputs. The input prompt is:\\n> \"DeepSpeed is a machine learning framework\"\\nwhich consists of 7 tokens with BLOOM\\'s tokenizer. The results for inference latency are displayed in the table below (the unit is \\\\*seconds\\\\*). | Model | Number of devices | Gaudi2 latency (seconds) | A100-80GB latency (seconds) | First-gen Gaudi latency (seconds) |\\n|:-----------:|:-----------------:|:-------------------------:|:-----------------:|:----------------------------------:|\\n| BLOOMZ | 8 | 3.103 | 4.402 | / |\\n| BLOOMZ-7B | 8 | 0.734 | 2.417 | 3.321 |\\n| BLOOMZ-7B | 1 | 0.772 | 2.119 | 2.387 |\\n\\\\*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, leading to a\\\\* x\\\\*1.42 speedup on BLOOMZ with Gaudi2 compared to A100.\\\\*\\nThe Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. \\\\*\\\\*For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB\\\\*\\\\*. Smaller checkpoints present interesting results too. \\\\*\\\\*Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!\\\\*\\\\* It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device. We also ran these models on first-gen Gaudi. While it is slower than Gaudi2, it is interesting from a price perspective as a DL1 instance on AWS costs approximately 13\\\\$ per hour. Latency for BLOOMZ-7B on first-gen Gaudi is 2.387 seconds. Thus, \\\\*\\\\*first-gen Gaudi offers for the 7-billion checkpoint a better price-performance ratio than A100\\\\*\\\\* which costs more than 30\\\\$ per hour! We expect the Habana team will optimize the performance of these models in the upcoming SynapseAI releases. For example, in our last benchmark, we saw that [Gaudi2 performs Stable Diffusion inference 2.2x faster than A100](https://huggingface.co/blog/habana-gaudi-2-benchmark#generating-images-from-text-with-stable-diffusion) and this has since been improved further to 2.37x with the latest optimizations provided by Habana. We will update these numbers as new versions of SynapseAI are released and integrated within Optimum Habana.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab7c13e684812d15f6e0e', 'page_content': '--\\ntitle: \"Accelerate Large Model Training using DeepSpeed\"\\nthumbnail: /blog/assets/83\\\\_accelerate\\\\_deepspeed/deepspeed-thumbnail.png\\nauthors:\\n- user: smangrul\\n- user: sgugger\\n---\\n# Accelerate Large Model Training using DeepSpeed\\nIn this post we will look at how we can leverage the \\\\*\\\\*[Accelerate](https://github.com/huggingface/accelerate)\\\\*\\\\* library for training large models which enables users to leverage the ZeRO features of \\\\*\\\\*[DeeSpeed](https://www.deepspeed.ai)\\\\*\\\\*. # Motivation 🤗\\n\\\\*\\\\*Tired of Out of Memory (OOM) errors while trying to train large models? We\\'ve got you covered. Large models are very performant [1] but difficult to train with the available hardware. To get the most of the available hardware for training large models one can leverage Data Parallelism using ZeRO - Zero Redundancy Optimizer [2]\\\\*\\\\*. Below is a short description of Data Parallelism using ZeRO with diagram from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)\\n![ZeRO Data Parallelism](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero.png)\\n(Source: [link](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/))\\na.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is designed to demonstrate how to leverage the DeepSpeed library for training large models efficiently on Habana Gaudi hardware. It provides guidance on utilizing features such as model parallelism and the ZeRO (Zero Redundancy Optimizer) to optimize memory usage and improve training performance for models with billions of parameters. This helps users overcome challenges related to out-of-memory errors and makes the most of the available hardware resources.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--8c065a95-a315-4398-80f6-da25d1744759-0', 'usage_metadata': {'input_tokens': 1632, 'output_tokens': 101, 'total_tokens': 1733, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:39:31,238 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 86%|████████▌ | 56/65 [04:25<00:58,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What command line module does PyTorch provide to run a script on multiple GPUs?\\n', 'type': 'human', 'id': '8c2fa1c3-373d-4b97-910f-e2a2398af6f1'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_lPNdvOWLt8wAU5J28sQHkRRP', 'function': {'arguments': '{\"query\":\"PyTorch command line module for running a script on multiple GPUs\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--6a2aef79-4a3c-41a2-a57c-01cd48858b5c-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'PyTorch command line module for running a script on multiple GPUs'}, 'id': 'call_lPNdvOWLt8wAU5J28sQHkRRP', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 364, 'output_tokens': 27, 'total_tokens': 391, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '\\\\* correct / len(test\\\\_loader.dataset)}\\')\\n```\\nTypically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook. However, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through \\\\*distributed\\\\* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\\n## PyTorch Distributed Data Parallelism\\nAs the name implies, `torch.distributed` is meant to work on \\\\*distributed\\\\* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both. To convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp\\\\_tutorial.html)\\nFirst a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through\\n> Note: for this section of the tutorial it should be assumed these are sent in python script files. Later on a launcher using Accelerate will be discussed that removes this necessity\\n```python\\nimport os\\nimport torch.distributed as dist\\ndef setup(rank, world\\\\_size):\\n\"Sets up the process group and configuration for PyTorch Distributed Data Parallelism\"\\nos.environ[\"MASTER\\\\_ADDR\"] = \\'localhost\\'\\nos.environ[\"MASTER\\\\_PORT\"] = \"12355\"\\n# Initialize the process group\\ndist.init\\\\_process\\\\_group(\"gloo\", rank=rank, world\\\\_size=world\\\\_size)\\ndef cleanup():\\n\"Cleans up the distributed environment\"\\ndist.destroy\\\\_process\\\\_group()\\n```\\nThe last piece of the puzzle is \\\\*how do I send my data and model to another GPU?\\\\*\\nThis is where the `DistributedDataParallel` module comes into play. It will copy your model onto each GPU, and when `loss.backward()` is called the backpropagation is performed and the resulting gradients across all these copies of the model will be averaged/reduced. This ensures each device has the same weights post the optimizer step. Below is an example of our training setup, refactored as a function, with this capability:\\n> Note: Here rank is the overall rank of the current GPU compared to all the other GPUs available, meaning they have a rank of `0 -> n-1`\\n```python\\nfrom torch.nn.parallel import DistributedDataParallel as DDP\\ndef train(model, rank, world\\\\_size):\\nsetup(rank, world\\\\_size)\\nmodel = model.to(rank)\\nddp\\\\_model = DDP(model, device\\\\_ids=[rank])\\noptimizer = optim.AdamW(ddp\\\\_model.parameters(), lr=1e-3)\\n# Train for one epoch\\nmodel.train()\\nfor batch\\\\_idx, (data, target) in enumerate(train\\\\_loader):\\ndata, target = data.to(device), target.to(device)\\noutput = model(data)\\nloss = F.nll\\\\_loss(output, target)\\nloss.backward()\\noptimizer.step()\\noptimizer.zero\\\\_grad()\\ncleanup()\\n```\\nThe optimizer needs to be declared based on the model \\\\*on the specific device\\\\* (so `ddp\\\\_model` and not `model`) for all of the gradients to properly be calculated. Lastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\\n```bash\\ntorchrun --nproc\\\\_per\\\\_node=2 --nnodes=1 example\\\\_script.py\\n```\\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\\n\\n. ! - \\\\; \\\\: \\\\\" “ % ‘ ” � \\\\\\n--fp16 \\\\\\n--group\\\\_by\\\\_length \\\\\\n--push\\\\_to\\\\_hub \\\\\\n--do\\\\_train --do\\\\_eval\\n```\\nOn a single V100 GPU, this script should run in \\\\*ca.\\\\* 1 hour 20 minutes and yield a CTC loss of \\\\*\\\\*0.39\\\\*\\\\* and word error rate\\nof \\\\*\\\\*0.35\\\\*\\\\*. ### Multi GPU CTC\\nThe following command shows how to fine-tune [XLSR-Wav2Vec2](https://huggingface.co/transformers/main/model\\\\_doc/xlsr\\\\_wav2vec2.html) on [Common Voice](https://huggingface.co/datasets/common\\\\_voice) using 8 GPUs in half-precision. ```bash\\ntorchrun \\\\\\n--nproc\\\\_per\\\\_node 8 run\\\\_speech\\\\_recognition\\\\_ctc.py \\\\\\n--dataset\\\\_name=\"common\\\\_voice\" \\\\\\n--model\\\\_name\\\\_or\\\\_path=\"facebook/wav2vec2-large-xlsr-53\" \\\\\\n--dataset\\\\_config\\\\_name=\"tr\" \\\\\\n--output\\\\_dir=\"./wav2vec2-common\\\\_voice-tr-demo-dist\" \\\\\\n--overwrite\\\\_output\\\\_dir \\\\\\n--num\\\\_train\\\\_epochs=\"15\" \\\\\\n--per\\\\_device\\\\_train\\\\_batch\\\\_size=\"4\" \\\\\\n--learning\\\\_rate=\"3e-4\" \\\\\\n--warmup\\\\_steps=\"500\" \\\\\\n--evaluation\\\\_strategy=\"steps\" \\\\\\n--text\\\\_column\\\\_name=\"sentence\" \\\\\\n--length\\\\_column\\\\_name=\"input\\\\_length\" \\\\\\n--save\\\\_steps=\"400\" \\\\\\n--eval\\\\_steps=\"100\" \\\\\\n--logging\\\\_steps=\"1\" \\\\\\n--layerdrop=\"0.0\" \\\\\\n--save\\\\_total\\\\_limit=\"3\" \\\\\\n--freeze\\\\_feature\\\\_encoder \\\\\\n--gradient\\\\_checkpointing \\\\\\n--chars\\\\_to\\\\_ignore , ?\\n\\nThe file naming is up to you. It\\'s recommended to use DeepSpeed\\'s `add\\\\_config\\\\_arguments` utility to add the necessary command line arguments to your code. For more information please see [DeepSpeed\\'s Argument Parsing](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) doc. You can use a launcher of your choice here. You can continue using the pytorch launcher:\\n```bash\\ntorch.distributed.run --nproc\\\\_per\\\\_node=2 your\\\\_program.py  --deepspeed ds\\\\_config.json\\n```\\nor use the launcher provided by `deepspeed`:\\n```bash\\ndeepspeed --num\\\\_gpus=2 your\\\\_program.py  --deepspeed ds\\\\_config.json\\n```\\nAs you can see the arguments aren\\'t the same, but for most needs either of them works. The\\nfull details on how to configure various nodes and GPUs can be found [here](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node). When you use the `deepspeed` launcher and you want to use all available gpus you can just omit the `--num\\\\_gpus` flag. Here is an example of running `run\\\\_translation.py` under DeepSpeed deploying all available GPUs:\\n```bash\\ndeepspeed examples/pytorch/translation/run\\\\_translation.py \\\\\\n--deepspeed tests/deepspeed/ds\\\\_config\\\\_zero3.json \\\\\\n--model\\\\_name\\\\_or\\\\_path t5-small --per\\\\_device\\\\_train\\\\_batch\\\\_size 1 \\\\\\n--output\\\\_dir output\\\\_dir --overwrite\\\\_output\\\\_dir --fp16 \\\\\\n--do\\\\_train --max\\\\_train\\\\_samples 500 --num\\\\_train\\\\_epochs 1 \\\\\\n--dataset\\\\_name wmt16 --dataset\\\\_config \"ro-en\" \\\\\\n--source\\\\_lang en --target\\\\_lang ro\\n```\\nNote that in the DeepSpeed documentation you are likely to see `--deepspeed --deepspeed\\\\_config ds\\\\_config.json` - i.e. two DeepSpeed-related arguments, but for the sake of simplicity, and since there are already so many arguments to deal\\nwith, we combined the two into a single argument. For some practical usage examples, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400). ### Deployment with one GPU\\nTo deploy DeepSpeed with one GPU adjust the [`Trainer`] command line arguments as follows:\\n```bash\\ndeepspeed --num\\\\_gpus=1 examples/pytorch/translation/run\\\\_translation.py \\\\\\n--deepspeed tests/deepspeed/ds\\\\_config\\\\_zero2.json \\\\\\n--model\\\\_name\\\\_or\\\\_path t5-small --per\\\\_device\\\\_train\\\\_batch\\\\_size 1 \\\\\\n--output\\\\_dir output\\\\_dir --overwrite\\\\_output\\\\_dir --fp16 \\\\\\n--do\\\\_train --max\\\\_train\\\\_samples 500 --num\\\\_train\\\\_epochs 1 \\\\\\n--dataset\\\\_name wmt16 --dataset\\\\_config \"ro-en\" \\\\\\n--source\\\\_lang en --target\\\\_lang ro\\n```\\nThis is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly to use just one GPU via\\n`--num\\\\_gpus=1`. By default, DeepSpeed deploys all GPUs it can see on the given node. If you have only 1 GPU to start\\nwith, then you don\\'t need this argument. The following [documentation](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) discusses the launcher options.', 'type': 'tool', 'name': 'data_retriever', 'id': '79240ced-4681-4510-b8f3-34d2cc29bd46', 'tool_call_id': 'call_lPNdvOWLt8wAU5J28sQHkRRP', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab8fb3e684812d15f6f44', 'page_content': '\\\\* correct / len(test\\\\_loader.dataset)}\\')\\n```\\nTypically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook. However, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through \\\\*distributed\\\\* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\\n## PyTorch Distributed Data Parallelism\\nAs the name implies, `torch.distributed` is meant to work on \\\\*distributed\\\\* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both. To convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp\\\\_tutorial.html)\\nFirst a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through\\n> Note: for this section of the tutorial it should be assumed these are sent in python script files. Later on a launcher using Accelerate will be discussed that removes this necessity\\n```python\\nimport os\\nimport torch.distributed as dist\\ndef setup(rank, world\\\\_size):\\n\"Sets up the process group and configuration for PyTorch Distributed Data Parallelism\"\\nos.environ[\"MASTER\\\\_ADDR\"] = \\'localhost\\'\\nos.environ[\"MASTER\\\\_PORT\"] = \"12355\"\\n# Initialize the process group\\ndist.init\\\\_process\\\\_group(\"gloo\", rank=rank, world\\\\_size=world\\\\_size)\\ndef cleanup():\\n\"Cleans up the distributed environment\"\\ndist.destroy\\\\_process\\\\_group()\\n```\\nThe last piece of the puzzle is \\\\*how do I send my data and model to another GPU?\\\\*\\nThis is where the `DistributedDataParallel` module comes into play. It will copy your model onto each GPU, and when `loss.backward()` is called the backpropagation is performed and the resulting gradients across all these copies of the model will be averaged/reduced. This ensures each device has the same weights post the optimizer step. Below is an example of our training setup, refactored as a function, with this capability:\\n> Note: Here rank is the overall rank of the current GPU compared to all the other GPUs available, meaning they have a rank of `0 -> n-1`\\n```python\\nfrom torch.nn.parallel import DistributedDataParallel as DDP\\ndef train(model, rank, world\\\\_size):\\nsetup(rank, world\\\\_size)\\nmodel = model.to(rank)\\nddp\\\\_model = DDP(model, device\\\\_ids=[rank])\\noptimizer = optim.AdamW(ddp\\\\_model.parameters(), lr=1e-3)\\n# Train for one epoch\\nmodel.train()\\nfor batch\\\\_idx, (data, target) in enumerate(train\\\\_loader):\\ndata, target = data.to(device), target.to(device)\\noutput = model(data)\\nloss = F.nll\\\\_loss(output, target)\\nloss.backward()\\noptimizer.step()\\noptimizer.zero\\\\_grad()\\ncleanup()\\n```\\nThe optimizer needs to be declared based on the model \\\\*on the specific device\\\\* (so `ddp\\\\_model` and not `model`) for all of the gradients to properly be calculated. Lastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\\n```bash\\ntorchrun --nproc\\\\_per\\\\_node=2 --nnodes=1 example\\\\_script.py\\n```\\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaaa13e684812d15f5e87', 'page_content': '. ! - \\\\; \\\\: \\\\\" “ % ‘ ” � \\\\\\n--fp16 \\\\\\n--group\\\\_by\\\\_length \\\\\\n--push\\\\_to\\\\_hub \\\\\\n--do\\\\_train --do\\\\_eval\\n```\\nOn a single V100 GPU, this script should run in \\\\*ca.\\\\* 1 hour 20 minutes and yield a CTC loss of \\\\*\\\\*0.39\\\\*\\\\* and word error rate\\nof \\\\*\\\\*0.35\\\\*\\\\*. ### Multi GPU CTC\\nThe following command shows how to fine-tune [XLSR-Wav2Vec2](https://huggingface.co/transformers/main/model\\\\_doc/xlsr\\\\_wav2vec2.html) on [Common Voice](https://huggingface.co/datasets/common\\\\_voice) using 8 GPUs in half-precision. ```bash\\ntorchrun \\\\\\n--nproc\\\\_per\\\\_node 8 run\\\\_speech\\\\_recognition\\\\_ctc.py \\\\\\n--dataset\\\\_name=\"common\\\\_voice\" \\\\\\n--model\\\\_name\\\\_or\\\\_path=\"facebook/wav2vec2-large-xlsr-53\" \\\\\\n--dataset\\\\_config\\\\_name=\"tr\" \\\\\\n--output\\\\_dir=\"./wav2vec2-common\\\\_voice-tr-demo-dist\" \\\\\\n--overwrite\\\\_output\\\\_dir \\\\\\n--num\\\\_train\\\\_epochs=\"15\" \\\\\\n--per\\\\_device\\\\_train\\\\_batch\\\\_size=\"4\" \\\\\\n--learning\\\\_rate=\"3e-4\" \\\\\\n--warmup\\\\_steps=\"500\" \\\\\\n--evaluation\\\\_strategy=\"steps\" \\\\\\n--text\\\\_column\\\\_name=\"sentence\" \\\\\\n--length\\\\_column\\\\_name=\"input\\\\_length\" \\\\\\n--save\\\\_steps=\"400\" \\\\\\n--eval\\\\_steps=\"100\" \\\\\\n--logging\\\\_steps=\"1\" \\\\\\n--layerdrop=\"0.0\" \\\\\\n--save\\\\_total\\\\_limit=\"3\" \\\\\\n--freeze\\\\_feature\\\\_encoder \\\\\\n--gradient\\\\_checkpointing \\\\\\n--chars\\\\_to\\\\_ignore , ?', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899be74666af48523f51c99', 'page_content': 'The file naming is up to you. It\\'s recommended to use DeepSpeed\\'s `add\\\\_config\\\\_arguments` utility to add the necessary command line arguments to your code. For more information please see [DeepSpeed\\'s Argument Parsing](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) doc. You can use a launcher of your choice here. You can continue using the pytorch launcher:\\n```bash\\ntorch.distributed.run --nproc\\\\_per\\\\_node=2 your\\\\_program.py  --deepspeed ds\\\\_config.json\\n```\\nor use the launcher provided by `deepspeed`:\\n```bash\\ndeepspeed --num\\\\_gpus=2 your\\\\_program.py  --deepspeed ds\\\\_config.json\\n```\\nAs you can see the arguments aren\\'t the same, but for most needs either of them works. The\\nfull details on how to configure various nodes and GPUs can be found [here](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node). When you use the `deepspeed` launcher and you want to use all available gpus you can just omit the `--num\\\\_gpus` flag. Here is an example of running `run\\\\_translation.py` under DeepSpeed deploying all available GPUs:\\n```bash\\ndeepspeed examples/pytorch/translation/run\\\\_translation.py \\\\\\n--deepspeed tests/deepspeed/ds\\\\_config\\\\_zero3.json \\\\\\n--model\\\\_name\\\\_or\\\\_path t5-small --per\\\\_device\\\\_train\\\\_batch\\\\_size 1 \\\\\\n--output\\\\_dir output\\\\_dir --overwrite\\\\_output\\\\_dir --fp16 \\\\\\n--do\\\\_train --max\\\\_train\\\\_samples 500 --num\\\\_train\\\\_epochs 1 \\\\\\n--dataset\\\\_name wmt16 --dataset\\\\_config \"ro-en\" \\\\\\n--source\\\\_lang en --target\\\\_lang ro\\n```\\nNote that in the DeepSpeed documentation you are likely to see `--deepspeed --deepspeed\\\\_config ds\\\\_config.json` - i.e. two DeepSpeed-related arguments, but for the sake of simplicity, and since there are already so many arguments to deal\\nwith, we combined the two into a single argument. For some practical usage examples, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400). ### Deployment with one GPU\\nTo deploy DeepSpeed with one GPU adjust the [`Trainer`] command line arguments as follows:\\n```bash\\ndeepspeed --num\\\\_gpus=1 examples/pytorch/translation/run\\\\_translation.py \\\\\\n--deepspeed tests/deepspeed/ds\\\\_config\\\\_zero2.json \\\\\\n--model\\\\_name\\\\_or\\\\_path t5-small --per\\\\_device\\\\_train\\\\_batch\\\\_size 1 \\\\\\n--output\\\\_dir output\\\\_dir --overwrite\\\\_output\\\\_dir --fp16 \\\\\\n--do\\\\_train --max\\\\_train\\\\_samples 500 --num\\\\_train\\\\_epochs 1 \\\\\\n--dataset\\\\_name wmt16 --dataset\\\\_config \"ro-en\" \\\\\\n--source\\\\_lang en --target\\\\_lang ro\\n```\\nThis is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly to use just one GPU via\\n`--num\\\\_gpus=1`. By default, DeepSpeed deploys all GPUs it can see on the given node. If you have only 1 GPU to start\\nwith, then you don\\'t need this argument. The following [documentation](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) discusses the launcher options.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'PyTorch provides the `torchrun` command line module to run a script on multiple GPUs. You can use it by passing in the number of nodes and the script to run, for example:\\n\\n```bash\\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\\n```', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--1fa0ab19-fed3-4e99-b03b-03c3aaa30dba-0', 'usage_metadata': {'input_tokens': 2382, 'output_tokens': 61, 'total_tokens': 2443, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:39:37,141 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 88%|████████▊ | 57/65 [04:31<00:50,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\\n', 'type': 'human', 'id': '91479c79-6aae-4e5c-82a8-b8667ae3da1d'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_pCuL1xhX4C3iGpIbUHTYdhvn', 'function': {'arguments': '{\"query\":\"most popular vision transformer model for image classification site:huggingface.co\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--34b7c4c3-8db8-453e-8747-d7d75f840064-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'most popular vision transformer model for image classification site:huggingface.co'}, 'id': 'call_pCuL1xhX4C3iGpIbUHTYdhvn', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 367, 'output_tokens': 29, 'total_tokens': 396, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Vision Transformers 图像分类\\n相关空间：https://huggingface.co/spaces/abidlabs/vision-transformer\\n标签：VISION, TRANSFORMERS, HUB\\n## 简介\\n图像分类是计算机视觉中的重要任务。构建更好的分类器以确定图像中存在的对象是当前研究的热点领域，因为它在从人脸识别到制造质量控制等方面都有应用。\\n最先进的图像分类器基于 \\\\_transformers\\\\_ 架构，该架构最初在自然语言处理任务中很受欢迎。这种架构通常被称为 vision transformers (ViT)。这些模型非常适合与 Gradio 的\\\\*图像\\\\*输入组件一起使用，因此在本教程中，我们将构建一个使用 Gradio 进行图像分类的 Web 演示。我们只需用\\\\*\\\\*一行 Python 代码\\\\*\\\\*即可构建整个 Web 应用程序，其效果如下（试用一下示例之一！）：\\n让我们开始吧！\\n### 先决条件\\n确保您已经[安装](/getting\\\\_started)了 `gradio` Python 包。\\n## 步骤 1 - 选择 Vision 图像分类模型\\n首先，我们需要一个图像分类模型。在本教程中，我们将使用[Hugging Face Model Hub](https://huggingface.co/models?pipeline\\\\_tag=image-classification)上的一个模型。该 Hub 包含数千个模型，涵盖了多种不同的机器学习任务。\\n在左侧边栏中展开 Tasks 类别，并选择我们感兴趣的“Image Classification”作为我们的任务。然后，您将看到 Hub 上为图像分类设计的所有模型。\\n在撰写时，最受欢迎的模型是 `google/vit-base-patch16-224`，该模型在分辨率为 224x224 像素的 ImageNet 图像上进行了训练。我们将在演示中使用此模型。\\n## 步骤 2 - 使用 Gradio 加载 Vision Transformer 模型\\n当使用 Hugging Face Hub 上的模型时，我们无需为演示定义输入或输出组件。同样，我们不需要关心预处理或后处理的细节。所有这些都可以从模型标签中自动推断出来。\\n除了导入语句外，我们只需要一行代码即可加载并启动演示。\\n我们使用 `gr.Interface.load()` 方法，并传入包含 `huggingface/` 的模型路径，以指定它来自 Hugging Face Hub。\\n```python\\nimport gradio as gr\\ngr.Interface.load(\\n\"huggingface/google/vit-base-patch16-224\",\\nexamples=[\"alligator.jpg\", \"laptop.jpg\"]).launch()\\n```\\n请注意，我们添加了一个 `examples` 参数，允许我们使用一些预定义的示例预填充我们的界面。\\n这将生成以下接口，您可以直接在浏览器中尝试。当您输入图像时，它会自动进行预处理并发送到 Hugging Face Hub API，通过模型处理，并以人类可解释的预测结果返回。尝试上传您自己的图像！\\n---\\n完成！只需一行代码，您就建立了一个图像分类器的 Web 演示。如果您想与他人分享，请在 `launch()` 接口时设置 `share=True`。\\n\\n--\\ntitle: \"Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\"\\nthumbnail: /blog/assets/97\\\\_vision\\\\_transformers/thumbnail.png\\nauthors:\\n- user: juliensimon\\n---\\n# Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\\nThis blog post will show how easy it is to fine-tune pre-trained Transformer models for your dataset using the Hugging Face Optimum library on Graphcore Intelligence Processing Units (IPUs). As an example, we will show a step-by-step guide and provide a notebook that takes a large, widely-used chest X-ray dataset and trains a vision transformer (ViT) model. ## Introducing vision transformer (ViT) models\\n\\nIn 2017 a group of Google AI researchers published a paper introducing the transformer model architecture. Characterised by a novel self-attention mechanism, transformers were proposed as a new and efficient group of models for language applications. Indeed, in the last five years, transformers have seen explosive popularity and are now accepted as the de facto standard for natural language processing (NLP). Transformers for language are perhaps most notably represented by the rapidly evolving GPT and BERT model families. Both can run easily and efficiently on Graphcore IPUs as part of the growing [Hugging Face Optimum Graphcore library](/posts/getting-started-with-hugging-face-transformers-for-ipus-with-optimum)). ![transformers_chrono](https://www.graphcore.ai/hs-fs/hubfs/transformers_chrono.png?width=1024&name=transformers_chrono.png)\\n\\nA timeline showing releases of prominent transformer language models (credit: Hugging Face)\\n\\nAn in-depth explainer about the transformer model architecture (with a focus on NLP) can be found [on the Hugging Face website](https://huggingface.co/course/chapter1/4?fw=pt).\\n\\nFor Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage. AutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate 🤗 Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details. ## The technical philosophy\\nIn this section, we wanted to share our philosophy behind adding support for Computer Vision in 🤗 Transformers so that the community is aware of the design choices specific to this area. Even though Transformers started with NLP, we support multiple modalities today, for example – vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:\\n- Easy model download with a single line of code with `from\\\\_pretrained()`\\n- Easy model upload with `push\\\\_to\\\\_hub()`\\n- Support for loading huge checkpoints with efficient checkpoint sharding techniques\\n- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))\\n- Initialization from model configurations\\n- Support for both PyTorch and TensorFlow (non-exhaustive)\\n- and many more\\nUnlike tokenizers, we have preprocessors (such as [this](https://huggingface.co/docs/transformers/model\\\\_doc/vit#transformers.ViTImageProcessor)) that take care of preparing data for the vision models. We have worked hard to ensure the user experience of using a vision model still feels easy and similar:\\n```py\\nfrom transformers import ViTImageProcessor, ViTForImageClassification\\nimport torch\\nfrom datasets import load\\\\_dataset\\ndataset = load\\\\_dataset(\"huggingface/cats-image\")\\nimage = dataset[\"test\"][\"image\"][0]\\nimage\\\\_processor  = ViTImageProcessor.from\\\\_pretrained(\"google/vit-base-patch16-224\")\\nmodel = ViTForImageClassification.from\\\\_pretrained(\"google/vit-base-patch16-224\")\\ninputs = image\\\\_processor(image, return\\\\_tensors=\"pt\")\\nwith torch.no\\\\_grad():\\nlogits = model(\\\\*\\\\*inputs).logits\\n# model predicts one of the 1000 ImageNet classes\\npredicted\\\\_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted\\\\_label])\\n# Egyptian cat\\n```\\nEven for a difficult task like object detection, the user experience doesn’t change very much:\\n```py\\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage\\\\_processor = AutoImageProcessor.from\\\\_pretrained(\"microsoft/conditional-detr-resnet-50\")\\nmodel = AutoModelForObjectDetection.from\\\\_pretrained(\"microsoft/conditional-detr-resnet-50\")\\ninputs = image\\\\_processor(images=image, return\\\\_tensors=\"pt\")\\noutputs = model(\\\\*\\\\*inputs)\\n# convert outputs (bounding boxes and class logits) to COCO API\\ntarget\\\\_sizes = torch.tensor([image.size[::-1]])\\nresults = image\\\\_processor.post\\\\_process\\\\_object\\\\_detection(\\noutputs, threshold=0.5, target\\\\_sizes=target\\\\_sizes\\n)[0]\\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\\nbox = [round(i, 2) for i in box.tolist()]\\nprint(\\nf\"Detected {model.config.id2label[label.item()]} with confidence \"\\nf\"{round(score.item(), 3)} at location {box}\"\\n)\\n```\\nLeads to:\\n```bash\\nDetected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\\nDetected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\\nDetected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\\nDetected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\\nDetected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\\n```\\n## Zero-shot models for vision\\nThere’s been a surge of models that reformulate core vision tasks like segmentation and detection in interesting ways and introduce even more flexibility.', 'type': 'tool', 'name': 'data_retriever', 'id': 'ab4fe598-f6c6-408f-8509-dfc5176813a2', 'tool_call_id': 'call_pCuL1xhX4C3iGpIbUHTYdhvn', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899ba9eaf3d2ebb270a0910', 'page_content': 'Vision Transformers 图像分类\\n相关空间：https://huggingface.co/spaces/abidlabs/vision-transformer\\n标签：VISION, TRANSFORMERS, HUB\\n## 简介\\n图像分类是计算机视觉中的重要任务。构建更好的分类器以确定图像中存在的对象是当前研究的热点领域，因为它在从人脸识别到制造质量控制等方面都有应用。\\n最先进的图像分类器基于 \\\\_transformers\\\\_ 架构，该架构最初在自然语言处理任务中很受欢迎。这种架构通常被称为 vision transformers (ViT)。这些模型非常适合与 Gradio 的\\\\*图像\\\\*输入组件一起使用，因此在本教程中，我们将构建一个使用 Gradio 进行图像分类的 Web 演示。我们只需用\\\\*\\\\*一行 Python 代码\\\\*\\\\*即可构建整个 Web 应用程序，其效果如下（试用一下示例之一！）：\\n让我们开始吧！\\n### 先决条件\\n确保您已经[安装](/getting\\\\_started)了 `gradio` Python 包。\\n## 步骤 1 - 选择 Vision 图像分类模型\\n首先，我们需要一个图像分类模型。在本教程中，我们将使用[Hugging Face Model Hub](https://huggingface.co/models?pipeline\\\\_tag=image-classification)上的一个模型。该 Hub 包含数千个模型，涵盖了多种不同的机器学习任务。\\n在左侧边栏中展开 Tasks 类别，并选择我们感兴趣的“Image Classification”作为我们的任务。然后，您将看到 Hub 上为图像分类设计的所有模型。\\n在撰写时，最受欢迎的模型是 `google/vit-base-patch16-224`，该模型在分辨率为 224x224 像素的 ImageNet 图像上进行了训练。我们将在演示中使用此模型。\\n## 步骤 2 - 使用 Gradio 加载 Vision Transformer 模型\\n当使用 Hugging Face Hub 上的模型时，我们无需为演示定义输入或输出组件。同样，我们不需要关心预处理或后处理的细节。所有这些都可以从模型标签中自动推断出来。\\n除了导入语句外，我们只需要一行代码即可加载并启动演示。\\n我们使用 `gr.Interface.load()` 方法，并传入包含 `huggingface/` 的模型路径，以指定它来自 Hugging Face Hub。\\n```python\\nimport gradio as gr\\ngr.Interface.load(\\n\"huggingface/google/vit-base-patch16-224\",\\nexamples=[\"alligator.jpg\", \"laptop.jpg\"]).launch()\\n```\\n请注意，我们添加了一个 `examples` 参数，允许我们使用一些预定义的示例预填充我们的界面。\\n这将生成以下接口，您可以直接在浏览器中尝试。当您输入图像时，它会自动进行预处理并发送到 Hugging Face Hub API，通过模型处理，并以人类可解释的预测结果返回。尝试上传您自己的图像！\\n---\\n完成！只需一行代码，您就建立了一个图像分类器的 Web 演示。如果您想与他人分享，请在 `launch()` 接口时设置 `share=True`。', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aaca53e684812d15f6106', 'page_content': '--\\ntitle: \"Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\"\\nthumbnail: /blog/assets/97\\\\_vision\\\\_transformers/thumbnail.png\\nauthors:\\n- user: juliensimon\\n---\\n# Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\\nThis blog post will show how easy it is to fine-tune pre-trained Transformer models for your dataset using the Hugging Face Optimum library on Graphcore Intelligence Processing Units (IPUs). As an example, we will show a step-by-step guide and provide a notebook that takes a large, widely-used chest X-ray dataset and trains a vision transformer (ViT) model. ## Introducing vision transformer (ViT) models\\n\\nIn 2017 a group of Google AI researchers published a paper introducing the transformer model architecture. Characterised by a novel self-attention mechanism, transformers were proposed as a new and efficient group of models for language applications. Indeed, in the last five years, transformers have seen explosive popularity and are now accepted as the de facto standard for natural language processing (NLP). Transformers for language are perhaps most notably represented by the rapidly evolving GPT and BERT model families. Both can run easily and efficiently on Graphcore IPUs as part of the growing [Hugging Face Optimum Graphcore library](/posts/getting-started-with-hugging-face-transformers-for-ipus-with-optimum)). ![transformers_chrono](https://www.graphcore.ai/hs-fs/hubfs/transformers_chrono.png?width=1024&name=transformers_chrono.png)\\n\\nA timeline showing releases of prominent transformer language models (credit: Hugging Face)\\n\\nAn in-depth explainer about the transformer model architecture (with a focus on NLP) can be found [on the Hugging Face website](https://huggingface.co/course/chapter1/4?fw=pt).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bf06666af48523f51d63', 'page_content': 'For Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage. AutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate 🤗 Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details. ## The technical philosophy\\nIn this section, we wanted to share our philosophy behind adding support for Computer Vision in 🤗 Transformers so that the community is aware of the design choices specific to this area. Even though Transformers started with NLP, we support multiple modalities today, for example – vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:\\n- Easy model download with a single line of code with `from\\\\_pretrained()`\\n- Easy model upload with `push\\\\_to\\\\_hub()`\\n- Support for loading huge checkpoints with efficient checkpoint sharding techniques\\n- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))\\n- Initialization from model configurations\\n- Support for both PyTorch and TensorFlow (non-exhaustive)\\n- and many more\\nUnlike tokenizers, we have preprocessors (such as [this](https://huggingface.co/docs/transformers/model\\\\_doc/vit#transformers.ViTImageProcessor)) that take care of preparing data for the vision models. We have worked hard to ensure the user experience of using a vision model still feels easy and similar:\\n```py\\nfrom transformers import ViTImageProcessor, ViTForImageClassification\\nimport torch\\nfrom datasets import load\\\\_dataset\\ndataset = load\\\\_dataset(\"huggingface/cats-image\")\\nimage = dataset[\"test\"][\"image\"][0]\\nimage\\\\_processor  = ViTImageProcessor.from\\\\_pretrained(\"google/vit-base-patch16-224\")\\nmodel = ViTForImageClassification.from\\\\_pretrained(\"google/vit-base-patch16-224\")\\ninputs = image\\\\_processor(image, return\\\\_tensors=\"pt\")\\nwith torch.no\\\\_grad():\\nlogits = model(\\\\*\\\\*inputs).logits\\n# model predicts one of the 1000 ImageNet classes\\npredicted\\\\_label = logits.argmax(-1).item()\\nprint(model.config.id2label[predicted\\\\_label])\\n# Egyptian cat\\n```\\nEven for a difficult task like object detection, the user experience doesn’t change very much:\\n```py\\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\\nfrom PIL import Image\\nimport requests\\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage\\\\_processor = AutoImageProcessor.from\\\\_pretrained(\"microsoft/conditional-detr-resnet-50\")\\nmodel = AutoModelForObjectDetection.from\\\\_pretrained(\"microsoft/conditional-detr-resnet-50\")\\ninputs = image\\\\_processor(images=image, return\\\\_tensors=\"pt\")\\noutputs = model(\\\\*\\\\*inputs)\\n# convert outputs (bounding boxes and class logits) to COCO API\\ntarget\\\\_sizes = torch.tensor([image.size[::-1]])\\nresults = image\\\\_processor.post\\\\_process\\\\_object\\\\_detection(\\noutputs, threshold=0.5, target\\\\_sizes=target\\\\_sizes\\n)[0]\\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\\nbox = [round(i, 2) for i in box.tolist()]\\nprint(\\nf\"Detected {model.config.id2label[label.item()]} with confidence \"\\nf\"{round(score.item(), 3)} at location {box}\"\\n)\\n```\\nLeads to:\\n```bash\\nDetected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\\nDetected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\\nDetected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\\nDetected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\\nDetected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\\n```\\n## Zero-shot models for vision\\nThere’s been a surge of models that reformulate core vision tasks like segmentation and detection in interesting ways and introduce even more flexibility.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '最受欢迎的 Vision Transformer 模型是 `google/vit-base-patch16-224`，该模型在分辨率为 224x224 像素的 ImageNet 图像上进行了训练。', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--aefadbe9-e0bd-4ad8-b7d9-107d8cf39ca7-0', 'usage_metadata': {'input_tokens': 2446, 'output_tokens': 45, 'total_tokens': 2491, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:39:41,523 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 89%|████████▉ | 58/65 [04:35<00:40,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the command to upload an ESPnet model to a Hugging Face repository?\\n', 'type': 'human', 'id': '8c6f20c3-978b-484f-a326-30c8d19b23ab'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_lP99WZYYdVemFrJebNipVG6d', 'function': {'arguments': '{\"query\":\"upload ESPnet model to Hugging Face repository\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--eef896b2-4060-41c1-8c20-de54bebdab03-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'upload ESPnet model to Hugging Face repository'}, 'id': 'call_lP99WZYYdVemFrJebNipVG6d', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 365, 'output_tokens': 24, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Using ESPnet at Hugging Face\\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks. ## Exploring ESPnet in the Hub\\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). All models on the Hub come up with useful features:\\n1. An automatically generated model card with a description, a training configuration, licenses and more. 2. Metadata tags that help for discoverability and contain information such as license, language and datasets. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png)\\n\\n## Using existing models\\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\\\_model\\\\_zoo)). If you\\'re interested in doing inference, different classes for different tasks have a `from\\\\_pretrained` method that allows loading models from the Hub.\\n\\nn this video, we\\'re going to understand how to manage a model repository on the HuggingFace model hub. In order to handle a repository, you should first have a Hugging Face account. A link to create a new account is available in the description. Once you are logged in, you can create a new repository by clicking on the \"New model\" option. You should be facing a similar modal to the following. In the \"Owner\" input, you can put either your own namespace or any of your organisations namespaces. The model name is the model identifier that will then be used to identify your model on your chosen namespace. The final choice is between public and private. Public models are accessible by anyone. This is the recommended, free option, as this makes your model easily accessible and shareable. The owners of your namespace are the only ones who can update and change your model. A more advanced option is the private option. In this case, only the owners of your namespace will have visibility over your model. Other users won\\'t know it exists and will not be able to use it. Let\\'s create a dummy model to play with. Once your model is created, comes the management of that model! Three tabs are available to you. You\\'re facing the first one, which is the model card page; this is the page used to showcase your model to the world.\\n\\nFrameworkSwitchCourse {fw} />\\n# Sharing pretrained models[[sharing-pretrained-models]]\\n{#if fw === \\'pt\\'}\\n{:else}\\n{/if}\\nIn the steps below, we\\'ll take a look at the easiest ways to share pretrained models to the 🤗 Hub. There are tools and utilities available that make it simple to share and update models directly on the Hub, which we will explore below. We encourage all users that train models to contribute by sharing them with the community — sharing models, even when trained on very specific datasets, will help others, saving them time and compute resources and providing access to useful trained artifacts. In turn, you can benefit from the work that others have done! There are three ways to go about creating new model repositories:\\n- Using the `push\\\\_to\\\\_hub` API\\n- Using the `huggingface\\\\_hub` Python library\\n- Using the web interface\\nOnce you\\'ve created a repository, you can upload files to it via git and git-lfs. We\\'ll walk you through creating model repositories and uploading files to them in the following sections. ## Using the `push\\\\_to\\\\_hub` API[[using-the-pushtohub-api]]\\n{#if fw === \\'pt\\'}\\n{:else}\\n{/if}\\nThe simplest way to upload files to the Hub is by leveraging the `push\\\\_to\\\\_hub` API.', 'type': 'tool', 'name': 'data_retriever', 'id': '4b5731fd-c6f0-4d58-8bf6-31e3eecb6673', 'tool_call_id': 'call_lP99WZYYdVemFrJebNipVG6d', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bdcc666af48523f51bb0', 'page_content': \"Using ESPnet at Hugging Face\\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks. ## Exploring ESPnet in the Hub\\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). All models on the Hub come up with useful features:\\n1. An automatically generated model card with a description, a training configuration, licenses and more. 2. Metadata tags that help for discoverability and contain information such as license, language and datasets. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png)\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png)\\n\\n## Using existing models\\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\\\_model\\\\_zoo)). If you're interested in doing inference, different classes for different tasks have a `from\\\\_pretrained` method that allows loading models from the Hub.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa5413e684812d15f5832', 'page_content': 'n this video, we\\'re going to understand how to manage a model repository on the HuggingFace model hub. In order to handle a repository, you should first have a Hugging Face account. A link to create a new account is available in the description. Once you are logged in, you can create a new repository by clicking on the \"New model\" option. You should be facing a similar modal to the following. In the \"Owner\" input, you can put either your own namespace or any of your organisations namespaces. The model name is the model identifier that will then be used to identify your model on your chosen namespace. The final choice is between public and private. Public models are accessible by anyone. This is the recommended, free option, as this makes your model easily accessible and shareable. The owners of your namespace are the only ones who can update and change your model. A more advanced option is the private option. In this case, only the owners of your namespace will have visibility over your model. Other users won\\'t know it exists and will not be able to use it. Let\\'s create a dummy model to play with. Once your model is created, comes the management of that model! Three tabs are available to you. You\\'re facing the first one, which is the model card page; this is the page used to showcase your model to the world.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b46caf3d2ebb270a01ad', 'page_content': \"FrameworkSwitchCourse {fw} />\\n# Sharing pretrained models[[sharing-pretrained-models]]\\n{#if fw === 'pt'}\\n{:else}\\n{/if}\\nIn the steps below, we'll take a look at the easiest ways to share pretrained models to the 🤗 Hub. There are tools and utilities available that make it simple to share and update models directly on the Hub, which we will explore below. We encourage all users that train models to contribute by sharing them with the community — sharing models, even when trained on very specific datasets, will help others, saving them time and compute resources and providing access to useful trained artifacts. In turn, you can benefit from the work that others have done! There are three ways to go about creating new model repositories:\\n- Using the `push\\\\_to\\\\_hub` API\\n- Using the `huggingface\\\\_hub` Python library\\n- Using the web interface\\nOnce you've created a repository, you can upload files to it via git and git-lfs. We'll walk you through creating model repositories and uploading files to them in the following sections. ## Using the `push\\\\_to\\\\_hub` API[[using-the-pushtohub-api]]\\n{#if fw === 'pt'}\\n{:else}\\n{/if}\\nThe simplest way to upload files to the Hub is by leveraging the `push\\\\_to\\\\_hub` API.\", 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The simplest way to upload files to the Hub is by leveraging the `push_to_hub` API. This command allows you to upload your ESPnet model to a Hugging Face repository. For detailed instructions on how to use this API, you can refer to the relevant sections in the documentation.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--d42a7d29-b28e-454e-9a1d-7b7c0612fcd5-0', 'usage_metadata': {'input_tokens': 1139, 'output_tokens': 59, 'total_tokens': 1198, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:39:46,331 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 91%|█████████ | 59/65 [04:40<00:32,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\\n', 'type': 'human', 'id': '0d75b05e-1bff-4e56-b249-da8023a15f49'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_kB9wZMMqrNScoO3jQ1NBThMb', 'function': {'arguments': '{\"query\":\"custom Python dependencies Inference Endpoints model repository file\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--80615cc3-e050-4f2f-925f-d1c86f064616-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'custom Python dependencies Inference Endpoints model repository file'}, 'id': 'call_kB9wZMMqrNScoO3jQ1NBThMb', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 368, 'output_tokens': 25, 'total_tokens': 393, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Add custom Dependencies\\nInference Endpoints’ base image includes all required libraries to run inference on 🤗 Transformers models, but it also supports custom dependencies. This is useful if you want to:\\n\\\\* [customize your inference pipeline](/docs/inference-endpoints/guides/custom\\\\_handler) and need additional Python dependencies\\n\\\\* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)). To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within. ```bash\\noptimum[onnxruntime]==1.2.3\\nmkl-include\\nmkl\\n```\\nCheck out the `requirements.txt` files in the following model repositories for examples:\\n\\\\* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\\n\\\\* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom\\\\_container) for inference.\\n\\nInference Endpoints Version\\nHugging Face Inference Endpoints comes with a default serving container which is used for all [supported Transformers and Sentence-Transformers tasks](/docs/inference-endpoints/supported\\\\_tasks) and for [custom inference handler](/docs/inference-endpoints/guides/custom\\\\_handler) and implement batching. Below you will find information about the installed packages and versions used. You can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository. Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom\\\\_dependencies). ## Installed packages & version\\nThe installed packages are split into `general`, `CPU` & `GPU` packages. The `general` packages are installed in all containers, the `CPU` and `GPU` packages are only installed in the corresponding containers. The Hugging Face Inference Runtime has separate versions for `PyTorch` and `TensorFlow` for `CPU` and `GPU`, which are used based on the selected `framework` when creating an Inference Endpoint. The `TensorFlow` and `PyTorch` flavors are grouped together in the list below. ### General\\n- `Python`: `3.9.13`\\n### CPU\\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\\n- `diffusers`: `0.14.0`\\n- `accelerate`: `0.17.1`\\n- `sentence\\\\_transformers`: `latest`\\n- `pandas`: `latest`\\n- `pytorch`: `1.13.1`\\n- `torchvision`: `0.14.1`\\n- `tensorflow`: `2.9.1`\\n### GPU\\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\\n- `diffusers`: `0.14.0`\\n- `accelerate`: `0.17.1`\\n- `sentence\\\\_transformers`: `latest`\\n- `pandas`: `latest`\\n- `pytorch`: `1.13.1=py3.9\\\\_cuda11.8\\\\*`\\n- `torchvision`: `0.14.1`\\n- `tensorflow`: `2.9.1=\\\\*cuda112\\\\*py39\\\\*`\\n\\nIt contains information about a deployed `InferenceEndpoint`, including its configuration and current state. Once deployed, you can run inference on the Endpoint using the [`InferenceEndpoint.client`] and [`InferenceEndpoint.async\\\\_client`] properties that respectively return an [`InferenceClient`] and an [`AsyncInferenceClient`] object. [[autodoc]] InferenceEndpoint\\n- from\\\\_raw\\n- client\\n- async\\\\_client\\n- all\\n## InferenceEndpointStatus\\n[[autodoc]] InferenceEndpointStatus\\n## InferenceEndpointType\\n[[autodoc]] InferenceEndpointType\\n## InferenceEndpointError\\n[[autodoc]] InferenceEndpointError', 'type': 'tool', 'name': 'data_retriever', 'id': '175cc4c8-7673-431a-a1be-ba1e5e637fdc', 'tool_call_id': 'call_kB9wZMMqrNScoO3jQ1NBThMb', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab8d53e684812d15f6f23', 'page_content': 'Add custom Dependencies\\nInference Endpoints’ base image includes all required libraries to run inference on 🤗 Transformers models, but it also supports custom dependencies. This is useful if you want to:\\n\\\\* [customize your inference pipeline](/docs/inference-endpoints/guides/custom\\\\_handler) and need additional Python dependencies\\n\\\\* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)). To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within. ```bash\\noptimum[onnxruntime]==1.2.3\\nmkl-include\\nmkl\\n```\\nCheck out the `requirements.txt` files in the following model repositories for examples:\\n\\\\* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\\n\\\\* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom\\\\_container) for inference.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa87b3e684812d15f5c02', 'page_content': 'Inference Endpoints Version\\nHugging Face Inference Endpoints comes with a default serving container which is used for all [supported Transformers and Sentence-Transformers tasks](/docs/inference-endpoints/supported\\\\_tasks) and for [custom inference handler](/docs/inference-endpoints/guides/custom\\\\_handler) and implement batching. Below you will find information about the installed packages and versions used. You can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository. Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom\\\\_dependencies). ## Installed packages & version\\nThe installed packages are split into `general`, `CPU` & `GPU` packages. The `general` packages are installed in all containers, the `CPU` and `GPU` packages are only installed in the corresponding containers. The Hugging Face Inference Runtime has separate versions for `PyTorch` and `TensorFlow` for `CPU` and `GPU`, which are used based on the selected `framework` when creating an Inference Endpoint. The `TensorFlow` and `PyTorch` flavors are grouped together in the list below. ### General\\n- `Python`: `3.9.13`\\n### CPU\\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\\n- `diffusers`: `0.14.0`\\n- `accelerate`: `0.17.1`\\n- `sentence\\\\_transformers`: `latest`\\n- `pandas`: `latest`\\n- `pytorch`: `1.13.1`\\n- `torchvision`: `0.14.1`\\n- `tensorflow`: `2.9.1`\\n### GPU\\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\\n- `diffusers`: `0.14.0`\\n- `accelerate`: `0.17.1`\\n- `sentence\\\\_transformers`: `latest`\\n- `pandas`: `latest`\\n- `pytorch`: `1.13.1=py3.9\\\\_cuda11.8\\\\*`\\n- `torchvision`: `0.14.1`\\n- `tensorflow`: `2.9.1=\\\\*cuda112\\\\*py39\\\\*`', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bc90af3d2ebb270a0b2f', 'page_content': 'It contains information about a deployed `InferenceEndpoint`, including its configuration and current state. Once deployed, you can run inference on the Endpoint using the [`InferenceEndpoint.client`] and [`InferenceEndpoint.async\\\\_client`] properties that respectively return an [`InferenceClient`] and an [`AsyncInferenceClient`] object. [[autodoc]] InferenceEndpoint\\n- from\\\\_raw\\n- client\\n- async\\\\_client\\n- all\\n## InferenceEndpointStatus\\n[[autodoc]] InferenceEndpointStatus\\n## InferenceEndpointType\\n[[autodoc]] InferenceEndpointType\\n## InferenceEndpointError\\n[[autodoc]] InferenceEndpointError', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'To install custom Python dependencies for Inference Endpoints, you should add a `requirements.txt` file to your model repository on the Hugging Face Hub. This file should list the Python dependencies you want to install.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--b4001439-3063-4baa-9bc6-a8676d73e160-0', 'usage_metadata': {'input_tokens': 1246, 'output_tokens': 43, 'total_tokens': 1289, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:39:51,842 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 92%|█████████▏| 60/65 [04:46<00:27,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\\n', 'type': 'human', 'id': 'ca8266e6-6aec-4082-bcb3-0ab415c05308'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_87FuDcXIySr6m2lgt0DO5xno', 'function': {'arguments': '{\"query\":\"Textual Inversion Stable Diffusion how many images needed\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--faca8d4d-1252-48da-a64f-27bb89ab8086-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Textual Inversion Stable Diffusion how many images needed'}, 'id': 'call_87FuDcXIySr6m2lgt0DO5xno', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 367, 'output_tokens': 26, 'total_tokens': 393, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '# Textual Inversion fine-tuning example\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\\nColab for training\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\\\_textual\\\\_inversion\\\\_training.ipynb)\\nColab for inference\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\\\_conceptualizer\\\\_inference.ipynb)\\n## Running locally with PyTorch\\n### Installing the dependencies\\nBefore running the scripts, make sure to install the library\\'s training dependencies:\\n\\\\*\\\\*Important\\\\*\\\\*\\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\\\*\\\\*installing from source\\\\*\\\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements.\\n\\nDistillation for quantization on Textual Inversion models to personalize text2image\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images.\\\\_By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images\\\\_\\nThe `textual\\\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. We have enabled distillation for quantization in `textual\\\\_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method. ## Installing the dependencies\\nBefore running the scripts, make sure to install the library\\'s training dependencies:\\n```bash\\npip install -r requirements.txt\\n```\\n## Prepare Datasets\\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\\n[![](https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg)](https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg)\\n## Get a FP32 Textual Inversion model\\nUse the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model. ```bash\\nexport MODEL\\\\_NAME=\"CompVis/stable-diffusion-v1-4\"\\nexport DATA\\\\_DIR=\"./dicoo\"\\naccelerate launch textual\\\\_inversion.py \\\\\\n--pretrained\\\\_model\\\\_name\\\\_or\\\\_path=$MODEL\\\\_NAME \\\\\\n--train\\\\_data\\\\_dir=$DATA\\\\_DIR \\\\\\n--learnable\\\\_property=\"object\" \\\\\\n--placeholder\\\\_token=\"\" --initializer\\\\_token=\"toy\" \\\\\\n--resolution=512 \\\\\\n--train\\\\_batch\\\\_size=1 \\\\\\n--gradient\\\\_accumulation\\\\_steps=4 \\\\\\n--max\\\\_train\\\\_steps=3000 \\\\\\n--learning\\\\_rate=5.0e-04 --scale\\\\_lr \\\\\\n--lr\\\\_scheduler=\"constant\" \\\\\\n--lr\\\\_warmup\\\\_steps=0 \\\\\\n--output\\\\_dir=\"dicoo\\\\_model\"\\n```\\n## Do distillation for quantization\\nDistillation for quantization is a method that combines [intermediate layer knowledge distillation](https://github.com/intel/neural-compressor/blob/master/docs/source/distillation.md#intermediate-layer-knowledge-distillation) and [quantization aware training](https://github.com/intel/neural-compressor/blob/master/docs/source/quantization.md#quantization-aware-training) in the same training process to improve the performance of the quantized model.\\n\\n# Textual Inversion fine-tuning example\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Training with Intel Extension for PyTorch\\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual\\\\_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual\\\\_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script. The example supports both single node and multi-node distributed training:\\n### Single node training\\n```bash\\nexport MODEL\\\\_NAME=\"CompVis/stable-diffusion-v1-4\"\\nexport DATA\\\\_DIR=\"path-to-dir-containing-dicoo-images\"\\npython textual\\\\_inversion.py \\\\\\n--pretrained\\\\_model\\\\_name\\\\_or\\\\_path=$MODEL\\\\_NAME \\\\\\n--train\\\\_data\\\\_dir=$DATA\\\\_DIR \\\\\\n--learnable\\\\_property=\"object\" \\\\\\n--placeholder\\\\_token=\"\" --initializer\\\\_token=\"toy\" \\\\\\n--seed=7 \\\\\\n--resolution=512 \\\\\\n--train\\\\_batch\\\\_size=1 \\\\\\n--gradient\\\\_accumulation\\\\_steps=1 \\\\\\n--max\\\\_train\\\\_steps=3000 \\\\\\n--learning\\\\_rate=2.5e-03 --scale\\\\_lr \\\\\\n--output\\\\_dir=\"textual\\\\_inversion\\\\_dicoo\"\\n```\\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. You may not get performance speedup without Bfloat16 support. ### Multi-node distributed training\\nBefore running the scripts, make sure to install the library\\'s training dependencies successfully:\\n```bash\\npython -m pip install oneccl\\\\_bind\\\\_pt==1.13 -f https://developer.intel.com/ipex-whl-stable-cpu\\n```\\n```bash\\nexport MODEL\\\\_NAME=\"CompVis/stable-diffusion-v1-4\"\\nexport DATA\\\\_DIR=\"path-to-dir-containing-dicoo-images\"\\noneccl\\\\_bindings\\\\_for\\\\_pytorch\\\\_path=$(python -c \"from oneccl\\\\_bindings\\\\_for\\\\_pytorch import cwd; print(cwd)\")\\nsource $oneccl\\\\_bindings\\\\_for\\\\_pytorch\\\\_path/env/setvars.sh\\npython -m intel\\\\_extension\\\\_for\\\\_pytorch.cpu.launch --distributed \\\\\\n--hostfile hostfile --nnodes 2 --nproc\\\\_per\\\\_node 2 textual\\\\_inversion.py \\\\\\n--pretrained\\\\_model\\\\_name\\\\_or\\\\_path=$MODEL\\\\_NAME \\\\\\n--train\\\\_data\\\\_dir=$DATA\\\\_DIR \\\\\\n--learnable\\\\_property=\"object\" \\\\\\n--placeholder\\\\_token=\"\" --initializer\\\\_token=\"toy\" \\\\\\n--seed=7 \\\\\\n--resolution=512 \\\\\\n--train\\\\_batch\\\\_size=1 \\\\\\n--gradient\\\\_accumulation\\\\_steps=1 \\\\\\n--max\\\\_train\\\\_steps=750 \\\\\\n--learning\\\\_rate=2.5e-03 --scale\\\\_lr \\\\\\n--output\\\\_dir=\"textual\\\\_inversion\\\\_dicoo\"\\n```\\nThe above is a simple distributed training usage on 2 nodes with 2 processes on each node.', 'type': 'tool', 'name': 'data_retriever', 'id': 'dee4e183-408a-463b-971a-1ba19b1e524b', 'tool_call_id': 'call_87FuDcXIySr6m2lgt0DO5xno', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab3403e684812d15f6873', 'page_content': \"# Textual Inversion fine-tuning example\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\\nColab for training\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\\\_textual\\\\_inversion\\\\_training.ipynb)\\nColab for inference\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\\\_conceptualizer\\\\_inference.ipynb)\\n## Running locally with PyTorch\\n### Installing the dependencies\\nBefore running the scripts, make sure to install the library's training dependencies:\\n\\\\*\\\\*Important\\\\*\\\\*\\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\\\*\\\\*installing from source\\\\*\\\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aac8f3e684812d15f60e9', 'page_content': 'Distillation for quantization on Textual Inversion models to personalize text2image\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images.\\\\_By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images\\\\_\\nThe `textual\\\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. We have enabled distillation for quantization in `textual\\\\_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method. ## Installing the dependencies\\nBefore running the scripts, make sure to install the library\\'s training dependencies:\\n```bash\\npip install -r requirements.txt\\n```\\n## Prepare Datasets\\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\\n[![](https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg)](https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg)\\n## Get a FP32 Textual Inversion model\\nUse the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model. ```bash\\nexport MODEL\\\\_NAME=\"CompVis/stable-diffusion-v1-4\"\\nexport DATA\\\\_DIR=\"./dicoo\"\\naccelerate launch textual\\\\_inversion.py \\\\\\n--pretrained\\\\_model\\\\_name\\\\_or\\\\_path=$MODEL\\\\_NAME \\\\\\n--train\\\\_data\\\\_dir=$DATA\\\\_DIR \\\\\\n--learnable\\\\_property=\"object\" \\\\\\n--placeholder\\\\_token=\"\" --initializer\\\\_token=\"toy\" \\\\\\n--resolution=512 \\\\\\n--train\\\\_batch\\\\_size=1 \\\\\\n--gradient\\\\_accumulation\\\\_steps=4 \\\\\\n--max\\\\_train\\\\_steps=3000 \\\\\\n--learning\\\\_rate=5.0e-04 --scale\\\\_lr \\\\\\n--lr\\\\_scheduler=\"constant\" \\\\\\n--lr\\\\_warmup\\\\_steps=0 \\\\\\n--output\\\\_dir=\"dicoo\\\\_model\"\\n```\\n## Do distillation for quantization\\nDistillation for quantization is a method that combines [intermediate layer knowledge distillation](https://github.com/intel/neural-compressor/blob/master/docs/source/distillation.md#intermediate-layer-knowledge-distillation) and [quantization aware training](https://github.com/intel/neural-compressor/blob/master/docs/source/quantization.md#quantization-aware-training) in the same training process to improve the performance of the quantized model.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899ba3baf3d2ebb270a0884', 'page_content': '# Textual Inversion fine-tuning example\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Training with Intel Extension for PyTorch\\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual\\\\_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual\\\\_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script. The example supports both single node and multi-node distributed training:\\n### Single node training\\n```bash\\nexport MODEL\\\\_NAME=\"CompVis/stable-diffusion-v1-4\"\\nexport DATA\\\\_DIR=\"path-to-dir-containing-dicoo-images\"\\npython textual\\\\_inversion.py \\\\\\n--pretrained\\\\_model\\\\_name\\\\_or\\\\_path=$MODEL\\\\_NAME \\\\\\n--train\\\\_data\\\\_dir=$DATA\\\\_DIR \\\\\\n--learnable\\\\_property=\"object\" \\\\\\n--placeholder\\\\_token=\"\" --initializer\\\\_token=\"toy\" \\\\\\n--seed=7 \\\\\\n--resolution=512 \\\\\\n--train\\\\_batch\\\\_size=1 \\\\\\n--gradient\\\\_accumulation\\\\_steps=1 \\\\\\n--max\\\\_train\\\\_steps=3000 \\\\\\n--learning\\\\_rate=2.5e-03 --scale\\\\_lr \\\\\\n--output\\\\_dir=\"textual\\\\_inversion\\\\_dicoo\"\\n```\\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. You may not get performance speedup without Bfloat16 support. ### Multi-node distributed training\\nBefore running the scripts, make sure to install the library\\'s training dependencies successfully:\\n```bash\\npython -m pip install oneccl\\\\_bind\\\\_pt==1.13 -f https://developer.intel.com/ipex-whl-stable-cpu\\n```\\n```bash\\nexport MODEL\\\\_NAME=\"CompVis/stable-diffusion-v1-4\"\\nexport DATA\\\\_DIR=\"path-to-dir-containing-dicoo-images\"\\noneccl\\\\_bindings\\\\_for\\\\_pytorch\\\\_path=$(python -c \"from oneccl\\\\_bindings\\\\_for\\\\_pytorch import cwd; print(cwd)\")\\nsource $oneccl\\\\_bindings\\\\_for\\\\_pytorch\\\\_path/env/setvars.sh\\npython -m intel\\\\_extension\\\\_for\\\\_pytorch.cpu.launch --distributed \\\\\\n--hostfile hostfile --nnodes 2 --nproc\\\\_per\\\\_node 2 textual\\\\_inversion.py \\\\\\n--pretrained\\\\_model\\\\_name\\\\_or\\\\_path=$MODEL\\\\_NAME \\\\\\n--train\\\\_data\\\\_dir=$DATA\\\\_DIR \\\\\\n--learnable\\\\_property=\"object\" \\\\\\n--placeholder\\\\_token=\"\" --initializer\\\\_token=\"toy\" \\\\\\n--seed=7 \\\\\\n--resolution=512 \\\\\\n--train\\\\_batch\\\\_size=1 \\\\\\n--gradient\\\\_accumulation\\\\_steps=1 \\\\\\n--max\\\\_train\\\\_steps=750 \\\\\\n--learning\\\\_rate=2.5e-03 --scale\\\\_lr \\\\\\n--output\\\\_dir=\"textual\\\\_inversion\\\\_dicoo\"\\n```\\nThe above is a simple distributed training usage on 2 nodes with 2 processes on each node.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'To teach new concepts to Stable Diffusion using Textual Inversion, you need just 3-5 images.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--39568f8e-3a4b-4b5f-8ba6-bb20e6d1c45c-0', 'usage_metadata': {'input_tokens': 1987, 'output_tokens': 23, 'total_tokens': 2010, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:39:57,807 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 94%|█████████▍| 61/65 [04:52<00:22,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\\n', 'type': 'human', 'id': 'cb601bfd-5ca5-4429-b0a8-5fbeb159ed85'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_f1y1UKUPVyAhmmak5UHJx2Ou', 'function': {'arguments': '{\"query\":\"maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--cc92cb97-eb9b-4bf5-bffe-40554d972ffa-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0'}, 'id': 'call_f1y1UKUPVyAhmmak5UHJx2Ou', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 373, 'output_tokens': 36, 'total_tokens': 409, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future. ## Sharded checkpoints\\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save\\\\_pretrained(save\\\\_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in. You can control the maximum size before sharding with the `max\\\\_shard\\\\_size` parameter, so for the sake of an example, we\\'ll use a normal-size models with a small shard size: let\\'s take a traditional BERT model. ```py\\nfrom transformers import AutoModel\\nmodel = AutoModel.from\\\\_pretrained(\"bert-base-cased\")\\n```\\nIf you save it using [`~PreTrainedModel.save\\\\_pretrained`], you will get a new folder with two files: the config of the model and its weights:\\n```py\\n>>> import os\\n>>> import tempfile\\n>>> with tempfile.TemporaryDirectory() as tmp\\\\_dir:\\n... model.save\\\\_pretrained(tmp\\\\_dir)\\n... print(sorted(os.listdir(tmp\\\\_dir)))\\n[\\'config.json\\', \\'pytorch\\\\_model.bin\\']\\n```\\nNow let\\'s use a maximum shard size of 200MB:\\n```py\\n>>> with tempfile.TemporaryDirectory() as tmp\\\\_dir:\\n... model.save\\\\_pretrained(tmp\\\\_dir, max\\\\_shard\\\\_size=\"200MB\")\\n... print(sorted(os.listdir(tmp\\\\_dir)))\\n[\\'config.json\\', \\'pytorch\\\\_model-00001-of-00003.bin\\', \\'pytorch\\\\_model-00002-of-00003.bin\\', \\'pytorch\\\\_model-00003-of-00003.bin\\', \\'pytorch\\\\_model.bin.index.json\\']\\n```\\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index.', 'type': 'tool', 'name': 'data_retriever', 'id': '39feba50-6075-4b53-b043-55750eeb75b9', 'tool_call_id': 'call_f1y1UKUPVyAhmmak5UHJx2Ou', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa9b23e684812d15f5d3f', 'page_content': 'In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future. ## Sharded checkpoints\\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save\\\\_pretrained(save\\\\_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in. You can control the maximum size before sharding with the `max\\\\_shard\\\\_size` parameter, so for the sake of an example, we\\'ll use a normal-size models with a small shard size: let\\'s take a traditional BERT model. ```py\\nfrom transformers import AutoModel\\nmodel = AutoModel.from\\\\_pretrained(\"bert-base-cased\")\\n```\\nIf you save it using [`~PreTrainedModel.save\\\\_pretrained`], you will get a new folder with two files: the config of the model and its weights:\\n```py\\n>>> import os\\n>>> import tempfile\\n>>> with tempfile.TemporaryDirectory() as tmp\\\\_dir:\\n... model.save\\\\_pretrained(tmp\\\\_dir)\\n... print(sorted(os.listdir(tmp\\\\_dir)))\\n[\\'config.json\\', \\'pytorch\\\\_model.bin\\']\\n```\\nNow let\\'s use a maximum shard size of 200MB:\\n```py\\n>>> with tempfile.TemporaryDirectory() as tmp\\\\_dir:\\n... model.save\\\\_pretrained(tmp\\\\_dir, max\\\\_shard\\\\_size=\"200MB\")\\n... print(sorted(os.listdir(tmp\\\\_dir)))\\n[\\'config.json\\', \\'pytorch\\\\_model-00001-of-00003.bin\\', \\'pytorch\\\\_model-00002-of-00003.bin\\', \\'pytorch\\\\_model-00003-of-00003.bin\\', \\'pytorch\\\\_model.bin.index.json\\']\\n```\\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'In Transformers version 4.18.0, the maximum size of a model checkpoint before it is automatically sharded is 10GB.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--cf81cbcd-1575-40c3-b742-3db41c909ca2-0', 'usage_metadata': {'input_tokens': 741, 'output_tokens': 28, 'total_tokens': 769, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:40:04,400 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 95%|█████████▌| 62/65 [04:58<00:17,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\\n', 'type': 'human', 'id': '48abb59d-2202-4365-8d8b-2968ecd1eb9e'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_jwTDCuCqZeyjYUkAUlRWH5P8', 'function': {'arguments': '{\"query\":\"Weights and Biases W&B purpose for data scientists and machine learning scientists\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--ca46f320-a5b1-490e-8719-db4f0f7834b6-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Weights and Biases W&B purpose for data scientists and machine learning scientists'}, 'id': 'call_jwTDCuCqZeyjYUkAUlRWH5P8', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 370, 'output_tokens': 29, 'total_tokens': 399, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'The company designs and develops software, systems, solutions and products to solve some of the most pressing challenges facing the Intelligence Community. #### \\\\*\\\\*1. How has ML made a positive impact on Engineering?\\\\*\\\\*\\nEngineering is vast in its applications and can encompass a great many areas. That said, more recently, we are seeing ML affect a range of engineering facets addressing obvious fields such as robotics and automobile engineering to not-so-obvious fields such as chemical and civil engineering. ML is so broad in its application that merely the very existence of training data consisting of prior recorded labor processes is all required to attempt to have ML affect your bottom line. In essence, we are in an age where ML has significantly impacted the automation of all sorts of previously human-only-operated engineering processes. #### \\\\*\\\\*2. What are the biggest ML challenges within Engineering?\\\\*\\\\*\\n1. The biggest challenges come with the operationalization and deployment of ML-trained solutions in a manner in which human operations can be replaced with minimal consequences. We’re seeing it now with fully self-driving automobiles. It’s challenging to automate processes with little to no fear of jeopardizing humans or processes that humans rely on. One of the most significant examples of this phenomenon that concerns me is ML and Bias. It is a reality that ML models trained on data containing, even if unaware, prejudiced decision-making can reproduce said bias in operation. Bias needs to be put front and center in the attempt to incorporate ML into engineering such that systemic racism isn’t propagated into future technological advances to then cause harm to disadvantaged populations. ML systems trained on data emanating from biased processes are doomed to repeat them, mainly if those training the ML solutions aren’t acutely aware of all forms of data present in the process to be automated.\\n\\nAs in the examples above, some common steps that may help decide whether and how to apply ML in a way that minimizes bias-related risk include:\\n\\\\* Investigate:\\n\\\\* Reports of bias in the field pre-ML\\n\\\\* At-risk demographic categories for your specific use case\\n\\\\* Examine:\\n\\\\* The impact of your optimization objective on reinforcing biases\\n\\\\* Alternative objectives that favor diversity and positive long-term impacts\\n### I am curating/picking a dataset for my ML system, how can I address bias? While training datasets are [not the sole source of bias](https://www.cell.com/patterns/fulltext/S2666-3899(21)00061-1) in the ML development cycle, they do play a significant role. Does your [dataset disproportionately associate](https://aclanthology.org/2020.emnlp-main.23/) biographies of women with life events but those of men with achievements? Those \\\\*\\\\*stereotypes\\\\*\\\\* are probably going to show up in your full ML system! Does your voice recognition dataset only feature specific accents? Not a good sign for [the inclusivity of technology](https://www.scientificamerican.com/article/speech-recognition-tech-is-yet-another-example-of-bias/) you build with it in terms of \\\\*\\\\*disparate performance\\\\*\\\\*! Whether you’re curating a dataset for ML applications or selecting a dataset to train an ML model, finding out, mitigating, and [communicating](https://dl.acm.org/doi/10.1145/3479582) to what extent the data exhibits these phenomena are all necessary steps to reducing bias-related risks. You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it comes from, who are the people represented on the data, and what the curation process was. Several frameworks for this reflection and documentation have been proposed such as [Data Statements for NLP](https://direct.mit.edu/tacl/article/doi/10.1162/tacl\\\\_a\\\\_00041/43452/Data-Statements-for-Natural-Language-Processing) or [Datasheets for Datasets](https://dl.acm.org/doi/10.1145/3458723). The Hugging Face Hub includes a Dataset Card [template](https://github.com/huggingface/datasets/blob/main/templates/README.md) and [guide](https://github.com/huggingface/datasets/blob/main/templates/README\\\\_guide.md#dataset-card-creation-guide) inspired by these works; the section on [considerations for using the data](https://github.com/huggingface/datasets/blob/main/templates/README\\\\_guide.md#considerations-for-using-the-data) is usually a good place to look for information about notable biases if you’re browsing datasets, or to write a paragraph sharing your insights on the topic if you’re sharing a new one. And if you’re looking for more inspiration on what to put there, check out these sections written by Hub users in the [BigLAM organization](https://huggingface.co/biglam) for historical datasets of [legal proceedings](https://huggingface.co/datasets/biglam/old\\\\_bailey\\\\_proceedings#social-impact-of-dataset), [image classification](https://huggingface.co/datasets/biglam/brill\\\\_iconclass#social-impact-of-dataset), and [newspapers](https://huggingface.co/datasets/biglam/bnl\\\\_newspapers1841-1879#social-impact-of-dataset). ![HF Dataset Card guide for the Social Impact and Bias Sections](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img3.png)\\n*[HF Dataset Card guide](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md#social-impact-of-dataset) for the Social Impact and Bias Sections*\\n\\nWhile describing the origin and context of a dataset is always a good starting point to understand the biases at play, [quantitatively measuring phenomena](https://arxiv.org/abs/2212.05129) that encode those biases can be just as helpful.\\n\\nWhether and to what extent bias in the system concretely affects people ultimately depends on what the system is used for. As such, the first place developers can work to mitigate bias is when deciding how ML fits in their system, e.g., by deciding what optimization objective it will use. For example, let’s go back to one of the first highly-publicized cases of a Machine Learning system used in production for algorithmic content recommendation. From 2006 to 2009, Netflix ran the [Netflix Prize](https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/The-Netflix-Prize-Bennett.pdf), a competition with a 1M$ cash prize challenging teams around the world to develop ML systems to accurately predict a user’s rating for a new movie based on their past ratings. The [winning submission](https://www.asc.ohio-state.edu/statistics/dmsl/GrandPrize2009\\\\_BPC\\\\_BigChaos.pdf) improved the RMSE (Root-mean-square-error) of predictions on unseen user-movie pairs by over 10% over Netflix’s own CineMatch algorithm, meaning it got much better at predicting how users would rate a new movie based on their history.', 'type': 'tool', 'name': 'data_retriever', 'id': '22b1b9b3-273f-4fef-b6db-743bd763eb77', 'tool_call_id': 'call_jwTDCuCqZeyjYUkAUlRWH5P8', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa7ae3e684812d15f5b1d', 'page_content': 'The company designs and develops software, systems, solutions and products to solve some of the most pressing challenges facing the Intelligence Community. #### \\\\*\\\\*1. How has ML made a positive impact on Engineering?\\\\*\\\\*\\nEngineering is vast in its applications and can encompass a great many areas. That said, more recently, we are seeing ML affect a range of engineering facets addressing obvious fields such as robotics and automobile engineering to not-so-obvious fields such as chemical and civil engineering. ML is so broad in its application that merely the very existence of training data consisting of prior recorded labor processes is all required to attempt to have ML affect your bottom line. In essence, we are in an age where ML has significantly impacted the automation of all sorts of previously human-only-operated engineering processes. #### \\\\*\\\\*2. What are the biggest ML challenges within Engineering?\\\\*\\\\*\\n1. The biggest challenges come with the operationalization and deployment of ML-trained solutions in a manner in which human operations can be replaced with minimal consequences. We’re seeing it now with fully self-driving automobiles. It’s challenging to automate processes with little to no fear of jeopardizing humans or processes that humans rely on. One of the most significant examples of this phenomenon that concerns me is ML and Bias. It is a reality that ML models trained on data containing, even if unaware, prejudiced decision-making can reproduce said bias in operation. Bias needs to be put front and center in the attempt to incorporate ML into engineering such that systemic racism isn’t propagated into future technological advances to then cause harm to disadvantaged populations. ML systems trained on data emanating from biased processes are doomed to repeat them, mainly if those training the ML solutions aren’t acutely aware of all forms of data present in the process to be automated.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c1e1666af48523f520aa', 'page_content': 'As in the examples above, some common steps that may help decide whether and how to apply ML in a way that minimizes bias-related risk include:\\n\\\\* Investigate:\\n\\\\* Reports of bias in the field pre-ML\\n\\\\* At-risk demographic categories for your specific use case\\n\\\\* Examine:\\n\\\\* The impact of your optimization objective on reinforcing biases\\n\\\\* Alternative objectives that favor diversity and positive long-term impacts\\n### I am curating/picking a dataset for my ML system, how can I address bias? While training datasets are [not the sole source of bias](https://www.cell.com/patterns/fulltext/S2666-3899(21)00061-1) in the ML development cycle, they do play a significant role. Does your [dataset disproportionately associate](https://aclanthology.org/2020.emnlp-main.23/) biographies of women with life events but those of men with achievements? Those \\\\*\\\\*stereotypes\\\\*\\\\* are probably going to show up in your full ML system! Does your voice recognition dataset only feature specific accents? Not a good sign for [the inclusivity of technology](https://www.scientificamerican.com/article/speech-recognition-tech-is-yet-another-example-of-bias/) you build with it in terms of \\\\*\\\\*disparate performance\\\\*\\\\*! Whether you’re curating a dataset for ML applications or selecting a dataset to train an ML model, finding out, mitigating, and [communicating](https://dl.acm.org/doi/10.1145/3479582) to what extent the data exhibits these phenomena are all necessary steps to reducing bias-related risks. You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it comes from, who are the people represented on the data, and what the curation process was. Several frameworks for this reflection and documentation have been proposed such as [Data Statements for NLP](https://direct.mit.edu/tacl/article/doi/10.1162/tacl\\\\_a\\\\_00041/43452/Data-Statements-for-Natural-Language-Processing) or [Datasheets for Datasets](https://dl.acm.org/doi/10.1145/3458723). The Hugging Face Hub includes a Dataset Card [template](https://github.com/huggingface/datasets/blob/main/templates/README.md) and [guide](https://github.com/huggingface/datasets/blob/main/templates/README\\\\_guide.md#dataset-card-creation-guide) inspired by these works; the section on [considerations for using the data](https://github.com/huggingface/datasets/blob/main/templates/README\\\\_guide.md#considerations-for-using-the-data) is usually a good place to look for information about notable biases if you’re browsing datasets, or to write a paragraph sharing your insights on the topic if you’re sharing a new one. And if you’re looking for more inspiration on what to put there, check out these sections written by Hub users in the [BigLAM organization](https://huggingface.co/biglam) for historical datasets of [legal proceedings](https://huggingface.co/datasets/biglam/old\\\\_bailey\\\\_proceedings#social-impact-of-dataset), [image classification](https://huggingface.co/datasets/biglam/brill\\\\_iconclass#social-impact-of-dataset), and [newspapers](https://huggingface.co/datasets/biglam/bnl\\\\_newspapers1841-1879#social-impact-of-dataset). ![HF Dataset Card guide for the Social Impact and Bias Sections](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img3.png)\\n*[HF Dataset Card guide](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md#social-impact-of-dataset) for the Social Impact and Bias Sections*\\n\\nWhile describing the origin and context of a dataset is always a good starting point to understand the biases at play, [quantitatively measuring phenomena](https://arxiv.org/abs/2212.05129) that encode those biases can be just as helpful.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c1e1666af48523f520a6', 'page_content': 'Whether and to what extent bias in the system concretely affects people ultimately depends on what the system is used for. As such, the first place developers can work to mitigate bias is when deciding how ML fits in their system, e.g., by deciding what optimization objective it will use. For example, let’s go back to one of the first highly-publicized cases of a Machine Learning system used in production for algorithmic content recommendation. From 2006 to 2009, Netflix ran the [Netflix Prize](https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/The-Netflix-Prize-Bennett.pdf), a competition with a 1M$ cash prize challenging teams around the world to develop ML systems to accurately predict a user’s rating for a new movie based on their past ratings. The [winning submission](https://www.asc.ohio-state.edu/statistics/dmsl/GrandPrize2009\\\\_BPC\\\\_BigChaos.pdf) improved the RMSE (Root-mean-square-error) of predictions on unseen user-movie pairs by over 10% over Netflix’s own CineMatch algorithm, meaning it got much better at predicting how users would rate a new movie based on their history.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"I'm unable to provide information on Weights and Biases (W&B) as it is not related to HuggingFace or gradio-app. If you have questions specifically about HuggingFace or gradio-app, feel free to ask!\", 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--a9b661ed-dbaa-45f2-997a-06e7f84e7506-0', 'usage_metadata': {'input_tokens': 1769, 'output_tokens': 48, 'total_tokens': 1817, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:40:09,185 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 97%|█████████▋| 63/65 [05:03<00:11,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\\n', 'type': 'human', 'id': '88e15205-c546-471b-8ca3-f96ef507f44a'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_KRYFZirJ41oEdpivl7nHvpcg', 'function': {'arguments': '{\"query\":\"open-source library created by Hugging Face to simplify Transformer acceleration\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--7ff7a1c3-66f8-494b-967d-6838bd498eb3-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'open-source library created by Hugging Face to simplify Transformer acceleration'}, 'id': 'call_KRYFZirJ41oEdpivl7nHvpcg', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 367, 'output_tokens': 27, 'total_tokens': 394, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Overview\\n🤗 Optimum provides an API called BetterTransformer, a fast path of standard PyTorch Transformer APIs to benefit from interesting speedups on CPU & GPU through sparsity and fused kernels as Flash Attention. For now, BetterTransformer supports the fastpath from the native [`nn.TransformerEncoderLayer`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) as well as Flash Attention and Memory-Efficient Attention from [`torch.nn.functional.scaled\\\\_dot\\\\_product\\\\_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled\\\\_dot\\\\_product\\\\_attention.html).\\n\\n--\\ntitle: \"Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\"\\nthumbnail: /blog/assets/80\\\\_intel/01.png\\nauthors:\\n- user: juliensimon\\n---\\n# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\\n![image](assets/80\\\\_intel/01.png)\\nThe mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption. Today, we\\'re excited to announce that Intel has officially joined our [Hardware Partner Program](https://huggingface.co/hardware). Thanks to the [Optimum](https://github.com/huggingface/optimum-intel) open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers. Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage. Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms. “\\\\*We’re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.\\\\*”, says Wei Li, Intel Vice President & General Manager, AI and Analytics. In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs. The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Let’s take a deeper look at it. ## Get Peak Transformers Performance with Optimum Intel\\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices.\\n\\n--\\ntitle: \"Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\"\\nthumbnail: /blog/assets/60\\\\_habana/habana.png\\nauthors:\\n- user: susanlansing\\nguest: true\\n---\\n# Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\\n\\\\*Santa Clara and San Francisco, CA, April 12th, 2022\\\\*\\nPowered by deep learning, transformer models deliver state-of-the-art performance on a wide range of machine learning tasks, such as natural language processing, computer vision, speech, and more. However, training them at scale often requires a large amount of computing power, making the whole process unnecessarily long, complex, and costly. Today, [Habana® Labs](https://habana.ai/), a pioneer in high-efficiency, purpose-built deep learning processors, and Hugging Face, the home of [Transformer](https://github.com/huggingface/transformers) models, are happy to announce that they’re joining forces to make it easier and quicker to train high-quality transformer models. Thanks to the integration of Habana’s [SynapseAI software suite](https://habana.ai/training-software/) with the Hugging Face [Optimum open-source library](https://github.com/huggingface/optimum), data scientists and machine learning engineers can now accelerate their Transformer training jobs on Habana processors with just a few lines of code and enjoy greater productivity as well as lower training cost. [Habana Gaudi](https://habana.ai/training/) training solutions, which power Amazon’s EC2 DL1 instances and Supermicro’s X12 Gaudi AI Training Server, deliver price/performance up to 40% lower than comparable training solutions and enable customers to train more while spending less. The integration of ten 100 Gigabit Ethernet ports onto every Gaudi processor enables system scaling from 1 to thousands of Gaudis with ease and cost-efficiency. Habana’s SynapseAI® is optimized—at inception—to enable Gaudi performance and usability, supports TensorFlow and PyTorch frameworks, with a focus on computer vision and natural language processing applications. With 60,000+ stars on Github, 30,000+ models, and millions of monthly visits, Hugging Face is one of the fastest-growing projects in open source software history, and the go-to place for the machine learning community. With its [Hardware Partner Program](https://huggingface.co/hardware), Hugging Face provides Gaudi’s advanced deep learning hardware with the ultimate Transformer toolset. This partnership will enable rapid expansion of the Habana Gaudi training transformer model library, bringing Gaudi efficiency and ease of use to a wide array of customer use cases like natural language processing, computer vision, speech, and more. “\\\\*We’re excited to partner with Hugging Face and its many open-source developers to address the growing demand for transformer models that benefit from the efficiency, usability, and scalability of the Gaudi training platform\\\\*”, said Sree Ganesan, head of software product management, Habana Labs. “Habana Gaudi brings a new level of efficiency to deep learning model training, and we’re super excited to make this performance easily accessible to Transformer users with minimal code changes through Optimum”, said Jeff Boudier, product director at Hugging Face. To learn how to get started training with Habana Gaudi, please visit [https://developer.habana.ai](https://developer.habana.ai).', 'type': 'tool', 'name': 'data_retriever', 'id': '17247c06-d8dc-4de3-a390-9f5742858c68', 'tool_call_id': 'call_KRYFZirJ41oEdpivl7nHvpcg', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa9493e684812d15f5cda', 'page_content': '!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License. -->\\n# Overview\\n🤗 Optimum provides an API called BetterTransformer, a fast path of standard PyTorch Transformer APIs to benefit from interesting speedups on CPU & GPU through sparsity and fused kernels as Flash Attention. For now, BetterTransformer supports the fastpath from the native [`nn.TransformerEncoderLayer`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) as well as Flash Attention and Memory-Efficient Attention from [`torch.nn.functional.scaled\\\\_dot\\\\_product\\\\_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled\\\\_dot\\\\_product\\\\_attention.html).', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899c052666af48523f51ebf', 'page_content': '--\\ntitle: \"Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\"\\nthumbnail: /blog/assets/80\\\\_intel/01.png\\nauthors:\\n- user: juliensimon\\n---\\n# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\\n![image](assets/80\\\\_intel/01.png)\\nThe mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption. Today, we\\'re excited to announce that Intel has officially joined our [Hardware Partner Program](https://huggingface.co/hardware). Thanks to the [Optimum](https://github.com/huggingface/optimum-intel) open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers. Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage. Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms. “\\\\*We’re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.\\\\*”, says Wei Li, Intel Vice President & General Manager, AI and Analytics. In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs. The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Let’s take a deeper look at it. ## Get Peak Transformers Performance with Optimum Intel\\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689aa7e23e684812d15f5b5a', 'page_content': '--\\ntitle: \"Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\"\\nthumbnail: /blog/assets/60\\\\_habana/habana.png\\nauthors:\\n- user: susanlansing\\nguest: true\\n---\\n# Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\\n\\\\*Santa Clara and San Francisco, CA, April 12th, 2022\\\\*\\nPowered by deep learning, transformer models deliver state-of-the-art performance on a wide range of machine learning tasks, such as natural language processing, computer vision, speech, and more. However, training them at scale often requires a large amount of computing power, making the whole process unnecessarily long, complex, and costly. Today, [Habana® Labs](https://habana.ai/), a pioneer in high-efficiency, purpose-built deep learning processors, and Hugging Face, the home of [Transformer](https://github.com/huggingface/transformers) models, are happy to announce that they’re joining forces to make it easier and quicker to train high-quality transformer models. Thanks to the integration of Habana’s [SynapseAI software suite](https://habana.ai/training-software/) with the Hugging Face [Optimum open-source library](https://github.com/huggingface/optimum), data scientists and machine learning engineers can now accelerate their Transformer training jobs on Habana processors with just a few lines of code and enjoy greater productivity as well as lower training cost. [Habana Gaudi](https://habana.ai/training/) training solutions, which power Amazon’s EC2 DL1 instances and Supermicro’s X12 Gaudi AI Training Server, deliver price/performance up to 40% lower than comparable training solutions and enable customers to train more while spending less. The integration of ten 100 Gigabit Ethernet ports onto every Gaudi processor enables system scaling from 1 to thousands of Gaudis with ease and cost-efficiency. Habana’s SynapseAI® is optimized—at inception—to enable Gaudi performance and usability, supports TensorFlow and PyTorch frameworks, with a focus on computer vision and natural language processing applications. With 60,000+ stars on Github, 30,000+ models, and millions of monthly visits, Hugging Face is one of the fastest-growing projects in open source software history, and the go-to place for the machine learning community. With its [Hardware Partner Program](https://huggingface.co/hardware), Hugging Face provides Gaudi’s advanced deep learning hardware with the ultimate Transformer toolset. This partnership will enable rapid expansion of the Habana Gaudi training transformer model library, bringing Gaudi efficiency and ease of use to a wide array of customer use cases like natural language processing, computer vision, speech, and more. “\\\\*We’re excited to partner with Hugging Face and its many open-source developers to address the growing demand for transformer models that benefit from the efficiency, usability, and scalability of the Gaudi training platform\\\\*”, said Sree Ganesan, head of software product management, Habana Labs. “Habana Gaudi brings a new level of efficiency to deep learning model training, and we’re super excited to make this performance easily accessible to Transformer users with minimal code changes through Optimum”, said Jeff Boudier, product director at Hugging Face. To learn how to get started training with Habana Gaudi, please visit [https://developer.habana.ai](https://developer.habana.ai).', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The name of the open-source library created by Hugging Face to simplify Transformer acceleration is Optimum.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--e30102de-08c4-44d8-85e0-c13791fd53b0-0', 'usage_metadata': {'input_tokens': 1895, 'output_tokens': 20, 'total_tokens': 1915, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:40:13,112 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 98%|█████████▊| 64/65 [05:07<00:05,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What parameter is used to ensure that elements in a row have the same height in Gradio?\\n', 'type': 'human', 'id': 'fd99de7d-1787-491f-808d-9985d1ccc4f4'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_kylEiYO25PLE5ZRpY6ZkPoww', 'function': {'arguments': '{\"query\":\"Gradio same height elements in a row parameter\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--805e1346-1047-4f27-8ea0-d83a9e785940-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'Gradio same height elements in a row parameter'}, 'id': 'call_kylEiYO25PLE5ZRpY6ZkPoww', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 367, 'output_tokens': 24, 'total_tokens': 391, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': 'Controlling Layout\\nBy default, Components in Blocks are arranged vertically. Let\\'s take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS\\\\_Flexible\\\\_Box\\\\_Layout/Basic\\\\_Concepts\\\\_of\\\\_Flexbox). ## Rows\\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\\n```python\\nwith gr.Blocks() as demo:\\nwith gr.Row():\\nbtn1 = gr.Button(\"Button 1\")\\nbtn2 = gr.Button(\"Button 2\")\\n```\\nTo make every element in a Row have the same height, use the `equal\\\\_height` argument of the `style` method. ```python\\nwith gr.Blocks() as demo:\\nwith gr.Row(equal\\\\_height=True):\\ntextbox = gr.Textbox()\\nbtn2 = gr.Button(\"Button 2\")\\n```\\nThe widths of elements in a Row can be controlled via a combination of `scale` and `min\\\\_width` arguments that are present in every Component. - `scale` is an integer that defines how an element will take up space in a Row. If scale is set to `0`, and element will not expand to take up space. If scale is set to `1` or greater, the element well expand. Multiple elements in a row will expand proportional to their scale. Below, `btn1` will expand twice as much as `btn2`, while `btn0` will not expand at all:\\n```python\\nwith gr.Blocks() as demo:\\nwith gr.Row():\\nbtn0 = gr.Button(\"Button 0\", scale=0)\\nbtn1 = gr.Button(\"Button 1\", scale=1)\\nbtn2 = gr.Button(\"Button 2\", scale=2)\\n```\\n- `min\\\\_width` will set the minimum width the element will take.\\n\\nThe Row will wrap if there isn\\'t sufficient space to satisfy all `min\\\\_width` values. Learn more about Rows in the [docs](https://gradio.app/docs/#row). ## Columns and Nesting\\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\\n$code\\\\_rows\\\\_and\\\\_columns\\n$demo\\\\_rows\\\\_and\\\\_columns\\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. Learn more about Columns in the [docs](https://gradio.app/docs/#column). # Dimensions\\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases. Below is an example illustrating the use of viewport width (vw):\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as demo:\\nim = gr.ImageEditor(\\nwidth=\"50vw\",\\n)\\ndemo.launch()\\n```\\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\\n```python\\nimport gradio as gr\\ncss = \"\"\"\\n.container {\\nheight: 100vh;\\n}\\n\"\"\"\\nwith gr.Blocks(css=css) as demo:\\nwith gr.Column(elem\\\\_classes=[\"container\"]):\\nname = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\\ndemo.launch()\\n```\\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column\\'s height.\\n\\nGradio Demo: blocks\\\\_group\\n```\\n!pip install -q gradio\\n```\\n```\\nimport gradio as gr\\ndef greet(name):\\nreturn \"Hello \" + name + \"!\"\\nwith gr.Blocks() as demo:\\ngr.Markdown(\"### This is a couple of elements without any gr.Group. Form elements naturally group together anyway.\")\\ngr.Textbox(\"A\")\\ngr.Number(3)\\ngr.Button()\\ngr.Image()\\ngr.Slider()\\ngr.Markdown(\"### This is the same set put in a gr.Group.\")\\nwith gr.Group():\\ngr.Textbox(\"A\")\\ngr.Number(3)\\ngr.Button()\\ngr.Image()\\ngr.Slider()\\ngr.Markdown(\"### Now in a Row, no group.\")\\nwith gr.Row():\\ngr.Textbox(\"A\")\\ngr.Number(3)\\ngr.Button()\\ngr.Image()\\ngr.Slider()\\ngr.Markdown(\"### Now in a Row in a group.\")\\nwith gr.Group():\\nwith gr.Row():\\ngr.Textbox(\"A\")\\ngr.Number(3)\\ngr.Button()\\ngr.Image()\\ngr.Slider()\\ngr.Markdown(\"### Several rows grouped together.\")\\nwith gr.Group():\\nwith gr.Row():\\ngr.Textbox(\"A\")\\ngr.Number(3)\\ngr.Button()\\nwith gr.Row():\\ngr.Image()\\ngr.Audio()\\ngr.Markdown(\"### Several columns grouped together. If columns are uneven, there is a gray group background.\")\\nwith gr.Group():\\nwith gr.Row():\\nwith gr.Column():\\nname = gr.Textbox(label=\"Name\")\\nbtn = gr.Button(\"Hello\")\\ngr.Dropdown([\"a\", \"b\", \"c\"], interactive=True)\\ngr.Number()\\ngr.Textbox()\\nwith gr.Column():\\ngr.Image()\\ngr.Dropdown([\"a\", \"b\", \"c\"], interactive=True)\\nwith gr.Row():\\ngr.Number(scale=2)\\ngr.Textbox()\\ngr.Markdown(\"### container=False removes label, padding, and block border, placing elements \\'directly\\' on background.\")\\ngr.Radio([1,2,3], container=False)\\ngr.Textbox(container=False)\\ngr.Image(\"https://picsum.photos/id/237/200/300\", container=False, height=200)\\ngr.Markdown(\"### Textbox, Dropdown, and Number input boxes takes up full space when within a group without a container.\")\\nwith gr.Group():\\nname = gr.Textbox(label=\"Name\")\\noutput = gr.Textbox(show\\\\_label=False, container=False)\\ngreet\\\\_btn = gr.Button(\"Greet\")\\nwith gr.Row():\\ngr.Dropdown([\"a\", \"b\", \"c\"], interactive=True, container=False)\\ngr.Textbox(container=False)\\ngr.Number(container=False)\\ngr.Image(height=100)\\ngreet\\\\_btn.click(fn=greet, inputs=name, outputs=output, api\\\\_name=\"greet\")\\ngr.Markdown(\"### More examples\")\\nwith gr.Group():\\ngr.Chatbot()\\nwith gr.Row():\\nname = gr.Textbox(label=\"Prompot\", container=False)\\ngo = gr.Button(\"go\", scale=0)\\nwith gr.Column():\\ngr.Radio([1,2,3], container=False)\\ngr.Slider(0, 20, container=False)\\nwith gr.Group():\\nwith gr.Row():\\ngr.Dropdown([\"a\", \"b\", \"c\"], interactive=True, container=False, elem\\\\_id=\"here2\")\\ngr.Number(container=False)\\ngr.Textbox(container=False)\\nwith gr.Row():\\nwith gr.Column():\\ngr.Dropdown([\"a\", \"b\", \"c\"], interactive=True, container=False, elem\\\\_id=\"here2\")\\nwith gr.Column():\\ngr.Number(container=False)\\nwith gr.Column():\\ngr.Textbox(container=False)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\ndemo.launch()\\n```', 'type': 'tool', 'name': 'data_retriever', 'id': 'a3d3b4cf-5f4e-4dfa-b5b5-1f2f31a56ee5', 'tool_call_id': 'call_kylEiYO25PLE5ZRpY6ZkPoww', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab67d3e684812d15f6c6e', 'page_content': 'Controlling Layout\\nBy default, Components in Blocks are arranged vertically. Let\\'s take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS\\\\_Flexible\\\\_Box\\\\_Layout/Basic\\\\_Concepts\\\\_of\\\\_Flexbox). ## Rows\\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\\n```python\\nwith gr.Blocks() as demo:\\nwith gr.Row():\\nbtn1 = gr.Button(\"Button 1\")\\nbtn2 = gr.Button(\"Button 2\")\\n```\\nTo make every element in a Row have the same height, use the `equal\\\\_height` argument of the `style` method. ```python\\nwith gr.Blocks() as demo:\\nwith gr.Row(equal\\\\_height=True):\\ntextbox = gr.Textbox()\\nbtn2 = gr.Button(\"Button 2\")\\n```\\nThe widths of elements in a Row can be controlled via a combination of `scale` and `min\\\\_width` arguments that are present in every Component. - `scale` is an integer that defines how an element will take up space in a Row. If scale is set to `0`, and element will not expand to take up space. If scale is set to `1` or greater, the element well expand. Multiple elements in a row will expand proportional to their scale. Below, `btn1` will expand twice as much as `btn2`, while `btn0` will not expand at all:\\n```python\\nwith gr.Blocks() as demo:\\nwith gr.Row():\\nbtn0 = gr.Button(\"Button 0\", scale=0)\\nbtn1 = gr.Button(\"Button 1\", scale=1)\\nbtn2 = gr.Button(\"Button 2\", scale=2)\\n```\\n- `min\\\\_width` will set the minimum width the element will take.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab67d3e684812d15f6c6f', 'page_content': 'The Row will wrap if there isn\\'t sufficient space to satisfy all `min\\\\_width` values. Learn more about Rows in the [docs](https://gradio.app/docs/#row). ## Columns and Nesting\\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\\n$code\\\\_rows\\\\_and\\\\_columns\\n$demo\\\\_rows\\\\_and\\\\_columns\\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. Learn more about Columns in the [docs](https://gradio.app/docs/#column). # Dimensions\\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases. Below is an example illustrating the use of viewport width (vw):\\n```python\\nimport gradio as gr\\nwith gr.Blocks() as demo:\\nim = gr.ImageEditor(\\nwidth=\"50vw\",\\n)\\ndemo.launch()\\n```\\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\\n```python\\nimport gradio as gr\\ncss = \"\"\"\\n.container {\\nheight: 100vh;\\n}\\n\"\"\"\\nwith gr.Blocks(css=css) as demo:\\nwith gr.Column(elem\\\\_classes=[\"container\"]):\\nname = gr.Chatbot(value=[[\"1\", \"2\"]], height=\"70%\")\\ndemo.launch()\\n```\\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column\\'s height.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899bf17666af48523f51d75', 'page_content': 'Gradio Demo: blocks\\\\_group\\n```\\n!pip install -q gradio\\n```\\n```\\nimport gradio as gr\\ndef greet(name):\\nreturn \"Hello \" + name + \"!\"\\nwith gr.Blocks() as demo:\\ngr.Markdown(\"### This is a couple of elements without any gr.Group. Form elements naturally group together anyway.\")\\ngr.Textbox(\"A\")\\ngr.Number(3)\\ngr.Button()\\ngr.Image()\\ngr.Slider()\\ngr.Markdown(\"### This is the same set put in a gr.Group.\")\\nwith gr.Group():\\ngr.Textbox(\"A\")\\ngr.Number(3)\\ngr.Button()\\ngr.Image()\\ngr.Slider()\\ngr.Markdown(\"### Now in a Row, no group.\")\\nwith gr.Row():\\ngr.Textbox(\"A\")\\ngr.Number(3)\\ngr.Button()\\ngr.Image()\\ngr.Slider()\\ngr.Markdown(\"### Now in a Row in a group.\")\\nwith gr.Group():\\nwith gr.Row():\\ngr.Textbox(\"A\")\\ngr.Number(3)\\ngr.Button()\\ngr.Image()\\ngr.Slider()\\ngr.Markdown(\"### Several rows grouped together.\")\\nwith gr.Group():\\nwith gr.Row():\\ngr.Textbox(\"A\")\\ngr.Number(3)\\ngr.Button()\\nwith gr.Row():\\ngr.Image()\\ngr.Audio()\\ngr.Markdown(\"### Several columns grouped together. If columns are uneven, there is a gray group background.\")\\nwith gr.Group():\\nwith gr.Row():\\nwith gr.Column():\\nname = gr.Textbox(label=\"Name\")\\nbtn = gr.Button(\"Hello\")\\ngr.Dropdown([\"a\", \"b\", \"c\"], interactive=True)\\ngr.Number()\\ngr.Textbox()\\nwith gr.Column():\\ngr.Image()\\ngr.Dropdown([\"a\", \"b\", \"c\"], interactive=True)\\nwith gr.Row():\\ngr.Number(scale=2)\\ngr.Textbox()\\ngr.Markdown(\"### container=False removes label, padding, and block border, placing elements \\'directly\\' on background.\")\\ngr.Radio([1,2,3], container=False)\\ngr.Textbox(container=False)\\ngr.Image(\"https://picsum.photos/id/237/200/300\", container=False, height=200)\\ngr.Markdown(\"### Textbox, Dropdown, and Number input boxes takes up full space when within a group without a container.\")\\nwith gr.Group():\\nname = gr.Textbox(label=\"Name\")\\noutput = gr.Textbox(show\\\\_label=False, container=False)\\ngreet\\\\_btn = gr.Button(\"Greet\")\\nwith gr.Row():\\ngr.Dropdown([\"a\", \"b\", \"c\"], interactive=True, container=False)\\ngr.Textbox(container=False)\\ngr.Number(container=False)\\ngr.Image(height=100)\\ngreet\\\\_btn.click(fn=greet, inputs=name, outputs=output, api\\\\_name=\"greet\")\\ngr.Markdown(\"### More examples\")\\nwith gr.Group():\\ngr.Chatbot()\\nwith gr.Row():\\nname = gr.Textbox(label=\"Prompot\", container=False)\\ngo = gr.Button(\"go\", scale=0)\\nwith gr.Column():\\ngr.Radio([1,2,3], container=False)\\ngr.Slider(0, 20, container=False)\\nwith gr.Group():\\nwith gr.Row():\\ngr.Dropdown([\"a\", \"b\", \"c\"], interactive=True, container=False, elem\\\\_id=\"here2\")\\ngr.Number(container=False)\\ngr.Textbox(container=False)\\nwith gr.Row():\\nwith gr.Column():\\ngr.Dropdown([\"a\", \"b\", \"c\"], interactive=True, container=False, elem\\\\_id=\"here2\")\\nwith gr.Column():\\ngr.Number(container=False)\\nwith gr.Column():\\ngr.Textbox(container=False)\\nif \\\\_\\\\_name\\\\_\\\\_ == \"\\\\_\\\\_main\\\\_\\\\_\":\\ndemo.launch()\\n```', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The parameter used to ensure that elements in a row have the same height in Gradio is the `equal_height` argument of the `style` method.', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--fefe6210-a0c4-406e-beec-6022b3926f22-0', 'usage_metadata': {'input_tokens': 1899, 'output_tokens': 31, 'total_tokens': 1930, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 11:40:17,155 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 65/65 [05:11<00:00,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'What is the command to install the latest version of Optimum with OpenVINO support?\\n', 'type': 'human', 'id': 'ac9f7a7e-c43f-4658-a725-0b98e3bcd136'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_qxvSKmiv4hoIv2WzraLeg8k7', 'function': {'arguments': '{\"query\":\"install latest version of Optimum with OpenVINO support\"}', 'name': 'data_retriever'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--cbe41801-bfa3-446a-90b3-66ed0dcc3b06-0', 'tool_calls': [{'name': 'data_retriever', 'args': {'query': 'install latest version of Optimum with OpenVINO support'}, 'id': 'call_qxvSKmiv4hoIv2WzraLeg8k7', 'type': 'tool_call'}], 'usage_metadata': {'input_tokens': 366, 'output_tokens': 26, 'total_tokens': 392, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'invalid_tool_calls': []}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'ToolMessage'], 'kwargs': {'content': '![ONNX Runtime](https://github.com/huggingface/optimum/actions/workflows/test\\\\_onnxruntime.yml/badge.svg)](https://github.com/huggingface/optimum/actions/workflows/test\\\\_onnxruntime.yml)\\n# Hugging Face Optimum\\n🤗 Optimum is an extension of 🤗 Transformers and Diffusers, providing a set of optimization tools enabling maximum efficiency to train and run models on targeted hardware, while keeping things easy to use. ## Installation\\n🤗 Optimum can be installed using `pip` as follows:\\n```bash\\npython -m pip install optimum\\n```\\nIf you\\'d like to use the accelerator-specific features of 🤗 Optimum, you can install the required dependencies according to the table below:\\n| Accelerator | Installation |\\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\\n| [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/overview) | `pip install --upgrade-strategy eager optimum[onnxruntime]` |\\n| [Intel Neural Compressor](https://huggingface.co/docs/optimum/intel/index) | `pip install --upgrade-strategy eager optimum[neural-compressor]`|\\n| [OpenVINO](https://huggingface.co/docs/optimum/intel/index) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |\\n| [AMD Instinct GPUs and Ryzen AI NPU](https://huggingface.co/docs/optimum/amd/index) | `pip install --upgrade-strategy eager optimum[amd]` |\\n| [Habana Gaudi Processor (HPU)](https://huggingface.co/docs/optimum/habana/index) | `pip install --upgrade-strategy eager optimum[habana]` |\\n| [FuriosaAI](https://huggingface.co/docs/optimum/furiosa/index) | `pip install --upgrade-strategy eager optimum[furiosa]` |\\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version. To install from source:\\n```bash\\npython -m pip install git+https://github.com/huggingface/optimum.git\\n```\\nFor the accelerator-specific features, append `optimum[accelerator\\\\_type]` to the above command:\\n```bash\\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\\n```\\n## Accelerated Inference\\n🤗 Optimum provides multiple tools to export and run optimized models on various ecosystems:\\n- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage\\\\_guides/export\\\\_a\\\\_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage\\\\_guides/models)\\n- TensorFlow Lite\\n- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)\\n- Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage\\\\_guides/accelerate\\\\_inference)\\nThe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line. ### Features summary\\n| Features | [ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview)| [Neural Compressor](https://huggingface.co/docs/optimum/main/en/intel/optimization\\\\_inc)| [OpenVINO](https://huggingface.co/docs/optimum/main/en/intel/inference)| [TensorFlow Lite](https://huggingface.co/docs/optimum/main/en/exporters/tflite/overview)|\\n|:----------------------------------:|:------------------:|:------------------:|:------------------:|:------------------:|\\n| Graph optimization | :heavy\\\\_check\\\\_mark: | N/A | :heavy\\\\_check\\\\_mark: | N/A |\\n| Post-training dynamic quantization | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A | :heavy\\\\_check\\\\_mark: |\\n| Post-training static quantization | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: |\\n| Quantization Aware Training (QAT) | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A |\\n| FP16 (half precision) | :heavy\\\\_check\\\\_mark: | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: |\\n| Pruning | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A |\\n| Knowledge Distillation | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A |\\n### OpenVINO\\nBefore you begin, make sure you have all the necessary libraries installed :\\n```bash\\npip install --upgrade-strategy eager optimum[openvino,nncf]\\n```\\nIt is possible to export 🤗 Transformers and Diffusers models to the OpenVINO format easily:\\n```bash\\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert\\\\_sst2\\\\_ov\\n```\\nIf you add `--int8`, the weights will be quantized to INT8.\\n\\nAs demonstrated by this [Intel Space](https://huggingface.co/spaces/Intel/Stable-Diffusion-Side-by-Side), the same code runs on a previous generation Intel Xeon (code name Ice Lake) in about 45 seconds. Out of the box, we can see that Sapphire Rapids CPUs are quite faster without any code change! Now, let\\'s accelerate! ## Optimum Intel and OpenVINO\\n[Optimum Intel](https://huggingface.co/docs/optimum/intel/index) accelerates end-to-end pipelines on Intel architectures. Its API is extremely similar to the vanilla [Diffusers](https://huggingface.co/docs/diffusers/index) API, making it trivial to adapt existing code. Optimum Intel supports [OpenVINO](https://docs.openvino.ai/latest/index.html), an Intel open-source toolkit for high-performance inference. Optimum Intel and OpenVINO can be installed as follows:\\n```\\npip install optimum[openvino]\\n```\\nStarting from the code above, we only need to replace `StableDiffusionPipeline` with `OVStableDiffusionPipeline`. To load a PyTorch model and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model. ```python\\nfrom optimum.intel.openvino import OVStableDiffusionPipeline\\n... ov\\\\_pipe = OVStableDiffusionPipeline.from\\\\_pretrained(model\\\\_id, export=True)\\nlatency = elapsed\\\\_time(ov\\\\_pipe, prompt)\\nprint(latency)\\n# Don\\'t forget to save the exported model\\nov\\\\_pipe.save\\\\_pretrained(\"./openvino\")\\n```\\nOpenVINO automatically optimizes the model for the `bfloat16` format. Thanks to this, the average latency is now \\\\*\\\\*16.7 seconds\\\\*\\\\*, a sweet 2x speedup. The pipeline above support dynamic input shapes, with no restriction on the number of images or their resolution. With Stable Diffusion, your application is usually restricted to one (or a few) different output resolutions, such as 512x512, or 256x256.\\n\\n--\\ntitle: \"Accelerate your models with 🤗 Optimum Intel and OpenVINO\"\\nthumbnail: /blog/assets/113\\\\_openvino/thumbnail.png\\nauthors:\\n- user: echarlaix\\n- user: juliensimon\\n---\\n# Accelerate your models with 🤗 Optimum Intel and OpenVINO\\n![image](assets/113\\\\_openvino/thumbnail.png)\\nLast July, we [announced](https://huggingface.co/blog/intel) that Intel and Hugging Face would collaborate on building state-of-the-art yet simple hardware acceleration tools for Transformer models. \\u200b\\nToday, we are very happy to announce that we added Intel [OpenVINO](https://docs.openvino.ai/latest/index.html) to [Optimum Intel](https://github.com/huggingface/optimum-intel). You can now easily perform inference with OpenVINO Runtime on a variety of Intel processors ([see](https://docs.openvino.ai/latest/openvino\\\\_docs\\\\_OV\\\\_UG\\\\_supported\\\\_plugins\\\\_Supported\\\\_Devices.html) the full list of supported devices) using Transformers models which can be hosted either on the Hugging Face hub or locally. You can also quantize your model with the OpenVINO Neural Network Compression Framework ([NNCF](https://github.com/openvinotoolkit/nncf)), and reduce its size and prediction latency in near minutes. \\u200b\\nThis first release is based on OpenVINO 2022.2 and enables inference for a large quantity of PyTorch models using our [`OVModels`](https://huggingface.co/docs/optimum/intel/inference). Post-training static quantization and quantization aware training can be applied on many encoder models (BERT, DistilBERT, etc.). More encoder models will be supported in the upcoming OpenVINO release. Currently the quantization of Encoder Decoder models is not enabled, however this restriction should be lifted with our integration of the next OpenVINO release. \\u200bLet us show you how to get started in minutes!\\u200b\\n## Quantizing a Vision Transformer with Optimum Intel and OpenVINO\\n\\u200b\\nIn this example, we will run post-training static quantization on a Vision Transformer (ViT) [model](https://huggingface.co/juliensimon/autotrain-food101-1471154050) fine-tuned for image classification on the [food101](https://huggingface.co/datasets/food101) dataset.', 'type': 'tool', 'name': 'data_retriever', 'id': '584dfef2-95b2-45c2-b659-e9cc7a50b611', 'tool_call_id': 'call_qxvSKmiv4hoIv2WzraLeg8k7', 'artifact': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '689ab57d3e684812d15f6b52', 'page_content': \"![ONNX Runtime](https://github.com/huggingface/optimum/actions/workflows/test\\\\_onnxruntime.yml/badge.svg)](https://github.com/huggingface/optimum/actions/workflows/test\\\\_onnxruntime.yml)\\n# Hugging Face Optimum\\n🤗 Optimum is an extension of 🤗 Transformers and Diffusers, providing a set of optimization tools enabling maximum efficiency to train and run models on targeted hardware, while keeping things easy to use. ## Installation\\n🤗 Optimum can be installed using `pip` as follows:\\n```bash\\npython -m pip install optimum\\n```\\nIf you'd like to use the accelerator-specific features of 🤗 Optimum, you can install the required dependencies according to the table below:\\n| Accelerator | Installation |\\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\\n| [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/overview) | `pip install --upgrade-strategy eager optimum[onnxruntime]` |\\n| [Intel Neural Compressor](https://huggingface.co/docs/optimum/intel/index) | `pip install --upgrade-strategy eager optimum[neural-compressor]`|\\n| [OpenVINO](https://huggingface.co/docs/optimum/intel/index) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |\\n| [AMD Instinct GPUs and Ryzen AI NPU](https://huggingface.co/docs/optimum/amd/index) | `pip install --upgrade-strategy eager optimum[amd]` |\\n| [Habana Gaudi Processor (HPU)](https://huggingface.co/docs/optimum/habana/index) | `pip install --upgrade-strategy eager optimum[habana]` |\\n| [FuriosaAI](https://huggingface.co/docs/optimum/furiosa/index) | `pip install --upgrade-strategy eager optimum[furiosa]` |\\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version. To install from source:\\n```bash\\npython -m pip install git+https://github.com/huggingface/optimum.git\\n```\\nFor the accelerator-specific features, append `optimum[accelerator\\\\_type]` to the above command:\\n```bash\\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\\n```\\n## Accelerated Inference\\n🤗 Optimum provides multiple tools to export and run optimized models on various ecosystems:\\n- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage\\\\_guides/export\\\\_a\\\\_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage\\\\_guides/models)\\n- TensorFlow Lite\\n- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)\\n- Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage\\\\_guides/accelerate\\\\_inference)\\nThe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line. ### Features summary\\n| Features | [ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview)| [Neural Compressor](https://huggingface.co/docs/optimum/main/en/intel/optimization\\\\_inc)| [OpenVINO](https://huggingface.co/docs/optimum/main/en/intel/inference)| [TensorFlow Lite](https://huggingface.co/docs/optimum/main/en/exporters/tflite/overview)|\\n|:----------------------------------:|:------------------:|:------------------:|:------------------:|:------------------:|\\n| Graph optimization | :heavy\\\\_check\\\\_mark: | N/A | :heavy\\\\_check\\\\_mark: | N/A |\\n| Post-training dynamic quantization | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A | :heavy\\\\_check\\\\_mark: |\\n| Post-training static quantization | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: |\\n| Quantization Aware Training (QAT) | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A |\\n| FP16 (half precision) | :heavy\\\\_check\\\\_mark: | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: |\\n| Pruning | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A |\\n| Knowledge Distillation | N/A | :heavy\\\\_check\\\\_mark: | :heavy\\\\_check\\\\_mark: | N/A |\\n### OpenVINO\\nBefore you begin, make sure you have all the necessary libraries installed :\\n```bash\\npip install --upgrade-strategy eager optimum[openvino,nncf]\\n```\\nIt is possible to export 🤗 Transformers and Diffusers models to the OpenVINO format easily:\\n```bash\\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert\\\\_sst2\\\\_ov\\n```\\nIf you add `--int8`, the weights will be quantized to INT8.\", 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899be06666af48523f51bf6', 'page_content': 'As demonstrated by this [Intel Space](https://huggingface.co/spaces/Intel/Stable-Diffusion-Side-by-Side), the same code runs on a previous generation Intel Xeon (code name Ice Lake) in about 45 seconds. Out of the box, we can see that Sapphire Rapids CPUs are quite faster without any code change! Now, let\\'s accelerate! ## Optimum Intel and OpenVINO\\n[Optimum Intel](https://huggingface.co/docs/optimum/intel/index) accelerates end-to-end pipelines on Intel architectures. Its API is extremely similar to the vanilla [Diffusers](https://huggingface.co/docs/diffusers/index) API, making it trivial to adapt existing code. Optimum Intel supports [OpenVINO](https://docs.openvino.ai/latest/index.html), an Intel open-source toolkit for high-performance inference. Optimum Intel and OpenVINO can be installed as follows:\\n```\\npip install optimum[openvino]\\n```\\nStarting from the code above, we only need to replace `StableDiffusionPipeline` with `OVStableDiffusionPipeline`. To load a PyTorch model and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model. ```python\\nfrom optimum.intel.openvino import OVStableDiffusionPipeline\\n... ov\\\\_pipe = OVStableDiffusionPipeline.from\\\\_pretrained(model\\\\_id, export=True)\\nlatency = elapsed\\\\_time(ov\\\\_pipe, prompt)\\nprint(latency)\\n# Don\\'t forget to save the exported model\\nov\\\\_pipe.save\\\\_pretrained(\"./openvino\")\\n```\\nOpenVINO automatically optimizes the model for the `bfloat16` format. Thanks to this, the average latency is now \\\\*\\\\*16.7 seconds\\\\*\\\\*, a sweet 2x speedup. The pipeline above support dynamic input shapes, with no restriction on the number of images or their resolution. With Stable Diffusion, your application is usually restricted to one (or a few) different output resolutions, such as 512x512, or 256x256.', 'type': 'Document'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'document', 'Document'], 'kwargs': {'id': '6899b3b7af3d2ebb270a00c8', 'page_content': '--\\ntitle: \"Accelerate your models with 🤗 Optimum Intel and OpenVINO\"\\nthumbnail: /blog/assets/113\\\\_openvino/thumbnail.png\\nauthors:\\n- user: echarlaix\\n- user: juliensimon\\n---\\n# Accelerate your models with 🤗 Optimum Intel and OpenVINO\\n![image](assets/113\\\\_openvino/thumbnail.png)\\nLast July, we [announced](https://huggingface.co/blog/intel) that Intel and Hugging Face would collaborate on building state-of-the-art yet simple hardware acceleration tools for Transformer models. \\u200b\\nToday, we are very happy to announce that we added Intel [OpenVINO](https://docs.openvino.ai/latest/index.html) to [Optimum Intel](https://github.com/huggingface/optimum-intel). You can now easily perform inference with OpenVINO Runtime on a variety of Intel processors ([see](https://docs.openvino.ai/latest/openvino\\\\_docs\\\\_OV\\\\_UG\\\\_supported\\\\_plugins\\\\_Supported\\\\_Devices.html) the full list of supported devices) using Transformers models which can be hosted either on the Hugging Face hub or locally. You can also quantize your model with the OpenVINO Neural Network Compression Framework ([NNCF](https://github.com/openvinotoolkit/nncf)), and reduce its size and prediction latency in near minutes. \\u200b\\nThis first release is based on OpenVINO 2022.2 and enables inference for a large quantity of PyTorch models using our [`OVModels`](https://huggingface.co/docs/optimum/intel/inference). Post-training static quantization and quantization aware training can be applied on many encoder models (BERT, DistilBERT, etc.). More encoder models will be supported in the upcoming OpenVINO release. Currently the quantization of Encoder Decoder models is not enabled, however this restriction should be lifted with our integration of the next OpenVINO release. \\u200bLet us show you how to get started in minutes!\\u200b\\n## Quantizing a Vision Transformer with Optimum Intel and OpenVINO\\n\\u200b\\nIn this example, we will run post-training static quantization on a Vision Transformer (ViT) [model](https://huggingface.co/juliensimon/autotrain-food101-1471154050) fine-tuned for image classification on the [food101](https://huggingface.co/datasets/food101) dataset.', 'type': 'Document'}}], 'status': 'success'}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The command to install the latest version of Optimum with OpenVINO support is:\\n\\n```bash\\npip install --upgrade-strategy eager optimum[openvino,nncf]\\n```', 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c'}, 'type': 'ai', 'id': 'run--f14d97ca-0c71-49a6-8f17-99d4c880f070-0', 'usage_metadata': {'input_tokens': 2439, 'output_tokens': 36, 'total_tokens': 2475, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [], 'invalid_tool_calls': []}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = run_rag_tests(eval_dataset=eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56be1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    FactualCorrectness\n",
    ")\n",
    "evaluation_dataset = EvaluationDataset.from_list(data)\n",
    "\n",
    "# Khởi tạo các metric\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "]\n",
    "# metrics = [FactualCorrectness()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9063c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "print(len(evaluation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "617a4806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/260 [00:00<?, ?it/s]2025-08-15 11:40:20,022 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:20,026 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:20,033 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:20,037 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:20,043 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:20,051 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:20,054 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:20,657 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,352 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,360 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,365 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,368 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,373 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,375 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,380 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,382 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,384 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,387 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,390 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,392 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,393 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,395 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   0%|          | 1/260 [00:02<11:42,  2.71s/it]2025-08-15 11:40:21,412 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:21,788 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:22,367 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:22,786 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:24,020 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:24,676 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:25,181 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:25,326 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:25,327 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   1%|          | 2/260 [00:06<14:42,  3.42s/it]2025-08-15 11:40:25,339 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   3%|▎         | 7/260 [00:06<02:50,  1.48it/s]2025-08-15 11:40:26,589 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:26,640 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:26,678 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:26,701 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:26,767 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:27,139 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:27,553 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:27,805 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:27,807 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:27,808 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:27,809 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:27,810 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:27,811 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:27,813 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:27,814 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:27,816 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:27,820 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:28,316 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:28,727 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   4%|▍         | 10/260 [00:10<03:38,  1.14it/s]2025-08-15 11:40:28,936 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:28,938 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:28,941 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:29,826 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-08-15 11:40:29,827 - INFO - Retrying request to /chat/completions in 0.439246 seconds\n",
      "2025-08-15 11:40:29,949 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:29,993 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:30,029 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:30,465 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:30,903 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:31,103 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:31,106 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:31,107 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:31,109 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:31,110 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:31,111 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:31,112 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:31,115 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:31,116 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:31,120 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   6%|▌         | 15/260 [00:12<02:41,  1.51it/s]2025-08-15 11:40:31,142 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:32,790 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:33,373 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   6%|▌         | 16/260 [00:14<03:29,  1.17it/s]2025-08-15 11:40:33,389 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:33,393 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:33,469 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   8%|▊         | 22/260 [00:14<01:41,  2.35it/s]2025-08-15 11:40:33,540 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:33,656 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:34,560 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:34,869 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:34,873 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:34,875 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:34,877 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:34,879 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:34,885 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:35,399 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:35,789 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:36,400 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:36,829 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:37,033 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:37,035 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:37,039 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:37,043 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:37,049 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:37,052 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:37,057 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:37,062 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:37,068 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:37,069 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  10%|▉         | 25/260 [00:18<02:28,  1.58it/s]2025-08-15 11:40:37,142 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  13%|█▎        | 33/260 [00:18<01:13,  3.09it/s]2025-08-15 11:40:37,698 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:37,746 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:38,305 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:38,333 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:38,371 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:38,380 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:38,414 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:38,423 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,260 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,891 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,893 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,896 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,898 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,901 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,905 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,907 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,908 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,910 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,911 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,914 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,916 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,920 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:39,922 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:40,349 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:41,032 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:41,481 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:42,044 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  14%|█▍        | 36/260 [00:23<02:13,  1.68it/s]2025-08-15 11:40:42,060 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:42,064 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:42,068 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:43,169 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:43,233 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:43,246 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:43,731 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:44,391 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:44,395 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:44,398 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:44,399 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:44,401 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:44,403 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:44,404 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:44,406 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:44,408 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:44,409 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:44,411 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  17%|█▋        | 43/260 [00:25<01:46,  2.05it/s]2025-08-15 11:40:44,853 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:45,313 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  17%|█▋        | 45/260 [00:26<01:46,  2.02it/s]2025-08-15 11:40:45,476 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:45,771 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  18%|█▊        | 48/260 [00:27<01:24,  2.50it/s]2025-08-15 11:40:46,507 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:46,589 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:46,686 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:47,038 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:47,223 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:47,396 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:47,399 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  19%|█▉        | 50/260 [00:28<01:40,  2.09it/s]2025-08-15 11:40:47,493 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:47,501 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  20%|█▉        | 51/260 [00:28<01:30,  2.30it/s]2025-08-15 11:40:47,565 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:47,647 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:48,025 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:48,654 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:48,661 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:48,665 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:48,666 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:48,668 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:48,671 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  20%|██        | 52/260 [00:29<01:51,  1.86it/s]2025-08-15 11:40:49,031 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:49,739 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  20%|██        | 53/260 [00:31<02:09,  1.60it/s]2025-08-15 11:40:49,744 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:49,747 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:49,748 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:49,751 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:49,753 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:49,756 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:49,758 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:49,759 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:49,761 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:50,805 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:50,813 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:50,888 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:50,952 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:51,458 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,090 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,095 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,098 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,101 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,102 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,105 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,107 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,109 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,111 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,115 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,118 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,121 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,124 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,126 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  23%|██▎       | 61/260 [00:33<01:21,  2.43it/s]2025-08-15 11:40:52,552 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:52,945 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  24%|██▍       | 62/260 [00:34<01:34,  2.10it/s]2025-08-15 11:40:53,814 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  27%|██▋       | 69/260 [00:35<00:53,  3.58it/s]2025-08-15 11:40:53,914 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  27%|██▋       | 70/260 [00:35<00:49,  3.81it/s]2025-08-15 11:40:54,288 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:54,304 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:54,370 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:54,371 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:54,374 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:54,938 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:55,514 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:55,518 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:55,520 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:55,521 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:55,522 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:55,524 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:55,525 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:55,526 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:55,527 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:55,529 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:55,531 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:55,533 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:55,535 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  27%|██▋       | 71/260 [00:36<01:20,  2.35it/s]2025-08-15 11:40:55,964 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:56,506 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:56,976 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:57,385 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  28%|██▊       | 72/260 [00:38<02:04,  1.51it/s]2025-08-15 11:40:57,594 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:57,597 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:57,600 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  30%|██▉       | 77/260 [00:39<01:01,  2.98it/s]2025-08-15 11:40:57,798 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:57,979 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:58,781 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:58,809 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:58,826 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:58,830 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:59,221 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:59,854 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:59,981 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:59,984 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:59,987 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:59,989 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:59,990 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:59,991 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:59,993 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:59,995 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:59,998 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:40:59,999 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:00,003 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  30%|███       | 79/260 [00:41<01:34,  1.92it/s]2025-08-15 11:41:00,468 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:00,892 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  31%|███       | 81/260 [00:42<01:34,  1.90it/s]2025-08-15 11:41:01,103 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:01,641 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  33%|███▎      | 85/260 [00:42<01:04,  2.73it/s]2025-08-15 11:41:02,157 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:02,178 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  33%|███▎      | 86/260 [00:43<01:07,  2.58it/s]2025-08-15 11:41:02,313 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:02,353 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:02,757 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:03,156 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:03,440 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:03,443 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:03,447 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:03,450 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:03,454 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:03,456 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:03,458 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:03,459 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:03,460 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:03,464 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:03,466 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  33%|███▎      | 87/260 [00:44<01:31,  1.90it/s]2025-08-15 11:41:03,506 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:03,525 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:03,964 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:04,424 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:04,625 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:04,627 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  35%|███▌      | 92/260 [00:45<01:02,  2.68it/s]2025-08-15 11:41:04,737 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:04,847 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  36%|███▌      | 94/260 [00:46<00:51,  3.21it/s]2025-08-15 11:41:05,030 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:05,037 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:05,059 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  37%|███▋      | 95/260 [00:46<00:49,  3.36it/s]2025-08-15 11:41:05,123 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:05,191 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  37%|███▋      | 96/260 [00:46<00:44,  3.70it/s]2025-08-15 11:41:05,588 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:05,590 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  37%|███▋      | 97/260 [00:46<00:48,  3.39it/s]2025-08-15 11:41:05,701 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:05,869 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  38%|███▊      | 98/260 [00:47<00:47,  3.43it/s]2025-08-15 11:41:06,022 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:06,380 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:06,820 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,126 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,128 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,132 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,133 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,136 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,138 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,140 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,141 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,143 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,144 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,147 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,149 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  38%|███▊      | 99/260 [00:48<01:24,  1.90it/s]2025-08-15 11:41:07,166 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,168 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,170 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,175 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,637 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:07,990 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  38%|███▊      | 100/260 [00:49<01:44,  1.53it/s]2025-08-15 11:41:08,203 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:08,585 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:09,097 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:09,200 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  41%|████      | 106/260 [00:50<00:49,  3.11it/s]2025-08-15 11:41:09,993 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  42%|████▏     | 108/260 [00:51<00:51,  2.94it/s]2025-08-15 11:41:10,260 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  42%|████▏     | 109/260 [00:51<00:49,  3.02it/s]2025-08-15 11:41:10,331 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:10,487 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:10,488 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:10,489 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:10,886 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:11,291 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:11,490 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:11,491 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:11,493 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:11,496 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:11,498 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:11,500 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  42%|████▏     | 110/260 [00:52<01:13,  2.05it/s]2025-08-15 11:41:11,941 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:12,358 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  43%|████▎     | 111/260 [00:53<01:29,  1.66it/s]2025-08-15 11:41:12,562 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:12,564 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:12,566 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:12,567 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:12,569 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:12,571 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:12,572 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:12,573 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:12,602 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:12,673 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:13,436 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:13,666 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:13,696 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:13,719 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:13,722 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:13,725 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:13,871 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:13,973 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:14,528 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:15,113 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:15,118 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:15,121 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:15,125 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:15,131 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:15,134 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:15,136 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:15,138 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:15,143 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  46%|████▌     | 120/260 [00:56<00:52,  2.65it/s]2025-08-15 11:41:15,582 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:16,180 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:16,183 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:16,184 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:16,185 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:16,187 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:16,188 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:16,189 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  47%|████▋     | 121/260 [00:57<01:01,  2.25it/s]2025-08-15 11:41:16,562 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:16,957 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  47%|████▋     | 122/260 [00:58<01:07,  2.06it/s]2025-08-15 11:41:18,072 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:18,100 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:18,128 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:18,135 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:18,160 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:18,574 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:19,033 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:19,300 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:19,302 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:19,305 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:19,307 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:19,308 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  49%|████▉     | 128/260 [01:00<00:57,  2.29it/s]2025-08-15 11:41:19,321 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:19,893 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:20,274 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  50%|████▉     | 129/260 [01:01<01:07,  1.93it/s]2025-08-15 11:41:20,498 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:20,499 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:20,500 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:20,502 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:20,503 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:20,504 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:21,186 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  52%|█████▏    | 134/260 [01:02<00:44,  2.84it/s]2025-08-15 11:41:21,686 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:21,739 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:21,756 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:21,828 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,238 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,874 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,878 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,879 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,881 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,882 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,883 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,885 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,886 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,887 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,888 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,889 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,890 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,891 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:22,892 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  52%|█████▏    | 135/260 [01:04<01:02,  1.99it/s]2025-08-15 11:41:23,354 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:23,798 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  52%|█████▏    | 136/260 [01:05<01:12,  1.71it/s]2025-08-15 11:41:24,006 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:24,009 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:24,013 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:25,024 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:25,158 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:25,164 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:25,194 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:25,240 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:25,261 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:25,275 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:25,292 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:25,765 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:26,386 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:26,391 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:26,395 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:26,397 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:26,399 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:26,402 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:26,404 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:26,406 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:26,409 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:26,413 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:26,416 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  57%|█████▋    | 147/260 [01:07<00:38,  2.97it/s]2025-08-15 11:41:26,439 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:26,887 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:27,543 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:27,551 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  57%|█████▋    | 148/260 [01:08<00:44,  2.50it/s]2025-08-15 11:41:27,949 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:28,414 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  57%|█████▋    | 149/260 [01:09<00:50,  2.20it/s]2025-08-15 11:41:28,484 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:28,485 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:29,652 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:29,655 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:29,658 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:29,676 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:29,731 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:29,769 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:30,188 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:30,779 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:30,990 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:30,992 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:30,995 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:30,998 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:31,000 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:31,006 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:31,008 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:31,011 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:31,017 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:31,019 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:31,024 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  60%|██████    | 156/260 [01:12<00:42,  2.44it/s]2025-08-15 11:41:31,056 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:31,550 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:31,994 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  60%|██████    | 157/260 [01:13<00:49,  2.09it/s]2025-08-15 11:41:32,198 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:32,200 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:33,236 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:33,300 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:33,331 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:33,339 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:33,354 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:33,710 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:34,344 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:34,354 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:34,358 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:34,361 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:34,365 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:34,367 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:34,370 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:34,371 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:34,374 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:34,375 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  63%|██████▎   | 163/260 [01:15<00:41,  2.33it/s]2025-08-15 11:41:34,391 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:34,796 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:35,257 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  63%|██████▎   | 164/260 [01:16<00:47,  2.04it/s]2025-08-15 11:41:35,461 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:35,465 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:36,411 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  66%|██████▌   | 171/260 [01:17<00:28,  3.15it/s]2025-08-15 11:41:36,591 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:36,599 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:36,688 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:36,718 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:37,100 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:37,742 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:37,751 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:37,754 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:37,756 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:37,758 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:37,762 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:37,765 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:37,768 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:37,770 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:37,772 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  66%|██████▌   | 172/260 [01:19<00:36,  2.42it/s]2025-08-15 11:41:38,510 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:39,142 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  67%|██████▋   | 173/260 [01:20<00:45,  1.91it/s]2025-08-15 11:41:39,167 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:39,172 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:39,176 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:40,448 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:40,470 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:40,632 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  68%|██████▊   | 178/260 [01:21<00:34,  2.38it/s]2025-08-15 11:41:40,674 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:40,979 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:41,030 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  69%|██████▉   | 180/260 [01:22<00:30,  2.66it/s]2025-08-15 11:41:41,065 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:41,067 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:41,122 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:41,263 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:41,348 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  70%|███████   | 182/260 [01:22<00:25,  3.06it/s]2025-08-15 11:41:41,373 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:41,611 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:41,699 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:42,120 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:42,503 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:42,707 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:42,709 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:42,711 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:42,714 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:42,716 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:42,718 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:42,721 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:42,722 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  71%|███████   | 184/260 [01:24<00:31,  2.41it/s]2025-08-15 11:41:42,872 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:42,874 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:43,260 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:43,989 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:44,398 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:45,012 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  71%|███████   | 185/260 [01:26<00:51,  1.44it/s]2025-08-15 11:41:45,017 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:45,018 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:45,020 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:45,021 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:45,024 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:45,277 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  74%|███████▍  | 193/260 [01:26<00:18,  3.60it/s]2025-08-15 11:41:46,219 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:46,278 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:46,303 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:46,309 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:46,680 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:47,321 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:47,325 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:47,328 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:47,332 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:47,333 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:47,335 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:47,336 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:47,338 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:47,339 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:47,361 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:47,778 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:48,143 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  75%|███████▌  | 195/260 [01:29<00:33,  1.97it/s]2025-08-15 11:41:48,363 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:48,366 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:49,043 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  77%|███████▋  | 200/260 [01:30<00:21,  2.76it/s]2025-08-15 11:41:49,604 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:49,605 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:49,606 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:49,607 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:49,615 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:49,978 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:50,601 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:50,605 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:50,607 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:50,609 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:50,612 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:50,615 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:50,616 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:50,618 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:50,619 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  77%|███████▋  | 201/260 [01:31<00:28,  2.06it/s]2025-08-15 11:41:51,073 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:51,673 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  78%|███████▊  | 202/260 [01:32<00:32,  1.80it/s]2025-08-15 11:41:51,681 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:51,684 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:51,685 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:51,686 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:51,688 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:53,615 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:53,623 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  80%|████████  | 208/260 [01:34<00:22,  2.29it/s]2025-08-15 11:41:53,650 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:53,787 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:53,813 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:53,921 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:53,949 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  80%|████████  | 209/260 [01:35<00:21,  2.36it/s]2025-08-15 11:41:53,968 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:54,180 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:54,260 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:54,536 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:54,539 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:54,543 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:54,549 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:54,897 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:55,561 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:55,567 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:55,568 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:55,569 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:55,570 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:55,571 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:55,572 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:55,573 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:55,575 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:55,579 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:55,582 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  81%|████████  | 211/260 [01:36<00:25,  1.93it/s]2025-08-15 11:41:56,022 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:56,650 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:57,096 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:57,817 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  82%|████████▏ | 212/260 [01:39<00:36,  1.30it/s]2025-08-15 11:41:57,823 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:57,825 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:57,828 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:58,842 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:58,938 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:59,144 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:59,147 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:59,149 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:59,150 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:41:59,601 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:00,194 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:00,201 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:00,202 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:00,203 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:00,204 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:00,205 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:00,206 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:00,208 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:00,212 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:00,218 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:00,221 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  85%|████████▍ | 220/260 [01:41<00:19,  2.10it/s]2025-08-15 11:42:00,626 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:01,008 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  85%|████████▌ | 221/260 [01:42<00:20,  1.87it/s]2025-08-15 11:42:01,490 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  87%|████████▋ | 227/260 [01:42<00:09,  3.30it/s]2025-08-15 11:42:01,624 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:02,359 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:02,497 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:02,551 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:02,597 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:02,685 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:02,695 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:02,731 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:03,133 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:03,485 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:03,689 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:03,691 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:03,693 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:03,695 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:03,699 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:03,700 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:03,701 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:03,702 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  88%|████████▊ | 229/260 [01:45<00:13,  2.23it/s]2025-08-15 11:42:04,081 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:04,673 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  88%|████████▊ | 230/260 [01:46<00:15,  1.91it/s]2025-08-15 11:42:04,827 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:04,829 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:04,830 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:06,107 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:06,108 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:06,115 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:06,118 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:06,119 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:06,121 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:06,573 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:06,948 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:07,170 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:07,175 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:07,180 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:07,184 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:07,188 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:07,191 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:07,197 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:07,199 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:07,201 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:07,203 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:07,206 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  91%|█████████ | 237/260 [01:48<00:09,  2.34it/s]2025-08-15 11:42:07,650 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:08,781 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  92%|█████████▏| 238/260 [01:50<00:11,  1.86it/s]2025-08-15 11:42:08,791 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:08,840 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  94%|█████████▍| 245/260 [01:50<00:04,  3.57it/s]2025-08-15 11:42:09,094 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:09,999 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:10,016 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:10,033 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:10,050 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:10,052 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:10,059 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:10,440 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:10,900 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:11,543 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:12,152 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:12,154 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:12,156 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:12,157 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:12,160 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:12,164 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:12,166 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:12,168 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:12,169 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:12,170 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  95%|█████████▌| 248/260 [01:53<00:05,  2.13it/s]2025-08-15 11:42:13,787 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:13,897 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  97%|█████████▋| 253/260 [01:55<00:02,  2.35it/s]2025-08-15 11:42:13,945 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:14,209 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-15 11:42:14,228 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  98%|█████████▊| 255/260 [01:55<00:01,  2.63it/s]2025-08-15 11:42:14,572 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  98%|█████████▊| 256/260 [01:55<00:01,  2.65it/s]2025-08-15 11:42:15,228 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  99%|█████████▉| 257/260 [01:56<00:01,  2.41it/s]2025-08-15 11:42:15,426 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  99%|█████████▉| 258/260 [01:56<00:00,  2.62it/s]2025-08-15 11:42:15,994 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating: 100%|█████████▉| 259/260 [01:57<00:00,  2.42it/s]2025-08-15 11:42:16,750 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating: 100%|██████████| 260/260 [01:58<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.9009, 'answer_relevancy': 0.9495, 'context_precision': 0.8962, 'context_recall': 0.8987}\n"
     ]
    }
   ],
   "source": [
    "from app.config import configs\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = configs.OPENAI_API_KEY\n",
    "\n",
    "result = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "print(result) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "58e04b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from call_model import call_chat_once\n",
    "import asyncio\n",
    "import httpx\n",
    "from uuid import uuid4\n",
    "import json\n",
    "import datasets\n",
    "\n",
    "from typing import Optional\n",
    "import os\n",
    "import csv\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b253fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eval_dataset = load_dataset(\"csv\", data_files=\"data/eval.csv\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "86cfe4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['context', 'question', 'answer', 'source_doc', 'groundedness_score', 'groundedness_eval', 'relevance_score', 'relevance_eval', 'standalone_score', 'standalone_eval'],\n",
      "    num_rows: 52\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "62badec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"data/eval_rag_results.csv\"\n",
    "\n",
    "def load_existing_results(file_path):\n",
    "    dataset = []\n",
    "    existing_questions = set()\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                existing_questions.add(row[\"user_input\"])\n",
    "                # parse retrieved_contexts lại từ JSON\n",
    "                row[\"retrieved_contexts\"] = json.loads(row[\"retrieved_contexts\"])\n",
    "                dataset.append(row)\n",
    "    return existing_questions, dataset\n",
    "\n",
    "\n",
    "def append_result_to_csv(file_path, row, fieldnames):\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    with open(file_path, \"a\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    verbose: Optional[bool] = False\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    fieldnames = [\"user_input\", \"retrieved_contexts\", \"response\", \"reference\"]\n",
    "\n",
    "    processed_questions, dataset = load_existing_results(OUTPUT_FILE)\n",
    "    print(f\"Đã có {len(processed_questions)} câu hỏi xử lý trước đó, sẽ bỏ qua chúng.\")\n",
    "\n",
    "\n",
    "    for i, example in enumerate(tqdm(eval_dataset)):\n",
    "        question = example[\"question\"]\n",
    "        print(question)\n",
    "        if question in processed_questions:\n",
    "            continue\n",
    "        \n",
    "        payload = {\n",
    "            \"question\": question,\n",
    "            \"session_id\": str(uuid4()),\n",
    "            \"chat_history\": []\n",
    "        }\n",
    "\n",
    "        # try:\n",
    "        relevant_docs, answer = asyncio.run(call_chat_once(payload))\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error from question: '{question}': {e}\")\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "\n",
    "        row = {\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": json.dumps(\n",
    "                [doc.get('page_content') for doc in relevant_docs],\n",
    "                ensure_ascii=False\n",
    "            ),\n",
    "            \"response\": answer,\n",
    "            \"reference\": example['answer']\n",
    "        }\n",
    "        append_result_to_csv(OUTPUT_FILE, row, fieldnames)\n",
    "        dataset.append({\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": json.loads(row[\"retrieved_contexts\"]),\n",
    "            \"response\": answer,\n",
    "            \"reference\": example['answer']\n",
    "        })\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd2530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã có 5 câu hỏi xử lý trước đó, sẽ bỏ qua chúng.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who proposed the hybrid Vision Transformer model?\n",
      "Who are the authors of the Longformer paper?\n",
      "How many layers does the distilbert-base-uncased model have?\n",
      "What is the style of TPU access when using Google Colab?\n",
      "What is the default label inferred by Gradio for the input parameter in the GUI?\n",
      "What does the 'beta_start' parameter value mean in DDPMScheduler?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:46:30,077 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 12%|█▏        | 6/52 [00:08<01:01,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you register a custom Resnet model to the auto classes in Transformers?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:46:36,260 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 13%|█▎        | 7/52 [00:14<01:41,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the author of the paper 'Learning Transferable Visual Models From Natural Language Supervision'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:46:39,864 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 15%|█▌        | 8/52 [00:17<01:52,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the purpose of local attention in Longformer?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:46:44,486 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 17%|█▋        | 9/52 [00:22<02:10,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What library is used to import the text encoder?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:46:48,595 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 19%|█▉        | 10/52 [00:26<02:19,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What license is used for TheBloke/Chronohermes-Grad-L2-13B-GPTQ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:46:52,788 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 21%|██        | 11/52 [00:30<02:25,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a potential legal issue with using outputs from language models?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:46:56,392 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 23%|██▎       | 12/52 [00:34<02:22,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the filename used when downloading the model from the Hugging Face Hub?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:46:59,381 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 25%|██▌       | 13/52 [00:37<02:12,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What method is used by ViTHybridImageProcessor?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:47:03,147 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 27%|██▋       | 14/52 [00:41<02:13,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who contributed the code component for syntax highlighting in Gradio?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:47:08,425 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 29%|██▉       | 15/52 [00:46<02:28,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where can you find the Apache License, Version 2.0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:47:11,847 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 31%|███       | 16/52 [00:49<02:18,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What tool can be used for sentiment analysis in NLP?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:47:17,666 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 33%|███▎      | 17/52 [00:55<02:34,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who are the authors of the paper associated with Graphormer?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:47:21,339 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 35%|███▍      | 18/52 [00:59<02:22,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What function does Accelerate provide to determine a device map?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:47:25,040 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 37%|███▋      | 19/52 [01:02<02:13,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is the __call__ method decorated with torch.no_grad?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:47:29,130 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 38%|███▊      | 20/52 [01:07<02:10,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the latency in milliseconds with fp16 optimization?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:47:32,316 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 40%|████      | 21/52 [01:10<01:57,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can you share a Gradio demo publicly?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:47:36,284 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 42%|████▏     | 22/52 [01:14<01:55,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the Top 1 Accuracy of swsl_resnet18 on ImageNet?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:47:39,192 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 44%|████▍     | 23/52 [01:17<01:43,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the license for the weights of the SWSL ResNet models?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:47:42,385 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 46%|████▌     | 24/52 [01:20<01:36,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who proposed Consistency Models?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:47:45,227 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 48%|████▊     | 25/52 [01:23<01:28,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the Top 1 Accuracy of tv_resnet152 on ImageNet?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:47:48,670 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 50%|█████     | 26/52 [01:26<01:26,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under which license is the HuggingFace Transformers library released?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 11:47:51,302 - INFO - HTTP Request: POST http://localhost:8001/api/v1/chat-evaluation \"HTTP/1.1 200 OK\"\n",
      " 52%|█████▏    | 27/52 [01:29<01:17,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the model structure of GPTSAN?\n"
     ]
    }
   ],
   "source": [
    "data = run_rag_tests(eval_dataset=eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    FactualCorrectness\n",
    ")\n",
    "evaluation_dataset = EvaluationDataset.from_list(data)\n",
    "\n",
    "# Khởi tạo các metric\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "]\n",
    "# metrics = [FactualCorrectness()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9063c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(evaluation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617a4806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]2025-08-13 11:44:14,131 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:14,278 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:14,389 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:14,407 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:14,437 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:14,453 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:14,462 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:14,477 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:14,503 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:14,524 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:14,550 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:14,578 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:14,589 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:15,371 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:15,848 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:16,081 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:16,086 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:16,090 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:16,095 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:16,099 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:16,103 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:16,106 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:16,111 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   5%|▌         | 1/20 [00:08<02:42,  8.53s/it]2025-08-13 11:44:16,659 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:17,207 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:18,141 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:18,876 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:19,565 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:20,162 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  10%|█         | 2/20 [00:12<01:47,  5.99s/it]2025-08-13 11:44:20,365 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:20,367 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:20,369 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:20,370 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:20,373 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:20,375 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:20,377 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:21,746 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:21,807 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:21,965 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:22,136 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:22,490 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:23,128 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:23,584 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:23,786 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:23,788 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:23,789 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:23,790 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:23,791 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  50%|█████     | 10/20 [00:16<00:11,  1.15s/it]2025-08-13 11:44:24,155 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:25,510 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  65%|██████▌   | 13/20 [00:17<00:06,  1.03it/s]2025-08-13 11:44:25,651 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:26,285 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:26,788 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:27,147 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:27,389 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:27,732 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  70%|███████   | 14/20 [00:20<00:06,  1.13s/it]2025-08-13 11:44:28,508 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  75%|███████▌  | 15/20 [00:20<00:05,  1.08s/it]2025-08-13 11:44:28,815 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:29,489 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  80%|████████  | 16/20 [00:21<00:04,  1.06s/it]2025-08-13 11:44:29,966 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  85%|████████▌ | 17/20 [00:22<00:02,  1.07it/s]2025-08-13 11:44:30,402 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:30,935 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-13 11:44:32,798 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  90%|█████████ | 18/20 [00:25<00:02,  1.37s/it]2025-08-13 11:44:35,226 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  95%|█████████▌| 19/20 [00:27<00:01,  1.64s/it]2025-08-13 11:44:35,754 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating: 100%|██████████| 20/20 [00:28<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.8000, 'answer_relevancy': 0.9630, 'context_precision': 0.7500, 'context_recall': 0.8000}\n"
     ]
    }
   ],
   "source": [
    "from app.config import configs\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = configs.OPENAI_API_KEY\n",
    "\n",
    "result = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "print(result) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fde7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "from langchain_core.prompts import MessagesPlaceholder, ChatPromptTemplate, PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from app.config import configs\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=configs.HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7326f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6787da4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2647/2647 [00:00<00:00, 19826.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ef65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4o',api_key=configs.OPENAI_API_KEY)\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "            \"\"\"Get question and answer from context\"\"\"\n",
    "\n",
    "            factoid_question: str = Field(\n",
    "                description=\"Factoid question\")\n",
    "            answer: str = Field(description=\"Answer\")\n",
    "\n",
    "def get_question_answer(llm_client: ChatOpenAI, system_prompt, context):\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"Now here is the context. Context: {context}\\n\")\n",
    "        ]\n",
    "    )\n",
    "    llm_with_struct_output = llm_client.with_structured_output(QuestionAnswer)\n",
    "    chain = qa_prompt | llm_with_struct_output\n",
    "    response = chain.invoke({\"context\": context})\n",
    "    return response\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4509f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8edfd065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100 QA couples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:04<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N_GENERATIONS = 100  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = get_question_answer(llm, QA_generation_prompt, context=sampled_context.page_content)\n",
    "    try:\n",
    "        question = output_QA_couple.factoid_question\n",
    "        answer = output_QA_couple.answer\n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b37b000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': '!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Hybrid Vision Transformer (ViT Hybrid)\\n\\n## Overview\\n\\nThe hybrid Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\\nat Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\\nUszkoreit, Neil Houlsby. It\\'s the first paper that successfully trains a Transformer encoder on ImageNet, attaining\\nvery good results compared to familiar convolutional architectures. ViT hybrid is a slight variant of the [plain Vision Transformer](vit),\\nby leveraging a convolutional backbone (specifically, [BiT](bit)) whose features are used as initial \"tokens\" for the Transformer.\\n\\nThe abstract from the paper is the following:',\n",
       "  'question': 'Who proposed the hybrid Vision Transformer model?',\n",
       "  'answer': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.',\n",
       "  'source_doc': 'huggingface/transformers/blob/main/docs/source/en/model_doc/vit_hybrid.md'},\n",
       " {'context': '## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@article{DBLP:journals/corr/abs-1801-04381,\\n  author    = {Mark Sandler and\\n               Andrew G. Howard and\\n               Menglong Zhu and\\n               Andrey Zhmoginov and\\n               Liang{-}Chieh Chen},\\n  title     = {Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification,\\n               Detection and Segmentation},\\n  journal   = {CoRR},\\n  volume    = {abs/1801.04381},\\n  year      = {2018},\\n  url       = {http://arxiv.org/abs/1801.04381},\\n  archivePrefix = {arXiv},\\n  eprint    = {1801.04381},\\n  timestamp = {Tue, 12 Jan 2021 15:30:06 +0100},\\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-04381.bib},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n```',\n",
       "  'question': 'Who is the first author of the article cited in the context?',\n",
       "  'answer': 'Mark Sandler',\n",
       "  'source_doc': 'huggingface/pytorch-image-models/blob/main/docs/models/mobilenet-v2.md'},\n",
       " {'context': '## Summaries\\n\\n### [Longformer - The Long-Document Transformer](https://arxiv.org/abs/2004.05150)\\n\\nIz Beltagy, Matthew E. Peters, Arman Cohan\\n\\nLongformer addresses the memory bottleneck of transformers by replacing conventional self-attention with a combination of windowed/local/sparse (cf. [Sparse Transformers (2019)](https://arxiv.org/abs/1904.10509)) attention and global attention that scales linearly with the sequence length. As opposed to previous long-range transformer models (e.g. [Transformer-XL (2019)](https://arxiv.org/abs/1901.02860), [Reformer (2020)](https://arxiv.org/abs/2001.04451), [Adaptive Attention Span (2019)](https://arxiv.org/abs/1905.07799)), Longformer‚Äôs self-attention layer is designed as a drop-in replacement for the standard self-attention, thus making it possible to leverage pre-trained checkpoints for further pre-training and/or fine-tuning on long sequence tasks.\\n\\nThe standard self-attention matrix (Figure a) scales quadratically with the input length:\\n\\n<figure>\\n  <img src=\"/blog/assets/14_long_range_transformers/Longformer.png\" alt=\"Longformer attention\"/>\\n  <figcaption>Figure taken from Longformer</figcaption>\\n</figure>',\n",
       "  'question': 'Who are the authors of the Longformer paper?',\n",
       "  'answer': 'Iz Beltagy, Matthew E. Peters, Arman Cohan',\n",
       "  'source_doc': 'huggingface/blog/blob/main/long-range-transformers.md'},\n",
       " {'context': '- `distilbert-base-uncased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-uncased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 66M parameters.\\n- `distilbert-base-uncased-distilled-squad`: A finetuned version of `distilbert-base-uncased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 86.9 on the dev set (for comparison, Bert `bert-base-uncased` version reaches a 88.5 F1 score).\\n- `distilbert-base-cased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-cased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 65M parameters.\\n- `distilbert-base-cased-distilled-squad`: A finetuned version of `distilbert-base-cased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 87.1 on the dev set (for comparison, Bert `bert-base-cased` version reaches a 88.7 F1 score).\\n- `distilbert-base-german-cased`: DistilBERT German language model pretrained on 1/2 of the data used to pretrain Bert using distillation with the supervision of the `bert-base-german-dbmdz-cased` version of German DBMDZ Bert. For NER tasks the model reaches a F1 score of 83.49 on the CoNLL-2003 test set (for comparison, `bert-base-german-dbmdz-cased` reaches a 84.52 F1 score), and a F1 score of 85.23 on the GermEval 2014 test set (`bert-base-german-dbmdz-cased` reaches a 86.89 F1 score).',\n",
       "  'question': 'How many layers does the distilbert-base-uncased model have?',\n",
       "  'answer': '6',\n",
       "  'source_doc': 'huggingface/transformers/blob/main/examples/research_projects/distillation/README.md'},\n",
       " {'context': 'New users are often very confused by the range of TPUs, and the different ways to access them. The first key distinction to understand is the difference between **TPU Nodes** and **TPU VMs.**\\n\\nWhen you use a **TPU Node**, you are effectively indirectly accessing a remote TPU. You will need a separate VM, which will initialize your network and data pipeline and then forward them to the remote node. When you use a TPU on Google Colab, you are accessing it in the **TPU Node** style.\\n\\nUsing TPU Nodes can have some quite unexpected behaviour for people who aren‚Äôt used to them! In particular, because the TPU is located on a physically different system to the machine you‚Äôre running your Python code on, your data cannot be local to your machine - any data pipeline that loads from your machine‚Äôs internal storage will totally fail! Instead, data must be stored in Google Cloud Storage where your data pipeline can still access it, even when the pipeline is running on the remote TPU node.\\n\\n<Tip>\\n\\nIf you can fit all your data in memory as `np.ndarray` or `tf.Tensor`, then you can `fit()` on that data even when using Colab or a TPU Node, without needing to upload it to Google Cloud Storage.\\n\\n</Tip>\\n\\n<Tip>\\n\\n**ü§óSpecific Hugging Face Tipü§ó:** The methods `Dataset.to_tf_dataset()` and its higher-level wrapper `model.prepare_tf_dataset()` , which you will see throughout our TF code examples, will both fail on a TPU Node. The reason for this is that even though they create a `tf.data.Dataset` it is not a ‚Äúpure‚Äù `tf.data` pipeline and uses `tf.numpy_function` or `Dataset.from_generator()` to stream data from the underlying HuggingFace `Dataset`. This HuggingFace `Dataset` is backed by data that is on a local disc and which the remote TPU Node will not be able to read.\\n\\n</Tip>',\n",
       "  'question': 'What is the style of TPU access when using Google Colab?',\n",
       "  'answer': 'TPU Node style.',\n",
       "  'source_doc': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "108ab595",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class QuestionCritique(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the evaluation result for a given question.\n",
    "    \n",
    "    This model contains both a written assessment and a numerical rating \n",
    "    that reflect the quality, accuracy, or relevance of the question.\n",
    "    \"\"\"\n",
    "    evaluation: str = Field(\n",
    "        description=\"A detailed written evaluation of the question, highlighting strengths, weaknesses, and areas for improvement.\"\n",
    "    )\n",
    "    total_rating: int = Field(\n",
    "        ge=1, \n",
    "        le=5, \n",
    "        description=\"An integer score from 1 (lowest) to 5 (highest) representing the overall quality of the question.\"\n",
    "    )\n",
    "\n",
    "def get_grounded_critique_prompt(llm_client: ChatOpenAI,system_prompt, context, question):\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"Now here are the question and context.\\n Question: {question}\\n Context: {context}\\n \"),\n",
    "        ]\n",
    "    )\n",
    "    llm_with_struct_output = llm_client.with_structured_output(QuestionCritique)\n",
    "    chain = qa_prompt | llm_with_struct_output\n",
    "    response = chain.invoke({\"context\": context, \"question\": question})\n",
    "    return response\n",
    "\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\"\"\"\n",
    "\n",
    "def get_evaluation_question(llm_client: ChatOpenAI,system_prompt, question):\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"Now here is the question.\\n Question: {question}\\n\"),\n",
    "        ]\n",
    "    )\n",
    "    llm_with_struct_output = llm_client.with_structured_output(QuestionCritique)\n",
    "    chain = qa_prompt | llm_with_struct_output\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb4011dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [14:50<00:00,  8.91s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": get_grounded_critique_prompt(\n",
    "            llm,\n",
    "            question_groundedness_critique_prompt,context=output[\"context\"], question=output[\"question\"],\n",
    "        ),\n",
    "        \"relevance\": get_evaluation_question(\n",
    "            llm,\n",
    "            question_relevance_critique_prompt, question=output[\"question\"],\n",
    "        ),\n",
    "        \"standalone\": get_evaluation_question(\n",
    "            llm,\n",
    "            question_standalone_critique_prompt, question=output[\"question\"],\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = int(evaluation.total_rating), evaluation.evaluation\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1507ef4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': '!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Hybrid Vision Transformer (ViT Hybrid)\\n\\n## Overview\\n\\nThe hybrid Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\\nat Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\\nUszkoreit, Neil Houlsby. It\\'s the first paper that successfully trains a Transformer encoder on ImageNet, attaining\\nvery good results compared to familiar convolutional architectures. ViT hybrid is a slight variant of the [plain Vision Transformer](vit),\\nby leveraging a convolutional backbone (specifically, [BiT](bit)) whose features are used as initial \"tokens\" for the Transformer.\\n\\nThe abstract from the paper is the following:',\n",
       "  'question': 'Who proposed the hybrid Vision Transformer model?',\n",
       "  'answer': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.',\n",
       "  'source_doc': 'huggingface/transformers/blob/main/docs/source/en/model_doc/vit_hybrid.md',\n",
       "  'groundedness_score': 5,\n",
       "  'groundedness_eval': 'The context provides very detailed information about the hybrid Vision Transformer model, including the exact authors who proposed it. The list of authors is comprehensive and directly answers the question. The context is clear and unambiguous, leaving no room for misinterpretation regarding who proposed the model. It mentions the specific paper in which the model was proposed, along with all the contributing authors, making it very easy to answer the question accurately. Therefore, the context is perfectly aligned to answer the question.',\n",
       "  'relevance_score': 2,\n",
       "  'relevance_eval': \"The question 'Who proposed the hybrid Vision Transformer model?' is relevant to those interested in the development and history of machine learning models, particularly in the field of computer vision. However, it is not directly related to NLP applications or the Hugging Face ecosystem, which primarily focuses on natural language processing tasks. Therefore, while the question might be useful for those studying machine learning models more generally, it is not specifically useful for NLP developers using Hugging Face tools. This limits its usefulness to the target audience specified.\",\n",
       "  'standalone_score': 5,\n",
       "  'standalone_eval': \"The question 'Who proposed the hybrid Vision Transformer model?' is context-independent and can be understood on its own. It asks for the name of an individual or group who proposed a specific model in the field of machine learning. Although it refers to a specific model, it does not require additional context to comprehend the question itself. Anyone familiar with Vision Transformers and their development can attempt to answer it with the right resources.\"},\n",
       " {'context': '## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@article{DBLP:journals/corr/abs-1801-04381,\\n  author    = {Mark Sandler and\\n               Andrew G. Howard and\\n               Menglong Zhu and\\n               Andrey Zhmoginov and\\n               Liang{-}Chieh Chen},\\n  title     = {Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification,\\n               Detection and Segmentation},\\n  journal   = {CoRR},\\n  volume    = {abs/1801.04381},\\n  year      = {2018},\\n  url       = {http://arxiv.org/abs/1801.04381},\\n  archivePrefix = {arXiv},\\n  eprint    = {1801.04381},\\n  timestamp = {Tue, 12 Jan 2021 15:30:06 +0100},\\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-04381.bib},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n```',\n",
       "  'question': 'Who is the first author of the article cited in the context?',\n",
       "  'answer': 'Mark Sandler',\n",
       "  'source_doc': 'huggingface/pytorch-image-models/blob/main/docs/models/mobilenet-v2.md',\n",
       "  'groundedness_score': 5,\n",
       "  'groundedness_eval': \"The context provides a BibTeX citation that includes the 'author' field listing multiple authors of the article. According to typical BibTeX formatting, the first name listed in the 'author' field is the first author of the article. In this case, 'Mark Sandler' is the first name listed, making it clear and unambiguous that he is the first author of the cited article. Thus, the question can be answered directly and unambiguously from the provided context.\",\n",
       "  'relevance_score': 2,\n",
       "  'relevance_eval': \"This question is not directly related to building NLP applications with the Hugging Face ecosystem. It pertains more to academic referencing or document analysis, which might be a task in NLP, but it doesn't specifically relate to the tools, models, or libraries provided by Hugging Face. Therefore, its relevance and usefulness to developers working within this ecosystem are quite limited.\",\n",
       "  'standalone_score': 1,\n",
       "  'standalone_eval': \"This question heavily relies on additional context. It refers explicitly to an 'article cited in the context,' which indicates that there is a specific context or document that the question is referring to. Without access to this context, it's impossible to identify the first author of the article, making the question entirely dependent on external information.\"},\n",
       " {'context': '## Summaries\\n\\n### [Longformer - The Long-Document Transformer](https://arxiv.org/abs/2004.05150)\\n\\nIz Beltagy, Matthew E. Peters, Arman Cohan\\n\\nLongformer addresses the memory bottleneck of transformers by replacing conventional self-attention with a combination of windowed/local/sparse (cf. [Sparse Transformers (2019)](https://arxiv.org/abs/1904.10509)) attention and global attention that scales linearly with the sequence length. As opposed to previous long-range transformer models (e.g. [Transformer-XL (2019)](https://arxiv.org/abs/1901.02860), [Reformer (2020)](https://arxiv.org/abs/2001.04451), [Adaptive Attention Span (2019)](https://arxiv.org/abs/1905.07799)), Longformer‚Äôs self-attention layer is designed as a drop-in replacement for the standard self-attention, thus making it possible to leverage pre-trained checkpoints for further pre-training and/or fine-tuning on long sequence tasks.\\n\\nThe standard self-attention matrix (Figure a) scales quadratically with the input length:\\n\\n<figure>\\n  <img src=\"/blog/assets/14_long_range_transformers/Longformer.png\" alt=\"Longformer attention\"/>\\n  <figcaption>Figure taken from Longformer</figcaption>\\n</figure>',\n",
       "  'question': 'Who are the authors of the Longformer paper?',\n",
       "  'answer': 'Iz Beltagy, Matthew E. Peters, Arman Cohan',\n",
       "  'source_doc': 'huggingface/blog/blob/main/long-range-transformers.md',\n",
       "  'groundedness_score': 5,\n",
       "  'groundedness_eval': \"The context provides the exact information needed to answer the question. It clearly lists the authors of the 'Longformer' paper as Iz Beltagy, Matthew E. Peters, and Arman Cohan. The question is straightforward, and the context gives a direct answer without any ambiguity or need for inference. Therefore, the question is completely answerable with the given context.\",\n",
       "  'relevance_score': 2,\n",
       "  'relevance_eval': 'This question is not directly useful for machine learning developers who are building NLP applications using the Hugging Face ecosystem. Knowing the authors of the Longformer paper does not provide any technical or practical insights into how to implement, use, or optimize the Longformer model within a project. Developers would benefit more from questions about model capabilities, integration specifics, performance benchmarks, or application examples. While understanding the authorship can be a part of academic interest, it is not directly relevant to the core development tasks related to NLP applications.',\n",
       "  'standalone_score': 4,\n",
       "  'standalone_eval': 'This question is specific and asks for the authors of a well-known paper, \"the Longformer paper.\" However, it assumes that the reader knows about the Longformer model and its related paper. The question is quite clear in its intention, and anyone familiar with academic papers or with access to academic databases could answer it. It does not intrinsically depend on additional context to be understood, aside from the general knowledge of what a paper is and that it has authors. Therefore, it leans towards being context-independent.'},\n",
       " {'context': '- `distilbert-base-uncased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-uncased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 66M parameters.\\n- `distilbert-base-uncased-distilled-squad`: A finetuned version of `distilbert-base-uncased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 86.9 on the dev set (for comparison, Bert `bert-base-uncased` version reaches a 88.5 F1 score).\\n- `distilbert-base-cased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-cased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 65M parameters.\\n- `distilbert-base-cased-distilled-squad`: A finetuned version of `distilbert-base-cased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 87.1 on the dev set (for comparison, Bert `bert-base-cased` version reaches a 88.7 F1 score).\\n- `distilbert-base-german-cased`: DistilBERT German language model pretrained on 1/2 of the data used to pretrain Bert using distillation with the supervision of the `bert-base-german-dbmdz-cased` version of German DBMDZ Bert. For NER tasks the model reaches a F1 score of 83.49 on the CoNLL-2003 test set (for comparison, `bert-base-german-dbmdz-cased` reaches a 84.52 F1 score), and a F1 score of 85.23 on the GermEval 2014 test set (`bert-base-german-dbmdz-cased` reaches a 86.89 F1 score).',\n",
       "  'question': 'How many layers does the distilbert-base-uncased model have?',\n",
       "  'answer': '6',\n",
       "  'source_doc': 'huggingface/transformers/blob/main/examples/research_projects/distillation/README.md',\n",
       "  'groundedness_score': 5,\n",
       "  'groundedness_eval': 'The context provides specific details about the `distilbert-base-uncased` model, including the number of layers it has. According to the context, the model has 6 layers. Therefore, the question about the number of layers in the `distilbert-base-uncased` model is directly and clearly answerable from the provided context without any ambiguity. The information is explicit and straightforward, ensuring that the question can be answered accurately.',\n",
       "  'relevance_score': 5,\n",
       "  'relevance_eval': \"The question is quite specific and pertains to a particular model within the Hugging Face ecosystem, namely the 'distilbert-base-uncased' model. Knowing the number of layers in a model is important for understanding its architecture, which can impact how a developer might choose to use or modify the model for their specific NLP tasks. This information is directly relevant and useful for machine learning developers working with transformer models, as it aids in tasks like model selection, understanding computational costs, and customizing models for specific applications. This question is highly relevant for practitioners looking to gain detailed knowledge of the model architecture, which is a common task when using Hugging Face's resources for building NLP applications.\",\n",
       "  'standalone_score': 5,\n",
       "  'standalone_eval': 'This question is context-independent because it seeks specific information about a well-known model, DistilBERT, which is a variant of the BERT model. The question does not depend on any external context or document to be understood. The DistilBERT model is a common term in machine learning and natural language processing literature, and the question is straightforward in asking for the number of layers in a specific model.'},\n",
       " {'context': 'New users are often very confused by the range of TPUs, and the different ways to access them. The first key distinction to understand is the difference between **TPU Nodes** and **TPU VMs.**\\n\\nWhen you use a **TPU Node**, you are effectively indirectly accessing a remote TPU. You will need a separate VM, which will initialize your network and data pipeline and then forward them to the remote node. When you use a TPU on Google Colab, you are accessing it in the **TPU Node** style.\\n\\nUsing TPU Nodes can have some quite unexpected behaviour for people who aren‚Äôt used to them! In particular, because the TPU is located on a physically different system to the machine you‚Äôre running your Python code on, your data cannot be local to your machine - any data pipeline that loads from your machine‚Äôs internal storage will totally fail! Instead, data must be stored in Google Cloud Storage where your data pipeline can still access it, even when the pipeline is running on the remote TPU node.\\n\\n<Tip>\\n\\nIf you can fit all your data in memory as `np.ndarray` or `tf.Tensor`, then you can `fit()` on that data even when using Colab or a TPU Node, without needing to upload it to Google Cloud Storage.\\n\\n</Tip>\\n\\n<Tip>\\n\\n**ü§óSpecific Hugging Face Tipü§ó:** The methods `Dataset.to_tf_dataset()` and its higher-level wrapper `model.prepare_tf_dataset()` , which you will see throughout our TF code examples, will both fail on a TPU Node. The reason for this is that even though they create a `tf.data.Dataset` it is not a ‚Äúpure‚Äù `tf.data` pipeline and uses `tf.numpy_function` or `Dataset.from_generator()` to stream data from the underlying HuggingFace `Dataset`. This HuggingFace `Dataset` is backed by data that is on a local disc and which the remote TPU Node will not be able to read.\\n\\n</Tip>',\n",
       "  'question': 'What is the style of TPU access when using Google Colab?',\n",
       "  'answer': 'TPU Node style.',\n",
       "  'source_doc': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md',\n",
       "  'groundedness_score': 5,\n",
       "  'groundedness_eval': \"The context provides an explanation of the TPU access style when using Google Colab. It clearly states that when using a TPU on Google Colab, one accesses it in the 'TPU Node' style, where the TPU is remotely accessed and requires data to be stored in Google Cloud Storage rather than locally. This information directly answers the question about the style of TPU access in Google Colab. The context adequately clarifies the differences between TPU Nodes and TPU VMs and provides specific details about data handling when using TPUs in this manner. Therefore, the question is answerable clearly and unambiguously with the given context.\",\n",
       "  'relevance_score': 3,\n",
       "  'relevance_eval': \"This question is somewhat relevant for machine learning developers, particularly those who are interested in utilizing TPUs for their NLP applications using Google Colab. TPUs (Tensor Processing Units) can significantly accelerate the training of machine learning models, and understanding the access style (e.g., how to enable and utilize TPU resources) is crucial for optimizing resource usage and performance. However, the question is quite specific to Google Colab and doesn't directly address the Hugging Face ecosystem or NLP applications. Thus, while it is useful, it is not directly aligned with the Hugging Face ecosystem focus, which slightly limits its broader applicability in that context.\",\n",
       "  'standalone_score': 5,\n",
       "  'standalone_eval': 'The question is clear and can be understood without additional context. It asks specifically about the style of TPU (Tensor Processing Unit) access when using Google Colab, a cloud service. An operator with knowledge of Google Colab or access to documentation would understand that the question is about the configuration or manner in which TPUs are accessed via this service. No specific context or prior information is necessary to comprehend the question, as it is both specific and self-contained.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b27c097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset before filtering:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who proposed the hybrid Vision Transformer model?</td>\n",
       "      <td>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is the first author of the article cited in the context?</td>\n",
       "      <td>Mark Sandler</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who are the authors of the Longformer paper?</td>\n",
       "      <td>Iz Beltagy, Matthew E. Peters, Arman Cohan</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many layers does the distilbert-base-uncased model have?</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the style of TPU access when using Google Colab?</td>\n",
       "      <td>TPU Node style.</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>What is the role of machine learning in predictive healthcare networks?</td>\n",
       "      <td>Machine learning helps spot high-risk patients more quickly and efficiently, checks the spread of contractible diseases faster, manages epidemics better, identifies at-risk patients more accurately, and creates experiences that adapt to both hospital staff and patients.</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>What is the size of the Parquet file for the train split of the rotten_tomatoes dataset?</td>\n",
       "      <td>698845</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>What are the two types of guides for contributing to Gradio?</td>\n",
       "      <td>Use cases and Feature explanation.</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>What method is called to compute gradients in the training pipeline?</td>\n",
       "      <td>backward()</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Who created the Falcon language models?</td>\n",
       "      <td>Technology Innovation Institute</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    question  \\\n",
       "0                                          Who proposed the hybrid Vision Transformer model?   \n",
       "1                               Who is the first author of the article cited in the context?   \n",
       "2                                               Who are the authors of the Longformer paper?   \n",
       "3                               How many layers does the distilbert-base-uncased model have?   \n",
       "4                                   What is the style of TPU access when using Google Colab?   \n",
       "..                                                                                       ...   \n",
       "95                   What is the role of machine learning in predictive healthcare networks?   \n",
       "96  What is the size of the Parquet file for the train split of the rotten_tomatoes dataset?   \n",
       "97                              What are the two types of guides for contributing to Gradio?   \n",
       "98                      What method is called to compute gradients in the training pipeline?   \n",
       "99                                                   Who created the Falcon language models?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                            answer  \\\n",
       "0                                                                     Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.   \n",
       "1                                                                                                                                                                                                                                                                     Mark Sandler   \n",
       "2                                                                                                                                                                                                                                       Iz Beltagy, Matthew E. Peters, Arman Cohan   \n",
       "3                                                                                                                                                                                                                                                                                6   \n",
       "4                                                                                                                                                                                                                                                                  TPU Node style.   \n",
       "..                                                                                                                                                                                                                                                                             ...   \n",
       "95  Machine learning helps spot high-risk patients more quickly and efficiently, checks the spread of contractible diseases faster, manages epidemics better, identifies at-risk patients more accurately, and creates experiences that adapt to both hospital staff and patients.   \n",
       "96                                                                                                                                                                                                                                                                          698845   \n",
       "97                                                                                                                                                                                                                                              Use cases and Feature explanation.   \n",
       "98                                                                                                                                                                                                                                                                      backward()   \n",
       "99                                                                                                                                                                                                                                                 Technology Innovation Institute   \n",
       "\n",
       "    groundedness_score  relevance_score  standalone_score  \n",
       "0                    5                2                 5  \n",
       "1                    5                2                 1  \n",
       "2                    5                2                 4  \n",
       "3                    5                5                 5  \n",
       "4                    5                3                 5  \n",
       "..                 ...              ...               ...  \n",
       "95                   5                3                 5  \n",
       "96                   5                4                 5  \n",
       "97                   5                3                 5  \n",
       "98                   5                3                 4  \n",
       "99                   5                4                 5  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Final evaluation dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who proposed the hybrid Vision Transformer model?</td>\n",
       "      <td>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who are the authors of the Longformer paper?</td>\n",
       "      <td>Iz Beltagy, Matthew E. Peters, Arman Cohan</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many layers does the distilbert-base-uncased model have?</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the style of TPU access when using Google Colab?</td>\n",
       "      <td>TPU Node style.</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the default label inferred by Gradio for the input parameter in the GUI?</td>\n",
       "      <td>name</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What does the 'beta_start' parameter value mean in DDPMScheduler?</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How do you register a custom Resnet model to the auto classes in Transformers?</td>\n",
       "      <td>Use AutoConfig.register(\"resnet\", ResnetConfig), AutoModel.register(ResnetConfig, ResnetModel), and AutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification) to register the custom Resnet model to the auto classes.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Who is the author of the paper 'Learning Transferable Visual Models From Natural Language Supervision'?</td>\n",
       "      <td>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is the purpose of local attention in Longformer?</td>\n",
       "      <td>To take action for a given token using the local context, such as the two tokens to the left and right, and to build a representation of the whole sentence by stacking attention layers with a small window.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What library is used to import the text encoder?</td>\n",
       "      <td>Transformers</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What license is used for TheBloke/Chronohermes-Grad-L2-13B-GPTQ?</td>\n",
       "      <td>llama-2-community-license</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What is a potential legal issue with using outputs from language models?</td>\n",
       "      <td>The legal landscape surrounding Generative AI is currently very unclear, with several ongoing lawsuits.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What is the filename used when downloading the model from the Hugging Face Hub?</td>\n",
       "      <td>q-learning.pkl</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>What method is used by ViTHybridImageProcessor?</td>\n",
       "      <td>preprocess</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Who contributed the code component for syntax highlighting in Gradio?</td>\n",
       "      <td>@pngwn</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Where can you find the Apache License, Version 2.0?</td>\n",
       "      <td>http://www.apache.org/licenses/LICENSE-2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>What tool can be used for sentiment analysis in NLP?</td>\n",
       "      <td>transformers pipeline</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Who are the authors of the paper associated with Graphormer?</td>\n",
       "      <td>Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>What function does Accelerate provide to determine a device map?</td>\n",
       "      <td>infer_auto_device_map</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Why is the __call__ method decorated with torch.no_grad?</td>\n",
       "      <td>Because pipelines should not be used for training.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>What is the latency in milliseconds with fp16 optimization?</td>\n",
       "      <td>10.32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>How can you share a Gradio demo publicly?</td>\n",
       "      <td>Set `share=True` in the `launch()` method of the Gradio demo.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>What is the Top 1 Accuracy of swsl_resnet18 on ImageNet?</td>\n",
       "      <td>73.28%</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>What is the license for the weights of the SWSL ResNet models?</td>\n",
       "      <td>CC-BY-NC 4.0 license</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Who proposed Consistency Models?</td>\n",
       "      <td>Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>What is the Top 1 Accuracy of tv_resnet152 on ImageNet?</td>\n",
       "      <td>78.32%</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Under which license is the HuggingFace Transformers library released?</td>\n",
       "      <td>Apache License, Version 2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>What is the model structure of GPTSAN?</td>\n",
       "      <td>Prefix-LM</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Which LCM LoRA model is derived from the SDXL 1.0 base?</td>\n",
       "      <td>latent-consistency/lcm-lora-sdxl</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>What is the Top 1 Accuracy for tf_efficientnet_b6 on ImageNet?</td>\n",
       "      <td>84.11%</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>What utility allows starting multi-gpu training in a Jupyter Notebook?</td>\n",
       "      <td>notebook_launcher</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Who are the authors of 'Contrastive Search Is What You Need For Neural Text Generation'?</td>\n",
       "      <td>Yixuan Su and Nigel Collier</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Where can you find BERT's conversion script for porting from TensorFlow to PyTorch?</td>\n",
       "      <td>https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>What are the advantages of streaming mode over downloading a dataset?</td>\n",
       "      <td>1. Disk space: No local disk space is required as data is loaded to memory one-by-one. 2. Download and processing time: Allows immediate use of the dataset without waiting for full download. 3. Easy experimentation: Enables testing on a few samples without downloading the entire dataset.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>What teacher model is used with frugalscore_tiny_bert-base_mover-score?</td>\n",
       "      <td>BERT-Base</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>What class is used to compile the model to run on IPUs?</td>\n",
       "      <td>IPUTrainer</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>What is the default activation function used in DistilBertConfig?</td>\n",
       "      <td>gelu</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>What is the top 1 accuracy of mobilenetv3_large_100 on ImageNet?</td>\n",
       "      <td>75.77%</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>What version of Python is required to contribute to ü§ó Transformers?</td>\n",
       "      <td>Python 3.8 or above</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>What is the default value of HF_INFERENCE_ENDPOINT?</td>\n",
       "      <td>\"https://api-inference.huggingface.com\"</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>What is the name of the ResNet variant that uses squeeze-and-excitation blocks?</td>\n",
       "      <td>SE ResNet</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>What Python package needs to be installed for the Gradio demo?</td>\n",
       "      <td>gradio</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>What does ZeRO-powered data parallelism store on each GPU?</td>\n",
       "      <td>A slice of the model parameters, gradients, and optimizer states.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>What is the license URL for TheBloke/Llama2-22B-Daydreamer-v3-GGUF?</td>\n",
       "      <td>https://huggingface.co/TheBloke/Llama2-22B-Daydreamer-v3-GGUF/blob/main/LICENSE.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>What is the recommended scheduler for the IP-Adapter face model?</td>\n",
       "      <td>DDIMScheduler and EulerDiscreteScheduler</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>What is one advantage of using a generator of lists of texts in `train_new_from_iterator()`?</td>\n",
       "      <td>You will avoid loading the whole dataset into memory at once.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>What does the stride argument control in the tokenizer?</td>\n",
       "      <td>The stride argument controls the number of overlapping tokens.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>What is the role of machine learning in predictive healthcare networks?</td>\n",
       "      <td>Machine learning helps spot high-risk patients more quickly and efficiently, checks the spread of contractible diseases faster, manages epidemics better, identifies at-risk patients more accurately, and creates experiences that adapt to both hospital staff and patients.</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>What is the size of the Parquet file for the train split of the rotten_tomatoes dataset?</td>\n",
       "      <td>698845</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>What are the two types of guides for contributing to Gradio?</td>\n",
       "      <td>Use cases and Feature explanation.</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>What method is called to compute gradients in the training pipeline?</td>\n",
       "      <td>backward()</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Who created the Falcon language models?</td>\n",
       "      <td>Technology Innovation Institute</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   question  \\\n",
       "0                                                         Who proposed the hybrid Vision Transformer model?   \n",
       "2                                                              Who are the authors of the Longformer paper?   \n",
       "3                                              How many layers does the distilbert-base-uncased model have?   \n",
       "4                                                  What is the style of TPU access when using Google Colab?   \n",
       "6                          What is the default label inferred by Gradio for the input parameter in the GUI?   \n",
       "7                                         What does the 'beta_start' parameter value mean in DDPMScheduler?   \n",
       "13                           How do you register a custom Resnet model to the auto classes in Transformers?   \n",
       "15  Who is the author of the paper 'Learning Transferable Visual Models From Natural Language Supervision'?   \n",
       "18                                                    What is the purpose of local attention in Longformer?   \n",
       "20                                                         What library is used to import the text encoder?   \n",
       "21                                         What license is used for TheBloke/Chronohermes-Grad-L2-13B-GPTQ?   \n",
       "23                                 What is a potential legal issue with using outputs from language models?   \n",
       "28                          What is the filename used when downloading the model from the Hugging Face Hub?   \n",
       "30                                                          What method is used by ViTHybridImageProcessor?   \n",
       "31                                    Who contributed the code component for syntax highlighting in Gradio?   \n",
       "32                                                      Where can you find the Apache License, Version 2.0?   \n",
       "36                                                     What tool can be used for sentiment analysis in NLP?   \n",
       "37                                             Who are the authors of the paper associated with Graphormer?   \n",
       "38                                         What function does Accelerate provide to determine a device map?   \n",
       "39                                                 Why is the __call__ method decorated with torch.no_grad?   \n",
       "42                                              What is the latency in milliseconds with fp16 optimization?   \n",
       "43                                                                How can you share a Gradio demo publicly?   \n",
       "44                                                 What is the Top 1 Accuracy of swsl_resnet18 on ImageNet?   \n",
       "45                                           What is the license for the weights of the SWSL ResNet models?   \n",
       "46                                                                         Who proposed Consistency Models?   \n",
       "47                                                  What is the Top 1 Accuracy of tv_resnet152 on ImageNet?   \n",
       "51                                    Under which license is the HuggingFace Transformers library released?   \n",
       "54                                                                   What is the model structure of GPTSAN?   \n",
       "55                                                  Which LCM LoRA model is derived from the SDXL 1.0 base?   \n",
       "57                                           What is the Top 1 Accuracy for tf_efficientnet_b6 on ImageNet?   \n",
       "58                                   What utility allows starting multi-gpu training in a Jupyter Notebook?   \n",
       "60                 Who are the authors of 'Contrastive Search Is What You Need For Neural Text Generation'?   \n",
       "63                      Where can you find BERT's conversion script for porting from TensorFlow to PyTorch?   \n",
       "65                                    What are the advantages of streaming mode over downloading a dataset?   \n",
       "67                                  What teacher model is used with frugalscore_tiny_bert-base_mover-score?   \n",
       "71                                                  What class is used to compile the model to run on IPUs?   \n",
       "73                                        What is the default activation function used in DistilBertConfig?   \n",
       "75                                         What is the top 1 accuracy of mobilenetv3_large_100 on ImageNet?   \n",
       "77                                      What version of Python is required to contribute to ü§ó Transformers?   \n",
       "78                                                      What is the default value of HF_INFERENCE_ENDPOINT?   \n",
       "81                          What is the name of the ResNet variant that uses squeeze-and-excitation blocks?   \n",
       "82                                           What Python package needs to be installed for the Gradio demo?   \n",
       "83                                               What does ZeRO-powered data parallelism store on each GPU?   \n",
       "88                                      What is the license URL for TheBloke/Llama2-22B-Daydreamer-v3-GGUF?   \n",
       "89                                         What is the recommended scheduler for the IP-Adapter face model?   \n",
       "90             What is one advantage of using a generator of lists of texts in `train_new_from_iterator()`?   \n",
       "91                                                  What does the stride argument control in the tokenizer?   \n",
       "95                                  What is the role of machine learning in predictive healthcare networks?   \n",
       "96                 What is the size of the Parquet file for the train split of the rotten_tomatoes dataset?   \n",
       "97                                             What are the two types of guides for contributing to Gradio?   \n",
       "98                                     What method is called to compute gradients in the training pipeline?   \n",
       "99                                                                  Who created the Falcon language models?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                              answer  \\\n",
       "0                                                                                       Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.   \n",
       "2                                                                                                                                                                                                                                                         Iz Beltagy, Matthew E. Peters, Arman Cohan   \n",
       "3                                                                                                                                                                                                                                                                                                  6   \n",
       "4                                                                                                                                                                                                                                                                                    TPU Node style.   \n",
       "6                                                                                                                                                                                                                                                                                               name   \n",
       "7                                                                                                                                                                                                                                                                                             0.0001   \n",
       "13                                            Use AutoConfig.register(\"resnet\", ResnetConfig), AutoModel.register(ResnetConfig, ResnetModel), and AutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification) to register the custom Resnet model to the auto classes.   \n",
       "15                                                                                                             Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.   \n",
       "18                                                                                     To take action for a given token using the local context, such as the two tokens to the left and right, and to build a representation of the whole sentence by stacking attention layers with a small window.   \n",
       "20                                                                                                                                                                                                                                                                                      Transformers   \n",
       "21                                                                                                                                                                                                                                                                         llama-2-community-license   \n",
       "23                                                                                                                                                                                           The legal landscape surrounding Generative AI is currently very unclear, with several ongoing lawsuits.   \n",
       "28                                                                                                                                                                                                                                                                                    q-learning.pkl   \n",
       "30                                                                                                                                                                                                                                                                                        preprocess   \n",
       "31                                                                                                                                                                                                                                                                                            @pngwn   \n",
       "32                                                                                                                                                                                                                                                        http://www.apache.org/licenses/LICENSE-2.0   \n",
       "36                                                                                                                                                                                                                                                                             transformers pipeline   \n",
       "37                                                                                                                                                                                              Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.   \n",
       "38                                                                                                                                                                                                                                                                             infer_auto_device_map   \n",
       "39                                                                                                                                                                                                                                                Because pipelines should not be used for training.   \n",
       "42                                                                                                                                                                                                                                                                                             10.32   \n",
       "43                                                                                                                                                                                                                                     Set `share=True` in the `launch()` method of the Gradio demo.   \n",
       "44                                                                                                                                                                                                                                                                                            73.28%   \n",
       "45                                                                                                                                                                                                                                                                              CC-BY-NC 4.0 license   \n",
       "46                                                                                                                                                                                                                                       Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever   \n",
       "47                                                                                                                                                                                                                                                                                            78.32%   \n",
       "51                                                                                                                                                                                                                                                                       Apache License, Version 2.0   \n",
       "54                                                                                                                                                                                                                                                                                         Prefix-LM   \n",
       "55                                                                                                                                                                                                                                                                  latent-consistency/lcm-lora-sdxl   \n",
       "57                                                                                                                                                                                                                                                                                            84.11%   \n",
       "58                                                                                                                                                                                                                                                                                 notebook_launcher   \n",
       "60                                                                                                                                                                                                                                                                       Yixuan Su and Nigel Collier   \n",
       "63                                                                                                                                                       https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91   \n",
       "65  1. Disk space: No local disk space is required as data is loaded to memory one-by-one. 2. Download and processing time: Allows immediate use of the dataset without waiting for full download. 3. Easy experimentation: Enables testing on a few samples without downloading the entire dataset.   \n",
       "67                                                                                                                                                                                                                                                                                         BERT-Base   \n",
       "71                                                                                                                                                                                                                                                                                        IPUTrainer   \n",
       "73                                                                                                                                                                                                                                                                                              gelu   \n",
       "75                                                                                                                                                                                                                                                                                            75.77%   \n",
       "77                                                                                                                                                                                                                                                                               Python 3.8 or above   \n",
       "78                                                                                                                                                                                                                                                           \"https://api-inference.huggingface.com\"   \n",
       "81                                                                                                                                                                                                                                                                                         SE ResNet   \n",
       "82                                                                                                                                                                                                                                                                                            gradio   \n",
       "83                                                                                                                                                                                                                                 A slice of the model parameters, gradients, and optimizer states.   \n",
       "88                                                                                                                                                                                                               https://huggingface.co/TheBloke/Llama2-22B-Daydreamer-v3-GGUF/blob/main/LICENSE.txt   \n",
       "89                                                                                                                                                                                                                                                          DDIMScheduler and EulerDiscreteScheduler   \n",
       "90                                                                                                                                                                                                                                     You will avoid loading the whole dataset into memory at once.   \n",
       "91                                                                                                                                                                                                                                    The stride argument controls the number of overlapping tokens.   \n",
       "95                    Machine learning helps spot high-risk patients more quickly and efficiently, checks the spread of contractible diseases faster, manages epidemics better, identifies at-risk patients more accurately, and creates experiences that adapt to both hospital staff and patients.   \n",
       "96                                                                                                                                                                                                                                                                                            698845   \n",
       "97                                                                                                                                                                                                                                                                Use cases and Feature explanation.   \n",
       "98                                                                                                                                                                                                                                                                                        backward()   \n",
       "99                                                                                                                                                                                                                                                                   Technology Innovation Institute   \n",
       "\n",
       "    groundedness_score  relevance_score  standalone_score  \n",
       "0                    5                2                 5  \n",
       "2                    5                2                 4  \n",
       "3                    5                5                 5  \n",
       "4                    5                3                 5  \n",
       "6                    5                3                 5  \n",
       "7                    5                4                 5  \n",
       "13                   5                5                 5  \n",
       "15                   5                2                 5  \n",
       "18                   5                5                 5  \n",
       "20                   5                2                 4  \n",
       "21                   5                3                 5  \n",
       "23                   5                4                 5  \n",
       "28                   5                3                 4  \n",
       "30                   4                2                 4  \n",
       "31                   5                2                 4  \n",
       "32                   5                2                 5  \n",
       "36                   5                4                 5  \n",
       "37                   5                3                 4  \n",
       "38                   5                4                 5  \n",
       "39                   5                4                 5  \n",
       "42                   5                3                 4  \n",
       "43                   5                4                 5  \n",
       "44                   5                2                 5  \n",
       "45                   5                4                 4  \n",
       "46                   5                2                 5  \n",
       "47                   5                2                 5  \n",
       "51                   5                3                 5  \n",
       "54                   5                3                 5  \n",
       "55                   5                3                 5  \n",
       "57                   5                2                 5  \n",
       "58                   5                4                 5  \n",
       "60                   5                2                 5  \n",
       "63                   5                4                 5  \n",
       "65                   5                4                 5  \n",
       "67                   5                4                 5  \n",
       "71                   5                4                 5  \n",
       "73                   5                4                 5  \n",
       "75                   5                2                 5  \n",
       "77                   5                3                 5  \n",
       "78                   5                3                 4  \n",
       "81                   5                4                 5  \n",
       "82                   5                4                 5  \n",
       "83                   5                4                 4  \n",
       "88                   5                3                 5  \n",
       "89                   5                5                 5  \n",
       "90                   5                4                 5  \n",
       "91                   5                5                 5  \n",
       "95                   5                3                 5  \n",
       "96                   5                4                 5  \n",
       "97                   5                3                 5  \n",
       "98                   5                3                 4  \n",
       "99                   5                4                 5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 79.70ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "189679"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"relevance_score\"] >= 2)\n",
    "    & (generated_questions[\"standalone_score\"] >= 4)\n",
    "]\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)\n",
    "eval_dataset.to_csv('data/eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e51b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

user_input,retrieved_contexts,response,reference
"What architecture is the `tokenizers-linux-x64-musl` binary designed for?
","[""`tokenizers-linux-x64-musl`\nThis is the \\*\\*x86\\_64-unknown-linux-musl\\*\\* binary for `tokenizers`""]",The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.,x86_64-unknown-linux-musl
"What is the purpose of the BLIP-Diffusion model?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# BLIP-Diffusion\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. The abstract from the paper is:\n\\*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).\\*\nThe original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization. `BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## BlipDiffusionPipeline\n[[autodoc]] BlipDiffusionPipeline\n- all\n- \\_\\_call\\_\\_\n## BlipDiffusionControlNetPipeline\n[[autodoc]] BlipDiffusionControlNetPipeline\n- all\n- \\_\\_call\\_\\_"", ""For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.\\*\\*\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!"", ""A (denoising) diffusion model isn't that complex if you compare it to other generative models such as Normalizing Flows, GANs or VAEs: they all convert noise from some simple distribution to a data sample. This is also the case here where \\*\\*a neural network learns to gradually denoise data\\*\\* starting from pure noise. In a bit more detail for images, the set-up consists of 2 processes:\n\\* a fixed (or predefined) forward diffusion process \\\\(q\\\\) of our choosing, that gradually adds Gaussian noise to an image, until you end up with pure noise\n\\* a learned reverse denoising diffusion process \\\\(p\\_\\theta\\\\), where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image. ![](assets/78_annotated-diffusion/diffusion_figure.png)\n\nBoth the forward and reverse process indexed by \\\\(t\\\\) happen for some number of finite time steps \\\\(T\\\\) (the DDPM authors use \\\\(T=1000\\\\)). You start with \\\\(t=0\\\\) where you sample a real image \\\\(\\mathbf{x}\\_0\\\\) from your data distribution (let's say an image of a cat from ImageNet), and the forward process samples some noise from a Gaussian distribution at each time step \\\\(t\\\\), which is added to the image of the previous time step. Given a sufficiently large \\\\(T\\\\) and a well behaved schedule for adding noise at each time step, you end up with what is called an [isotropic Gaussian distribution](https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic) at \\\\(t=T\\\\) via a gradual process. ## In more mathematical form\nLet's write this down more formally, as ultimately we need a tractable loss function which our neural network needs to optimize."", ""--\ntitle: \""Finetune Stable Diffusion Models with DDPO via TRL\""\nthumbnail: /blog/assets/166\\_trl\\_ddpo/thumbnail.png\nauthors:\n- user: metric-space\nguest: true\n- user: sayakpaul\n- user: kashif\n- user: lvwerra\n---\n# Finetune Stable Diffusion Models with DDPO via TRL\n## Introduction\nDiffusion models (e.g., DALL-E 2, Stable Diffusion) are a class of generative models that are widely successful at generating images most notably of the photorealistic kind. However, the images generated by these models may not always be on par with human preference or human intention. Thus arises the alignment problem i.e. how does one go about making sure that the outputs of a model are aligned with human preferences like “quality” or that outputs are aligned with intent that is hard to express via prompts? This is where Reinforcement Learning comes into the picture. In the world of Large Language Models (LLMs), Reinforcement learning (RL) has proven to become a very effective tool for aligning said models to human preferences. It’s one of the main recipes behind the superior performance of systems like ChatGPT. More precisely, RL is the critical ingredient of Reinforcement Learning from Human Feedback (RLHF), which makes ChatGPT chat like human beings. In [Training Diffusion Models with Reinforcement Learning, Black](https://arxiv.org/abs/2305.13301) et al. show how to augment diffusion models to leverage RL to fine-tune them with respect to an objective function via a method named Denoising Diffusion Policy Optimization (DDPO). In this blog post, we discuss how DDPO came to be, a brief description of how it works, and how DDPO can be incorporated into an RLHF workflow to achieve model outputs more aligned with the human aesthetics. We then quickly switch gears to talk about how you can apply DDPO to your models with the newly integrated `DDPOTrainer` from the `trl` library and discuss our findings from running DDPO on Stable Diffusion. ## The Advantages of DDPO\nDDPO is not the only working answer to the question of how to attempt to fine-tune diffusion models with RL.""]","The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation in text-to-image generation tasks. It creates novel renditions of an input subject based on text prompts while overcoming limitations such as lengthy fine-tuning and difficulties in preserving subject fidelity. BLIP-Diffusion introduces a new multimodal encoder pre-trained to provide subject representation, allowing for efficient fine-tuning and flexible combinations with existing techniques for novel subject-driven generation and editing applications.",The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.
"How can a user claim authorship of a paper on the Hugging Face Hub?
","[""Paper Pages\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png)\n\n## Linking a Paper to a model, dataset or Space\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository's tags. Clicking on the arxiv tag will let you:\n\\* Visit the Paper page. \\* Filter for other models or datasets on the Hub that cite the same paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\n\n## Claiming authorship to a Paper\nThe Hub will attempt to automatically match paper to users based on their email. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png)\n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \""claim authorship\""."", ""| Partially handled by `huggingface\\_hub`. |"", ""\\* Is it related to something you would need for a project? We'd love to hear\nabout it! \\* Is it something you worked on and think could benefit the community? Awesome! Tell us what problem it solved for you. 2. Write a \\*full paragraph\\* describing the feature;\n3. Provide a \\*\\*code snippet\\*\\* that demonstrates its future use;\n4. In case this is related to a paper, please attach a link;\n5. Attach any additional information (drawings, screenshots, etc.) you think may help. If your issue is well written we're already 80% of the way there by the time you post it! ## Submitting a pull request (PR)\nBefore writing code, we strongly advise you to search through the existing PRs or\nissues to make sure that nobody is already working on the same thing. If you are\nunsure, it is always a good idea to open an issue to get some feedback. You will need basic `git` proficiency to be able to contribute to\n`huggingface\\_hub`. `git` is not the easiest tool to use but it has the greatest\nmanual. Type `git --help` in a shell and enjoy. If you prefer books, [Pro\nGit](https://git-scm.com/book/en/v2) is a very good reference. Follow these steps to start contributing:\n1. Fork the [repository](https://github.com/huggingface/huggingface\\_hub) by\nclicking on the 'Fork' button on the repository's page. This creates a copy of the code\nunder your GitHub user account. 2. Clone your fork to your local disk, and add the base repository as a remote.""]","A user can claim authorship of a paper on the Hugging Face Hub by clicking on their name in the corresponding Paper page and then selecting the ""claim authorship"" option. The Hub will also attempt to automatically match the paper to users based on their email.","By clicking their name on the corresponding Paper page and clicking ""claim authorship"", then confirming the request in paper settings for admin team validation."
"What is the purpose of the /healthcheck endpoint in the Datasets server API?
","[""Datasets server API - rows endpoint\n> /rows endpoint\n## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### API service\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split"", ""Datasets server admin machine\n> Admin endpoints\n## Configuration\nThe worker can be configured using environment variables. They are grouped by scope. ### Admin service\nSet environment variables to configure the application (`ADMIN\\_` prefix):\n- `ADMIN\\_HF\\_ORGANIZATION`: the huggingface organization from which the authenticated user must be part of in order to access the protected routes, eg. \""huggingface\"". If empty, the authentication is disabled. Defaults to None. - `ADMIN\\_CACHE\\_REPORTS\\_NUM\\_RESULTS`: the number of results in /cache-reports/... endpoints. Defaults to `100`. - `ADMIN\\_CACHE\\_REPORTS\\_WITH\\_CONTENT\\_NUM\\_RESULTS`: the number of results in /cache-reports-with-content/... endpoints. Defaults to `100`. - `ADMIN\\_HF\\_TIMEOUT\\_SECONDS`: the timeout in seconds for the requests to the Hugging Face Hub. Defaults to `0.2` (200 ms). - `ADMIN\\_HF\\_WHOAMI\\_PATH`: the path of the external whoami service, on the hub (see `HF\\_ENDPOINT`), eg. \""/api/whoami-v2\"". Defaults to `/api/whoami-v2`. - `ADMIN\\_MAX\\_AGE`: number of seconds to set in the `max-age` header on technical endpoints. Defaults to `10` (10 seconds). ### Uvicorn\nThe following environment variables are used to configure the Uvicorn server (`ADMIN\\_UVICORN\\_` prefix):\n- `ADMIN\\_UVICORN\\_HOSTNAME`: the hostname. Defaults to `\""localhost\""`. - `ADMIN\\_UVICORN\\_NUM\\_WORKERS`: the number of uvicorn workers. Defaults to `2`. - `ADMIN\\_UVICORN\\_PORT`: the port. Defaults to `8000`. ### Prometheus\n- `PROMETHEUS\\_MULTIPROC\\_DIR`: the directory where the uvicorn workers share their prometheus metrics. See https://github.com/prometheus/client\\_python#multiprocess-mode-eg-gunicorn. Defaults to empty, in which case every worker manages its own metrics, and the /metrics endpoint returns the metrics of a random worker. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nThe admin service provides endpoints:\n- `/healthcheck`\n- `/metrics`: give info about the cache and the queue\n- `/cache-reports{processing\\_step}`: give detailed reports on the content of the cache for a processing step\n- `/cache-reports-with-content{processing\\_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\n- `/force-refresh{processing\\_step}`: force refresh cache entries for the processing step. It's a POST endpoint. Pass the requested parameters, depending on the processing step's input type:\n- `dataset`: `?dataset={dataset}`\n- `config`: `?dataset={dataset}&config={config}`\n- `split`: `?dataset={dataset}&config={config}&split={split}`\n- `/recreate-dataset`: deletes all the cache entries related to a specific dataset, then run all the steps in order. It's a POST endpoint."", ""Datasets server SSE API\n> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache. ## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub\\_cache` field is null for deleted entries, or when the response is an error. The `num\\_rows` value is `0` if it could not be determined. ```typescript\n{\ndataset: string;\nhub\\_cache: null | {\npreview: boolean;\nviewer: boolean;\npartial: boolean;\nnum\\_rows: int;\n};\n}\n```""]",The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the app is running.,Ensure the app is running
"What is the default context window size for Local Attention in the LongT5 model?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# LongT5\n## Overview\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention. The abstract from the paper is the following:\n\\*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.\\*\nThis model was contributed by [stancld](https://huggingface.co/stancld). The original code can be found [here](https://github.com/google-research/longt5). ## Usage tips\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder \\*self-attention\\* layer with efficient either \\*local\\* attention or \\*transient-global\\* (\\*tglobal\\*) attention. - Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`]. - LongT5 model is designed to work efficiently and very well on long-range \\*sequence-to-sequence\\* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens. - For \\*Local Attention\\*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). \\*Local Attention\\* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l\\*r)`. - \\*Transient Global Attention\\* is an extension of the \\*Local Attention\\*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\nalso every global token like in the case of standard global attention (\\*transient\\* represents the fact the global tokens\nare constructed dynamically within each attention operation). As a consequence, \\*TGlobal\\* attention introduces\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.""]","The default context window size for Local Attention in the LongT5 model is `r=127`, which allows a given token to attend to 127 tokens to the left and right of it.",127 tokens
"What method is used to load a checkpoint for a task using `AutoPipeline`?
","[""The `AutoPipeline` class is designed to simplify the variety of pipelines in 🤗 Diffusers. It is a generic, \\*task-first\\* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name. Take a look at the [AutoPipeline](../api/pipelines/auto\\_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting. This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights. ## Choose an AutoPipeline for your task\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nprompt = \""peasant and dragon combat, wood cutting style, viking era, bevel with rune\""\nimage = pipeline(prompt, num\\_inference\\_steps=25).images[0]\nimage\n```\n\n![generated image of peasant fighting dragon in wood cutting style](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png)\n\nUnder the hood, [`AutoPipelineForText2Image`]:\n1. automatically detects a `\""stable-diffusion\""` class from the [`model\\_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model\\_index.json) file\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\""stable-diffusion\""` class name\nLikewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\""stable-diffusion\""` checkpoint from the `model\\_index.json` file and it'll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image:\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\npipeline = AutoPipelineForImage2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"",\ntorch\\_dtype=torch.float16,\nuse\\_safetensors=True,\n).to(\""cuda\"")\nprompt = \""a portrait of a dog wearing a pearl earring\""\nurl = \""https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665\\_Girl\\_with\\_a\\_Pearl\\_Earring.jpg/800px-1665\\_Girl\\_with\\_a\\_Pearl\\_Earring.jpg\""\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\""RGB\"")\nimage.thumbnail((768, 768))\nimage = pipeline(prompt, image, num\\_inference\\_steps=200, strength=0.75, guidance\\_scale=10.5).images[0]\nimage\n```\n\n![generated image of a vermeer portrait of a dog wearing a pearl earring](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png)\n\nAnd if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way:\n```py\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image\nimport torch\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""stabilityai/stable-diffusion-xl-base-1.0\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nimg\\_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\_examples/overture-creations-5sI6fQgYIuo.png\""\nmask\\_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\_examples/overture-creations-5sI6fQgYIuo\\_mask.png\""\ninit\\_image = load\\_image(img\\_url).convert(\""RGB\"")\nmask\\_image = load\\_image(mask\\_url).convert(\""RGB\"")\nprompt = \""A majestic tiger sitting on a bench\""\nimage = pipeline(prompt, image=init\\_image, mask\\_image=mask\\_image, num\\_inference\\_steps=50, strength=0.80).images[0]\nimage\n```\n\n![generated image of a tiger sitting on a bench](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png)\n\nIf you try to load an unsupported checkpoint, it'll throw an error:\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\npipeline = AutoPipelineForImage2Image.from\\_pretrained(\n\""openai/shap-e-img2img\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n)\n\""ValueError: AutoPipeline can't find a pipeline linked to ShapEImg2ImgPipeline for None\""\n```\n## Use multiple pipelines\nFor some workflows or if you're loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory."", ""You'll find the checkpoints saved in a `checkpoint-000` subfolder where the numbers at the end correspond to the training step. Saving checkpoints are useful for resuming training later. ```py\n# resume from latest checkpoint\ntrainer.train(resume\\_from\\_checkpoint=True)\n# resume from specific checkpoint saved in output directory\ntrainer.train(resume\\_from\\_checkpoint=\""your-model/checkpoint-1000\"")\n```\nYou can save your checkpoints (the optimizer state is not saved by default) to the Hub by setting `push\\_to\\_hub=True` in [`TrainingArguments`] to commit and push them. Other options for deciding how your checkpoints are saved are set up in the [`hub\\_strategy`](https://huggingface.co/docs/transformers/main\\_classes/trainer#transformers.TrainingArguments.hub\\_strategy) parameter:\n\\* `hub\\_strategy=\""checkpoint\""` pushes the latest checkpoint to a subfolder named \""last-checkpoint\"" from which you can resume training\n\\* `hug\\_strategy=\""all\\_checkpoints\""` pushes all checkpoints to the directory defined in `output\\_dir` (you'll see one checkpoint per folder in your model repository)\nWhen you resume training from a checkpoint, the [`Trainer`] tries to keep the Python, NumPy, and PyTorch RNG states the same as they were when the checkpoint was saved."", ""A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from\\_pretrained`] method:\n```py\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... new\\_model = AutoModel.from\\_pretrained(tmp\\_dir)\n```\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard. Behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n```py\n>>> import json\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... with open(os.path.join(tmp\\_dir, \""pytorch\\_model.bin.index.json\""), \""r\"") as f:\n... index = json.load(f)\n>>> print(index.keys())\ndict\\_keys(['metadata', 'weight\\_map'])\n```\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n```py\n>>> index[\""metadata\""]\n{'total\\_size': 433245184}\n```\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state\\_dict`) to the file it's stored in:\n```py\n>>> index[\""weight\\_map\""]\n{'embeddings.LayerNorm.bias': 'pytorch\\_model-00001-of-00003.bin',\n'embeddings.LayerNorm.weight': 'pytorch\\_model-00001-of-00003.bin',\n... ```\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from\\_pretrained`] (like you would do `model.load\\_state\\_dict()` for a full checkpoint) you should use [`~modeling\\_utils.load\\_sharded\\_checkpoint`]:\n```py\n>>> from transformers.modeling\\_utils import load\\_sharded\\_checkpoint\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... load\\_sharded\\_checkpoint(model, tmp\\_dir)\n```\n## Low memory loading\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library. Please read the following guide for more information: [Large model loading using Accelerate](./main\\_classes/model#large-model-loading)"", ""We'll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task). {/if}\nIn the code sample above we didn't use `BertConfig`, and instead loaded a pretrained model via the `bert-base-cased` identifier. This is a model checkpoint that was trained by the authors of BERT themselves; you can find more details about it in its [model card](https://huggingface.co/bert-base-cased). This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results. The weights have been downloaded and cached (so future calls to the `from\\_pretrained()` method won't re-download them) in the cache folder, which defaults to \\*~/.cache/huggingface/transformers\\*. You can customize your cache folder by setting the `HF\\_HOME` environment variable."", ""- [LyCORIS checkpoints](https://github.com/KohakuBlueleaf/LyCORIS) aren't fully supported. The [`~loaders.LoraLoaderMixin.load\\_lora\\_weights`] method loads LyCORIS checkpoints with LoRA and LoCon modules, but Hada and LoKR are not supported. Loading a checkpoint from TheLastBen is very similar. For example, to load the [TheLastBen/William\\_Eggleston\\_Style\\_SDXL](https://huggingface.co/TheLastBen/William\\_Eggleston\\_Style\\_SDXL) checkpoint:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\""stabilityai/stable-diffusion-xl-base-1.0\"", torch\\_dtype=torch.float16).to(\""cuda\"")\npipeline.load\\_lora\\_weights(\""TheLastBen/William\\_Eggleston\\_Style\\_SDXL\"", weight\\_name=\""wegg.safetensors\"")\n# use by william eggleston in the prompt to trigger the LoRA\nprompt = \""a house by william eggleston, sunrays, beautiful, sunlight, sunrays, beautiful\""\nimage = pipeline(prompt=prompt).images[0]\nimage\n```\n## IP-Adapter\n[IP-Adapter](https://ip-adapter.github.io/) is an effective and lightweight adapter that adds image prompting capabilities to a diffusion model."", ""If you want to use one of the official checkpoints for a task, explore the [gligen](https://huggingface.co/gligen) Hub organizations! [`StableDiffusionGLIGENPipeline`] was contributed by [Nikhil Gajendrakumar](https://github.com/nikhil-masterful) and [`StableDiffusionGLIGENTextImagePipeline`] was contributed by [Nguyễn Công Tú Anh](https://github.com/tuanh123789). ## StableDiffusionGLIGENPipeline\n[[autodoc]] StableDiffusionGLIGENPipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n- enable\\_model\\_cpu\\_offload\n- prepare\\_latents\n- enable\\_fuser\n## StableDiffusionGLIGENTextImagePipeline\n[[autodoc]] StableDiffusionGLIGENTextImagePipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n- enable\\_model\\_cpu\\_offload\n- prepare\\_latents\n- enable\\_fuser\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput"", ""This is a custom pipeline that merges upto 3 pretrained model checkpoints as long as they are in the HuggingFace model\\_index.json format. The checkpoint merging is currently memory intensive as it modifies the weights of a DiffusionPipeline object in place. Expect atleast 13GB RAM Usage on Kaggle GPU kernels and\non colab you might run out of the 12GB memory even while merging two checkpoints. Usage:-\n```python\nfrom diffusers import DiffusionPipeline\n#Return a CheckpointMergerPipeline class that allows you to merge checkpoints. #The checkpoint passed here is ignored. But still pass one of the checkpoints you plan to\n#merge for convenience\npipe = DiffusionPipeline.from\\_pretrained(\""CompVis/stable-diffusion-v1-4\"", custom\\_pipeline=\""checkpoint\\_merger\"")\n#There are multiple possible scenarios:\n#The pipeline with the merged checkpoints is returned in all the scenarios\n#Compatible checkpoints a.k.a matched model\\_index.json files. Ignores the meta attributes in model\\_index.json during comparison.( attrs with \\_ as prefix )\nmerged\\_pipe = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""CompVis/stable-diffusion-v1-2\""], interp = \""sigmoid\"", alpha = 0.4)\n#Incompatible checkpoints in model\\_index.json but merge might be possible. Use force = True to ignore model\\_index.json compatibility\nmerged\\_pipe\\_1 = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""hakurei/waifu-diffusion\""], force = True, interp = \""sigmoid\"", alpha = 0.4)\n#Three checkpoint merging. Only \""add\\_difference\"" method actually works on all three checkpoints. Using any other options will ignore the 3rd checkpoint. merged\\_pipe\\_2 = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""hakurei/waifu-diffusion\"",\""prompthero/openjourney\""], force = True, interp = \""add\\_difference\"", alpha = 0.4)\nprompt = \""An astronaut riding a horse on Mars\""\nimage = merged\\_pipe(prompt).images[0]\n```\nSome examples along with the merge details:\n1. \""CompVis/stable-diffusion-v1-4\"" + \""hakurei/waifu-diffusion\"" ; Sigmoid interpolation; alpha = 0.8\n![Stable plus Waifu Sigmoid 0.8](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/stability\\_v1\\_4\\_waifu\\_sig\\_0.8.png)\n2. \""hakurei/waifu-diffusion\"" + \""prompthero/openjourney\"" ; Inverse Sigmoid interpolation; alpha = 0.8\n![Stable plus Waifu Sigmoid 0.8](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/waifu\\_openjourney\\_inv\\_sig\\_0.8.png)\n3. \""CompVis/stable-diffusion-v1-4\"" + \""hakurei/waifu-diffusion\"" + \""prompthero/openjourney\""; Add Difference interpolation; alpha = 0.5\n![Stable plus Waifu plus openjourney add\\_diff 0.5](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/stable\\_waifu\\_openjourney\\_add\\_diff\\_0.5.png)\n### Stable Diffusion Comparisons\nThis Community Pipeline enables the comparison between the 4 checkpoints that exist for Stable Diffusion.""]","The `AutoPipeline` class automatically detects the correct pipeline class to use for a specific task and loads the corresponding checkpoint. You can use methods like `AutoPipelineForText2Image`, `AutoPipelineForImage2Image`, or `AutoPipelineForInpainting` to load the appropriate pipeline for text-to-image, image-to-image, or inpainting tasks, respectively. For example, you can load a checkpoint using:

```python
pipeline = AutoPipelineForText2Image.from_pretrained(""checkpoint_name"")
```

This method simplifies the process by allowing you to focus on the task without needing to know the specific pipeline class name.",from_pretrained()
"What is the purpose of Diffusers library?
","[""🧨 Diffusers Experimental\nWe are adding experimental code to support novel applications and usages of the Diffusers library."", ""This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\nis very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline. ## Tweakable, contributor-friendly over abstraction\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t\\_repeat\\_yourself). In short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers. Functions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable. \\*\\*However\\*\\*, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions. - Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions. - Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel. At Hugging Face, we call this design the \\*\\*single-file policy\\*\\* which means that almost all of the code of a certain class should be written in a single, self-contained file."", ""For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.\\*\\*\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!"", ""--\ntitle: Swift 🧨Diffusers - Fast Stable Diffusion for Mac\nthumbnail: /blog/assets/fast-mac-diffusers/thumbnail.png\nauthors:\n- user: pcuenq\n- user: reach-vb\n---\n# Swift 🧨Diffusers: Fast Stable Diffusion for Mac\nTransform your text into stunning images with ease using Diffusers for Mac, a native app powered by state-of-the-art diffusion models. It leverages a bouquet of SoTA Text-to-Image models contributed by the community to the Hugging Face Hub, and converted to Core ML for blazingly fast performance. Our latest version, 1.1, is now available on the [Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) with significant performance upgrades and user-friendly interface tweaks. It's a solid foundation for future feature updates. Plus, the app is fully open source with a permissive [license](https://github.com/huggingface/swift-coreml-diffusers/blob/main/LICENSE), so you can build on it too! Check out our GitHub repository at https://github.com/huggingface/swift-coreml-diffusers for more information. ![Screenshot showing Diffusers for Mac UI](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-mac-diffusers/UI.png)\n## What exactly is 🧨Diffusers for Mac anyway? The Diffusers app ([App Store](https://apps.apple.com/app/diffusers/id1666309574), [source code](https://github.com/huggingface/swift-coreml-diffusers)) is the Mac counterpart to our [🧨`diffusers` library](https://github.com/huggingface/diffusers). This library is written in Python with PyTorch, and uses a modular design to train and run diffusion models."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# 🧨 Diffusers’ Ethical Guidelines\n## Preamble\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training. Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users’ contributions, and usage of the Diffusers library. The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups."", ""## Credits\nThis library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We'd like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:\n- @CompVis' latent diffusion models library, available [here](https://github.com/CompVis/latent-diffusion)\n- @hojonathanho original DDPM implementation, available [here](https://github.com/hojonathanho/diffusion) as well as the extremely useful translation into PyTorch by @pesser, available [here](https://github.com/pesser/pytorch\\_diffusion)\n- @ermongroup's DDIM implementation, available [here](https://github.com/ermongroup/ddim)\n- @yang-song's Score-VE and Score-VP implementations, available [here](https://github.com/yang-song/score\\_sde\\_pytorch)\nWe also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available [here](https://github.com/heejkoo/Awesome-Diffusion-Models) as well as @crowsonkb and @rromb for useful discussions and insights. ## Citation\n```bibtex\n@misc{von-platen-etal-2022-diffusers,\nauthor = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},\ntitle = {Diffusers: State-of-the-art diffusion models},\nyear = {2022},\npublisher = {GitHub},\njournal = {GitHub repository},\nhowpublished = {\\url{https://github.com/huggingface/diffusers}}\n}\n```"", ""Research projects\nThis folder contains various research projects using 🧨 Diffusers. They are not really maintained by the core maintainers of this library and often require a specific version of Diffusers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice. If you need help with any of those, please open an issue where you directly ping the author(s), as indicated at the top of the README of each folder."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Using Diffusers with other modalities\nDiffusers is in the process of expanding to modalities other than images. Example type | Colab | Pipeline |\n:-------------------------:|:-------------------------:|:-------------------------:|\n[Molecule conformation](https://www.nature.com/subjects/molecular-conformation#:~:text=Definition,to%20changes%20in%20their%20environment.) generation | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/geodiff\\_molecule\\_conformation.ipynb) | ❌\nMore coming soon!""]","The purpose of the Diffusers library is to provide pre-trained diffusion models and serve as a modular toolbox for inference and training. It allows users to handle the interaction between different model components and facilitates easier debugging and customization. The library is designed to support various applications, including DreamBooth and Textual Inversion training, by separating single components of the diffusion pipeline.",To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.
"What method does the EulerAncestralDiscreteScheduler use for sampling?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# EulerAncestralDiscreteScheduler\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling\\_euler\\_ancestral\\_discrete.EulerAncestralDiscreteSchedulerOutput""]",The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling.,Ancestral sampling with Euler method steps.
"What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?
","[""Also, thanks to Niels Rogge, Younes Belkada, and Suraj Patil, among many others at Hugging Face, who laid out the foundations for increasing the use of multi-modal models from Transformers.\\*"", ""The negative samples are usually randomly sampled from the dataset itself. The MLM and ITM objectives are often combined during the pre-training of multi-modal models. For instance, VisualBERT proposes a BERT-like architecture that uses a pre-trained object detection model, [Faster-RCNN](https://arxiv.org/abs/1506.01497), to detect objects. This model uses a combination of the MLM and ITM objectives during pre-training to implicitly align elements of an input text and regions in an associated input image with self-attention. Another work, FLAVA, consists of an image encoder, a text encoder, and a multi-modal encoder to fuse and align the image and text representations for multi-modal reasoning, all of which are based on transformers. In order to achieve this, FLAVA uses a variety of pre-training objectives: MLM, ITM, as well as Masked-Image Modeling (MIM), and contrastive learning. ### 5) No Training\nFinally, various optimization strategies aim to bridge image and text representations using the pre-trained image and text models or adapt pre-trained multi-modal models to new downstream tasks without additional training. For example, [MaGiC](https://arxiv.org/abs/2205.02655) proposes iterative optimization through a pre-trained autoregressive language model to generate a caption for the input image. To do this, MaGiC computes a CLIP-based “Magic score” using CLIP embeddings of the generated tokens and the input image. ![ASIF](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/asif.png)  \n*Crafting a similarity search space using pre-trained, frozen unimodal image and text encoders ([image source](https://luca.moschella.dev/publication/norelli-asif-2022))*\n\n[ASIF](https://arxiv.org/abs/2210.01738) proposes a simple method to turn pre-trained uni-modal image and text models into a multi-modal model for image captioning using a relatively small multi-modal dataset without additional training. The key intuition behind ASIF is that captions of similar images are also similar to each other."", ""The abstract from the paper is the following:\n\\*While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a \""red bounding box\"" or \""pointed arrow\"". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available.\\*\nTips:\n- The architecture is similar than llava architecture except that the multi-modal projector takes a set of concatenated vision hidden states and has an additional layernorm layer on that module.""]",The large multimodal model that can solve image-text tasks and is based on Flamingo is called **LLaVA**.,IDEFICS
"What is the purpose of the `gradio.Blocks` API?
","[""Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our \\*\\*newly developed\\*\\* low-level API called `gradio.Blocks`. Now, what's the difference between `Interface` and `Blocks`? - ⚡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs. - 🧱 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \""building blocks\""). ### Why Blocks 🧱?[[why-blocks-]]\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\n- Group together related demos as multiple tabs in one web application\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a dropdown) or its visibility based on user input\nWe will explore all of these concepts below. ### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook. ```py\nimport gradio as gr\ndef flip\\_text(x):\nreturn x[::-1]\ndemo = gr.Blocks()\nwith demo:\ngr.Markdown(\n\""\""\""\n# Flip Text! Start typing below to see the output. \""\""\""\n)\ninput = gr.Textbox(placeholder=\""Flip this text\"")\noutput = gr.Textbox()\ninput.change(fn=flip\\_text, inputs=input, outputs=output)\ndemo.launch()\n```\nThis simple example above introduces 4 concepts that underlie Blocks:\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context."", ""Gradio Demo: json\\_component\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.JSON(value={\""Key 1\"": \""Value 1\"", \""Key 2\"": {\""Key 3\"": \""Value 2\"", \""Key 4\"": \""Value 3\""}, \""Key 5\"": [\""Item 1\"", \""Item 2\"", \""Item 3\""]})\ndemo.launch()\n```"", ""### 7. Select all the steps necessary for adding state to your Gradio interface\n### 8. Which of the following are components included in the Gradio library? ### 9. What does Gradio `Blocks` allow you to do? ### 10. You can share a public link to a `Blocks` demo and host a `Blocks` demo on Hugging Face spaces."", ""Gradio Blocks Party[[gradio-blocks-party]]\nAlong with the release of the Gradio chapter of the course, Hugging Face hosted a community event on building cool machine learning demos using the new Gradio Blocks feature. You can find all the demos that the community created under the [`Gradio-Blocks`](https://huggingface.co/Gradio-Blocks) organisation on the Hub. Here's a few examples from the winners:\n\\*\\*Natural language to SQL\\*\\*"", ""使用 Gradio 块像函数一样\nTags: TRANSLATION, HUB, SPACES\n\\*\\*先决条件\\*\\*: 本指南是在块介绍的基础上构建的。请确保[先阅读该指南](https://gradio.app/quickstart/#blocks-more-flexibility-and-control)。\n## 介绍\n你知道吗，除了作为一个全栈机器学习演示，Gradio 块应用其实也是一个普通的 Python 函数！？\n这意味着如果你有一个名为 `demo` 的 Gradio 块（或界面）应用，你可以像使用任何 Python 函数一样使用 `demo`。\n所以，像 `output = demo(\""Hello\"", \""friend\"")` 这样的操作会在输入为 \""Hello\"" 和 \""friend\"" 的情况下运行 `demo` 中定义的第一个事件，并将其存储在变量 `output` 中。\n如果以上内容让你打瞌睡 🥱，请忍耐一下！通过将应用程序像函数一样使用，你可以轻松地组合 Gradio 应用。\n接下来的部分将展示如何实现。\n## 将块视为函数\n假设我们有一个将英文文本翻译为德文文本的演示块。\n$code\\_english\\_translator\n我已经将它托管在 Hugging Face Spaces 上的 [gradio/english\\_translator](https://huggingface.co/spaces/gradio/english\\_translator)。\n你也可以在下面看到演示：\n$demo\\_english\\_translator\n现在，假设你有一个生成英文文本的应用程序，但你还想额外生成德文文本。\n你可以选择：\n1. 将我的英德翻译的源代码复制粘贴到你的应用程序中。\n2. 在你的应用程序中加载我的英德翻译，并将其当作普通的 Python 函数处理。\n选项 1 从技术上讲总是有效的，但它经常引入不必要的复杂性。\n选项 2 允许你借用所需的功能，而不会过于紧密地耦合我们的应用程序。\n你只需要在源文件中调用 `Blocks.load` 类方法即可。\n之后，你就可以像使用普通的 Python 函数一样使用我的翻译应用程序了！\n下面的代码片段和演示展示了如何使用 `Blocks.load`。\n请注意，变量 `english\\_translator` 是我的英德翻译应用程序，但它在 `generate\\_text` 中像普通函数一样使用。\n$code\\_generate\\_english\\_german\n$demo\\_generate\\_english\\_german\n## 如何控制使用应用程序中的哪个函数\n如果你正在加载的应用程序定义了多个函数，你可以使用 `fn\\_index` 和 `api\\_name` 参数指定要使用的函数。\n在英德演示的代码中，你会看到以下代码行：\ntranslate\\_btn.click(translate, inputs=english, outputs=german, api\\_name=\""translate-to-german\"")\n这个 `api\\_name` 在我们的应用程序中给这个函数一个唯一的名称。你可以使用这个名称告诉 Gradio 你想使用\n上游空间中的哪个函数：\nenglish\\_generator(text, api\\_name=\""translate-to-german\"")[0][\""generated\\_text\""]\n你也可以使用 `fn\\_index` 参数。\n假设我的应用程序还定义了一个英语到西班牙语的翻译函数。\n为了在我们的文本生成应用程序中使用它，我们将使用以下代码：\nenglish\\_generator(text, fn\\_index=1)[0][\""generated\\_text\""]\nGradio 空间中的函数是从零开始索引的，所以西班牙语翻译器将是我的空间中的第二个函数，\n因此你会使用索引 1。\n## 结语\n我们展示了将块应用视为普通 Python 函数的方法，这有助于在不同的应用程序之间组合功能。\n任何块应用程序都可以被视为一个函数，但一个强大的模式是在将其视为函数之前，\n在[自己的应用程序中加载](https://huggingface.co/spaces)托管在[Hugging Face Spaces](https://huggingface.co/spaces)上的应用程序。\n您也可以加载托管在[Hugging Face Model Hub](https://huggingface.co/models)上的模型——有关示例，请参阅[使用 Hugging Face 集成](/using\\_hugging\\_face\\_integrations)指南。\n### 开始构建！⚒️\n## Parting Remarks\n我们展示了如何将 Blocks 应用程序视为常规 Python 函数，以便在不同的应用程序之间组合功能。\n任何 Blocks 应用程序都可以被视为函数，但是一种有效的模式是在将其视为自己应用程序的函数之前，先`加载`托管在[Hugging Face Spaces](https://huggingface.co/spaces)上的应用程序。\n您还可以加载托管在[Hugging Face Model Hub](https://huggingface.co/models)上的模型-请参见[使用 Hugging Face 集成指南](/using\\_hugging\\_face\\_integrations)中的示例。\n### Happy building!"", ""Gradio Demo: blocks\\_scroll\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndemo = gr.Blocks()\nwith demo:\ninp = gr.Textbox(placeholder=\""Enter text.\"")\nscroll\\_btn = gr.Button(\""Scroll\"")\nno\\_scroll\\_btn = gr.Button(\""No Scroll\"")\nbig\\_block = gr.HTML(\""\""\""\""\""\"")\nout = gr.Textbox()\nscroll\\_btn.click(lambda x: x,\ninputs=inp,\noutputs=out,\nscroll\\_to\\_output=True)\nno\\_scroll\\_btn.click(lambda x: x,\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""How to Create a Custom Chatbot with Gradio Blocks\nTags: NLP, TEXT, CHAT\nRelated spaces: https://huggingface.co/spaces/gradio/chatbot\\_streaming, https://huggingface.co/spaces/project-baize/Baize-7B,\n## Introduction\n\\*\\*Important Note\\*\\*: if you are getting started, we recommend using the `gr.ChatInterface` to create chatbots -- its a high-level abstraction that makes it possible to create beautiful chatbot applications fast, often with a single line of code. [Read more about it here](/guides/creating-a-chatbot-fast). This tutorial will show how to make chatbot UIs from scratch with Gradio's low-level Blocks API. This will give you full control over your Chatbot UI. You'll start by first creating a a simple chatbot to display text, a second one to stream text responses, and finally a chatbot that can handle media files as well. The chatbot interface that we create will look something like this:\n$demo\\_chatbot\\_streaming\n\\*\\*Prerequisite\\*\\*: We'll be using the `gradio.Blocks` class to build our Chatbot demo. You can [read the Guide to Blocks first](https://gradio.app/quickstart/#blocks-more-flexibility-and-control) if you are not already familiar with it. Also please make sure you are using the \\*\\*latest version\\*\\* version of Gradio: `pip install --upgrade gradio`. ## A Simple Chatbot Demo\nLet's start with recreating the simple demo above. As you may have noticed, our bot simply randomly responds \""How are you?\"", \""I love you\"", or \""I'm very hungry\"" to any input. Here's the code to create this with Gradio:\n$code\\_chatbot\\_simple\nThere are three Gradio components here:\n- A `Chatbot`, whose value stores the entire history of the conversation, as a list of response pairs between the user and bot. - A `Textbox` where the user can type their message, and then hit enter/submit to trigger the chatbot response\n- A `ClearButton` button to clear the Textbox and entire Chatbot history\nWe have a single function, `respond()`, which takes in the entire history of the chatbot, appends a random message, waits 1 second, and then returns the updated chat history.""]","The `gradio.Blocks` API is a low-level API that allows you to have full control over the data flows and layout of your application. It provides more flexibility compared to the high-level `Interface` API, enabling you to build complex, multi-step applications. With `Blocks`, you can group related demos, change layouts, create multi-step interfaces, and modify component properties based on user input.","The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications."
"What is the purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents""?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# unCLIP\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in 🤗 Diffusers comes from kakaobrain's [karlo](https://github.com/kakaobrain/karlo). The abstract from the paper is following:\n\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\*\nYou can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines."", ""We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art\nperformances on a bunch of tasks including ImageNet-CN, Flicker30k- CN, and COCO-CN. Further, we obtain very close performances with\nCLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding.\\*\nThis model was contributed by [jongjyh](https://huggingface.co/jongjyh). ## Usage tips and example\nThe usage of AltCLIP is very similar to the CLIP. the difference between CLIP is the text encoder. Note that we use bidirectional attention instead of casual attention\nand we take the [CLS] token in XLM-R to represent text embedding. AltCLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image\nclassification. AltCLIP uses a ViT like transformer to get visual features and a bidirectional language model to get the text\nfeatures. Both the text and visual features are then projected to a latent space with identical dimension. The dot\nproduct between the projected image and text features is then used as a similar score. To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-overlapping patches,\nwhich are then linearly embedded."", ""Note, however, that the number of prompts we used to compute the CLIP scores is quite low. For a more practical evaluation, this number should be way higher, and the prompts should be diverse. By construction, there are some limitations in this score. The captions in the training dataset\nwere crawled from the web and extracted from `alt` and similar tags associated an image on the internet. They are not necessarily representative of what a human being would use to describe an image. Hence we\nhad to \""engineer\"" some prompts here. ### Image-conditioned text-to-image generation\nIn this case, we condition the generation pipeline with an input image as well as a text prompt. Let's take the [`StableDiffusionInstructPix2PixPipeline`], as an example. It takes an edit instruction as an input prompt and an input image to be edited. Here is one example:\n![edit-instruction](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation\\_diffusion\\_models/edit-instruction.png)\nOne strategy to evaluate such a model is to measure the consistency of the change between the two images (in [CLIP](https://huggingface.co/docs/transformers/model\\_doc/clip) space) with the change between the two image captions (as shown in [CLIP-Guided Domain Adaptation of Image Generators](https://arxiv.org/abs/2108.00946)). This is referred to as the \""\\*\\*CLIP directional similarity\\*\\*\"". - Caption 1 corresponds to the input image (image 1) that is to be edited. - Caption 2 corresponds to the edited image (image 2). It should reflect the edit instruction. Following is a pictorial overview:\n![edit-consistency](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation\\_diffusion\\_models/edit-consistency.png)\nWe have prepared a mini dataset to implement this metric. Let's first load the dataset. ```python\nfrom datasets import load\\_dataset\ndataset = load\\_dataset(\""sayakpaul/instructpix2pix-demo\"", split=\""train\"")\ndataset.features\n```\n```bash\n{'input': Value(dtype='string', id=None),\n'edit': Value(dtype='string', id=None),\n'output': Value(dtype='string', id=None),\n'image': Image(decode=True, id=None)}\n```\nHere we have:\n- `input` is a caption corresponding to the `image`."", ""Clark, Dan Garrette, Iulia Turc, John Wieting. 1. \\*\\*[Chinese-CLIP](https://huggingface.co/docs/transformers/model\\_doc/chinese\\_clip)\\*\\* (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou. 1. \\*\\*[CLAP](https://huggingface.co/docs/transformers/model\\_doc/clap)\\*\\* (from LAION-AI) released with the paper [Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation](https://arxiv.org/abs/2211.06687) by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov. 1. \\*\\*[CLIP](https://huggingface.co/docs/transformers/model\\_doc/clip)\\*\\* (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. 1. \\*\\*[CLIPSeg](https://huggingface.co/docs/transformers/model\\_doc/clipseg)\\*\\* (from University of Göttingen) released with the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke and Alexander Ecker."", ""It can be\ninstructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing\nfor the task, similarly to the zero-shot capabilities of GPT-2 and 3. The abstract from the paper is the following:\n\\*State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This\nrestricted form of supervision limits their generality and usability since additional labeled data is needed to specify\nany other visual concept. Learning directly from raw text about images is a promising alternative which leverages a\nmuch broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes\nwith which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400\nmillion (image, text) pairs collected from the internet. After pre-training, natural language is used to reference\nlearned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study\nthe performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks\nsuch as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need\nfor any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot\nwithout needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained\nmodel weights at this https URL.\\*\nThis model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/openai/CLIP). ## Usage tips and example\nCLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image\nclassification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text\nfeatures."", ""Generated image is a mix of the layout semantics of the given image and the content semantics of the prompt. E.g. the above script generates the following image:\n`phone.jpg`\n![206903102-34e79b9f-9ed2-4fac-bb38-82871343c655](https://user-images.githubusercontent.com/59410571/209578593-141467c7-d831-4792-8b9a-b17dc5e47816.jpg)\n`phone\\_bed\\_mix.jpg`\n![206903104-913a671d-ef53-4ae4-919d-64c3059c8f67](https://user-images.githubusercontent.com/59410571/209578602-70f323fa-05b7-4dd6-b055-e40683e37914.jpg)\nFor more example generations check out this [demo notebook](https://github.com/daspartho/MagicMix/blob/main/demo.ipynb). ### Stable UnCLIP\nUnCLIPPipeline(\""kakaobrain/karlo-v1-alpha\"") provide a prior model that can generate clip image embedding from text. StableDiffusionImageVariationPipeline(\""lambdalabs/sd-image-variations-diffusers\"") provide a decoder model than can generate images from clip image embedding. ```python\nimport torch\nfrom diffusers import DiffusionPipeline\ndevice = torch.device(\""cpu\"" if not torch.cuda.is\\_available() else \""cuda\"")\npipeline = DiffusionPipeline.from\\_pretrained(\n\""kakaobrain/karlo-v1-alpha\"",\ntorch\\_dtype=torch.float16,\ncustom\\_pipeline=\""stable\\_unclip\"",\ndecoder\\_pipe\\_kwargs=dict(\nimage\\_encoder=None,\n),\n)\npipeline.to(device)\nprompt = \""a shiba inu wearing a beret and black turtleneck\""\nrandom\\_generator = torch.Generator(device=device).manual\\_seed(1000)\noutput = pipeline(\nprompt=prompt,\nwidth=512,\nheight=512,\ngenerator=random\\_generator,\nprior\\_guidance\\_scale=4,\nprior\\_num\\_inference\\_steps=25,\ndecoder\\_guidance\\_scale=8,\ndecoder\\_num\\_inference\\_steps=50,\n)\nimage = output.images[0]\nimage.save(\""./shiba-inu.jpg\"")\n# debug\n# `pipeline.decoder\\_pipe` is a regular StableDiffusionImageVariationPipeline instance.""]","The purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents"" is to leverage robust representations of images learned by contrastive models like CLIP for image generation. The model consists of a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. This approach aims to improve image diversity with minimal loss in photorealism and caption similarity, allowing for variations of an image that preserve its semantics and style while varying non-essential details. Additionally, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion.",The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.
"What command is used to install the requirements for a research project using 🤗 Transformers?
","[""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Research projects\nThis folder contains various research projects using 🤗 Transformers. They are not maintained and require a specific\nversion of 🤗 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice. If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder."", ""\\* [TensorFlow 2.0](https://www.tensorflow.org/install/pip) installation instructions. \\* [Flax](https://flax.readthedocs.io/en/latest/) installation instructions. ## Install with pip\nYou should install 🤗 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies. Start by creating a virtual environment in your project directory:\n```bash\npython -m venv .env\n```\nActivate the virtual environment. On Linux and MacOs:\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n```bash\n.env/Scripts/activate\n```\nNow you're ready to install 🤗 Transformers with the following command:\n```bash\npip install transformers\n```\nFor CPU-support only, you can conveniently install 🤗 Transformers and a deep learning library in one line. For example, install 🤗 Transformers and PyTorch with:\n```bash\npip install 'transformers[torch]'\n```\n🤗 Transformers and TensorFlow 2.0:\n```bash\npip install 'transformers[tf-cpu]'\n```\nM1 / ARM Users\nYou will need to install the following before installing TensorFLow 2.0\n```\nbrew install cmake\nbrew install pkg-config\n```\n🤗 Transformers and Flax:\n```bash\npip install 'transformers[flax]'\n```\nFinally, check if 🤗 Transformers has been properly installed by running the following command. It will download a pretrained model:\n```bash\npython -c \""from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\""\n```\nThen print out the label and score:\n```bash\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```\n## Install from source\nInstall 🤗 Transformers from source with the following command:\n```bash\npip install git+https://github.com/huggingface/transformers\n```\nThis command installs the bleeding edge `main` version rather than the latest `stable` version."", ""## Additional resources\n\\* Transformers [library](https://github.com/huggingface/transformers). \\* Transformers [docs](https://huggingface.co/docs/transformers/index). \\* Share a model [guide](https://huggingface.co/docs/transformers/model\\_sharing)."", ""Set up a development environment, for instance by running the\nfollowing command:\n```bash\npython -m venv .env\nsource .env/bin/activate\npip install -e \"".[dev]\""\n```\nand return to the parent directory\n```bash\ncd .. ```\n4. We recommend adding the PyTorch version of \\*[camelcase name of model]\\* to\nTransformers. To install PyTorch, please follow the instructions [here](https://pytorch.org/get-started/locally/). \\*\\*Note:\\*\\* You don't need to have CUDA installed. Making the new model\nwork on CPU is sufficient. 5. To port \\*[camelcase name of model]\\*, you will also need access to its\noriginal repository:\n```bash\ngit clone [link to original repo].git\ncd [lowercase name of model]\npip install -e . ```\nNow you have set up a development environment to port \\*[camelcase name of model]\\*\nto 🤗 Transformers. ### Run a pretrained checkpoint using the original repository\n\\*\\*3. Set up debugging environment\\*\\*\nAt first, you will work on the original \\*[camelcase name of model]\\* repository. Often, the original implementation is very \""researchy\"". Meaning that\ndocumentation might be lacking and the code can be difficult to\nunderstand. But this should be exactly your motivation to reimplement\n\\*[camelcase name of model]\\*. At Hugging Face, one of our main goals is to \\*make\npeople stand on the shoulders of giants\\* which translates here very well\ninto taking a working model and rewriting it to make it as \\*\\*accessible,\nuser-friendly, and beautiful\\*\\* as possible. This is the number-one\nmotivation to re-implement models into 🤗 Transformers - trying to make\ncomplex new NLP technology accessible to \\*\\*everybody\\*\\*."", ""In all likelihood, you will need to adapt one of the example scripts, so we recommend forking and cloning the 🤗 Transformers repository as follows. Doing so will allow you to share your fork of the Transformers library with your team members so that the team effectively works on the same code base. It will also automatically install the newest versions of `flax`, `jax` and `optax`. 1. Fork the [repository](https://github.com/huggingface/transformers) by\nclicking on the 'Fork' button on the repository's page. This creates a copy of the code\nunder your GitHub user account. 2. Clone your fork to your local disk, and add the base repository as a remote:\n```bash\n$ git clone https://github.com//transformers.git\n$ cd transformers\n$ git remote add upstream https://github.com/huggingface/transformers.git\n```\n3. Create a new branch to hold your development changes. This is especially useful to share code changes with your team:\n```bash\n$ git checkout -b a-descriptive-name-for-my-project\n```\n4. Set up a flax environment by running the following command in a virtual environment:\n```bash\n$ pip install -e \"".[flax]\""\n```\n(If transformers was already installed in the virtual environment, remove\nit with `pip uninstall transformers` before reinstalling it in editable\nmode with the `-e` flag.)\nIf you have already cloned that repo, you might need to `git pull` to get the most recent changes in the `datasets`\nlibrary. Running this command will automatically install `flax`, `jax` and `optax`. Next, you should also install the 🤗 Datasets library. We strongly recommend installing the\nlibrary from source to profit from the most current additions during the community week. Simply run the following steps:\n```\n$ cd ~/\n$ git clone https://github.com/huggingface/datasets.git\n$ cd datasets\n$ pip install -e \"".[streaming]\""\n```\nIf you plan on contributing a specific dataset during\nthe community week, please fork the datasets repository and follow the instructions\n[here](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-create-a-pull-request)."", ""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\nSetup transformers following instructions in README.md, (I would fork first). ```bash\ngit clone git@github.com:huggingface/transformers.git\ncd transformers\npip install -e . pip install pandas GitPython wget\n```\nGet required metadata\n```\ncurl https://cdn-datasets.huggingface.co/language\\_codes/language-codes-3b2.csv > language-codes-3b2.csv\ncurl https://cdn-datasets.huggingface.co/language\\_codes/iso-639-3.csv > iso-639-3.csv\n```\nInstall Tatoeba-Challenge repo inside transformers\n```bash\ngit clone git@github.com:Helsinki-NLP/Tatoeba-Challenge.git\n```\nTo convert a few models, call the conversion script from command line:\n```bash\npython src/transformers/models/marian/convert\\_marian\\_tatoeba\\_to\\_pytorch.py --models heb-eng eng-heb --save\\_dir converted\n```\nTo convert lots of models you can pass your list of Tatoeba model names to `resolver.convert\\_models` in a python client or script. ```python\nfrom transformers.convert\\_marian\\_tatoeba\\_to\\_pytorch import TatoebaConverter\nresolver = TatoebaConverter(save\\_dir='converted')\nresolver.convert\\_models(['heb-eng', 'eng-heb'])\n```\n### Upload converted models\nSince version v3.5.0, the model sharing workflow is switched to git-based system . Refer to [model sharing doc](https://huggingface.co/transformers/main/model\\_sharing.html#model-sharing-and-uploading) for more details. To upload all converted models,\n1. Install [git-lfs](https://git-lfs.github.com/)."", ""We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\nelse to build their dream projects. In order to celebrate the 100,000 stars of transformers, we have decided to put the spotlight on the\ncommunity, and we have created the [awesome-transformers](./awesome-transformers.md) page which lists 100\nincredible projects built in the vicinity of transformers. If you own or use a project that you believe should be part of the list, please open a PR to add it! ## If you are looking for custom support from the Hugging Face team\n[![HuggingFace Expert Acceleration Program](https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png)](https://huggingface.co/support)  \n## Quick tour\nTo immediately use a model on a given input (text, image, audio, ...), we provide the `pipeline` API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts:\n```python\n>>> from transformers import pipeline\n# Allocate a pipeline for sentiment-analysis\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('We are very happy to introduce pipeline to the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```\nThe second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text.""]","The command used to install the requirements for a research project using 🤗 Transformers is:

```bash
pip install -r requirements.txt
```

You should run this command inside the folder of your choice.",pip install -r requirements.txt
"What task does the `roberta-large-mnli` checkpoint perform?
","[""Its primary objective is to reduce the model's trainable parameters. LoRA's operation involves\nlearning a low rank update matrix while keeping the pre-trained weights frozen. ![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral/lora.png)\n## Setup\nRoBERTa has a limitatiom of maximum sequence length of 512, so we set the `MAX\\_LEN=512` for all models to ensure a fair comparison. ```python\nMAX\\_LEN = 512\nroberta\\_checkpoint = \""roberta-large\""\nmistral\\_checkpoint = \""mistralai/Mistral-7B-v0.1\""\nllama\\_checkpoint = \""meta-llama/Llama-2-7b-hf\""\n```\n## Data preparation\n### Data loading\nWe will load the dataset from Hugging Face:\n```python\nfrom datasets import load\\_dataset\ndataset = load\\_dataset(\""mehdiiraqui/twitter\\_disaster\"")\n```\nNow, let's split the dataset into training and validation datasets. Then add the test set:\n```python\nfrom datasets import Dataset\n# Split the dataset into training and validation datasets\ndata = dataset['train'].train\\_test\\_split(train\\_size=0.8, seed=42)\n# Rename the default \""test\"" split to \""validation\""\ndata['val'] = data.pop(\""test\"")\n# Convert the test dataframe to HuggingFace dataset and add it into the first dataset\ndata['test'] = dataset['test']\n```\nHere's an overview of the dataset:\n```bash\nDatasetDict({\ntrain: Dataset({\nfeatures: ['id', 'keyword', 'location', 'text', 'target'],\nnum\\_rows: 6090\n})\nval: Dataset({\nfeatures: ['id', 'keyword', 'location', 'text', 'target'],\nnum\\_rows: 1523\n})\ntest: Dataset({\nfeatures: ['id', 'keyword', 'location', 'text', 'target'],\nnum\\_rows: 3263\n})\n})\n```\nLet's check the data distribution:\n```python\nimport pandas as pd\ndata['train'].to\\_pandas().info()\ndata['test'].to\\_pandas().info()\n```\n- Train dataset\n```\nRangeIndex: 7613 entries, 0 to 7612\nData columns (total 5 columns):\n# Column Non-Null Count Dtype\n--- ------ -------------- -----\n0 id 7613 non-null int64\n1 keyword 7552 non-null object\n2 location 5080 non-null object\n3 text 7613 non-null object\n4 target 7613 non-null int64\ndtypes: int64(2), object(3)\nmemory usage: 297.5+ KB\n```\n- Test dataset\n```\nRangeIndex: 3263 entries, 0 to 3262\nData columns (total 5 columns):\n# Column Non-Null Count Dtype\n--- ------ -------------- -----\n0 id 3263 non-null int64\n1 keyword 3237 non-null object\n2 location 2158 non-null object\n3 text 3263 non-null object\n4 target 3263 non-null int64\ndtypes: int64(2), object(3)\nmemory usage: 127.6+ KB\n```\n\\*\\*Target distribution in the train dataset\\*\\*\n```\ntarget\n0 4342\n1 3271\nName: count, dtype: int64\n```\nAs the classes are not balanced, we will compute the positive and negative weights and use them for loss calculation later:\n```python\npos\\_weights = len(data['train'].to\\_pandas()) / (2 \\* data['train'].to\\_pandas().target.value\\_counts()[1])\nneg\\_weights = len(data['train'].to\\_pandas()) / (2 \\* data['train'].to\\_pandas().target.value\\_counts()[0])\n```\nThe final weights are:\n```\nPOS\\_WEIGHT, NEG\\_WEIGHT = (1.1637114032405993, 0.8766697374481806)\n```\nThen, we compute the maximum length of the column text:\n```python\n# Number of Characters\nmax\\_char = data['train'].to\\_pandas()['text'].str.len().max()\n# Number of Words\nmax\\_words = data['train'].to\\_pandas()['text'].str.split().str.len().max()\n```\n```\nThe maximum number of characters is 152."", ""!-- DISABLE-FRONTMATTER-SECTIONS -->\n# End-of-chapter quiz[[end-of-chapter-quiz]]\nThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood. First, though, let's test what you learned in this chapter! ### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform? roberta-large-mnli page.\""\n},\n{\ntext: \""Text classification\"",\nexplain: \""More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) — a task also called *natural language inference*.\"",\ncorrect: true\n},\n{\ntext: \""Text generation\"",\nexplain: \""Look again on the [roberta-large-mnli page](\\\""https://huggingface.co/roberta-large-mnli\\\"").\""\n}\n]}\n/>\n### 2. What will the following code return? ```py\nfrom transformers import pipeline\nner = pipeline(\""ner\"", grouped\\_entities=True)\nner(\""My name is Sylvain and I work at Hugging Face in Brooklyn.\"")\n```\nsentiment-analysis pipeline.\""\n},\n{\ntext: \""It will return a generated text completing this sentence.\"",\nexplain: \""This is incorrect — it would be a `text-generation` pipeline.\"",\n},\n{\ntext: \""It will return the words representing persons, organizations or locations.\"",\nexplain: \""Furthermore, with `grouped_entities=True`, it will group together the words belonging to the same entity, like \\\""Hugging Face\\\"".\"",\ncorrect: true\n}\n]}\n/>\n### 3.""]","The `roberta-large-mnli` checkpoint performs text classification, specifically classifying if two sentences are logically linked across three labels: contradiction, neutral, and entailment. This task is also known as natural language inference.",Text classification
"What service is replacing the Paid tier of the Inference API at Hugging Face?
","[""Access 🤗 Inference Endpoints\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account. You can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method. There are two pricing plans:\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. - There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested! After you've added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying!"", ""The ability to use Hugging Face’s comprehensive hub of foundation models means that users can pick the models that best align with their business needs without having to invest in the resources required to train them. This integration is a significant step forward in making foundation models more accessible to enterprises around the world. \\_If you’re interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!\\_"", ""Benefit from higher rate limits, custom accelerated endpoints for the latest models, and early access to features. If you've built some exciting projects with the Inference API or are looking for a model not available in Inference for PROs, please [use this discussion](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/13). [Enterprise users](https://huggingface.co/enterprise) also benefit from PRO Inference API on top of other features, such as SSO. ## FAQ\n\n**Does this affect the free Inference API?**\n\nNo. We still expose thousands of models through free APIs that allow people to prototype and explore model capabilities quickly. **Does this affect Enterprise users?**\n\nUsers with an Enterprise subscription also benefit from accelerated inference API for curated models. **Can I use my own models with PRO Inference API?**\n\nThe free Inference API already supports a wide range of small and medium models from a variety of libraries (such as diffusers, transformers, and sentence transformers). If you have a custom model or custom inference logic, we recommend using [Inference Endpoints](https://ui.endpoints.huggingface.co/catalog)."", ""## Can I send large volumes of requests? Can I get accelerated APIs? If you are interested in accelerated inference, higher volumes of requests, or an SLA, please contact us at `api-enterprise at huggingface.co`. ## How can I see my usage? You can head to the [Inference API dashboard](https://api-inference.huggingface.co/dashboard/). Learn more about it in the [Inference API documentation](https://huggingface.co/docs/api-inference/usage). ## Is there programmatic access to the Inference API? Yes, the `huggingface\\_hub` library has a client wrapper documented [here](https://huggingface.co/docs/huggingface\\_hub/how-to-inference)."", ""--\ntitle: Introducing our new pricing\nthumbnail: /blog/assets/114\\_pricing-update/thumbnail.png\nauthors:\n- user: sbrandeis\n- user: pierric\n---\n# Introducing our new pricing\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently. First of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints). Along with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations). Your billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records. \\*\\*TL;DR\\*\\*: \\*\\*At HF we monetize by providing simple access to compute for AI\\*\\*, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub. [Read more](https://huggingface.co/docs/hub/billing) about our pricing and billing system. If you have any questions, feel free to reach out. We welcome your feedback 🔥""]",The service replacing the Paid tier of the Inference API at Hugging Face is the Inference Endpoints.,Inference Endpoints
"What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# SqueezeBERT\n## Overview\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers."", ""But, is it even necessary to compute the full intermediate matrix \\\\( \\mathbf{Y}\\_\\text{int} \\\\) ? Not really, because relevant is only the output matrix \\\\( \\mathbf{Y}\\_\\text{out} \\\\). To trade memory for speed, one can thus chunk the linear layers computation to only process one chunk at the time. Defining `config.chunk\\_size\\_feed\\_forward` as \\\\( c\\_{f} \\\\), chunked linear layers are defined as \\\\( \\mathbf{Y}\\_{\\text{out}} = \\left[\\mathbf{Y}\\_{\\text{out}, 1: c\\_{f}}, \\ldots, \\mathbf{Y}\\_{\\text{out}, (n - c\\_{f}): n}\\right] \\\\) with \\\\( \\mathbf{Y}\\_{\\text{out}, (c\\_{f} \\* i): (i \\* c\\_{f} + i)} = \\text{Linear}\\_{\\text{out}}(\\text{Linear}\\_{\\text{int}}(\\mathbf{\\overline{Z}}\\_{(c\\_{f} \\* i): (i \\* c\\_{f} + i)})) \\\\). In practice, it just means that the output is incrementally computed and concatenated to avoid having to store the whole intermediate tensor \\\\( \\mathbf{Y}\\_{\\text{int}} \\\\) in memory. Assuming \\\\( c\\_{f}=1 \\\\) for our example we can illustrate the incremental computation of the output for position \\\\( i=9 \\\\) as follows. ![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific\\_images/master/reformer\\_benchmark/chunked\\_feed\\_forward.png)\nBy processing the inputs in chunks of size 1, the only tensors that have to be stored in memory at the same time are \\\\( \\mathbf{Y}\\_\\text{out} \\\\) of a maximum size of \\\\( 16 \\times d\\_{h} \\\\), \\\\( \\mathbf{y}\\_{\\text{int}, i} \\\\) of size \\\\( d\\_{f} \\\\) and the input \\\\( \\mathbf{\\overline{Z}} \\\\) of size \\\\( 16 \\times d\\_{h} \\\\), with \\\\( d\\_{h} \\\\) being `config.hidden\\_size` \\\\(^{3}\\\\). Finally, it is important to remember that \\*chunked linear layers\\* yield a mathematically equivalent output to conventional linear layers and can therefore be applied to all transformer linear layers. Making use of `config.chunk\\_size\\_feed\\_forward` therefore allows a better trade-off between memory and speed in certain use cases. ---\n\\\\( {}^1 \\\\) For a simpler explanation, the layer norm layer which is normally applied to \\\\( \\mathbf{\\overline{Z}} \\\\) before being processed by the feed forward layers is omitted for now. \\\\( {}^2 \\\\) In `bert-base-uncased`, \\*e.g.\\* the intermediate dimension \\\\( d\\_{f} \\\\) is with 3072 four times larger than the output dimension \\\\( d\\_{h} \\\\). \\\\( {}^3 \\\\) As a reminder, the output `config.num\\_attention\\_heads` is assumed to be 1 for the sake of clarity and illustration in this notebook, so that the output of the self-attention layers can be assumed to be of size `config.hidden\\_size`.""]","SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers.",Grouped convolutions
"What type of license is the HuggingFace Team's software distributed under?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction""]","The HuggingFace Team's software is distributed under the Apache License, Version 2.0.","Apache License, Version 2.0"
"What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?
","[""It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\n\\*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\*\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# AdaLoRA\n[AdaLoRA](https://hf.co/papers/2303.10512) is a method for optimizing the number of trainable parameters to assign to weight matrices and layers, unlike LoRA, which distributes parameters evenly across all modules. More parameters are budgeted for important weight matrices and layers while less important ones receive fewer parameters. The abstract from the paper is:\n\\*Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings. Our code is publicly available at https://github.com/QingruZhang/AdaLoRA\\*. ## AdaLoraConfig\n[[autodoc]] tuners.adalora.config.AdaLoraConfig\n## AdaLoraModel\n[[autodoc]] tuners.adalora.model.AdaLoraModel"", ""Movement Pruning: Adaptive Sparsity by Fine-Tuning\nAuthor: @VictorSanh\n\\*Magnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of \\*movement pruning\\*, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3% of the model parameters:\\*\n| Fine-pruning+Distillation  \n(Teacher=BERT-base fine-tuned) | BERT base  \nfine-tuned | Remaining  \nWeights (%) | Magnitude Pruning | L0 Regularization | Movement Pruning | Soft Movement Pruning |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| SQuAD - Dev  \nEM/F1 | 80.4/88.1 | 10%  \n3% | 70.2/80.1  \n45.5/59.6 | 72.4/81.9  \n64.3/75.8 | 75.6/84.3  \n67.5/78.0 | \\*\\*76.6/84.9\\*\\*  \n\\*\\*72.7/82.3\\*\\* |\n| MNLI - Dev  \nacc/MM acc | 84.5/84.9 | 10%  \n3% | 78.3/79.3  \n69.4/70.6 | 78.7/79.7  \n76.0/76.2 | 80.1/80.4  \n76.5/77.4 | \\*\\*81.2/81.8\\*\\*  \n\\*\\*79.5/80.1\\*\\* |\n| QQP - Dev  \nacc/F1 | 91.4/88.4 | 10%  \n3% | 79.8/65.0  \n72.4/57.8 | 88.1/82.8  \n87.0/81.9 | 89.7/86.2  \n86.1/81.5 | \\*\\*90.2/86.8\\*\\*  \n\\*\\*89.1/85.5\\*\\* |\nThis page contains information on how to fine-prune pre-trained models such as `BERT` to obtain extremely sparse models with movement pruning."", ""## 모델 구조\n\\*\\*🤗 Transformers가 제공하는 [모든 모델 체크포인트](https://huggingface.co/models)\\*\\* 는 huggingface.co [모델 허브](https://huggingface.co)에 완벽히 연동되어 있습니다. [개인](https://huggingface.co/users)과 [기관](https://huggingface.co/organizations)이 모델 허브에 직접 업로드할 수 있습니다. 현재 사용 가능한 모델 체크포인트의 개수: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)\n🤗 Transformers는 다음 모델들을 제공합니다 (각 모델의 요약은 [여기](https://huggingface.co/docs/transformers/model\\_summary)서 확인하세요):\n1. \\*\\*[ALBERT](https://huggingface.co/docs/transformers/model\\_doc/albert)\\*\\* (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. 1. \\*\\*[ALIGN](https://huggingface.co/docs/transformers/model\\_doc/align)\\*\\* (Google Research 에서 제공)은 Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.의 [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918)논문과 함께 발표했습니다.""]","The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:

1. Splitting the embedding matrix into two smaller matrices.
2. Using repeating layers split among groups.",Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.
"What are the three main steps for fine-tuning a model with the 🤗 Datasets library?
","[""Introduction[[introduction]]\nIn [Chapter 3](/course/chapter3) you got your first taste of the 🤗 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics. But this is just scratching the surface of what 🤗 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\\* What do you do when your dataset is not on the Hub? \\* How can you slice and dice a dataset? (And what if you \\_really\\_ need to use Pandas?)\n\\* What do you do when your dataset is huge and will melt your laptop's RAM? \\* What the heck are \""memory mapping\"" and Apache Arrow? \\* How can you create your own dataset and push it to the Hub? The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!"", ""To verify that all libraries are correctly installed, you can run the following command in a Python shell. It verifies that both `transformers` and `datasets` have been correclty installed. ```python\nfrom transformers import AutoModelForCTC, AutoProcessor\nfrom datasets import load\\_dataset\ndummy\\_dataset = load\\_dataset(\""common\\_voice\"", \""ab\"", split=\""test\"")\nmodel = AutoModelForCTC.from\\_pretrained(\""hf-internal-testing/tiny-random-wav2vec2\"")\nmodel.to(\""cuda\"")\nprocessor = AutoProcessor.from\\_pretrained(\""hf-internal-testing/tiny-random-wav2vec2\"")\ninput\\_values = processor(dummy\\_dataset[0][\""audio\""][\""array\""], return\\_tensors=\""pt\"", sampling\\_rate=16\\_000).input\\_values\ninput\\_values = input\\_values.to(\""cuda\"")\nlogits = model(input\\_values).logits\nassert logits.shape[-1] == 32\n```\n## How to finetune an acoustic model\nIn this section, we show you how to fine-tune a pre-trained [XLS-R Model](https://huggingface.co/docs/transformers/model\\_doc/xls\\_r) on the [Common Voice 7 dataset](https://huggingface.co/datasets/mozilla-foundation/common\\_voice\\_7\\_0)."", ""!---\nCopyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Token classification with LayoutLMv3 (PyTorch version)\nThis directory contains a script, `run\\_funsd\\_cord.py`, that can be used to fine-tune (or evaluate) LayoutLMv3 on form understanding datasets, such as [FUNSD](https://guillaumejaume.github.io/FUNSD/) and [CORD](https://github.com/clovaai/cord). The script `run\\_funsd\\_cord.py` leverages the 🤗 Datasets library and the Trainer API. You can easily customize it to your needs. ## Fine-tuning on FUNSD\nFine-tuning LayoutLMv3 for token classification on [FUNSD](https://guillaumejaume.github.io/FUNSD/) can be done as follows:\n```bash\npython run\\_funsd\\_cord.py \\\n--model\\_name\\_or\\_path microsoft/layoutlmv3-base \\\n--dataset\\_name funsd \\\n--output\\_dir layoutlmv3-test \\\n--do\\_train \\\n--do\\_eval \\\n--max\\_steps 1000 \\\n--evaluation\\_strategy steps \\\n--eval\\_steps 100 \\\n--learning\\_rate 1e-5 \\\n--load\\_best\\_model\\_at\\_end \\\n--metric\\_for\\_best\\_model \""eval\\_f1\"" \\\n--push\\_to\\_hub \\\n--push\\_to\\_hub°model\\_id layoutlmv3-finetuned-funsd\n```\n👀 The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd.""]","The three main steps for fine-tuning a model with the 🤗 Datasets library are:
1. Load a dataset from the Hugging Face Hub.
2. Preprocess the data with `Dataset.map()`.
3. Load and compute metrics.",1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.
"What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?
","[""It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark. Figure 4. Latency (Batch=1, Physical Cores=2)\n\n---\n## Conclusion\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput. Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens. The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial)\n## Resources\n\\* [Hugging Face Infinity](https://huggingface.co/infinity)\n\\* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\n\\* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i)\n\\* [DistilBERT](https://huggingface.co/docs/transformers/model\\_doc/distilbert)\n\\* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\\* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\n\\* [🤗 Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)"", ""--\ntitle: \""How we sped up transformer inference 100x for 🤗 API customers\""\nthumbnail: /blog/assets/09\\_accelerated\\_inference/thumbnail.png\n---\n# How we sped up transformer inference 100x for 🤗 API customers\n🤗 Transformers has become the default library for data scientists all around the world to explore state of the art NLP models and build new NLP features. With over 5,000 pre-trained and fine-tuned models available, in over 250 languages, it is a rich playground, easily accessible whichever framework you are working in. While experimenting with models in 🤗 Transformers is easy, deploying these large models into production with maximum performance, and managing them into an architecture that scales with usage is a \\*\\*hard engineering challenge\\*\\* for any Machine Learning Engineer. This 100x performance gain and built-in scalability is why subscribers of our hosted [Accelerated Inference API](https://huggingface.co/pricing) chose to build their NLP features on top of it. To get to the \\*\\*last 10x of performance\\*\\* boost, the optimizations need to be low-level, specific to the model, and to the target hardware. This post shares some of our approaches squeezing every drop of compute juice for our customers. 🍋\n## Getting to the first 10x speedup\nThe first leg of the optimization journey is the most accessible, all about using the best combination of techniques offered by the [Hugging Face libraries](https://github.com/huggingface/), independent of the target hardware. We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main\\_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:\n-| Naive version | Optimized version |\n-|:---------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------:|\n-|![](/blog/assets/09\\_accelerated\\_inference/unoptimized\\_graph.png)|![](/blog/assets/09\\_accelerated\\_inference/optimized\\_graph.png)|\nTokenization is often a bottleneck for efficiency during inference. We use the most efficient methods from the [🤗 Tokenizers](https://github.com/huggingface/tokenizers/) library, leveraging the Rust implementation of the model tokenizer in combination with smart caching to get up to 10x speedup for the overall latency. Leveraging the latest features of the Hugging Face libraries, we achieve a reliable 10x speed up compared to an out-of-box deployment for a given model/hardware pair. As new releases of Transformers and Tokenizers typically ship every month, our API customers do not need to constantly adapt to new optimization opportunities, their models just keep running faster.""]",Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.,+800%
"What is the command to upload a spaCy pipeline to the Hugging Face Hub?
","[""Hub methods\nMethods for using the Hugging Face Hub:\n## Push to hub\n[[autodoc]] evaluate.push\\_to\\_hub"", ""\\* Launch [blog post](https://huggingface.co/blog/spacy)\n\\* spaCy v 3.1 [Announcement](https://explosion.ai/blog/spacy-v3-1#huggingface-hub)\n\\* spaCy [documentation](https://spacy.io/universe/project/spacy-huggingface-hub/)"", ""Defaults to `hub` in the current working directory. |\n| `--verbose`, `-V` | bool | Output additional info for debugging, e.g. the full generated hub metadata. |\nYou can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`. ```bash\nhuggingface-cli login\npython -m spacy package ./en\\_ner\\_fashion ./output --build wheel\ncd ./output/en\\_ner\\_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en\\_ner\\_fashion-0.0.0-py3-none-any.whl\n```\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card. The command will output two things:\n\\* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en\\_core\\_web\\_sm\n\\* And how to install the pipeline directly from the Hub! ### From a Python script\nYou can use the `push` function from Python. It returns a dictionary containing the `\""url\""` and \""`whl\\_url`\"" of the published model and the wheel file, which you can later install with `pip install`. ```py\nfrom spacy\\_huggingface\\_hub import push\nresult = push(\""./en\\_ner\\_fashion-0.0.0-py3-none-any.whl\"")\nprint(result[\""url\""])\n```\n## Additional resources\n\\* spacy-huggingface-hub [library](https://github.com/explosion/spacy-huggingface-hub)."", ""p align=\""center\"">\n\n![huggingface_hub library logo](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingface_hub.svg)\n\n*공식 Huggingface Hub 파이썬 클라이언트*\n\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc)](https://huggingface.co/docs/huggingface_hub/ko/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub/releases)\n[![PyPi version](https://img.shields.io/pypi/pyversions/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub)\n[![downloads](https://static.pepy.tech/badge/huggingface_hub/month)](https://pypi.org/project/huggingface-hub)\n[![Code coverage](https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL)](https://codecov.io/gh/huggingface/huggingface_hub)\n\n#### [English](https://github.com/huggingface/huggingface_hub/blob/main/README.md) | [Deutsch](https://github.com/huggingface/huggingface_hub/blob/main/README_de.md) | [हिंदी](https://github.com/huggingface/huggingface_hub/blob/main/README_hi.md) | **한국어**| [中文（简体）](https://github.com/huggingface/huggingface_hub/blob/main/README_cn.md)\n\n---\n\\*\\*기술 문서\\*\\*: <https://hf.co/docs/huggingface_hub>\n\\*\\*소스 코드\\*\\*: <https://github.com/huggingface/huggingface_hub>\n---\n## huggingface\\_hub 라이브러리 개요\n`huggingface\\_hub` 라이브러리는 [Hugging Face Hub](https://huggingface.co/)와 상호작용할 수 있게 해줍니다."", ""| Partially handled by `huggingface\\_hub`. |"", ""The Model Hub\n## What is the Model Hub? The Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing. Download pre-trained models with the [`huggingface\\_hub` client library](https://huggingface.co/docs/huggingface\\_hub/index), with 🤗 [`Transformers`](https://huggingface.co/docs/transformers/index) for fine-tuning and other usages or with any of the over [15 integrated libraries](./models-libraries). You can even leverage the [Inference API](./models-inference) to use models in production settings. You can refer to the following video for a guide on navigating the Model Hub:\nTo learn how to upload models to the Hub, you can refer to the [Repositories Getting Started Guide](./repositories-getting-started).""]","To upload a spaCy pipeline to the Hugging Face Hub, you can use the following command:

```bash
python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
```

Make sure to replace `en_ner_fashion-0.0.0-py3-none-any.whl` with the name of your packaged model file.",python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
"What is the time and memory complexity of the Nyströmformer's approximation of self-attention?
","[""--\ntitle: \""Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\""\nthumbnail: /blog/assets/86\\_nystromformer/thumbnail.png\nauthors:\n- user: asi\nguest: true\n---\n# Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\n## Introduction\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. The [Nyströmformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nyströmformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nyström method and how it can be adapted to approximate self-attention. ## Nyström method for matrix approximation\nAt the heart of Nyströmformer is the Nyström method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nyström method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:\n\nRepresenting P as a block matrix\n\nWe now have four submatrices: \\\\(A\\_P, B\\_P, F\\_P,\\\\) and \\\\(C\\_P\\\\), with sizes \\\\(m \\times m, m \\times (n - m), (n - m) \\times m\\\\) and\n\\\\((n - m) \\times (n - m)\\\\) respectively. The \\\\(m\\\\) sampled columns are contained in \\\\(A\\_P\\\\) and \\\\(F\\_P\\\\), whereas the \\\\(m\\\\) sampled rows are contained in \\\\(A\\_P\\\\) and \\\\(B\\_P\\\\). So, the entries of \\\\(A\\_P, B\\_P,\\\\) and \\\\(F\\_P\\\\) are known to us, and we will estimate \\\\(C\\_P\\\\). According to the Nyström method, \\\\(C\\_P\\\\) is given by:\n$$C\\_P = F\\_P A\\_P^+ B\\_P$$\nHere, \\\\(+\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). Thus, the Nyström approximation of \\\\(P, \\hat{P}\\\\) can be written as:\n\nNyström approximation of P\n\nAs shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices."", ""Only at longer sequence lengths of 4096, a slight decrease in memory usage can be seen. Let's see what happens to the memory peak usage if we increase the size of the feed forward layer by a factor of 4 and reduce the number of attention heads also by a factor of 4 so that the feed forward layer becomes the memory bottleneck. ```\nconfig\\_no\\_chunk = ReformerConfig.from\\_pretrained(\""google/reformer-enwik8\"", chunk\\_size\\_feed\\_forward=0, num\\_attention\\_{h}eads=2, feed\\_forward\\_size=16384) # no chuck\nconfig\\_chunk = ReformerConfig.from\\_pretrained(\""google/reformer-enwik8\"", chunk\\_size\\_feed\\_forward=1, num\\_attention\\_{h}eads=2, feed\\_forward\\_size=16384) # feed forward chunk\nbenchmark\\_args = PyTorchBenchmarkArguments(sequence\\_lengths=[1024, 2048, 4096], batch\\_sizes=[8], models=[\""Reformer-No-Chunk\"", \""Reformer-Chunk\""], no\\_speed=True, no\\_env\\_print=True)\nbenchmark = PyTorchBenchmark(configs=[config\\_no\\_chunk, config\\_chunk], args=benchmark\\_args)\nresult = benchmark.run()\n```\n1 / 2\n2 / 2\n==================== INFERENCE - MEMORY - RESULT ====================\n--------------------------------------------------------------------------------\nModel Name Batch Size Seq Length Memory in MB\n--------------------------------------------------------------------------------\nReformer-No-Chunk 8 1024 3743\nReformer-No-Chunk 8 2048 5539\nReformer-No-Chunk 8 4096 9087\nReformer-Chunk 8 1024 2973\nReformer-Chunk 8 2048 3999\nReformer-Chunk 8 4096 6011\n--------------------------------------------------------------------------------\nNow a clear decrease in peak memory usage can be seen for longer input sequences."", ""Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap\n[Transformer-XL (2019)](https://arxiv.org/abs/1901.02860) showed that caching previously computed layer activations in a memory can boost performance on language modeling tasks (such as \\*enwik8\\*). Instead of just attending the current \\\\(n\\\\) input tokens, the model can also attend to the past \\\\(n\\_m\\\\) tokens, with \\\\(n\\_m\\\\) being the memory size of the model. Transformer-XL has a memory complexity of \\\\(O(n^2+ n n\\_m)\\\\), which shows that memory cost can increase significantly for very large \\\\(n\\_m\\\\). Hence, Transformer-XL has to eventually discard past activations from the memory when the number of cached activations gets larger than \\\\(n\\_m\\\\). Compressive Transformer addresses this problem by adding an additional compressed memory to efficiently cache past activations that would have otherwise eventually been discarded. This way the model can learn better long-range sequence dependencies having access to significantly more past activations. ![Compressive Tranformer recurrence](/blog/assets/14_long_range_transformers/CompressiveTransformer.png)\n\nFigure taken from Compressive Transfomer\n\nA compression factor \\\\(c\\\\) (equal to 3 in the illustration) is chosen to decide the rate at which past activations are compressed."", ""For more detail see Appendix A of the [paper](https://arxiv.org/pdf/2001.04451.pdf). For our example, multi-round LSH self-attention can be illustrated as follows. ![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific\\_images/master/reformer\\_benchmark/lsh\\_attention\\_4.png)\nGreat. That's it. Now we know how LSH self-attention works in Reformer. Regarding the memory complexity, we now have two terms that compete which each other to be the memory bottleneck: the dot-product: \\\\( \\mathcal{O}(n\\_{h} \\* n\\_{c} \\* l\\_{c}^2) = \\mathcal{O}(n \\* n\\_{h} \\* l\\_{c}) \\\\) and the required memory for LSH bucketing: \\\\( \\mathcal{O}(n \\* n\\_{h} \\* \\frac{n\\_{b}}{2}) \\\\) with \\\\( l\\_{c} \\\\) being the chunk length. Because for large \\\\( n \\\\), the number of buckets \\\\( \\frac{n\\_{b}}{2} \\\\) grows much faster than the chunk length \\\\( l\\_{c} \\\\), the user can again factorize the number of buckets `config.num\\_buckets` as explained [here](https://huggingface.co/transformers/model\\_doc/reformer.html#lsh-self-attention).""]","The Nyströmformer approximates standard self-attention with a time and memory complexity of \\(O(n)\\), where \\(n\\) is the length of the input sequence. This is an improvement over the standard self-attention mechanism, which has a complexity of \\(O(n^2)\\).",O(n)
"What is the goal of the Named Entity Recognition task in token classification?
","[""These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. Two common types of token classification are:\n\\* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names. \\* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb). ```py\n>>> from transformers import pipeline\n>>> classifier = pipeline(task=\""ner\"")\n>>> preds = classifier(\""Hugging Face is a French company based in New York City.\"")\n>>> preds = [\n... {\n... \""entity\"": pred[\""entity\""],\n... \""score\"": round(pred[\""score\""], 4),\n... \""index\"": pred[\""index\""],\n... \""word\"": pred[\""word\""],\n... \""start\"": pred[\""start\""],\n... \""end\"": pred[\""end\""],\n..."", ""p align=\""center\""> ![FSNER LOGO](http://sayef.tech:8082/uploads/FSNER-LOGO-2.png) \n\nImplemented by  [sayef](https://huggingface.co/sayef) . ## Overview\nThe FSNER model was proposed in [Example-Based Named Entity Recognition](https://arxiv.org/abs/2008.10570) by Morteza Ziyadi, Yuting Sun, Abhishek Goswami, Jade Huang, Weizhu Chen. To identify entity spans in a new domain, it uses a train-free few-shot learning approach inspired by question-answering. ## Abstract\n----\n> We present a novel approach to named entity recognition (NER) in the presence of scarce data that we call example-based NER. Our train-free few-shot learning approach takes inspiration from question-answering to identify entity spans in a new and unseen domain. In comparison with the current state-of-the-art, the proposed method performs significantly better, especially when using a low number of support examples. ## Model Training Details\n-----\n| identifier | epochs | datasets |\n| ---------- |:----------:| :-----:|\n| [sayef/fsner-bert-base-uncased](https://huggingface.co/sayef/fsner-bert-base-uncased) | 10 | ontonotes5, conll2003, wnut2017, and fin (Alvarado et al.). |\n## Installation and Example Usage\n------\nYou can use the FSNER model in 3 ways:\n1. Install directly from PyPI: `pip install fsner` and import the model as shown in the code example below\nor\n2. Install from source: `python setup.py install` and import the model as shown in the code example below\nor\n3. Clone repo and change directory to `src` and import the model as shown in the code example below\n```python\nfrom fsner import FSNERModel, FSNERTokenizerUtils\nmodel = FSNERModel(\""sayef/fsner-bert-base-uncased\"")\ntokenizer = FSNERTokenizerUtils(\""sayef/fsner-bert-base-uncased\"")\n# size of query and supports must be the same."", ""The goal of this task is to classify each token in the input text into one of the following categories: person, organization, location, or none of them. - News Category Classification (NCC) on the Soham articles dataset from [IndicGLUE](https://aclanthology.org/2020.findings-emnlp.445/). The goal of this task is to predict the category to which belong the input text. We evaluated it during training on the NER task to check that everything was going well; as you can see on the following plot, this was indeed the case! Evaluation metrics of fine-tuned models on the NER task from different checkpoints of pre-trained models. At the end of training, we compared sahajBERT with three other pretrained language models: [XLM-R Large](https://arxiv.org/abs/1911.02116), [IndicBert](https://aclanthology.org/2020.findings-emnlp.445/), and [bnRoBERTa](https://huggingface.co/neuralspace-reverie/indic-transformers-bn-roberta). In the table below, you can see that our model has results comparable to the best Bengali language models available on HF Hub, even though our model has only ~18M trained parameters, while, for instance, XLM-R (a strong multilingual baseline), has ~559M parameters and was trained on several hundred V100 GPUs."", ""Named-Entity Recognition\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert\\_ner\\_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial\\_Analyst\\_AI\nTags: NER, TEXT, HIGHLIGHT\n## Introduction\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \""token\"") into different categories, such as names of people or names of locations, or different parts of speech. For example, given the sentence:\n> Does Chicago have any Pakistani restaurants? A named-entity recognition algorithm may identify:\n- \""Chicago\"" as a \\*\\*location\\*\\*\n- \""Pakistani\"" as an \\*\\*ethnicity\\*\\*\nand so on. Using `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team. Here is an example of a demo that you'll be able to build:\n$demo\\_ner\\_pipeline\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!"", ""# Token classification\nBased on the scripts [`run\\_ner.py`](https://github.com/huggingface/transformers/blob/main/examples/legacy/token-classification/run\\_ner.py). The following examples are covered in this section:\n\\* NER on the GermEval 2014 (German NER) dataset\n\\* Emerging and Rare Entities task: WNUT’17 (English NER) dataset\nDetails and results for the fine-tuning provided by @stefan-it. ### GermEval 2014 (German NER) dataset\n#### Data (Download and pre-processing steps)\nData can be obtained from the [GermEval 2014](https://sites.google.com/site/germeval2014ner/data) shared task page. Here are the commands for downloading and pre-processing train, dev and test datasets. The original data format has four (tab-separated) columns, in a pre-processing step only the two relevant columns (token and outer span NER annotation) are extracted:\n```bash\ncurl -L 'https://drive.google.com/uc?export=download&id=1Jjhbal535VVz2ap4v4r\\_rN1UEHTdLK5P' \\\n| grep -v \""^#\"" | cut -f 2,3 | tr '\\t' ' ' > train.txt.tmp\ncurl -L 'https://drive.google.com/uc?export=download&id=1ZfRcQThdtAR5PPRjIDtrVP7BtXSCUBbm' \\\n| grep -v \""^#\"" | cut -f 2,3 | tr '\\t' ' ' > dev.txt.tmp\ncurl -L 'https://drive.google.com/uc?export=download&id=1u9mb7kNJHWQCWyweMDRMuTFoOHOfeBTH' \\\n| grep -v \""^#\"" | cut -f 2,3 | tr '\\t' ' ' > test.txt.tmp\n```\nThe GermEval 2014 dataset contains some strange \""control character\"" tokens like `'\\x96', '\\u200e', '\\x95', '\\xad' or '\\x80'`. One problem with these tokens is, that `BertTokenizer` returns an empty token for them, resulting in misaligned `InputExample`s."", ""The model config contains the label mapping in its id2label field. Using it, we can map every token to its corresponding label. The label O correspond to \""no entity\"", which is why we didn't see it in our results in the first slide. On top of the label and the probability, those results included the start and end character in the sentence. We will need to use the offset mapping of the tokenizer to get those (look at the video linked below if you don't know about them already). Then, looping through each token that has a label distinct from O, we can build the list of results we got with our first pipeline. The last step is to group together tokens that correspond to the same entity.This is why we had two labels for each type of entity: I-PER and B-PER for instance. It allows us to know if a token is in the same entity as the previous one.() Note that there are two ways of labelling used for token classification, one (in pink here) uses the B-PER label at the beginning of each new entity, but the other (in blue) only uses it to separate two adjacent entities of the same type. In both cases, we can flag a new entity each time we see a new label appearing (either with the I or B prefix) then take all the following tokens labelled the same, with an I-flag. This, coupled with the offset mapping to get the start and end characters allows us to get the span of texts for each entity."", ""3. The input embeddings are passed through multiple encoder layers to output some final hidden states. To use the pretrained model for text classification, add a sequence classification head on top of the base BERT model. The sequence classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and target to find the most likely label. Ready to try your hand at text classification? Check out our complete [text classification guide](tasks/sequence\\_classification) to learn how to finetune DistilBERT and use it for inference! ### Token classification\nTo use BERT for token classification tasks like named entity recognition (NER), add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and each token to find the most likely label. Ready to try your hand at token classification? Check out our complete [token classification guide](tasks/token\\_classification) to learn how to finetune DistilBERT and use it for inference! ### Question answering\nTo use BERT for question answering, add a span classification head on top of the base BERT model.""]","The goal of the Named Entity Recognition (NER) task in token classification is to classify each token in the input text into predefined categories, such as person, organization, location, or none of them. For example, in a sentence, NER may identify ""Chicago"" as a location and ""Pakistani"" as an ethnicity.","The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization."
"What is the resolution of images used by the CLIPSeg model?
","[""For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm\\_source=hf&utm\\_medium=blog&utm\\_campaign=clipseg). Before we describe how to do that, let’s first take a look at how CLIPSeg works. ## CLIP: the magic model behind CLIPSeg\n[CLIP](https://huggingface.co/docs/transformers/main/en/model\\_doc/clip), which stands for \\*\\*C\\*\\*ontrastive \\*\\*L\\*\\*anguage–\\*\\*I\\*\\*mage \\*\\*P\\*\\*re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract \\*representation\\* of your input. This abstract representation, also called an \\*embedding\\*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together)."", ""cropping the image and darkening the background. ```python\nurl = \""https://i.imgur.com/mRSORqz.jpg\""\nalternative\\_prompt = Image.open(requests.get(url, stream=True).raw)\nalternative\\_prompt\n```\n\n```python\nencoded\\_alternative\\_prompt = processor(images=[alternative\\_prompt], return\\_tensors=\""pt\"")\n# predict\nwith torch.no\\_grad():\noutputs = model(\\*\\*encoded\\_image, conditional\\_pixel\\_values=encoded\\_alternative\\_prompt.pixel\\_values)\npreds = outputs.logits.unsqueeze(1)\npreds = torch.transpose(preds, 0, 1)\n```\n```python\n\\_, ax = plt.subplots(1, 2, figsize=(6, 4))\n[a.axis('off') for a in ax.flatten()]\nax[0].imshow(image)\nax[1].imshow(torch.sigmoid(preds[0]))\n```\n\nIn this case, the result is pretty much the same. This is probably\nbecause the coffee cup was already separated well from the background in\nthe original image. ## Using CLIPSeg to pre-label images on Segments.ai\nAs you can see, the results from CLIPSeg are a little fuzzy and very\nlow-res. If we want to obtain better results, you can fine-tune a\nstate-of-the-art segmentation model, as explained in [our previous\nblogpost](https://huggingface.co/blog/fine-tune-segformer). To finetune\nthe model, we\\'ll need labeled data. In this section, we\\'ll show you\nhow you can use CLIPSeg to create some rough segmentation masks and then\nrefine them on\n[Segments.ai](https://segments.ai/?utm\\_source=hf&utm\\_medium=blog&utm\\_campaign=clipseg),\na labeling platform with smart labeling tools for image segmentation.""]",The resolution of images used by the CLIPSeg model is 352 x 352 pixels.,352 x 352 pixels
"What can you use Gradio for?
","[""It uses Gradio's Audio and Label components."", ""Gradio Demo: image\\_segmentation\n### Simple image segmentation using gradio's AnnotatedImage component."", ""The Gradio Client is mostly used with apps hosted on [Hugging Face Spaces](https://hf.space), but your app can be hosted anywhere, such as your own server. \\*\\*Prequisites\\*\\*: To use the Gradio client, you do \\_not\\_ need to know the `gradio` library in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components. ## Installation\nThe lightweight `@gradio/client` package can be installed from the npm registry with a package manager of your choice and support node version 18 and above:\n```bash\nnpm i @gradio/client\n```\n## Connecting to a running Gradio App\nStart by connecting instantiating a `client` instance and connecting it to a Gradio app that is running on Hugging Face Spaces or generally anywhere on the web. ## Connecting to a Hugging Face Space\n```js\nimport { client } from \""@gradio/client\"";\nconst app = client(\""abidlabs/en2fr\""); // a Space that translates from English to French\n```\nYou can also connect to private Spaces by passing in your HF token with the `hf\\_token` property of the options parameter. You can get your HF token here: https://huggingface.co/settings/tokens\n```js\nimport { client } from \""@gradio/client\"";\nconst app = client(\""abidlabs/my-private-space\"", { hf\\_token=\""hf\\_...\"" })\n```\n## Duplicating a Space for private use\nWhile you can use any public Space as an API, you may get rate limited by Hugging Face if you make too many requests. For unlimited usage of a Space, simply duplicate the Space to create a private Space, and then use it to make as many requests as you'd like! The `@gradio/client` exports another function, `duplicate`, to make this process simple (you'll need to pass in your [Hugging Face token](https://huggingface.co/settings/tokens)). `duplicate` is almost identical to `client`, the only difference is under the hood:\n```js\nimport { client } from \""@gradio/client\"";\nconst response = await fetch(\n\""https://audio-samples.github.io/samples/mp3/blizzard\\_unconditional/sample-0.mp3\""\n);\nconst audio\\_file = await response.blob();\nconst app = await duplicate(\""abidlabs/whisper\"", { hf\\_token: \""hf\\_...\"" });\nconst transcription = app.predict(\""/predict\"", [audio\\_file]);\n```\nIf you have previously duplicated a Space, re-running `duplicate` will \\_not\\_ create a new Space."", ""Gradio Blocks Party[[gradio-blocks-party]]\nAlong with the release of the Gradio chapter of the course, Hugging Face hosted a community event on building cool machine learning demos using the new Gradio Blocks feature. You can find all the demos that the community created under the [`Gradio-Blocks`](https://huggingface.co/Gradio-Blocks) organisation on the Hub. Here's a few examples from the winners:\n\\*\\*Natural language to SQL\\*\\*"", ""Setting Up a Demo for Maximum Performance\nTags: CONCURRENCY, LATENCY, PERFORMANCE\nLet's say that your Gradio demo goes \\_viral\\_ on social media -- you have lots of users trying it out simultaneously, and you want to provide your users with the best possible experience or, in other words, minimize the amount of time that each user has to wait in the queue to see their prediction. How can you configure your Gradio demo to handle the most traffic? In this Guide, we dive into some of the parameters of Gradio's `.queue()` method as well as some other related parameters, and discuss how to set these parameters in a way that allows you to serve lots of users simultaneously with minimal latency. This is an advanced guide, so make sure you know the basics of Gradio already, such as [how to create and launch a Gradio Interface](https://gradio.app/guides/quickstart/). Most of the information in this Guide is relevant whether you are hosting your demo on [Hugging Face Spaces](https://hf.space) or on your own server. ## Overview of Gradio's Queueing System\nBy default, every Gradio demo includes a built-in queuing system that scales to thousands of requests.""]","Gradio can be used to create interactive machine learning demos and applications. It allows users to easily build and share interfaces for their models, enabling functionalities such as image segmentation, audio processing, and natural language processing. Gradio provides various input and output components, making it simple to connect models to user-friendly interfaces. Additionally, it supports hosting on platforms like Hugging Face Spaces, allowing for easy access and sharing of demos.","Create a demo for your machine learning model, share your machine learning model with others, and debug your model."
"What TensorFlow API function is used to load a saved tensor file?
","[""Tensorflow API\n[[autodoc]] safetensors.tensorflow.load\\_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save\\_file\n[[autodoc]] safetensors.tensorflow.save""]",The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`.,safetensors.tensorflow.load_file
"Where can you access the logs of your Endpoints in Hugging Face Endpoints?
","[""Access and view Metrics\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the “Analytics” tab of your Endpoints. ![metric dashboard](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png)\n## Access Metrics via API\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api\\_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.""]",You can access the logs of your Endpoints in Hugging Face Endpoints through the UI on the detailed overview in the “Logs” tab of your Endpoints.,"In the ""Logs"" tab of your Endpoint through the UI."
"What is the latest task added to Hugging Face AutoTrain for Computer Vision?
","[""--\ntitle: Image Classification with AutoTrain\nthumbnail: /blog/assets/105\\_autotrain-image-classification/thumbnail.png\nauthors:\n- user: nimaboscarino\n---\n# Image Classification with AutoTrain\nSo you’ve heard all about the cool things that are happening in the machine learning world, and you want to join in. There’s just one problem – you don’t know how to code! 😱 Or maybe you’re a seasoned software engineer who wants to add some ML to your side-project, but you don’t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. That’s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature we’ve just added, we’re making “no-code” machine learning better than ever. Best of all, you can create your first project for ✨ free! ✨\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with \\*\\*zero\\*\\* configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer 🤯 We’ve been expanding the number of tasks that we support, and we’re proud to announce that \\*\\*you can now use AutoTrain for Computer Vision\\*\\*!""]",The latest task added to Hugging Face AutoTrain for Computer Vision is image classification.,Image Classification
"What is the default repository type created by the `create_repo` function on Hugging Face Hub?
","[""Firstly, there are a few methods to manage repository creation, deletion, and others:\n```python no-format\nfrom huggingface\\_hub import (\n# User management\nlogin,\nlogout,\nwhoami,\n# Repository creation and management\ncreate\\_repo,\ndelete\\_repo,\nupdate\\_repo\\_visibility,\n# And some methods to retrieve/change information about the content\nlist\\_models,\nlist\\_datasets,\nlist\\_metrics,\nlist\\_repo\\_files,\nupload\\_file,\ndelete\\_file,\n)\n```\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them. The `create\\_repo` method can be used to create a new repository on the hub:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"")\n```\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"", organization=\""huggingface\"")\n```\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization."", ""!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# 创建和管理存储库\nHugging Face Hub是一组 Git 存储库。[Git](https://git-scm.com/)是软件开发中广泛使用的工具，可以在协作工作时轻松对项目进行版本控制。本指南将向您展示如何与 Hub 上的存储库进行交互，特别关注以下内容：\n- 创建和删除存储库\n- 管理分支和标签\n- 重命名您的存储库\n- 更新您的存储库可见性\n- 管理存储库的本地副本\n如果您习惯于使用类似于GitLab/GitHub/Bitbucket等平台，您可能首先想到使用 `git`命令行工具来克隆存储库（`git clone`）、提交更改（`git add` , ` git commit`）并推送它们（`git push`）。在使用 Hugging Face Hub 时，这是有效的。然而，软件工程和机器学习并不具有相同的要求和工作流程。模型存储库可能会维护大量模型权重文件以适应不同的框架和工具，因此克隆存储库会导致您维护大量占用空间的本地文件夹。因此，使用我们的自定义HTTP方法可能更有效。您可以阅读我们的[git与HTTP相比较](../concepts/git\\_vs\\_http)解释页面以获取更多详细信息\n如果你想在Hub上创建和管理一个仓库，你的计算机必须处于登录状态。如果尚未登录，请参考[此部分](../quick-start#login)。在本指南的其余部分，我们将假设你的计算机已登录\n## 仓库创建和删除\n第一步是了解如何创建和删除仓库。你只能管理你拥有的仓库（在你的用户名命名空间下）或者你具有写入权限的组织中的仓库\n### 创建一个仓库\n使用 [`create\\_repo`] 创建一个空仓库，并通过 `repo\\_id`参数为其命名 `repo\\_id`是你的命名空间，后面跟着仓库名称：`username\\_or\\_org/repo\\_name`\n运行以下代码，以创建仓库：\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-model\"")\n'https://huggingface.co/lysandre/test-model'\n```\n默认情况下，[`create\\_repo`] 会创建一个模型仓库。但是你可以使用 `repo\\_type`参数来指定其他仓库类型。例如，如果你想创建一个数据集仓库\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-dataset\"", repo\\_type=\""dataset\"")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```\n创建仓库时，你可以使用 `private`参数设置仓库的可见性\n请运行以下代码\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-private\"", private=True)\n```\n如果你想在以后更改仓库的可见性，你可以使用[`update\\_repo\\_visibility`] 函数\n### 删除一个仓库\n使用 [`delete\\_repo`] 删除一个仓库。确保你确实想要删除仓库，因为这是一个不可逆转的过程！做完上述过程后，指定你想要删除的仓库的 `repo\\_id`\n请运行以下代码：\n```py\n>>> delete\\_repo(repo\\_id=\""lysandre/my-corrupted-dataset\"", repo\\_type=\""dataset\"")\n```\n### 克隆一个仓库（仅适用于 Spaces）\n在某些情况下，你可能想要复制别人的仓库并根据自己的用例进行调整。对于 Spaces，你可以使用 [`duplicate\\_space`] 方法来实现。它将复制整个仓库。\n你仍然需要配置自己的设置（硬件和密钥）。查看我们的[管理你的Space指南](./manage-spaces)以获取更多详细信息。\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import duplicate\\_space\n>>> duplicate\\_space(\""multimodalart/dreambooth-training\"", private=False)\nRepoUrl('https://huggingface.co/spaces/nateraw/dreambooth-training',...)\n```\n## 上传和下载文件\n既然您已经创建了您的存储库，您现在也可以推送更改至其中并从中下载文件\n这两个主题有它们自己的指南。请[上传指南](./upload) 和[下载指南](./download)来学习如何使用您的存储库。\n## 分支和标签\nGit存储库通常使用分支来存储同一存储库的不同版本。标签也可以用于标记存储库的特定状态，例如，在发布版本这个情况下。更一般地说，分支和标签被称为[git引用](https://git-scm.com/book/en/v2/Git-Internals-Git-References). ### 创建分支和标签\n你可以使用[`create\\_branch`]和[`create\\_tag`]来创建新的分支和标签:\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import create\\_branch, create\\_tag\n# Create a branch on a Space repo from `main` branch\n>>> create\\_branch(\""Matthijs/speecht5-tts-demo\"", repo\\_type=\""space\"", branch=\""handle-dog-speaker\"")\n# Create a tag on a Dataset repo from `v0.1-release` branch\n>>> create\\_branch(\""bigcode/the-stack\"", repo\\_type=\""dataset\"", revision=\""v0.1-release\"", tag=\""v0.1.1\"", tag\\_message=\""Bump release version.\"")\n```\n同时,你可以以相同的方式使用 [`delete\\_branch`] 和 [`delete\\_tag`] 函数来删除分支或标签\n### 列出所有的分支和标签\n你还可以使用 [`list\\_repo\\_refs`] 列出存储库中的现有 Git 引用\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import list\\_repo\\_refs\n>>> api.list\\_repo\\_refs(\""bigcode/the-stack\"", repo\\_type=\""dataset\"")\nGitRefs(\nbranches=[\nGitRefInfo(name='main', ref='refs/heads/main', target\\_commit='18edc1591d9ce72aa82f56c4431b3c969b210ae3'),\nGitRefInfo(name='v1.1.a1', ref='refs/heads/v1.1.a1', target\\_commit='f9826b862d1567f3822d3d25649b0d6d22ace714')\n],\nconverts=[],\ntags=[\nGitRefInfo(name='v1.0', ref='refs/tags/v1.0', target\\_commit='c37a8cd1e382064d8aced5e05543c5f7753834da')\n]\n)\n```\n## 修改存储库设置\n存储库具有一些可配置的设置。大多数情况下，您通常会在浏览器中的存储库设置页面上手动配置这些设置。要配置存储库，您必须具有对其的写访问权限（拥有它或属于组织）。在本节中，我们将看到您还可以使用 `huggingface\\_hub` 在编程方式上配置的设置。\n一些设置是特定于 Spaces（硬件、环境变量等）的。要配置这些设置，请参考我们的[管理Spaces](../guides/manage-spaces)指南。\n### 更新可见性\n一个存储库可以是公共的或私有的。私有存储库仅对您或存储库所在组织的成员可见。\n请运行以下代码将存储库更改为私有：\n```py\n>>> from huggingface\\_hub import update\\_repo\\_visibility\n>>> update\\_repo\\_visibility(repo\\_id=repo\\_id, private=True)\n```\n### 重命名您的存储库\n您可以使用 [`move\\_repo`] 在 Hub 上重命名您的存储库。使用这种方法，您还可以将存储库从一个用户移动到一个组织。在这样做时，有一些[限制](https://hf.co/docs/hub/repositories-settings#renaming-or-transferring-a-repo)需要注意。例如，您不能将存储库转移到另一个用户。\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import move\\_repo\n>>> move\\_repo(from\\_id=\""Wauplin/cool-model\"", to\\_id=\""huggingface/cool-model\"")\n```\n## 管理存储库的本地副本\n上述所有操作都可以通过HTTP请求完成。然而，在某些情况下，您可能希望在本地拥有存储库的副本，并使用您熟悉的Git命令与之交互。\n[`Repository`] 类允许您使用类似于Git命令的函数与Hub上的文件和存储库进行交互。它是对Git和Git-LFS方法的包装，以使用您已经了解和喜爱的Git命令。在开始之前，请确保已安装Git-LFS（请参阅[此处](https://git-lfs.github.com/)获取安装说明）。\n### 使用本地存储库\n使用本地存储库路径实例化一个 [`Repository`] 对象：\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(local\\_dir=\""//\"")\n```\n### 克隆\n`clone\\_from`参数将一个存储库从Hugging Face存储库ID克隆到由 `local\\_dir`参数指定的本地目录：\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(local\\_dir=\""w2v2\"", clone\\_from=\""facebook/wav2vec2-large-960h-lv60\"")\n```\n`clone\\_from`还可以使用URL克隆存储库：\n请运行以下代码：\n```py\n>>> repo = Repository(local\\_dir=\""huggingface-hub\"", clone\\_from=\""https://huggingface.co/facebook/wav2vec2-large-960h-lv60\"")\n```\n你可以将`clone\\_from`参数与[`create\\_repo`]结合使用，以创建并克隆一个存储库：\n请运行以下代码：\n```py\n>>> repo\\_url = create\\_repo(repo\\_id=\""repo\\_name\"")\n>>> repo = Repository(local\\_dir=\""repo\\_local\\_path\"", clone\\_from=repo\\_url)\n```\n当你克隆一个存储库时，通过在克隆时指定`git\\_user`和`git\\_email`参数，你还可以为克隆的存储库配置Git用户名和电子邮件。当用户提交到该存储库时，Git将知道提交的作者是谁。\n请运行以下代码：\n```py\n>>> repo = Repository(\n..."", ""It's a model repo by default. Parameters:\n- `type`: Type of repo (dataset or space; model by default). - `name`: Name of repo. - `organization`: Name of organization (optional). Payload:\n```js\npayload = {\n\""type\"": \""model\"",\n\""name\"": \""name\"",\n\""organization\"": \""organization\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.delete\\_repo()`. ### PUT /api/repos/{repo\\_type}/{repo\\_id}/settings\nUpdate repo visibility. Payload:\n```js\npayload = {\n\""private\"": \""private\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.update\\_repo\\_visibility()`. ### POST /api/repos/move\nMove a repository (rename within the same namespace or transfer from user to organization). Parameters:\n- `fromRepo`: repo to rename. - `toRepo`: new name of the repo. - `type`: Type of repo (dataset or space; model by default). Payload:\n```js\npayload = {\n\""fromRepo\"" : \""namespace/repo\\_name\"",\n\""toRepo\"" : \""namespace2/repo\\_name2\"",\n\""type\"": \""model\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.move\\_repo()`. ## User API\nThe following endpoint gets information about a user. ### GET /api/whoami-v2\nGet username and organizations the user belongs to. Payload:\n```js\nheaders = { \""authorization\"" : \""Bearer $token\"" }\n```\nThis is equivalent to `huggingface\\_hub.whoami()`.""]",The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository.,model
"How many splits does the ""duorc"" dataset have?
","[""List splits and configurations\nDatasets typically have splits and may also have configurations. A \\_split\\_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A \\_configuration\\_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you're interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load\\_hub)! This guide shows you how to use Datasets Server's `/splits` endpoint to retrieve a dataset's splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\nThe `/splits` endpoint accepts the dataset name as its query parameter:\n\n```python\nimport requests\nheaders = {\""Authorization\"": f\""Bearer {API\\_TOKEN}\""}\nAPI\\_URL = \""https://datasets-server.huggingface.co/splits?dataset=duorc\""\ndef query():\nresponse = requests.get(API\\_URL, headers=headers)\nreturn response.json()\ndata = query()\n```\n\n```js\nimport fetch from \""node-fetch\"";\nasync function query(data) {\nconst response = await fetch(\n\""https://datasets-server.huggingface.co/splits?dataset=duorc\"",\n{\nheaders: { Authorization: `Bearer ${API\\_TOKEN}` },\nmethod: \""GET\""\n}\n);\nconst result = await response.json();\nreturn result;\n}\nquery().then((response) => {\nconsole.log(JSON.stringify(response));\n});\n```\n\n```curl\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\n-X GET \\\n-H \""Authorization: Bearer ${API\\_TOKEN}\""\n```\nThe endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n```json\n{\n\""splits\"": [\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""test\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""test\"" }\n],\n\""pending\"": [],\n\""failed\"": []\n}\n```"", ""### Download and extract the data. It consists of two compressed folders: one with images, and the other—with associated image captions. Note that the compressed images folder is 13GB in size. ```bash\nwget http://images.cocodataset.org/annotations/annotations\\_trainval2014.zip\nwget http://images.cocodataset.org/zips/train2014.zip\nunzip annotations\\_trainval2014.zip\nunzip train2014.zip\nmkdir coco\\_dataset\nmv train2014 coco\\_dataset/\nmv annotations coco\\_dataset/\n```\n### Prepare dataset files and split the dataset. ```python\nimport json\nimport collections\nimages\\_dir = \""coco\\_dataset/train2014\""\nannotation\\_file = \""coco\\_dataset/annotations/captions\\_train2014.json\""\nwith open(annotation\\_file, \""r\"") as f:\nannotations = json.load(f)[\""annotations\""]\nimage\\_path\\_to\\_caption = collections.defaultdict(list)\nfor element in annotations:\ncaption = f\""{element['caption'].lower().rstrip('.')}\""\nimage\\_path = images\\_dir + \""/COCO\\_train2014\\_\"" + \""%012d.jpg\"" % (element[\""image\\_id\""])\nimage\\_path\\_to\\_caption[image\\_path].append(caption)\nlines = []\nfor image\\_path, captions in image\\_path\\_to\\_caption.items():\nlines.append(json.dumps({\""image\\_path\"": image\\_path, \""captions\"": captions}))\ntrain\\_lines = lines[:-8000]\nvalid\\_line = lines[-8000:]\nwith open(\""coco\\_dataset/train\\_dataset.json\"", \""w\"") as f:\nf.write(\""\\n\"".join(train\\_lines))\nwith open(\""coco\\_dataset/valid\\_dataset.json\"", \""w\"") as f:\nf.write(\""\\n\"".join(valid\\_line))\n```\n> Note: The data loading and processing part of this script can still be improved for maximum performance."", ""### Distributed\nTo split your dataset across your training nodes, you can use [`datasets.distributed.split\\_dataset\\_by\\_node`]:\n```python\nimport os\nfrom datasets.distributed import split\\_dataset\\_by\\_node\nds = split\\_dataset\\_by\\_node(ds, rank=int(os.environ[\""RANK\""]), world\\_size=int(os.environ[\""WORLD\\_SIZE\""]))\n```\nThis works for both map-style datasets and iterable datasets. The dataset is split for the node at rank `rank` in a pool of nodes of size `world\\_size`. For map-style datasets:\nEach node is assigned a chunk of data, e.g. rank 0 is given the first chunk of the dataset. For iterable datasets:\nIf the dataset has a number of shards that is a factor of `world\\_size` (i.e. if `dataset.n\\_shards % world\\_size == 0`),\nthen the shards are evenly assigned across the nodes, which is the most optimized. Otherwise, each node keeps 1 example out of `world\\_size`, skipping the other examples. This can also be combined with a `torch.utils.data.DataLoader` if you want each node to use multiple workers to load the data."", ""- The slice of `rows` of a dataset and the content contained in each column of a specific row. For example, here are the `features` and the slice of `rows` of the `duorc`/`SelfRC` train split from 150 to 151:\n```json\n// https://datasets-server.huggingface.co/rows?dataset=duorc&config=SelfRC&split=train&offset=150&length=2\n{\n\""features\"": [\n{\n\""feature\\_idx\"": 0,\n\""name\"": \""plot\\_id\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 1,\n\""name\"": \""plot\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 2,\n\""name\"": \""title\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 3,\n\""name\"": \""question\\_id\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 4,\n\""name\"": \""question\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 5,\n\""name\"": \""answers\"",\n\""type\"": {\n\""feature\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" },\n\""\\_type\"": \""Sequence\""\n}\n},\n{\n\""feature\\_idx\"": 6,\n\""name\"": \""no\\_answer\"",\n\""type\"": { \""dtype\"": \""bool\"", \""\\_type\"": \""Value\"" }\n}\n],\n\""rows\"": [\n{\n\""row\\_idx\"": 150,\n\""row\"": {\n\""plot\\_id\"": \""/m/03wj\\_q\"",\n\""plot\"": \""The film is centered on Mortal Kombat, a fighting tournament between the representatives of the realms of Earth and Outworld conceived by the Elder Gods amid looming invasion of the Earth by Outworld.""]","The ""duorc"" dataset has six splits.",Six
"What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Fully Sharded Data Parallel\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or \\*rank\\*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class. Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer. ```bash\npip install accelerate\n```\n## FSDP configuration\nTo start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package\\_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`. ```bash\naccelerate config\n```\nWhen you run `accelerate config`, you'll be prompted with a series of options to configure your training environment."", ""This would result in the CPU RAM getting out of memory leading to processes being terminated. 2. Saving entire intermediate checkpoints using `FULL\\_STATE\\_DICT` with CPU offloading on rank 0 takes a lot of time and often results in NCCL Timeout errors due to indefinite hanging during broadcasting. However, at the end of training, we want the whole model state dict instead of the sharded state dict which is only compatible with FSDP. 3. We need to improve the speed and reduce the VRAM usage to train faster and save compute costs. Let’s look at how to solve the above challenges and fine-tune a 70B model! Before we get started, here's all the required resources to reproduce our results:\n1. Codebase:\nhttps://github.com/pacman100/DHS-LLM-Workshop/tree/main/chat\\_assistant/training with flash-attn V2 monkey patch\n2. FSDP config: https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat\\_assistant/training/configs/fsdp\\_config.yaml\n3. SLURM script `launch.slurm`: https://gist.github.com/pacman100/1cb1f17b2f1b3139a63b764263e70b25\n4. Model: `meta-llama/Llama-2-70b-chat-hf`\n5. Dataset: [smangrul/code-chat-assistant-v1](https://huggingface.co/datasets/smangrul/code-chat-assistant-v1) (mix of LIMA+GUANACO with proper formatting in a ready-to-train format)\n### Pre-requisites\nFirst follow these steps to install Flash Attention V2: Dao-AILab/flash-attention: Fast and memory-efficient exact attention (github.com)."", ""In terms of train time, DDP with mixed precision is the fastest followed by FSDP using ZERO Stage 2 and Stage 3, respectively. As the task of causal language modelling always has fixed context sequence length (--block\\_size), the train time speedup with FSDP wasn’t that great. For applications with dynamic batching, FSDP which enables larger batch sizes will likely have considerable speed up in terms of train time. FSDP mixed precision support currently has few [issues](https://github.com/pytorch/pytorch/issues/75676) with transformer. Once this is supported, the training time speed up will further improve considerably. ### CPU Offloading to enable training humongous models that won’t fit the GPU memory\nCommand for training GPT-2 XL Model (1.5B parameters):\n```bash\nexport BS=#`try with different batch sizes till you don't get OOM error,\n#i.e., start with larger batch size and go on decreasing till it fits on GPU`\ntime accelerate launch run\\_clm\\_no\\_trainer.py \\\n--model\\_name\\_or\\_path gpt2-xl \\\n--dataset\\_name wikitext \\\n--dataset\\_config\\_name wikitext-2-raw-v1 \\\n--per\\_device\\_train\\_batch\\_size $BS\n--per\\_device\\_eval\\_batch\\_size $BS\n--num\\_train\\_epochs 1\n--block\\_size 12\n```\n| Method | Batch Size Max ($BS) | Num GPUs | Approx Train Time (Hours) | Notes |\n| --- | --- | --- | --- | --- |\n| DDP | 1 | 1 | NA | OOM Error RuntimeError: CUDA out of memory."", ""Why PEFT? Full fine-tuning is expensive. Let’s have some numbers to put things in perspective:\nMinimum GPU memory required for full fine-tuning:\n1. Weight: 2 bytes (Mixed-Precision training)\n2. Weight gradient: 2 bytes\n3. Optimizer state when using Adam: 4 bytes for original FP32 weight + 8 bytes for first and second moment estimates\n4. Cost per parameter adding all of the above: 16 bytes per parameter\n5. \\*\\*15.5B model -> 248GB of GPU memory without even considering huge memory requirements for storing intermediate activations -> minimum 4X A100 80GB GPUs required\\*\\*\nSince the hardware requirements are huge, we'll be using parameter-efficient fine-tuning using [QLoRA](https://arxiv.org/abs/2305.14314). Here are the minimal GPU memory requirements for fine-tuning StarCoder using QLoRA:\n> trainable params: 110,428,160 || all params: 15,627,884,544 || trainable%: 0.7066097761926236\n1. Base model Weight: 0.5 bytes \\* 15.51B frozen params = 7.755 GB\n2. Adapter weight: 2 bytes \\* 0.11B trainable params = 0.22GB\n3. Weight gradient: 2 bytes \\* 0.11B trainable params = 0.12GB\n4. Optimizer state when using Adam: 4 bytes \\* 0.11B trainable params \\* 3 = 1.32GB\n5. \\*\\*Adding all of the above -> 9.51 GB ~10GB -> 1 A100 40GB GPU required\\*\\* 🤯. The reason for A100 40GB GPU is that the intermediate activations for long sequence lengths of 2048 and batch size of 4 for training lead to higher memory requirements. As we will see below, GPU memory required is 26GB which can be accommodated on A100 40GB GPU. Also, A100 GPUs have better compatibilty with Flash Attention 2. In the above calculations, we didn't consider memory required for intermediate activation checkpointing which is considerably huge. We leverage Flash Attention V2 and Gradient Checkpointing to overcome this issue. 1. For QLoRA along with flash attention V2 and gradient checkpointing, the total memory occupied by the model on a single A100 40GB GPU is \\*\\*26 GB\\*\\* with a \\*\\*batch size of 4\\*\\*. 2. For full fine-tuning using FSDP along with Flash Attention V2 and Gradient Checkpointing, the memory occupied per GPU ranges between \\*\\*70 GB to 77.6 GB\\*\\* with a \\*\\*per\\_gpu\\_batch\\_size of 1\\*\\*. Please refer to the [model-memory-usage](https://huggingface.co/spaces/hf-accelerate/model-memory-usage) to easily calculate how much vRAM is needed to train and perform big model inference on a model hosted on the 🤗 Hugging Face Hub. ## Full Finetuning\nWe will look at how to do full fine-tuning of `bigcode/starcoder` (15B params) on 8 A100 80GB GPUs using PyTorch Fully Sharded Data Parallel (FSDP) technique. For more information on FSDP, please refer to [Fine-tuning Llama 2 70B using PyTorch FSDP](https://huggingface.co/blog/ram-efficient-pytorch-fsdp) and [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface.co/blog/pytorch-fsdp)."", ""This section covers some of the most important FSDP options. To learn more about the other available FSDP options, take a look at the [fsdp\\_config](https://huggingface.co/docs/transformers/main\\_classes/trainer#transformers.TrainingArguments.fsdp\\_config) parameters. ### Sharding strategy\nFSDP offers a number of sharding strategies to select from:\n\\* `FULL\\_SHARD` - shards model parameters, gradients and optimizer states across workers; select `1` for this option\n\\* `SHARD\\_GRAD\\_OP`- shard gradients and optimizer states across workers; select `2` for this option\n\\* `NO\\_SHARD` - don't shard anything (this is equivalent to DDP); select `3` for this option\n\\* `HYBRID\\_SHARD` - shard model parameters, gradients and optimizer states within each worker where each worker also has a full copy; select `4` for this option\n\\* `HYBRID\\_SHARD\\_ZERO2` - shard gradients and optimizer states within each worker where each worker also has a full copy; select `5` for this option\nThis is enabled by the `fsdp\\_sharding\\_strategy` flag."", ""After your dataset is prepared, and all the necessary training components are loaded, the script checks if you're using the `fsdp\\_plugin`. PyTorch offers two ways for wrapping model layers in FSDP, automatically or manually. The simplest method is to allow FSDP to automatically recursively wrap model layers without changing any other code. You can choose to wrap the model layers based on the layer name or on the size (number of parameters). In the FSDP configuration file, it uses the `TRANSFORMER\\_BASED\\_WRAP` option to wrap the [`T5Block`] layer. ```py\nif getattr(accelerator.state, \""fsdp\\_plugin\"", None) is not None:\naccelerator.state.fsdp\\_plugin.auto\\_wrap\\_policy = fsdp\\_auto\\_wrap\\_policy(model)\n```\nNext, use 🤗 Accelerate's [`~accelerate.Accelerator.prepare`] function to prepare the model, datasets, optimizer, and scheduler for training. ```py\nmodel, train\\_dataloader, eval\\_dataloader, optimizer, lr\\_scheduler = accelerator.prepare(\nmodel, train\\_dataloader, eval\\_dataloader, optimizer, lr\\_scheduler\n)\n```\nFrom here, the remainder of the script handles the training loop, evaluation, and sharing your model to the Hub. ## Train\nRun the following command to launch the training script. Earlier, you saved the configuration file to `fsdp\\_config.yaml`, so you'll need to pass the path to the launcher with the `--config\\_file` argument like this:\n```bash\naccelerate launch --config\\_file fsdp\\_config.yaml examples/peft\\_lora\\_seq2seq\\_accelerate\\_fsdp.py\n```\nOnce training is complete, the script returns the accuracy and compares the predictions to the labels.""]","Fully Sharded Data Parallel (FSDP) is a data parallel method that shards a model's parameters, gradients, and optimizer states across the number of available GPUs (also called workers or rank). The purpose of FSDP in distributed training is to reduce memory usage by not replicating the entire model on each GPU, which improves GPU memory efficiency. This allows for training much larger models on fewer GPUs compared to traditional methods like DistributedDataParallel (DDP). FSDP is integrated with the Accelerate library, making it easier to manage training in distributed environments.","FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes."
"What file format is used to save and store PyTorch model weights more securely than `.bin` files?
","[""Convert weights to safetensors\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`. The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository. For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights. Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.""]",The more secure file format used to save and store PyTorch model weights is `.safetensors`.,`.safetensors`
"What type of security certification does Hugging Face have?
","[""Security\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)"", ""The certification process\nThe certification process is \\*\\*completely free\\*\\*:\n- To get a \\*certificate of completion\\*: you need \\*\\*to pass 80% of the assignments\\*\\*. - To get a \\*certificate of excellence\\*: you need \\*\\*to pass 100% of the assignments\\*\\*. There's \\*\\*no deadlines, the course is self-paced\\*\\*. ![Course certification](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit0/certification.jpg)\nWhen we say pass, \\*\\*we mean that your model must be pushed to the Hub and get a result equal or above the minimal requirement\\*\\*. To check your progression and which unit you passed/not passed: https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course\nNow that you're ready for the certification process, you need to:\n1. Go here: https://huggingface.co/spaces/huggingface-projects/Deep-RL-Course-Certification/\n2. Type your \\*hugging face username\\*, your \\*first name\\*, \\*last name\\*\n3. Click on \""Generate my certificate\"". - If you passed 80% of the assignments, \\*\\*congratulations\\*\\* you've just got the certificate of completion. - If you passed 100% of the assignments, \\*\\*congratulations\\*\\* you've just got the excellence certificate. - If you are below 80%, don't be discouraged!"", ""| Version | Supported |\n|---------|--------------------|\n| 1.x.x | :white\\_check\\_mark: |\n| 2.x.x | :white\\_check\\_mark: |\n## Reporting a Vulnerability\nTo report a security vulnerability, please contact: security@huggingface.co"", ""--\ntitle: \""Hugging Face Selected for the French Data Protection Agency Enhanced Support Program\""\nthumbnail: /blog/assets/146\\_cnil-accompaniment/logo.png\nauthors:\n- user: yjernite\n- user: julien-c\n- user: annatrdj\n- user: Ima1\n---\n# Hugging Face Selected for the French Data Protection Agency Enhanced Support Program\n\\*This blog post was originally published on [LinkedIn on 05/15/2023](https://www.linkedin.com/pulse/accompagnement-renforc%25C3%25A9-de-la-cnil-et-protection-des-donn%25C3%25A9es/)\\*\nWe are happy to announce that Hugging Face has been selected by the [CNIL](https://www.cnil.fr/en/home) (French Data Protection Authority) to benefit from its [Enhanced Support program](https://www.cnil.fr/en/enhanced-support-cnil-selects-3-digital-companies-strong-potential)! This new program picked three companies with “strong potential for economic development” out of over 40 candidates, who will receive support in understanding and implementing their duties with respect to data protection -\na daunting and necessary endeavor in the context of the rapidly evolving field of Artificial Intelligence.""]","Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses.",SOC2 Type 2 certified
"What do RAG models combine to generate outputs?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# RAG\n\n[![Models](https://img.shields.io/badge/All_model_pages-rag-blueviolet)](https://huggingface.co/models?filter=rag)\n\n## Overview\nRetrieval-augmented generation (\""RAG\"") models combine the powers of pretrained dense retrieval (DPR) and\nsequence-to-sequence models."", ""If this path already exists, the script will use saved predictions to calculate metrics. Add `--recalculate` parameter to force the script to perform inference from scratch. An example e2e evaluation run could look as follows:\n```bash\npython examples/research\\_projects/rag/eval\\_rag.py \\\n--model\\_name\\_or\\_path facebook/rag-sequence-nq \\\n--model\\_type rag\\_sequence \\\n--evaluation\\_set path/to/test.source \\\n--gold\\_data\\_path path/to/gold\\_data \\\n--predictions\\_path path/to/e2e\\_preds.txt \\\n--eval\\_mode e2e \\\n--gold\\_data\\_mode qa \\\n--n\\_docs 5 \\ # You can experiment with retrieving different number of documents at evaluation time\n--print\\_predictions \\\n--recalculate \\ # adding this parameter will force recalculating predictions even if predictions\\_path already exists\n```\n# Use your own knowledge source\nBy default, RAG uses the English Wikipedia as a knowledge source, known as the 'wiki\\_dpr' dataset. With `use\\_custom\\_knowledge\\_dataset.py` you can build your own knowledge source, \\*e.g.\\* for RAG. For instance, if documents are serialized as tab-separated csv files with the columns \""title\"" and \""text\"", one can use `use\\_own\\_knowledge\\_dataset.py` as follows:\n```bash\npython examples/research\\_projects/rag/use\\_own\\_knowledge\\_dataset.py \\\n--csv\\_path path/to/my\\_csv \\\n--output\\_dir path/to/my\\_knowledge\\_dataset \\\n```\nThe created outputs in `path/to/my\\_knowledge\\_dataset` can then be used to finetune RAG as follows:\n```bash\npython examples/research\\_projects/rag/finetune\\_rag.py \\\n--data\\_dir $DATA\\_DIR \\\n--output\\_dir $OUTPUT\\_DIR \\\n--model\\_name\\_or\\_path $MODEL\\_NAME\\_OR\\_PATH \\\n--model\\_type rag\\_sequence \\\n--fp16 \\\n--gpus 8\n--index\\_name custom\n--passages\\_path path/to/data/my\\_knowledge\\_dataset\n--index\\_path path/to/my\\_knowledge\\_dataset\\_hnsw\\_index.faiss\n```"", ""To get those metrics, we'll need to generate outputs from the model and convert them to strings. Let's build some lists of labels and predictions for the ROUGE metric to compare (note that if you get import errors for this section, you may need to`!pip install tqdm`). We're also going to use a trick that dramatically increases performance - compiling our generation code with [XLA](https://www.tensorflow.org/xla), TensorFlow's accelerated linear algebra compiler. XLA applies various optimizations to the model's computation graph, and results in significant improvements to speed and memory usage. As described in the Hugging Face [blog](https://huggingface.co/blog/tf-xla-generate), XLA works best when our input shapes don't vary too much. To handle this, we'll pad our inputs to multiples of 128, and make a new dataset with the padding collator, and then we'll apply the `@tf.function(jit\\_compile=True)` decorator to our generation function, which marks the whole function for compilation with XLA. ```python\nfrom tqdm import tqdm\nimport numpy as np\ngeneration\\_data\\_collator = DataCollatorForSeq2Seq(\ntokenizer, model=model, return\\_tensors=\""tf\"", pad\\_to\\_multiple\\_of=320\n)\ntf\\_generate\\_dataset = model.prepare\\_tf\\_dataset(\ntokenized\\_datasets[\""validation\""],\ncollate\\_fn=generation\\_data\\_collator,\nshuffle=False,\nbatch\\_size=8,\ndrop\\_remainder=True,\n)\n@tf.function(jit\\_compile=True)\ndef generate\\_with\\_xla(batch):\nreturn model.generate(\ninput\\_ids=batch[\""input\\_ids\""],\nattention\\_mask=batch[\""attention\\_mask\""],\nmax\\_new\\_tokens=32,\n)\nall\\_preds = []\nall\\_labels = []\nfor batch, labels in tqdm(tf\\_generate\\_dataset):\npredictions = generate\\_with\\_xla(batch)\ndecoded\\_preds = tokenizer.batch\\_decode(predictions, skip\\_special\\_tokens=True)\nlabels = labels.numpy()\nlabels = np.where(labels != -100, labels, tokenizer.pad\\_token\\_id)\ndecoded\\_labels = tokenizer.batch\\_decode(labels, skip\\_special\\_tokens=True)\ndecoded\\_preds = [\""\\n\"".join(sent\\_tokenize(pred.strip())) for pred in decoded\\_preds]\ndecoded\\_labels = [\""\\n\"".join(sent\\_tokenize(label.strip())) for label in decoded\\_labels]\nall\\_preds.extend(decoded\\_preds)\nall\\_labels.extend(decoded\\_labels)\n```\nOnce we have our lists of label and prediction strings, computing the ROUGE score is easy:\n```python\nresult = rouge\\_score.compute(\npredictions=decoded\\_preds, references=decoded\\_labels, use\\_stemmer=True\n)\nresult = {key: value.mid.fmeasure \\* 100 for key, value in result.items()}\n{k: round(v, 4) for k, v in result.items()}\n```\n```\n{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}\n```\n{/if}\n{#if fw === 'pt'}\n## Fine-tuning mT5 with 🤗 Accelerate[[fine-tuning-mt5-with-accelerate]]\nFine-tuning our model with 🤗 Accelerate is very similar to the text classification example we encountered in [Chapter 3](/course/chapter3).""]",RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs.,Pretrained dense retrieval (DPR) and sequence-to-sequence models.
"What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# MarkupLM\n## Overview\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm). The model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838\\_From\\_one\\_tree\\_to\\_a\\_forest\\_a\\_unified\\_solution\\_for\\_structured\\_web\\_data\\_extraction), a dataset\nfor information extraction from web pages (basically named-entity recogntion on web pages)\nThe abstract from the paper is the following:\n\\*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\ntasks. The pre-trained model and code will be publicly available.\\*\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm). ## Usage tips\n- In addition to `input\\_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath\\_tags\\_seq` and `xpath\\_subs\\_seq`. These are the XPATH tags and subscripts respectively for each token in the input sequence. - One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info. ![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg)\n MarkupLM architecture. Taken from the [original paper.](https://arxiv.org/abs/2110.08518) \n## Usage: MarkupLMProcessor\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input\\_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks. ```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\nfeature\\_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from\\_pretrained(\""microsoft/markuplm-base\"")\nprocessor = MarkupLMProcessor(feature\\_extractor, tokenizer)\n```\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input\\_ids`, `attention\\_mask`, `token\\_type\\_ids`, `xpath\\_subs\\_seq`, `xpath\\_tags\\_seq`. Optionally, one can provide node labels to the processor, which are turned into token-level `labels`. [`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]."", ""In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs). \\*\\*Use case 1: web page classification (training, inference) + token classification (inference), parse\\_html = True\\*\\*\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML. ```python\n>>> from transformers import MarkupLMProcessor\n>>> processor = MarkupLMProcessor.from\\_pretrained(\""microsoft/markuplm-base\"")\n>>> html\\_string = \""\""\""\n... ... ... ... Hello world\n... ... ... # Welcome\n\n... Here is my website. ... ... \""\""\""\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\n>>> encoding = processor(html\\_string, return\\_tensors=\""pt\"")\n>>> print(encoding.keys())\ndict\\_keys(['input\\_ids', 'token\\_type\\_ids', 'attention\\_mask', 'xpath\\_tags\\_seq', 'xpath\\_subs\\_seq'])\n```\n\\*\\*Use case 2: web page classification (training, inference) + token classification (inference), parse\\_html=False\\*\\*\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse\\_html` to `False`. ```python\n>>> from transformers import MarkupLMProcessor\n>>> processor = MarkupLMProcessor.from\\_pretrained(\""microsoft/markuplm-base\"")\n>>> processor.parse\\_html = False\n>>> nodes = [\""hello\"", \""world\"", \""how\"", \""are\""]\n>>> xpaths = [\""/html/body/div/li[1]/div/span\"", \""/html/body/div/li[1]/div/span\"", \""html/body\"", \""html/body/div\""]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return\\_tensors=\""pt\"")\n>>> print(encoding.keys())\ndict\\_keys(['input\\_ids', 'token\\_type\\_ids', 'attention\\_mask', 'xpath\\_tags\\_seq', 'xpath\\_subs\\_seq'])\n```\n\\*\\*Use case 3: token classification (training), parse\\_html=False\\*\\*\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\ncorresponding node labels in order to train a model.""]","MarkupLMFeatureExtractor uses Beautiful Soup, a Python library for pulling data out of HTML and XML files, to extract data from HTML and XML documents.",Beautiful Soup
"What is the file size limit for syncing to HF Spaces without using Git-LFS?
","[""There are a few reasons for this:\n- Uploading and downloading smaller files is much easier both for you and the other users. Connection issues can always\nhappen when streaming data and smaller files avoid resuming from the beginning in case of errors. - Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\nleading to a slower download speed. In all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size. - \\*\\*Number of commits\\*\\*: There is no hard limit for the total number of commits on your repo history. However, from\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a\nfresh start using `huggingface\\_hub`'s [`super\\_squash\\_history`](https://huggingface.co/docs/huggingface\\_hub/main/en/package\\_reference/hf\\_api#huggingface\\_hub.HfApi.super\\_squash\\_history)."", ""Managing Spaces with CircleCI Workflows\nYou can keep your app in sync with your GitHub repository with a \\*\\*CircleCI workflow\\*\\*. [CircleCI](https://circleci.com) is a continuous integration and continuous delivery (CI/CD) platform that helps automate the software development process. A [CircleCI workflow](https://circleci.com/docs/workflows/) is a set of automated tasks defined in a configuration file, orchestrated by CircleCI, to streamline the process of building, testing, and deploying software applications. \\*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\\*\nFirst, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository. ```bash\ngit remote add space https://huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME\n```\nThen force push to sync everything for the first time:\n```bash\ngit push --force space main\n```\nNext, set up a [CircleCI workflow](https://circleci.com/docs/workflows/) to push your `main` git branch to Spaces. In the example below:\n\\* Replace `HF\\_USERNAME` with your username and `SPACE\\_NAME` with your Space name. \\* [Create a context in CircleCI](https://circleci.com/docs/contexts/) and add an env variable into it called \\*HF\\_PERSONAL\\_TOKEN\\* (you can give it any name, use the key you create in place of HF\\_PERSONAL\\_TOKEN) and the value as your Hugging Face API token. You can find your Hugging Face API token under \\*\\*API Tokens\\*\\* on [your Hugging Face profile](https://huggingface.co/settings/tokens). ```yaml\nversion: 2.1\nworkflows:\nmain:\njobs:\n- sync-to-huggingface:\ncontext:\n- HuggingFace\nfilters:\nbranches:\nonly:\n- main\njobs:\nsync-to-huggingface:\ndocker:\n- image: alpine\nresource\\_class: small\nsteps:\n- run:\nname: install git\ncommand: apk update && apk add openssh-client git\n- checkout\n- run:\nname: push to Huggingface hub\ncommand: |\ngit config user.email \""\""\ngit config user.name \""\""\ngit push -f https://HF\\_USERNAME:${HF\\_PERSONAL\\_TOKEN}@huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME main\n```""]","The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. If your files are larger than 10MB, you will need to use Git-LFS.",10MB
"What is the title of the paper introducing the ByT5 model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ByT5\n## Overview\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, Colin Raffel."", "")\n>>> labels\\_dict = tokenizer(\n... [\""La vie est comme une boîte de chocolat.\"", \""Aujourd'hui c'est lundi.\""], padding=\""longest\"", return\\_tensors=\""pt\""\n... )\n>>> labels = labels\\_dict.input\\_ids\n>>> loss = model(\\*\\*model\\_inputs, labels=labels).loss\n>>> loss.item()\n17.9\n```\nSimilar to [T5](t5), ByT5 was trained on the span-mask denoising task. However,\nsince the model works directly on characters, the pretraining task is a bit\ndifferent. Let's corrupt some characters of the\ninput sentence `\""The dog chases a ball in the park.\""` and ask ByT5 to predict them\nfor us. ```python\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n>>> import torch\n>>> tokenizer = AutoTokenizer.from\\_pretrained(\""google/byt5-base\"")\n>>> model = AutoModelForSeq2SeqLM.from\\_pretrained(\""google/byt5-base\"")\n>>> input\\_ids\\_prompt = \""The dog chases a ball in the park.\""\n>>> input\\_ids = tokenizer(input\\_ids\\_prompt).input\\_ids\n>>> # Note that we cannot add \""{extra\\_id\\_...}\"" to the string directly\n>>> # as the Byte tokenizer would incorrectly merge the tokens\n>>> # For ByT5, we need to work directly on the character level\n>>> # Contrary to T5, ByT5 does not use sentinel tokens for masking, but instead\n>>> # uses final utf character ids.""]","The title of the paper introducing the ByT5 model is ""ByT5: Towards a token-free future with pre-trained byte-to-byte models.""",ByT5: Towards a token-free future with pre-trained byte-to-byte models
"What is the dimension of the feature vector for the base BERT model?
","[""n this video, we'll study the encoder architecture. An example of a popular encoder-only architecture is BERT, which is the most popular model of its kind. Let's first start by understanding how it works. We'll use a small example, using three words. We use these as inputs, and pass them through the encoder. We retrieve a numerical representation of each word. Here, for example, the encoder converts the three words “Welcome to NYC” in these three sequences of numbers. The encoder outputs exactly one sequence of numbers per input word. This numerical representation can also be called a \""Feature vector\"", or \""Feature tensor\"". Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \""to\"", isn't the representation of only the \""to\"" word. It also takes into account the words around it, which we call the “context”.As in, it looks to the left context, the word on the left of the one we're studying (here the word \""Welcome\"") and the context on the right (here the word \""NYC\"") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \""meaning\"" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence.""]",The dimension of the feature vector for the base BERT model is 768.,768
"What special identifier does the WordPiece Model use for continuing subwords?
","[""In turn, the model will now be able to make sense of token in different situations. It will understand that the words token, tokens, tokenizing, and tokenization are linked and have a similar meaning. It will also understand that tokenization, modernization, and immunization, which all have the same suffixes, are probably used in the same syntactic situations. Subword-based tokenizers generally have a way to identify which tokens are start of words, and which tokens complete start of words: token as the start of a word. ##ization as completing a word. Here the ## prefix indicates that ization is part of a word rather than the beginning of it. The ## comes from the BERT tokenizer, based on the WordPiece algorithm. Other tokenizers use other prefixes, which can be placed to indicate part of words like seen here, or start of words instead! There are a lot of different algorithms that can be used for subword tokenization, and most models obtaining state-of-the-art results in English today use some kind of subword-tokenization algorithm."", ""This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |\n## Post-Processors\nAfter the whole pipeline, we sometimes want to insert some special\ntokens before feed a tokenized string into a model like \""[CLS] My\nhorse is amazing [SEP]\"". The `PostProcessor` is the component doing\njust that. | Name | Description | Example |\n| :--- | :--- | :--- |\n| TemplateProcessing | Let’s you easily template the post processing, adding special tokens, and specifying the `type\\_id` for each sequence/special token. The template is given two strings representing the single sequence and the pair of sequences, as well as a set of special tokens to use. | Example, when specifying a template with these values:  \n\n* single: `\""[CLS] $A [SEP]\""`\n* pair: `\""[CLS] $A [SEP] $B [SEP]\""`\n* special tokens:\n  + `\""[CLS]\""`\n  + `\""[SEP]\""`\n\n Input: `(\""I like this\"", \""but not this\"")`   \n Output: `\""[CLS] I like this [SEP] but not this [SEP]\""` |\n## Decoders\nThe Decoder knows how to go from the IDs used by the Tokenizer, back to\na readable piece of text. Some `Normalizer` and `PreTokenizer` use\nspecial characters or identifiers that need to be reverted for example. | Name | Description |\n| :--- | :--- |\n| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTokenizer encodes at the byte-level, using a set of visible Unicode characters to represent each byte, so we need a Decoder to revert this process and get something readable again. |\n| Metaspace | Reverts the Metaspace PreTokenizer. This PreTokenizer uses a special identifer `▁` to identify whitespaces, and so this Decoder helps with decoding these. |\n| WordPiece | Reverts the WordPiece Model. This model uses a special identifier `##` for continuing subwords, and so this Decoder helps with decoding these. |"", ""WordPiece tokenization[[wordpiece-tokenization]]\nWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It's very similar to BPE in terms of the training, but the actual tokenization is done differently. 💡 This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm. ## Training algorithm[[training-algorithm]]\n⚠️ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate. Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\""word\""` gets split like this:\n```\nw ##o ##r ##d\n```\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix. Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\n$$\\mathrm{score} = (\\mathrm{freq\\\\_of\\\\_pair}) / (\\mathrm{freq\\\\_of\\\\_first\\\\_element} \\times \\mathrm{freq\\\\_of\\\\_second\\\\_element})$$\nBy dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary."", ""💡 Using `train\\_new\\_from\\_iterator()` on the same corpus won't result in the exact same vocabulary. This is because the 🤗 Tokenizers library does not implement WordPiece for the training (since we are not completely sure of its internals), but uses BPE instead. To tokenize a new text, we pre-tokenize it, split it, then apply the tokenization algorithm on each word. That is, we look for the biggest subword starting at the beginning of the first word and split it, then we repeat the process on the second part, and so on for the rest of that word and the following words in the text:\n```python\ndef encode\\_word(word):\ntokens = []\nwhile len(word) > 0:\ni = len(word)\nwhile i > 0 and word[:i] not in vocab:\ni -= 1\nif i == 0:\nreturn [\""[UNK]\""]\ntokens.append(word[:i])\nword = word[i:]\nif len(word) > 0:\nword = f\""##{word}\""\nreturn tokens\n```\nLet's test it on one word that's in the vocabulary, and another that isn't:\n```python\nprint(encode\\_word(\""Hugging\""))\nprint(encode\\_word(\""HOgging\""))\n```\n```python out\n['Hugg', '##i', '##n', '##g']\n['[UNK]']\n```\nNow, let's write a function that tokenizes a text:\n```python\ndef tokenize(text):\npre\\_tokenize\\_result = tokenizer.\\_tokenizer.pre\\_tokenizer.pre\\_tokenize\\_str(text)\npre\\_tokenized\\_text = [word for word, offset in pre\\_tokenize\\_result]\nencoded\\_words = [encode\\_word(word) for word in pre\\_tokenized\\_text]\nreturn sum(encoded\\_words, [])\n```\nWe can try it on any text:\n```python\ntokenize(\""This is the Hugging Face course!\"")\n```\n```python out\n['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',\n'##e', '[UNK]']\n```\nThat's it for the WordPiece algorithm! Now let's take a look at Unigram.""]",The WordPiece Model uses the special identifier `##` for continuing subwords.,##
"What is the purpose of the 🧨 Diffusers tutorials?
","[""Overview\nThese examples show how to run [Diffuser](https://arxiv.org/abs/2205.09991) in Diffusers. There are two ways to use the script, `run\\_diffuser\\_locomotion.py`. The key option is a change of the variable `n\\_guide\\_steps`. When `n\\_guide\\_steps=0`, the trajectories are sampled from the diffusion model, but not fine-tuned to maximize reward in the environment. By default, `n\\_guide\\_steps=2` to match the original implementation. You will need some RL specific requirements to run the examples:\n```\npip install -f https://download.pytorch.org/whl/torch\\_stable.html \\\nfree-mujoco-py \\\neinops \\\ngym==0.24.1 \\\nprotobuf==3.20.1 \\\ngit+https://github.com/rail-berkeley/d4rl.git \\\nmediapy \\\nPillow==9.0.0\n```"", ""For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.\\*\\*\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!"", ""--\ntitle: Swift 🧨Diffusers - Fast Stable Diffusion for Mac\nthumbnail: /blog/assets/fast-mac-diffusers/thumbnail.png\nauthors:\n- user: pcuenq\n- user: reach-vb\n---\n# Swift 🧨Diffusers: Fast Stable Diffusion for Mac\nTransform your text into stunning images with ease using Diffusers for Mac, a native app powered by state-of-the-art diffusion models. It leverages a bouquet of SoTA Text-to-Image models contributed by the community to the Hugging Face Hub, and converted to Core ML for blazingly fast performance. Our latest version, 1.1, is now available on the [Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) with significant performance upgrades and user-friendly interface tweaks. It's a solid foundation for future feature updates. Plus, the app is fully open source with a permissive [license](https://github.com/huggingface/swift-coreml-diffusers/blob/main/LICENSE), so you can build on it too! Check out our GitHub repository at https://github.com/huggingface/swift-coreml-diffusers for more information. ![Screenshot showing Diffusers for Mac UI](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-mac-diffusers/UI.png)\n## What exactly is 🧨Diffusers for Mac anyway? The Diffusers app ([App Store](https://apps.apple.com/app/diffusers/id1666309574), [source code](https://github.com/huggingface/swift-coreml-diffusers)) is the Mac counterpart to our [🧨`diffusers` library](https://github.com/huggingface/diffusers). This library is written in Python with PyTorch, and uses a modular design to train and run diffusion models."", ""If you liked this topic and want to learn more, we recommend the following resources:\n- Our [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). - The [Getting Started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers\\_intro.ipynb) notebook, that gives a broader overview on Diffusion systems. - The [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) blog post. - Our [code in GitHub](https://github.com/huggingface/diffusers) where we'd be more than happy if you leave a ⭐ if `diffusers` is useful to you! ### Citation:\n```\n@article{patil2022stable,\nauthor = {Patil, Suraj and Cuenca, Pedro and Lambert, Nathan and von Platen, Patrick},\ntitle = {Stable Diffusion with 🧨 Diffusers},\njournal = {Hugging Face Blog},\nyear = {2022},\nnote = {[https://huggingface.co/blog/rlhf](https://huggingface.co/blog/stable\\_diffusion)},\n}\n```""]","The purpose of the 🧨 Diffusers tutorials is to provide users with guidance on how to effectively use the Diffusers library, which is designed for training and running diffusion models. These tutorials help users understand the functionalities of the library, demonstrate how to implement various features, and offer practical examples to enhance the learning experience. They serve as a resource for both beginners and experienced users to get started with and make the most out of the Diffusers library.",To provide a gentle introduction to diffusion models and help understand the library fundamentals.
"What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?
","[""Using Flagging\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\nTags: FLAGGING, DATA\n## Introduction\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \""hard\"" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust. Gradio simplifies the collection of this data by including a \\*\\*Flag\\*\\* button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`. ## The \\*\\*Flag\\*\\* button in `gradio.Interface`\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked \\*\\*Flag\\*\\*. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file. There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail. - `allow\\_flagging`: this parameter can be set to either `\""manual\""` (default), `\""auto\""`, or `\""never\""`. - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked. - `auto`: users will not see a button to flag, but every sample will be flagged automatically. - `never`: users will not see a button to flag, and no sample will be flagged. - `flagging\\_options`: this parameter can be either `None` (default) or a list of strings. - If `None`, then the user simply clicks on the \\*\\*Flag\\*\\* button and no additional options are shown. - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\""Incorrect\"", \""Ambiguous\""]`, then buttons labeled \\*\\*Flag as Incorrect\\*\\* and \\*\\*Flag as Ambiguous\\*\\* appear. This only applies if `allow\\_flagging` is `\""manual\""`. - The chosen option is then logged along with the input and output. - `flagging\\_dir`: this parameter takes a string. - It represents what to name the directory where flagged data is stored. - `flagging\\_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n- Using this parameter allows you to write custom code that gets run when the flag button is clicked\n- By default, this is set to an instance of `gr.CSVLogger`\n- One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset.""]","The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `""manual""`.","""manual"""
"Where can the full code for the Stable Diffusion demo be found?
","[""ote: This is a simplified version of the code needed to create the Stable Diffusion demo."", ""--\ntitle: 🧨 Stable Diffusion in JAX / Flax ! thumbnail: /blog/assets/108\\_stable\\_diffusion\\_jax/thumbnail.png\nauthors:\n- user: pcuenq\n- user: patrickvonplaten\n---\n# 🧨 Stable Diffusion in JAX / Flax ! [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_jax_how_to.ipynb)\n# \\*\\*Stable Diffusion in JAX / Flax\\*\\* 🚀\n🤗 Hugging Face [Diffusers](https://github.com/huggingface/diffusers) supports Flax since version `0.5.1`! This allows for super fast inference on Google TPUs, such as those available in Colab, Kaggle or Google Cloud Platform. This post shows how to run inference using JAX / Flax. If you want more details about how Stable Diffusion works or want to run it in GPU, please refer to [this Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). If you want to follow along, click the button above to open this post as a Colab notebook. First, make sure you are using a TPU backend. If you are running this notebook in Colab, select `Runtime` in the menu above, then select the option \""Change runtime type\"" and then select `TPU` under the `Hardware accelerator` setting. Note that JAX is not exclusive to TPUs, but it shines on that hardware because each TPU server has 8 TPU accelerators working in parallel. ## Setup\n``` python\nimport jax\nnum\\_devices = jax.device\\_count()\ndevice\\_type = jax.devices()[0].device\\_kind\nprint(f\""Found {num\\_devices} JAX devices of type {device\\_type}.\"")\nassert \""TPU\"" in device\\_type, \""Available device is not a TPU, please select TPU from Edit > Notebook settings > Hardware accelerator\""\n```\n\\*Output\\*:\n```bash\nFound 8 JAX devices of type TPU v2."", ""Hence we call it ControlNet-XS. We provide the code for controlling StableDiffusion-XL [Podell et al., 2023] (Model B, 48M Parameters) and StableDiffusion 2.1 [Rombach et al. 2022] (Model B, 14M Parameters), all under openrail license.\\*\nThis model was contributed by [UmerHA](https://twitter.com/UmerHAdil). ❤️\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## StableDiffusionControlNetXSPipeline\n[[autodoc]] StableDiffusionControlNetXSPipeline\n- all\n- \\_\\_call\\_\\_\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput"", ""--\ntitle: Using Stable Diffusion with Core ML on Apple Silicon\nthumbnail: /blog/assets/diffusers\\_coreml/thumbnail.png\nauthors:\n- user: pcuenq\n---\n# Using Stable Diffusion with Core ML on Apple Silicon\nThanks to Apple engineers, you can now run Stable Diffusion on Apple Silicon using Core ML! [This Apple repo](https://github.com/apple/ml-stable-diffusion) provides conversion scripts and inference code based on [🧨 Diffusers](https://github.com/huggingface/diffusers), and we love it! To make it as easy as possible for you, we converted the weights ourselves and put the Core ML versions of the models in [the Hugging Face Hub](https://hf.co/apple). \\*\\*Update\\*\\*: some weeks after this post was written we created a native Swift app that you can use to run Stable Diffusion effortlessly on your own hardware. We released [an app in the Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) as well as [the source code to allow other projects to use it](https://github.com/huggingface/swift-coreml-diffusers). The rest of this post guides you on how to use the converted weights in your own code or convert additional weights yourself. ## Available Checkpoints\nThe official Stable Diffusion checkpoints are already converted and ready for use:\n- Stable Diffusion v1.4: [converted](https://hf.co/apple/coreml-stable-diffusion-v1-4) [original](https://hf.co/CompVis/stable-diffusion-v1-4)\n- Stable Diffusion v1.5: [converted](https://hf.co/apple/coreml-stable-diffusion-v1-5) [original](https://hf.co/runwayml/stable-diffusion-v1-5)\n- Stable Diffusion v2 base: [converted](https://hf.co/apple/coreml-stable-diffusion-2-base) [original](https://huggingface.co/stabilityai/stable-diffusion-2-base)\n- Stable Diffusion v2.1 base: [converted](https://hf.co/apple/coreml-stable-diffusion-2-1-base) [original](https://huggingface.co/stabilityai/stable-diffusion-2-1-base)\nCore ML supports all the compute units available in your device: CPU, GPU and Apple's Neural Engine (NE)."", ""Free solutions include many [spaces](https://huggingface.co/spaces) here on 🤗 Hugging Face, such as the [Stable Diffusion 2.1 Demo](https://huggingface.co/spaces/stabilityai/stable-diffusion) or the [camemduru webui](https://huggingface.co/spaces/camenduru/webui). You can find a list of additional online services [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services). You can even use 🤗 [Diffusers](https://huggingface.co/docs/diffusers/index) to write your own free solution! You can find a simple code example to get started [here](https://colab.research.google.com/drive/1HebngGyjKj7nLdXfj6Qi0N1nh7WvD74z?usp=sharing). \\*Note:\\* Parts of this series will use advanced features such as image2image, which may not be available on all online services. ### Generating Concept Art \nLet's generate some concept art. The steps are simple:\n1. Type what you want. 2. Click generate. ![Stable Diffusion Demo Space](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/sd-demo.png)\nBut, how do you get the results you actually want?"", ""They can be found through the following links:\n1. [Stable Diffusion v1.1](https://huggingface.co/CompVis/stable-diffusion-v1-1)\n2. [Stable Diffusion v1.2](https://huggingface.co/CompVis/stable-diffusion-v1-2)\n3. [Stable Diffusion v1.3](https://huggingface.co/CompVis/stable-diffusion-v1-3)\n4. [Stable Diffusion v1.4](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n```python\nfrom diffusers import DiffusionPipeline\nimport matplotlib.pyplot as plt\npipe = DiffusionPipeline.from\\_pretrained('CompVis/stable-diffusion-v1-4', custom\\_pipeline='suvadityamuk/StableDiffusionComparison')\npipe.enable\\_attention\\_slicing()\npipe = pipe.to('cuda')\nprompt = \""an astronaut riding a horse on mars\""\noutput = pipe(prompt)\nplt.subplots(2,2,1)\nplt.imshow(output.images[0])\nplt.title('Stable Diffusion v1.1')\nplt.axis('off')\nplt.subplots(2,2,2)\nplt.imshow(output.images[1])\nplt.title('Stable Diffusion v1.2')\nplt.axis('off')\nplt.subplots(2,2,3)\nplt.imshow(output.images[2])\nplt.title('Stable Diffusion v1.3')\nplt.axis('off')\nplt.subplots(2,2,4)\nplt.imshow(output.images[3])\nplt.title('Stable Diffusion v1.4')\nplt.axis('off')\nplt.show()\n```\nAs a result, you can look at a grid of all 4 generated images being shown together, that captures a difference the advancement of the training between the 4 checkpoints."", ""If you liked this topic and want to learn more, we recommend the following resources:\n- Our [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). - The [Getting Started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers\\_intro.ipynb) notebook, that gives a broader overview on Diffusion systems. - The [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) blog post. - Our [code in GitHub](https://github.com/huggingface/diffusers) where we'd be more than happy if you leave a ⭐ if `diffusers` is useful to you! ### Citation:\n```\n@article{patil2022stable,\nauthor = {Patil, Suraj and Cuenca, Pedro and Lambert, Nathan and von Platen, Patrick},\ntitle = {Stable Diffusion with 🧨 Diffusers},\njournal = {Hugging Face Blog},\nyear = {2022},\nnote = {[https://huggingface.co/blog/rlhf](https://huggingface.co/blog/stable\\_diffusion)},\n}\n```""]",The full code for the Stable Diffusion demo can be found in the [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).,https://hf.co/spaces/stabilityai/stable-diffusion/tree/main
"What transformation does the FNet model use to replace the self-attention layer in a BERT model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# FNet\n## Overview\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:\n\\*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \""mix\"" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \""efficient\"" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.\\*\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f\\_net). ## Usage tips\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\nsequence length for fine-tuning and inference. ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## FNetConfig\n[[autodoc]] FNetConfig\n## FNetTokenizer\n[[autodoc]] FNetTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## FNetTokenizerFast\n[[autodoc]] FNetTokenizerFast\n## FNetModel\n[[autodoc]] FNetModel\n- forward\n## FNetForPreTraining\n[[autodoc]] FNetForPreTraining\n- forward\n## FNetForMaskedLM\n[[autodoc]] FNetForMaskedLM\n- forward\n## FNetForNextSentencePrediction\n[[autodoc]] FNetForNextSentencePrediction\n- forward\n## FNetForSequenceClassification\n[[autodoc]] FNetForSequenceClassification\n- forward\n## FNetForMultipleChoice\n[[autodoc]] FNetForMultipleChoice\n- forward\n## FNetForTokenClassification\n[[autodoc]] FNetForTokenClassification\n- forward\n## FNetForQuestionAnswering\n[[autodoc]] FNetForQuestionAnswering\n- forward"", ""The abstract from the paper is the following:\n\\*Pre-trained language models like BERT and its variants have recently achieved impressive performance in various\nnatural language understanding tasks. However, BERT heavily relies on the global self-attention block and thus suffers\nlarge memory footprint and computation cost. Although all its attention heads query on the whole input sequence for\ngenerating the attention map from a global perspective, we observe some heads only need to learn local dependencies,\nwhich means the existence of computation redundancy. We therefore propose a novel span-based dynamic convolution to\nreplace these self-attention heads to directly model local dependencies. The novel convolution heads, together with the\nrest self-attention heads, form a new mixed attention block that is more efficient at both global and local context\nlearning. We equip BERT with this mixed attention design and build a ConvBERT model. Experiments have shown that\nConvBERT significantly outperforms BERT and its variants in various downstream tasks, with lower training cost and\nfewer model parameters. Remarkably, ConvBERTbase model achieves 86.4 GLUE score, 0.7 higher than ELECTRAbase, while\nusing less than 1/4 training cost. Code and pre-trained models will be released.\\*\nThis model was contributed by [abhishek](https://huggingface.co/abhishek). The original implementation can be found\nhere: https://github.com/yitu-opensource/ConvBert\n## Usage tips\nConvBERT training tips are similar to those of BERT. For usage tips refer to [BERT documentation](bert). ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## ConvBertConfig\n[[autodoc]] ConvBertConfig\n## ConvBertTokenizer\n[[autodoc]] ConvBertTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## ConvBertTokenizerFast\n[[autodoc]] ConvBertTokenizerFast\n\n## ConvBertModel\n[[autodoc]] ConvBertModel\n- forward\n## ConvBertForMaskedLM\n[[autodoc]] ConvBertForMaskedLM\n- forward\n## ConvBertForSequenceClassification\n[[autodoc]] ConvBertForSequenceClassification\n- forward\n## ConvBertForMultipleChoice\n[[autodoc]] ConvBertForMultipleChoice\n- forward\n## ConvBertForTokenClassification\n[[autodoc]] ConvBertForTokenClassification\n- forward\n## ConvBertForQuestionAnswering\n[[autodoc]] ConvBertForQuestionAnswering\n- forward\n\n## TFConvBertModel\n[[autodoc]] TFConvBertModel\n- call\n## TFConvBertForMaskedLM\n[[autodoc]] TFConvBertForMaskedLM\n- call\n## TFConvBertForSequenceClassification\n[[autodoc]] TFConvBertForSequenceClassification\n- call\n## TFConvBertForMultipleChoice\n[[autodoc]] TFConvBertForMultipleChoice\n- call\n## TFConvBertForTokenClassification\n[[autodoc]] TFConvBertForTokenClassification\n- call\n## TFConvBertForQuestionAnswering\n[[autodoc]] TFConvBertForQuestionAnswering\n- call"", ""This is illustrated examplary for the second\nencoder block (red boxes at bottow) whose weight parameters\n\\\\(\\theta\\_{\\text{enc}}^{\\text{self-attn}, 2}\\\\) and\n\\\\(\\theta\\_{\\text{enc}}^{\\text{feed-forward}, 2}\\\\) are set to BERT\\'s weight\nparameters \\\\(\\theta\\_{\\text{BERT}}^{\\text{feed-forward}, 2}\\\\) and\n\\\\(\\theta\\_{\\text{BERT}}^{\\text{self-attn}, 2}\\\\), respectively at\ninitialization. Before fine-tuning, the encoder therefore behaves exactly like a\npre-trained BERT model. Assuming the input sequence\n\\\\(\\mathbf{x}\\_1, \\ldots, \\mathbf{x}\\_n\\\\) (shown in green) passed to the\nencoder is equal to the input sequence\n\\\\(\\mathbf{x}\\_1^{\\text{BERT}}, \\ldots, \\mathbf{x}\\_n^{\\text{BERT}}\\\\) (shown\nin grey) passed to BERT, this means that the respective output vector\nsequences \\\\(\\mathbf{\\overline{x}}\\_1, \\ldots, \\mathbf{\\overline{x}}\\_n\\\\)\n(shown in darker green) and\n\\\\(\\mathbf{\\overline{x}}\\_1^{\\text{BERT}}, \\ldots, \\mathbf{\\overline{x}}\\_n^{\\text{BERT}}\\\\)\n(shown in darker grey) also have to be equal. Next, let\\'s illustrate how the decoder is warm-started. ![texte du\nlien](https://raw.githubusercontent.com/patrickvonplaten/scientific\\_images/master/encoder\\_decoder/leverage\\_decoder.png)\nThe architecture of the decoder is different from BERT\\'s architecture\nin three ways. 1. First, the decoder has to be conditioned on the contextualized\nencoded sequence \\\\(\\mathbf{\\overline{X}}\\_{1:n}\\\\) by means of\ncross-attention layers. Consequently, randomly initialized\ncross-attention layers are added between the self-attention layer\nand the two feed-forward layers in each BERT block. This is\nrepresented exemplary for the second block by\n\\\\(+\\theta\\_{\\text{dec}}^{\\text{cross-attention, 2}}\\\\) and illustrated\nby the newly added fully connected graph in red in the lower red box\non the right. This necessarily changes the behavior of each modified\nBERT block so that an input vector, \\*e.g.\\* \\\\(\\mathbf{y'}\\_0\\\\) now\nyields a random output vector \\\\(\\mathbf{y''}\\_0\\\\) (highlighted by the\nred border around the output vector \\\\(\\mathbf{y''}\\_0\\\\)). 2. Second, BERT\\'s \\*bi-directional\\* self-attention layers have to be\nchanged to \\*uni-directional\\* self-attention layers to comply with\nauto-regressive generation. Because both the bi-directional and the\nuni-directional self-attention layer are based on the same \\*key\\*,\n\\*query\\* and \\*value\\* projection weights, the decoder\\'s\nself-attention layer weights can be initialized with BERT\\'s\nself-attention layer weights. \\*E.g.\\* the query, key and value weight\nparameters of the decoder\\'s uni-directional self-attention layer\nare initialized with those of BERT\\'s bi-directional self-attention\nlayer \\\\(\\theta\\_{\\text{BERT}}^{\\text{self-attn}, 2} = \\{\\mathbf{W}\\_{\\text{BERT}, k}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{BERT}, v}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{BERT}, q}^{\\text{self-attn}, 2} \\} \\to \\theta\\_{\\text{dec}}^{\\text{self-attn}, 2} = \\{\\mathbf{W}\\_{\\text{dec}, k}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{dec}, v}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{dec}, q}^{\\text{self-attn}, 2} \\}. \\\\) However, in \\*uni-directional\\* self-attention each token only\nattends to all previous tokens, so that the decoder\\'s\nself-attention layers yield different output vectors than BERT\\'s\nself-attention layers even though they share the same weights.""]",The FNet model replaces the self-attention layer in a BERT model with a Fourier transform that returns only the real parts of the transform.,Fourier transform
"What type of test should typically accompany a bug fix in Gradio's testing strategy?
","[""Test Strategy\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to. This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Objectives\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n1. Ensure that the Gradio library functions as we expect it to. 2. Enable the maintenance team to quickly identify both the presence and source of defects. 3. Prevent regressions, i.e. if we fix something it should stay fixed. 4. Improve the quality of the codebase in order to ease maintenance efforts. 5. Reduce the amount of manual testing required. ## Scope\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas. We should not focus on code coverage but on test coverage following the below criteria:\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n## Types of testing\nOur tests will broadly fall into one of three categories:\n- Static Quality checks\n- Dynamic 'Code' tests\n- Dynamic Functional tests\n### Static Quality checks\nStatic quality checks are generally very fast to run and do not require building the code base."", ""Gradio Demo: queue\\_full\\_e2e\\_test\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nimport time\nimport random\nn\\_calls = 0\ndef get\\_random\\_number():\nglobal n\\_calls\nif n\\_calls == 1:\nn\\_calls += 1\nraise gr.Error(\""This is a gradio error\"")\nn\\_calls += 1\ntime.sleep(5)\nreturn random.randrange(1, 10)\nwith gr.Blocks() as demo:\nwith gr.Row():\nwith gr.Column():\nfirst = gr.Button(\""First Call\"")\nsecond = gr.Button(\""Second Call\"")\nthird = gr.Button(\""Third Call\"")\nfourth = gr.Button(\""Fourth Call\"")\nwith gr.Column():\nfirst\\_o = gr.Number(label=\""First Result\"")\nsecond\\_o = gr.Number(label=\""Second Result\"")\nthird\\_o = gr.Number(label=\""Third Result\"")\nfourth\\_o = gr.Number(label=\""Fourth Result\"")\nfirst.click(get\\_random\\_number, None, first\\_o, concurrency\\_id=\""f\"")\nsecond.click(get\\_random\\_number, None, second\\_o, concurrency\\_id=\""f\"")\nthird.click(get\\_random\\_number, None, third\\_o, concurrency\\_id=\""f\"")\nfourth.click(get\\_random\\_number, None, fourth\\_o, concurrency\\_id=\""f\"")\ndemo.queue(max\\_size=2)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""### Testing and Infrastructure Changes:\nNo changes to highlight. ### Breaking Changes:\nNo changes to highlight. ### Full Changelog:\nNo changes to highlight. ### Contributors Shoutout:\nNo changes to highlight. # 0.2.3\n### New Features:\nNo changes to highlight. ### Bug Fixes:\n- Fix example inputs for `gr.File(file\\_count='multiple')` output components by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4153](https://github.com/gradio-app/gradio/pull/4153)\n### Documentation Changes:\nNo changes to highlight. ### Testing and Infrastructure Changes:\nNo changes to highlight. ### Breaking Changes:\nNo changes to highlight. ### Full Changelog:\nNo changes to highlight. ### Contributors Shoutout:\nNo changes to highlight. # 0.2.2\n### New Features:\nNo changes to highlight. ### Bug Fixes:\n- Only send request to `/info` route if demo version is above `3.28.3` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4109](https://github.com/gradio-app/gradio/pull/4109)\n### Other Changes:\n- Fix bug in test from gradio 3.29.0 refactor by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4138](https://github.com/gradio-app/gradio/pull/4138)\n### Breaking Changes:\nNo changes to highlight. # 0.2.1\n### New Features:\nNo changes to highlight. ### Bug Fixes:\nRemoves extraneous `State` component info from the `Client.view\\_api()` method by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)\n### Documentation Changes:\nNo changes to highlight. ### Testing and Infrastructure Changes:\nSeparates flaky tests from non-flaky tests by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)\n### Breaking Changes:\nNo changes to highlight."", ""- [#5682](https://github.com/gradio-app/gradio/pull/5682) [`c57f1b75e`](https://github.com/gradio-app/gradio/commit/c57f1b75e272c76b0af4d6bd0c7f44743ff34f26) - Fix functional tests. Thanks [@abidlabs](https://github.com/abidlabs)! - [#5681](https://github.com/gradio-app/gradio/pull/5681) [`40de3d217`](https://github.com/gradio-app/gradio/commit/40de3d2178b61ebe424b6f6228f94c0c6f679bea) - add query parameters to the `gr.Request` object through the `query\\_params` attribute. Thanks [@DarhkVoyd](https://github.com/DarhkVoyd)! - [#5653](https://github.com/gradio-app/gradio/pull/5653) [`ea0e00b20`](https://github.com/gradio-app/gradio/commit/ea0e00b207b4b90a10e9d054c4202d4e705a29ba) - Prevent Clients from accessing API endpoints that set `api\\_name=False`. Thanks [@abidlabs](https://github.com/abidlabs)! - [#5639](https://github.com/gradio-app/gradio/pull/5639) [`e1874aff8`](https://github.com/gradio-app/gradio/commit/e1874aff814d13b23f3e59ef239cc13e18ad3fa7) - Add `gr.on` listener method. Thanks [@aliabid94](https://github.com/aliabid94)! - [#5652](https://github.com/gradio-app/gradio/pull/5652) [`2e25d4305`](https://github.com/gradio-app/gradio/commit/2e25d430582264945ae3316acd04c4453a25ce38) - Pause autoscrolling if a user scrolls up in a `gr.Textbox` and resume autoscrolling if they go all the way down."", ""gradio\\_test\n## 0.3.3\n### Patch Changes\n- Updated dependencies [[`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d), [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:\n- @gradio/statustracker@0.4.3\n- @gradio/atoms@0.4.1\n## 0.3.2\n### Patch Changes\n- Updated dependencies [[`4d1cbbc`](https://github.com/gradio-app/gradio/commit/4d1cbbcf30833ef1de2d2d2710c7492a379a9a00)]:\n- @gradio/atoms@0.4.0\n- @gradio/statustracker@0.4.2\n## 0.3.1\n### Patch Changes\n- Updated dependencies []:\n- @gradio/atoms@0.3.1\n- @gradio/statustracker@0.4.1\n## 0.3.0\n### Features\n- [#6532](https://github.com/gradio-app/gradio/pull/6532) [`96290d304`](https://github.com/gradio-app/gradio/commit/96290d304a61064b52c10a54b2feeb09ca007542) - tweak deps. Thanks [@pngwn](https://github.com/pngwn)! ## 0.2.3\n### Patch Changes\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8)]:\n- @gradio/atoms@0.3.0\n- @gradio/statustracker@0.4.0\n## 0.2.2\n### Patch Changes\n- Updated dependencies [[`f816136a0`](https://github.com/gradio-app/gradio/commit/f816136a039fa6011be9c4fb14f573e4050a681a)]:\n- @gradio/atoms@0.2.2\n- @gradio/statustracker@0.3.2\n## 0.2.1\n### Patch Changes\n- Updated dependencies [[`3cdeabc68`](https://github.com/gradio-app/gradio/commit/3cdeabc6843000310e1a9e1d17190ecbf3bbc780), [`fad92c29d`](https://github.com/gradio-app/gradio/commit/fad92c29dc1f5cd84341aae417c495b33e01245f)]:\n- @gradio/atoms@0.2.1\n- @gradio/statustracker@0.3.1\n## 0.2.0\n### Features\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - fix cc build."", ""### Bug Fixes:\n- Updated components with `info` attribute to update when `update()` is called on them. by [@jebarpg](https://github.com/jebarpg) in [PR 4715](https://github.com/gradio-app/gradio/pull/4715). - Ensure the `Image` components undo button works mode is `mask` or `color-sketch` by [@amyorz](https://github.com/AmyOrz) in [PR 4692](https://github.com/gradio-app/gradio/pull/4692)\n- Load the iframe resizer external asset asynchronously, by [@akx](https://github.com/akx) in [PR 4336](https://github.com/gradio-app/gradio/pull/4336)\n- Restored missing imports in `gr.components` by [@abidlabs](https://github.com/abidlabs) in [PR 4566](https://github.com/gradio-app/gradio/pull/4566)\n- Fix bug where `select` event was not triggered in `gr.Gallery` if `height` was set to be large with `allow_preview=False` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4551](https://github.com/gradio-app/gradio/pull/4551)\n- Fix bug where setting `visible=False` in `gr.Group` event did not work by [@abidlabs](https://github.com/abidlabs) in [PR 4567](https://github.com/gradio-app/gradio/pull/4567)\n- Fix `make_waveform` to work with paths that contain spaces [@akx](https://github.com/akx) in [PR 4570](https://github.com/gradio-app/gradio/pull/4570) & [PR 4578](https://github.com/gradio-app/gradio/pull/4578)\n- Send captured data in `stop_recording` event for `gr.Audio` and `gr.Video` components by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4554](https://github.com/gradio-app/gradio/pull/4554)\n- Fix bug in `gr.Gallery` where `height` and `object_fit` parameters where being ignored by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4576](https://github.com/gradio-app/gradio/pull/4576)\n- Fixes an HTML sanitization issue in DOMPurify where links in markdown were not opening in a new window by [@hannahblair] in [PR 4577](https://github.com/gradio-app/gradio/pull/4577)\n- Fixed Dropdown height rendering in Columns by [@aliabid94](https://github.com/aliabid94) in [PR 4584](https://github.com/gradio-app/gradio/pull/4584)\n- Fixed bug where `AnnotatedImage` css styling was causing the annotation masks to not be displayed correctly by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4628](https://github.com/gradio-app/gradio/pull/4628)\n- Ensure that Gradio does not silently fail when running on a port that is occupied by [@abidlabs](https://github.com/abidlabs) in [PR 4624](https://github.com/gradio-app/gradio/pull/4624). - Fix double upload bug that caused lag in file uploads by [@aliabid94](https://github.com/aliabid94) in [PR 4661](https://github.com/gradio-app/gradio/pull/4661)\n- `Progress` component now appears even when no `iterable` is specified in `tqdm` constructor by [@itrushkin](https://github.com/itrushkin) in [PR 4475](https://github.com/gradio-app/gradio/pull/4475)\n- Deprecation warnings now point at the user code using those deprecated features, instead of Gradio internals, by (https://github.com/akx) in [PR 4694](https://github.com/gradio-app/gradio/pull/4694)\n- Adapt column widths in gr.Examples based on content by [@pngwn](https://github.com/pngwn) & [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 4700](https://github.com/gradio-app/gradio/pull/4700)\n- The `plot` parameter deprecation warnings should now only be emitted for `Image` components by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4709](https://github.com/gradio-app/gradio/pull/4709)\n- Removed uncessessary `type` deprecation warning by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4709](https://github.com/gradio-app/gradio/pull/4709)\n- Ensure Audio autoplays works when `autoplay=True` and the video source is dynamically updated [@pngwn](https://github.com/pngwn) in [PR 4705](https://github.com/gradio-app/gradio/pull/4705)\n- When an error modal is shown in spaces, ensure we scroll to the top so it can be seen by [@pngwn](https://github.com/pngwn) in [PR 4712](https://github.com/gradio-app/gradio/pull/4712)\n- Update depedencies by [@pngwn](https://github.com/pngwn) in [PR 4675](https://github.com/gradio-app/gradio/pull/4675)\n- Fixes `gr.Dropdown` being cutoff at the bottom by [@abidlabs](https://github.com/abidlabs) in [PR 4691](https://github.com/gradio-app/gradio/pull/4691). - Scroll top when clicking \""View API\"" in spaces by [@pngwn](https://github.com/pngwn) in [PR 4714](https://github.com/gradio-app/gradio/pull/4714)\n- Fix bug where `show_label` was hiding the entire component for `gr.Label` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4713](https://github.com/gradio-app/gradio/pull/4713)\n- Don't crash when uploaded image has broken EXIF data, by [@akx](https://github.com/akx) in [PR 4764](https://github.com/gradio-app/gradio/pull/4764)\n- Place toast messages at the top of the screen by [@pngwn](https://github.com/pngwn) in [PR 4796](https://github.com/gradio-app/gradio/pull/4796)\n- Fix regressed styling of Login page when auth is enabled by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4797](https://github.com/gradio-app/gradio/pull/4797)\n- Prevent broken scrolling to output on Spaces by [@aliabid94](https://github.com/aliabid94) in [PR 4822](https://github.com/gradio-app/gradio/pull/4822)\n### Other Changes:\n- Add `.git-blame-ignore-revs` by [@akx](https://github.com/akx) in [PR 4586](https://github.com/gradio-app/gradio/pull/4586)\n- Update frontend dependencies in [PR 4601](https://github.com/gradio-app/gradio/pull/4601)\n- Use `typing.Literal` where possible in gradio library and client by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4608](https://github.com/gradio-app/gradio/pull/4608)\n- Remove unnecessary mock json files for frontend E2E tests by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 4625](https://github.com/gradio-app/gradio/pull/4625)\n- Update dependencies by [@pngwn](https://github.com/pngwn) in [PR 4643](https://github.com/gradio-app/gradio/pull/4643)\n- The theme builder now launches successfully, and the API docs are cleaned up."", ""Tests in this category could be browser-based end-to-end tests, accessibility tests, or performance tests. They are sometimes called acceptance tests. ## Testing tools\nWe currently use the following tools:\n### Static quality checks\n- Python type-checking (python)\n- Black linting (python)\n- ruff formatting (python)\n- prettier formatting (javascript/svelte)\n- TypeScript type-checking (javascript/svelte)\n- eslint linting (javascript/svelte) [in progress]\n### Dynamic code tests\n- pytest (python unit and integration tests)\n- vitest (node-based unit and integration tests)\n- playwright (browser-based unit and integration tests)\n### Functional/acceptance tests\n- playwright (full end to end testing)\n- chromatic (visual testing) [in progress]\n- Accessibility testing [to do]\n## Supported environments and versions\nAll operating systems refer to the current runner variants supported by GitHub actions. All unspecified version segments (`x`) refer to latest. | Software | Version(s) | Operating System(s) |\n| -------- | --------------------- | --------------------------------- |\n| Python | `3.8.x` | `ubuntu-latest`, `windows-latest` |\n| Node | `18.x.x` | `ubuntu-latest` |\n| Browser | `playwright-chrome-x` | `ubuntu-latest` |\n## Test execution\nTests need to be executed in a number of environments and at different stages of the development cycle in order to be useful. The requirements for tests are as follows:\n- \\*\\*Locally\\*\\*: it is important that developers can easily run most tests locally to ensure a passing suite before making a PR. There are some exceptions to this, certain tests may require access to secret values which we cannot make available to all possible contributors for practical security reasons. It is reasonable that it isn't possible to run these tests but they should be disabled by default when running locally. - \\*\\*CI\\*\\* - It is \\_critical\\_ that all tests run successfully in CI with no exceptions. Not every test is required to pass to satisfy CI checks for practical reasons but it is required that all tests should run in CI and notify us if something unexpected happens in order for the development team to take appropriate action. For instructions on how to write and run tests see the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Managing defects\nAs we formalise our testing strategy and bring / keep our test up to standard, it is important that we have some principles on managing defects as they occur/ are reported. For now we can have one very simple rule:\n- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should \\_typically\\_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule.""]",Every bug fix in Gradio's testing strategy should typically be accompanied by a dynamic code test that failed before the fix and passes afterwards.,Dynamic code test
"How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?
","[""Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of 🤗 Accelerate is:\n- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible. - to have the same functions work for any distributed setup, so only have to learn one API. ### How does it work? To see how the library works in practice, let's have a look at each line of code we need to add to a training loop. ```python\naccelerator = Accelerator()\n```\nOn top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script. ```python\nmodel, optim, data = accelerator.prepare(model, optim, data)\n```\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torch.nn.Module`), optimizers (`torch.optim.Optimizer`) and dataloaders (`torch.data.dataloader.DataLoader`). #### Model\nModel preparation include wrapping it in the proper container (for instance `DistributedDataParallel`) and putting it on the proper device. Like with a regular distributed training, you will need to unwrap your model for saving, or to access its specific methods, which can be done with `accelerator.unwrap\\_model(model)`. #### Optimizer\nThe optimizer is also wrapped in a special container that will perform the necessary operations in the step to make mixed precision work."", ""For details and\nbenchmarks, please, see [TensorFloat-32(TF32) on Ampere devices](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices). The document includes\ninstructions on how to disable this automatic conversion if for some reason you prefer not to use it. With the 🤗 Trainer you can use `--tf32` to enable it, or disable it with `--tf32 0` or `--no\\_tf32`. By default the PyTorch default is used. ### Automatic Mixed Precision\nYou can use automatic mixed precision with either a pytorch-like AMP way or the apex-like way:\n### fp16\nTo configure pytorch AMP-like mode with fp16 (float16) set:\n```json\n{\n\""fp16\"": {\n\""enabled\"": \""auto\"",\n\""loss\\_scale\"": 0,\n\""loss\\_scale\\_window\"": 1000,\n\""initial\\_scale\\_power\"": 16,\n\""hysteresis\"": 2,\n\""min\\_loss\\_scale\"": 1\n}\n}\n```\nand the [`Trainer`] will automatically enable or disable it based on the value of\n`args.fp16\\_backend`. The rest of config values are up to you. This mode gets enabled when `--fp16 --fp16\\_backend amp` or `--fp16\\_full\\_eval` command line args are passed. You can also enable/disable this mode explicitly:\n```json\n{\n\""fp16\"": {\n\""enabled\"": true,\n\""loss\\_scale\"": 0,\n\""loss\\_scale\\_window\"": 1000,\n\""initial\\_scale\\_power\"": 16,\n\""hysteresis\"": 2,\n\""min\\_loss\\_scale\"": 1\n}\n}\n```\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\nconfiguration. Here is the [documentation](https://www.deepspeed.ai/docs/config-json/#fp16-training-options). ### bf16\nIf bf16 (bfloat16) is desired instead of fp16 then the following configuration section is to be used:\n```json\n{\n\""bf16\"": {\n\""enabled\"": \""auto\""\n}\n}\n```\nbf16 has the same dynamic range as fp32 and thus doesn't require loss scaling. This mode gets enabled when `--bf16` or `--bf16\\_full\\_eval` command line args are passed. You can also enable/disable this mode explicitly:\n```json\n{\n\""bf16\"": {\n\""enabled\"": true\n}\n}\n```\nAs of `deepspeed==0.6.0` the bf16 support is new and experimental. If you use [gradient accumulation](#gradient-accumulation) with bf16-enabled, you need to be aware that it'll accumulate gradients in bf16, which may not be what you want due to this format's low precision, as it may lead to a lossy accumulation. A work is being done to fix that and provide an option to use a higher precision `dtype` (fp16 or fp32). ### NCCL Collectives\nThere is the `dtype` of the training regime and there is a separate `dtype` that is used for communication collectives like various reduction and gathering/scattering operations. All gather/scatter ops are performed in the same `dtype` the data is in, so if you're using bf16 training regime it gets gathered in bf16 - gathering is a non-lossy operation."", ""It'll automatically configure your training setup based on your hardware and environment. Take a look at the 🤗 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more. Initialize an 🤗 Accelerate environment:\n```bash\naccelerate config\n```\nTo setup a default 🤗 Accelerate environment without choosing any configurations:\n```bash\naccelerate config default\n```\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n```bash\nfrom accelerate.utils import write\\_basic\\_config\nwrite\\_basic\\_config()\n```\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create\\_dataset) guide to learn how to create a dataset that works with the training script. The following sections highlight parts of the training script that are important for understanding how to modify it, but it doesn't cover every aspect of the script in detail. If you're interested in learning more, feel free to read through the [script](https://github.com/huggingface/diffusers/blob/main/examples/controlnet/train\\_controlnet.py) and let us know if you have any questions or concerns. ## Script parameters\nThe training script provides many parameters to help you customize your training run. All of the parameters and their descriptions are found in the [`parse\\_args()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/controlnet/train\\_controlnet.py#L231) function. This function provides default values for each parameter, such as the training batch size and learning rate, but you can also set your own values in the training command if you'd like. For example, to speedup training with mixed precision using the fp16 format, add the `--mixed\\_precision` parameter to the training command:\n```bash\naccelerate launch train\\_controlnet.py \\\n--mixed\\_precision=\""fp16\""\n```\nMany of the basic and important parameters are described in the [Text-to-image](text2image#script-parameters) training guide, so this guide just focuses on the relevant parameters for ControlNet:\n- `--max\\_train\\_samples`: the number of training samples; this can be lowered for faster training, but if you want to stream really large datasets, you'll need to include this parameter and the `--streaming` parameter in your training command\n- `--gradient\\_accumulation\\_steps`: number of update steps to accumulate before the backward pass; this allows you to train with a bigger batch size than your GPU memory can typically handle\n### Min-SNR weighting\nThe [Min-SNR](https://huggingface.co/papers/2303.09556) weighting strategy can help with training by rebalancing the loss to achieve faster convergence."", ""# Initialize accelerator and tensorboard logging\n... accelerator = Accelerator(\n... mixed\\_precision=config.mixed\\_precision,\n... gradient\\_accumulation\\_steps=config.gradient\\_accumulation\\_steps,\n... log\\_with=\""tensorboard\"",\n... project\\_dir=os.path.join(config.output\\_dir, \""logs\""),\n... )\n... if accelerator.is\\_main\\_process:\n... if config.output\\_dir is not None:\n... os.makedirs(config.output\\_dir, exist\\_ok=True)\n... if config.push\\_to\\_hub:\n... repo\\_id = create\\_repo(\n... repo\\_id=config.hub\\_model\\_id or Path(config.output\\_dir).name, exist\\_ok=True\n... ).repo\\_id\n... accelerator.init\\_trackers(\""train\\_example\"")\n... # Prepare everything\n... # There is no specific order to remember, you just need to unpack the\n... # objects in the same order you gave them to the prepare method. ... model, optimizer, train\\_dataloader, lr\\_scheduler = accelerator.prepare(\n... model, optimizer, train\\_dataloader, lr\\_scheduler\n... )\n... global\\_step = 0\n... # Now you train the model\n... for epoch in range(config.num\\_epochs):\n... progress\\_bar = tqdm(total=len(train\\_dataloader), disable=not accelerator.is\\_local\\_main\\_process)\n... progress\\_bar.set\\_description(f\""Epoch {epoch}\"")\n...""]","You can force mixed precision training when initializing the Accelerator in 🤗 Accelerate by passing the `fp16=True` argument to the `Accelerator()` initialization. Here is an example:

```python
accelerator = Accelerator(fp16=True)
```

This will enable mixed precision training during the setup.",By passing `fp16=True` to the Accelerator init.
"What is the purpose of tokenizers in the NLP pipeline?
","[""p align=\""center\"">\n\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\n\n![Build](https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg)\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\n[![Doc](https://docs.rs/tokenizers/badge.svg)](https://docs.rs/tokenizers/)\n\nThe core of `tokenizers`, written in Rust. Provides an implementation of today's most used tokenizers, with a focus on performance and\nversatility. ## What is a Tokenizer\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`. The various steps of the pipeline are:\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\nthe [unicode normalization standards](https://unicode.org/reports/tr15/#Norm\\_Forms), such as `NFD` or `NFKC`. More details about how to use the `Normalizers` are available on the\n[Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\nsplitting text is simply on whitespace. 3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\n`BPE` or `WordPiece`."", ""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a PyTorch tensor."", ""These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. Two common types of token classification are:\n\\* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names. \\* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb). ```py\n>>> from transformers import pipeline\n>>> classifier = pipeline(task=\""ner\"")\n>>> preds = classifier(\""Hugging Face is a French company based in New York City.\"")\n>>> preds = [\n... {\n... \""entity\"": pred[\""entity\""],\n... \""score\"": round(pred[\""score\""], 4),\n... \""index\"": pred[\""index\""],\n... \""word\"": pred[\""word\""],\n... \""start\"": pred[\""start\""],\n... \""end\"": pred[\""end\""],\n..."", ""n these few videos, we'll take a look at the tokenizers. In Natural Language Processing, most of the data that we handle consists of raw text. However, machine learning models cannot read and understand text in its raw form they can only work with numbers. The tokenizer's objective will be to translate the text into numbers. There are several possible approaches to this conversion, and the objective is to find the most meaningful representation. We'll take a look at three distinct tokenization algorithms. We compare them one to one, so we recommend you look at the videos in the following order: Word-based, Character-based, and Subword-based."", ""he tokenization pipeline involves several steps that convert raw text into numbers. In this video, we will see what happens during the pre-tokenization step. The pre-tokenization operation is the operation performed after the normalization of the text and before the application of the tokenization algorithm. This step consists in applying rules that do not need to be learned to perform a first division of the text. Let's look at how several tokenizers pre\\_tokenize this example. The gpt 2 pretokenization divides the text on spaces and some punctuation - but the apostrophe is not a division criterion for example. We also notice that spaces have been replaced by a capital G with a dot above. Albert's pre-tokenization divides the text at the level of spaces, adds a space at the beginning of the sentence and replaces spaces with a special underscore. Finally, Bert's pre-tokenization divides the text at the level of punctuation and spaces. Unlike the previous tokenizers, spaces are not transformed and integrated to the tokens produced with this pre-tokenizer. Through these 3 examples, we could observe the two main types of operations brought by the pre-tokenization: some changes on the text and the division of the string into tokens that can be associated to words. Finally, the \""backend\\_tokenizer\"" of the fast tokenizers also allows to test the pre-tokenization operation very easily thanks to its \""pre\\_tokenize\\_str\"" method. We notice that the output of this operation is composed of both tokens and offsets which allow to link the token to its position in the text given in input of the method."", ""The algorithm simply picks the most\nlikely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their\nprobabilities. Those probabilities are defined by the loss the tokenizer is trained on. Assuming that the training data consists of\nthe words \\\\(x\\_{1}, \\dots, x\\_{N}\\\\) and that the set of all possible tokenizations for a word \\\\(x\\_{i}\\\\) is\ndefined as \\\\(S(x\\_{i})\\\\), then the overall loss is defined as\n$$\\mathcal{L} = -\\sum\\_{i=1}^{N} \\log \\left ( \\sum\\_{x \\in S(x\\_{i})} p(x) \\right )$$\n### SentencePiece\nAll tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to\nseparate words. However, not all languages use spaces to separate words. One possible solution is to use language\nspecific pre-tokenizers, \\*e.g.\\* [XLM](model\\_doc/xlm) uses a specific Chinese, Japanese, and Thai pre-tokenizer). To solve this problem more generally, [SentencePiece: A simple and language independent subword tokenizer and\ndetokenizer for Neural Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) treats the input\nas a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram\nalgorithm to construct the appropriate vocabulary. The [`XLNetTokenizer`] uses SentencePiece for example, which is also why in the example earlier the\n`\""▁\""` character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be\nconcatenated and `\""▁\""` is replaced by a space. All transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models\nusing SentencePiece are [ALBERT](model\\_doc/albert), [XLNet](model\\_doc/xlnet), [Marian](model\\_doc/marian), and [T5](model\\_doc/t5)."", ""FrameworkSwitchCourse {fw} />\n# Tokenizers[[tokenizers]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. In NLP tasks, the data that is generally processed is raw text. Here's an example of such text:\n```\nJim Henson was a puppeteer\n```\nHowever, models can only process numbers, so we need to find a way to convert the raw text to numbers. That's what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation — that is, the one that makes the most sense to the model — and, if possible, the smallest representation. Let's take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization. ## Word-based[[word-based]]\nThe first type of tokenizer that comes to mind is \\_word-based\\_. It's generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:\n\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg)\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg)\n\nThere are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python's `split()` function:\n```py\ntokenized\\_text = \""Jim Henson was a puppeteer\"".split()\nprint(tokenized\\_text)\n```\n```python out\n['Jim', 'Henson', 'was', 'a', 'puppeteer']\n```\nThere are also variations of word tokenizers that have extra rules for punctuation.""]","The purpose of tokenizers in the NLP pipeline is to translate text into data that can be processed by models. Since models can only process numbers, tokenizers convert raw text inputs into numerical data. This conversion is essential for enabling machine learning models to understand and work with the text. Tokenizers aim to find the most meaningful representation of the text, making it easier for the model to interpret and analyze the input.",To translate text into data that can be processed by the model.
"What is the purpose of the Safety Checker in the Diffusers library?
","[""### Swap components in a pipeline\nYou can customize the default components of any pipeline with another compatible component. Customization is important because:\n- Changing the scheduler is important for exploring the trade-off between generation speed and quality. - Different components of a model are typically trained independently and you can swap out a component with a better-performing one. - During finetuning, usually only some components - like the UNet or text encoder - are trained. To find out which schedulers are compatible for customization, you can use the `compatibles` method:\n```py\nfrom diffusers import DiffusionPipeline\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\nstable\\_diffusion = DiffusionPipeline.from\\_pretrained(repo\\_id, use\\_safetensors=True)\nstable\\_diffusion.scheduler.compatibles\n```\nLet's use the [`SchedulerMixin.from\\_pretrained`] method to replace the default [`PNDMScheduler`] with a more performant scheduler, [`EulerDiscreteScheduler`]. The `subfolder=\""scheduler\""` argument is required to load the scheduler configuration from the correct [subfolder](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main/scheduler) of the pipeline repository. Then you can pass the new [`EulerDiscreteScheduler`] instance to the `scheduler` argument in [`DiffusionPipeline`]:\n```python\nfrom diffusers import DiffusionPipeline, EulerDiscreteScheduler\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\nscheduler = EulerDiscreteScheduler.from\\_pretrained(repo\\_id, subfolder=\""scheduler\"")\nstable\\_diffusion = DiffusionPipeline.from\\_pretrained(repo\\_id, scheduler=scheduler, use\\_safetensors=True)\n```\n### Safety checker\nDiffusion models like Stable Diffusion can generate harmful content, which is why 🧨 Diffusers has a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\_diffusion/safety\\_checker.py) to check generated outputs against known hardcoded NSFW content. If you'd like to disable the safety checker for whatever reason, pass `None` to the `safety\\_checker` argument:\n```python\nfrom diffusers import DiffusionPipeline\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\nstable\\_diffusion = DiffusionPipeline.from\\_pretrained(repo\\_id, safety\\_checker=None, use\\_safetensors=True)\n\""\""\""\nYou have disabled the safety checker for  by passing `safety\\_checker=None`."", ""Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us. - [\\*\\*Community tab\\*\\*](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project. - \\*\\*Bias exploration and evaluation\\*\\*: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations. - \\*\\*Encouraging safety in deployment\\*\\*\n- [\\*\\*Safe Stable Diffusion\\*\\*](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable\\_diffusion/stable\\_diffusion\\_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105). - [\\*\\*Safety Checker\\*\\*](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\_diffusion/safety\\_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated.""]","The purpose of the Safety Checker in the Diffusers library is to check generated outputs against known hardcoded NSFW content to prevent the generation of harmful content. It helps ensure that the outputs from diffusion models like Stable Diffusion are safe and appropriate. If needed, users can disable the safety checker by passing `None` to the `safety_checker` argument when initializing the pipeline.",The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.
"What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?
","[""!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Interact with Discussions and Pull Requests\nThe `huggingface\\_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood. ## Retrieve Discussions and Pull Requests from the Hub\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(repo\\_id=\""bigscience/bloom\""):\n... print(f\""{discussion.num} - {discussion.title}, pr: {discussion.is\\_pull\\_request}\"")\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer\\_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]\n```\n`HfApi.get\\_repo\\_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(\n..."", ""Hugging Face Hub documentation\nThe Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. The Hub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine Learning. Are you ready to join the path towards open source Machine Learning? 🤗\n\nRepositories\n\n[Introduction](./repositories)\n[Getting Started](./repositories-getting-started)\n[Repository Settings](./repositories-settings)\n[Pull requests and Discussions](./repositories-pull-requests-discussions)\n[Notifications](./notifications)\n[Collections](./collections)\n[Webhooks](./webhooks)\n[Next Steps](./repositories-next-steps)\n[Licenses](./repositories-licenses)\n\nModels\n\n[Introduction](./models)\n[The Model Hub](./models-the-hub)\n[Model Cards](./model-cards)\n[Gated Models](./models-gated)\n[Uploading Models](./models-uploading)\n[Downloading Models](./models-downloading)\n[Libraries](./models-libraries)\n[Tasks](./models-tasks)\n[Widgets](./models-widgets)\n[Inference API](./models-inference)\n[Download Stats](./models-download-stats)\n\nDatasets\n\n[Introduction](./datasets)\n[Datasets Overview](./datasets-overview)\n[Dataset Cards](./datasets-cards)\n[Gated Datasets](./datasets-gated)\n[Uploading Datasets](./datasets-adding)\n[Downloading Datasets](./datasets-downloading)\n[Libraries](./datasets-libraries)\n[Dataset Viewer](./datasets-viewer)\n[Download Stats](./datasets-download-stats)\n[Data files Configuration](./datasets-data-files-configuration)\n\nSpaces\n\n[Introduction](./spaces)\n[Spaces Overview](./spaces-overview)\n[Gradio Spaces](./spaces-sdks-gradio)\n[Streamlit Spaces](./spaces-sdks-streamlit)\n[Static HTML Spaces](./spaces-sdks-static)\n[Docker Spaces](./spaces-sdks-docker)\n[Embed your Space](./spaces-embed)\n[Run with Docker](./spaces-run-with-docker)\n[Reference](./spaces-config-reference)\n[Changelog](./spaces-changelog)\n[Advanced Topics](./spaces-advanced)\n[Sign in with HF](./spaces-oauth)\n\nOther\n\n[Organizations](./organizations)\n[Enterprise Hub](./enterprise-hub)\n[Billing](./billing)\n[Security](./security)\n[Moderation](./moderation)\n[Paper Pages](./paper-pages)\n[Search](./search)\n[Digital Object Identifier (DOI)](./doi)\n[Hub API Endpoints](./api)\n[Sign in with HF](./oauth)\n[Contributor Code of Conduct](https://huggingface.co/code-of-conduct)\n[Content Guidelines](https://huggingface.co/content-guidelines)\n\n## What's the Hugging Face Hub?"", ""--\ntitle: Introducing Pull Requests and Discussions 🥳\nthumbnail: /blog/assets/76\\_community\\_update/thumbnail.png\n---\n# Introducing Pull Requests and Discussions 🥳\n![Pull requests and discussions on the hub](assets/76\\_community\\_update/community-update.png)\nWe are thrilled to announce the release of our latest collaborative features: pull requests and discussions on the Hugging Face Hub! Pull requests and discussions are available today under the [community tab](https://huggingface.co/gpt2/discussions) for all repository types: models, datasets, and Spaces. Any member of the community can create and participate in discussions and pull requests, facilitating collaborations not only within teams, but also with everyone else in the community! It's the biggest update ever done to the Hub, and we can't wait to see the community members start collaborating with it 🤩. The new \""Community\"" tab also aligns with proposals in ethical ML throughout the years. Feedback and iterations have a central place in the development of ethical machine learning software. We really believe having it in the community's toolset will unlock new kinds of positive patterns in ML, collaborations, and progress. Some example use cases for discussions and pull requests:\n- Propose suggestions in model cards to improve disclosures of ethical biases. - Let users flag concerning generations of a given Space demo. - Provide a venue through which model and dataset authors can have a direct discussion with community members. - Allow others to improve your repositories! For example, users might want to provide TensorFlow weights! ## Discussions\n![Discussions on the Hugging Face Hub](assets/76\\_community\\_update/new-discussion.png)\n[Discussions](https://huggingface.co/gpt2/discussions?type=discussion) allow community members ask and answer questions as well as share their ideas and suggestions directly with the repository owners and the community. Anyone can create and participate in discussions in the community tab of a repository. ## Pull requests\n![Pull requests on the Hugging Face Hub](assets/76\\_community\\_update/new-pr.png)\n[Pull requests](https://huggingface.co/gpt2/discussions?type=pull\\_request) allow community members open, comment, merge, or close pull requests directly from the website. The easiest way to open a pull request is to use the \""Collaborate\"" button in the \""Files and versions\"" tab. It will let you do single file contributions very easily. Under the hood, our Pull requests do not use forks and branches, but instead, custom \""branches\"" called `refs` that are stored directly on the source repo. This approach to avoids the need to create a forks for each new version of the model/dataset. ## How is this different from other git hosts\nAt a high level, we aim to build a simpler version of other git hosts' (like GitHub's) PRs and Issues:\n- no forks are involved: contributors push to a special `ref` branch directly on the source repo\n- no hard distinction between issues and PRs: they are essentially the same so we display them in the same lists\n- streamlined for ML (i.e."", ""Next Steps\nThese next sections highlight features and additional information that you may find useful to make the most out of the Git repositories on the Hugging Face Hub. ## How to programmatically manage repositories\nHugging Face supports accessing repos with Python via the [`huggingface\\_hub` library](https://huggingface.co/docs/huggingface\\_hub/index). The operations that we've explored, such as downloading repositories and uploading files, are available through the library, as well as other useful functions! If you prefer to use git directly, please read the sections below. ## Learning more about Git\nA good place to visit if you want to continue learning about Git is [this Git tutorial](https://learngitbranching.js.org/). For even more background on Git, you can take a look at [GitHub's Git Guides](https://github.com/git-guides). ## How to use branches\nTo effectively use Git repos collaboratively and to work on features without releasing premature code you can use \\*\\*branches\\*\\*. Branches allow you to separate your \""work in progress\"" code from your \""production-ready\"" code, with the additional benefit of letting multiple people work on a project without frequently conflicting with each others' contributions. You can use branches to isolate experiments in their own branch, and even [adopt team-wide practices for managing branches](https://ericmjl.github.io/essays-on-data-science/workflow/gitflow/). To learn about Git branching, you can try out the [Learn Git Branching interactive tutorial](https://learngitbranching.js.org/). ## Using tags\nGit allows you to \\*tag\\* commits so that you can easily note milestones in your project. As such, you can use tags to mark commits in your Hub repos! To learn about using tags, you can visit [this DevConnected post](https://devconnected.com/how-to-create-git-tags/). Beyond making it easy to identify important commits in your repo's history, using Git tags also allows you to do A/B testing, [clone a repository at a specific tag](https://www.techiedelight.com/clone-specific-tag-with-git/), and more! The `huggingface\\_hub` library also supports working with tags, such as [downloading files from a specific tagged commit](https://huggingface.co/docs/huggingface\\_hub/main/en/how-to-downstream#hfhuburl).""]",The `HfApi` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.,HfApi
"What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?
","[""--\ntitle: \""Introducing Skops\""\nthumbnail: /blog/assets/94\\_skops/introducing\\_skops.png\nauthors:\n- user: merve\n- user: adrin\n- user: BenjaminB\n---\n# Introducing Skops\n## Introducing Skops\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others. Let's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production. ```python\n# let's import the libraries first\nimport sklearn\nfrom sklearn.datasets import load\\_breast\\_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model\\_selection import train\\_test\\_split\n# Load the data and split\nX, y = load\\_breast\\_cancer(as\\_frame=True, return\\_X\\_y=True)\nX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(\nX, y, test\\_size=0.3, random\\_state=42\n)\n# Train the model\nmodel = DecisionTreeClassifier().fit(X\\_train, y\\_train)\n```\nYou can use any model filename and serialization method, like `pickle` or `joblib`.""]",The name of the new library introduced by Hugging Face for hosting scikit-learn models is Skops.,Skops
"What is the purpose of Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""However, we observed that fine-tuning the encoder produces better results. We experimented with this approach after seeing it used in other Dreambooth implementations, and the results are striking! Frozen text encoder\n![Frozen text encoder](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/f\\_froxen\\_encoder.jpg)\nFine-tuned text encoder\n![Fine-tuned text encoder](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/g\\_unfrozen\\_encoder.jpg)\nFine-tuning the text encoder produces the best results, especially with faces. It generates more realistic images, it's less prone to overfitting and it also achieves better prompt interpretability, being able to handle more complex prompts. ## Epilogue: Textual Inversion + Dreambooth\nWe also ran a final experiment where we combined [Textual Inversion](https://textual-inversion.github.io) with Dreambooth. Both techniques have a similar goal, but their approaches are different. In this experiment we first ran textual inversion for 2000 steps. From that model, we then ran Dreambooth for an additional 500 steps using a learning rate of `1e-6`. These are the results:\n![Textual Inversion + Dreambooth](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/h\\_textual\\_inversion\\_dreambooth.jpg)\nWe think the results are much better than doing plain Dreambooth but not as good as when we fine-tune the whole text encoder. It seems to copy the style of the training images a bit more, so it could be overfitting to them. We didn't explore this combination further, but it could be an interesting alternative to improve Dreambooth and still fit the process in a 16GB GPU. Feel free to explore and tell us about your results!""]","The purpose of Textual Inversion is to personalize text-to-image models, like stable diffusion, using just 3-5 examples of your own images. It allows users to adapt the model to generate images that reflect specific styles or concepts based on the provided examples. This method enhances the model's ability to interpret prompts and produce more relevant and realistic images.",Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.
"What is the recommended multiple of batch size for fp16 data type on an A100 GPU?
","[""For instance, for fp16 data type a multiple of 8 is recommended, unless\nit's an A100 GPU, in which case use multiples of 64. For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup. ## Gradient Accumulation\nThe \\*\\*gradient accumulation\\*\\* method aims to calculate gradients in smaller increments instead of computing them for the\nentire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward\nand backward passes through the model and accumulating the gradients during the process. Once a sufficient number of\ngradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\nbecomes possible to increase the \\*\\*effective batch size\\*\\* beyond the limitations imposed by the GPU's memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can\nslow down the training process. You can enable gradient accumulation by adding the `gradient\\_accumulation\\_steps` argument to [`TrainingArguments`]:\n```py\ntraining\\_args = TrainingArguments(per\\_device\\_train\\_batch\\_size=1, gradient\\_accumulation\\_steps=4, \\*\\*default\\_args)\n```\nIn the above example, your effective batch size becomes 4."", ""It was run on a single NVIDIA A100-SXM4-80GB GPU with a prompt length of 512. The model we used was `meta-llama/Llama-2-13b-hf`. with batch size = 1:\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None |None|None |None |26.0 |36.958 |27.058 |29152.98 |\n|gptq |False |4 |128 |exllama|36.2 |33.711 |29.663 |10484.34 |\n|bitsandbytes|None |4|None |None |37.64 |52.00 |19.23 |11018.36 |\nwith batch size = 16:\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None |None|None |None |26.0 |69.94 |228.76 |53986.51 |\n|gptq |False |4 |128 |exllama|36.2 |95.41 |167.68 |34777.04 |\n|bitsandbytes|None |4|None |None |37.64 |113.98 |140.38 |35532.37 |\nFrom the benchmark, we can see that bitsandbyes and GPTQ are equivalent, with GPTQ being slightly faster for large batch size."", ""The impact of `fp16` on latency is less marked with `batch\\_size = 1`, but here it is of enormous interest as it can reduce latency by almost half, and almost double throughput! # Concluding remarks\nThis blog post showcased a few simple optimization tricks bundled in the 🤗 ecosystem. Using anyone of these techniques, or a combination of all three, can greatly improve Bark inference speed and memory footprint. \\* You can use the large version of Bark without any performance degradation and a footprint of just 2GB instead of 5GB, 15% faster, \\*\\*using 🤗 Better Transformer and CPU offload\\*\\*. \\* Do you prefer high throughput? \\*\\*Batch by 8 with 🤗 Better Transformer and half-precision\\*\\*. \\* You can get the best of both worlds by using \\*\\*fp16, 🤗 Better Transformer and CPU offload\\*\\*!""]","For fp16 data type on an A100 GPU, it is recommended to use multiples of 64.",64
"How do you run a Gradio Blocks app in reload mode using a Python IDE?
","[""Developing Faster with Auto-Reloading\n\\*\\*Prerequisite\\*\\*: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control). This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks. ## Why Auto-Reloading? When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes. To make it faster and more convenient to write your code, we've made it easier to \""reload\"" your Gradio apps instantly when you are developing in a \\*\\*Python IDE\\*\\* (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \""magic command\"" that allows you to re-run cells faster if you use \\*\\*Jupyter Notebooks\\*\\* (or any similar environment like Colab). This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster. ## Python IDE Reload 🔥\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Markdown(\""# Greetings from Gradio!\"")\ninp = gr.Textbox(placeholder=\""What is your name?\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""Welcome, {x}!\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`."", ""Gradio Demo: dataframe\\_block-ui-test\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\ncount = gr.Slider(minimum=1, maximum=10, step=1, label=\""count\"")\ndata = gr.DataFrame(\nheaders=[\""A\"", \""B\""], col\\_count=(2, \""fixed\""), type=\""array\"", interactive=True\n)\nbtn = gr.Button(value=\""click\"")\nbtn.click(\nfn=lambda cnt: [[str(2 \\* i), str(2 \\* i + 1)] for i in range(int(cnt))],\ninputs=[count],\noutputs=[data],\n)\ndemo.launch()\n```"", ""By default, the Gradio use UTF-8 encoding for scripts. \\*\\*For reload mode\\*\\*, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n1. Configure encoding declaration of python script, for example: `# -\\*- coding: cp1252 -\\*-`\n2. Confirm that your code editor has identified that encoding format. 3. Run like this: `gradio run.py --encoding cp1252`\n🔥 If your application accepts command line arguments, you can pass them in as well. Here's an example:\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# Greetings {args.name}!\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nWhich you could run like this: `gradio run.py --name Gretel`\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code."", ""WARNING: The --reload flag should not be used in production on Windows. ```\n这里最重要的一行是 `正在观察 ...`。这里发生的情况是 Gradio 将观察 `run.py` 文件所在的目录，如果文件发生更改，它将自动为您重新运行文件。因此，您只需专注于编写代码，Gradio 演示将自动刷新 🥳\n⚠️ 警告：`gradio` 命令不会检测传递给 `launch()` 方法的参数，因为在重新加载模式下从未调用 `launch()` 方法。例如，设置 `launch()` 中的 `auth` 或 `show\\_error` 不会在应用程序中反映出来。\n当您使用重新加载模式时，请记住一件重要的事情：Gradio 专门查找名为 `demo` 的 Gradio Blocks/Interface 演示。如果您将演示命名为其他名称，您需要在代码中的第二个参数中传入演示的 FastAPI 应用程序的名称。对于 Gradio 演示，可以使用 `.app` 属性访问 FastAPI 应用程序。因此，如果您的 `run.py` 文件如下所示：\n```python\nimport gradio as gr\nwith gr.Blocks() as my\\_demo:\ngr.Markdown(\""# 来自Gradio的问候！\"")\ninp = gr.Textbox(placeholder=\""您叫什么名字？\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""欢迎，{x}！\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\nmy\\_demo.launch()\n```\n那么您可以这样启动它：`gradio run.py my\\_demo.app`。\nGradio默认使用UTF-8编码格式。对于\\*\\*重新加载模式\\*\\*，如果你的脚本使用的是除UTF-8以外的编码（如GBK）：\n1. 在Python脚本的编码声明处指定你想要的编码格式，如：`# -\\*- coding: gbk -\\*-`\n2. 确保你的代码编辑器识别到该格式。\n3. 执行：`gradio run.py --encoding gbk`\n🔥 如果您的应用程序接受命令行参数，您也可以传递它们。下面是一个例子：\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# 欢迎 {args.name}！\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\n您可以像这样运行它：`gradio run.py --name Gretel`\n作为一个小提示，只要更改了 `run.py` 源代码或 Gradio 源代码，自动重新加载就会发生。这意味着如果您决定[为 Gradio 做贡献](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md)，这将非常有用 ✅\n## Jupyter Notebook 魔法命令🔮\n如果您使用 Jupyter Notebooks（或 Colab Notebooks 等）进行开发，我们也为您提供了一个解决方案！\n我们开发了一个 \\*\\*magic command 魔法命令\\*\\*，可以为您创建和运行一个 Blocks 演示。要使用此功能，在笔记本顶部加载 gradio 扩展：\n`%load\\_ext gradio`\n然后，在您正在开发 Gradio 演示的单元格中，只需在顶部写入魔法命令\\*\\*`%%blocks`\\*\\*，然后像平常一样编写布局和组件：\n```py\n%%blocks\nimport gradio as gr\ngr.Markdown(\""# 来自Gradio的问候！\"")\ninp = gr.Textbox(placeholder=\""您叫什么名字？\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""欢迎，{x}！\"",\ninputs=inp,\noutputs=out)\n```\n请注意：\n- 您不需要放置样板代码 `with gr.Blocks() as demo:` 和 `demo.launch()` — Gradio 会自动为您完成！\n- 每次重新运行单元格时，Gradio 都将在相同的端口上重新启动您的应用程序，并使用相同的底层网络服务器。这意味着您将比正常重新运行单元格更快地看到变化。\n下面是在 Jupyter Notebook 中的示例：\n![](https://i.ibb.co/nrszFws/Blocks.gif)\n🪄这在 colab 笔记本中也适用！[这是一个 colab 笔记本](https://colab.research.google.com/drive/1jUlX1w7JqckRHVE-nbDyMPyZ7fYD8488?authuser=1#scrollTo=zxHYjbCTTz\\_5)，您可以在其中看到 Blocks 魔法效果。尝试进行一些更改并重新运行带有 Gradio 代码的单元格！\nNotebook Magic 现在是作者构建 Gradio 演示的首选方式。无论您如何编写 Python 代码，我们都希望这两种方法都能为您提供更好的 Gradio 开发体验。\n---\n## 下一步\n既然您已经了解了如何使用 Gradio 快速开发，请开始构建自己的应用程序吧！\n如果你正在寻找灵感，请尝试浏览其他人用 Gradio 构建的演示，[浏览 Hugging Face Spaces](http://hf.space/) 🤗"", ""This means you'll see your changes \\_much, much faster\\_ than if you were rerunning the cell normally. Here's what it looks like in a jupyter notebook:\n![](https://gradio-builds.s3.amazonaws.com/demo-files/jupyter\\_reload.gif)\n🪄 This works in colab notebooks too! [Here's a colab notebook](https://colab.research.google.com/drive/1zAuWoiTIb3O2oitbtVb2\\_ekv1K6ggtC1?usp=sharing) where you can see the Blocks magic in action. Try making some changes and re-running the cell with the Gradio code! The Notebook Magic is now the author's preferred way of building Gradio demos. Regardless of how you write Python code, we hope either of these methods will give you a much better development experience using Gradio. ---\n## Next Steps\nNow that you know how to develop quickly using Gradio, start building your own! If you are looking for inspiration, try exploring demos other people have built with Gradio, [browse public Hugging Face Spaces](http://hf.space/) 🤗"", ""Gradio Demo: blocks\\_js\\_load\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndef welcome(name):\nreturn f\""Welcome to Gradio, {name}!\""\njs = \""\""\""\nfunction createGradioAnimation() {\nvar container = document.createElement('div');\ncontainer.id = 'gradio-animation';\ncontainer.style.fontSize = '2em';\ncontainer.style.fontWeight = 'bold';\ncontainer.style.textAlign = 'center';\ncontainer.style.marginBottom = '20px';\nvar text = 'Welcome to Gradio!';\nfor (var i = 0; i < text.length; i++) {\n(function(i){\nsetTimeout(function(){\nvar letter = document.createElement('span');\nletter.style.opacity = '0';\nletter.style.transition = 'opacity 0.5s';\nletter.innerText = text[i];\ncontainer.appendChild(letter);\nsetTimeout(function() {\nletter.style.opacity = '1';\n}, 50);\n}, i \\* 250);\n})(i);\n}\nvar gradioContainer = document.querySelector('.gradio-container');\ngradioContainer.insertBefore(container, gradioContainer.firstChild);\nreturn 'Animation created';\n}\n\""\""\""\nwith gr.Blocks(js=js) as demo:\ninp = gr.Textbox(placeholder=\""What is your name?\"")\nout = gr.Textbox()\ninp.change(welcome, inp, out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Instead of doing this, you can run your code in \\*\\*reload mode\\*\\* by changing 1 word: `python` to `gradio`:\nIn the terminal, run `gradio run.py`. That's it! Now, you'll see that after you'll see something like this:\n```bash\nWatching: '/Users/freddy/sources/gradio/gradio', '/Users/freddy/sources/gradio/demo/'\nRunning on local URL: http://127.0.0.1:7860\n```\nThe important part here is the line that says `Watching...` What's happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically 🥳\n⚠️ Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show\\_error` in `launch()` will not be reflected in the app. There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n```python\nimport gradio as gr\nwith gr.Blocks() as my\\_demo:\ngr.Markdown(\""# Greetings from Gradio!\"")\ninp = gr.Textbox(placeholder=\""What is your name?\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""Welcome, {x}!\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\nmy\\_demo.launch()\n```\nThen you would launch it in reload mode like this: `gradio run.py my\\_demo`."", ""Gradio Demo: blocks\\_plug\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndef change\\_tab():\nreturn gr.Tabs(selected=2)\nidentity\\_demo, input\\_demo, output\\_demo = gr.Blocks(), gr.Blocks(), gr.Blocks()\nwith identity\\_demo:\ngr.Interface(lambda x: x, \""text\"", \""text\"")\nwith input\\_demo:\nt = gr.Textbox(label=\""Enter your text here\"")\nwith gr.Row():\nbtn = gr.Button(\""Submit\"")\nclr = gr.ClearButton(t)\nwith output\\_demo:\ngr.Textbox(\""This is a static output\"")\nwith gr.Blocks() as demo:\ngr.Markdown(\""Three demos in one!\"")\nwith gr.Tabs(selected=1) as tabs:\nwith gr.TabItem(\""Text Identity\"", id=0) as tab0:\ntab0.select(lambda: gr.Tabs(selected=0), None, tabs)\nidentity\\_demo.render()\nwith gr.TabItem(\""Text Input\"", id=1) as tab1:\ntab1.select(lambda: gr.Tabs(selected=1), None, tabs)\ninput\\_demo.render()\nwith gr.TabItem(\""Text Static\"", id=2) as tab2:\ntab2.select(lambda: gr.Tabs(selected=2), None, tabs)\noutput\\_demo.render()\nbtn = gr.Button(\""Change tab\"")\nbtn.click(inputs=None, outputs=tabs, fn=change\\_tab)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```""]","To run a Gradio Blocks app in reload mode using a Python IDE, you need to follow these steps:

1. Write your Gradio app code in a Python file (e.g., `run.py`). Here is an example structure:

   ```python
   import gradio as gr
   with gr.Blocks() as demo:
       gr.Markdown(""# Greetings from Gradio!"")
       inp = gr.Textbox(placeholder=""What is your name?"")
       out = gr.Textbox()
       inp.change(fn=lambda x: f""Welcome, {x}!"", inputs=inp, outputs=out)
   if __name__ == ""__main__"":
       demo.launch()
   ```

2. Instead of running your app with the command `python run.py`, you will run it in reload mode by using the command `gradio run.py` in your terminal.

3. When you run this command, you will see output indicating that Gradio is watching for changes in your code. For example, you might see something like:

   ```
   Watching: '/path/to/your/directory'
   Running on local URL: http://127.0.0.1:7860
   ```

4. Now, whenever you make changes to your `run.py` file, Gradio will automatically reload the app, allowing you to see your changes instantly without needing to restart the application manually.

Keep in mind that the `gradio` command does not detect parameters passed to the `launch()` method in reload mode, and Gradio specifically looks for a demo called `demo` in your code. If your demo is named differently, you will need to pass its name as a parameter when launching.",Run `gradio run.py` in the terminal.
"How can you install the Hugging Face Unity API in your Unity project?
","[""--\ntitle: \""How to Install and Use the Hugging Face Unity API\""\nthumbnail: /blog/assets/124\\_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n# How to Install and Use the Hugging Face Unity API\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API. ## Installation\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif)\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens). 7. Test the API key by clicking `Test API key` in the API Wizard. 8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API\\_URL` field.""]","To install the Hugging Face Unity API in your Unity project, follow these steps:

1. Open your Unity project.
2. Go to `Window` -> `Package Manager`.
3. Click `+` and select `Add Package from git URL`.
4. Enter `https://github.com/huggingface/unity-api.git`.
5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.
6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).
7. Test the API key by clicking `Test API key` in the API Wizard.
8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.","To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`."
"What is the pretraining objective of the Wav2Vec2 context network?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Wav2Vec2\n## Overview\nThe Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. The abstract from the paper is the following:\n\\*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\nrecognition with limited amounts of labeled data.\\*\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). ## Usage tips\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\nusing [`Wav2Vec2CTCTokenizer`]. ## Resources\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Wav2Vec2."", ""The results of this run can be seen [here](https://wandb.ai/patrickvonplaten/pretraining-wav2vec2/reports/Wav2Vec2-Large--VmlldzoxMTAwODM4?accessToken=wm3qzcnldrwsa31tkvf2pdmilw3f63d4twtffs86ou016xjbyilh55uoi3mo1qzc) and the checkpoint pretrained for 120,000 steps can be accessed [here](https://huggingface.co/patrickvonplaten/wav2vec2-large-repro-960h-libri-120k-steps)"", ""We are interested in the most likely prediction of\nthe model and thus take the `argmax(...)` of the logits. Also, we\ntransform the encoded labels back to the original string by replacing\n`-100` with the `pad\\_token\\_id` and decoding the ids while making sure\nthat consecutive tokens are \\*\\*not\\*\\* grouped to the same token in CTC\nstyle \\\\({}^1\\\\). ```python\ndef compute\\_metrics(pred):\npred\\_logits = pred.predictions\npred\\_ids = np.argmax(pred\\_logits, axis=-1)\npred.label\\_ids[pred.label\\_ids == -100] = processor.tokenizer.pad\\_token\\_id\npred\\_str = processor.batch\\_decode(pred\\_ids)\n# we do not want to group tokens when computing the metrics\nlabel\\_str = processor.batch\\_decode(pred.label\\_ids, group\\_tokens=False)\nwer = wer\\_metric.compute(predictions=pred\\_str, references=label\\_str)\nreturn {\""wer\"": wer}\n```\nNow, we can load the pretrained `Wav2Vec2` checkpoint. The tokenizer\\'s\n`pad\\_token\\_id` must be to define the model\\'s `pad\\_token\\_id` or in the\ncase of `Wav2Vec2ForCTC` also CTC\\'s \\*blank token\\* \\\\({}^2\\\\). To save GPU\nmemory, we enable PyTorch\\'s [gradient\ncheckpointing](https://pytorch.org/docs/stable/checkpoint.html) and also\nset the loss reduction to \\\""\\*mean\\*\\\"". ```python\nfrom transformers import Wav2Vec2ForCTC\nmodel = Wav2Vec2ForCTC.from\\_pretrained(\n\""facebook/wav2vec2-base\"",\nctc\\_loss\\_reduction=\""mean\"",\npad\\_token\\_id=processor.tokenizer.pad\\_token\\_id,\n)\n```\n\\*\\*Print Output:\\*\\*\n```bash\nSome weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm\\_head.weight', 'lm\\_head.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."", ""Using the Buckwalter format, text is also logged in Arabic abjad. `--target\\_feature\\_extractor\\_sampling\\_rate` resamples audio to target feature extractor's sampling rate (16kHz). `--max\\_duration\\_in\\_seconds=\""15\""` filters out examples whose audio is longer than the specified limit,\nwhich helps with capping GPU memory usage. ### DeepSpeed Integration\nTo learn how to deploy Deepspeed Integration please refer to [this guide](https://huggingface.co/transformers/main/main\\_classes/deepspeed.html#deepspeed-trainer-integration). But to get started quickly all you need is to install:\n```\npip install deepspeed\n```\nand then use the default configuration files in this directory:\n\\* `ds\\_config\\_wav2vec2\\_zero2.json`\n\\* `ds\\_config\\_wav2vec2\\_zero3.json`\nHere are examples of how you can use DeepSpeed:\n(edit the value for `--num\\_gpus` to match the number of GPUs you have)\nZeRO-2:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 2 \\\nrun\\_asr.py \\\n--output\\_dir=output\\_dir --num\\_train\\_epochs=2 --per\\_device\\_train\\_batch\\_size=2 \\\n--per\\_device\\_eval\\_batch\\_size=2 --evaluation\\_strategy=steps --save\\_steps=500 --eval\\_steps=100 \\\n--logging\\_steps=5 --learning\\_rate=5e-4 --warmup\\_steps=3000 \\\n--model\\_name\\_or\\_path=patrickvonplaten/wav2vec2\\_tiny\\_random\\_robust \\\n--dataset\\_name=hf-internal-testing/librispeech\\_asr\\_dummy --dataset\\_config\\_name=clean \\\n--train\\_split\\_name=validation --validation\\_split\\_name=validation --orthography=timit \\\n--preprocessing\\_num\\_workers=1 --group\\_by\\_length --freeze\\_feature\\_extractor --verbose\\_logging \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero2.json\n```\nFor ZeRO-2 with more than 1 gpu you need to use (which is already in the example configuration file):\n```\n\""zero\\_optimization\"": {\n... \""find\\_unused\\_parameters\"": true,\n... }\n```\nZeRO-3:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 2 \\\nrun\\_asr.py \\\n--output\\_dir=output\\_dir --num\\_train\\_epochs=2 --per\\_device\\_train\\_batch\\_size=2 \\\n--per\\_device\\_eval\\_batch\\_size=2 --evaluation\\_strategy=steps --save\\_steps=500 --eval\\_steps=100 \\\n--logging\\_steps=5 --learning\\_rate=5e-4 --warmup\\_steps=3000 \\\n--model\\_name\\_or\\_path=patrickvonplaten/wav2vec2\\_tiny\\_random\\_robust \\\n--dataset\\_name=hf-internal-testing/librispeech\\_asr\\_dummy --dataset\\_config\\_name=clean \\\n--train\\_split\\_name=validation --validation\\_split\\_name=validation --orthography=timit \\\n--preprocessing\\_num\\_workers=1 --group\\_by\\_length --freeze\\_feature\\_extractor --verbose\\_logging \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero3.json\n```\n### Pretraining Wav2Vec2\nThe `run\\_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss objective (see official [paper](https://arxiv.org/abs/2006.11477) for more information). It is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed (please refer to [this guide](https://huggingface.co/transformers/main/main\\_classes/deepspeed.html#deepspeed-trainer-integration) for more information). Here is an example of how you can use DeepSpeed ZeRO-2 to pretrain a small Wav2Vec2 model:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 4 run\\_pretrain.py \\\n--output\\_dir=\""./wav2vec2-base-libri-100h\"" \\\n--num\\_train\\_epochs=\""3\"" \\\n--per\\_device\\_train\\_batch\\_size=\""32\"" \\\n--per\\_device\\_eval\\_batch\\_size=\""32\"" \\\n--gradient\\_accumulation\\_steps=\""2\"" \\\n--save\\_total\\_limit=\""3\"" \\\n--save\\_steps=\""500\"" \\\n--logging\\_steps=\""10\"" \\\n--learning\\_rate=\""5e-4\"" \\\n--weight\\_decay=\""0.01\"" \\\n--warmup\\_steps=\""3000\"" \\\n--model\\_name\\_or\\_path=\""patrickvonplaten/wav2vec2-base-libri-100h\"" \\\n--dataset\\_name=\""librispeech\\_asr\"" \\\n--dataset\\_config\\_name=\""clean\"" \\\n--train\\_split\\_name=\""train.100\"" \\\n--preprocessing\\_num\\_workers=\""4\"" \\\n--max\\_duration\\_in\\_seconds=\""10.0\"" \\\n--group\\_by\\_length \\\n--verbose\\_logging \\\n--fp16 \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero2.json \\\n```\n### Forced Alignment\nCharacter level forced alignment for audio and text pairs with wav2vec2 models finetuned on ASR task for a specific language. Inspired by [this](https://pytorch.org/tutorials/intermediate/forced\\_alignment\\_with\\_torchaudio\\_tutorial.html) Pytorch tutorial. #### Input Formats\nInput format in script.txt Input format in wavs directroy\n0000 sentence1 0000.wav\n0001 sentence2 0001.wav\n#### Output Format\nOutput directory will contain 0000.txt and 0001.txt. Each file will have format like below\nchar score start\\_ms end\\_ms\nh 0.25 1440 1520\n#### Run command\n```\npython alignment.py \\\n--model\\_name=\""arijitx/wav2vec2-xls-r-300m-bengali\"" \\\n--wav\\_dir=\""./wavs\""\n--text\\_file=\""script.txt\"" \\\n--input\\_wavs\\_sr=48000 \\\n--output\\_dir=\""./out\\_alignment\"" \\\n--cuda\n```"", ""Ready to try your hand at audio classification? Check out our complete [audio classification guide](tasks/audio\\_classification) to learn how to finetune Wav2Vec2 and use it for inference! ### Automatic speech recognition\nTo use the pretrained model for automatic speech recognition, add a language modeling head on top of the base Wav2Vec2 model for [connectionist temporal classification (CTC)](glossary#connectionist-temporal-classification-ctc). The language modeling head is a linear layer that accepts the encoder's hidden states and transforms them into logits. Each logit represents a token class (the number of tokens comes from the task vocabulary). The CTC loss is calculated between the logits and targets to find the most likely sequence of tokens, which are then decoded into a transcription. Ready to try your hand at automatic speech recognition? Check out our complete [automatic speech recognition guide](tasks/asr) to learn how to finetune Wav2Vec2 and use it for inference! ## Computer vision\nThere are two ways to approach computer vision tasks:\n1. Split an image into a sequence of patches and process them in parallel with a Transformer. 2. Use a modern CNN, like [ConvNeXT](model\\_doc/convnext), which relies on convolutional layers but adopts modern network designs. A third approach mixes Transformers with convolutions (for example, [Convolutional Vision Transformer](model\\_doc/cvt) or [LeViT](model\\_doc/levit)). We won't discuss those because they just combine the two approaches we examine here. ViT and ConvNeXT are commonly used for image classification, but for other vision tasks like object detection, segmentation, and depth estimation, we'll look at DETR, Mask2Former and GLPN, respectively; these models are better suited for those tasks. ### Image classification\nViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an attention mechanism while ConvNeXT uses convolutions."", ""Wav2Vec2 Contrastive Loss PreTraining examples\nThe following example showcases how to pretrain a wav2vec2 model using the JAX/Flax backend. Pretraining Wav2Vec2 is rather complex, so it is highly recommended to read the\n[official paper](https://arxiv.org/abs/2006.11477). JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU. Models written in JAX/Flax are \\*\\*immutable\\*\\* and updated in a purely functional\nway which enables simple and efficient model parallelism. `run\\_wav2vec2\\_pretrain\\_flax.py` is a lightweight example of how to download and preprocess a dataset from the 🤗 Datasets library or use your own files (jsonlines or csv), then pretrain the wav2vec2 architectures above on it. For custom datasets in `jsonlines` format please see: [the Datasets documentation](https://huggingface.co/docs/datasets/loading\\_datasets#json-files) and you also will find examples of these below. Let's start by creating a model repository to save the trained model and logs. Here we call the model `\""wav2vec2-base-robust\""`, but you can change the model name as you like. You can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\nyou are logged in) or via the command line:\n```\nhuggingface-cli repo create wav2vec2-base-robust\n```\nNext we clone the model repository to add the tokenizer and model files. ```\ngit clone https://huggingface.co//wav2vec2-base-robust\n```\nTo ensure that all tensorboard traces will be uploaded correctly, we need to\ntrack them. You can run the following command inside your model repo to do so. ```\ncd wav2vec2-base-robust\ngit lfs track \""\\*tfevents\\*\""\n```\nGreat, we have set up our model repository. During training, we will automatically\npush the training logs and model weights to the repo. Next, let's add a symbolic link to the `run\\_wav2vec2\\_pretrain\\_flax`. ```bash\nexport MODEL\\_DIR=\""./wav2vec2-base-robust\""\nln -s ~/transformers/examples/research\\_projects/jax-projects/wav2vec2/run\\_wav2vec2\\_pretrain\\_flax.py ./\n```\n### Create the model configuration\nLet's first create the model configuration and store it in the model repository."", ""The\nsampling rate hereby plays an important role in that it defines how many\ndata points of the speech signal are measured per second. Therefore,\nsampling with a higher sampling rate results in a better approximation\nof the \\*real\\* speech signal but also necessitates more values per\nsecond. A pretrained checkpoint expects its input data to have been sampled more\nor less from the same distribution as the data it was trained on. The\nsame speech signals sampled at two different rates have a very different\ndistribution, \\*e.g.\\*, doubling the sampling rate results in data points\nbeing twice as long. Thus, before fine-tuning a pretrained checkpoint of\nan ASR model, it is crucial to verify that the sampling rate of the data\nthat was used to pretrain the model matches the sampling rate of the\ndataset used to fine-tune the model. Wav2Vec2 was pretrained on the audio data of\n[LibriSpeech](https://huggingface.co/datasets/librispeech\\_asr) and\nLibriVox which both were sampling with 16kHz. Our fine-tuning dataset,\n[Timit](hhtps://huggingface.co/datasets/timit\\_asr), was luckily also\nsampled with 16kHz. If the fine-tuning dataset would have been sampled\nwith a rate lower or higher than 16kHz, we first would have had to up or\ndownsample the speech signal to match the sampling rate of the data used\nfor pretraining. A Wav2Vec2 feature extractor object requires the following parameters to\nbe instantiated:\n- `feature\\_size`: Speech models take a sequence of feature vectors as\nan input. While the length of this sequence obviously varies, the\nfeature size should not. In the case of Wav2Vec2, the feature size\nis 1 because the model was trained on the raw speech signal \\\\({}^2\\\\) . - `sampling\\_rate`: The sampling rate at which the model is trained on. - `padding\\_value`: For batched inference, shorter inputs need to be\npadded with a specific value\n- `do\\_normalize`: Whether the input should be\n\\*zero-mean-unit-variance\\* normalized or not. Usually, speech models\nperform better when normalizing the input\n- `return\\_attention\\_mask`: Whether the model should make use of an\n`attention\\_mask` for batched inference. In general, models should\n\\*\\*always\\*\\* make use of the `attention\\_mask` to mask padded tokens. However, due to a very specific design choice of `Wav2Vec2`\\'s\n\\\""base\\\"" checkpoint, better results are achieved when using no\n`attention\\_mask`.""]","The pretraining objective of the Wav2Vec2 context network is to learn powerful representations from speech audio alone by masking the speech input in the latent space and solving a contrastive task defined over a quantization of the latent representations, which are jointly learned. This approach allows the model to outperform the best semi-supervised methods while being conceptually simpler.",The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.
"What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?
","[""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a TensorFlow tensor."", ""Мы хотим, чтобы Transformers позволил разработчикам, исследователям, студентам, профессорам, инженерам и всем желающим\nсоздавать проекты своей мечты. Чтобы отпраздновать 100 тысяч звезд Transformers, мы решили сделать акцент на сообществе, и создали страницу [awesome-transformers](./awesome-transformers.md), на которой перечислены 100\nневероятных проектов, созданных с помощью transformers. Если вы являетесь владельцем или пользователем проекта, который, по вашему мнению, должен быть включен в этот список, пожалуйста, откройте PR для его добавления! ## Если вы хотите получить индивидуальную поддержку от команды Hugging Face\n[![HuggingFace Expert Acceleration Program](https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png)](https://huggingface.co/support)  \n## Быстрый гайд\nДля использования модели на заданном входе (текст, изображение, звук, ...) мы предоставляем API `pipeline`. Конвейеры объединяют предварительно обученную модель с препроцессингом, который использовался при ее обучении. Вот как можно быстро использовать конвейер для классификации положительных и отрицательных текстов:\n```python\n>>> from transformers import pipeline\n# Выделение конвейера для анализа настроений\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('Мы очень рады представить конвейер в transformers.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```\nВторая строка кода загружает и кэширует предварительно обученную модель, используемую конвейером, а третья оценивает ее на заданном тексте."", ""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_logo_name.png)\n\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\n\n#### [English](https://github.com/huggingface/transformers/) | [简体中文](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [繁體中文](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | **한국어** | [Español](https://github.com/huggingface/transformers/blob/main/README_es.md) | [日本語](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [हिन्दी](https://github.com/huggingface/transformers/blob/main/README_hd.md) [తెలుగు](https://github.com/huggingface/transformers//blob/main/README_te.md) |\n\n### Jax, Pytorch, TensorFlow를 위한 최첨단 자연어처리\n\n### \n\n🤗 Transformers는 분류, 정보 추출, 질문 답변, 요약, 번역, 문장 생성 등을 100개 이상의 언어로 수행할 수 있는 수천개의 사전학습된 모델을 제공합니다."", ""For instance, if the review was negative, I could create a feature which would contact the customer, apologize for the poor quality of service, and inform him/her that our support team will contact him/her as soon as possible to assist him and hopefully fix the problem. Since I don't plan to get more than 2,000 requests per month, I didn't impose any performance constraints regarding the time and the scalability. ## The Transformers library\nI was a bit confused at the beginning when I downloaded the .h5 file. I thought it would be compatible with `tensorflow.keras.models.load\\_model`, but this wasn't the case. After a few minutes of research I was able to figure out that the file was a weights checkpoint rather than a Keras model. After that, I tried out the API that Hugging Face offers and read a bit more about the pipeline feature they offer. Since the results of the API & the pipeline were great, I decided that I could serve the model through the pipeline on my own server. Below is the [official example](https://github.com/huggingface/transformers#quick-tour) from the Transformers GitHub page. ```python\nfrom transformers import pipeline\n# Allocate a pipeline for sentiment-analysis\nclassifier = pipeline('sentiment-analysis')\nclassifier('We are very happy to include pipeline into the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9978193640708923}]\n```\n## Deploy transformers to Google Cloud\n> GCP is chosen as it is the cloud environment I am using in my personal organization.""]",The default checkpoint used by the sentiment analysis pipeline in the Transformers library is `distilbert base uncased finetuned sst2 english`.,distilbert base uncased finetuned sst2 english
"What is the purpose of the notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi""?
","[""Note that while [CUDA graphs](https://developer.nvidia.com/blog/cuda-graphs/) are not currently compatible with model parallelism in DeepSpeed (DeepSpeed v0.8.2, see [here](https://github.com/microsoft/DeepSpeed/blob/v0.8.2/deepspeed/inference/engine.py#L158)), HPU graphs are supported in Habana's DeepSpeed fork. All benchmarks are doing [greedy generation](https://huggingface.co/blog/how-to-generate#greedy-search) of 100 token outputs. The input prompt is:\n> \""DeepSpeed is a machine learning framework\""\nwhich consists of 7 tokens with BLOOM's tokenizer. The results for inference latency are displayed in the table below (the unit is \\*seconds\\*). | Model | Number of devices | Gaudi2 latency (seconds) | A100-80GB latency (seconds) | First-gen Gaudi latency (seconds) |\n|:-----------:|:-----------------:|:-------------------------:|:-----------------:|:----------------------------------:|\n| BLOOMZ | 8 | 3.103 | 4.402 | / |\n| BLOOMZ-7B | 8 | 0.734 | 2.417 | 3.321 |\n| BLOOMZ-7B | 1 | 0.772 | 2.119 | 2.387 |\n\\*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, leading to a\\* x\\*1.42 speedup on BLOOMZ with Gaudi2 compared to A100.\\*\nThe Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. \\*\\*For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB\\*\\*. Smaller checkpoints present interesting results too. \\*\\*Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!\\*\\* It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device. We also ran these models on first-gen Gaudi. While it is slower than Gaudi2, it is interesting from a price perspective as a DL1 instance on AWS costs approximately 13\\$ per hour. Latency for BLOOMZ-7B on first-gen Gaudi is 2.387 seconds. Thus, \\*\\*first-gen Gaudi offers for the 7-billion checkpoint a better price-performance ratio than A100\\*\\* which costs more than 30\\$ per hour! We expect the Habana team will optimize the performance of these models in the upcoming SynapseAI releases. For example, in our last benchmark, we saw that [Gaudi2 performs Stable Diffusion inference 2.2x faster than A100](https://huggingface.co/blog/habana-gaudi-2-benchmark#generating-images-from-text-with-stable-diffusion) and this has since been improved further to 2.37x with the latest optimizations provided by Habana. We will update these numbers as new versions of SynapseAI are released and integrated within Optimum Habana."", ""--\ntitle: Getting Started with Transformers on Habana Gaudi\nthumbnail: /blog/assets/61\\_getting\\_started\\_habana/habana01.png\nauthors:\n- user: juliensimon\n---\n# Getting Started with Transformers on Habana Gaudi\nA couple of weeks ago, we've had the pleasure to [announce](https://huggingface.co/blog/habana) that [Habana Labs](https://habana.ai) and [Hugging Face](https://huggingface.co/) would partner to accelerate Transformer model training. Habana Gaudi accelerators deliver up to 40% better price performance for training machine learning models compared to the latest GPU-based Amazon EC2 instances. We are super excited to bring this price performance advantages to Transformers 🚀\nIn this hands-on post, I'll show you how to quickly set up a Habana Gaudi instance on Amazon Web Services, and then fine-tune a BERT model for text classification. As usual, all code is provided so that you may reuse it in your projects. Let's get started! ## Setting up an Habana Gaudi instance on AWS\nThe simplest way to work with Habana Gaudi accelerators is to launch an Amazon EC2 [DL1](https://aws.amazon.com/ec2/instance-types/dl1/) instance. These instances are equipped with 8 Habana Gaudi processors that can easily be put to work thanks to the [Habana Deep Learning Amazon Machine Image](https://aws.amazon.com/marketplace/server/procurement?productId=9a75c51a-a4d1-4470-884f-6be27933fcc8) (AMI)."", ""--\ntitle: \""Accelerate Large Model Training using DeepSpeed\""\nthumbnail: /blog/assets/83\\_accelerate\\_deepspeed/deepspeed-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n# Accelerate Large Model Training using DeepSpeed\nIn this post we will look at how we can leverage the \\*\\*[Accelerate](https://github.com/huggingface/accelerate)\\*\\* library for training large models which enables users to leverage the ZeRO features of \\*\\*[DeeSpeed](https://www.deepspeed.ai)\\*\\*. # Motivation 🤗\n\\*\\*Tired of Out of Memory (OOM) errors while trying to train large models? We've got you covered. Large models are very performant [1] but difficult to train with the available hardware. To get the most of the available hardware for training large models one can leverage Data Parallelism using ZeRO - Zero Redundancy Optimizer [2]\\*\\*. Below is a short description of Data Parallelism using ZeRO with diagram from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)\n![ZeRO Data Parallelism](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero.png)\n(Source: [link](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/))\na."", ""This final run is thus x1.41 faster than our base run on Gaudi2 \\*\\*simply adding 2 ready-to-use training arguments.\\*\\* It is also \\*\\*x1.41 faster than H100\\*\\* and \\*\\*x2.45 faster than A100\\*\\* with `dataloader\\_num\\_workers=2`! ### Reproducing this benchmark\nTo reproduce this benchmark, you first need to get access to Gaudi2 through the [Intel Developer Cloud](https://www.intel.com/content/www/us/en/secure/developer/devcloud/cloud-launchpad.html) (see [this guide](https://huggingface.co/blog/habana-gaudi-2-benchmark#how-to-get-access-to-gaudi2) for more information). Then, you need to install the latest version of Optimum Habana and run `run\\_bridgetower.py` which you can find [here](https://github.com/huggingface/optimum-habana/blob/main/examples/contrastive-image-text/run\\_bridgetower.py). Here is how to do it:\n```bash\npip install optimum[habana]\ngit clone https://github.com/huggingface/optimum-habana.git\ncd optimum-habana/examples/contrastive-image-text\npip install -r requirements.txt\n```\nThe base command line to run the script is:\n```bash\npython ../gaudi\\_spawn.py --use\\_mpi --world\\_size 8 run\\_bridgetower.py \\\n--output\\_dir /tmp/bridgetower-test \\\n--model\\_name\\_or\\_path BridgeTower/bridgetower-large-itm-mlm-itc \\\n--dataset\\_name jmhessel/newyorker\\_caption\\_contest --dataset\\_config\\_name matching \\\n--dataset\\_revision 3c6c4f6c0ff7e902833d3afa5f8f3875c2b036e6 \\\n--image\\_column image --caption\\_column image\\_description \\\n--remove\\_unused\\_columns=False \\\n--do\\_train --do\\_eval --do\\_predict \\\n--per\\_device\\_train\\_batch\\_size=\""40\"" --per\\_device\\_eval\\_batch\\_size=\""16\"" \\\n--num\\_train\\_epochs 5 \\\n--learning\\_rate=\""1e-5\"" \\\n--push\\_to\\_hub --report\\_to tensorboard --hub\\_model\\_id bridgetower\\\n--overwrite\\_output\\_dir \\\n--use\\_habana --use\\_lazy\\_mode --use\\_hpu\\_graphs\\_for\\_inference --gaudi\\_config\\_name Habana/clip \\\n--throughput\\_warmup\\_steps 3 \\\n--logging\\_steps 10\n```\nwhich corresponds to the case `--dataloader\\_num\\_workers 0`. You can then add `--dataloader\\_num\\_workers N` and `--mediapipe\\_dataloader` to test other configurations. To push your model and Tensorboard logs to the Hugging Face Hub, you will have to log in to your account beforehand with:\n```bash\nhuggingface-cli login\n```\nFor A100 and H100, you can use the same `run\\_bridgetower.py` script with a few small changes:\n- Replace `GaudiTrainer` and `GaudiTrainingArguments` with `Trainer` and `TrainingArguments` from Transformers\n- Remove references to `GaudiConfig`, `gaudi\\_config` and `HabanaDataloaderTrainer`\n- Import `set\\_seed` directly from Transformers: `from transformers import set\\_seed`\nThe results displayed in this benchmark were obtained with a Nvidia H100 Lambda instance and a Nvidia A100 80GB GCP instance both with 8 devices using [Nvidia's Docker images](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/index.html).""]","The notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi"" is designed to demonstrate how to leverage the DeepSpeed framework for training large models, specifically those with billions of parameters, on Habana Gaudi accelerators. It provides practical guidance and code examples to help users efficiently utilize the capabilities of DeepSpeed in conjunction with Habana's hardware to optimize training performance and manage memory effectively.",To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.
"What command line module does PyTorch provide to run a script on multiple GPUs?
","[""\\* correct / len(test\\_loader.dataset)}')\n```\nTypically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook. However, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through \\*distributed\\* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\n## PyTorch Distributed Data Parallelism\nAs the name implies, `torch.distributed` is meant to work on \\*distributed\\* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both. To convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp\\_tutorial.html)\nFirst a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through\n> Note: for this section of the tutorial it should be assumed these are sent in python script files. Later on a launcher using Accelerate will be discussed that removes this necessity\n```python\nimport os\nimport torch.distributed as dist\ndef setup(rank, world\\_size):\n\""Sets up the process group and configuration for PyTorch Distributed Data Parallelism\""\nos.environ[\""MASTER\\_ADDR\""] = 'localhost'\nos.environ[\""MASTER\\_PORT\""] = \""12355\""\n# Initialize the process group\ndist.init\\_process\\_group(\""gloo\"", rank=rank, world\\_size=world\\_size)\ndef cleanup():\n\""Cleans up the distributed environment\""\ndist.destroy\\_process\\_group()\n```\nThe last piece of the puzzle is \\*how do I send my data and model to another GPU?\\*\nThis is where the `DistributedDataParallel` module comes into play. It will copy your model onto each GPU, and when `loss.backward()` is called the backpropagation is performed and the resulting gradients across all these copies of the model will be averaged/reduced. This ensures each device has the same weights post the optimizer step. Below is an example of our training setup, refactored as a function, with this capability:\n> Note: Here rank is the overall rank of the current GPU compared to all the other GPUs available, meaning they have a rank of `0 -> n-1`\n```python\nfrom torch.nn.parallel import DistributedDataParallel as DDP\ndef train(model, rank, world\\_size):\nsetup(rank, world\\_size)\nmodel = model.to(rank)\nddp\\_model = DDP(model, device\\_ids=[rank])\noptimizer = optim.AdamW(ddp\\_model.parameters(), lr=1e-3)\n# Train for one epoch\nmodel.train()\nfor batch\\_idx, (data, target) in enumerate(train\\_loader):\ndata, target = data.to(device), target.to(device)\noutput = model(data)\nloss = F.nll\\_loss(output, target)\nloss.backward()\noptimizer.step()\noptimizer.zero\\_grad()\ncleanup()\n```\nThe optimizer needs to be declared based on the model \\*on the specific device\\* (so `ddp\\_model` and not `model`) for all of the gradients to properly be calculated. Lastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\n```bash\ntorchrun --nproc\\_per\\_node=2 --nnodes=1 example\\_script.py\n```\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch."", "". ! - \\; \\: \\\"" “ % ‘ ” � \\\n--fp16 \\\n--group\\_by\\_length \\\n--push\\_to\\_hub \\\n--do\\_train --do\\_eval\n```\nOn a single V100 GPU, this script should run in \\*ca.\\* 1 hour 20 minutes and yield a CTC loss of \\*\\*0.39\\*\\* and word error rate\nof \\*\\*0.35\\*\\*. ### Multi GPU CTC\nThe following command shows how to fine-tune [XLSR-Wav2Vec2](https://huggingface.co/transformers/main/model\\_doc/xlsr\\_wav2vec2.html) on [Common Voice](https://huggingface.co/datasets/common\\_voice) using 8 GPUs in half-precision. ```bash\ntorchrun \\\n--nproc\\_per\\_node 8 run\\_speech\\_recognition\\_ctc.py \\\n--dataset\\_name=\""common\\_voice\"" \\\n--model\\_name\\_or\\_path=\""facebook/wav2vec2-large-xlsr-53\"" \\\n--dataset\\_config\\_name=\""tr\"" \\\n--output\\_dir=\""./wav2vec2-common\\_voice-tr-demo-dist\"" \\\n--overwrite\\_output\\_dir \\\n--num\\_train\\_epochs=\""15\"" \\\n--per\\_device\\_train\\_batch\\_size=\""4\"" \\\n--learning\\_rate=\""3e-4\"" \\\n--warmup\\_steps=\""500\"" \\\n--evaluation\\_strategy=\""steps\"" \\\n--text\\_column\\_name=\""sentence\"" \\\n--length\\_column\\_name=\""input\\_length\"" \\\n--save\\_steps=\""400\"" \\\n--eval\\_steps=\""100\"" \\\n--logging\\_steps=\""1\"" \\\n--layerdrop=\""0.0\"" \\\n--save\\_total\\_limit=\""3\"" \\\n--freeze\\_feature\\_encoder \\\n--gradient\\_checkpointing \\\n--chars\\_to\\_ignore , ?"", ""Scripts\nA train, validation, inference, and checkpoint cleaning script included in the github root folder. Scripts are not currently packaged in the pip release. The training and validation scripts evolved from early versions of the [PyTorch Imagenet Examples](https://github.com/pytorch/examples). I have added significant functionality over time, including CUDA specific performance enhancements based on\n[NVIDIA's APEX Examples](https://github.com/NVIDIA/apex/tree/master/examples). ## Training Script\nThe variety of training args is large and not all combinations of options (or even options) have been fully tested. For the training dataset folder, specify the folder to the base that contains a `train` and `validation` folder. To train an SE-ResNet34 on ImageNet, locally distributed, 4 GPUs, one process per GPU w/ cosine schedule, random-erasing prob of 50% and per-pixel random value:\n```bash\n./distributed\\_train.sh 4 /data/imagenet --model seresnet34 --sched cosine --epochs 150 --warmup-epochs 5 --lr 0.4 --reprob 0.5 --remode pixel --batch-size 256 --amp -j 4\n```\nIt is recommended to use PyTorch 1.9+ w/ PyTorch native AMP and DDP instead of APEX AMP."", ""The file naming is up to you. It's recommended to use DeepSpeed's `add\\_config\\_arguments` utility to add the necessary command line arguments to your code. For more information please see [DeepSpeed's Argument Parsing](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) doc. You can use a launcher of your choice here. You can continue using the pytorch launcher:\n```bash\ntorch.distributed.run --nproc\\_per\\_node=2 your\\_program.py  --deepspeed ds\\_config.json\n```\nor use the launcher provided by `deepspeed`:\n```bash\ndeepspeed --num\\_gpus=2 your\\_program.py  --deepspeed ds\\_config.json\n```\nAs you can see the arguments aren't the same, but for most needs either of them works. The\nfull details on how to configure various nodes and GPUs can be found [here](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node). When you use the `deepspeed` launcher and you want to use all available gpus you can just omit the `--num\\_gpus` flag. Here is an example of running `run\\_translation.py` under DeepSpeed deploying all available GPUs:\n```bash\ndeepspeed examples/pytorch/translation/run\\_translation.py \\\n--deepspeed tests/deepspeed/ds\\_config\\_zero3.json \\\n--model\\_name\\_or\\_path t5-small --per\\_device\\_train\\_batch\\_size 1 \\\n--output\\_dir output\\_dir --overwrite\\_output\\_dir --fp16 \\\n--do\\_train --max\\_train\\_samples 500 --num\\_train\\_epochs 1 \\\n--dataset\\_name wmt16 --dataset\\_config \""ro-en\"" \\\n--source\\_lang en --target\\_lang ro\n```\nNote that in the DeepSpeed documentation you are likely to see `--deepspeed --deepspeed\\_config ds\\_config.json` - i.e. two DeepSpeed-related arguments, but for the sake of simplicity, and since there are already so many arguments to deal\nwith, we combined the two into a single argument. For some practical usage examples, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400). ### Deployment with one GPU\nTo deploy DeepSpeed with one GPU adjust the [`Trainer`] command line arguments as follows:\n```bash\ndeepspeed --num\\_gpus=1 examples/pytorch/translation/run\\_translation.py \\\n--deepspeed tests/deepspeed/ds\\_config\\_zero2.json \\\n--model\\_name\\_or\\_path t5-small --per\\_device\\_train\\_batch\\_size 1 \\\n--output\\_dir output\\_dir --overwrite\\_output\\_dir --fp16 \\\n--do\\_train --max\\_train\\_samples 500 --num\\_train\\_epochs 1 \\\n--dataset\\_name wmt16 --dataset\\_config \""ro-en\"" \\\n--source\\_lang en --target\\_lang ro\n```\nThis is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly to use just one GPU via\n`--num\\_gpus=1`. By default, DeepSpeed deploys all GPUs it can see on the given node. If you have only 1 GPU to start\nwith, then you don't need this argument. The following [documentation](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) discusses the launcher options."", ""### Older CUDA versions\nSometimes, older CUDA versions may refuse to build with newer compilers. For example, if you have `gcc-9` but CUDA wants `gcc-7`. Usually, installing the latest CUDA toolkit enables support for the newer compiler. You could also install an older version of the compiler in addition to the one you're currently using (or it may already be installed but it's not used by default and the build system can't see it). To resolve this, you can create a symlink to give the build system visibility to the older compiler. ```bash\n# adapt the path to your system\nsudo ln -s /usr/bin/gcc-7 /usr/local/cuda-10.2/bin/gcc\nsudo ln -s /usr/bin/g++-7 /usr/local/cuda-10.2/bin/g++\n```\n## Multi-GPU Network Issues Debug\nWhen training or inferencing with `DistributedDataParallel` and multiple GPU, if you run into issue of inter-communication between processes and/or nodes, you can use the following script to diagnose network issues. ```bash\nwget https://raw.githubusercontent.com/huggingface/transformers/main/scripts/distributed/torch-distributed-gpu-test.py\n```\nFor example to test how 2 GPUs interact do:\n```bash\npython -m torch.distributed.run --nproc\\_per\\_node 2 --nnodes 1 torch-distributed-gpu-test.py\n```\nIf both processes can talk to each and allocate GPU memory each will print an OK status."", ""It is more than linearly faster than run\\_eval.py in some cases\nbecause it uses SortishSampler to minimize padding. You can also use it on 1 GPU. `data\\_dir` must have\n`{type\\_path}.source` and `{type\\_path}.target`. Run `./run\\_distributed\\_eval.py --help` for all clargs. ```bash\ntorchrun --nproc\\_per\\_node=8 run\\_distributed\\_eval.py \\\n--model\\_name sshleifer/distilbart-large-xsum-12-3 \\\n--save\\_dir xsum\\_generations \\\n--data\\_dir xsum \\\n--fp16 # you can pass generate kwargs like num\\_beams here, just like run\\_eval.py\n```\nContributions that implement this command for other distributed hardware setups are welcome! #### Single-GPU Eval: Tips and Tricks\nWhen using `run\\_eval.py`, the following features can be useful:\n\\* if you running the script multiple times and want to make it easier to track what arguments produced that output, use `--dump-args`. Along with the results it will also dump any custom params that were passed to the script. For example if you used: `--num\\_beams 8 --early\\_stopping true`, the output will be:\n```\n{'bleu': 26.887, 'n\\_obs': 10, 'runtime': 1, 'seconds\\_per\\_sample': 0.1, 'num\\_beams': 8, 'early\\_stopping': True}\n```\n`--info` is an additional argument available for the same purpose of tracking the conditions of the experiment. It's useful to pass things that weren't in the argument list, e.g. a language pair `--info \""lang:en-ru\""`. But also if you pass `--info` without a value it will fallback to the current date/time string, e.g. `2020-09-13 18:44:43`. If using `--dump-args --info`, the output will be:\n```\n{'bleu': 26.887, 'n\\_obs': 10, 'runtime': 1, 'seconds\\_per\\_sample': 0.1, 'num\\_beams': 8, 'early\\_stopping': True, 'info': '2020-09-13 18:44:43'}\n```\nIf using `--dump-args --info \""pair:en-ru chkpt=best`, the output will be:\n```\n{'bleu': 26.887, 'n\\_obs': 10, 'runtime': 1, 'seconds\\_per\\_sample': 0.1, 'num\\_beams': 8, 'early\\_stopping': True, 'info': 'pair=en-ru chkpt=best'}\n```\n\\* if you need to perform a parametric search in order to find the best ones that lead to the highest BLEU score, let `run\\_eval\\_search.py` to do the searching for you.""]","PyTorch provides the `torchrun` command line module to run a script on multiple GPUs. You can use it by passing in the number of nodes and the script to run, for example:

```bash
torchrun --nproc_per_node=2 example_script.py
```",torchrun
"What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?
","[""Vision Transformers 图像分类\n相关空间：https://huggingface.co/spaces/abidlabs/vision-transformer\n标签：VISION, TRANSFORMERS, HUB\n## 简介\n图像分类是计算机视觉中的重要任务。构建更好的分类器以确定图像中存在的对象是当前研究的热点领域，因为它在从人脸识别到制造质量控制等方面都有应用。\n最先进的图像分类器基于 \\_transformers\\_ 架构，该架构最初在自然语言处理任务中很受欢迎。这种架构通常被称为 vision transformers (ViT)。这些模型非常适合与 Gradio 的\\*图像\\*输入组件一起使用，因此在本教程中，我们将构建一个使用 Gradio 进行图像分类的 Web 演示。我们只需用\\*\\*一行 Python 代码\\*\\*即可构建整个 Web 应用程序，其效果如下（试用一下示例之一！）：\n让我们开始吧！\n### 先决条件\n确保您已经[安装](/getting\\_started)了 `gradio` Python 包。\n## 步骤 1 - 选择 Vision 图像分类模型\n首先，我们需要一个图像分类模型。在本教程中，我们将使用[Hugging Face Model Hub](https://huggingface.co/models?pipeline\\_tag=image-classification)上的一个模型。该 Hub 包含数千个模型，涵盖了多种不同的机器学习任务。\n在左侧边栏中展开 Tasks 类别，并选择我们感兴趣的“Image Classification”作为我们的任务。然后，您将看到 Hub 上为图像分类设计的所有模型。\n在撰写时，最受欢迎的模型是 `google/vit-base-patch16-224`，该模型在分辨率为 224x224 像素的 ImageNet 图像上进行了训练。我们将在演示中使用此模型。\n## 步骤 2 - 使用 Gradio 加载 Vision Transformer 模型\n当使用 Hugging Face Hub 上的模型时，我们无需为演示定义输入或输出组件。同样，我们不需要关心预处理或后处理的细节。所有这些都可以从模型标签中自动推断出来。\n除了导入语句外，我们只需要一行代码即可加载并启动演示。\n我们使用 `gr.Interface.load()` 方法，并传入包含 `huggingface/` 的模型路径，以指定它来自 Hugging Face Hub。\n```python\nimport gradio as gr\ngr.Interface.load(\n\""huggingface/google/vit-base-patch16-224\"",\nexamples=[\""alligator.jpg\"", \""laptop.jpg\""]).launch()\n```\n请注意，我们添加了一个 `examples` 参数，允许我们使用一些预定义的示例预填充我们的界面。\n这将生成以下接口，您可以直接在浏览器中尝试。当您输入图像时，它会自动进行预处理并发送到 Hugging Face Hub API，通过模型处理，并以人类可解释的预测结果返回。尝试上传您自己的图像！\n---\n完成！只需一行代码，您就建立了一个图像分类器的 Web 演示。如果您想与他人分享，请在 `launch()` 接口时设置 `share=True`。"", ""Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. 1. \\*\\*[SwiftFormer](https://huggingface.co/docs/transformers/model\\_doc/swiftformer)\\*\\* (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan. 1. \\*\\*[Swin Transformer](https://huggingface.co/docs/transformers/model\\_doc/swin)\\*\\* (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. 1. \\*\\*[Swin Transformer V2](https://huggingface.co/docs/transformers/model\\_doc/swinv2)\\*\\* (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo. 1. \\*\\*[Swin2SR](https://huggingface.co/docs/transformers/model\\_doc/swin2sr)\\*\\* (from University of Würzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V."", ""Note that one should\nuse [`DeiTImageProcessor`] in order to prepare images for the model. - [BEiT](beit) (BERT pre-training of Image Transformers) by Microsoft Research. BEiT models outperform supervised pre-trained\nvision transformers using a self-supervised method inspired by BERT (masked image modeling) and based on a VQ-VAE. - DINO (a method for self-supervised training of Vision Transformers) by Facebook AI. Vision Transformers trained using\nthe DINO method show very interesting properties not seen with convolutional models. They are capable of segmenting\nobjects, without having ever been trained to do so. DINO checkpoints can be found on the [hub](https://huggingface.co/models?other=dino). - [MAE](vit\\_mae) (Masked Autoencoders) by Facebook AI. By pre-training Vision Transformers to reconstruct pixel values for a high portion\n(75%) of masked patches (using an asymmetric encoder-decoder architecture), the authors show that this simple method outperforms\nsupervised pre-training after fine-tuning."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Transformer2D\nA Transformer model for image-like data from [CompVis](https://huggingface.co/CompVis) that is based on the [Vision Transformer](https://huggingface.co/papers/2010.11929) introduced by Dosovitskiy et al. The [`Transformer2DModel`] accepts discrete (classes of vector embeddings) or continuous (actual embeddings) inputs. When the input is \\*\\*continuous\\*\\*:\n1. Project the input and reshape it to `(batch\\_size, sequence\\_length, feature\\_dimension)`. 2. Apply the Transformer blocks in the standard way. 3. Reshape to image. When the input is \\*\\*discrete\\*\\*:\nIt is assumed one of the input classes is the masked latent pixel. The predicted classes of the unnoised image don't contain a prediction for the masked pixel because the unnoised image cannot be masked."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Pyramid Vision Transformer (PVT)\n## Overview\nThe PVT model was proposed in\n[Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/abs/2102.12122)\nby Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao. The PVT is a type of\nvision transformer that utilizes a pyramid structure to make it an effective backbone for dense prediction tasks.""]",最受欢迎的视觉变换器模型是 `google/vit-base-patch16-224`，该模型在分辨率为 224x224 像素的 ImageNet 图像上进行了训练。,google/vit-base-patch16-224
"What is the command to upload an ESPnet model to a Hugging Face repository?
","[""Using ESPnet at Hugging Face\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks. ## Exploring ESPnet in the Hub\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). All models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more. 2. Metadata tags that help for discoverability and contain information such as license, language and datasets. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png)\n\n## Using existing models\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\_model\\_zoo)). If you're interested in doing inference, different classes for different tasks have a `from\\_pretrained` method that allows loading models from the Hub."", ""n this video, we're going to understand how to manage a model repository on the HuggingFace model hub. In order to handle a repository, you should first have a Hugging Face account. A link to create a new account is available in the description. Once you are logged in, you can create a new repository by clicking on the \""New model\"" option. You should be facing a similar modal to the following. In the \""Owner\"" input, you can put either your own namespace or any of your organisations namespaces. The model name is the model identifier that will then be used to identify your model on your chosen namespace. The final choice is between public and private. Public models are accessible by anyone. This is the recommended, free option, as this makes your model easily accessible and shareable. The owners of your namespace are the only ones who can update and change your model. A more advanced option is the private option. In this case, only the owners of your namespace will have visibility over your model. Other users won't know it exists and will not be able to use it. Let's create a dummy model to play with. Once your model is created, comes the management of that model! Three tabs are available to you. You're facing the first one, which is the model card page; this is the page used to showcase your model to the world."", ""- Option1: Pushing the model to Hugging Face Hub\n```python\nmodel.push\\_to\\_hub(\nf\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\""/\"", \""\\_\""),\ntoken = \""hf\\_...\""\n)\n```\ntoken (`bool` or `str`, \\*optional\\*):\n`token` is to be used for HTTP Bearer authorization when accessing remote files. If `True`, will use the token generated\nwhen running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo\\_url`\nis not specified. Or you can get your token from https://huggingface.co/settings/token\n```\n- Or save model locally\n```python\npeft\\_model\\_id = f\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\""/\"", \""\\_\"")\nmodel.save\\_pretrained(peft\\_model\\_id)\n```\n```python\n# saving model\npeft\\_model\\_id = f\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\n\""/\"", \""\\_\""\n)\nmodel.save\\_pretrained(peft\\_model\\_id)\n```\n```python\nckpt = f\""{peft\\_model\\_id}/adapter\\_model.bin\""\n!du -h $ckpt\n```\n```python\nfrom peft import PeftModel, PeftConfig\npeft\\_model\\_id = f\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\n\""/\"", \""\\_\""\n)\nconfig = PeftConfig.from\\_pretrained(peft\\_model\\_id)\nmodel = AutoModelForCausalLM.from\\_pretrained(config.base\\_model\\_name\\_or\\_path)\nmodel = PeftModel.from\\_pretrained(model, peft\\_model\\_id)\n```\n```python\nmodel.to(device)\nmodel.eval()\ni = 4\ninputs = tokenizer(f'{text\\_column} : {dataset[\""test\""][i][\""Tweet text\""]} Label : ', return\\_tensors=\""pt\"")\nprint(dataset[\""test\""][i][\""Tweet text\""])\nprint(inputs)\nwith torch.no\\_grad():\ninputs = {k: v.to(device) for k, v in inputs.items()}\noutputs = model.generate(\ninput\\_ids=inputs[\""input\\_ids\""], attention\\_mask=inputs[\""attention\\_mask\""], max\\_new\\_tokens=10, eos\\_token\\_id=3\n)\nprint(outputs)\nprint(tokenizer.batch\\_decode(outputs.detach().cpu().numpy(), skip\\_special\\_tokens=True))\n```"", ""4. As soon as your Space is built, Hugging Face will detect that it is associated with the model. A \""Linked Models\"" button should appear in the top right corner of the Space, as shown here:\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/linked-models.png)\n\\*Note\\*: You can also add linked models manually by explicitly updating them in the [README metadata for the Space, as described here](https://huggingface.co/docs/hub/spaces-config-reference). Your Space should appear in the Demo tab next to the paper on ArXiv in a few minutes 🤗"", ""Philipp: Sure you can use the new Hugging Face Deep Learning Container. Jeff: ok. Jeff: and how can I get started? Jeff: where can I find documentation? Philipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face\n'''\nnlp(conversation)\n## Results\n| key | value |\n| --- | ----- |\n{eval\\_table}\n{test\\_table}\n\""\""\""\n# Generate model card (todo: add more data from Trainer)\nmodel\\_card = MODEL\\_CARD\\_TEMPLATE.format(\nmodel\\_name=f\""{hyperparameters['model\\_name\\_or\\_path'].split('/')[1]}-{hyperparameters['dataset\\_name']}\"",\nhyperparameters=json.dumps(hyperparameters, indent=4, sort\\_keys=True),\neval\\_table=\""\\n\"".join(f\""| {k} | {v} |\"" for k, v in eval\\_results.items()),\ntest\\_table=\""\\n\"".join(f\""| {k} | {v} |\"" for k, v in test\\_results.items()),\n)\nwith open(f\""{local\\_path}/README.md\"", \""w\"") as f:\nf.write(model\\_card)\n```\nAfter we have our unzipped model and model card located in `my\\_bart\\_model` we can use the either `huggingface\\_hub` SDK to create a repository and upload it to [huggingface.co](https://huggingface.co) – or just to https://huggingface.co/new an create a new repository and upload it. ```python\nfrom getpass import getpass\nfrom huggingface\\_hub import HfApi, Repository\nhf\\_username = \""philschmid\"" # your username on huggingface.co\nhf\\_email = \""philipp@huggingface.co\"" # email used for commit\nrepository\\_name = f\""{hyperparameters['model\\_name\\_or\\_path'].split('/')[1]}-{hyperparameters['dataset\\_name']}\"" # repository name on huggingface.co\npassword = getpass(\""Enter your password:\"") # creates a prompt for entering password\n# get hf token\ntoken = HfApi().login(username=hf\\_username, password=password)\n# create repository\nrepo\\_url = HfApi().create\\_repo(token=token, name=repository\\_name, exist\\_ok=True)\n# create a Repository instance\nmodel\\_repo = Repository(use\\_auth\\_token=token,\nclone\\_from=repo\\_url,\nlocal\\_dir=local\\_path,\ngit\\_user=hf\\_username,\ngit\\_email=hf\\_email)\n# push model to the hub\nmodel\\_repo.push\\_to\\_hub()\n```\n---\n## Test inference\nAfter we uploaded our model we can access it at `https://huggingface.co/{hf\\_username}/{repository\\_name}`\n```python\nprint(f\""https://huggingface.co/{hf\\_username}/{repository\\_name}\"")\n```\nAnd use the \""Hosted Inference API\"" widget to test it. [https://huggingface.co/philschmid/bart-large-cnn-samsum](https://huggingface.co/philschmid/bart-large-cnn-samsum)\n![inference](assets/19\\_sagemaker\\_distributed\\_training\\_seq2seq/inference-test.png)""]","To upload an ESPnet model to a Hugging Face repository, you can use the following command:

```python
model.push_to_hub(
    f""{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}"".replace(""/"", ""_""),
    token=""hf_...""
)
```

In this command, replace `dataset_name`, `model_name_or_path`, `peft_config.peft_type`, and `peft_config.task_type` with your specific values, and provide your Hugging Face token.",./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo
"What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?
","[""Add custom Dependencies\nInference Endpoints’ base image includes all required libraries to run inference on 🤗 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\\* [customize your inference pipeline](/docs/inference-endpoints/guides/custom\\_handler) and need additional Python dependencies\n\\* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)). To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within. ```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\\* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n\\* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom\\_container) for inference."", ""It contains information about a deployed `InferenceEndpoint`, including its configuration and current state. Once deployed, you can run inference on the Endpoint using the [`InferenceEndpoint.client`] and [`InferenceEndpoint.async\\_client`] properties that respectively return an [`InferenceClient`] and an [`AsyncInferenceClient`] object. [[autodoc]] InferenceEndpoint\n- from\\_raw\n- client\n- async\\_client\n- all\n## InferenceEndpointStatus\n[[autodoc]] InferenceEndpointStatus\n## InferenceEndpointType\n[[autodoc]] InferenceEndpointType\n## InferenceEndpointError\n[[autodoc]] InferenceEndpointError"", ""### 6. Deploy your Custom Handler as an Inference Endpoint\nThe last step is to deploy your Custom Handler as an Inference Endpoint. You can deploy your Custom Handler like you would a regular Inference Endpoint. Add your repository, select your cloud and region, your instance and security setting, and deploy. When creating your Endpoint, the Inference Endpoint Service will check for an available and valid `handler.py`, and will use it for serving requests no matter which “Task” you select. \\_Note: In your [Inference Endpoints dashboard](https://ui.endpoints.huggingface.co/), the Task for this Endpoint should now be set to Custom\\_"", ""The file name here should be `README.md` since it is what Hugging Face Hub expects. ```python\nmodel\\_card.save(Path(local\\_repo) / \""README.md\"")\n```\nWe can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub\\_utils`. Hugging Face Hub requires tokens for authentication, therefore you need to pass your token in either `notebook\\_login` if you're logging in from a notebook, or `huggingface-cli login` if you're logging in from the CLI. ```python\n# if the repository doesn't exist remotely on the Hugging Face Hub, it will be created when we set create\\_remote to True\nrepo\\_id = \""skops-user/my-awesome-model\""\nhub\\_utils.push(\nrepo\\_id=repo\\_id,\nsource=local\\_repo,\ntoken=token,\ncommit\\_message=\""pushing files to the repo from the example!\"",\ncreate\\_remote=True,\n)\n```\nOnce we push the model to the Hub, anyone can use it unless the repository is private. You can download the models using `download`. Apart from the model file, the repository contains the model configuration and the environment requirements. ```python\ndownload\\_repo = \""downloaded-model\""\nhub\\_utils.download(repo\\_id=repo\\_id, dst=download\\_repo)\n```\nThe inference widget is enabled to make predictions in the repository. ![Hosted Inference Widget](assets/94\\_skops/skops\\_widget.png)\nIf the requirements of your project have changed, you can use `update\\_env` to update the environment. ```python\nhub\\_utils.update\\_env(path=local\\_repo, requirements=[\""scikit-learn\""])\n```\nYou can see the example repository pushed with above code [here](https://huggingface.co/scikit-learn/skops-blog-example)."", ""In both the cases the `model.generate` method produces the audio and follows the same principles as text generation. You can read more about it in our [how to generate](https://huggingface.co/blog/how-to-generate) blog post. Alright! With the basic usage outlined above, let's deploy MusicGen for fun and profit! First, we'll define a custom handler in `handler.py`. We can use the [Inference Endpoints template](https://huggingface.co/docs/inference-endpoints/guides/custom\\_handler#3-customize-endpointhandler) and override the `\\_\\_init\\_\\_` and `\\_\\_call\\_\\_` methods with our custom inference code. `\\_\\_init\\_\\_` will initialize the model and the processor, and `\\_\\_call\\_\\_` will take the data and return the generated music. You can find the modified `EndpointHandler` class below. 👇\n```python\nfrom typing import Dict, List, Any\nfrom transformers import AutoProcessor, MusicgenForConditionalGeneration\nimport torch\nclass EndpointHandler:\ndef \\_\\_init\\_\\_(self, path=\""\""):\n# load model and processor from path\nself.processor = AutoProcessor.from\\_pretrained(path)\nself.model = MusicgenForConditionalGeneration.from\\_pretrained(path, torch\\_dtype=torch.float16).to(\""cuda\"")\ndef \\_\\_call\\_\\_(self, data: Dict[str, Any]) -> Dict[str, str]:\n\""\""\""\nArgs:\ndata (:dict:):\nThe payload with the text prompt and generation parameters. \""\""\""\n# process input\ninputs = data.pop(\""inputs\"", data)\nparameters = data.pop(\""parameters\"", None)\n# preprocess\ninputs = self.processor(\ntext=[inputs],\npadding=True,\nreturn\\_tensors=\""pt\"",).to(\""cuda\"")\n# pass inputs with all kwargs in data\nif parameters is not None:\nwith torch.autocast(\""cuda\""):\noutputs = self.model.generate(\\*\\*inputs, \\*\\*parameters)\nelse:\nwith torch.autocast(\""cuda\""):\noutputs = self.model.generate(\\*\\*inputs,)\n# postprocess the prediction\nprediction = outputs[0].cpu().numpy().tolist()\nreturn [{\""generated\\_audio\"": prediction}]\n```\nTo keep things simple, in this example we are only generating audio from text, and not conditioning it with a melody. Next, we will create a `requirements.txt` file containing all the dependencies we need to run our inference code:\n```\ntransformers==4.31.0\naccelerate>=0.20.3\n```\nUploading these two files to our repository will suffice to serve the model. ![inference-files](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ie\\_musicgen/files.png)\nWe can now create the Inference Endpoint. Head to the [Inference Endpoints](https://huggingface.co/inference-endpoints) page and click `Deploy your first model`. In the \""Model repository\"" field, enter the identifier of your duplicated repository. Then select the hardware you want and create the endpoint. Any instance with a minimum of 16 GB RAM should work for `musicgen-large`."", ""This can be done by using the `huggingface-cli`. \\_Note: Make sure to configure git config as well.\\_\n```\n# setup cli with token\nhuggingface-cli login\ngit config --global credential.helper store\n```\n### 2. Create EndpointHandler (CP)\nAfter we have set up our environment, we can start creating your custom handler. The custom handler is a Python class (`EndpointHandler`) inside a `handler.py` file in our repository. The `EndpointHandler` needs to implement an `\\_\\_init\\_\\_` and a `\\_\\_call\\_\\_` method. - The `\\_\\_init\\_\\_` method will be called when starting the Endpoint and will receive 1 argument, a string with the path to your model weights. This allows you to load your model correctly. - The `\\_\\_call\\_\\_` method will be called on every request and receive a dictionary with your request body as a python dictionary. It will always contain the `inputs` key. The first step is to create our `handler.py` in the local clone of our repository. ```\n!cd distilbert-base-uncased-emotion && touch handler.py\n```\nIn there, you define your `EndpointHandler` class with the `\\_\\_init\\_\\_` and `\\_\\_call\\_\\_ `method. ```python\nfrom typing import Dict, List, Any\nclass EndpointHandler():\ndef \\_\\_init\\_\\_(self, path=\""\""):\n# Preload all the elements you are going to need at inference. # pseudo:\n# self.model= load\\_model(path)\ndef \\_\\_call\\_\\_(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:\n\""\""\""\ndata args:\ninputs (:obj: `str` | `PIL.Image` | `np.array`)\nkwargs\nReturn:\nA :obj:`list` | `dict`: will be serialized and returned\n\""\""\""\n# pseudo\n# self.model(input)\n```\n### 3.""]","To install custom Python dependencies for Inference Endpoints, you should add a `requirements.txt` file to your model repository on the Hugging Face Hub. This file should contain the Python dependencies you want to install.",requirements.txt
"How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Textual inversion\n[[open-in-colab]]\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer). This guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text\\_inversion) training guide. Import the necessary libraries:\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import make\\_image\\_grid\n```\n## Stable Diffusion 1 and 2\nPick a Stable Diffusion checkpoint and a pre-learned concept from the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer):\n```py\npretrained\\_model\\_name\\_or\\_path = \""runwayml/stable-diffusion-v1-5\""\nrepo\\_id\\_embeds = \""sd-concepts-library/cat-toy\""\n```\nNow you can load a pipeline, and pass the pre-learned concept to it:\n```py\npipeline = StableDiffusionPipeline.from\\_pretrained(\npretrained\\_model\\_name\\_or\\_path, torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\npipeline.load\\_textual\\_inversion(repo\\_id\\_embeds)\n```\nCreate a prompt with the pre-learned concept by using the special placeholder token ``, and choose the number of samples and rows of images you'd like to generate:\n```py\nprompt = \""a grafitti in a favela wall with a  on it\""\nnum\\_samples\\_per\\_row = 2\nnum\\_rows = 2\n```\nThen run the pipeline (feel free to adjust the parameters like `num\\_inference\\_steps` and `guidance\\_scale` to see how they affect image quality), save the generated images and visualize them with the helper function you created at the beginning:\n```py\nall\\_images = []\nfor \\_ in range(num\\_rows):\nimages = pipeline(prompt, num\\_images\\_per\\_prompt=num\\_samples\\_per\\_row, num\\_inference\\_steps=50, guidance\\_scale=7.5).images\nall\\_images.extend(images)\ngrid = make\\_image\\_grid(all\\_images, num\\_rows, num\\_samples\\_per\\_row)\ngrid\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/textual_inversion_inference.png)\n\n## Stable Diffusion XL\nStable Diffusion XL (SDXL) can also use textual inversion vectors for inference. In contrast to Stable Diffusion 1 and 2, SDXL has two text encoders so you'll need two textual inversion embeddings - one for each text encoder model. Let's download the SDXL textual inversion embeddings and have a closer look at it's structure:\n```py\nfrom huggingface\\_hub import hf\\_hub\\_download\nfrom safetensors.torch import load\\_file\nfile = hf\\_hub\\_download(\""dn118/unaestheticXL\"", filename=\""unaestheticXLv31.safetensors\"")\nstate\\_dict = load\\_file(file)\nstate\\_dict\n```\n```\n{'clip\\_g': tensor([[ 0.0077, -0.0112, 0.0065, ..., 0.0195, 0.0159, 0.0275],\n...,\n[-0.0170, 0.0213, 0.0143, ..., -0.0302, -0.0240, -0.0362]],\n'clip\\_l': tensor([[ 0.0023, 0.0192, 0.0213, ..., -0.0385, 0.0048, -0.0011],\n...,\n[ 0.0475, -0.0508, -0.0145, ..., 0.0070, -0.0089, -0.0163]],\n```\nThere are two tensors, `\""clip\\_g\""` and `\""clip\\_l\""`. `\""clip\\_g\""` corresponds to the bigger text encoder in SDXL and refers to\n`pipe.text\\_encoder\\_2` and `\""clip\\_l\""` refers to `pipe.text\\_encoder`. Now you can load each tensor separately by passing them along with the correct text encoder and tokenizer\nto [`~loaders.TextualInversionLoaderMixin.load\\_textual\\_inversion`]:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipe = AutoPipelineForText2Image.from\\_pretrained(\""stabilityai/stable-diffusion-xl-base-1.0\"", variant=\""fp16\"", torch\\_dtype=torch.float16)\npipe.to(\""cuda\"")\npipe.load\\_textual\\_inversion(state\\_dict[\""clip\\_g\""], token=\""unaestheticXLv31\"", text\\_encoder=pipe.text\\_encoder\\_2, tokenizer=pipe.tokenizer\\_2)\npipe.load\\_textual\\_inversion(state\\_dict[\""clip\\_l\""], token=\""unaestheticXLv31\"", text\\_encoder=pipe.text\\_encoder, tokenizer=pipe.tokenizer)\n# the embedding should be used as a negative embedding, so we pass it as a negative prompt\ngenerator = torch.Generator().manual\\_seed(33)\nimage = pipe(\""a woman standing in front of a mountain\"", negative\\_prompt=\""unaestheticXLv31\"", generator=generator).images[0]\nimage\n```""]","To teach new concepts to Stable Diffusion using Textual Inversion, you need just 3-5 example images.",3-5 images
"What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?
","[""In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future. ## Sharded checkpoints\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save\\_pretrained(save\\_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in. You can control the maximum size before sharding with the `max\\_shard\\_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model. ```py\nfrom transformers import AutoModel\nmodel = AutoModel.from\\_pretrained(\""bert-base-cased\"")\n```\nIf you save it using [`~PreTrainedModel.save\\_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n```py\n>>> import os\n>>> import tempfile\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir)\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model.bin']\n```\nNow let's use a maximum shard size of 200MB:\n```py\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model-00001-of-00003.bin', 'pytorch\\_model-00002-of-00003.bin', 'pytorch\\_model-00003-of-00003.bin', 'pytorch\\_model.bin.index.json']\n```\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index.""]","In Transformers version 4.18.0, the maximum size of a model checkpoint before it is automatically sharded is 10GB.",10GB
"What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?
",[],"Weights and Biases (W&B) is a tool that helps data scientists and machine learning practitioners track experiments, visualize results, and manage datasets. It provides functionalities for logging metrics, visualizing model performance, and collaborating with team members. By using W&B, users can streamline their workflow, reproduce experiments, and gain insights into their models' behavior, ultimately improving the efficiency and effectiveness of their machine learning projects.","To track their machine learning experiments at every stage, from training to production."
"What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?
","[""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\n\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\n\n#### **English** | [简体中文](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [繁體中文](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [한국어](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Español](https://github.com/huggingface/transformers/blob/main/README_es.md) | [日本語](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [हिन्दी](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [Русский](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [Рortuguês](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [తెలుగు](https://github.com/huggingface/transformers//blob/main/README_te.md) |\n\n### State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n\n### \n\n🤗 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio."", ""Please let us know if you have any issue or feature you would like the library to support. For all questions, the [forums](https://discuss.huggingface.co/c/accelerate) is the place to check! For more complex examples in situation, you can look at the official [Transformers examples](https://github.com/huggingface/transformers/tree/master/examples). Each folder contains a `run\\_task\\_no\\_trainer.py` that leverages the Accelerate library!"", ""--\ntitle: \""Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\""\nthumbnail: /blog/assets/80\\_intel/01.png\nauthors:\n- user: juliensimon\n---\n# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\n![image](assets/80\\_intel/01.png)\nThe mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption. Today, we're excited to announce that Intel has officially joined our [Hardware Partner Program](https://huggingface.co/hardware). Thanks to the [Optimum](https://github.com/huggingface/optimum-intel) open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers. Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage. Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms. “\\*We’re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.\\*”, says Wei Li, Intel Vice President & General Manager, AI and Analytics. In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs. The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Let’s take a deeper look at it. ## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\n🤗 Optimum provides an API called BetterTransformer, a fast path of standard PyTorch Transformer APIs to benefit from interesting speedups on CPU & GPU through sparsity and fused kernels as Flash Attention. For now, BetterTransformer supports the fastpath from the native [`nn.TransformerEncoderLayer`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) as well as Flash Attention and Memory-Efficient Attention from [`torch.nn.functional.scaled\\_dot\\_product\\_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled\\_dot\\_product\\_attention.html)."", ""This collaboration set the seeds for Leandro and I to eventually join Hugging Face. And I've been here now for around nine months. ### That is incredible. How does it feel to have a copy of your book in your hands? \\*\\*Lewis:\\*\\* I have to say, I just became a parent about a year and a half ago and it feels kind of similar to my son being born. You're holding this thing that you created. It's quite an exciting feeling and so different to actually hold it (compared to reading a PDF). Confirms that it’s actually real and I didn't just dream about it. ### Exactly. Congratulations! Want to briefly read one endorsement that I love about this book;\n“\\_Complexity made simple. This is a rare and precious book about NLP, transformers, and the growing ecosystem around them, Hugging Face. Whether these are still buzzwords to you or you already have a solid grasp of it all, the authors will navigate you with humor, scientific rigor, and plenty of code examples into the deepest secrets of the coolest technology around. From “off-the-shelf pre-trained” to “from-scratch custom” models, and from performance to missing labels issues, the authors address practically every real-life struggle of an ML engineer and provide state-of-the-art solutions, making this book destined to dictate the standards in the field for years to come.\\_”\n—Luca Perrozi Ph.D., Data Science and Machine Learning Associate Manager at Accenture. Checkout [Natural Language Processing with Transformers](https://transformersbook.com/). ### Can you talk about the work you've done with the transformers library? \\*\\*Lewis:\\*\\* One of the things that I experienced in my previous jobs before Hugging Face was there's this challenge in the industry when deploying these models into production; these models are really large in terms of the number of parameters and this adds a lot of complexity to the requirements you might have. So for example, if you're trying to build a chatbot you need this model to be very fast and responsive. And most of the time these models are a bit too slow if you just take an off-the-shelf model, train it, and then try to integrate it into your application. So what I've been working on for the last few months on the transformers library is providing the functionality to export these models into a format that lets you run them much more efficiently using tools that we have at Hugging Face, but also just general tools in the open-source ecosystem. In a way, the philosophy of the transformers library is like writing lots of code so that the users don't have to write that code. In this particular example, what we're talking about is something called the ONNX format. It's a special format that is used in industry where you can basically have a model that's written in PyTorch but you can then convert it to TensorFlow or you can run it on some very dedicated hardware. And if you actually look at what's needed to make this conversion happen in the transformers library, it's fairly gnarly. But we make it so that you only really have to run one line of code and the library will take care of you. So the idea is that this particular feature lets machine learning engineers or even data scientists take their model, convert it to this format, and then optimize it to get faster latency and higher throughput."", ""Port of Hugging Face's Transformers library, using the tch-rs crate and pre-processing from rust-tokenizers. Supports multi-threaded tokenization and GPU inference. This repository exposes the model base architecture, task-specific heads and ready-to-use pipelines. Keywords: Rust, BERT, Inference\n## [EasyNLP](https://github.com/alibaba/EasyNLP)\n[EasyNLP](https://github.com/alibaba/EasyNLP) is an easy-to-use NLP development and application toolkit in PyTorch, first released inside Alibaba in 2021. It is built with scalable distributed training strategies and supports a comprehensive suite of NLP algorithms for various NLP applications. [EasyNLP](https://github.com/alibaba/EasyNLP) integrates knowledge distillation and few-shot learning for landing large pre-trained models, together with various popular multi-modality pre-trained models. It provides a unified framework of model training, inference, and deployment for real-world applications. Keywords: NLP, Knowledge distillation, Few-shot learning, Multi-modality, Training, Inference, Deployment\n## [TurboTransformers](https://github.com/Tencent/TurboTransformers)\nA fast and user-friendly runtime for transformer inference (Bert, Albert, GPT2, Decoders, etc) on CPU and GPU."", ""## Additional resources\n\\* Transformers.js [repository](https://github.com/xenova/transformers.js)\n\\* Transformers.js [docs](https://huggingface.co/docs/transformers.js)\n\\* Transformers.js [demo](https://xenova.github.io/transformers.js/)""]",The name of the open-source library created by Hugging Face to simplify Transformer acceleration is **Optimum**.,Optimum
"What parameter is used to ensure that elements in a row have the same height in Gradio?
","[""The Row will wrap if there isn't sufficient space to satisfy all `min\\_width` values. Learn more about Rows in the [docs](https://gradio.app/docs/#row). ## Columns and Nesting\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n$code\\_rows\\_and\\_columns\n$demo\\_rows\\_and\\_columns\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. Learn more about Columns in the [docs](https://gradio.app/docs/#column). # Dimensions\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases. Below is an example illustrating the use of viewport width (vw):\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\nim = gr.ImageEditor(\nwidth=\""50vw\"",\n)\ndemo.launch()\n```\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n```python\nimport gradio as gr\ncss = \""\""\""\n.container {\nheight: 100vh;\n}\n\""\""\""\nwith gr.Blocks(css=css) as demo:\nwith gr.Column(elem\\_classes=[\""container\""]):\nname = gr.Chatbot(value=[[\""1\"", \""2\""]], height=\""70%\"")\ndemo.launch()\n```\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height.""]","In Gradio, the `equal_height` parameter is used to ensure that elements in a row have the same height. You can set this parameter to `True` when creating a Row to achieve uniform heights for all components within that Row.",equal_height
"What is the command to install the latest version of Optimum with OpenVINO support?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Installation\n🤗 Optimum can be installed using `pip` as follows:\n```bash\npython -m pip install optimum\n```\nIf you'd like to use the accelerator-specific features of 🤗 Optimum, you can install the required dependencies according to the table below:\n| Accelerator | Installation |\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\n| [ONNX runtime](https://onnxruntime.ai/docs/) | `pip install --upgrade-strategy eager install optimum[onnxruntime]`|\n| [Intel Neural Compressor (INC)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) | `pip install --upgrade-strategy eager optimum[neural-compressor]` |\n| [Intel OpenVINO](https://docs.openvino.ai/latest/index.html) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |\n| [AMD Instinct GPUs and Ryzen AI NPU](https://www.amd.com/en/graphics/instinct-server-accelerators) | `pip install --upgrade-strategy eager optimum[amd]` |\n| [Habana Gaudi Processor (HPU)](https://habana.ai/training/) | `pip install --upgrade-strategy eager optimum[habana]` |\n| [FuriosaAI](https://www.furiosa.ai/) | `pip install --upgrade-strategy eager optimum[furiosa]` |\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version. If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you can install the base library from source as follows:\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\nFor the accelerator-specific features, you can install them by appending `optimum[accelerator\\_type]` to the `pip` command, e.g. ```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```"", ""You can find more examples in the 🤗 Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion-xl), and running SDXL in OpenVINO is supported for text-to-image and image-to-image."", ""--\ntitle: \""Accelerate your models with 🤗 Optimum Intel and OpenVINO\""\nthumbnail: /blog/assets/113\\_openvino/thumbnail.png\nauthors:\n- user: echarlaix\n- user: juliensimon\n---\n# Accelerate your models with 🤗 Optimum Intel and OpenVINO\n![image](assets/113\\_openvino/thumbnail.png)\nLast July, we [announced](https://huggingface.co/blog/intel) that Intel and Hugging Face would collaborate on building state-of-the-art yet simple hardware acceleration tools for Transformer models. ​\nToday, we are very happy to announce that we added Intel [OpenVINO](https://docs.openvino.ai/latest/index.html) to [Optimum Intel](https://github.com/huggingface/optimum-intel). You can now easily perform inference with OpenVINO Runtime on a variety of Intel processors ([see](https://docs.openvino.ai/latest/openvino\\_docs\\_OV\\_UG\\_supported\\_plugins\\_Supported\\_Devices.html) the full list of supported devices) using Transformers models which can be hosted either on the Hugging Face hub or locally. You can also quantize your model with the OpenVINO Neural Network Compression Framework ([NNCF](https://github.com/openvinotoolkit/nncf)), and reduce its size and prediction latency in near minutes. ​\nThis first release is based on OpenVINO 2022.2 and enables inference for a large quantity of PyTorch models using our [`OVModels`](https://huggingface.co/docs/optimum/intel/inference). Post-training static quantization and quantization aware training can be applied on many encoder models (BERT, DistilBERT, etc.). More encoder models will be supported in the upcoming OpenVINO release. Currently the quantization of Encoder Decoder models is not enabled, however this restriction should be lifted with our integration of the next OpenVINO release. ​Let us show you how to get started in minutes!​\n## Quantizing a Vision Transformer with Optimum Intel and OpenVINO\n​\nIn this example, we will run post-training static quantization on a Vision Transformer (ViT) [model](https://huggingface.co/juliensimon/autotrain-food101-1471154050) fine-tuned for image classification on the [food101](https://huggingface.co/datasets/food101) dataset."", ""```\n\\* If you want to install the dependencies beyond in a local Python environment. You can pip install them once you have [ROCM 5.7](https://rocmdocs.amd.com/en/latest/deploy/linux/quick\\_start.html) well installed. ```bash\npip install onnx ninja\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm5.7\npip install pip install --pre onnxruntime-training -f https://download.onnxruntime.ai/onnxruntime\\_nightly\\_rocm57.html\npip install torch-ort\npip install --upgrade protobuf==3.20.2\n```\nAnd run post-installation configuration:\n```bash\npython -m torch\\_ort.configure\n```\n### Install Optimum\nYou can install Optimum via pypi:\n```bash\npip install optimum\n```\nOr install from source:\n```bash\npip install git+https://github.com/huggingface/optimum.git\n```\nThis command installs the current main dev version of Optimum, which could include latest developments(new features, bug fixes). However, the\nmain version might not be very stable. If you run into any problem, please open an [issue](https://github.com/huggingface/optimum/issues) so\nthat we can fix it as soon as possible. ## ORTTrainer\nThe [`ORTTrainer`] class inherits the [`Trainer`](https://huggingface.co/docs/transformers/main/en/main\\_classes/trainer#trainer)\nof Transformers. You can easily adapt the codes by replacing `Trainer` of transformers with `ORTTrainer` to take advantage of the acceleration\nempowered by ONNX Runtime. Here is an example of how to use `ORTTrainer` compared with `Trainer`:\n```diff\n-from transformers import Trainer, TrainingArguments\n+from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments\n# Step 1: Define training arguments\n-training\\_args = TrainingArguments(\n+training\\_args = ORTTrainingArguments(\noutput\\_dir=\""path/to/save/folder/\"",\n- optim = \""adamw\\_hf\"",\n+ optim=\""adamw\\_ort\\_fused\"",\n... )\n# Step 2: Create your ONNX Runtime Trainer\n-trainer = Trainer(\n+trainer = ORTTrainer(\nmodel=model,\nargs=training\\_args,\ntrain\\_dataset=train\\_dataset,\n+ feature=\""text-classification\"",\n... )\n# Step 3: Use ONNX Runtime for training!🤗\ntrainer.train()\n```\nCheck out more detailed [example scripts](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training) in the optimum repository. ## ORTSeq2SeqTrainer\nThe [`ORTSeq2SeqTrainer`] class is similar to the [`Seq2SeqTrainer`](https://huggingface.co/docs/transformers/main/en/main\\_classes/trainer#transformers.Seq2SeqTrainer)\nof Transformers. You can easily adapt the codes by replacing `Seq2SeqTrainer` of transformers with `ORTSeq2SeqTrainer` to take advantage of the acceleration\nempowered by ONNX Runtime. Here is an example of how to use `ORTSeq2SeqTrainer` compared with `Seq2SeqTrainer`:\n```diff\n-from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n+from optimum.onnxruntime import ORTSeq2SeqTrainer, ORTSeq2SeqTrainingArguments\n# Step 1: Define training arguments\n-training\\_args = Seq2SeqTrainingArguments(\n+training\\_args = ORTSeq2SeqTrainingArguments(\noutput\\_dir=\""path/to/save/folder/\"",\n- optim = \""adamw\\_hf\"",\n+ optim=\""adamw\\_ort\\_fused\"",\n... )\n# Step 2: Create your ONNX Runtime Seq2SeqTrainer\n-trainer = Seq2SeqTrainer(\n+trainer = ORTSeq2SeqTrainer(\nmodel=model,\nargs=training\\_args,\ntrain\\_dataset=train\\_dataset,\n+ feature=\""text2text-generation\"",\n... )\n# Step 3: Use ONNX Runtime for training!🤗\ntrainer.train()\n```\nCheck out more detailed [example scripts](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training) in the optimum repository."", ""In this blog post, you'll learn:\n- [1. What is Optimum? An ELI5](#1-what-is-optimum-an-eli5)\n- [2. New Optimum inference and pipeline features](#2-new-optimum-inference-and-pipeline-features)\n- [3. End-to-End tutorial on accelerating RoBERTa for Question-Answering including quantization and optimization](#3-end-to-end-tutorial-on-accelerating-roberta-for-question-answering-including-quantization-and-optimization)\n- [4. Current Limitations](#4-current-limitations)\n- [5. Optimum Inference FAQ](#5-optimum-inference-faq)\n- [6. What’s next?](#6-whats-next)\nLet's get started! 🚀\n## 1. What is Optimum? An ELI5\n[Hugging Face Optimum](https://github.com/huggingface/optimum) is an open-source library and an extension of [Hugging Face Transformers](https://github.com/huggingface/transformers), that provides a unified API of performance optimization tools to achieve maximum efficiency to train and run models on accelerated hardware, including toolkits for optimized performance on [Graphcore IPU](https://github.com/huggingface/optimum-graphcore) and [Habana Gaudi](https://github.com/huggingface/optimum-habana).""]","To install the latest version of Optimum with OpenVINO support, you can use the following command:

```bash
pip install --upgrade-strategy eager optimum[openvino,nncf]
```","pip install --upgrade-strategy eager optimum[""openvino""]"

user_input,retrieved_contexts,response,reference
"What architecture is the `tokenizers-linux-x64-musl` binary designed for?
","[""`tokenizers-linux-x64-musl`\nThis is the \\*\\*x86\\_64-unknown-linux-musl\\*\\* binary for `tokenizers`"", ""The latter applies low-precision recipes for deep-learning models to achieve optimal product objectives,\nsuch as inference performance and memory usage, with expected performance criteria. Neural Compressor supports post-training quantization, quantization-aware training and dynamic quantization. In order to specify the quantization approach, objective and performance criteria, the user must provide a configuration yaml file specifying the tuning parameters. The configuration file can either be hosted on the Hugging Face's Model Hub or can be given through a local directory path. ### 🔥 How to easily quantize Transformers for Intel Xeon CPUs with Optimum\n![Automatic quantization code snippet](assets/25\\_hardware\\_partners\\_program/carbon\\_inc\\_quantizer.png)\n## Follow 🤗 Optimum: a journey to democratize ML production performance\n### ⚡️State of the Art Hardware\nOptimum will focus on achieving optimal production performance on dedicated hardware, where software and hardware acceleration techniques can be applied for maximum efficiency."", ""The mitigation is rather simple, we sanitize the fact\nthat the offsets must be contiguous and non overlapping. - Proposal 5: The offsets could mismatch the declared shapes + dtype. This validated against. - Proposal 6: The file being mmaped could be modified while it's opened (attacker has access to your filesystem, seems like you're already pwned). - Proposal 7: serde JSON deserialization abuse (nothing so far: https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=serde). It doesn't mean there isn't a flaw. Same goes for the actual rust compiled binary. ```\npython safetensors\\_abuse\\_attempt\\_1.py\npython safetensors\\_abuse\\_attempt\\_2.py\npython safetensors\\_abuse\\_attempt\\_3.py\n```"", ""Most of the tokenizers used by the current state of the art language models need to be trained on a corpus that is similar to the one used to pre-train the language model. This training consists in learning rules to divide the text into tokens and the way to learn these rules and use them depends on the chosen tokenizer model. Thus, to train a new tokenizer it is first necessary to build a training corpus composed of raw texts. Then, you have to choose an architecture for your tokenizer. Here there are two options: the simplest is to reuse the same architecture as the one of a tokenizer used by another model already trained,otherwise it is also possible to completely design your tokenizer but it requires more experience and attention. Once the architecture is chosen, one can thus train this tokenizer on your constituted corpus. Finally, the last thing that you need to do is to save the learned rules to be able to use this tokenizer which is now ready to be used. Let's take an example: let's say you want to train a GPT-2 model on Python code. Even if the python code is in English this type of text is very specific and deserves a tokenizer trained on it - to convince you of this we will see at the end the difference produced on an example. For that we are going to use the method \""train\\_new\\_from\\_iterator\"" that all the fast tokenizers of the library have and thus in particular GPT2TokenizerFast."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Adding support for an unsupported architecture\nIf you wish to export a model whose architecture is not already supported by the library, the PR [#813 Adds support for ResNet](https://github.com/huggingface/optimum/pull/813 ) can be used as a reference. You can make sure tests pass for the new `my\\_new\\_modeltype` model type by running:\n```bash\npytest tests/exporters/tflite/test\\_\\*.py -k \""my\\_new\\_modeltype\"" -s --exitfirst\n```""]",The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.,x86_64-unknown-linux-musl
"What is the purpose of the BLIP-Diffusion model?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# BLIP-Diffusion\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. The abstract from the paper is:\n\\*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).\\*\nThe original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization. `BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## BlipDiffusionPipeline\n[[autodoc]] BlipDiffusionPipeline\n- all\n- \\_\\_call\\_\\_\n## BlipDiffusionControlNetPipeline\n[[autodoc]] BlipDiffusionControlNetPipeline\n- all\n- \\_\\_call\\_\\_"", ""ote: This is a simplified version of the code needed to create the Stable Diffusion demo."", ""However, we don't know \\\\(p(\\mathbf{x}\\_{t-1} | \\mathbf{x}\\_t)\\\\). It's intractable since it requires knowing the distribution of all possible images in order to calculate this conditional probability. Hence, we're going to leverage a neural network to \\*\\*approximate (learn) this conditional probability distribution\\*\\*, let's call it \\\\(p\\_\\theta (\\mathbf{x}\\_{t-1} | \\mathbf{x}\\_t)\\\\), with \\\\(\\theta\\\\) being the parameters of the neural network, updated by gradient descent. Ok, so we need a neural network to represent a (conditional) probability distribution of the backward process. If we assume this reverse process is Gaussian as well, then recall that any Gaussian distribution is defined by 2 parameters:\n\\* a mean parametrized by \\\\(\\mu\\_\\theta\\\\);\n\\* a variance parametrized by \\\\(\\Sigma\\_\\theta\\\\);\nso we can parametrize the process as\n$$ p\\_\\theta (\\mathbf{x}\\_{t-1} | \\mathbf{x}\\_t) = \\mathcal{N}(\\mathbf{x}\\_{t-1}; \\mu\\_\\theta(\\mathbf{x}\\_{t},t), \\Sigma\\_\\theta (\\mathbf{x}\\_{t},t))$$\nwhere the mean and variance are also conditioned on the noise level \\\\(t\\\\). Hence, our neural network needs to learn/represent the mean and variance. However, the DDPM authors decided to \\*\\*keep the variance fixed, and let the neural network only learn (represent) the mean \\\\(\\mu\\_\\theta\\\\) of this conditional probability distribution\\*\\*. From the paper:\n> First, we set \\\\(\\Sigma\\_\\theta ( \\mathbf{x}\\_t, t) = \\sigma^2\\_t \\mathbf{I}\\\\) to untrained time dependent constants. Experimentally, both \\\\(\\sigma^2\\_t = \\beta\\_t\\\\) and \\\\(\\sigma^2\\_t = \\tilde{\\beta}\\_t\\\\) (see paper) had similar results. This was then later improved in the [Improved diffusion models](https://openreview.net/pdf?id=-NEXDKk8gZ) paper, where a neural network also learns the variance of this backwards process, besides the mean. So we continue, assuming that our neural network only needs to learn/represent the mean of this conditional probability distribution. ## Defining an objective function (by reparametrizing the mean)\nTo derive an objective function to learn the mean of the backward process, the authors observe that the combination of \\\\(q\\\\) and \\\\(p\\_\\theta\\\\) can be seen as a variational auto-encoder (VAE) [(Kingma et al., 2013)](https://arxiv.org/abs/1312.6114). Hence, the \\*\\*variational lower bound\\*\\* (also called ELBO) can be used to minimize the negative log-likelihood with respect to ground truth data sample \\\\(\\mathbf{x}\\_0\\\\) (we refer to the VAE paper for details regarding ELBO)."", ""--\n{{ card\\_data }}\n---\n# {{ model\\_name | default(\""Diffusion Model\"") }}\n## Model description\nThis diffusion model is trained with the [🤗 Diffusers](https://github.com/huggingface/diffusers) library\non the `{{ dataset\\_name }}` dataset."", ""For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.\\*\\*\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!""]","The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation in text-to-image generation tasks. It creates novel renditions of an input subject based on text prompts while overcoming limitations such as lengthy fine-tuning and difficulties in preserving subject fidelity. BLIP-Diffusion introduces a new multimodal encoder pre-trained to provide subject representation, allowing for efficient fine-tuning and flexible combinations with existing techniques for novel subject-driven generation and editing applications.",The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.
"How can a user claim authorship of a paper on the Hugging Face Hub?
","[""Paper Pages\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png)\n\n## Linking a Paper to a model, dataset or Space\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository's tags. Clicking on the arxiv tag will let you:\n\\* Visit the Paper page. \\* Filter for other models or datasets on the Hub that cite the same paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\n\n## Claiming authorship to a Paper\nThe Hub will attempt to automatically match paper to users based on their email. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png)\n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \""claim authorship\""."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction"", ""Thanks a lot to Sascha Rothe, Shashi Narayan, and Aliaksei Severyn from\nGoogle Research, and Victor Sanh, Sylvain Gugger, and Thomas Wolf from\n🤗Hugging Face for proof-reading and giving very much appreciated\nfeedback."", ""Hub methods\nMethods for using the Hugging Face Hub:\n## Push to hub\n[[autodoc]] evaluate.push\\_to\\_hub"", ""Once you are satisfied, go to the webpage of your fork on GitHub. Click on \""Pull request\"". Make sure to add the GitHub handle of Patrick\nas one reviewer, so that the Hugging Face team gets notified for future changes. 6. Change the PR into a draft by clicking on \""Convert to draft\"" on the\nright of the GitHub pull request web page. In the following, whenever you have done some progress, don't forget to\ncommit your work and push it to your account so that it shows in the\npull request. Additionally, you should make sure to update your work\nwith the current main from time to time by doing:\ngit fetch upstream\ngit merge upstream/main\nIn general, all questions you might have regarding the model or your\nimplementation should be asked in your PR and discussed/solved in the\nPR. This way, Patrick will always be notified when you are\ncommitting new code or if you have a question. It is often very helpful\nto point Patrick to your added code so that the Hugging\nFace team can efficiently understand your problem or question. To do so, you can go to the \""Files changed\"" tab where you see all of\nyour changes, go to a line regarding which you want to ask a question,\nand click on the \""+\"" symbol to add a comment. Whenever a question or\nproblem has been solved, you can click on the \""Resolve\"" button of the\ncreated comment. In the same way, Patrick will open comments when reviewing\nyour code. We recommend asking most questions on GitHub on your PR.""]","A user can claim authorship of a paper on the Hugging Face Hub by clicking on their name in the corresponding Paper page and then selecting ""claim authorship."" The Hub will also attempt to automatically match the paper to users based on their email.","By clicking their name on the corresponding Paper page and clicking ""claim authorship"", then confirming the request in paper settings for admin team validation."
"What is the purpose of the /healthcheck endpoint in the Datasets server API?
","[""Datasets server API - rows endpoint\n> /rows endpoint\n## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### API service\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split"", ""Some of my requests failed because I deliberately omitted the `Content-Type` header. `![](assets/109_inference_endpoints/endpoints06.png)`\nFor additional details, I can check the full logs in the `Logs` tab. ```\n5c7fbb4485cd8w7 2022-10-10T08:19:04.915Z 2022-10-10 08:19:04,915 | INFO | POST / | Duration: 142.76 ms\n5c7fbb4485cd8w7 2022-10-10T08:19:05.860Z 2022-10-10 08:19:05,860 | INFO | POST / | Duration: 148.06 ms\n5c7fbb4485cd8w7 2022-10-10T09:21:39.251Z 2022-10-10 09:21:39,250 | ERROR | Content type \""None\"" not supported. Supported content types are: application/json, text/csv, text/plain, image/png, image/jpeg, image/jpg, image/tiff, image/bmp, image/gif, image/webp, image/x-image, audio/x-flac, audio/flac, audio/mpeg, audio/wave, audio/wav, audio/x-wav, audio/ogg, audio/x-audio, audio/webm, audio/webm;codecs=opus\n5c7fbb4485cd8w7 2022-10-10T09:21:44.114Z 2022-10-10 09:21:44,114 | ERROR | Content type \""None\"" not supported. Supported content types are: application/json, text/csv, text/plain, image/png, image/jpeg, image/jpg, image/tiff, image/bmp, image/gif, image/webp, image/x-image, audio/x-flac, audio/flac, audio/mpeg, audio/wave, audio/wav, audio/x-wav, audio/ogg, audio/x-audio, audio/webm, audio/webm;codecs=opus\n```\nNow, let's increase our security level and deploy a private endpoint. ### Deploying a Private Inference Endpoint\nRepeating the steps above, I select `Private` this time."", ""The dataset is generated with a Python generator, which doesn't load all the data in memory. As a result, the generator can handle large datasets. However, before the generated samples are flushed to the dataset file on disk, they are stored in an `ArrowWriter` buffer. This means the generated samples are written by batch. If your dataset samples consumes a lot of memory (images or videos), then make sure to specify a low value for the `DEFAULT\\_WRITER\\_BATCH\\_SIZE` attribute in [`DatasetBuilder`]. We recommend not exceeding a size of 200 MB. ## Maintaining integrity\nTo ensure a dataset is complete, [`load\\_dataset`] will perform a series of tests on the downloaded files to make sure everything is there. This way, you don't encounter any surprises when your requested dataset doesn't get generated as expected. [`load\\_dataset`] verifies:\n- The number of splits in the generated `DatasetDict`. - The number of samples in each split of the generated `DatasetDict`. - The list of downloaded files. - The SHA256 checksums of the downloaded files (disabled by defaut). If the dataset doesn't pass the verifications, it is likely that the original host of the dataset made some changes in the data files. If it is your own dataset, you'll need to recompute the information above and update the `README.md` file in your dataset repository. Take a look at this [section](dataset\\_script#optional-generate-dataset-metadata) to learn how to generate and update this metadata. In this case, an error is raised to alert that the dataset has changed. To ignore the error, one needs to specify `verification\\_mode=\""no\\_checks\""` in [`load\\_dataset`]. Anytime you see a verification error, feel free to open a discussion or pull request in the corresponding dataset \""Community\"" tab, so that the integrity checks for that dataset are updated. ## Security\nThe dataset repositories on the Hub are scanned for malware, see more information [here](https://huggingface.co/docs/hub/security#malware-scanning). Moreover the datasets without a namespace (originally contributed on our GitHub repository) have all been reviewed by our maintainers. The code of these datasets is considered \\*\\*safe\\*\\*. It concerns datasets that are not under a namespace, e.g. \""squad\"" or \""glue\"", unlike the other datasets that are named \""username/dataset\\_name\"" or \""org/dataset\\_name\""."", ""Hub API Endpoints\nWe have open endpoints that you can use to retrieve information from the Hub as well as perform certain actions such as creating model, dataset or Space repos. We offer a wrapper Python library, [`huggingface\\_hub`](https://github.com/huggingface/huggingface\\_hub), that allows easy access to these endpoints. We also provide [webhooks](./webhooks) to receive real-time incremental info about repos. Enjoy! The base URL for those endpoints below is `https://huggingface.co`. For example, to construct the `/api/models` call below, one can call the URL [https://huggingface.co/api/models](https://huggingface.co/api/models)\n## The Hub API Playground\nWant to try out our API? Try it out now on our [Playground](https://huggingface.co/spaces/enzostvs/hub-api-playground)! [![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/hub-api-playground.png)](https://huggingface.co/spaces/enzostvs/hub-api-playground)\n\n## Repo listing API\nThe following endpoints help get information about models, datasets, Spaces, and metrics stored on the Hub. When making API calls to retrieve information about repositories, the `createdAt` attribute indicates the time when the respective repository was created. It's important to note that there is a unique value, `2022-03-02T23:29:04.000Z` assigned to all repositories that were created before we began storing creation dates. ### GET /api/models\nGet information from all models in the Hub."", ""Datasets server maintenance job\n> Job to run maintenance actions on the datasets-server\nAvailable actions:\n- `backfill`: backfill the cache (i.e. create jobs to add the missing entries or update the outdated entries)\n- `collect-cache-metrics`: compute and store the cache metrics\n- `collect-queue-metrics`: compute and store the queue metrics\n- `clean-directory`: clean obsolete files/directories for a given path\n- `post-messages`: post messages in Hub discussions\n- `skip`: do nothing\n## Configuration\nThe script can be configured using environment variables. They are grouped by scope. - `DISCUSSIONS\\_BOT\\_ASSOCIATED\\_USER\\_NAME`: name of the Hub user associated with the Datasets Server bot app. - `DISCUSSIONS\\_BOT\\_TOKEN`: token of the Datasets Server bot used to post messages in Hub discussions. - `DISCUSSIONS\\_PARQUET\\_REVISION`: revision (branch) where the converted Parquet files are stored. ### Actions\nSet environment variables to configure the job (`CACHE\\_MAINTENANCE\\_` prefix):\n- `CACHE\\_MAINTENANCE\\_ACTION`: the action to launch, among `backfill`, `metrics`, `skip`. Defaults to `skip`. Specific to the backfill action:\n- `CACHE\\_MAINTENANCE\\_BACKFILL\\_ERROR\\_CODES\\_TO\\_RETRY`: the list of error codes to retry. Defaults to None. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Launch\n```shell\nmake run\n```""]",The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the app is running.,Ensure the app is running
"What is the default context window size for Local Attention in the LongT5 model?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# LongT5\n## Overview\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention. The abstract from the paper is the following:\n\\*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.\\*\nThis model was contributed by [stancld](https://huggingface.co/stancld). The original code can be found [here](https://github.com/google-research/longt5). ## Usage tips\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder \\*self-attention\\* layer with efficient either \\*local\\* attention or \\*transient-global\\* (\\*tglobal\\*) attention. - Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`]. - LongT5 model is designed to work efficiently and very well on long-range \\*sequence-to-sequence\\* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens. - For \\*Local Attention\\*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). \\*Local Attention\\* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l\\*r)`. - \\*Transient Global Attention\\* is an extension of the \\*Local Attention\\*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\nalso every global token like in the case of standard global attention (\\*transient\\* represents the fact the global tokens\nare constructed dynamically within each attention operation). As a consequence, \\*TGlobal\\* attention introduces\na few new parameters -- global relative position biases and a layer normalization for global token's embedding."", ""7B LlaMa is 7GB in memory). Instead of training the original weights directly, LoRA adds small adapter layers on top of some specific layers (usually the attention layers); thus, the number of trainable parameters is drastically reduced. In this scenario, a rule of thumb is to allocate ~1.2-1.4GB per billion parameters (depending on the batch size and sequence length) to fit the entire fine-tuning setup. As detailed in the attached blog post above, this enables fine-tuning larger models (up to 50-60B scale models on a NVIDIA A100 80GB) at low cost. These techniques have enabled fine-tuning large models on consumer devices and Google Colab. Notable demos are fine-tuning `facebook/opt-6.7b` (13GB in `float16` ), and `openai/whisper-large` on Google Colab (15GB GPU RAM). To learn more about using `peft`, refer to our [github repo](https://github.com/huggingface/peft) or the [previous blog post](https://huggingface.co/blog/trl-peft)(https://huggingface.co/blog/trl-peft)) on training 20b parameter models on consumer hardware."", ""We multiply this matrix with the values ( \\\\(V\\\\)) to obtain a linear approximation of self-attention. Note that we never calculated the product \\\\(QK^T\\\\), avoiding the \\\\(O(n^2)\\\\) complexity. ## How do we select landmarks? Instead of sampling \\\\(m\\\\) rows from \\\\(Q\\\\) and \\\\(K\\\\), the authors propose to construct \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\)\nusing segment means. In this procedure, \\\\(n\\\\) tokens are grouped into \\\\(m\\\\) segments, and the mean of each segment is computed. Ideally, \\\\(m\\\\) is much smaller than \\\\(n\\\\). According to experiments from the paper, selecting just \\\\(32\\\\) or \\\\(64\\\\) landmarks produces competetive performance compared to standard self-attention and other efficient attention mechanisms, even for long sequences lengths ( \\\\(n=4096\\\\) or \\\\(8192\\\\)). The overall algorithm is summarised by the following figure from the paper:\n\nEfficient self-attention with the Nyström method\n\nThe three orange matrices above correspond to the three matrices we constructed using the key and query landmarks."", ""With our sliding window approach, however, there is overlap in\nthe tokens we pass to the model at each iteration. We don't want the log-likelihood for the tokens we're just treating\nas context to be included in our loss, so we can set these targets to `-100` so that they are ignored. The following\nis an example of how we could do this with a stride of `512`. This means that the model will have at least 512 tokens\nfor context when calculating the conditional likelihood of any one token (provided there are 512 preceding tokens\navailable to condition on). ```python\nimport torch\nfrom tqdm import tqdm\nmax\\_length = model.config.n\\_positions\nstride = 512\nseq\\_len = encodings.input\\_ids.size(1)\nnlls = []\nprev\\_end\\_loc = 0\nfor begin\\_loc in tqdm(range(0, seq\\_len, stride)):\nend\\_loc = min(begin\\_loc + max\\_length, seq\\_len)\ntrg\\_len = end\\_loc - prev\\_end\\_loc # may be different from stride on last loop\ninput\\_ids = encodings.input\\_ids[:, begin\\_loc:end\\_loc].to(device)\ntarget\\_ids = input\\_ids.clone()\ntarget\\_ids[:, :-trg\\_len] = -100\nwith torch.no\\_grad():\noutputs = model(input\\_ids, labels=target\\_ids)\n# loss is calculated using CrossEntropyLoss which averages over valid labels\n# N.B. the model only calculates loss over trg\\_len - 1 labels, because it internally shifts the labels\n# to the left by 1. neg\\_log\\_likelihood = outputs.loss\nnlls.append(neg\\_log\\_likelihood)\nprev\\_end\\_loc = end\\_loc\nif end\\_loc == seq\\_len:\nbreak\nppl = torch.exp(torch.stack(nlls).mean())\n```\nRunning this with the stride length equal to the max input length is equivalent to the suboptimal, non-sliding-window\nstrategy we discussed above. The smaller the stride, the more context the model will have in making each prediction,\nand the better the reported perplexity will typically be. When we run the above with `stride = 1024`, i.e. no overlap, the resulting PPL is `19.44`, which is about the same\nas the `19.93` reported in the GPT-2 paper. By using `stride = 512` and thereby employing our striding window\nstrategy, this jumps down to `16.45`. This is not only a more favorable score, but is calculated in a way that is\ncloser to the true autoregressive decomposition of a sequence likelihood."", ""![](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/mixtral-7b-inference-large-seqlen.png)\n\n### Sliding window Attention\nThe current implementation supports the sliding window attention mechanism and memory efficient cache management. To enable sliding window attention, just make sure to have a `flash-attn` version that is compatible with sliding window attention (`>=2.3.0`). The Flash Attention-2 model uses also a more memory efficient cache slicing mechanism - as recommended per the official implementation of Mistral model that use rolling cache mechanism we keep the cache size fixed (`self.config.sliding\\_window`), support batched generation only for `padding\\_side=\""left\""` and use the absolute position of the current token to compute the positional embedding. ## The Mistral Team\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed. ## MixtralConfig\n[[autodoc]] MixtralConfig\n## MixtralModel\n[[autodoc]] MixtralModel\n- forward\n## MixtralForCausalLM\n[[autodoc]] MixtralForCausalLM\n- forward\n## MixtralForSequenceClassification\n[[autodoc]] MixtralForSequenceClassification\n- forward""]","The default context window size for Local Attention in the LongT5 model is `r=127`, which means a given token can attend to 127 tokens to the left and right of it.",127 tokens
"What method is used to load a checkpoint for a task using `AutoPipeline`?
","[""This is a custom pipeline that merges upto 3 pretrained model checkpoints as long as they are in the HuggingFace model\\_index.json format. The checkpoint merging is currently memory intensive as it modifies the weights of a DiffusionPipeline object in place. Expect atleast 13GB RAM Usage on Kaggle GPU kernels and\non colab you might run out of the 12GB memory even while merging two checkpoints. Usage:-\n```python\nfrom diffusers import DiffusionPipeline\n#Return a CheckpointMergerPipeline class that allows you to merge checkpoints. #The checkpoint passed here is ignored. But still pass one of the checkpoints you plan to\n#merge for convenience\npipe = DiffusionPipeline.from\\_pretrained(\""CompVis/stable-diffusion-v1-4\"", custom\\_pipeline=\""checkpoint\\_merger\"")\n#There are multiple possible scenarios:\n#The pipeline with the merged checkpoints is returned in all the scenarios\n#Compatible checkpoints a.k.a matched model\\_index.json files. Ignores the meta attributes in model\\_index.json during comparison.( attrs with \\_ as prefix )\nmerged\\_pipe = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""CompVis/stable-diffusion-v1-2\""], interp = \""sigmoid\"", alpha = 0.4)\n#Incompatible checkpoints in model\\_index.json but merge might be possible. Use force = True to ignore model\\_index.json compatibility\nmerged\\_pipe\\_1 = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""hakurei/waifu-diffusion\""], force = True, interp = \""sigmoid\"", alpha = 0.4)\n#Three checkpoint merging. Only \""add\\_difference\"" method actually works on all three checkpoints. Using any other options will ignore the 3rd checkpoint. merged\\_pipe\\_2 = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""hakurei/waifu-diffusion\"",\""prompthero/openjourney\""], force = True, interp = \""add\\_difference\"", alpha = 0.4)\nprompt = \""An astronaut riding a horse on Mars\""\nimage = merged\\_pipe(prompt).images[0]\n```\nSome examples along with the merge details:\n1. \""CompVis/stable-diffusion-v1-4\"" + \""hakurei/waifu-diffusion\"" ; Sigmoid interpolation; alpha = 0.8\n![Stable plus Waifu Sigmoid 0.8](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/stability\\_v1\\_4\\_waifu\\_sig\\_0.8.png)\n2. \""hakurei/waifu-diffusion\"" + \""prompthero/openjourney\"" ; Inverse Sigmoid interpolation; alpha = 0.8\n![Stable plus Waifu Sigmoid 0.8](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/waifu\\_openjourney\\_inv\\_sig\\_0.8.png)\n3. \""CompVis/stable-diffusion-v1-4\"" + \""hakurei/waifu-diffusion\"" + \""prompthero/openjourney\""; Add Difference interpolation; alpha = 0.5\n![Stable plus Waifu plus openjourney add\\_diff 0.5](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/stable\\_waifu\\_openjourney\\_add\\_diff\\_0.5.png)\n### Stable Diffusion Comparisons\nThis Community Pipeline enables the comparison between the 4 checkpoints that exist for Stable Diffusion."", ""While you can still use the pipelines just as before, we will stop testing the pipelines and will not accept any changes to existing files."", ""## DiTPipeline\n[[autodoc]] DiTPipeline\n- all\n- \\_\\_call\\_\\_\n## ImagePipelineOutput\n[[autodoc]] pipelines.ImagePipelineOutput"", ""i.e. you will need to re-initialize the deepspeed engine, since\n`model.load\\_state\\_dict(state\\_dict)` will remove all the DeepSpeed magic from it. So do this only at the very end\nof the training. Of course, you don't have to use class:\\*~transformers.Trainer\\* and you can adjust the examples above to your own\ntrainer. If for some reason you want more refinement, you can also extract the fp32 `state\\_dict` of the weights and apply\nthese yourself as is shown in the following example:\n```python\nfrom deepspeed.utils.zero\\_to\\_fp32 import get\\_fp32\\_state\\_dict\\_from\\_zero\\_checkpoint\nstate\\_dict = get\\_fp32\\_state\\_dict\\_from\\_zero\\_checkpoint(checkpoint\\_dir) # already on cpu\nmodel = model.cpu()\nmodel.load\\_state\\_dict(state\\_dict)\n```\n\\*\\*Offline FP32 Weights Recovery:\\*\\*\nDeepSpeed creates a special conversion script `zero\\_to\\_fp32.py` which it places in the top-level of the checkpoint\nfolder. Using this script you can extract the weights at any point. The script is standalone and you no longer need to\nhave the configuration file or a `Trainer` to do the extraction. Let's say your checkpoint folder looks like this:\n```bash\n$ ls -l output\\_dir/checkpoint-1/\n-rw-rw-r-- 1 stas stas 1.4K Mar 27 20:42 config.json\ndrwxrwxr-x 2 stas stas 4.0K Mar 25 19:52 global\\_step1/\n-rw-rw-r-- 1 stas stas 12 Mar 27 13:16 latest\n-rw-rw-r-- 1 stas stas 827K Mar 27 20:42 optimizer.pt\n-rw-rw-r-- 1 stas stas 231M Mar 27 20:42 pytorch\\_model.bin\n-rw-rw-r-- 1 stas stas 623 Mar 27 20:42 scheduler.pt\n-rw-rw-r-- 1 stas stas 1.8K Mar 27 20:42 special\\_tokens\\_map.json\n-rw-rw-r-- 1 stas stas 774K Mar 27 20:42 spiece.model\n-rw-rw-r-- 1 stas stas 1.9K Mar 27 20:42 tokenizer\\_config.json\n-rw-rw-r-- 1 stas stas 339 Mar 27 20:42 trainer\\_state.json\n-rw-rw-r-- 1 stas stas 2.3K Mar 27 20:42 training\\_args.bin\n-rwxrw-r-- 1 stas stas 5.5K Mar 27 13:16 zero\\_to\\_fp32.py\\*\n```\nIn this example there is just one DeepSpeed checkpoint sub-folder \\*global\\_step1\\*."", ""Deprecated Pipelines\nThis folder contains pipelines that have very low usage as measured by model downloads, issues and PRs.""]",The context does not provide information about `AutoPipeline` or its methods for loading a checkpoint. Please refer to the relevant documentation or resources for details on `AutoPipeline`.,from_pretrained()
"What is the purpose of Diffusers library?
","[""🧨 Diffusers Experimental\nWe are adding experimental code to support novel applications and usages of the Diffusers library."", ""Models\nFor more detail on the models, please refer to the [docs](https://huggingface.co/docs/diffusers/api/models/overview)."", ""This design philosophy is reflected in multiple parts of the library:\n- We follow PyTorch's API with methods like [`DiffusionPipeline.to`](https://huggingface.co/docs/diffusers/main/en/api/diffusion\\_pipeline#diffusers.DiffusionPipeline.to) to let the user handle device management. - Raising concise error messages is preferred to silently correct erroneous input. Diffusers aims at teaching the user, rather than making the library as easy to use as possible. - Complex model vs. scheduler logic is exposed instead of magically handled inside. Schedulers/Samplers are separated from diffusion models with minimal dependencies on each other. This forces the user to write the unrolled denoising loop. However, the separation allows for easier debugging and gives the user more control over adapting the denoising process or switching out diffusion models or schedulers. - Separately trained components of the diffusion pipeline, \\*e.g.\\* the text encoder, the UNet, and the variational autoencoder, each has their own model class."", ""--\ntitle: \""Ethical Guidelines for developing the Diffusers library\""\nthumbnail: /blog/assets/ethics-diffusers/thumbnail.png\nauthors:\n- user: giadap\n---\n# Ethical guidelines for developing the Diffusers library\nWe are on a journey to make our libraries more responsible, one commit at a time! As part of the [Diffusers library documentation](https://huggingface.co/docs/diffusers/main/en/index), we are proud to announce the publication of an [ethical framework](https://huggingface.co/docs/diffusers/main/en/conceptual/ethical\\_guidelines). Given diffusion models' real case applications in the world and potential negative impacts on society, this initiative aims to guide the technical decisions of the Diffusers library maintainers about community contributions. We wish to be transparent in how we make decisions, and above all, we aim to clarify what values guide those decisions. We see ethics as a process that leverages guiding values, concrete actions, and continuous adaptation. For this reason, we are committed to adjusting our guidelines over time, following the evolution of the Diffusers project and the valuable feedback from the community that keeps it alive. # Ethical guidelines\n\\* \\*\\*Transparency\\*\\*: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions. \\* \\*\\*Consistency\\*\\*: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. \\* \\*\\*Simplicity\\*\\*: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project’s goals lean and coherent. \\* \\*\\*Accessibility\\*\\*: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community. \\* \\*\\*Reproducibility\\*\\*: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library."", ""For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.\\*\\*\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!""]","The purpose of the Diffusers library is to support novel applications and usages of diffusion models. It aims to provide a framework that allows users to handle device management, exposes complex model and scheduler logic for easier debugging, and gives users control over adapting the denoising process. The library also emphasizes ethical guidelines to ensure responsible development and community contributions.",To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.
"What method does the EulerAncestralDiscreteScheduler use for sampling?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# EulerAncestralDiscreteScheduler\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling\\_euler\\_ancestral\\_discrete.EulerAncestralDiscreteSchedulerOutput"", ""We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.\\*\n## ScoreSdeVeScheduler\n[[autodoc]] ScoreSdeVeScheduler\n## SdeVeOutput\n[[autodoc]] schedulers.scheduling\\_sde\\_ve.SdeVeOutput"", ""ConsistencyDecoderScheduler\nThis scheduler is a part of the [`ConsistencyDecoderPipeline`] and was introduced in [DALL-E 3](https://openai.com/dall-e-3). The original codebase can be found at [openai/consistency\\_models](https://github.com/openai/consistency\\_models). ## ConsistencyDecoderScheduler\n[[autodoc]] schedulers.scheduling\\_consistency\\_decoder.ConsistencyDecoderScheduler"", ""This helps the model determine its position in the diffusion process, whether it is closer to the start or the end. Use the `sample` method to get the model output:\n```py\n>>> with torch.no\\_grad():\n... noisy\\_residual = model(sample=noisy\\_sample, timestep=2).sample\n```\nTo generate actual examples though, you'll need a scheduler to guide the denoising process. In the next section, you'll learn how to couple a model with a scheduler. ## Schedulers\nSchedulers manage going from a noisy sample to a less noisy sample given the model output - in this case, it is the `noisy\\_residual`. 🧨 Diffusers is a toolbox for building diffusion systems. While the [`DiffusionPipeline`] is a convenient way to get started with a pre-built diffusion system, you can also choose your own model and scheduler components separately to build a custom diffusion system. For the quicktour, you'll instantiate the [`DDPMScheduler`] with its [`~diffusers.ConfigMixin.from\\_config`] method:\n```py\n>>> from diffusers import DDPMScheduler\n>>> scheduler = DDPMScheduler.from\\_pretrained(repo\\_id)\n>>> scheduler\nDDPMScheduler {\n\""\\_class\\_name\"": \""DDPMScheduler\"",\n\""\\_diffusers\\_version\"": \""0.21.4\"",\n\""beta\\_end\"": 0.02,\n\""beta\\_schedule\"": \""linear\"",\n\""beta\\_start\"": 0.0001,\n\""clip\\_sample\"": true,\n\""clip\\_sample\\_range\"": 1.0,\n\""dynamic\\_thresholding\\_ratio\"": 0.995,\n\""num\\_train\\_timesteps\"": 1000,\n\""prediction\\_type\"": \""epsilon\"",\n\""sample\\_max\\_value\"": 1.0,\n\""steps\\_offset\"": 0,\n\""thresholding\"": false,\n\""timestep\\_spacing\"": \""leading\"",\n\""trained\\_betas\"": null,\n\""variance\\_type\"": \""fixed\\_small\""\n}\n```\n💡 Unlike a model, a scheduler does not have trainable weights and is parameter-free!"", ""Schedulers\nFor more information on the schedulers, please refer to the [docs](https://huggingface.co/docs/diffusers/api/schedulers/overview).""]",The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling.,Ancestral sampling with Euler method steps.
"What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# FLAVA\n## Overview\nThe FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022. The paper aims at creating a single unified foundation model which can work across vision, language\nas well as vision-and-language multimodal tasks. The abstract from the paper is the following:\n\\*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety\nof downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal\n(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising\ndirection would be to use a single holistic universal model, as a \""foundation\"", that targets all modalities\nat once -- a true vision and language foundation model should be good at vision tasks, language tasks, and\ncross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate\nimpressive performance on a wide range of 35 tasks spanning these target modalities.\\*\nThis model was contributed by [aps](https://huggingface.co/aps). The original code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava). ## FlavaConfig\n[[autodoc]] FlavaConfig\n## FlavaTextConfig\n[[autodoc]] FlavaTextConfig\n## FlavaImageConfig\n[[autodoc]] FlavaImageConfig\n## FlavaMultimodalConfig\n[[autodoc]] FlavaMultimodalConfig\n## FlavaImageCodebookConfig\n[[autodoc]] FlavaImageCodebookConfig\n## FlavaProcessor\n[[autodoc]] FlavaProcessor\n## FlavaFeatureExtractor\n[[autodoc]] FlavaFeatureExtractor\n## FlavaImageProcessor\n[[autodoc]] FlavaImageProcessor\n- preprocess\n## FlavaForPreTraining\n[[autodoc]] FlavaForPreTraining\n- forward\n## FlavaModel\n[[autodoc]] FlavaModel\n- forward\n- get\\_text\\_features\n- get\\_image\\_features\n## FlavaImageCodebook\n[[autodoc]] FlavaImageCodebook\n- forward\n- get\\_codebook\\_indices\n- get\\_codebook\\_probs\n## FlavaTextModel\n[[autodoc]] FlavaTextModel\n- forward\n## FlavaImageModel\n[[autodoc]] FlavaImageModel\n- forward\n## FlavaMultimodalModel\n[[autodoc]] FlavaMultimodalModel\n- forward"", ""Also, thanks to Niels Rogge, Younes Belkada, and Suraj Patil, among many others at Hugging Face, who laid out the foundations for increasing the use of multi-modal models from Transformers.\\*"", ""{\n... \""score\"": round(pred[\""score\""], 4),\n... \""token\"": pred[\""token\""],\n... \""token\\_str\"": pred[\""token\\_str\""],\n... \""sequence\"": pred[\""sequence\""],\n... }\n... for pred in preds\n... ]\n>>> preds\n[{'score': 0.2236,\n'token': 1761,\n'token\\_str': ' platform',\n'sequence': 'Hugging Face is a community-based open-source platform for machine learning.'}]\n```\n## Multimodal\nMultimodal tasks require a model to process multiple data modalities (text, image, audio, video) to solve a particular problem. Image captioning is an example of a multimodal task where the model takes an image as input and outputs a sequence of text describing the image or some properties of the image. Although multimodal models work with different data types or modalities, internally, the preprocessing steps help the model convert all the data types into embeddings (vectors or list of numbers that holds meaningful information about the data). For a task like image captioning, the model learns relationships between image embeddings and text embeddings. ### Document question answering\nDocument question answering is a task that answers natural language questions from a document. Unlike a token-level question answering task which takes text as input, document question answering takes an image of a document as input along with a question about the document and returns an answer. Document question answering can be used to parse structured documents and extract key information from it. In the example below, the total amount and change due can be extracted from a receipt. ```py\n>>> from transformers import pipeline\n>>> from PIL import Image\n>>> import requests\n>>> url = \""https://datasets-server.huggingface.co/assets/hf-internal-testing/example-documents/--/hf-internal-testing--example-documents/test/2/image/image.jpg\""\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> doc\\_question\\_answerer = pipeline(\""document-question-answering\"", model=\""magorshunov/layoutlm-invoices\"")\n>>> preds = doc\\_question\\_answerer(\n..."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# XLM-RoBERTa-XL\n## Overview\nThe XLM-RoBERTa-XL model was proposed in [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau. The abstract from the paper is the following:\n\\*Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI."", ""Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality. Keywords: Large models, Training, Fine-tuning, Deployment, Multi-modal\n## [pyserini](https://github.com/castorini/pyserini)\n[pyserini](https://github.com/castorini/pyserini) is a Python toolkit for reproducible information retrieval research with sparse and dense representations. Retrieval using sparse representations is provided via integration with the group's Anserini IR toolkit. Retrieval using dense representations is provided via integration with Facebook's Faiss library. Keywords: IR, Information Retrieval, Dense, Sparse\n## [baal](https://github.com/baal-org/baal)\n[baal](https://github.com/baal-org/baal) is an active learning library that supports both industrial applications and research usecases. [baal](https://github.com/baal-org/baal) currently supports Monte-Carlo Dropout, MCDropConnect, deep ensembles, and semi-supervised learning. Keywords: Active Learning, Research, Labeling\n## [cleanlab](https://github.com/cleanlab/cleanlab)\n[cleanlab](https://github.com/cleanlab/cleanlab) is the standard data-centric AI package for data quality and machine learning with messy, real-world data and labels. For text, image, tabular, audio (among others) datasets, you can use cleanlab to automatically: detect data issues (outliers, label errors, near duplicates, etc), train robust ML models, infer consensus + annotator-quality for multi-annotator data, suggest data to (re)label next (active learning). Keywords: Data-Centric AI, Data Quality, Noisy Labels, Outlier Detection, Active Learning\n## [BentoML](https://github.com/bentoml/BentoML)\n[BentoML](https://github.com/bentoml) is the unified framework for for building, shipping, and scaling production-ready AI applications incorporating traditional ML, pre-trained AI models, Generative and Large Language Models. All Hugging Face models and pipelines can be seamlessly integrated into BentoML applications, enabling the running of models on the most suitable hardware and independent scaling based on usage. Keywords: BentoML, Framework, Deployment, AI Applications\n## [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning)\n[LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) offers a user-friendly fine-tuning framework that incorporates PEFT. The repository includes training(fine-tuning) and inference examples for LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, and other LLMs. A ChatGLM version is also available in [ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning). Keywords: PEFT, fine-tuning, LLaMA-2, ChatGLM, Qwen""]",The large multimodal model that can solve image-text tasks and is based on Flamingo is called FLAVA.,IDEFICS
"What is the purpose of the `gradio.Blocks` API?
","[""Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our \\*\\*newly developed\\*\\* low-level API called `gradio.Blocks`. Now, what's the difference between `Interface` and `Blocks`? - ⚡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs. - 🧱 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \""building blocks\""). ### Why Blocks 🧱?[[why-blocks-]]\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\n- Group together related demos as multiple tabs in one web application\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a dropdown) or its visibility based on user input\nWe will explore all of these concepts below. ### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook. ```py\nimport gradio as gr\ndef flip\\_text(x):\nreturn x[::-1]\ndemo = gr.Blocks()\nwith demo:\ngr.Markdown(\n\""\""\""\n# Flip Text! Start typing below to see the output. \""\""\""\n)\ninput = gr.Textbox(placeholder=\""Flip this text\"")\noutput = gr.Textbox()\ninput.change(fn=flip\\_text, inputs=input, outputs=output)\ndemo.launch()\n```\nThis simple example above introduces 4 concepts that underlie Blocks:\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context."", ""\\*\\*Prerequisites\\*\\*: This Guide uses [Gradio Blocks](/guides/quickstart/#blocks-more-flexibility-and-control), so make your are familiar with the Blocks class. ## Setting up your BigQuery Credentials\nTo use Gradio with BigQuery, you will need to obtain your BigQuery credentials and use them with the [BigQuery Python client](https://pypi.org/project/google-cloud-bigquery/). If you already have BigQuery credentials (as a `.json` file), you can skip this section. If not, you can do this for free in just a couple of minutes. 1. First, log in to your Google Cloud account and go to the Google Cloud Console (https://console.cloud.google.com/)\n2. In the Cloud Console, click on the hamburger menu in the top-left corner and select \""APIs & Services\"" from the menu. If you do not have an existing project, you will need to create one. 3. Then, click the \""+ Enabled APIs & services\"" button, which allows you to enable specific services for your project. Search for \""BigQuery API\"", click on it, and click the \""Enable\"" button. If you see the \""Manage\"" button, then the BigQuery is already enabled, and you're all set."", ""mport { Meta } from \""@storybook/blocks\"";\n\n![Gradio Logo](./gradio.svg)\n\nWelcome to the Gradio Storybook\n\nThis is Gradio's component library. It is a collection of reusable components that are used to build Gradio interfaces. Here you'll find documentation for each component, the props they take, and the visual variations they have, as well as some examples of how they can be used. This is still a work in progress and we welcome any contributions. Resources\n\n* [Documentation](https://gradio.app)\n* [Guides](https://gradio.app/guides/)\n* [GitHub](https://github.com/gradio-app/gradio)\n* [Discord](https://discord.com/invite/feTf9x3ZSB)\n* [Hugging Face Forum](https://discuss.huggingface.co/c/gradio/26)\n\nFeedback\n\nIf you have any questions, issues, or feedback on our components, please refer to our Discord or raise an issue in our GitHub repo."", ""You can also specify what file types it should accept in the form of a list (ex: `image`, `video`, `audio`, `text`, or generic `file`). Added by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 2591](https://github.com/gradio-app/gradio/pull/2591). Example of how it can be used:\n```python\nimport gradio as gr\ndef upload_file(files):\nfile_paths = [file.name for file in files]\nreturn file_paths\nwith gr.Blocks() as demo:\nfile_output = gr.File()\nupload_button = gr.UploadButton(\""Click to Upload a File\"", file_types=[\""image\"", \""video\""], file_count=\""multiple\"")\nupload_button.upload(upload_file, upload_button, file_output)\ndemo.launch()\n```\n###### Revamped API documentation page\nNew API Docs page with in-browser playground and updated aesthetics. [@gary149](https://github.com/gary149) in [PR 2652](https://github.com/gradio-app/gradio/pull/2652)\n###### Revamped Login page\nPreviously our login page had its own CSS, had no dark mode, and had an ugly json message on the wrong credentials. Made the page more aesthetically consistent, added dark mode support, and a nicer error message. [@aliabid94](https://github.com/aliabid94) in [PR 2684](https://github.com/gradio-app/gradio/pull/2684)\n###### Accessing the Requests Object Directly\nYou can now access the Request object directly in your Python function by [@abidlabs](https://github.com/abidlabs) in [PR 2641](https://github.com/gradio-app/gradio/pull/2641)."", ""Gradio Demo: leaderboard\n### A simple dashboard ranking spaces by number of likes. ```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nimport requests\nimport pandas as pd\nfrom huggingface\\_hub.hf\\_api import SpaceInfo\npath = f\""https://huggingface.co/api/spaces\""\ndef get\\_blocks\\_party\\_spaces():\nr = requests.get(path)\nd = r.json()\nspaces = [SpaceInfo(\\*\\*x) for x in d]\nblocks\\_spaces = {}\nfor i in range(0,len(spaces)):\nif spaces[i].id.split('/')[0] == 'Gradio-Blocks' and hasattr(spaces[i], 'likes') and spaces[i].id != 'Gradio-Blocks/Leaderboard' and spaces[i].id != 'Gradio-Blocks/README':\nblocks\\_spaces[spaces[i].id]=spaces[i].likes\ndf = pd.DataFrame(\n[{\""Spaces\\_Name\"": Spaces, \""likes\"": likes} for Spaces,likes in blocks\\_spaces.items()])\ndf = df.sort\\_values(by=['likes'],ascending=False)\nreturn df\nblock = gr.Blocks()\nwith block:\ngr.Markdown(\""\""\""Leaderboard for the most popular Blocks Event Spaces. To learn more and join, see [Blocks Party Event](https://huggingface.co/Gradio-Blocks)\""\""\"")\nwith gr.Tabs():\nwith gr.TabItem(\""Blocks Party Leaderboard\""):\nwith gr.Row():\ndata = gr.Dataframe(type=\""pandas\"")\nwith gr.Row():\ndata\\_run = gr.Button(\""Refresh\"")\ndata\\_run.click(get\\_blocks\\_party\\_spaces, inputs=None, outputs=data)\n# running the function on page load in addition to when the button is clicked\nblock.load(get\\_blocks\\_party\\_spaces, inputs=None, outputs=data)\nblock.launch()\n```""]","The `gradio.Blocks` API is a low-level API that allows you to have full control over the data flows and layout of your application. It provides more flexibility compared to the high-level `Interface` API, enabling you to build complex, multi-step applications. With `Blocks`, you can:

- Group related demos as multiple tabs in one web application.
- Change the layout of your demo, specifying where inputs and outputs are located.
- Create multi-step interfaces where the output of one model becomes the input to the next model.
- Modify a component's properties or visibility based on user input.

Overall, `gradio.Blocks` is designed for users who need more control and customization in their Gradio applications.","The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications."
"What is the purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents""?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# unCLIP\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in 🤗 Diffusers comes from kakaobrain's [karlo](https://github.com/kakaobrain/karlo). The abstract from the paper is following:\n\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\*\nYou can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines."", ""While CLIPSeg is trained on simple object descriptions (e.g., “a car”), its CLIP backbone is pre-trained on engineered text templates (e.g., “an image of a car”, “a photo of a car”) and kept frozen during training. Once the inputs are preprocessed, we can perform inference to get a binary segmentation map of shape (height, width) for each text query. ```py\nimport torch\nwith torch.no\\_grad():\noutputs = model(\\*\\*inputs)\nlogits = outputs.logits\nprint(logits.shape)\n>>> torch.Size([3, 352, 352])\n```\nLet’s visualize the results to see how well CLIPSeg performed (code is adapted from [this post](https://huggingface.co/blog/clipseg-zero-shot)). ```py\nimport matplotlib.pyplot as plt\nlogits = logits.unsqueeze(1)\n\\_, ax = plt.subplots(1, len(texts) + 1, figsize=(3\\*(len(texts) + 1), 12))\n[a.axis('off') for a in ax.flatten()]\nax[0].imshow(image)\n[ax[i+1].imshow(torch.sigmoid(logits[i][0])) for i in range(len(texts))];\n[ax[i+1].text(0, -15, prompt) for i, prompt in enumerate(texts)]\n```\n\n![CLIPSeg results](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/clipseg_result.png)\n\nAmazing, isn’t it?"", ""🌎\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we will review it. The resource should ideally demonstrate something new instead of duplicating an existing resource. ## CLIPConfig\n[[autodoc]] CLIPConfig\n- from\\_text\\_vision\\_configs\n## CLIPTextConfig\n[[autodoc]] CLIPTextConfig\n## CLIPVisionConfig\n[[autodoc]] CLIPVisionConfig\n## CLIPTokenizer\n[[autodoc]] CLIPTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## CLIPTokenizerFast\n[[autodoc]] CLIPTokenizerFast\n## CLIPImageProcessor\n[[autodoc]] CLIPImageProcessor\n- preprocess\n## CLIPFeatureExtractor\n[[autodoc]] CLIPFeatureExtractor\n## CLIPProcessor\n[[autodoc]] CLIPProcessor\n\n## CLIPModel\n[[autodoc]] CLIPModel\n- forward\n- get\\_text\\_features\n- get\\_image\\_features\n## CLIPTextModel\n[[autodoc]] CLIPTextModel\n- forward\n## CLIPTextModelWithProjection\n[[autodoc]] CLIPTextModelWithProjection\n- forward\n## CLIPVisionModelWithProjection\n[[autodoc]] CLIPVisionModelWithProjection\n- forward\n## CLIPVisionModel\n[[autodoc]] CLIPVisionModel\n- forward\n\n## TFCLIPModel\n[[autodoc]] TFCLIPModel\n- call\n- get\\_text\\_features\n- get\\_image\\_features\n## TFCLIPTextModel\n[[autodoc]] TFCLIPTextModel\n- call\n## TFCLIPVisionModel\n[[autodoc]] TFCLIPVisionModel\n- call\n\n## FlaxCLIPModel\n[[autodoc]] FlaxCLIPModel\n- \\_\\_call\\_\\_\n- get\\_text\\_features\n- get\\_image\\_features\n## FlaxCLIPTextModel\n[[autodoc]] FlaxCLIPTextModel\n- \\_\\_call\\_\\_\n## FlaxCLIPTextModelWithProjection\n[[autodoc]] FlaxCLIPTextModelWithProjection\n- \\_\\_call\\_\\_\n## FlaxCLIPVisionModel\n[[autodoc]] FlaxCLIPVisionModel\n- \\_\\_call\\_\\_"", ""These approaches model the process of image generation as a step-wise probabilistic processes and leverage large amounts of compute and data to learn the image distribution. This methodology of improving performance need not be confined to images. This paper describes a way to apply advances in the image generative domain to speech synthesis. The result is TorToise - an expressive, multi-voice text-to-speech system.\\*\nThis model was contributed by [Susnato Dhar](https://huggingface.co/susnato). The original code can be found [here](https://github.com/neonbjb/tortoise-tts). ## Usage tips\n1. CLVP is an integral part of the Tortoise TTS model. 2. CLVP can be used to compare different generated speech candidates with the provided text, and the best speech tokens are forwarded to the diffusion model. 3. The use of the [`ClvpModelForConditionalGeneration.generate()`] method is strongly recommended for tortoise usage. 4. Note that the CLVP model expects the audio to be sampled at 22.05 kHz contrary to other audio models which expects 16 kHz. ## Brief Explanation:\n- The [`ClvpTokenizer`] tokenizes the text input, and the [`ClvpFeatureExtractor`] extracts the log mel-spectrogram from the desired audio. - [`ClvpConditioningEncoder`] takes those text tokens and audio representations and converts them into embeddings conditioned on the text and audio. - The [`ClvpForCausalLM`] uses those embeddings to generate multiple speech candidates. - Each speech candidate is passed through the speech encoder ([`ClvpEncoder`]) which converts them into a vector representation, and the text encoder ([`ClvpEncoder`]) converts the text tokens into the same latent space. - At the end, we compare each speech vector with the text vector to see which speech vector is most similar to the text vector. - [`ClvpModelForConditionalGeneration.generate()`] compresses all of the logic described above into a single method. Example :\n```python\n>>> import datasets\n>>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration\n>>> # Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using `datasets` library). >>> text = \""This is an example text.\""\n>>> ds = datasets.load\\_dataset(\""hf-internal-testing/librispeech\\_asr\\_dummy\"", \""clean\"", split=\""validation\"")\n>>> ds = ds.cast\\_column(\""audio\"", datasets.Audio(sampling\\_rate=22050))\n>>> sample = ds[0][\""audio\""]\n>>> # Define processor and model. >>> processor = ClvpProcessor.from\\_pretrained(\""susnato/clvp\\_dev\"")\n>>> model = ClvpModelForConditionalGeneration.from\\_pretrained(\""susnato/clvp\\_dev\"")\n>>> # Generate processor output and model output. >>> processor\\_output = processor(raw\\_speech=sample[\""array\""], sampling\\_rate=sample[\""sampling\\_rate\""], text=text, return\\_tensors=\""pt\"")\n>>> generated\\_output = model.generate(\\*\\*processor\\_output)\n```\n## ClvpConfig\n[[autodoc]] ClvpConfig\n- from\\_sub\\_model\\_configs\n## ClvpEncoderConfig\n[[autodoc]] ClvpEncoderConfig\n## ClvpDecoderConfig\n[[autodoc]] ClvpDecoderConfig\n## ClvpTokenizer\n[[autodoc]] ClvpTokenizer\n- save\\_vocabulary\n## ClvpFeatureExtractor\n[[autodoc]] ClvpFeatureExtractor\n- \\_\\_call\\_\\_\n## ClvpProcessor\n[[autodoc]] ClvpProcessor\n- \\_\\_call\\_\\_\n- decode\n- batch\\_decode\n## ClvpModelForConditionalGeneration\n[[autodoc]] ClvpModelForConditionalGeneration\n- forward\n- generate\n- get\\_text\\_features\n- get\\_speech\\_features\n## ClvpForCausalLM\n[[autodoc]] ClvpForCausalLM\n## ClvpModel\n[[autodoc]] ClvpModel\n## ClvpEncoder\n[[autodoc]] ClvpEncoder\n## ClvpDecoder\n[[autodoc]] ClvpDecoder"", ""We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art\nperformances on a bunch of tasks including ImageNet-CN, Flicker30k- CN, and COCO-CN. Further, we obtain very close performances with\nCLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding.\\*\nThis model was contributed by [jongjyh](https://huggingface.co/jongjyh). ## Usage tips and example\nThe usage of AltCLIP is very similar to the CLIP. the difference between CLIP is the text encoder. Note that we use bidirectional attention instead of casual attention\nand we take the [CLS] token in XLM-R to represent text embedding. AltCLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image\nclassification. AltCLIP uses a ViT like transformer to get visual features and a bidirectional language model to get the text\nfeatures. Both the text and visual features are then projected to a latent space with identical dimension. The dot\nproduct between the projected image and text features is then used as a similar score. To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-overlapping patches,\nwhich are then linearly embedded.""]","The purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents"" is to leverage robust representations of images learned by contrastive models like CLIP for image generation. The model consists of a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. This approach aims to improve image diversity with minimal loss in photorealism and caption similarity, allowing for variations of an image that preserve its semantics and style while varying non-essential details. Additionally, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion.",The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.
"What command is used to install the requirements for a research project using 🤗 Transformers?
","[""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Research projects\nThis folder contains various research projects using 🤗 Transformers. They are not maintained and require a specific\nversion of 🤗 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice. If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder."", ""[SEP] [UNK] Transformers : State of the Art NLP\n[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\nquestion answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting-edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\nthen share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\ncan be modified to enable quick research experiments. Why should I use transformers? 1. Easy-to-use state-of-the-art models:\n- High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user-facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint:\n2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages."", ""Устанавливать 🤗 Transformers следует в [виртуальной среде](https://docs.python.org/3/library/venv.html). Если вы не знакомы с виртуальными средами Python, ознакомьтесь с [руководством пользователя](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). Сначала создайте виртуальную среду с той версией Python, которую вы собираетесь использовать, и активируйте ее. Затем необходимо установить хотя бы один бекенд из Flax, PyTorch или TensorFlow. Пожалуйста, обратитесь к страницам [TensorFlow установочная страница](https://www.tensorflow.org/install/), [PyTorch установочная страница](https://pytorch.org/get-started/locally/#start-locally) и/или [Flax](https://github.com/google/flax#quick-install) и [Jax](https://github.com/google/jax#installation), где описаны команды установки для вашей платформы. После установки одного из этих бэкендов 🤗 Transformers может быть установлен с помощью pip следующим образом:\n```bash\npip install transformers\n```\nЕсли вы хотите поиграть с примерами или вам нужен самый современный код и вы не можете ждать нового релиза, вы должны [установить библиотеку из исходного кода](https://huggingface.co/docs/transformers/installation#installing-from-source). ### С помощью conda\nНачиная с версии Transformers v4.0.0, у нас появилсась поддержка conda: `huggingface`. Установить Transformers с помощью conda можно следующим образом:\n```bash\nconda install -c huggingface transformers\n```\nО том, как установить Flax, PyTorch или TensorFlow с помощью conda, читайте на страницах, посвященных их установке."", ""If that's the case make sure to install the Deep Learning framework you are working with\n(PyTorch, TensorFlow and/or Flax) then do:\n```bash\npip install -e \"".[quality]\""\n```\nwhich should be enough for most use cases. You can then return to the parent directory\n```bash\ncd .. ```\n4. We recommend adding the PyTorch version of \\*brand\\_new\\_bert\\* to Transformers. To install PyTorch, please follow the\ninstructions on https://pytorch.org/get-started/locally/. \\*\\*Note:\\*\\* You don't need to have CUDA installed. Making the new model work on CPU is sufficient. 5. To port \\*brand\\_new\\_bert\\*, you will also need access to its original repository:\n```bash\ngit clone https://github.com/org\\_that\\_created\\_brand\\_new\\_bert\\_org/brand\\_new\\_bert.git\ncd brand\\_new\\_bert\npip install -e . ```\nNow you have set up a development environment to port \\*brand\\_new\\_bert\\* to 🤗 Transformers. ### 3.-4. Run a pretrained checkpoint using the original repository\nAt first, you will work on the original \\*brand\\_new\\_bert\\* repository."", ""\\* A short, self-contained, code snippet that allows us to reproduce the bug in\nless than 30s. \\* The \\*full\\* traceback if an exception is raised. \\* Attach any other additional information, like screenshots, you think may help. To get the OS and software versions automatically, run the following command:\n```bash\ntransformers-cli env\n```\nYou can also run the same command from the root of the repository:\n```bash\npython src/transformers/commands/transformers\\_cli.py env\n```\n### Do you want a new feature? If there is a new feature you'd like to see in 🤗 Transformers, please open an issue and describe:\n1. What is the \\*motivation\\* behind this feature? Is it related to a problem or frustration with the library? Is it a feature related to something you need for a project? Is it something you worked on and think it could benefit the community? Whatever it is, we'd love to hear about it! 2. Describe your requested feature in as much detail as possible. The more you can tell us about it, the better we'll be able to help you. 3. Provide a \\*code snippet\\* that demonstrates the features usage. 4. If the feature is related to a paper, please include a link. If your issue is well written we're already 80% of the way there by the time you create it. We have added [templates](https://github.com/huggingface/transformers/tree/main/templates) to help you get started with your issue. ## Do you want to implement a new model? New models are constantly released and if you want to implement a new model, please provide the following information\n\\* A short description of the model and a link to the paper. \\* Link to the implementation if it is open-sourced. \\* Link to the model weights if they are available. If you are willing to contribute the model yourself, let us know so we can help you add it to 🤗 Transformers! We have added a [detailed guide and templates](https://github.com/huggingface/transformers/tree/main/templates) to help you get started with adding a new model, and we also have a more technical guide for [how to add a model to 🤗 Transformers](https://huggingface.co/docs/transformers/add\\_new\\_model).""]","The command used to install the requirements for a research project using 🤗 Transformers is:

```
pip install -r requirements.txt
```

You should run this command inside the folder of your choice.",pip install -r requirements.txt
"What task does the `roberta-large-mnli` checkpoint perform?
","[""- [Question answering task guide](../tasks/question\\_answering)\n\\*\\*Multiple choice\\*\\*\n- [`RobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice.ipynb). - [`TFRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice-tf.ipynb). - [Multiple choice task guide](../tasks/multiple\\_choice)\n## RobertaConfig\n[[autodoc]] RobertaConfig\n## RobertaTokenizer\n[[autodoc]] RobertaTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## RobertaTokenizerFast\n[[autodoc]] RobertaTokenizerFast\n- build\\_inputs\\_with\\_special\\_tokens\n\n## RobertaModel\n[[autodoc]] RobertaModel\n- forward\n## RobertaForCausalLM\n[[autodoc]] RobertaForCausalLM\n- forward\n## RobertaForMaskedLM\n[[autodoc]] RobertaForMaskedLM\n- forward\n## RobertaForSequenceClassification\n[[autodoc]] RobertaForSequenceClassification\n- forward\n## RobertaForMultipleChoice\n[[autodoc]] RobertaForMultipleChoice\n- forward\n## RobertaForTokenClassification\n[[autodoc]] RobertaForTokenClassification\n- forward\n## RobertaForQuestionAnswering\n[[autodoc]] RobertaForQuestionAnswering\n- forward\n\n## TFRobertaModel\n[[autodoc]] TFRobertaModel\n- call\n## TFRobertaForCausalLM\n[[autodoc]] TFRobertaForCausalLM\n- call\n## TFRobertaForMaskedLM\n[[autodoc]] TFRobertaForMaskedLM\n- call\n## TFRobertaForSequenceClassification\n[[autodoc]] TFRobertaForSequenceClassification\n- call\n## TFRobertaForMultipleChoice\n[[autodoc]] TFRobertaForMultipleChoice\n- call\n## TFRobertaForTokenClassification\n[[autodoc]] TFRobertaForTokenClassification\n- call\n## TFRobertaForQuestionAnswering\n[[autodoc]] TFRobertaForQuestionAnswering\n- call\n\n## FlaxRobertaModel\n[[autodoc]] FlaxRobertaModel\n- \\_\\_call\\_\\_\n## FlaxRobertaForCausalLM\n[[autodoc]] FlaxRobertaForCausalLM\n- \\_\\_call\\_\\_\n## FlaxRobertaForMaskedLM\n[[autodoc]] FlaxRobertaForMaskedLM\n- \\_\\_call\\_\\_\n## FlaxRobertaForSequenceClassification\n[[autodoc]] FlaxRobertaForSequenceClassification\n- \\_\\_call\\_\\_\n## FlaxRobertaForMultipleChoice\n[[autodoc]] FlaxRobertaForMultipleChoice\n- \\_\\_call\\_\\_\n## FlaxRobertaForTokenClassification\n[[autodoc]] FlaxRobertaForTokenClassification\n- \\_\\_call\\_\\_\n## FlaxRobertaForQuestionAnswering\n[[autodoc]] FlaxRobertaForQuestionAnswering\n- \\_\\_call\\_\\_"", ""- [LyCORIS checkpoints](https://github.com/KohakuBlueleaf/LyCORIS) aren't fully supported. The [`~loaders.LoraLoaderMixin.load\\_lora\\_weights`] method loads LyCORIS checkpoints with LoRA and LoCon modules, but Hada and LoKR are not supported. Loading a checkpoint from TheLastBen is very similar. For example, to load the [TheLastBen/William\\_Eggleston\\_Style\\_SDXL](https://huggingface.co/TheLastBen/William\\_Eggleston\\_Style\\_SDXL) checkpoint:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\""stabilityai/stable-diffusion-xl-base-1.0\"", torch\\_dtype=torch.float16).to(\""cuda\"")\npipeline.load\\_lora\\_weights(\""TheLastBen/William\\_Eggleston\\_Style\\_SDXL\"", weight\\_name=\""wegg.safetensors\"")\n# use by william eggleston in the prompt to trigger the LoRA\nprompt = \""a house by william eggleston, sunrays, beautiful, sunlight, sunrays, beautiful\""\nimage = pipeline(prompt=prompt).images[0]\nimage\n```\n## IP-Adapter\n[IP-Adapter](https://ip-adapter.github.io/) is an effective and lightweight adapter that adds image prompting capabilities to a diffusion model."", ""The event brought together 40 participants, 30 of whom were Bengali-speaking volunteers, and 10 were volunteers from one of the authors' organizations. These 40 volunteers joined the [Neuropark](https://neuropark.co/) Discord channel to receive all information regarding the event and participate in discussions. To join the experiment, volunteers were asked to:\n1. Send their username to the moderators to be allowlisted;\n2. Open the provided notebook locally, on Google Colaboratory, or on Kaggle;\n3. Run one code cell and fill in their Hugging Face credentials when requested;\n4. Watch the training loss decrease on the shared dashboards! For security purposes, we set up an authorization system so that only members of the Neuropark community could train the model. Sparing you the technical details, our authorization protocol allows us to guarantee that every participant is in the allowlist and to acknowledge the individual contribution of each peer. In the following figure, you can see the activity of each volunteer. Over the experiment, the volunteers logged in 600 different sessions. Participants regularly launched multiple runs in parallel, and many of them spread out the runs they launched over time. The runs of individual participants lasted 4 hours on average, and the maximum length was 21 hours. You can read more about the participation statistics in the paper. Chart showing participants of the  [sahajBERT](https://huggingface.co/neuropark/sahajBERT) experiment. Circle radius is relative to the total number of processed batches, the circle is greyed if the participant is not active. Every purple square represents an active device, darker color corresponds to higher performance\n\nAlong with the resources provided by participants, we also used 16 preemptible (cheap but frequently interrupted) single-GPU T4 cloud instances to ensure the stability of the run. The cumulative runtime for the experiment was 234 days, and in the figure below you can see parts of the loss curve that each peer contributed to!The final model was uploaded to the Model Hub, so you can download and play with it if you want to: [https://hf.co/neuropark/sahajBERT](https://huggingface.co/neuropark/sahajBERT)\n### Evaluation\nTo evaluate the performance of sahajBERT, we finetuned it on two downstream tasks in Bengali:\n- Named entity recognition (NER) on the Bengali split of [WikiANN](https://aclanthology.org/P17-1178/)."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Image tasks with IDEFICS\n[[open-in-colab]]\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach\nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. For instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can\nsolve image-text tasks with a large multimodal model called IDEFICS. [IDEFICS](../model\\_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198),\na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image\nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content,\ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b)\nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the 🤗 Hub."", ""If you want to use one of the official checkpoints for a task, explore the [gligen](https://huggingface.co/gligen) Hub organizations! [`StableDiffusionGLIGENPipeline`] was contributed by [Nikhil Gajendrakumar](https://github.com/nikhil-masterful) and [`StableDiffusionGLIGENTextImagePipeline`] was contributed by [Nguyễn Công Tú Anh](https://github.com/tuanh123789). ## StableDiffusionGLIGENPipeline\n[[autodoc]] StableDiffusionGLIGENPipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n- enable\\_model\\_cpu\\_offload\n- prepare\\_latents\n- enable\\_fuser\n## StableDiffusionGLIGENTextImagePipeline\n[[autodoc]] StableDiffusionGLIGENTextImagePipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n- enable\\_model\\_cpu\\_offload\n- prepare\\_latents\n- enable\\_fuser\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput""]","The `roberta-large-mnli` checkpoint is designed for the task of natural language inference (NLI). It is specifically fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset, which allows it to determine the relationship between pairs of sentences, such as whether one sentence entails, contradicts, or is neutral with respect to the other.",Text classification
"What service is replacing the Paid tier of the Inference API at Hugging Face?
","[""Access 🤗 Inference Endpoints\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account. You can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method. There are two pricing plans:\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. - There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested! After you've added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying!"", ""Data is hosted on HuggingFace Datasets."", ""These services can be called with the [`InferenceClient`] object. Please refer to [this guide](../guides/inference)\nfor more information on how to use it. ## Inference Client\n[[autodoc]] InferenceClient\n## Async Inference Client\nAn async version of the client is also provided, based on `asyncio` and `aiohttp`. To use it, you can either install `aiohttp` directly or use the `[inference]` extra:\n```sh\npip install --upgrade huggingface\\_hub[inference]\n# or\n# pip install aiohttp\n```\n[[autodoc]] AsyncInferenceClient\n## InferenceTimeoutError\n[[autodoc]] InferenceTimeoutError\n## Return types\nFor most tasks, the return value has a built-in type (string, list, image...). Here is a list for the more complex types. ### ClassificationOutput\n[[autodoc]] huggingface\\_hub.inference.\\_types.ClassificationOutput\n### ConversationalOutputConversation\n[[autodoc]] huggingface\\_hub.inference.\\_types.ConversationalOutputConversation\n### ConversationalOutput\n[[autodoc]] huggingface\\_hub.inference.\\_types.ConversationalOutput\n### ImageSegmentationOutput\n[[autodoc]] huggingface\\_hub.inference.\\_types.ImageSegmentationOutput\n### ModelStatus\n[[autodoc]] huggingface\\_hub.inference.\\_common.ModelStatus\n### TokenClassificationOutput\n[[autodoc]] huggingface\\_hub.inference.\\_types.TokenClassificationOutput\n### Text generation types\n[`~InferenceClient.text\\_generation`] task has a greater support than other tasks in `InferenceClient`. In\nparticular, user inputs and server outputs are validated using [Pydantic](https://docs.pydantic.dev/latest/)\nif this package is installed. Therefore, we recommend installing it (`pip install pydantic`)\nfor a better user experience. You can find below the dataclasses used to validate data and in particular [`~huggingface\\_hub.inference.\\_text\\_generation.TextGenerationParameters`] (input),\n[`~huggingface\\_hub.inference.\\_text\\_generation.TextGenerationResponse`] (output) and\n[`~huggingface\\_hub.inference.\\_text\\_generation.TextGenerationStreamResponse`] (streaming output). [[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.TextGenerationParameters\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.TextGenerationResponse\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.TextGenerationStreamResponse\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.InputToken\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.Token\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.FinishReason\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.BestOfSequence\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.Details\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.StreamDetails\n## InferenceAPI\n[`InferenceAPI`] is the legacy way to call the Inference API."", ""The base foundation model can then be fine-tuned on the updated labels and evaluated once again, with this iterative “detect and correct” process continuing until the adapted foundation model is sufficiently high quality to deploy. Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One – or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) – specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment. ## How does Hugging Face help? Snorkel AI’s partnership with Hugging Face supercharges Snorkel Flow’s foundation model capabilities. Initially we only made a small number of foundation models available. Each one required a dedicated service, making it prohibitively expensive and difficult for us to offer enterprises the flexibility to capitalize on the rapidly growing variety of models available. Adopting Hugging Face’s Inference Endpoint service enabled us to expand the number of foundation models our users could tap into while keeping costs manageable. Hugging Face’s service allows users to create a model API in a few clicks and begin using it immediately. Crucially, the new service has “pause and resume” capabilities that allow us to activate a model API when a client needs it, and put it to sleep when they don’t. \""We were pleasantly surprised to see how straightforward Hugging Face Inference Endpoint service was to set up.. All the configuration options were pretty self-explanatory, but we also had access to all the options we needed in terms of what cloud to run on, what security level we needed, etc.\""\n– Snorkel CTO and Co-founder Braden Hancock\n## How does this help Snorkel customers?"", ""--\ntitle: \""Accelerating Hugging Face Transformers with AWS Inferentia2\""\nthumbnail: /blog/assets/140\\_accelerate\\_transformers\\_with\\_inferentia2/thumbnail.png\nauthors:\n- user: philschmid\n- user: juliensimon\n---\n# Accelerating Hugging Face Transformers with AWS Inferentia2\nIn the last five years, Transformer models [[1](https://arxiv.org/abs/1706.03762)] have become the \\_de facto\\_ standard for many machine learning (ML) tasks, such as natural language processing (NLP), computer vision (CV), speech, and more. Today, many data scientists and ML engineers rely on popular transformer architectures like BERT [[2](https://arxiv.org/abs/1810.04805)], RoBERTa [[3](https://arxiv.org/abs/1907.11692)], the Vision Transformer [[4](https://arxiv.org/abs/2010.11929)], or any of the 130,000+ pre-trained models available on the [Hugging Face](https://huggingface.co) hub to solve complex business problems with state-of-the-art accuracy. However, for all their greatness, Transformers can be challenging to deploy in production. On top of the infrastructure plumbing typically associated with model deployment, which we largely solved with our [Inference Endpoints](https://huggingface.co/inference-endpoints) service, Transformers are large models which routinely exceed the multi-gigabyte mark. Large language models (LLMs) like [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B), [Flan-T5](https://huggingface.co/google/flan-t5-xxl), or [Opt-30B](https://huggingface.co/facebook/opt-30b) are in the tens of gigabytes, not to mention behemoths like [BLOOM](https://huggingface.co/bigscience/bloom), our very own LLM, which clocks in at 350 gigabytes. Fitting these models on a single accelerator can be quite difficult, let alone getting the high throughput and low inference latency that applications require, like conversational applications and search. So far, ML experts have designed complex manual techniques to slice large models, distribute them on a cluster of accelerators, and optimize their latency. Unfortunately, this work is extremely difficult, time-consuming, and completely out of reach for many ML practitioners. At Hugging Face, we're democratizing ML and always looking to partner with companies who also believe that every developer and organization should benefit from state-of-the-art models. For this purpose, we're excited to partner with Amazon Web Services to optimize Hugging Face Transformers for AWS [Inferentia 2](https://aws.amazon.com/machine-learning/inferentia/)! It’s a new purpose-built inference accelerator that delivers unprecedented levels of throughput, latency, performance per watt, and scalability. ## Introducing AWS Inferentia2\nAWS Inferentia2 is the next generation to Inferentia1 launched in 2019. Powered by Inferentia1, Amazon EC2 Inf1 instances delivered 25% higher throughput and 70% lower cost than comparable G5 instances based on NVIDIA A10G GPU, and with Inferentia2, AWS is pushing the envelope again. The new Inferentia2 chip delivers a 4x throughput increase and a 10x latency reduction compared to Inferentia. Likewise, the new [Amazon EC2 Inf2](https://aws.amazon.com/de/ec2/instance-types/inf2/) instances have up to 2.6x better throughput, 8.1x lower latency, and 50% better performance per watt than comparable G5 instances. Inferentia 2 gives you the best of both worlds: cost-per-inference optimization thanks to high throughput and response time for your application thanks to low inference latency. Inf2 instances are available in multiple sizes, which are equipped with between 1 to 12 Inferentia 2 chips. When several chips are present, they are interconnected by a blazing-fast direct Inferentia2 to Inferentia2 connectivity for distributed inference on large models. For example, the largest instance size, inf2.48xlarge, has 12 chips and enough memory to load a 175-billion parameter model like GPT-3 or BLOOM.""]",The service replacing the Paid tier of the Inference API at Hugging Face is the Inference Endpoints service.,Inference Endpoints
"What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# SqueezeBERT\n## Overview\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers."", ""In the original model it is possible to use an average pooling layer with a fixed 7x7 window and stride 1 instead of global pooling. For inputs that are larger than the recommended image size, this gives a pooled output that is larger than 1x1. The Hugging Face implementation does not support this. - The original TensorFlow checkpoints include quantized models. We do not support these models as they include additional \""FakeQuantization\"" operations to unquantize the weights. - It's common to extract the output from the expansion layers at indices 10 and 13, as well as the output from the final 1x1 convolution layer, for downstream purposes. Using `output\\_hidden\\_states=True` returns the output from all intermediate layers. There is currently no way to limit this to specific layers. - The DeepLabV3+ segmentation head does not use the final convolution layer from the backbone, but this layer gets computed anyway. There is currently no way to tell [`MobileNetV2Model`] up to which layer it should run."", ""EfficientNet (Knapsack Pruned)\n\\*\\*EfficientNet\\*\\* is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a \\*compound coefficient\\*. Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use \\\\( 2^N \\\\) times more computational resources, then we can simply increase the network depth by \\\\( \\alpha ^ N \\\\), width by \\\\( \\beta ^ N \\\\), and image size by \\\\( \\gamma ^ N \\\\), where \\\\( \\alpha, \\beta, \\gamma \\\\) are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\\\( \\phi \\\\) to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of [MobileNetV2](https://paperswithcode.com/method/mobilenetv2), in addition to [squeeze-and-excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block)."", ""This can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion, especially with a small number of layers. Growing the receptive field has been shown to improve quality significantly (Salimans et al.). Doing so, however, comes at a significant cost in number of parameters and consequently computational performance and can make training such models more challenging. >\n> ... self-attention can achieve a better balance in the trade-off between the virtually unlimited receptive field of the necessarily sequential PixelRNN and the limited receptive field of the much more parallelizable PixelCNN and its various extensions. [Image Transformer](https://arxiv.org/abs/1802.05751) uses transformers by restricting self attention over local neighborhoods of pixels. [Taming Transformers](https://arxiv.org/abs/2012.09841) and [DALL-E 1](https://arxiv.org/abs/2102.12092) combine convolutions and transformers. Both train a VQ-VAE to learn a discrete latent space, and then a transformer is trained in the compressed latent space. The transformer context is global but masked, because attention is provided over all previously predicted latent pixels, but the model is still AR so attention cannot be provided over not yet predicted pixels. [ImageBART](https://arxiv.org/abs/2108.08827) combines convolutions, transformers, and diffusion processes."", ""n this video, we'll study the encoder architecture. An example of a popular encoder-only architecture is BERT, which is the most popular model of its kind. Let's first start by understanding how it works. We'll use a small example, using three words. We use these as inputs, and pass them through the encoder. We retrieve a numerical representation of each word. Here, for example, the encoder converts the three words “Welcome to NYC” in these three sequences of numbers. The encoder outputs exactly one sequence of numbers per input word. This numerical representation can also be called a \""Feature vector\"", or \""Feature tensor\"". Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \""to\"", isn't the representation of only the \""to\"" word. It also takes into account the words around it, which we call the “context”.As in, it looks to the left context, the word on the left of the one we're studying (here the word \""Welcome\"") and the context on the right (here the word \""NYC\"") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \""meaning\"" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence.""]","SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers.",Grouped convolutions
"What type of license is the HuggingFace Team's software distributed under?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction"", ""And HuggingFace for providing access to hardware the benchmarks were run on. Sylvain Gugger [@sgugger](https://github.com/sgugger/) and Stas Bekman [@stas00](https://github.com/stas00) worked on the integration of these projects."", ""Enterprise Hub\nEnterprise Hub adds advanced capabilities to organizations, enabling safe, compliant and managed collaboration for companies and teams on Hugging Face."", ""he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library that provides an API to quickly download many public datasets and preprocess them. In this video we will explore how to do that. The downloading part is easy: with the load\\_dataset function, you can directly download and cache a dataset from its identifier on the Dataset hub. Here we fetch the MRPC dataset from the GLUE benchmark, which is a dataset containing pairs of sentences where the task is to determine the paraphrases. The object returned by the load\\_dataset function is a DatasetDict, which is a sort of dictionary containing each split of our dataset. We can access each split by indexing with its name. This split is then an instance of the Dataset class, with columns (here sentence1, sentence2. label and idx) and rows. We can access a given element by its index. The amazing thing about the Hugging Face Datasets library is that everything is saved to disk using Apache Arrow, which means that even if your dataset is huge you won't get out of RAM: only the elements you request are loaded in memory."", ""For more info on the Hugging Face and Habana Gaudi collaboration, please visit [https://huggingface.co/Habana](https://huggingface.co/Habana).""]","The HuggingFace Team's software is distributed under the Apache License, Version 2.0.","Apache License, Version 2.0"
"What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?
","[""It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\n\\*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\*\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A."", ""However, we don't know \\\\(p(\\mathbf{x}\\_{t-1} | \\mathbf{x}\\_t)\\\\). It's intractable since it requires knowing the distribution of all possible images in order to calculate this conditional probability. Hence, we're going to leverage a neural network to \\*\\*approximate (learn) this conditional probability distribution\\*\\*, let's call it \\\\(p\\_\\theta (\\mathbf{x}\\_{t-1} | \\mathbf{x}\\_t)\\\\), with \\\\(\\theta\\\\) being the parameters of the neural network, updated by gradient descent. Ok, so we need a neural network to represent a (conditional) probability distribution of the backward process. If we assume this reverse process is Gaussian as well, then recall that any Gaussian distribution is defined by 2 parameters:\n\\* a mean parametrized by \\\\(\\mu\\_\\theta\\\\);\n\\* a variance parametrized by \\\\(\\Sigma\\_\\theta\\\\);\nso we can parametrize the process as\n$$ p\\_\\theta (\\mathbf{x}\\_{t-1} | \\mathbf{x}\\_t) = \\mathcal{N}(\\mathbf{x}\\_{t-1}; \\mu\\_\\theta(\\mathbf{x}\\_{t},t), \\Sigma\\_\\theta (\\mathbf{x}\\_{t},t))$$\nwhere the mean and variance are also conditioned on the noise level \\\\(t\\\\). Hence, our neural network needs to learn/represent the mean and variance. However, the DDPM authors decided to \\*\\*keep the variance fixed, and let the neural network only learn (represent) the mean \\\\(\\mu\\_\\theta\\\\) of this conditional probability distribution\\*\\*. From the paper:\n> First, we set \\\\(\\Sigma\\_\\theta ( \\mathbf{x}\\_t, t) = \\sigma^2\\_t \\mathbf{I}\\\\) to untrained time dependent constants. Experimentally, both \\\\(\\sigma^2\\_t = \\beta\\_t\\\\) and \\\\(\\sigma^2\\_t = \\tilde{\\beta}\\_t\\\\) (see paper) had similar results. This was then later improved in the [Improved diffusion models](https://openreview.net/pdf?id=-NEXDKk8gZ) paper, where a neural network also learns the variance of this backwards process, besides the mean. So we continue, assuming that our neural network only needs to learn/represent the mean of this conditional probability distribution. ## Defining an objective function (by reparametrizing the mean)\nTo derive an objective function to learn the mean of the backward process, the authors observe that the combination of \\\\(q\\\\) and \\\\(p\\_\\theta\\\\) can be seen as a variational auto-encoder (VAE) [(Kingma et al., 2013)](https://arxiv.org/abs/1312.6114). Hence, the \\*\\*variational lower bound\\*\\* (also called ELBO) can be used to minimize the negative log-likelihood with respect to ground truth data sample \\\\(\\mathbf{x}\\_0\\\\) (we refer to the VAE paper for details regarding ELBO)."", ""Advantage Actor-Critic (A2C) [[advantage-actor-critic]]\n## Reducing variance with Actor-Critic methods\nThe solution to reducing the variance of the Reinforce algorithm and training our agent faster and better is to use a combination of Policy-Based and Value-Based methods: \\*the Actor-Critic method\\*. To understand the Actor-Critic, imagine you're playing a video game. You can play with a friend that will provide you with some feedback. You're the Actor and your friend is the Critic. ![Actor Critic](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/ac.jpg)\nYou don't know how to play at the beginning, \\*\\*so you try some actions randomly\\*\\*. The Critic observes your action and \\*\\*provides feedback\\*\\*. Learning from this feedback, \\*\\*you'll update your policy and be better at playing that game.\\*\\*\nOn the other hand, your friend (Critic) will also update their way to provide feedback so it can be better next time. This is the idea behind Actor-Critic. We learn two function approximations:\n- \\*A policy\\* that \\*\\*controls how our agent acts\\*\\*: \\\\( \\pi\\_{\\theta}(s) \\\\)\n- \\*A value function\\* to assist the policy update by measuring how good the action taken is: \\\\( \\hat{q}\\_{w}(s,a) \\\\)\n## The Actor-Critic Process\nNow that we have seen the Actor Critic's big picture, let's dive deeper to understand how the Actor and Critic improve together during the training. As we saw, with Actor-Critic methods, there are two function approximations (two neural networks):\n- \\*Actor\\*, a \\*\\*policy function\\*\\* parameterized by theta: \\\\( \\pi\\_{\\theta}(s) \\\\)\n- \\*Critic\\*, a \\*\\*value function\\*\\* parameterized by w: \\\\( \\hat{q}\\_{w}(s,a) \\\\)\nLet's see the training process to understand how the Actor and Critic are optimized:\n- At each timestep, t, we get the current state \\\\( S\\_t\\\\) from the environment and \\*\\*pass it as input through our Actor and Critic\\*\\*."", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Reformer\n\n[![Models](https://img.shields.io/badge/All_model_pages-reformer-blueviolet)](https://huggingface.co/models?filter=reformer)\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/reformer-crime-and-punishment)\n\n## Overview\nThe Reformer model was proposed in the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451.pdf) by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya."", ""Model Based Reinforcement Learning (MBRL)\nModel-based reinforcement learning only differs from its model-free counterpart in learning a \\*dynamics model\\*, but that has substantial downstream effects on how the decisions are made. The dynamics model usually models the environment transition dynamics, \\\\( s\\_{t+1} = f\\_\\theta (s\\_t, a\\_t) \\\\), but things like inverse dynamics models (mapping from states to actions) or reward models (predicting rewards) can be used in this framework. ## Simple definition\n- There is an agent that repeatedly tries to solve a problem, \\*\\*accumulating state and action data\\*\\*. - With that data, the agent creates a structured learning tool, \\*a dynamics model\\*, to reason about the world. - With the dynamics model, the agent \\*\\*decides how to act by predicting the future\\*\\*. - With those actions, \\*\\*the agent collects more data, improves said model, and hopefully improves future actions\\*\\*. ## Academic definition\nModel-based reinforcement learning (MBRL) follows the framework of an agent interacting in an environment, \\*\\*learning a model of said environment\\*\\*, and then \\*\\*leveraging the model for control (making decisions). Specifically, the agent acts in a Markov Decision Process (MDP) governed by a transition function \\\\( s\\_{t+1} = f (s\\_t , a\\_t) \\\\) and returns a reward at each step \\\\( r(s\\_t, a\\_t) \\\\). With a collected dataset \\\\( D :={ s\\_i, a\\_i, s\\_{i+1}, r\\_i} \\\\), the agent learns a model, \\\\( s\\_{t+1} = f\\_\\theta (s\\_t , a\\_t) \\\\) \\*\\*to minimize the negative log-likelihood of the transitions\\*\\*. We employ sample-based model-predictive control (MPC) using the learned dynamics model, which optimizes the expected reward over a finite, recursively predicted horizon, \\\\( \\tau \\\\), from a set of actions sampled from a uniform distribution \\\\( U(a) \\\\), (see [paper](https://arxiv.org/pdf/2002.04523) or [paper](https://arxiv.org/pdf/2012.09156.pdf) or [paper](https://arxiv.org/pdf/2009.01221.pdf)). ## Further reading\nFor more information on MBRL, we recommend you check out the following resources:\n- A [blog post on debugging MBRL](https://www.natolambert.com/writing/debugging-mbrl).""]","The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:

1. Splitting the embedding matrix into two smaller matrices.
2. Using repeating layers split among groups.",Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.
"What are the three main steps for fine-tuning a model with the 🤗 Datasets library?
","[""Introduction[[introduction]]\nIn [Chapter 3](/course/chapter3) you got your first taste of the 🤗 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics. But this is just scratching the surface of what 🤗 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\\* What do you do when your dataset is not on the Hub? \\* How can you slice and dice a dataset? (And what if you \\_really\\_ need to use Pandas?)\n\\* What do you do when your dataset is huge and will melt your laptop's RAM? \\* What the heck are \""memory mapping\"" and Apache Arrow? \\* How can you create your own dataset and push it to the Hub? The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!"", ""To verify that all libraries are correctly installed, you can run the following command in a Python shell. It verifies that both `transformers` and `datasets` have been correclty installed. ```python\nfrom transformers import AutoModelForCTC, AutoProcessor\nfrom datasets import load\\_dataset\ndummy\\_dataset = load\\_dataset(\""common\\_voice\"", \""ab\"", split=\""test\"")\nmodel = AutoModelForCTC.from\\_pretrained(\""hf-internal-testing/tiny-random-wav2vec2\"")\nmodel.to(\""cuda\"")\nprocessor = AutoProcessor.from\\_pretrained(\""hf-internal-testing/tiny-random-wav2vec2\"")\ninput\\_values = processor(dummy\\_dataset[0][\""audio\""][\""array\""], return\\_tensors=\""pt\"", sampling\\_rate=16\\_000).input\\_values\ninput\\_values = input\\_values.to(\""cuda\"")\nlogits = model(input\\_values).logits\nassert logits.shape[-1] == 32\n```\n## How to finetune an acoustic model\nIn this section, we show you how to fine-tune a pre-trained [XLS-R Model](https://huggingface.co/docs/transformers/model\\_doc/xls\\_r) on the [Common Voice 7 dataset](https://huggingface.co/datasets/mozilla-foundation/common\\_voice\\_7\\_0)."", ""tf\\_train\\_dataset,\n... validation\\_data=tf\\_eval\\_dataset,\n... callbacks=callbacks,\n... epochs=num\\_epochs,\n... )\n```\nCongratulations! You have fine-tuned your model and shared it on the 🤗 Hub. You can now use it for inference! ### Inference\nGreat, now that you've finetuned a model, you can use it for inference! Load an image for inference:\n```py\n>>> image = ds[0][\""image\""]\n>>> image\n```\n\n![Image of bedroom](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png)\n\nWe will now see how to infer without a pipeline. Process the image with an image processor and place the `pixel\\_values` on a GPU:\n```py\n>>> device = torch.device(\""cuda\"" if torch.cuda.is\\_available() else \""cpu\"") # use GPU if available, otherwise use a CPU\n>>> encoding = image\\_processor(image, return\\_tensors=\""pt\"")\n>>> pixel\\_values = encoding.pixel\\_values.to(device)\n```\nPass your input to the model and return the `logits`:\n```py\n>>> outputs = model(pixel\\_values=pixel\\_values)\n>>> logits = outputs.logits.cpu()\n```\nNext, rescale the logits to the original image size:\n```py\n>>> upsampled\\_logits = nn.functional.interpolate(\n..."", ""Fine-tuning is the process of taking a pre-trained large language model (e.g. roBERTa in this case) and then tweaking it with additional training data to make it perform a second similar task (e.g. sentiment analysis). - [Bert-base-multilingual-uncased-sentiment](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) is a model fine-tuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. - [Distilbert-base-uncased-emotion](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion?text=I+feel+a+bit+let+down) is a model fine-tuned for detecting emotions in texts, including sadness, joy, love, anger, fear and surprise. Are you interested in doing sentiment analysis in languages such as Spanish, French, Italian or German? On the Hub, you will find many models fine-tuned for different use cases and ~28 languages. You can check out the complete list of sentiment analysis models [here](https://huggingface.co/models?pipeline\\_tag=text-classification&sort=downloads&search=sentiment) and filter at the left according to the language of your interest. ## 3. Building Your Own Sentiment Analysis Model\nUsing pre-trained models publicly available on the Hub is a great way to get started right away with sentiment analysis. These models use deep learning architectures such as transformers that achieve state-of-the-art performance on sentiment analysis and other machine learning tasks. However, you can fine-tune a model with your own data to further improve the sentiment analysis results and get an extra boost of accuracy in your particular use case. In this section, we'll go over two approaches on how to fine-tune a model for sentiment analysis with your own data and criteria. The first approach uses the Trainer API from the [🤗Transformers](https://github.com/huggingface/transformers), an open source library with 50K stars and 1K+ contributors and requires a bit more coding and experience. The second approach is a bit easier and more straightforward, it uses [AutoNLP](https://huggingface.co/autonlp), a tool to automatically train, evaluate and deploy state-of-the-art NLP models without code or ML experience."", ""You'll also want to create dictionaries that map a label id to a label class (`id2label`) and the other way around (`label2id`). You can use them later when setting up the model. Including these maps will make your model reusable by others if you share\nit on the Hugging Face Hub. As a final step of getting familiar with the data, explore it for potential issues. One common problem with datasets for\nobject detection is bounding boxes that \""stretch\"" beyond the edge of the image. Such \""runaway\"" bounding boxes can raise\nerrors during training and should be addressed at this stage. There are a few examples with this issue in this dataset. To keep things simple in this guide, we remove these images from the data. ```py\n>>> remove\\_idx = [590, 821, 822, 875, 876, 878, 879]\n>>> keep = [i for i in range(len(cppe5[\""train\""])) if i not in remove\\_idx]\n>>> cppe5[\""train\""] = cppe5[\""train\""].select(keep)\n```\n## Preprocess the data\nTo finetune a model, you must preprocess the data you plan to use to match precisely the approach used for the pre-trained model.""]","The three main steps for fine-tuning a model with the 🤗 Datasets library are:

1. Load a dataset from the Hugging Face Hub.
2. Preprocess the data with `Dataset.map()`.
3. Load and compute metrics.",1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.
"What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?
","[""It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark. Figure 4. Latency (Batch=1, Physical Cores=2)\n\n---\n## Conclusion\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput. Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens. The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial)\n## Resources\n\\* [Hugging Face Infinity](https://huggingface.co/infinity)\n\\* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\n\\* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i)\n\\* [DistilBERT](https://huggingface.co/docs/transformers/model\\_doc/distilbert)\n\\* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\\* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\n\\* [🤗 Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)"", ""!---\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\n\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\n\n#### [English](https://github.com/huggingface/transformers/blob/main/README.md) | [简体中文](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [繁體中文](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [한국어](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Español](https://github.com/huggingface/transformers/blob/main/README_es.md) | [日本語](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [हिन्दी](https://github.com/huggingface/transformers/blob/main/README_hd.md) | **Русский** [తెలుగు](https://github.com/huggingface/transformers//blob/main/README_te.md) |\n\n### Современное машинное обучение для JAX, PyTorch и TensorFlow\n\n### \n\n🤗 Transformers предоставляет тысячи предварительно обученных моделей для выполнения различных задач, таких как текст, зрение и аудио."", ""--\ntitle: \""Hugging Face and Graphcore partner for IPU-optimized Transformers\""\nthumbnail: /blog/assets/26\\_graphcore-ipu/thumbnail.png\nauthors:\n- user: sallydoherty\nguest: true\n---\n# Hugging Face and Graphcore partner for IPU-optimized Transformers\n> ##### Speaking at the 2021 AI Hardware Summit, Hugging Face announced the launch of their new Hardware Partner Program, including device-optimized models and software integrations. Here, Graphcore - creators of the Intelligence Processing Unit (IPU) and a founding member of the program – explain how their partnership with Hugging Face will allow developers to easily accelerate their use of state-of-the-art Transformer models. Graphcore and Hugging Face are two companies with a common goal – to make it easier for innovators to harness the power of machine intelligence. Hugging Face’s Hardware Partner Program will allow developers using Graphcore systems to deploy state-of-the-art Transformer models, optimised for our Intelligence Processing Unit (IPU), at production scale, with minimum coding complexity."", ""This took a while to pinpoint and either we went for 100% compliance and the model\nwas much slower, or we would take a small difference in generation\nbut was much faster to run and simpler code. We opted for a configurable flag. # First inference (PP + Accelerate)\n```\nNote: Pipeline Parallelism (PP) means in this context that each GPU will own\nsome layers so each GPU will work on a given chunk of data before handing\nit off to the next GPU. ```\nNow we have a workable `transformers` clean version of the start\nworking on running this. Bloom is a 352GB (176B parameters in bf16) model, we need at least that much\nGPU RAM to make it fit. We briefly explored offloading to CPU on smaller machines\nbut the inference speed was orders of magnitude slower so we discarded it. Then we wanted to basically use the [pipeline](https://huggingface.co/docs/transformers/v4.22.2/en/pipeline\\_tutorial#pipeline-usage). So it's dogfooding and this is what the API uses under the hood all the time. However `pipelines` are not distributed aware (it's not their goal). After briefly\ndiscussing options, we ended up using [accelerate](https://github.com/huggingface/accelerate/) newly\ncreated `device\\_map=\""auto\""` to manage the sharding of the model. We had to iron\nout a few bugs, and fix the `transformers` code a bit to help `accelerate` do the right job."", ""### Important note:\nHere, the number of iterations is actually quite low. To accurately measure and compare results, one should increase it to at least 100. One of the main reasons for the importance of increasing `nb\\_loops` is that the speech lengths generated vary greatly between different iterations, even with a fixed input. One consequence of this is that the latency measured by `measure\\_latency\\_and\\_memory\\_use` may not actually reflect the actual performance of optimization techniques! The benchmark at the end of the blog post reports the results averaged over 100 iterations, which gives a true indication of the performance of the model. ## 1. 🤗 Better Transformer\nBetter Transformer is an 🤗 Optimum feature that performs kernel fusion under the hood. This means that certain model operations will be better optimized on the GPU and that the model will ultimately be faster. To be more specific, most models supported by 🤗 Transformers rely on attention, which allows them to selectively focus on certain parts of the input when generating output. This enables the models to effectively handle long-range dependencies and capture complex contextual relationships in the data. The naive attention technique can be greatly optimized via a technique called [Flash Attention](https://arxiv.org/abs/2205.14135), proposed by the authors Dao et. al. in 2022. Flash Attention is a faster and more efficient algorithm for attention computations that combines traditional methods (such as tiling and recomputation) to minimize memory usage and increase speed. Unlike previous algorithms, Flash Attention reduces memory usage from quadratic to linear in sequence length, making it particularly useful for applications where memory efficiency is important. Turns out that Flash Attention is supported by 🤗 Better Transformer out of the box! It requires one line of code to export the model to 🤗 Better Transformer and enable Flash Attention:\n```python\nmodel = model.to\\_bettertransformer()\nwith torch.inference\\_mode():\nspeech\\_output = measure\\_latency\\_and\\_memory\\_use(model, inputs, nb\\_loops = 5)\n```\n\\*\\*Output:\\*\\*\n```\nExecution time: 5.43284375 seconds\nMax memory footprint 1.9151841280000002 GB\n```\nThe output sounds like this ([download audio](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/bark\\_optimization/audio\\_sample\\_bettertransformer.wav)):\n\nYour browser does not support the audio element.""]",Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.,+800%
"What is the command to upload a spaCy pipeline to the Hugging Face Hub?
","[""Hub methods\nMethods for using the Hugging Face Hub:\n## Push to hub\n[[autodoc]] evaluate.push\\_to\\_hub"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Super-resolution\nThe Stable Diffusion upscaler diffusion model was created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/), and [LAION](https://laion.ai/). It is used to enhance the resolution of input images by a factor of 4. Make sure to check out the Stable Diffusion [Tips](overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently! If you're interested in using one of the official checkpoints for a task, explore the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations! ## StableDiffusionUpscalePipeline\n[[autodoc]] StableDiffusionUpscalePipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_attention\\_slicing\n- disable\\_attention\\_slicing\n- enable\\_xformers\\_memory\\_efficient\\_attention\n- disable\\_xformers\\_memory\\_efficient\\_attention\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput"", ""\\* Launch [blog post](https://huggingface.co/blog/spacy)\n\\* spaCy v 3.1 [Announcement](https://explosion.ai/blog/spacy-v3-1#huggingface-hub)\n\\* spaCy [documentation](https://spacy.io/universe/project/spacy-huggingface-hub/)"", ""You can share your dataset on https://huggingface.co/datasets directly using your account, see the documentation:\n\\* [Create a dataset and upload files on the website](https://huggingface.co/docs/datasets/upload\\_dataset)\n\\* [Advanced guide using the CLI](https://huggingface.co/docs/datasets/share)"", ""ChatUI on Spaces\n\\*\\*Hugging Chat\\*\\* is an open-source interface enabling everyone to try open-source large language models such as Falcon, StarCoder, and BLOOM. Thanks to an official Docker template called ChatUI, you can deploy your own Hugging Chat based on a model of your choice with a few clicks using Hugging Face's infrastructure. ## Deploy your own Chat UI\nTo get started, simply head [here](https://huggingface.co/new-space?template=huggingchat/chat-ui-template). In the backend of this application, [text-generation-inference](https://github.com/huggingface/text-generation-inference) is used for better optimized model inference. Since these models can't run on CPUs, you can select the GPU depending on your choice of model. [![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/huggingface.co_spaces_docker_chatui_landing.png)](https://huggingface.co/new-space?template=huggingchat/chat-ui-template)\nYou should provide a MongoDB endpoint where your chats will be written. If you leave this section blank, your logs will be persisted to a database inside the Space.""]","To upload a spaCy pipeline to the Hugging Face Hub, you can use the `push_to_hub` method. For detailed instructions, please refer to the documentation on how to share your dataset or model on the Hugging Face Hub.",python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
"What is the time and memory complexity of the Nyströmformer's approximation of self-attention?
","[""--\ntitle: \""Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\""\nthumbnail: /blog/assets/86\\_nystromformer/thumbnail.png\nauthors:\n- user: asi\nguest: true\n---\n# Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\n## Introduction\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. The [Nyströmformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nyströmformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nyström method and how it can be adapted to approximate self-attention. ## Nyström method for matrix approximation\nAt the heart of Nyströmformer is the Nyström method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nyström method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:\n\nRepresenting P as a block matrix\n\nWe now have four submatrices: \\\\(A\\_P, B\\_P, F\\_P,\\\\) and \\\\(C\\_P\\\\), with sizes \\\\(m \\times m, m \\times (n - m), (n - m) \\times m\\\\) and\n\\\\((n - m) \\times (n - m)\\\\) respectively. The \\\\(m\\\\) sampled columns are contained in \\\\(A\\_P\\\\) and \\\\(F\\_P\\\\), whereas the \\\\(m\\\\) sampled rows are contained in \\\\(A\\_P\\\\) and \\\\(B\\_P\\\\). So, the entries of \\\\(A\\_P, B\\_P,\\\\) and \\\\(F\\_P\\\\) are known to us, and we will estimate \\\\(C\\_P\\\\). According to the Nyström method, \\\\(C\\_P\\\\) is given by:\n$$C\\_P = F\\_P A\\_P^+ B\\_P$$\nHere, \\\\(+\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). Thus, the Nyström approximation of \\\\(P, \\hat{P}\\\\) can be written as:\n\nNyström approximation of P\n\nAs shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices."", ""will only get faster and allow for more memory, but one should nevertheless always make sure to use the best available algorithms and architectures to get the most bang for your buck 🤗"", ""Only at longer sequence lengths of 4096, a slight decrease in memory usage can be seen. Let's see what happens to the memory peak usage if we increase the size of the feed forward layer by a factor of 4 and reduce the number of attention heads also by a factor of 4 so that the feed forward layer becomes the memory bottleneck. ```\nconfig\\_no\\_chunk = ReformerConfig.from\\_pretrained(\""google/reformer-enwik8\"", chunk\\_size\\_feed\\_forward=0, num\\_attention\\_{h}eads=2, feed\\_forward\\_size=16384) # no chuck\nconfig\\_chunk = ReformerConfig.from\\_pretrained(\""google/reformer-enwik8\"", chunk\\_size\\_feed\\_forward=1, num\\_attention\\_{h}eads=2, feed\\_forward\\_size=16384) # feed forward chunk\nbenchmark\\_args = PyTorchBenchmarkArguments(sequence\\_lengths=[1024, 2048, 4096], batch\\_sizes=[8], models=[\""Reformer-No-Chunk\"", \""Reformer-Chunk\""], no\\_speed=True, no\\_env\\_print=True)\nbenchmark = PyTorchBenchmark(configs=[config\\_no\\_chunk, config\\_chunk], args=benchmark\\_args)\nresult = benchmark.run()\n```\n1 / 2\n2 / 2\n==================== INFERENCE - MEMORY - RESULT ====================\n--------------------------------------------------------------------------------\nModel Name Batch Size Seq Length Memory in MB\n--------------------------------------------------------------------------------\nReformer-No-Chunk 8 1024 3743\nReformer-No-Chunk 8 2048 5539\nReformer-No-Chunk 8 4096 9087\nReformer-Chunk 8 1024 2973\nReformer-Chunk 8 2048 3999\nReformer-Chunk 8 4096 6011\n--------------------------------------------------------------------------------\nNow a clear decrease in peak memory usage can be seen for longer input sequences."", ""its implementation. Hopefully, you now have a better background to understand the actual code. Feel free to dive into it and to connect each part of the code with one of the components above. ## Time & Memory complexity\n| Attention Type | Sequence length | Time & Memory Complexity |\n|-----------------|-----------------|--------------------------|\n| `original\\_full` | 512 | `T` |\n| | 1024 | 4 x `T` |\n| | 4096 | 64 x `T` |\n| `block\\_sparse` | 1024 | 2 x `T` |\n| | 4096 | 8 x `T` |\n\\*Comparison of time & space complexity of BERT attention and BigBird block sparse attention.\\*\nExpand this snippet in case you wanna see the calculations\n```md\nBigBird time complexity = O(w x n + r x n + g x n)\nBERT time complexity = O(n^2)\nAssumptions:\nw = 3 x 64\nr = 3 x 64\ng = 2 x 64\nWhen seqlen = 512\n=> \\*\\*time complexity in BERT = 512^2\\*\\*\nWhen seqlen = 1024\n=> time complexity in BERT = (2 x 512)^2\n=> \\*\\*time complexity in BERT = 4 x 512^2\\*\\*\n=> time complexity in BigBird = (8 x 64) x (2 x 512)\n=> \\*\\*time complexity in BigBird = 2 x 512^2\\*\\*\nWhen seqlen = 4096\n=> time complexity in BERT = (8 x 512)^2\n=> \\*\\*time complexity in BERT = 64 x 512^2\\*\\*\n=> compute in BigBird = (8 x 64) x (8 x 512)\n=> compute in BigBird = 8 x (512 x 512)\n=> \\*\\*time complexity in BigBird = 8 x 512^2\\*\\*\n```\n## ITC vs ETC\nThe BigBird model can be trained using 2 different strategies: \\*\\*ITC\\*\\* & \\*\\*ETC\\*\\*."", ""There are the input and output that are being passed and returned by the forward and the backward functions and the\nforward activations saved for gradient computation. \\*\\*Temporary Memory\\*\\*\nAdditionally, there are all kinds of temporary variables which get released once the calculation is done, but in the\nmoment these could require additional memory and could push to OOM. Therefore, when coding it's crucial to think\nstrategically about such temporary variables and sometimes to explicitly free those as soon as they are no longer needed. \\*\\*Functionality-specific memory\\*\\*\nThen, your software could have special memory needs. For example, when generating text using beam search, the software\nneeds to maintain multiple copies of inputs and outputs. \\*\\*`forward` vs `backward` Execution Speed\\*\\*\nFor convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates\ninto ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually\nbandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward\n(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward,\nand writes once, gradInput). As you can see, there are potentially a few places where we could save GPU memory or speed up operations. Now that you understand what affects GPU utilization and computation speed, refer to\nthe [Methods and tools for efficient training on a single GPU](perf\\_train\\_gpu\\_one) documentation page to learn about\nperformance optimization techniques.""]","The Nyströmformer approximates self-attention with a time and memory complexity of \\(O(n)\\), where \\(n\\) is the length of the input sequence. This is a significant improvement over the standard self-attention mechanism, which has a complexity of \\(O(n^2)\\).",O(n)
"What is the goal of the Named Entity Recognition task in token classification?
","[""These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. Two common types of token classification are:\n\\* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names. \\* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb). ```py\n>>> from transformers import pipeline\n>>> classifier = pipeline(task=\""ner\"")\n>>> preds = classifier(\""Hugging Face is a French company based in New York City.\"")\n>>> preds = [\n... {\n... \""entity\"": pred[\""entity\""],\n... \""score\"": round(pred[\""score\""], 4),\n... \""index\"": pred[\""index\""],\n... \""word\"": pred[\""word\""],\n... \""start\"": pred[\""start\""],\n... \""end\"": pred[\""end\""],\n..."", ""The proposed model\nachieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains\nstate-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification),\nCoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question\nanswering).\\*\nThis model was contributed by [ikuyamada](https://huggingface.co/ikuyamada) and [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/studio-ousia/luke). ## Usage tips\n- This implementation is the same as [`RobertaModel`] with the addition of entity embeddings as well\nas an entity-aware self-attention mechanism, which improves performance on tasks involving reasoning about entities. - LUKE treats entities as input tokens; therefore, it takes `entity\\_ids`, `entity\\_attention\\_mask`,\n`entity\\_token\\_type\\_ids` and `entity\\_position\\_ids` as extra input. You can obtain those using\n[`LukeTokenizer`]. - [`LukeTokenizer`] takes `entities` and `entity\\_spans` (character-based start and end\npositions of the entities in the input text) as extra input. `entities` typically consist of [MASK] entities or\nWikipedia entities. The brief description when inputting these entities are as follows:\n- \\*Inputting [MASK] entities to compute entity representations\\*: The [MASK] entity is used to mask entities to be\npredicted during pretraining. When LUKE receives the [MASK] entity, it tries to predict the original entity by\ngathering the information about the entity from the input text. Therefore, the [MASK] entity can be used to address\ndownstream tasks requiring the information of entities in text such as entity typing, relation classification, and\nnamed entity recognition. - \\*Inputting Wikipedia entities to compute knowledge-enhanced token representations\\*: LUKE learns rich information\n(or knowledge) about Wikipedia entities during pretraining and stores the information in its entity embedding."", ""The goal of this task is to classify each token in the input text into one of the following categories: person, organization, location, or none of them. - News Category Classification (NCC) on the Soham articles dataset from [IndicGLUE](https://aclanthology.org/2020.findings-emnlp.445/). The goal of this task is to predict the category to which belong the input text. We evaluated it during training on the NER task to check that everything was going well; as you can see on the following plot, this was indeed the case! Evaluation metrics of fine-tuned models on the NER task from different checkpoints of pre-trained models. At the end of training, we compared sahajBERT with three other pretrained language models: [XLM-R Large](https://arxiv.org/abs/1911.02116), [IndicBert](https://aclanthology.org/2020.findings-emnlp.445/), and [bnRoBERTa](https://huggingface.co/neuralspace-reverie/indic-transformers-bn-roberta). In the table below, you can see that our model has results comparable to the best Bengali language models available on HF Hub, even though our model has only ~18M trained parameters, while, for instance, XLM-R (a strong multilingual baseline), has ~559M parameters and was trained on several hundred V100 GPUs."", ""n this video we will see together what is the purpose of training a tokenizer, what are the key steps to follow and what is the easiest way to do it. You will ask yourself the question \""Should I train a new tokenizer?\"" when you plan to train a new model from scratch. A trained tokenizer would not be suitable for your corpus if your corpus is in a different language, uses new characters such as accents or upper cased letters, has a specific vocabulary, for example medical or legal, or uses a different style, a language from another century for instance. For example, if I take the tokenizer trained for the bert-base-uncased model and ignore its normalization step then we can see that the tokenization operation on the English sentence \""here is a sentence adapted to our tokenizer\"" produces a rather satisfactory list of tokens in the sense that this sentence of 8 words is tokenized into 9 tokens. On the other hand if we use this same tokenizer on a sentence in Bengali, we see that either a word is divided into many sub tokens or that the tokenizer does not know one of the unicode characters and returns only an unknown token. The fact that a \""common\"" word is split into many tokens can be problematic because language models can only handle a sequence of tokens of limited length. A tokenizer that excessively splits your initial text may even impact the performance of your model. Unknown tokens are also problematic because the model will not be able to extract any information from the \""unknown\"" part of the text. In this other example, we can see that the tokenizer replaces words containing characters with accents and capital letters with unknown tokens. Finally, if we use again this tokenizer to tokenize medical vocabulary we see again that a single word is divided into many sub tokens: 4 for \""paracetamol\"" and \""pharyngitis\""."", ""命名实体识别 （Named-Entity Recognition）\n相关空间：https://huggingface.co/spaces/rajistics/biobert\\_ner\\_demo，https://huggingface.co/spaces/abidlabs/ner，https://huggingface.co/spaces/rajistics/Financial\\_Analyst\\_AI\n标签：NER，TEXT，HIGHLIGHT\n## 简介\n命名实体识别（NER）又称为标记分类或文本标记，它的任务是对一个句子进行分类，将每个单词（或 \""token\""）归为不同的类别，比如人名、地名或词性等。\n例如，给定以下句子：\n> 芝加哥有巴基斯坦餐厅吗？\n命名实体识别算法可以识别出：\n- \""Chicago\"" as a \\*\\*location\\*\\*\n- \""Pakistani\"" as an \\*\\*ethnicity\\*\\*\n等等。\n使用 `gradio`（特别是 `HighlightedText` 组件），您可以轻松构建一个 NER 模型的 Web 演示并与团队分享。\n这是您将能够构建的一个演示的示例：\n$demo\\_ner\\_pipeline\n本教程将展示如何使用预训练的 NER 模型并使用 Gradio 界面部署该模型。我们将展示两种不同的使用 `HighlightedText` 组件的方法--根据您的 NER 模型，可以选择其中任何一种更容易学习的方式！\n### 环境要求\n确保您已经[安装](/getting\\_started)了 `gradio` Python 包。您还需要一个预训练的命名实体识别模型。在本教程中，我们将使用 `transformers` 库中的一个模型。\n### 方法一：实体字典列表\n许多命名实体识别模型输出的是一个字典列表。每个字典包含一个\\*实体\\*，一个 \"" 起始 \"" 索引和一个 \"" 结束 \"" 索引。这就是 `transformers` 库中的 NER 模型的操作方式。\n```py\nfrom transformers import pipeline\nner\\_pipeline = pipeline(\""ner\"")\nner\\_pipeline(\""芝加哥有巴基斯坦餐厅吗？\"")\n```\n输出结果：\n```bash\n[{'entity': 'I-LOC',\n'score': 0.9988978,\n'index': 2,\n'word': 'Chicago',\n'start': 5,\n'end': 12},\n{'entity': 'I-MISC',\n'score': 0.9958592,\n'index': 5,\n'word': 'Pakistani',\n'start': 22,\n'end': 31}]\n```\n如果您有这样的模型，将其连接到 Gradio 的 `HighlightedText` 组件非常简单。您只需要将这个\\*\\*实体列表\\*\\*与\\*\\*原始文本\\*\\*以字典的形式传递给模型，其中键分别为 `\""entities\""` 和 `\""text\""`。\n下面是一个完整的示例：\n$code\\_ner\\_pipeline\n$demo\\_ner\\_pipeline\n### 方法二：元组列表\n将数据传递给 `HighlightedText` 组件的另一种方法是使用元组列表。每个元组的第一个元素应该是被归类为特定实体的单词或词组。第二个元素应该是实体标签（如果不需要标签，则为 `None`）。`HighlightedText` 组件会自动组合单词和标签来显示实体。\n在某些情况下，这比第一种方法更简单。下面是一个使用 Spacy 的词性标注器演示此方法的示例：\n$code\\_text\\_analysis\n$demo\\_text\\_analysis\n---\n到此为止！您已经了解了为您的 NER 模型构建基于 Web 的图形用户界面所需的全部内容。\n有趣的提示：只需在 `launch()` 中设置 `share=True`，即可立即与其他人分享您的 NER 演示。""]","The goal of the Named Entity Recognition (NER) task in token classification is to classify each token in the input text into one of the following categories: person, organization, location, or none of them. NER aims to identify and label entities within the text, such as names of people, organizations, and locations.","The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization."
"What is the resolution of images used by the CLIPSeg model?
","[""For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm\\_source=hf&utm\\_medium=blog&utm\\_campaign=clipseg). Before we describe how to do that, let’s first take a look at how CLIPSeg works. ## CLIP: the magic model behind CLIPSeg\n[CLIP](https://huggingface.co/docs/transformers/main/en/model\\_doc/clip), which stands for \\*\\*C\\*\\*ontrastive \\*\\*L\\*\\*anguage–\\*\\*I\\*\\*mage \\*\\*P\\*\\*re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract \\*representation\\* of your input. This abstract representation, also called an \\*embedding\\*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together)."", ""For this model, the acceptable height\nand width are set to 224, and the number of channels is 3. You can verify\nthis by inspecting the config argument of the model (`model.config`). The model yields a 1000-d vector of `logits`. # Model Surgery\nUsually, every ML model has certain preprocessing and postprocessing\nsteps. The ViT model is no exception to this. The major preprocessing\nsteps include:\n- Scaling the image pixel values to [0, 1] range. - Normalizing the scaled pixel values to [-1, 1]. - Resizing the image so that it has a spatial resolution of (224, 224). You can confirm these by investigating the image processor associated\nwith the model:\n```py\nfrom transformers import AutoImageProcessor\nprocessor = AutoImageProcessor.from\\_pretrained(ckpt)\nprint(processor)\n```\nThis should print:\n```bash\nViTImageProcessor {\n\""do\\_normalize\"": true,\n\""do\\_resize\"": true,\n\""image\\_mean\"": [\n0.5,\n0.5,\n0.5\n],\n\""image\\_std\"": [\n0.5,\n0.5,\n0.5\n],\n\""resample\"": 2,\n\""size\"": 224\n}\n```\nSince this is an image classification model pre-trained on the\n[ImageNet-1k dataset](https://huggingface.co/datasets/imagenet-1k), the model\noutputs need to be mapped to the ImageNet-1k classes as the\npost-processing step."", ""Generated image is a mix of the layout semantics of the given image and the content semantics of the prompt. E.g. the above script generates the following image:\n`phone.jpg`\n![206903102-34e79b9f-9ed2-4fac-bb38-82871343c655](https://user-images.githubusercontent.com/59410571/209578593-141467c7-d831-4792-8b9a-b17dc5e47816.jpg)\n`phone\\_bed\\_mix.jpg`\n![206903104-913a671d-ef53-4ae4-919d-64c3059c8f67](https://user-images.githubusercontent.com/59410571/209578602-70f323fa-05b7-4dd6-b055-e40683e37914.jpg)\nFor more example generations check out this [demo notebook](https://github.com/daspartho/MagicMix/blob/main/demo.ipynb). ### Stable UnCLIP\nUnCLIPPipeline(\""kakaobrain/karlo-v1-alpha\"") provide a prior model that can generate clip image embedding from text. StableDiffusionImageVariationPipeline(\""lambdalabs/sd-image-variations-diffusers\"") provide a decoder model than can generate images from clip image embedding. ```python\nimport torch\nfrom diffusers import DiffusionPipeline\ndevice = torch.device(\""cpu\"" if not torch.cuda.is\\_available() else \""cuda\"")\npipeline = DiffusionPipeline.from\\_pretrained(\n\""kakaobrain/karlo-v1-alpha\"",\ntorch\\_dtype=torch.float16,\ncustom\\_pipeline=\""stable\\_unclip\"",\ndecoder\\_pipe\\_kwargs=dict(\nimage\\_encoder=None,\n),\n)\npipeline.to(device)\nprompt = \""a shiba inu wearing a beret and black turtleneck\""\nrandom\\_generator = torch.Generator(device=device).manual\\_seed(1000)\noutput = pipeline(\nprompt=prompt,\nwidth=512,\nheight=512,\ngenerator=random\\_generator,\nprior\\_guidance\\_scale=4,\nprior\\_num\\_inference\\_steps=25,\ndecoder\\_guidance\\_scale=8,\ndecoder\\_num\\_inference\\_steps=50,\n)\nimage = output.images[0]\nimage.save(\""./shiba-inu.jpg\"")\n# debug\n# `pipeline.decoder\\_pipe` is a regular StableDiffusionImageVariationPipeline instance."", ""It should be noted that the `StableDiffusionInstructPix2PixPipeline` exposes two arguments, namely, `image\\_guidance\\_scale` and `guidance\\_scale` that let you control the quality of the final edited image. We encourage you to experiment with these two arguments and see the impact of that on the directional similarity. We can extend the idea of this metric to measure how similar the original image and edited version are. To do that, we can just do `F.cosine\\_similarity(img\\_feat\\_two, img\\_feat\\_one)`. For these kinds of edits, we would still want the primary semantics of the images to be preserved as much as possible, i.e., a high similarity score. We can use these metrics for similar pipelines such as the [`StableDiffusionPix2PixZeroPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pix2pix\\_zero#diffusers.StableDiffusionPix2PixZeroPipeline). Both CLIP score and CLIP direction similarity rely on the CLIP model, which can make the evaluations biased. \\*\\*\\*Extending metrics like IS, FID (discussed later), or KID can be difficult\\*\\*\\* when the model under evaluation was pre-trained on a large image-captioning dataset (such as the [LAION-5B dataset](https://laion.ai/blog/laion-5b/)). This is because underlying these metrics is an InceptionNet (pre-trained on the ImageNet-1k dataset) used for extracting intermediate image features. The pre-training dataset of Stable Diffusion may have limited overlap with the pre-training dataset of InceptionNet, so it is not a good candidate here for feature extraction."", ""```python\nencoding = image\\_processor(image.convert(\""RGB\""), return\\_tensors=\""pt\"")\n```\nRun inference with the encoded image. ```python\nwith torch.no\\_grad():\noutputs = inference\\_model(pixel\\_values=encoding.pixel\\_values)\nlogits = outputs.logits\nupsampled\\_logits = nn.functional.interpolate(\nlogits,\nsize=image.size[::-1],\nmode=\""bilinear\"",\nalign\\_corners=False,\n)\npred\\_seg = upsampled\\_logits.argmax(dim=1)[0]\n```\nNext, visualize the results. We need a color palette for this. Here, we use ade\\_palette(). As it is a long array, so\nwe don't include it in this guide, please copy it from [the TensorFlow Model Garden repository](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get\\_dataset\\_colormap.py#L51). ```python\nimport matplotlib.pyplot as plt\ncolor\\_seg = np.zeros((pred\\_seg.shape[0], pred\\_seg.shape[1], 3), dtype=np.uint8)\npalette = np.array(ade\\_palette())\nfor label, color in enumerate(palette):\ncolor\\_seg[pred\\_seg == label, :] = color\ncolor\\_seg = color\\_seg[..., ::-1] # convert to BGR\nimg = np.array(image) \\* 0.5 + color\\_seg \\* 0.5 # plot the image with the segmentation map\nimg = img.astype(np.uint8)\nplt.figure(figsize=(15, 10))\nplt.imshow(img)\nplt.show()\n```\nAs you can see, the results are far from perfect, however, this example is designed to illustrate the end-to-end workflow of\nfine-tuning a semantic segmentation model with LoRa technique, and is not aiming to achieve state-of-the-art\nresults.""]",The CLIPSeg model uses images with a resolution of 224 x 224 pixels.,352 x 352 pixels
"What can you use Gradio for?
","[""### What's Next? Keep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features). Or, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/). ## Questions? If you'd like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help. If you like Gradio, please leave us a ⭐ on GitHub! ## Open Source Stack\nGradio is built on top of many wonderful open-source libraries! [![huggingface](readme_files/huggingface_mini.svg)](https://huggingface.co)\n[![python](readme_files/python.svg)](https://www.python.org)\n[![fastapi](readme_files/fastapi.svg)](https://fastapi.tiangolo.com)\n[![encode](readme_files/encode.svg)](https://www.encode.io)\n[![svelte](readme_files/svelte.svg)](https://svelte.dev)\n[![vite](readme_files/vite.svg)](https://vitejs.dev)\n[![pnpm](readme_files/pnpm.svg)](https://pnpm.io)\n[![tailwind](readme_files/tailwind.svg)](https://tailwindcss.com)\n[![storybook](readme_files/storybook.svg)](https://storybook.js.org/)\n[![chromatic](readme_files/chromatic.svg)](https://www.chromatic.com/)\n## License\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository. ## Citation\nAlso check out the paper \\_[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019\\_, and please cite it if you use Gradio in your work."", ""Gradio Demo: upload\\_button\n### A simple demo showcasing the upload button used with its `upload` event trigger."", ""Gradio Demo: sentiment\\_analysis\n### This sentiment analaysis demo takes in input text and returns its classification for either positive, negative or neutral using Gradio's Label output."", ""he simplest possible Gradio demo."", ""It uses Gradio's Audio and Label components.""]","Gradio can be used for creating user-friendly interfaces for machine learning models, allowing users to interact with models through web applications. It supports various input and output components, such as text, images, audio, and more, making it easy to showcase and test models. Gradio is particularly useful for tasks like sentiment analysis, image classification, and any other machine learning applications where user interaction is beneficial. Additionally, it facilitates hassle-free sharing and testing of ML models in a web environment.","Create a demo for your machine learning model, share your machine learning model with others, and debug your model."
"What TensorFlow API function is used to load a saved tensor file?
","[""Tensorflow API\n[[autodoc]] safetensors.tensorflow.load\\_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save\\_file\n[[autodoc]] safetensors.tensorflow.save"", ""After such a [`VisionEncoderDecoderModel`] has been trained/fine-tuned, it can be saved/loaded just like any other models (see the examples below\nfor more information). An example application is image captioning, in which the encoder is used to encode the image, after which an autoregressive language model generates\nthe caption. Another example is optical character recognition. Refer to [TrOCR](trocr), which is an instance of [`VisionEncoderDecoderModel`]. ## Randomly initializing `VisionEncoderDecoderModel` from model configurations. [`VisionEncoderDecoderModel`] can be randomly initialized from an encoder and a decoder config. In the following example, we show how to do this using the default [`ViTModel`] configuration for the encoder\nand the default [`BertForCausalLM`] configuration for the decoder. ```python\n>>> from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel\n>>> config\\_encoder = ViTConfig()\n>>> config\\_decoder = BertConfig()\n>>> config = VisionEncoderDecoderConfig.from\\_encoder\\_decoder\\_configs(config\\_encoder, config\\_decoder)\n>>> model = VisionEncoderDecoderModel(config=config)\n```\n## Initialising `VisionEncoderDecoderModel` from a pretrained encoder and a pretrained decoder. [`VisionEncoderDecoderModel`] can be initialized from a pretrained encoder checkpoint and a pretrained decoder checkpoint. Note that any pretrained Transformer-based vision model, \\*e.g.\\* [Swin](swin), can serve as the encoder and both pretrained auto-encoding models, \\*e.g.\\* BERT, pretrained causal language models, \\*e.g.\\* GPT2, as well as the pretrained decoder part of sequence-to-sequence models, \\*e.g.\\* decoder of BART, can be used as the decoder. Depending on which architecture you choose as the decoder, the cross-attention layers might be randomly initialized. Initializing [`VisionEncoderDecoderModel`] from a pretrained encoder and decoder checkpoint requires the model to be fine-tuned on a downstream task, as has been shown in [the \\*Warm-starting-encoder-decoder blog post\\*](https://huggingface.co/blog/warm-starting-encoder-decoder). To do so, the `VisionEncoderDecoderModel` class provides a [`VisionEncoderDecoderModel.from\\_encoder\\_decoder\\_pretrained`] method. ```python\n>>> from transformers import VisionEncoderDecoderModel\n>>> model = VisionEncoderDecoderModel.from\\_encoder\\_decoder\\_pretrained(\n... \""microsoft/swin-base-patch4-window7-224-in22k\"", \""bert-base-uncased\""\n... )\n```\n## Loading an existing `VisionEncoderDecoderModel` checkpoint and perform inference. To load fine-tuned checkpoints of the `VisionEncoderDecoderModel` class, [`VisionEncoderDecoderModel`] provides the `from\\_pretrained(...)` method just like any other model architecture in Transformers. To perform inference, one uses the [`generate`] method, which allows to autoregressively generate text. This method supports various forms of decoding, such as greedy, beam search and multinomial sampling. ```python\n>>> import requests\n>>> from PIL import Image\n>>> from transformers import GPT2TokenizerFast, ViTImageProcessor, VisionEncoderDecoderModel\n>>> # load a fine-tuned image captioning model and corresponding tokenizer and image processor\n>>> model = VisionEncoderDecoderModel.from\\_pretrained(\""nlpconnect/vit-gpt2-image-captioning\"")\n>>> tokenizer = GPT2TokenizerFast.from\\_pretrained(\""nlpconnect/vit-gpt2-image-captioning\"")\n>>> image\\_processor = ViTImageProcessor.from\\_pretrained(\""nlpconnect/vit-gpt2-image-captioning\"")\n>>> # let's perform inference on an image\n>>> url = \""http://images.cocodataset.org/val2017/000000039769.jpg\""\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> pixel\\_values = image\\_processor(image, return\\_tensors=\""pt\"").pixel\\_values\n>>> # autoregressively generate caption (uses greedy decoding by default)\n>>> generated\\_ids = model.generate(pixel\\_values)\n>>> generated\\_text = tokenizer.batch\\_decode(generated\\_ids, skip\\_special\\_tokens=True)[0]\n>>> print(generated\\_text)\na cat laying on a blanket next to a cat laying on a bed\n```\n## Loading a PyTorch checkpoint into `TFVisionEncoderDecoderModel`."", ""Why? Because the tokenized array and labels would have to be fully loaded into memory, and because Numpy doesn’t handle “jagged” arrays, so every tokenized sample would have to be padded to the length of the longest sample in the whole dataset. That’s going to make your array even bigger, and all those padding tokens will slow down training too! As a TensorFlow engineer, this is normally where you’d turn to `tf.data` to make a pipeline that will stream the data from storage rather than loading it all into memory. That’s a hassle, though, so we’ve got you. First, let’s use the `map()` method to add the tokenizer columns to the dataset. Remember that our datasets are disc-backed by default - they won’t load into memory until you convert them into arrays! ```py\ndef tokenize\\_dataset(data):\n# Keys of the returned dictionary will be added to the dataset as columns\nreturn tokenizer(data[\""text\""])\ndataset = dataset.map(tokenize\\_dataset)\n```\nNow our dataset has the columns we want, but how do we train on it? Simple - wrap it with a `tf.data.Dataset` and all our problems are solved - data is loaded on-the-fly, and padding is applied only to batches rather than the whole dataset, which means that we need way fewer padding tokens:\n```py\ntf\\_dataset = model.prepare\\_tf\\_dataset(\ndataset,\nbatch\\_size=16,\nshuffle=True\n)\nmodel.fit(tf\\_dataset)\n```\nWhy is [prepare\\_tf\\_dataset()](https://huggingface.co/docs/transformers/main/en/main\\_classes/model#transformers.TFPreTrainedModel.prepare\\_tf\\_dataset) a method on your model?"", ""500 may more likely for my usage. That's why I considered 2,000 requests as an upper bound when designing my microservice. Due to that low number of requests, I didn't bother so much regarding the scalability but might come back into it if my billing increases. Nevertheless, it's important to stress that you will pay the storage for each Gigabyte of your build image. It's roughly €0.10 per Gb per month, which is fine if you don't keep all your versions on the cloud since my version is slightly above 1 Gb (Pytorch for 700 Mb & the model for 250 Mb). ## Conclusion\nBy using Transformers' sentiment analysis pipeline, I saved a non-negligible amount of time. Instead of training/fine-tuning a model, I could find one ready to be used in production and start the deployment in my system. I might fine-tune it in the future, but as shown on my test, the accuracy is already amazing! I would have liked a \""pure TensorFlow\"" model, or at least a way to load it in TensorFlow without Transformers dependencies to use the AI platform. It would also be great to have a lite version."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Single files\nDiffusers supports loading pretrained pipeline (or model) weights stored in a single file, such as a `ckpt` or `safetensors` file. These single file types are typically produced from community trained models. There are three classes for loading single file weights:\n- [`FromSingleFileMixin`] supports loading pretrained pipeline weights stored in a single file, which can either be a `ckpt` or `safetensors` file. - [`FromOriginalVAEMixin`] supports loading a pretrained [`AutoencoderKL`] from pretrained ControlNet weights stored in a single file, which can either be a `ckpt` or `safetensors` file. - [`FromOriginalControlnetMixin`] supports loading pretrained ControlNet weights stored in a single file, which can either be a `ckpt` or `safetensors` file. To learn more about how to load single file weights, see the [Load different Stable Diffusion formats](../../using-diffusers/other-formats) loading guide. ## FromSingleFileMixin\n[[autodoc]] loaders.single\\_file.FromSingleFileMixin\n## FromOriginalVAEMixin\n[[autodoc]] loaders.single\\_file.FromOriginalVAEMixin\n## FromOriginalControlnetMixin\n[[autodoc]] loaders.single\\_file.FromOriginalControlnetMixin""]",The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`.,safetensors.tensorflow.load_file
"Where can you access the logs of your Endpoints in Hugging Face Endpoints?
","[""Access and view Metrics\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the “Analytics” tab of your Endpoints. ![metric dashboard](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png)\n## Access Metrics via API\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api\\_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction"", ""After this time, members will be prompted to re-authenticate with your Identity Provider to access the organization's resources. The default value is 7 days. #### Role Mapping\nWhen enabled, Role Mapping allows you to dynamically assign [roles](./organizations-security#access-control-in-organizations) to organization members based on data provided by your Identity Provider. This section allows you to define a mapping from your IdP's user profile data from your IdP to the assigned role in Hugging Face. - IdP Role Attribute Mapping\nA JSON path to an attribute in your user's IdP profile data. - Role Mapping\nA mapping from the IdP attribute value to the assigned role in the Hugging Face organization. You must map at least one admin role. If there is no match, a user will be assigned the default role for your organization. The default role can be customized in the `Members` section of the organization's settings. Role synchronization is performed on login."", ""`huggingface\\_hub` let you configure this globally using [`configure\\_http\\_backend`]. All requests made to the Hub will\nthen use your settings. Under the hood, `huggingface\\_hub` uses `requests.Session` so you might want to refer to the\n[`requests` documentation](https://requests.readthedocs.io/en/latest/user/advanced) to learn more about the available\nparameters. Since `requests.Session` is not guaranteed to be thread-safe, `huggingface\\_hub` creates one session instance per thread. Using sessions allows us to keep the connection open between HTTP calls and ultimately save time. If you are\nintegrating `huggingface\\_hub` in a third-party library and wants to make a custom call to the Hub, use [`get\\_session`]\nto get a Session configured by your users (i.e. replace any `requests.get(...)` call by `get\\_session().get(...)`). [[autodoc]] configure\\_http\\_backend\n[[autodoc]] get\\_session\n## Handle HTTP errors\n`huggingface\\_hub` defines its own HTTP errors to refine the `HTTPError` raised by\n`requests` with additional information sent back by the server."", ""This means that you must be logged in to a Hugging Face user account. Requesting access can only be done from your browser. Go to the dataset on the Hub and you will be prompted to share your information:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-gated-user-side.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-gated-user-side-dark.png)\n\nBy clicking on \\*\\*Agree\\*\\*, you agree to share your username and email address with the dataset authors. In some cases, additional fields might be requested. To help the dataset authors decide whether to grant you access, try to fill out the form as completely as possible. Once the access request is sent, there are two possibilities. If the approval mechanism is automatic, you immediately get access to the dataset files. Otherwise, the requests have to be approved manually by the authors, which can take more time. The dataset authors have complete control over dataset access. In particular, they can decide at any time to block your access to the dataset without prior notice, regardless of approval mechanism or if your request has already been approved. ### Download files\nTo download files from a gated dataset you'll need to be authenticated.""]",You can access the logs of your Endpoints in Hugging Face Endpoints through the UI on the detailed overview in the “Logs” tab of your Endpoints.,"In the ""Logs"" tab of your Endpoint through the UI."
"What is the latest task added to Hugging Face AutoTrain for Computer Vision?
","[""--\ntitle: Image Classification with AutoTrain\nthumbnail: /blog/assets/105\\_autotrain-image-classification/thumbnail.png\nauthors:\n- user: nimaboscarino\n---\n# Image Classification with AutoTrain\nSo you’ve heard all about the cool things that are happening in the machine learning world, and you want to join in. There’s just one problem – you don’t know how to code! 😱 Or maybe you’re a seasoned software engineer who wants to add some ML to your side-project, but you don’t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. That’s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature we’ve just added, we’re making “no-code” machine learning better than ever. Best of all, you can create your first project for ✨ free! ✨\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with \\*\\*zero\\*\\* configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer 🤯 We’ve been expanding the number of tasks that we support, and we’re proud to announce that \\*\\*you can now use AutoTrain for Computer Vision\\*\\*!"", ""Data is hosted on HuggingFace Datasets."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Mask2Former\n## Overview\nThe Mask2Former model was proposed in [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar. Mask2Former is a unified framework for panoptic, instance and semantic segmentation and features significant performance and efficiency improvements over [MaskFormer](maskformer)."", ""Tasks\n## What's a task? Tasks, or pipeline types, describe the \""shape\"" of each model's API (inputs and outputs) and are used to determine which Inference API and widget we want to display for any given model. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/tasks.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/tasks-dark.png)\n\nThis classification is relatively coarse-grained (you can always add more fine-grained task names in your model tags), so \\*\\*you should rarely have to create a new task\\*\\*. If you want to add support for a new task, this document explains the required steps. ## Overview\nHaving a new task integrated into the Hub means that:\n\\* Users can search for all models – and datasets – of a given task. \\* The Inference API supports the task. \\* Users can try out models directly with the widget. 🏆\nNote that you don't need to implement all the steps by yourself. Adding a new task is a community effort, and multiple people can contribute. 🧑‍🤝‍🧑\nTo begin the process, open a new issue in the [huggingface\\_hub](https://github.com/huggingface/huggingface\\_hub/issues) repository."", ""\\* Click on the Hugging Face Model Catalog. \\* Filter by task or license and search the models. \\* Click the model tile to open the model page and choose the real-time deployment option to deploy the model. \\* Select an Azure instance type and click deploy. ![Creating a Hugging Face Endpoint on Azure](assets/75\\_hugging\\_face\\_endpoints\\_on\\_azure/03.jpg \""Creating a Hugging Face Endpoint on Azure\"")\nWithin minutes, you can test your endpoint and add its inference API to your application. It’s never been easier! ![Predicting with a Hugging Face Endpoint on Azure](assets/75\\_hugging\\_face\\_endpoints\\_on\\_azure/04.jpg \""Predicting with a Hugging Face Endpoint on Azure\"")\nIf you'd like to see the service in action, you can click on the image below to launch a video walkthrough. [![Video walkthrough of Hugging Face Endpoints](assets/75\\_hugging\\_face\\_endpoints\\_on\\_azure/05.jpg)](https://youtu.be/cjXYjN2mNVM \""Video walkthrough of Hugging Face Endpoints\"")\nHugging Face Model Catalog on Azure Machine Learning is available today in public preview in all Azure Regions where Azure Machine Learning is available. Give the service a try and [let us know your feedback and questions in the forum](https://discuss.huggingface.co/c/azureml/68)!""]","The latest task added to Hugging Face AutoTrain for Computer Vision is the ability to use AutoTrain for image classification. This feature allows users to train models with zero configuration needed, simply by choosing their task, uploading their data, and letting Hugging Face handle the rest.",Image Classification
"What is the default repository type created by the `create_repo` function on Hugging Face Hub?
","[""Firstly, there are a few methods to manage repository creation, deletion, and others:\n```python no-format\nfrom huggingface\\_hub import (\n# User management\nlogin,\nlogout,\nwhoami,\n# Repository creation and management\ncreate\\_repo,\ndelete\\_repo,\nupdate\\_repo\\_visibility,\n# And some methods to retrieve/change information about the content\nlist\\_models,\nlist\\_datasets,\nlist\\_metrics,\nlist\\_repo\\_files,\nupload\\_file,\ndelete\\_file,\n)\n```\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them. The `create\\_repo` method can be used to create a new repository on the hub:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"")\n```\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"", organization=\""huggingface\"")\n```\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization."", ""## How to duplicate or fork a repo (including LFS pointers)\nIf you'd like to copy a repository, depending on whether you want to preserve the Git history there are two options. ### Duplicating without Git history\nIn many scenarios, if you want your own copy of a particular codebase you might not be concerned about the previous Git history. In this case, you can quickly duplicate a repo with the handy [Repo Duplicator](https://huggingface.co/spaces/huggingface-projects/repo\\_duplicator)! You'll have to create a User Access Token, which you can read more about in the [security documentation](./security-tokens). ### Duplicating with the Git history (Fork)\nA duplicate of a repository with the commit history preserved is called a \\*fork\\*. You may choose to fork one of your own repos, but it also common to fork other people's projects if you would like to tinker with them. \\*\\*Note that you will need to [install Git LFS](https://git-lfs.github.com/) and the [`huggingface\\_hub` CLI](https://huggingface.co/docs/huggingface\\_hub/index) to follow this process\\*\\*. When you want to fork or [rebase](https://git-scm.com/docs/git-rebase) a repository with LFS files you cannot use the usual Git approach that you might be familiar with since you need to be careful to not break the LFS pointers. Forking can take time depending on your bandwidth because you will have to fetch and re-upload all the LFS files in your fork."", ""## What should I use ? And when ? Overall, the \\*\\*HTTP-based approach is the recommended way to use\\*\\* `huggingface\\_hub`\nin all cases. [`HfApi`] allows to pull and push changes, work with PRs, tags and branches, interact with discussions and much more. Since the `0.16` release, the http-based methods can also run in the background, which was the last major advantage of the [`Repository`] class. However, not all git commands are available through [`HfApi`]. Some may never be implemented, but we are always trying to improve and close the gap. If you don't see your use case covered, please open [an issue on Github](https://github.com/huggingface/huggingface\\_hub)! We welcome feedback to help build the 🤗 ecosystem with and for our users. This preference of the http-based [`HfApi`] over the git-based [`Repository`] does not mean that git versioning will disappear from the Hugging Face Hub anytime soon. It will always be possible to use `git` commands locally in workflows where it makes sense."", ""FrameworkSwitchCourse {fw} />\n# Sharing pretrained models[[sharing-pretrained-models]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nIn the steps below, we'll take a look at the easiest ways to share pretrained models to the 🤗 Hub. There are tools and utilities available that make it simple to share and update models directly on the Hub, which we will explore below. We encourage all users that train models to contribute by sharing them with the community — sharing models, even when trained on very specific datasets, will help others, saving them time and compute resources and providing access to useful trained artifacts. In turn, you can benefit from the work that others have done! There are three ways to go about creating new model repositories:\n- Using the `push\\_to\\_hub` API\n- Using the `huggingface\\_hub` Python library\n- Using the web interface\nOnce you've created a repository, you can upload files to it via git and git-lfs. We'll walk you through creating model repositories and uploading files to them in the following sections. ## Using the `push\\_to\\_hub` API[[using-the-pushtohub-api]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nThe simplest way to upload files to the Hub is by leveraging the `push\\_to\\_hub` API."", ""Define a `HF\\_TOKEN` secret to be automatically authenticated! Authentication via an environment variable or a secret has priority over the token stored on your machine. ### Method parameters\nFinally, it is also possible to authenticate by passing your token to any method that accepts `token` as a parameter. ```\nfrom transformers import whoami\nuser = whoami(token=...)\n```\nThis is usually discouraged except in an environment where you don't want to store your token permanently or if you need to handle several tokens at once. Please be careful when passing tokens as a parameter. It is always best practice to load the token from a secure vault instead of hardcoding it in your codebase or notebook. Hardcoded tokens present a major leak risk if you share your code inadvertently. ## Create a repository\nOnce you've registered and logged in, create a repository with the [`create\\_repo`]\nfunction:\n```py\n>>> from huggingface\\_hub import HfApi\n>>> api = HfApi()\n>>> api.create\\_repo(repo\\_id=\""super-cool-model\"")\n```\nIf you want your repository to be private, then:\n```py\n>>> from huggingface\\_hub import HfApi\n>>> api = HfApi()\n>>> api.create\\_repo(repo\\_id=\""super-cool-model\"", private=True)\n```\nPrivate repositories will not be visible to anyone except yourself. To create a repository or to push content to the Hub, you must provide a User Access\nToken that has the `write` permission. You can choose the permission when creating the\ntoken in your [Settings page](https://huggingface.co/settings/tokens). ## Upload files\nUse the [`upload\\_file`] function to add a file to your newly created repository.""]","The default repository type created by the `create_repo` function on Hugging Face Hub is a public repository. If you want to create a private repository, you need to specify the `private=True` argument when calling the function.",model
"How many splits does the ""duorc"" dataset have?
","[""List splits and configurations\nDatasets typically have splits and may also have configurations. A \\_split\\_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A \\_configuration\\_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you're interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load\\_hub)! This guide shows you how to use Datasets Server's `/splits` endpoint to retrieve a dataset's splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\nThe `/splits` endpoint accepts the dataset name as its query parameter:\n\n```python\nimport requests\nheaders = {\""Authorization\"": f\""Bearer {API\\_TOKEN}\""}\nAPI\\_URL = \""https://datasets-server.huggingface.co/splits?dataset=duorc\""\ndef query():\nresponse = requests.get(API\\_URL, headers=headers)\nreturn response.json()\ndata = query()\n```\n\n```js\nimport fetch from \""node-fetch\"";\nasync function query(data) {\nconst response = await fetch(\n\""https://datasets-server.huggingface.co/splits?dataset=duorc\"",\n{\nheaders: { Authorization: `Bearer ${API\\_TOKEN}` },\nmethod: \""GET\""\n}\n);\nconst result = await response.json();\nreturn result;\n}\nquery().then((response) => {\nconsole.log(JSON.stringify(response));\n});\n```\n\n```curl\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\n-X GET \\\n-H \""Authorization: Bearer ${API\\_TOKEN}\""\n```\nThe endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n```json\n{\n\""splits\"": [\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""test\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""test\"" }\n],\n\""pending\"": [],\n\""failed\"": []\n}\n```"", ""Next, standard cross-entropy (for\nthe classes) and a linear combination of the L1 and [generalized IoU loss](https://giou.stanford.edu/) (for the\nbounding boxes) are used to optimize the parameters of the model. DETR can be naturally extended to perform panoptic segmentation (which unifies semantic segmentation and instance\nsegmentation). [`~transformers.DetrForSegmentation`] adds a segmentation mask head on top of\n[`~transformers.DetrForObjectDetection`]. The mask head can be trained either jointly, or in a two steps process,\nwhere one first trains a [`~transformers.DetrForObjectDetection`] model to detect bounding boxes around both\n\""things\"" (instances) and \""stuff\"" (background things like trees, roads, sky), then freeze all the weights and train only\nthe mask head for 25 epochs. Experimentally, these two approaches give similar results. Note that predicting boxes is\nrequired for the training to be possible, since the Hungarian matching is computed using distances between boxes. ## Usage tips\n- DETR uses so-called \\*\\*object queries\\*\\* to detect objects in an image. The number of queries determines the maximum\nnumber of objects that can be detected in a single image, and is set to 100 by default (see parameter\n`num\\_queries` of [`~transformers.DetrConfig`]). Note that it's good to have some slack (in COCO, the\nauthors used 100, while the maximum number of objects in a COCO image is ~70)."", "". ! - \\; \\: \\\"" “ % ‘ ” � \\\n--fp16 \\\n--group\\_by\\_length \\\n--push\\_to\\_hub \\\n--do\\_train --do\\_eval\n```\nOn 8 V100 GPUs, this script should run in \\*ca.\\* 18 minutes and yield a CTC loss of \\*\\*0.39\\*\\* and word error rate\nof \\*\\*0.36\\*\\*. ### Multi GPU CTC with Dataset Streaming\nThe following command shows how to use [Dataset Streaming mode](https://huggingface.co/docs/datasets/dataset\\_streaming)\nto fine-tune [XLS-R](https://huggingface.co/transformers/main/model\\_doc/xls\\_r.html)\non [Common Voice](https://huggingface.co/datasets/common\\_voice) using 4 GPUs in half-precision. Streaming mode imposes several constraints on training:\n1. We need to construct a tokenizer beforehand and define it via `--tokenizer\\_name\\_or\\_path`. 2. `--num\\_train\\_epochs` has to be replaced by `--max\\_steps`. Similarly, all other epoch-based arguments have to be\nreplaced by step-based ones. 3. Full dataset shuffling on each epoch is not possible, since we don't have the whole dataset available at once. However, the `--shuffle\\_buffer\\_size` argument controls how many examples we can pre-download before shuffling them. ```bash\n\\*\\*torchrun \\\n--nproc\\_per\\_node 4 run\\_speech\\_recognition\\_ctc\\_streaming.py \\\n--dataset\\_name=\""common\\_voice\"" \\\n--model\\_name\\_or\\_path=\""facebook/wav2vec2-xls-r-300m\"" \\\n--tokenizer\\_name\\_or\\_path=\""anton-l/wav2vec2-tokenizer-turkish\"" \\\n--dataset\\_config\\_name=\""tr\"" \\\n--train\\_split\\_name=\""train+validation\"" \\\n--eval\\_split\\_name=\""test\"" \\\n--output\\_dir=\""wav2vec2-xls-r-common\\_voice-tr-ft\"" \\\n--overwrite\\_output\\_dir \\\n--max\\_steps=\""5000\"" \\\n--per\\_device\\_train\\_batch\\_size=\""8\"" \\\n--gradient\\_accumulation\\_steps=\""2\"" \\\n--learning\\_rate=\""5e-4\"" \\\n--warmup\\_steps=\""500\"" \\\n--evaluation\\_strategy=\""steps\"" \\\n--text\\_column\\_name=\""sentence\"" \\\n--save\\_steps=\""500\"" \\\n--eval\\_steps=\""500\"" \\\n--logging\\_steps=\""1\"" \\\n--layerdrop=\""0.0\"" \\\n--eval\\_metrics wer cer \\\n--save\\_total\\_limit=\""1\"" \\\n--mask\\_time\\_prob=\""0.3\"" \\\n--mask\\_time\\_length=\""10\"" \\\n--mask\\_feature\\_prob=\""0.1\"" \\\n--mask\\_feature\\_length=\""64\"" \\\n--freeze\\_feature\\_encoder \\\n--chars\\_to\\_ignore , ?"", ""- Dataset streaming\nTODO(Patrick)"", ""atasets and DataFrames equals love. Although the processing functions of Datasets will cover most the cases needed to train a model, there are times when you’ll need to switch to a library like Pandas to access more powerful features or high-level APIs for visualisation. Fortunately, Datasets is designed to be interoperable with libraries like Pandas, as well as NumPy, PyTorch, TensorFlow, and JAX. In this video, we'll take a look at how we can quickly switch our data to Pandas DataFrames and back. As an example, let's suppose we're analysing Supreme Court cases from Switzerland. As usual we download our dataset from the Hub using the load\\_dataset() function, and you can see that the first element of the training set is an ordinary Python dictionary with various fields of interest. Now suppose that before we train any models, we'd like to explore the data a bit. For example we might be interested in knowing which legal area is most common or we might want to know how the languages are distributed across regions. Answering these questions with the native Arrow format isn't easy, but we can easily switch to Pandas to get our answers!""]","The ""duorc"" dataset has six splits.",Six
"What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Fully Sharded Data Parallel\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or \\*rank\\*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class. Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer. ```bash\npip install accelerate\n```\n## FSDP configuration\nTo start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package\\_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`. ```bash\naccelerate config\n```\nWhen you run `accelerate config`, you'll be prompted with a series of options to configure your training environment."", ""This solution allows 🤗 Datasets to perform distributed predictions, which is important for evaluation speed in distributed settings. At the same time, you can also use complex non-additive metrics without wasting valuable GPU or CPU memory."", ""Therefore those communication costs are going to hurt. In addition, there are already fewer layers than normal due to PP and so the memory savings won't be huge. PP already reduces gradient size by ``1/PP``, and so gradient sharding savings on top of that are less significant than pure DP. ZeRO stage 3 can also be used to train models at this scale, however, it requires more communication than the DeepSpeed 3D parallel implementation. After careful evaluation in our environment which happened a year ago we found Megatron-DeepSpeed 3D parallelism performed best. Since then ZeRO stage 3 performance has dramatically improved and if we were to evaluate it today perhaps we would have chosen stage 3 instead. ## BF16Optimizer\nTraining huge LLM models in FP16 is a no-no. We have proved it to ourselves by spending several months [training a 104B model](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr8-104B-wide) which as you can tell from the [tensorboard](https://huggingface.co/bigscience/tr8-104B-logs/tensorboard) was but a complete failure. We learned a lot of things while fighting the ever diverging lm-loss:\n![104B-fail](assets/86\\_bloom\\_megatron\\_deepspeed/104b-lm-loss.png)\nand we also got the same advice from the Megatron-LM and DeepSpeed teams after they trained the [530B model](https://arxiv.org/abs/2201.11990). The recent release of [OPT-175B](https://arxiv.org/abs/2205.01068) too reported that they had a very difficult time training in FP16. So back in January as we knew we would be training on A100s which support the BF16 format Olatunji Ruwase developed a `BF16Optimizer` which we used to train BLOOM."", ""This wraps as much training as possible while still being able to train on distributed systems without the user needing to do anything at all. First we need to import the Trainer:\n```python\nfrom transformers import Trainer\n```\nThen we define some `TrainingArguments` to control all the usual hyper-parameters. The trainer also works through dictionaries, so a custom collate function needs to be made. Finally, we subclass the trainer and write our own `compute\\_loss`. Afterwards, this code will also work on a distributed setup without any training code needing to be written whatsoever! ```python\nfrom transformers import Trainer, TrainingArguments\nmodel = BasicNet()\ntraining\\_args = TrainingArguments(\n\""basic-trainer\"",\nper\\_device\\_train\\_batch\\_size=64,\nper\\_device\\_eval\\_batch\\_size=64,\nnum\\_train\\_epochs=1,\nevaluation\\_strategy=\""epoch\"",\nremove\\_unused\\_columns=False\n)\ndef collate\\_fn(examples):\npixel\\_values = torch.stack([example[0] for example in examples])\nlabels = torch.tensor([example[1] for example in examples])\nreturn {\""x\"":pixel\\_values, \""labels\"":labels}\nclass MyTrainer(Trainer):\ndef compute\\_loss(self, model, inputs, return\\_outputs=False):\noutputs = model(inputs[\""x\""])\ntarget = inputs[\""labels\""]\nloss = F.nll\\_loss(outputs, target)\nreturn (loss, outputs) if return\\_outputs else loss\ntrainer = MyTrainer(\nmodel,\ntraining\\_args,\ntrain\\_dataset=train\\_dset,\neval\\_dataset=test\\_dset,\ndata\\_collator=collate\\_fn,\n)\n```\n```python\ntrainer.train()\n```\n```python out\n\\*\\*\\*\\*\\* Running training \\*\\*\\*\\*\\*\nNum examples = 60000\nNum Epochs = 1\nInstantaneous batch size per device = 64\nTotal train batch size (w. parallel, distributed & accumulation) = 64\nGradient Accumulation steps = 1\nTotal optimization steps = 938\n```\n| Epoch | Training Loss | Validation Loss |\n|-------|---------------|-----------------|\n| 1 | 0.875700 | 0.282633 |\nSimilarly to the above examples with the `notebook\\_launcher`, this can be done again here by throwing it all into a training function:\n```python\ndef train\\_trainer\\_ddp():\nmodel = BasicNet()\ntraining\\_args = TrainingArguments(\n\""basic-trainer\"",\nper\\_device\\_train\\_batch\\_size=64,\nper\\_device\\_eval\\_batch\\_size=64,\nnum\\_train\\_epochs=1,\nevaluation\\_strategy=\""epoch\"",\nremove\\_unused\\_columns=False\n)\ndef collate\\_fn(examples):\npixel\\_values = torch.stack([example[0] for example in examples])\nlabels = torch.tensor([example[1] for example in examples])\nreturn {\""x\"":pixel\\_values, \""labels\"":labels}\nclass MyTrainer(Trainer):\ndef compute\\_loss(self, model, inputs, return\\_outputs=False):\noutputs = model(inputs[\""x\""])\ntarget = inputs[\""labels\""]\nloss = F.nll\\_loss(outputs, target)\nreturn (loss, outputs) if return\\_outputs else loss\ntrainer = MyTrainer(\nmodel,\ntraining\\_args,\ntrain\\_dataset=train\\_dset,\neval\\_dataset=test\\_dset,\ndata\\_collator=collate\\_fn,\n)\ntrainer.train()\nnotebook\\_launcher(train\\_trainer\\_ddp, args=(), num\\_processes=2)\n```\n## Resources\nTo learn more about PyTorch Distributed Data Parallelism, check out the documentation [here](https://pytorch.org/docs/stable/distributed.html)\nTo learn more about 🤗 Accelerate, check out the documentation [here](https://huggingface.co/docs/accelerate)\nTo learn more about 🤗 Transformers, check out the documentation [here](https://huggingface.co/docs/transformers)"", ""This would result in the CPU RAM getting out of memory leading to processes being terminated. 2. Saving entire intermediate checkpoints using `FULL\\_STATE\\_DICT` with CPU offloading on rank 0 takes a lot of time and often results in NCCL Timeout errors due to indefinite hanging during broadcasting. However, at the end of training, we want the whole model state dict instead of the sharded state dict which is only compatible with FSDP. 3. We need to improve the speed and reduce the VRAM usage to train faster and save compute costs. Let’s look at how to solve the above challenges and fine-tune a 70B model! Before we get started, here's all the required resources to reproduce our results:\n1. Codebase:\nhttps://github.com/pacman100/DHS-LLM-Workshop/tree/main/chat\\_assistant/training with flash-attn V2 monkey patch\n2. FSDP config: https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat\\_assistant/training/configs/fsdp\\_config.yaml\n3. SLURM script `launch.slurm`: https://gist.github.com/pacman100/1cb1f17b2f1b3139a63b764263e70b25\n4. Model: `meta-llama/Llama-2-70b-chat-hf`\n5. Dataset: [smangrul/code-chat-assistant-v1](https://huggingface.co/datasets/smangrul/code-chat-assistant-v1) (mix of LIMA+GUANACO with proper formatting in a ready-to-train format)\n### Pre-requisites\nFirst follow these steps to install Flash Attention V2: Dao-AILab/flash-attention: Fast and memory-efficient exact attention (github.com).""]","Fully Sharded Data Parallel (FSDP) is a data parallel method that shards a model's parameters, gradients, and optimizer states across the available GPUs (also called workers or ranks). The purpose of FSDP is to reduce memory usage by not replicating the entire model on each GPU, which improves GPU memory efficiency. This allows for the training of much larger models on fewer GPUs. FSDP is integrated with the Accelerate library, making it easier to manage training in distributed environments.","FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes."
"What file format is used to save and store PyTorch model weights more securely than `.bin` files?
","[""Convert weights to safetensors\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`. The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository. For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights. Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space."", ""If you are not familiar with this data format, please have a look [at the bits layout]( https://en.wikipedia.org/wiki/Bfloat16\\_floating-point\\_format#bfloat16\\_floating-point\\_format). The key to BF16 format is that it has the same exponent as FP32 and thus doesn't suffer from overflow FP16 suffers from a lot! With FP16, which has a max numerical range of 64k, you can only multiply small numbers. e.g. you can do `250\\*250=62500`, but if you were to try `255\\*255=65025` you got yourself an overflow, which is what causes the main problems during training. This means your weights have to remain tiny. A technique called loss scaling can help with this problem, but the limited range of FP16 is still an issue when models become very large. BF16 has no such problem, you can easily do `10\\_000\\*10\\_000=100\\_000\\_000` and it's no problem. Of course, since BF16 and FP16 have the same size of 2 bytes, one doesn't get a free lunch and one pays with really bad precision when using BF16. However, if you remember the training using stochastic gradient descent and its variations is a sort of stumbling walk, so if you don't get the perfect direction immediately it's no problem, you will correct yourself in the next steps. Regardless of whether one uses BF16 or FP16 there is also a copy of weights which is always in FP32 - this is what gets updated by the optimizer. So the 16-bit formats are only used for the computation, the optimizer updates the FP32 weights with full precision and then casts them into the 16-bit format for the next iteration. All PyTorch components have been updated to ensure that they perform any accumulation in FP32, so no loss happening there. One crucial issue is gradient accumulation, and it's one of the main features of pipeline parallelism as the gradients from each microbatch processing get accumulated. It's crucial to implement gradient accumulation in FP32 to keep the training precise, and this is what `BF16Optimizer` does. Besides other improvements we believe that using BF16 mixed precision training turned a potential nightmare into a relatively smooth process which can be observed from the following lm loss graph:\n![176B-fail](assets/86\\_bloom\\_megatron\\_deepspeed/176b-lm-loss.png)\n## Fused CUDA Kernels\nThe GPU performs two things."", ""I've taken care to make sure any third party code included or adapted has compatible (permissive) licenses such as MIT, BSD, etc. I've made an effort to avoid any GPL / LGPL conflicts. That said, it is your responsibility to ensure you comply with licenses here and conditions of any dependent licenses. Where applicable, I've linked the sources/references for various components in docstrings. If you think I've missed anything please create an issue. ### Pretrained Weights\nSo far all of the pretrained weights available here are pretrained on ImageNet with a select few that have some additional pretraining (see extra note below). ImageNet was released for non-commercial research purposes only (https://image-net.org/download). It's not clear what the implications of that are for the use of pretrained weights from that dataset. Any models I have trained with ImageNet are done for research purposes and one should assume that the original dataset license applies to the weights. It's best to seek legal advice if you intend to use the pretrained weights in a commercial product. #### Pretrained on more than ImageNet\nSeveral weights included or references here were pretrained with proprietary datasets that I do not have access to. These include the Facebook WSL, SSL, SWSL ResNe(Xt) and the Google Noisy Student EfficientNet models. The Facebook models have an explicit non-commercial license (CC-BY-NC 4.0, https://github.com/facebookresearch/semi-supervised-ImageNet1K-models, https://github.com/facebookresearch/WSL-Images). The Google models do not appear to have any restriction beyond the Apache 2.0 license (and ImageNet concerns). In either case, you should contact Facebook or Google with any questions. ## Citing\n### BibTeX\n```bibtex\n@misc{rw2019timm,\nauthor = {Ross Wightman},\ntitle = {PyTorch Image Models},\nyear = {2019},\npublisher = {GitHub},\njournal = {GitHub repository},\ndoi = {10.5281/zenodo.4414861},\nhowpublished = {\\url{https://github.com/rwightman/pytorch-image-models}}\n}\n```\n### Latest DOI\n[![DOI](https://zenodo.org/badge/168799526.svg)](https://zenodo.org/badge/latestdoi/168799526)"", ""- The byte buffer needs to be entirely indexed, and cannot contain holes. This prevents\nthe creation of polyglot files. - Endianness: Little-endian. moment. - Order: 'C' or row-major. ### Yet another format ? The main rationale for this crate is to remove the need to use\n`pickle` on `PyTorch` which is used by default. There are other formats out there used by machine learning and more general\nformats. Let's take a look at alternatives and why this format is deemed interesting. This is my very personal and probably biased view:\n| Format | Safe | Zero-copy | Lazy loading | No file size limit | Layout control | Flexibility | Bfloat16/Fp8\n| ----------------------- | --- | --- | --- | --- | --- | --- | --- |\n| pickle (PyTorch) | ✗ | ✗ | ✗ | 🗸 | ✗ | 🗸 | 🗸 |\n| H5 (Tensorflow) | 🗸 | ✗ | 🗸 | 🗸 | ~ | ~ | ✗ |\n| SavedModel (Tensorflow) | 🗸 | ✗ | ✗ | 🗸 | 🗸 | ✗ | 🗸 |\n| MsgPack (flax) | 🗸 | 🗸 | ✗ | 🗸 | ✗ | ✗ | 🗸 |\n| Protobuf (ONNX) | 🗸 | ✗ | ✗ | ✗ | ✗ | ✗ | 🗸 |\n| Cap'n'Proto | 🗸 | 🗸 | ~ | 🗸 | 🗸 | ~ | ✗ |\n| Arrow | ?"", ""You can't use this variant if you're continuing training or using a CPU. - Non-exponential mean averaged (EMA) weights, which shouldn't be used for inference. You should use these to continue fine-tuning a model. 💡 When the checkpoints have identical model structures, but they were trained on different datasets and with a different training setup, they should be stored in separate repositories instead of variations (for example, [`stable-diffusion-v1-4`] and [`stable-diffusion-v1-5`]). Otherwise, a variant is \\*\\*identical\\*\\* to the original checkpoint. They have exactly the same serialization format (like [Safetensors](./using\\_safetensors)), model structure, and weights that have identical tensor shapes. | \\*\\*checkpoint type\\*\\* | \\*\\*weight name\\*\\* | \\*\\*argument for loading weights\\*\\* |\n|---------------------|-------------------------------------|----------------------------------|\n| original | diffusion\\_pytorch\\_model.bin | |\n| floating point | diffusion\\_pytorch\\_model.fp16.bin | `variant`, `torch\\_dtype` |\n| non-EMA | diffusion\\_pytorch\\_model.non\\_ema.bin | `variant` |\nThere are two important arguments to know for loading variants:\n- `torch\\_dtype` defines the floating point precision of the loaded checkpoints.""]",The more secure file format used to save and store PyTorch model weights is `.safetensors`.,`.safetensors`
"What type of security certification does Hugging Face have?
","[""Security\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)"", ""The certification process\nThe certification process is \\*\\*completely free\\*\\*:\n- To get a \\*certificate of completion\\*: you need \\*\\*to pass 80% of the assignments\\*\\*. - To get a \\*certificate of excellence\\*: you need \\*\\*to pass 100% of the assignments\\*\\*. There's \\*\\*no deadlines, the course is self-paced\\*\\*. ![Course certification](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit0/certification.jpg)\nWhen we say pass, \\*\\*we mean that your model must be pushed to the Hub and get a result equal or above the minimal requirement\\*\\*. To check your progression and which unit you passed/not passed: https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course\nNow that you're ready for the certification process, you need to:\n1. Go here: https://huggingface.co/spaces/huggingface-projects/Deep-RL-Course-Certification/\n2. Type your \\*hugging face username\\*, your \\*first name\\*, \\*last name\\*\n3. Click on \""Generate my certificate\"". - If you passed 80% of the assignments, \\*\\*congratulations\\*\\* you've just got the certificate of completion. - If you passed 100% of the assignments, \\*\\*congratulations\\*\\* you've just got the excellence certificate. - If you are below 80%, don't be discouraged!"", ""For more info on the Hugging Face and Habana Gaudi collaboration, please visit [https://huggingface.co/Habana](https://huggingface.co/Habana)."", ""Hugging Face Inference Endpoints documentation\n## Setup\n```bash\npip install hf-doc-builder==0.4.0 watchdog --upgrade\n```\n## Local Development\n```bash\ndoc-builder preview endpoints docs/source/ --not\\_python\\_module\n```\n## Build Docs\n```bash\ndoc-builder build endpoints docs/source/ --build\\_dir build/ --not\\_python\\_module\n```\n## Add assets/Images\nAdding images/assets is only possible through `https://` links meaning you need to use `https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/` prefix. example\n```bash\n![Login](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/0_login.png)\n```\n## Generate API Reference\n1. Copy openapi spec from `https://api.endpoints.huggingface.cloud/api-doc/openapi.json`\n2. create markdown `widdershins --environment env.json openapi.json -o myOutput.md`\n3. copy into `api\\_reference.mdx`"", ""\\* Set Name ID format to EmailAddress. \\* Under \""Show Advanced Settings\"", verify that Response and Assertion Signature are set to: Signed. Save your new application. ### Step 3: Finalize configuration on Hugging Face\nIn your Okta application, under \""Sign On/Settings/More details\"", find the following fields:\n- Sign-on URL\n- Public certificate\n- SP Entity ID\nYou will need them to finalize the SSO setup on Hugging Face. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-okta-guide-saml-4.png)\n\nIn the SSO section of your organization's settings, copy-paste these values from Okta:\n- Sign-on URL\n- SP Entity ID\n- Public certificate\nThe public certificate must have the following format:\n```\n-----BEGIN CERTIFICATE-----\n{certificate}\n-----END CERTIFICATE-----\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-okta-guide-saml-5.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-okta-guide-saml-5-dark.png)\n\nYou can now click on \""Update and Test SAML configuration\"" to save the settings. You should be redirected to your SSO provider (IdP) login prompt. Once logged in, you'll be redirected to your organization's settings page. A green check mark near the SAML selector will attest that the test was successful. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-okta-guide-saml-6.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-okta-guide-saml-6-dark.png)\n\n### Step 4: Enable SSO in your organization\nNow that Single Sign-On is configured and tested, you can enable it for members of your organization by clicking on the \""Enable\"" button. Once enabled, members of your organization must complete the SSO authentication flow described in the [How does it work?](./security-sso#how-does-it-work) section.""]","Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses.",SOC2 Type 2 certified
"What do RAG models combine to generate outputs?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# RAG\n\n[![Models](https://img.shields.io/badge/All_model_pages-rag-blueviolet)](https://huggingface.co/models?filter=rag)\n\n## Overview\nRetrieval-augmented generation (\""RAG\"") models combine the powers of pretrained dense retrieval (DPR) and\nsequence-to-sequence models."", ""These outputs are then passed to the decoder. The decoder uses the encoder's output alongside other inputs, in order to generate a prediction. It then predicts an output, which it will re-use in future iterations, hence the term \""auto-regressive\"". Finally, to get an understanding of the encoder-decoders as a whole, we recommend you check out the video on encoder-decoders."", ""It can simply be created by setting the values of the `inputs` and `outputs` parameter as the same component. Here's an example demo of a text generation model:\n$code\\_unified\\_demo\\_text\\_generation\n$demo\\_unified\\_demo\\_text\\_generation"", ""Here we have the `loss` since we passed along `labels`, but we don't have\n`hidden\\_states` and `attentions` because we didn't pass `output\\_hidden\\_states=True` or\n`output\\_attentions=True`. When passing `output\\_hidden\\_states=True` you may expect the `outputs.hidden\\_states[-1]` to match `outputs.last\\_hidden\\_states` exactly. However, this is not always the case. Some models apply normalization or subsequent process to the last hidden state when it's returned. You can access each attribute as you would usually do, and if that attribute has not been returned by the model, you\nwill get `None`. Here for instance `outputs.loss` is the loss computed by the model, and `outputs.attentions` is\n`None`. When considering our `outputs` object as tuple, it only considers the attributes that don't have `None` values. Here for instance, it has two elements, `loss` then `logits`, so\n```python\noutputs[:2]\n```\nwill return the tuple `(outputs.loss, outputs.logits)` for instance. When considering our `outputs` object as dictionary, it only considers the attributes that don't have `None`\nvalues. Here for instance, it has two keys that are `loss` and `logits`. We document here the generic model outputs that are used by more than one model type. Specific output types are\ndocumented on their corresponding model page. ## ModelOutput\n[[autodoc]] utils.ModelOutput\n- to\\_tuple\n## BaseModelOutput\n[[autodoc]] modeling\\_outputs.BaseModelOutput\n## BaseModelOutputWithPooling\n[[autodoc]] modeling\\_outputs.BaseModelOutputWithPooling\n## BaseModelOutputWithCrossAttentions\n[[autodoc]] modeling\\_outputs.BaseModelOutputWithCrossAttentions\n## BaseModelOutputWithPoolingAndCrossAttentions\n[[autodoc]] modeling\\_outputs.BaseModelOutputWithPoolingAndCrossAttentions\n## BaseModelOutputWithPast\n[[autodoc]] modeling\\_outputs.BaseModelOutputWithPast\n## BaseModelOutputWithPastAndCrossAttentions\n[[autodoc]] modeling\\_outputs.BaseModelOutputWithPastAndCrossAttentions\n## Seq2SeqModelOutput\n[[autodoc]] modeling\\_outputs.Seq2SeqModelOutput\n## CausalLMOutput\n[[autodoc]] modeling\\_outputs.CausalLMOutput\n## CausalLMOutputWithCrossAttentions\n[[autodoc]] modeling\\_outputs.CausalLMOutputWithCrossAttentions\n## CausalLMOutputWithPast\n[[autodoc]] modeling\\_outputs.CausalLMOutputWithPast\n## MaskedLMOutput\n[[autodoc]] modeling\\_outputs.MaskedLMOutput\n## Seq2SeqLMOutput\n[[autodoc]] modeling\\_outputs.Seq2SeqLMOutput\n## NextSentencePredictorOutput\n[[autodoc]] modeling\\_outputs.NextSentencePredictorOutput\n## SequenceClassifierOutput\n[[autodoc]] modeling\\_outputs.SequenceClassifierOutput\n## Seq2SeqSequenceClassifierOutput\n[[autodoc]] modeling\\_outputs.Seq2SeqSequenceClassifierOutput\n## MultipleChoiceModelOutput\n[[autodoc]] modeling\\_outputs.MultipleChoiceModelOutput\n## TokenClassifierOutput\n[[autodoc]] modeling\\_outputs.TokenClassifierOutput\n## QuestionAnsweringModelOutput\n[[autodoc]] modeling\\_outputs.QuestionAnsweringModelOutput\n## Seq2SeqQuestionAnsweringModelOutput\n[[autodoc]] modeling\\_outputs.Seq2SeqQuestionAnsweringModelOutput\n## Seq2SeqSpectrogramOutput\n[[autodoc]] modeling\\_outputs.Seq2SeqSpectrogramOutput\n## SemanticSegmenterOutput\n[[autodoc]] modeling\\_outputs.SemanticSegmenterOutput\n## ImageClassifierOutput\n[[autodoc]] modeling\\_outputs.ImageClassifierOutput\n## ImageClassifierOutputWithNoAttention\n[[autodoc]] modeling\\_outputs.ImageClassifierOutputWithNoAttention\n## DepthEstimatorOutput\n[[autodoc]] modeling\\_outputs.DepthEstimatorOutput\n## Wav2Vec2BaseModelOutput\n[[autodoc]] modeling\\_outputs.Wav2Vec2BaseModelOutput\n## XVectorOutput\n[[autodoc]] modeling\\_outputs.XVectorOutput\n## Seq2SeqTSModelOutput\n[[autodoc]] modeling\\_outputs.Seq2SeqTSModelOutput\n## Seq2SeqTSPredictionOutput\n[[autodoc]] modeling\\_outputs.Seq2SeqTSPredictionOutput\n## SampleTSPredictionOutput\n[[autodoc]] modeling\\_outputs.SampleTSPredictionOutput\n## TFBaseModelOutput\n[[autodoc]] modeling\\_tf\\_outputs.TFBaseModelOutput\n## TFBaseModelOutputWithPooling\n[[autodoc]] modeling\\_tf\\_outputs.TFBaseModelOutputWithPooling\n## TFBaseModelOutputWithPoolingAndCrossAttentions\n[[autodoc]] modeling\\_tf\\_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions\n## TFBaseModelOutputWithPast\n[[autodoc]] modeling\\_tf\\_outputs.TFBaseModelOutputWithPast\n## TFBaseModelOutputWithPastAndCrossAttentions\n[[autodoc]] modeling\\_tf\\_outputs.TFBaseModelOutputWithPastAndCrossAttentions\n## TFSeq2SeqModelOutput\n[[autodoc]] modeling\\_tf\\_outputs.TFSeq2SeqModelOutput\n## TFCausalLMOutput\n[[autodoc]] modeling\\_tf\\_outputs.TFCausalLMOutput\n## TFCausalLMOutputWithCrossAttentions\n[[autodoc]] modeling\\_tf\\_outputs.TFCausalLMOutputWithCrossAttentions\n## TFCausalLMOutputWithPast\n[[autodoc]] modeling\\_tf\\_outputs.TFCausalLMOutputWithPast\n## TFMaskedLMOutput\n[[autodoc]] modeling\\_tf\\_outputs.TFMaskedLMOutput\n## TFSeq2SeqLMOutput\n[[autodoc]] modeling\\_tf\\_outputs.TFSeq2SeqLMOutput\n## TFNextSentencePredictorOutput\n[[autodoc]] modeling\\_tf\\_outputs.TFNextSentencePredictorOutput\n## TFSequenceClassifierOutput\n[[autodoc]] modeling\\_tf\\_outputs.TFSequenceClassifierOutput\n## TFSeq2SeqSequenceClassifierOutput\n[[autodoc]] modeling\\_tf\\_outputs.TFSeq2SeqSequenceClassifierOutput\n## TFMultipleChoiceModelOutput\n[[autodoc]] modeling\\_tf\\_outputs.TFMultipleChoiceModelOutput\n## TFTokenClassifierOutput\n[[autodoc]] modeling\\_tf\\_outputs.TFTokenClassifierOutput\n## TFQuestionAnsweringModelOutput\n[[autodoc]] modeling\\_tf\\_outputs.TFQuestionAnsweringModelOutput\n## TFSeq2SeqQuestionAnsweringModelOutput\n[[autodoc]] modeling\\_tf\\_outputs.TFSeq2SeqQuestionAnsweringModelOutput\n## FlaxBaseModelOutput\n[[autodoc]] modeling\\_flax\\_outputs.FlaxBaseModelOutput\n## FlaxBaseModelOutputWithPast\n[[autodoc]] modeling\\_flax\\_outputs.FlaxBaseModelOutputWithPast\n## FlaxBaseModelOutputWithPooling\n[[autodoc]] modeling\\_flax\\_outputs.FlaxBaseModelOutputWithPooling\n## FlaxBaseModelOutputWithPastAndCrossAttentions\n[[autodoc]] modeling\\_flax\\_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions\n## FlaxSeq2SeqModelOutput\n[[autodoc]] modeling\\_flax\\_outputs.FlaxSeq2SeqModelOutput\n## FlaxCausalLMOutputWithCrossAttentions\n[[autodoc]] modeling\\_flax\\_outputs.FlaxCausalLMOutputWithCrossAttentions\n## FlaxMaskedLMOutput\n[[autodoc]] modeling\\_flax\\_outputs.FlaxMaskedLMOutput\n## FlaxSeq2SeqLMOutput\n[[autodoc]] modeling\\_flax\\_outputs.FlaxSeq2SeqLMOutput\n## FlaxNextSentencePredictorOutput\n[[autodoc]] modeling\\_flax\\_outputs.FlaxNextSentencePredictorOutput\n## FlaxSequenceClassifierOutput\n[[autodoc]] modeling\\_flax\\_outputs.FlaxSequenceClassifierOutput\n## FlaxSeq2SeqSequenceClassifierOutput\n[[autodoc]] modeling\\_flax\\_outputs.FlaxSeq2SeqSequenceClassifierOutput\n## FlaxMultipleChoiceModelOutput\n[[autodoc]] modeling\\_flax\\_outputs.FlaxMultipleChoiceModelOutput\n## FlaxTokenClassifierOutput\n[[autodoc]] modeling\\_flax\\_outputs.FlaxTokenClassifierOutput\n## FlaxQuestionAnsweringModelOutput\n[[autodoc]] modeling\\_flax\\_outputs.FlaxQuestionAnsweringModelOutput\n## FlaxSeq2SeqQuestionAnsweringModelOutput\n[[autodoc]] modeling\\_flax\\_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput"", ""This can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion, especially with a small number of layers. Growing the receptive field has been shown to improve quality significantly (Salimans et al.). Doing so, however, comes at a significant cost in number of parameters and consequently computational performance and can make training such models more challenging. >\n> ... self-attention can achieve a better balance in the trade-off between the virtually unlimited receptive field of the necessarily sequential PixelRNN and the limited receptive field of the much more parallelizable PixelCNN and its various extensions. [Image Transformer](https://arxiv.org/abs/1802.05751) uses transformers by restricting self attention over local neighborhoods of pixels. [Taming Transformers](https://arxiv.org/abs/2012.09841) and [DALL-E 1](https://arxiv.org/abs/2102.12092) combine convolutions and transformers. Both train a VQ-VAE to learn a discrete latent space, and then a transformer is trained in the compressed latent space. The transformer context is global but masked, because attention is provided over all previously predicted latent pixels, but the model is still AR so attention cannot be provided over not yet predicted pixels. [ImageBART](https://arxiv.org/abs/2108.08827) combines convolutions, transformers, and diffusion processes.""]",RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs.,Pretrained dense retrieval (DPR) and sequence-to-sequence models.
"What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# MarkupLM\n## Overview\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm). The model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838\\_From\\_one\\_tree\\_to\\_a\\_forest\\_a\\_unified\\_solution\\_for\\_structured\\_web\\_data\\_extraction), a dataset\nfor information extraction from web pages (basically named-entity recogntion on web pages)\nThe abstract from the paper is the following:\n\\*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\ntasks. The pre-trained model and code will be publicly available.\\*\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm). ## Usage tips\n- In addition to `input\\_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath\\_tags\\_seq` and `xpath\\_subs\\_seq`. These are the XPATH tags and subscripts respectively for each token in the input sequence. - One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info. ![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg)\n MarkupLM architecture. Taken from the [original paper.](https://arxiv.org/abs/2110.08518) \n## Usage: MarkupLMProcessor\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input\\_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks. ```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\nfeature\\_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from\\_pretrained(\""microsoft/markuplm-base\"")\nprocessor = MarkupLMProcessor(feature\\_extractor, tokenizer)\n```\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input\\_ids`, `attention\\_mask`, `token\\_type\\_ids`, `xpath\\_subs\\_seq`, `xpath\\_tags\\_seq`. Optionally, one can provide node labels to the processor, which are turned into token-level `labels`. [`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]."", ""`@gradio/code`\n```html\n```\nBaseCode\n```javascript\nexport let classNames = \""\"";\nexport let value = \""\"";\nexport let dark\\_mode: boolean;\nexport let basic = true;\nexport let language: string;\nexport let lines = 5;\nexport let extensions: Extension[] = [];\nexport let useTab = true;\nexport let readonly = false;\nexport let placeholder: string | HTMLElement | null | undefined = undefined;\n```\nBaseCopy\n```javascript\nexport let value: string;\n```\nBaseDownload\n```javascript\nexport let value: string;\nexport let language: string;\n```\nBaseWidget\n```javascript\nexport let value: string;\nexport let language: string;\n```\nBaseExample\n```\nexport let value: string;\nexport let type: \""gallery\"" | \""table\"";\nexport let selected = false;\n```"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Utilities for `FeatureExtractors`\nThis page lists all the utility functions that can be used by the audio [`FeatureExtractor`] in order to compute special features from a raw audio using common algorithms such as \\*Short Time Fourier Transform\\* or \\*log mel spectrogram\\*. Most of those are only useful if you are studying the code of the audio processors in the library. ## Audio Transformations\n[[autodoc]] audio\\_utils.hertz\\_to\\_mel\n[[autodoc]] audio\\_utils.mel\\_to\\_hertz\n[[autodoc]] audio\\_utils.mel\\_filter\\_bank\n[[autodoc]] audio\\_utils.optimal\\_fft\\_length\n[[autodoc]] audio\\_utils.window\\_function\n[[autodoc]] audio\\_utils.spectrogram\n[[autodoc]] audio\\_utils.power\\_to\\_db\n[[autodoc]] audio\\_utils.amplitude\\_to\\_db"", ""his simple demo takes advantage of Gradio's HighlightedText, JSON and HTML outputs to create a clear NER segmentation."", ""If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. ## ASTConfig\n[[autodoc]] ASTConfig\n## ASTFeatureExtractor\n[[autodoc]] ASTFeatureExtractor\n- \\_\\_call\\_\\_\n## ASTModel\n[[autodoc]] ASTModel\n- forward\n## ASTForAudioClassification\n[[autodoc]] ASTForAudioClassification\n- forward""]","MarkupLMFeatureExtractor uses Beautiful Soup, a Python library for pulling data out of HTML and XML files, to extract data from HTML and XML documents.",Beautiful Soup
"What is the file size limit for syncing to HF Spaces without using Git-LFS?
","[""There are a few reasons for this:\n- Uploading and downloading smaller files is much easier both for you and the other users. Connection issues can always\nhappen when streaming data and smaller files avoid resuming from the beginning in case of errors. - Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\nleading to a slower download speed. In all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size. - \\*\\*Number of commits\\*\\*: There is no hard limit for the total number of commits on your repo history. However, from\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a\nfresh start using `huggingface\\_hub`'s [`super\\_squash\\_history`](https://huggingface.co/docs/huggingface\\_hub/main/en/package\\_reference/hf\\_api#huggingface\\_hub.HfApi.super\\_squash\\_history)."", ""repo\\_id=repo\\_id,\n... repo\\_type=\""space\"",\n... space\\_sdk=\""gradio\""\n... space\\_storage=\""large\"",\n... )\n```\n```py\n>>> api.duplicate\\_space(\n... from\\_id=repo\\_id,\n... storage=\""large\"",\n... )\n```\n## More advanced: temporarily upgrade your Space ! Spaces allow for a lot of different use cases. Sometimes, you might want\nto temporarily run a Space on a specific hardware, do something and then shut it down. In\nthis section, we will explore how to benefit from Spaces to finetune a model on demand. This is only one way of solving this particular problem. It has to be taken as a suggestion\nand adapted to your use case. Let's assume we have a Space to finetune a model. It is a Gradio app that takes as input\na model id and a dataset id. The workflow is as follows:\n0. (Prompt the user for a model and a dataset)\n1. Load the model from the Hub. 2. Load the dataset from the Hub. 3. Finetune the model on the dataset. 4. Upload the new model to the Hub. Step 3. requires a custom hardware but you don't want your Space to be running all the time on a paid\nGPU. A solution is to dynamically request hardware for the training and shut it\ndown afterwards. Since requesting hardware restarts your Space, your app must somehow \""remember\""\nthe current task it is performing."", ""Files are saved in the default `huggingface\\_hub` disk cache `~/.cache/huggingface/hub`. If you application expects them elsewhere or you changed your `HF\\_HOME` variable, this pre-loading does not follow that at this time."", ""This means you can use them but we do not guarantee they will be\nmaintained in the future. In particular, we might update the API or behavior of such features without any deprecation\ncycle. A warning message is triggered when using an experimental feature to warn you about it. If you're comfortable debugging any potential issues using an experimental feature, you can set `HF\\_HUB\\_DISABLE\\_EXPERIMENTAL\\_WARNING=1` to disable the warning. If you are using an experimental feature, please let us know! Your feedback can help us design and improve it. ### HF\\_HUB\\_DISABLE\\_TELEMETRY\nBy default, some data is collected by HF libraries (`transformers`, `datasets`, `gradio`,..) to monitor usage, debug issues and help prioritize features. Each library defines its own policy (i.e. which usage to monitor) but the core implementation happens in `huggingface\\_hub` (see [`send\\_telemetry`]). You can set `HF\\_HUB\\_DISABLE\\_TELEMETRY=1` as environment variable to globally disable telemetry. ### HF\\_HUB\\_ENABLE\\_HF\\_TRANSFER\nSet to `True` for faster uploads and downloads from the Hub using `hf\\_transfer`."", ""Managing Spaces with Github Actions\nYou can keep your app in sync with your GitHub repository with \\*\\*Github Actions\\*\\*. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup. First, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository. ```bash\ngit remote add space https://huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME\n```\nThen force push to sync everything for the first time:\n```bash\ngit push --force space main\n```\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:\n\\* Replace `HF\\_USERNAME` with your username and `SPACE\\_NAME` with your Space name. \\* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF\\_TOKEN`. You can find your Hugging Face API token under \\*\\*API Tokens\\*\\* on your Hugging Face profile. ```yaml\nname: Sync to Hugging Face hub\non:\npush:\nbranches: [main]\n# to run this workflow manually from the Actions tab\nworkflow\\_dispatch:\njobs:\nsync-to-hub:\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@v3\nwith:\nfetch-depth: 0\nlfs: true\n- name: Push to hub\nenv:\nHF\\_TOKEN: ${{ secrets.HF\\_TOKEN }}\nrun: git push https://HF\\_USERNAME:$HF\\_TOKEN@huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME main\n```\nFinally, create an Action that automatically checks the file size of any new pull request:\n```yaml\nname: Check file size\non: # or directly `on: [push]` to run the action on every push on any branch\npull\\_request:\nbranches: [main]\n# to run this workflow manually from the Actions tab\nworkflow\\_dispatch:\njobs:\nsync-to-hub:\nruns-on: ubuntu-latest\nsteps:\n- name: Check large files\nuses: ActionsDesk/lfs-warning@v2.0\nwith:\nfilesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\n```""]",The file size limit for syncing to HF Spaces without using Git-LFS is 10MB.,10MB
"What is the title of the paper introducing the ByT5 model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ByT5\n## Overview\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, Colin Raffel."", ""## UMT5Config\n[[autodoc]] UMT5Config\n## UMT5Model\n[[autodoc]] UMT5Model\n- forward\n## UMT5ForConditionalGeneration\n[[autodoc]] UMT5ForConditionalGeneration\n- forward\n## UMT5EncoderModel\n[[autodoc]] UMT5EncoderModel\n- forward\n## UMT5ForSequenceClassification\n[[autodoc]] UMT5ForSequenceClassification\n- forward\n## UMT5ForQuestionAnswering\n[[autodoc]] UMT5ForQuestionAnswering\n- forward"", ""Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte. 1. \\*\\*[SwitchTransformers](https://huggingface.co/docs/transformers/model\\_doc/switch\\_transformers)\\*\\* (from Google) released with the paper [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer. 1. \\*\\*[T5](https://huggingface.co/docs/transformers/model\\_doc/t5)\\*\\* (来自 Google AI)कॉलिन रैफेल और नोम शज़ीर और एडम रॉबर्ट्स और कैथरीन ली और शरण नारंग और माइकल मटेना द्वारा साथ में पेपर [एक एकीकृत टेक्स्ट-टू-टेक्स्ट ट्रांसफॉर्मर के साथ स्थानांतरण सीखने की सीमा की खोज] (https://arxiv.org/abs/1910.10683) और यांकी झोउ और वेई ली और पीटर जे लियू।\n1. \\*\\*[T5v1.1](https://huggingface.co/docs/transformers/model\\_doc/t5v1.1)\\*\\* (Google AI से) साथ वाला पेपर [google-research/text-to-text-transfer- ट्रांसफॉर्मर](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released\\_checkpoints.md#t511) कॉलिन रैफेल और नोम शज़ीर और एडम रॉबर्ट्स और कैथरीन ली और शरण नारंग द्वारा और माइकल मटेना और यांकी झोउ और वेई ली और पीटर जे लियू।\n1. \\*\\*[Table Transformer](https://huggingface.co/docs/transformers/model\\_doc/table-transformer)\\*\\* (माइक्रोसॉफ्ट रिसर्च से) साथ में पेपर [पबटेबल्स-1एम: टूवर्ड्स कॉम्प्रिहेंसिव टेबल एक्सट्रैक्शन फ्रॉम अनस्ट्रक्चर्ड डॉक्यूमेंट्स ](https://arxiv.org/abs/2110.00061) ब्रैंडन स्मॉक, रोहित पेसाला, रॉबिन अब्राहम द्वारा पोस्ट किया गया।\n1. \\*\\*[TAPAS](https://huggingface.co/docs/transformers/model\\_doc/tapas)\\*\\* (Google AI से) साथ में कागज [TAPAS: पूर्व-प्रशिक्षण के माध्यम से कमजोर पर्यवेक्षण तालिका पार्सिंग](https:// arxiv.org/abs/2004.02349) जोनाथन हर्ज़िग, पावेल क्रिज़िस्तोफ़ नोवाक, थॉमस मुलर, फ्रांसेस्को पिकिन्नो और जूलियन मार्टिन ईसेन्च्लोस द्वारा।\n1. \\*\\*[TAPEX](https://huggingface.co/docs/transformers/model\\_doc/tapex)\\*\\* (माइक्रोसॉफ्ट रिसर्च से) साथ में पेपर [TAPEX: टेबल प्री-ट्रेनिंग थ्रू लर्निंग अ न्यूरल SQL एक्ज़ीक्यूटर](https: //arxiv.org/abs/2107.07653) कियान लियू, बेई चेन, जियाकी गुओ, मोर्टेज़ा ज़ियादी, ज़ेकी लिन, वीज़ू चेन, जियान-गुआंग लू द्वारा पोस्ट किया गया।\n1."", ""## Conclusion\nSpeechT5 is an interesting model because — unlike most other models — it allows you to perform multiple tasks with the same architecture. Only the pre-nets and post-nets change. By pre-training the model on these combined tasks, it becomes more capable at doing each of the individual tasks when fine-tuned. We have only included checkpoints for the speech recognition (ASR), speech synthesis (TTS), and voice conversion tasks but the paper also mentions the model was successfully used for speech translation, speech enhancement, and speaker identification. It’s very versatile!"", ""This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released\\_checkpoints.md#t511). ## Usage tips\nOne can directly plug in the weights of T5v1.1 into a T5 model, like so:\n```python\n>>> from transformers import T5ForConditionalGeneration\n>>> model = T5ForConditionalGeneration.from\\_pretrained(\""google/t5-v1\\_1-base\"")\n```\nT5 Version 1.1 includes the following improvements compared to the original T5 model:\n- GEGLU activation in the feed-forward hidden layer, rather than ReLU. See [this paper](https://arxiv.org/abs/2002.05202). - Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning. - Pre-trained on C4 only without mixing in the downstream tasks. - No parameter sharing between the embedding and classifier layer. - \""xl\"" and \""xxl\"" replace \""3B\"" and \""11B\"". The model shapes are a bit different - larger `d\\_model` and smaller\n`num\\_heads` and `d\\_ff`. Note: T5 Version 1.1 was only pre-trained on [C4](https://huggingface.co/datasets/c4) excluding any supervised\ntraining. Therefore, this model has to be fine-tuned before it is usable on a downstream task, unlike the original T5\nmodel. Since t5v1.1 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix. Google has released the following variants:\n- [google/t5-v1\\_1-small](https://huggingface.co/google/t5-v1\\_1-small)\n- [google/t5-v1\\_1-base](https://huggingface.co/google/t5-v1\\_1-base)\n- [google/t5-v1\\_1-large](https://huggingface.co/google/t5-v1\\_1-large)\n- [google/t5-v1\\_1-xl](https://huggingface.co/google/t5-v1\\_1-xl)\n- [google/t5-v1\\_1-xxl](https://huggingface.co/google/t5-v1\\_1-xxl). Refer to [T5's documentation page](t5) for all API reference, tips, code examples and notebooks.""]","The title of the paper introducing the ByT5 model is ""ByT5: Towards a token-free future with pre-trained byte-to-byte models.""",ByT5: Towards a token-free future with pre-trained byte-to-byte models
"What is the dimension of the feature vector for the base BERT model?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# BERT\n\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\n\n## Overview\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It's a\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\n\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\narchitecture modifications.\\*\n\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\*\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf)."", ""It's currently only used internally during certain operations. In the machine learning jargon FP32 is called full precision (4 bytes), while BF16 and FP16 are referred to as half-precision (2 bytes). On top of that, the int8 (INT8) data type consists of an 8-bit representation that can store 2^8 different values (between [0, 255] or [-128, 127] for signed integers). While, ideally the training and inference should be done in FP32, it is two times slower than FP16/BF16 and therefore a mixed precision approach is used where the weights are held in FP32 as a precise \""main weights\"" reference, while computation in a forward and backward pass are done for FP16/BF16 to enhance training speed. The FP16/BF16 gradients are then used to update the FP32 main weights. During training, the main weights are always stored in FP32, but in practice, the half-precision weights often provide similar quality during inference as their FP32 counterpart -- a precise reference of the model is only needed when it receives multiple gradient updates. This means we can use the half-precision weights and use half the GPUs to accomplish the same outcome. ![Model-storage](assets/96\\_hf\\_bitsandbytes\\_integration/Model-storage.png)\nTo calculate the model size in bytes, one multiplies the number of parameters by the size of the chosen precision in bytes. For example, if we use the bfloat16 version of the BLOOM-176B model, we have `176\\*10\\*\\*9 x 2 bytes = 352GB`!"", ""## L\n### labels\nThe labels are an optional argument which can be passed in order for the model to compute the loss itself. These labels\nshould be the expected prediction of the model: it will use the standard loss in order to compute the loss between its\npredictions and the expected value (the label). These labels are different according to the model head, for example:\n- For sequence classification models, ([`BertForSequenceClassification`]), the model expects a tensor of dimension\n`(batch\\_size)` with each value of the batch corresponding to the expected label of the entire sequence. - For token classification models, ([`BertForTokenClassification`]), the model expects a tensor of dimension\n`(batch\\_size, seq\\_length)` with each value corresponding to the expected label of each individual token. - For masked language modeling, ([`BertForMaskedLM`]), the model expects a tensor of dimension `(batch\\_size,\nseq\\_length)` with each value corresponding to the expected label of each individual token: the labels being the token\nID for the masked token, and values to be ignored for the rest (usually -100). - For sequence to sequence tasks, ([`BartForConditionalGeneration`], [`MBartForConditionalGeneration`]), the model\nexpects a tensor of dimension `(batch\\_size, tgt\\_seq\\_length)` with each value corresponding to the target sequences\nassociated with each input sequence."", ""BPE relies on a pre-tokenizer that splits the training data into\nwords. Pretokenization can be as simple as space tokenization, e.g. [GPT-2](model\\_doc/gpt2), [RoBERTa](model\\_doc/roberta). More advanced pre-tokenization include rule-based tokenization, e.g. [XLM](model\\_doc/xlm),\n[FlauBERT](model\\_doc/flaubert) which uses Moses for most languages, or [GPT](model\\_doc/gpt) which uses\nSpacy and ftfy, to count the frequency of each word in the training corpus. After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the\ntraining data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set\nof unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until\nthe vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to\ndefine before training the tokenizer. As an example, let's assume that after pre-tokenization, the following set of words including their frequency has been\ndetermined:\n```\n(\""hug\"", 10), (\""pug\"", 5), (\""pun\"", 12), (\""bun\"", 4), (\""hugs\"", 5)\n```\nConsequently, the base vocabulary is `[\""b\"", \""g\"", \""h\"", \""n\"", \""p\"", \""s\"", \""u\""]`. Splitting all words into symbols of the\nbase vocabulary, we obtain:\n```\n(\""h\"" \""u\"" \""g\"", 10), (\""p\"" \""u\"" \""g\"", 5), (\""p\"" \""u\"" \""n\"", 12), (\""b\"" \""u\"" \""n\"", 4), (\""h\"" \""u\"" \""g\"" \""s\"", 5)\n```\nBPE then counts the frequency of each possible symbol pair and picks the symbol pair that occurs most frequently."", ""Hence, models like BERT and RoBERTa are limited to a max sequence length of 512\ntokens. Perceiver aims to solve this issue by, instead of performing self-attention on the inputs, perform it on a set\nof latent variables, and only use the inputs for cross-attention. In this way, the time and memory requirements don't\ndepend on the length of the inputs anymore, as one uses a fixed amount of latent variables, like 256 or 512. These are\nrandomly initialized, after which they are trained end-to-end using backpropagation. Internally, [`PerceiverModel`] will create the latents, which is a tensor of shape `(batch\\_size, num\\_latents,\nd\\_latents)`. One must provide `inputs` (which could be text, images, audio, you name it!) to the model, which it will\nuse to perform cross-attention with the latents. The output of the Perceiver encoder is a tensor of the same shape. One\ncan then, similar to BERT, convert the last hidden states of the latents to classification logits by averaging along\nthe sequence dimension, and placing a linear layer on top of that to project the `d\\_latents` to `num\\_labels`.""]",The dimension of the feature vector for the base BERT model is 768.,768
"What special identifier does the WordPiece Model use for continuing subwords?
","[""In turn, the model will now be able to make sense of token in different situations. It will understand that the words token, tokens, tokenizing, and tokenization are linked and have a similar meaning. It will also understand that tokenization, modernization, and immunization, which all have the same suffixes, are probably used in the same syntactic situations. Subword-based tokenizers generally have a way to identify which tokens are start of words, and which tokens complete start of words: token as the start of a word. ##ization as completing a word. Here the ## prefix indicates that ization is part of a word rather than the beginning of it. The ## comes from the BERT tokenizer, based on the WordPiece algorithm. Other tokenizers use other prefixes, which can be placed to indicate part of words like seen here, or start of words instead! There are a lot of different algorithms that can be used for subword tokenization, and most models obtaining state-of-the-art results in English today use some kind of subword-tokenization algorithm."", ""In\nthe example above `\""h\""` followed by `\""u\""` is present \\_10 + 5 = 15\\_ times (10 times in the 10 occurrences of\n`\""hug\""`, 5 times in the 5 occurrences of `\""hugs\""`). However, the most frequent symbol pair is `\""u\""` followed by\n`\""g\""`, occurring \\_10 + 5 + 5 = 20\\_ times in total. Thus, the first merge rule the tokenizer learns is to group all\n`\""u\""` symbols followed by a `\""g\""` symbol together. Next, `\""ug\""` is added to the vocabulary. The set of words then\nbecomes\n```\n(\""h\"" \""ug\"", 10), (\""p\"" \""ug\"", 5), (\""p\"" \""u\"" \""n\"", 12), (\""b\"" \""u\"" \""n\"", 4), (\""h\"" \""ug\"" \""s\"", 5)\n```\nBPE then identifies the next most common symbol pair. It's `\""u\""` followed by `\""n\""`, which occurs 16 times. `\""u\""`,\n`\""n\""` is merged to `\""un\""` and added to the vocabulary. The next most frequent symbol pair is `\""h\""` followed by\n`\""ug\""`, occurring 15 times. Again the pair is merged and `\""hug\""` can be added to the vocabulary. At this stage, the vocabulary is `[\""b\"", \""g\"", \""h\"", \""n\"", \""p\"", \""s\"", \""u\"", \""ug\"", \""un\"", \""hug\""]` and our set of unique words\nis represented as\n```\n(\""hug\"", 10), (\""p\"" \""ug\"", 5), (\""p\"" \""un\"", 12), (\""b\"" \""un\"", 4), (\""hug\"" \""s\"", 5)\n```\nAssuming, that the Byte-Pair Encoding training would stop at this point, the learned merge rules would then be applied\nto new words (as long as those new words do not include symbols that were not in the base vocabulary)."", ""etc. |\n| 3 | Word number | |\n| 4 | Word | This is the token as segmented/tokenized in the Treebank. Initially the \\*\\_skel file contain the placeholder [WORD] which gets replaced by the actual token from the Treebank which is part of the OntoNotes release. |\n| 5 | Part-of-Speech | |\n| 6 | Parse bit | This is the bracketed structure broken before the first open parenthesis in the parse, and the word/part-of-speech leaf replaced with a \\*. The full parse can be created by substituting the asterix with the \""([pos] [word])\"" string (or leaf) and concatenating the items in the rows of that column. |\n| 7 | Predicate lemma | The predicate lemma is mentioned for the rows for which we have semantic role information. All other rows are marked with a \""-\"". |\n| 8 | Predicate Frameset ID | This is the PropBank frameset ID of the predicate in Column 7. |\n| 9 | Word sense | This is the word sense of the word in Column 3. |\n| 10 | Speaker/Author | This is the speaker or author name where available. Mostly in Broadcast Conversation and Web Log data. |\n| 11 | Named Entities | These columns identifies the spans representing various named entities."", ""Note that we did not exclude those special characters because the model has to learn to predict when a word is finished, otherwise predictions would always be a sequence of letters that would make it impossible to separate words from each other. One should always keep in mind that pre-processing is a very important step before training your model. E.g., we don't want our model to differentiate between `a` and `A` just because we forgot to normalize the data. The difference between `a` and `A` does not depend on the \""sound\"" of the letter at all, but more on grammatical rules - \\*e.g.\\* use a capitalized letter at the beginning of the sentence. So it is sensible to remove the difference between capitalized and non-capitalized letters so that the model has an easier time learning to transcribe speech. To make it clearer that `\"" \""` has its own token class, we give it a more visible character `|`. In addition, we also add an \""unknown\"" token so that the model can later deal with characters not encountered in Common Voice's training set. ```python\nvocab\\_dict[\""|\""] = vocab\\_dict[\"" \""]\ndel vocab\\_dict[\"" \""]\n```\nFinally, we also add a padding token that corresponds to CTC's \""\\*blank token\\*\""."", ""Models\n\n## BPE\n[[autodoc]] tokenizers.models.BPE\n## Model\n[[autodoc]] tokenizers.models.Model\n## Unigram\n[[autodoc]] tokenizers.models.Unigram\n## WordLevel\n[[autodoc]] tokenizers.models.WordLevel\n## WordPiece\n[[autodoc]] tokenizers.models.WordPiece\n\nThe Rust API Reference is available directly on the [Docs.rs](https://docs.rs/tokenizers/latest/tokenizers/) website.""]",The WordPiece Model uses the `##` prefix as a special identifier for continuing subwords. This prefix indicates that the following tokens are part of a word rather than the beginning of it.,##
"What is the purpose of the 🧨 Diffusers tutorials?
","[""🧨 Diffusers Experimental\nWe are adding experimental code to support novel applications and usages of the Diffusers library."", ""Models\nFor more detail on the models, please refer to the [docs](https://huggingface.co/docs/diffusers/api/models/overview)."", ""This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\nis very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline. ## Tweakable, contributor-friendly over abstraction\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t\\_repeat\\_yourself). In short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers. Functions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable. \\*\\*However\\*\\*, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions. - Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions. - Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel. At Hugging Face, we call this design the \\*\\*single-file policy\\*\\* which means that almost all of the code of a certain class should be written in a single, self-contained file."", ""Overview\nThese examples show how to run [Diffuser](https://arxiv.org/abs/2205.09991) in Diffusers. There are two ways to use the script, `run\\_diffuser\\_locomotion.py`. The key option is a change of the variable `n\\_guide\\_steps`. When `n\\_guide\\_steps=0`, the trajectories are sampled from the diffusion model, but not fine-tuned to maximize reward in the environment. By default, `n\\_guide\\_steps=2` to match the original implementation. You will need some RL specific requirements to run the examples:\n```\npip install -f https://download.pytorch.org/whl/torch\\_stable.html \\\nfree-mujoco-py \\\neinops \\\ngym==0.24.1 \\\nprotobuf==3.20.1 \\\ngit+https://github.com/rail-berkeley/d4rl.git \\\nmediapy \\\nPillow==9.0.0\n```"", ""For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.\\*\\*\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!""]","The purpose of the 🧨 Diffusers tutorials is to provide experimental code to support novel applications and usages of the Diffusers library. These tutorials aim to help users understand how to run and implement various functionalities within the Diffusers framework, including the interaction between different model components and the customization of diffusion pipelines.",To provide a gentle introduction to diffusion models and help understand the library fundamentals.
"What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?
","[""Using Flagging\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\nTags: FLAGGING, DATA\n## Introduction\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \""hard\"" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust. Gradio simplifies the collection of this data by including a \\*\\*Flag\\*\\* button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`. ## The \\*\\*Flag\\*\\* button in `gradio.Interface`\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked \\*\\*Flag\\*\\*. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file. There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail. - `allow\\_flagging`: this parameter can be set to either `\""manual\""` (default), `\""auto\""`, or `\""never\""`. - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked. - `auto`: users will not see a button to flag, but every sample will be flagged automatically. - `never`: users will not see a button to flag, and no sample will be flagged. - `flagging\\_options`: this parameter can be either `None` (default) or a list of strings. - If `None`, then the user simply clicks on the \\*\\*Flag\\*\\* button and no additional options are shown. - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\""Incorrect\"", \""Ambiguous\""]`, then buttons labeled \\*\\*Flag as Incorrect\\*\\* and \\*\\*Flag as Ambiguous\\*\\* appear. This only applies if `allow\\_flagging` is `\""manual\""`. - The chosen option is then logged along with the input and output. - `flagging\\_dir`: this parameter takes a string. - It represents what to name the directory where flagged data is stored. - `flagging\\_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n- Using this parameter allows you to write custom code that gets run when the flag button is clicked\n- By default, this is set to an instance of `gr.CSVLogger`\n- One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset."", ""It uses Gradio's Audio and Label components."", ""- \\*\\*Files that you explicitly allow via the `allowed\\_paths` parameter in `launch()`\\*\\*. This parameter allows you to pass in a list of additional directories or exact filepaths you'd like to allow users to have access to. (By default, this parameter is an empty list). Gradio DOES NOT ALLOW access to:\n- \\*\\*Files that you explicitly block via the `blocked\\_paths` parameter in `launch()`\\*\\*. You can pass in a list of additional directories or exact filepaths to the `blocked\\_paths` parameter in `launch()`. This parameter takes precedence over the files that Gradio exposes by default or by the `allowed\\_paths`. - \\*\\*Any other paths on the host machine\\*\\*. Users should NOT be able to access other arbitrary paths on the host. Please make sure you are running the latest version of `gradio` for these security settings to apply."", ""This change will improve performance for many applications. Thanks [@pngwn](https://github.com/pngwn)! - [#5216](https://github.com/gradio-app/gradio/pull/5216) [`4b58ea6d`](https://github.com/gradio-app/gradio/commit/4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8) - Update i18n tokens and locale files. Thanks [@hannahblair](https://github.com/hannahblair)! - [#5232](https://github.com/gradio-app/gradio/pull/5232) [`c57d4c23`](https://github.com/gradio-app/gradio/commit/c57d4c232a97e03b4671f9e9edc3af456438fe89) - `gr.Radio` and `gr.CheckboxGroup` can now accept different names and values. Thanks [@abidlabs](https://github.com/abidlabs)!"", ""`@gradio/button`\n```html\n\ncontent\n```""]","The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `""manual""`.","""manual"""
"Where can the full code for the Stable Diffusion demo be found?
","[""ote: This is a simplified version of the code needed to create the Stable Diffusion demo."", ""For [Stable Diffusion v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5) on 512x512 images:\n| | Latency (batch size = 1) | Throughput |\n| ---------------------- |:------------------------:|:---------------------------:|\n| first-generation Gaudi | 3.80s | 0.308 images/s (batch size = 8) |\n| Gaudi2 | 1.33s | 1.081 images/s (batch size = 8) |\nFor [Stable Diffusion v2.1](https://huggingface.co/stabilityai/stable-diffusion-2-1) on 768x768 images:\n| | Latency (batch size = 1) | Throughput |\n| ---------------------- |:------------------------:|:-------------------------------:|\n| first-generation Gaudi | 10.2s | 0.108 images/s (batch size = 4) |\n| Gaudi2 | 3.17s | 0.379 images/s (batch size = 8) |"", ""❤️\n🧪 Many of the SDXL ControlNet checkpoints are experimental, and there is a lot of room for improvement. Feel free to open an [Issue](https://github.com/huggingface/diffusers/issues/new/choose) and leave us feedback on how we can improve! Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## StableDiffusionXLControlNetXSPipeline\n[[autodoc]] StableDiffusionXLControlNetXSPipeline\n- all\n- \\_\\_call\\_\\_\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput"", ""--\ntitle: 🧨 Stable Diffusion in JAX / Flax ! thumbnail: /blog/assets/108\\_stable\\_diffusion\\_jax/thumbnail.png\nauthors:\n- user: pcuenq\n- user: patrickvonplaten\n---\n# 🧨 Stable Diffusion in JAX / Flax ! [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_jax_how_to.ipynb)\n# \\*\\*Stable Diffusion in JAX / Flax\\*\\* 🚀\n🤗 Hugging Face [Diffusers](https://github.com/huggingface/diffusers) supports Flax since version `0.5.1`! This allows for super fast inference on Google TPUs, such as those available in Colab, Kaggle or Google Cloud Platform. This post shows how to run inference using JAX / Flax. If you want more details about how Stable Diffusion works or want to run it in GPU, please refer to [this Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). If you want to follow along, click the button above to open this post as a Colab notebook. First, make sure you are using a TPU backend. If you are running this notebook in Colab, select `Runtime` in the menu above, then select the option \""Change runtime type\"" and then select `TPU` under the `Hardware accelerator` setting. Note that JAX is not exclusive to TPUs, but it shines on that hardware because each TPU server has 8 TPU accelerators working in parallel. ## Setup\n``` python\nimport jax\nnum\\_devices = jax.device\\_count()\ndevice\\_type = jax.devices()[0].device\\_kind\nprint(f\""Found {num\\_devices} JAX devices of type {device\\_type}.\"")\nassert \""TPU\"" in device\\_type, \""Available device is not a TPU, please select TPU from Edit > Notebook settings > Hardware accelerator\""\n```\n\\*Output\\*:\n```bash\nFound 8 JAX devices of type TPU v2."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Safe Stable Diffusion\nSafe Stable Diffusion was proposed in [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://huggingface.co/papers/2211.05105) and mitigates inappropriate degeneration from Stable Diffusion models because they're trained on unfiltered web-crawled datasets. For instance Stable Diffusion may unexpectedly generate nudity, violence, images depicting self-harm, and otherwise offensive content. Safe Stable Diffusion is an extension of Stable Diffusion that drastically reduces this type of content. The abstract from the paper is:\n\\*Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications.""]",The full code for the Stable Diffusion demo can be found in the [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).,https://hf.co/spaces/stabilityai/stable-diffusion/tree/main
"What transformation does the FNet model use to replace the self-attention layer in a BERT model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# FNet\n## Overview\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:\n\\*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \""mix\"" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \""efficient\"" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.\\*\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f\\_net). ## Usage tips\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\nsequence length for fine-tuning and inference. ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## FNetConfig\n[[autodoc]] FNetConfig\n## FNetTokenizer\n[[autodoc]] FNetTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## FNetTokenizerFast\n[[autodoc]] FNetTokenizerFast\n## FNetModel\n[[autodoc]] FNetModel\n- forward\n## FNetForPreTraining\n[[autodoc]] FNetForPreTraining\n- forward\n## FNetForMaskedLM\n[[autodoc]] FNetForMaskedLM\n- forward\n## FNetForNextSentencePrediction\n[[autodoc]] FNetForNextSentencePrediction\n- forward\n## FNetForSequenceClassification\n[[autodoc]] FNetForSequenceClassification\n- forward\n## FNetForMultipleChoice\n[[autodoc]] FNetForMultipleChoice\n- forward\n## FNetForTokenClassification\n[[autodoc]] FNetForTokenClassification\n- forward\n## FNetForQuestionAnswering\n[[autodoc]] FNetForQuestionAnswering\n- forward"", ""2017](https://arxiv.org/abs/1706.03762). This blog post uses the same notation and coloring as the popular blog post [The illustrated transformer](http://jalammar.github.io/illustrated-transformer/), so the reader is strongly advised to read this blog first. \\*\\*Important\\*\\*: While Reformer was originally introduced for causal self-attention, it can very well be used for bi-directional self-attention as well. In this post, Reformer's self-attention is presented for \\*bidirectional\\* self-attention. ### Recap Global Self-Attention\nThe core of every Transformer model is the \\*\\*self-attention\\*\\* layer. To recap the conventional self-attention layer, which we refer to here as the \\*\\*global self-attention\\*\\* layer, let us assume we apply a transformer layer on the embedding vector sequence \\\\(\\mathbf{X} = \\mathbf{x}\\_1, \\ldots, \\mathbf{x}\\_n\\\\) where each vector \\\\(\\mathbf{x}\\_{i}\\\\) is of size `config.hidden\\_size`, \\*i.e.\\* \\\\(d\\_h\\\\). In short, a global self-attention layer projects \\\\(\\mathbf{X}\\\\) to the query, key and value matrices \\\\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\\\) and computes the output \\\\(\\mathbf{Z}\\\\) using the \\*softmax\\* operation as follows:\n\\\\(\\mathbf{Z} = \\text{SelfAttn}(\\mathbf{X}) = \\text{softmax}(\\mathbf{Q}\\mathbf{K}^T) \\mathbf{V}\\\\) with \\\\(\\mathbf{Z}\\\\) being of dimension \\\\(d\\_h \\times n\\\\) (leaving out the key normalization factor and self-attention weights \\\\(\\mathbf{W}^{O}\\\\) for simplicity)."", ""## References\n1. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. [https://arxiv.org/abs/1908.10084](https://arxiv.org/abs/1908.10084)"", ""Alright, let\\'s warm-start our \\*BERT2BERT\\* model. As mentioned earlier\nwe will warm-start both the encoder and decoder with the\n`\""bert-base-cased\""` checkpoint. ```python\nbert2bert = EncoderDecoderModel.from\\_encoder\\_decoder\\_pretrained(\""bert-base-uncased\"", \""bert-base-uncased\"")\n```\n```python\nOUTPUT:\n-------\n\""\""\""Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq\\_relationship.weight', 'cls.seq\\_relationship.bias']\n- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias']\""\""\""\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\""\""\""\n```\nFor once, we should take a good look at the warning here."", ""However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model\\_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model\\_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model\\_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix. ### Decoder[[nlp-decoder]]\n[GPT-2](model\\_doc/gpt2) is a decoder-only Transformer that predicts the next word in the sequence.""]",The FNet model replaces the self-attention layer in a BERT model with a Fourier transform that returns only the real parts of the transform.,Fourier transform
"What type of test should typically accompany a bug fix in Gradio's testing strategy?
","[""Test Strategy\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to. This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Objectives\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n1. Ensure that the Gradio library functions as we expect it to. 2. Enable the maintenance team to quickly identify both the presence and source of defects. 3. Prevent regressions, i.e. if we fix something it should stay fixed. 4. Improve the quality of the codebase in order to ease maintenance efforts. 5. Reduce the amount of manual testing required. ## Scope\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas. We should not focus on code coverage but on test coverage following the below criteria:\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n## Types of testing\nOur tests will broadly fall into one of three categories:\n- Static Quality checks\n- Dynamic 'Code' tests\n- Dynamic Functional tests\n### Static Quality checks\nStatic quality checks are generally very fast to run and do not require building the code base."", ""gradio\n## 4.11.0\n### Features\n- [#6842](https://github.com/gradio-app/gradio/pull/6842) [`846d52d`](https://github.com/gradio-app/gradio/commit/846d52d1c92d429077382ce494eea27fd062d9f6) - Fix md highlight. Thanks [@pngwn](https://github.com/pngwn)! - [#6831](https://github.com/gradio-app/gradio/pull/6831) [`f3abde8`](https://github.com/gradio-app/gradio/commit/f3abde80884d96ad69b825020c46486d9dd5cac5) - Add an option to enable header links for markdown. Thanks [@pngwn](https://github.com/pngwn)! - [#6814](https://github.com/gradio-app/gradio/pull/6814) [`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d) - Refactor queue so that there are separate queues for each concurrency id. Thanks [@aliabid94](https://github.com/aliabid94)! - [#6809](https://github.com/gradio-app/gradio/pull/6809) [`1401d99`](https://github.com/gradio-app/gradio/commit/1401d99ade46d87da75b5f5808a3354c49f1d1ea) - Fix `ImageEditor` interaction story. Thanks [@hannahblair](https://github.com/hannahblair)! - [#6803](https://github.com/gradio-app/gradio/pull/6803) [`77c9003`](https://github.com/gradio-app/gradio/commit/77c900311e2ba37b8f849ce088ceb48aa196af18) - Fixes issue 5781: Enables specifying a caching directory for Examples. Thanks [@cswamy](https://github.com/cswamy)! - [#6823](https://github.com/gradio-app/gradio/pull/6823) [`67a2b7f`](https://github.com/gradio-app/gradio/commit/67a2b7f12cb06355fcc41e40d47e8b2ad211d7d1) - Fixed duplicate word (\""this this\""). Thanks [@Cassini-chris](https://github.com/Cassini-chris)!"", ""Gradio Demo: queue\\_full\\_e2e\\_test\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nimport time\nimport random\nn\\_calls = 0\ndef get\\_random\\_number():\nglobal n\\_calls\nif n\\_calls == 1:\nn\\_calls += 1\nraise gr.Error(\""This is a gradio error\"")\nn\\_calls += 1\ntime.sleep(5)\nreturn random.randrange(1, 10)\nwith gr.Blocks() as demo:\nwith gr.Row():\nwith gr.Column():\nfirst = gr.Button(\""First Call\"")\nsecond = gr.Button(\""Second Call\"")\nthird = gr.Button(\""Third Call\"")\nfourth = gr.Button(\""Fourth Call\"")\nwith gr.Column():\nfirst\\_o = gr.Number(label=\""First Result\"")\nsecond\\_o = gr.Number(label=\""Second Result\"")\nthird\\_o = gr.Number(label=\""Third Result\"")\nfourth\\_o = gr.Number(label=\""Fourth Result\"")\nfirst.click(get\\_random\\_number, None, first\\_o, concurrency\\_id=\""f\"")\nsecond.click(get\\_random\\_number, None, second\\_o, concurrency\\_id=\""f\"")\nthird.click(get\\_random\\_number, None, third\\_o, concurrency\\_id=\""f\"")\nfourth.click(get\\_random\\_number, None, fourth\\_o, concurrency\\_id=\""f\"")\ndemo.queue(max\\_size=2)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""If you'd like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help. If you like Gradio, please leave us a ⭐ on GitHub! ## Open Source Stack\nGradio is built on top of many wonderful open-source libraries! [![huggingface](readme_files/huggingface_mini.svg)](https://huggingface.co)\n[![python](readme_files/python.svg)](https://www.python.org)\n[![fastapi](readme_files/fastapi.svg)](https://fastapi.tiangolo.com)\n[![encode](readme_files/encode.svg)](https://www.encode.io)\n[![svelte](readme_files/svelte.svg)](https://svelte.dev)\n[![vite](readme_files/vite.svg)](https://vitejs.dev)\n[![pnpm](readme_files/pnpm.svg)](https://pnpm.io)\n[![tailwind](readme_files/tailwind.svg)](https://tailwindcss.com)\n[![storybook](readme_files/storybook.svg)](https://storybook.js.org/)\n[![chromatic](readme_files/chromatic.svg)](https://www.chromatic.com/)\n## License\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository. ## Citation\nAlso check out the paper \\_[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019\\_, and please cite it if you use Gradio in your work. ```\n@article{abid2019gradio,\ntitle = {Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild},\nauthor = {Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},\njournal = {arXiv preprint arXiv:1906.02569},\nyear = {2019},\n}\n```"", ""### Testing and Infrastructure Changes:\nNo changes to highlight. ### Breaking Changes:\nNo changes to highlight. ### Full Changelog:\nNo changes to highlight. ### Contributors Shoutout:\nNo changes to highlight. # 0.2.3\n### New Features:\nNo changes to highlight. ### Bug Fixes:\n- Fix example inputs for `gr.File(file\\_count='multiple')` output components by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4153](https://github.com/gradio-app/gradio/pull/4153)\n### Documentation Changes:\nNo changes to highlight. ### Testing and Infrastructure Changes:\nNo changes to highlight. ### Breaking Changes:\nNo changes to highlight. ### Full Changelog:\nNo changes to highlight. ### Contributors Shoutout:\nNo changes to highlight. # 0.2.2\n### New Features:\nNo changes to highlight. ### Bug Fixes:\n- Only send request to `/info` route if demo version is above `3.28.3` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4109](https://github.com/gradio-app/gradio/pull/4109)\n### Other Changes:\n- Fix bug in test from gradio 3.29.0 refactor by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4138](https://github.com/gradio-app/gradio/pull/4138)\n### Breaking Changes:\nNo changes to highlight. # 0.2.1\n### New Features:\nNo changes to highlight. ### Bug Fixes:\nRemoves extraneous `State` component info from the `Client.view\\_api()` method by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)\n### Documentation Changes:\nNo changes to highlight. ### Testing and Infrastructure Changes:\nSeparates flaky tests from non-flaky tests by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)\n### Breaking Changes:\nNo changes to highlight.""]",A bug fix in Gradio's testing strategy should typically be accompanied by dynamic 'Code' tests wherever it is reasonably possible.,Dynamic code test
"How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?
","[""upercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on which you can run your training: it could be on CPU, GPUs, TPUs. Distributed on one machine with several devices, or several machines (often called nodes) each with multiple devices. On top of that there are new tweaks to make your training faster or more memory efficient, like mixed precision and DeepSpeed. Each of those setups or training tweaks, requires you to change the code of your training loop in one way or another and to learn a new API. All those setups are handled by the Trainer API, and there are several third-party libraries that can also help you with that. The problem with those is that they can feel like a black box and that it might not be easy to implement the tweak to the training loop you need. Accelerate has been designed specifically to let you retain full control over your training loop and be as non-intrusive as possible. With just four lines to add to your training loop (here shown on the code of the training loop from the \""Raw training loop\"" video), Accelerate will handle all the setups and training tweaks mentioned on the first slide."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Efficient Training on CPU\nThis guide focuses on training large models efficiently on CPU. ## Mixed precision with IPEX\nIPEX is optimized for CPUs with AVX-512 or above, and functionally works for CPUs with only AVX2. So, it is expected to bring performance benefit for Intel CPU generations with AVX-512 or above while CPUs with only AVX2 (e.g., AMD CPUs or older Intel CPUs) might result in a better performance under IPEX, but not guaranteed. IPEX provides performance optimizations for CPU training with both Float32 and BFloat16."", ""For details and\nbenchmarks, please, see [TensorFloat-32(TF32) on Ampere devices](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices). The document includes\ninstructions on how to disable this automatic conversion if for some reason you prefer not to use it. With the 🤗 Trainer you can use `--tf32` to enable it, or disable it with `--tf32 0` or `--no\\_tf32`. By default the PyTorch default is used. ### Automatic Mixed Precision\nYou can use automatic mixed precision with either a pytorch-like AMP way or the apex-like way:\n### fp16\nTo configure pytorch AMP-like mode with fp16 (float16) set:\n```json\n{\n\""fp16\"": {\n\""enabled\"": \""auto\"",\n\""loss\\_scale\"": 0,\n\""loss\\_scale\\_window\"": 1000,\n\""initial\\_scale\\_power\"": 16,\n\""hysteresis\"": 2,\n\""min\\_loss\\_scale\"": 1\n}\n}\n```\nand the [`Trainer`] will automatically enable or disable it based on the value of\n`args.fp16\\_backend`. The rest of config values are up to you. This mode gets enabled when `--fp16 --fp16\\_backend amp` or `--fp16\\_full\\_eval` command line args are passed. You can also enable/disable this mode explicitly:\n```json\n{\n\""fp16\"": {\n\""enabled\"": true,\n\""loss\\_scale\"": 0,\n\""loss\\_scale\\_window\"": 1000,\n\""initial\\_scale\\_power\"": 16,\n\""hysteresis\"": 2,\n\""min\\_loss\\_scale\"": 1\n}\n}\n```\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\nconfiguration. Here is the [documentation](https://www.deepspeed.ai/docs/config-json/#fp16-training-options). ### bf16\nIf bf16 (bfloat16) is desired instead of fp16 then the following configuration section is to be used:\n```json\n{\n\""bf16\"": {\n\""enabled\"": \""auto\""\n}\n}\n```\nbf16 has the same dynamic range as fp32 and thus doesn't require loss scaling. This mode gets enabled when `--bf16` or `--bf16\\_full\\_eval` command line args are passed. You can also enable/disable this mode explicitly:\n```json\n{\n\""bf16\"": {\n\""enabled\"": true\n}\n}\n```\nAs of `deepspeed==0.6.0` the bf16 support is new and experimental. If you use [gradient accumulation](#gradient-accumulation) with bf16-enabled, you need to be aware that it'll accumulate gradients in bf16, which may not be what you want due to this format's low precision, as it may lead to a lossy accumulation. A work is being done to fix that and provide an option to use a higher precision `dtype` (fp16 or fp32). ### NCCL Collectives\nThere is the `dtype` of the training regime and there is a separate `dtype` that is used for communication collectives like various reduction and gathering/scattering operations. All gather/scatter ops are performed in the same `dtype` the data is in, so if you're using bf16 training regime it gets gathered in bf16 - gathering is a non-lossy operation."", ""To learn more about Accelerate big model inference, see the [documentation](https://huggingface.co/docs/accelerate/usage\\_guides/big\\_modeling)."", ""!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# DeepSpeed\n[DeepSpeed](https://www.deepspeed.ai/) is a library designed for speed and scale for distributed training of large models with billions of parameters. At its core is the Zero Redundancy Optimizer (ZeRO) that shards optimizer states (ZeRO-1), gradients (ZeRO-2), and parameters (ZeRO-3) across data parallel processes. This drastically reduces memory usage, allowing you to scale your training to billion parameter models. To unlock even more memory efficiency, ZeRO-Offload reduces GPU compute and memory by leveraging CPU resources during optimization. Both of these features are supported in 🤗 Accelerate, and you can use them with 🤗 PEFT. This guide will help you learn how to use our DeepSpeed [training script](https://github.com/huggingface/peft/blob/main/examples/conditional\\_generation/peft\\_lora\\_seq2seq\\_accelerate\\_ds\\_zero3\\_offload.py). You'll configure the script to train a large model for conditional generation with ZeRO-3 and ZeRO-Offload. 💡 To help you get started, check out our example training scripts for [causal language modeling](https://github.com/huggingface/peft/blob/main/examples/causal\\_language\\_modeling/peft\\_lora\\_clm\\_accelerate\\_ds\\_zero3\\_offload.py) and [conditional generation](https://github.com/huggingface/peft/blob/main/examples/conditional\\_generation/peft\\_lora\\_seq2seq\\_accelerate\\_ds\\_zero3\\_offload.py).""]","To force mixed precision training when initializing the Accelerator in 🤗 Accelerate, you can set the `mixed_precision` argument to `""fp16""` or `""bf16""` depending on your preference. Here is an example of how to do this:

```python
from accelerate import Accelerator

accelerator = Accelerator(mixed_precision=""fp16"")  # For fp16 mixed precision
# or
accelerator = Accelerator(mixed_precision=""bf16"")  # For bf16 mixed precision
```

This will enable mixed precision training for your training loop.",By passing `fp16=True` to the Accelerator init.
"What is the purpose of tokenizers in the NLP pipeline?
","[""p align=\""center\"">\n\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\n\n![Build](https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg)\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\n[![Doc](https://docs.rs/tokenizers/badge.svg)](https://docs.rs/tokenizers/)\n\nThe core of `tokenizers`, written in Rust. Provides an implementation of today's most used tokenizers, with a focus on performance and\nversatility. ## What is a Tokenizer\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`. The various steps of the pipeline are:\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\nthe [unicode normalization standards](https://unicode.org/reports/tr15/#Norm\\_Forms), such as `NFD` or `NFKC`. More details about how to use the `Normalizers` are available on the\n[Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\nsplitting text is simply on whitespace. 3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\n`BPE` or `WordPiece`."", ""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a PyTorch tensor."", ""These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. Two common types of token classification are:\n\\* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names. \\* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb). ```py\n>>> from transformers import pipeline\n>>> classifier = pipeline(task=\""ner\"")\n>>> preds = classifier(\""Hugging Face is a French company based in New York City.\"")\n>>> preds = [\n... {\n... \""entity\"": pred[\""entity\""],\n... \""score\"": round(pred[\""score\""], 4),\n... \""index\"": pred[\""index\""],\n... \""word\"": pred[\""word\""],\n... \""start\"": pred[\""start\""],\n... \""end\"": pred[\""end\""],\n..."", ""et's take a look at word-based tokenization. Word-based tokenization is the idea of splitting the raw text into words, by splitting on spaces or other specific rules like punctuation. In this algorithm, each word has a specific number, an \""ID\"", attributed to it. In this example, \""Let's\"" has the ID 250, do has ID 861, and tokenization followed by an exclamation point has the ID 345. This approach is interesting, as the model has representations that are based on entire words. The information held in a single number is high as a word contains a lot of contextual and semantic information in a sentence. However, this approach does have its limits. For example, the word dog and the word dogs are very similar, and their meaning is close. However, the word-based tokenization will attribute entirely different IDs to these two words, and the model will therefore learn different meanings for these two words. This is unfortunate, as we would like the model to understand that these words are indeed related and that dogs is the plural form of the word dog."", ""And with that, you now know how to train your very own tokenizers!""]","The purpose of tokenizers in the NLP pipeline is to process raw text as input and output an `Encoding`. They perform several key functions, including:

1. **Normalization**: The tokenizer normalizes the text, which may involve applying unicode normalization standards.
2. **PreTokenization**: It creates initial word splits in the text, typically by splitting on whitespace.
3. **Modeling**: The tokenizer performs the actual tokenization, converting the text into tokens, which can be words, parts of words, or punctuation symbols.

Overall, tokenizers are essential for converting raw text into a format that models can understand and process effectively.",To translate text into data that can be processed by the model.
"What is the purpose of the Safety Checker in the Diffusers library?
","[""🧨 Diffusers Experimental\nWe are adding experimental code to support novel applications and usages of the Diffusers library."", ""### Swap components in a pipeline\nYou can customize the default components of any pipeline with another compatible component. Customization is important because:\n- Changing the scheduler is important for exploring the trade-off between generation speed and quality. - Different components of a model are typically trained independently and you can swap out a component with a better-performing one. - During finetuning, usually only some components - like the UNet or text encoder - are trained. To find out which schedulers are compatible for customization, you can use the `compatibles` method:\n```py\nfrom diffusers import DiffusionPipeline\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\nstable\\_diffusion = DiffusionPipeline.from\\_pretrained(repo\\_id, use\\_safetensors=True)\nstable\\_diffusion.scheduler.compatibles\n```\nLet's use the [`SchedulerMixin.from\\_pretrained`] method to replace the default [`PNDMScheduler`] with a more performant scheduler, [`EulerDiscreteScheduler`]. The `subfolder=\""scheduler\""` argument is required to load the scheduler configuration from the correct [subfolder](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main/scheduler) of the pipeline repository. Then you can pass the new [`EulerDiscreteScheduler`] instance to the `scheduler` argument in [`DiffusionPipeline`]:\n```python\nfrom diffusers import DiffusionPipeline, EulerDiscreteScheduler\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\nscheduler = EulerDiscreteScheduler.from\\_pretrained(repo\\_id, subfolder=\""scheduler\"")\nstable\\_diffusion = DiffusionPipeline.from\\_pretrained(repo\\_id, scheduler=scheduler, use\\_safetensors=True)\n```\n### Safety checker\nDiffusion models like Stable Diffusion can generate harmful content, which is why 🧨 Diffusers has a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\_diffusion/safety\\_checker.py) to check generated outputs against known hardcoded NSFW content. If you'd like to disable the safety checker for whatever reason, pass `None` to the `safety\\_checker` argument:\n```python\nfrom diffusers import DiffusionPipeline\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\nstable\\_diffusion = DiffusionPipeline.from\\_pretrained(repo\\_id, safety\\_checker=None, use\\_safetensors=True)\n\""\""\""\nYou have disabled the safety checker for  by passing `safety\\_checker=None`."", ""Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us. - [\\*\\*Community tab\\*\\*](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project. - \\*\\*Bias exploration and evaluation\\*\\*: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations. - \\*\\*Encouraging safety in deployment\\*\\*\n- [\\*\\*Safe Stable Diffusion\\*\\*](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable\\_diffusion/stable\\_diffusion\\_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105). - [\\*\\*Safety Checker\\*\\*](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\_diffusion/safety\\_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Logging\n🤗 Diffusers has a centralized logging system to easily manage the verbosity of the library. The default verbosity is set to `WARNING`. To change the verbosity level, use one of the direct setters. For instance, to change the verbosity to the `INFO` level. ```python\nimport diffusers\ndiffusers.logging.set\\_verbosity\\_info()\n```\nYou can also use the environment variable `DIFFUSERS\\_VERBOSITY` to override the default verbosity. You can set it\nto one of the following: `debug`, `info`, `warning`, `error`, `critical`. For example:\n```bash\nDIFFUSERS\\_VERBOSITY=error ./myprogram.py\n```\nAdditionally, some `warnings` can be disabled by setting the environment variable\n`DIFFUSERS\\_NO\\_ADVISORY\\_WARNINGS` to a true value, like `1`. This disables any warning logged by\n[`logger.warning\\_advice`]. For example:\n```bash\nDIFFUSERS\\_NO\\_ADVISORY\\_WARNINGS=1 ./myprogram.py\n```\nHere is an example of how to use the same logger as the library in your own module or script:\n```python\nfrom diffusers.utils import logging\nlogging.set\\_verbosity\\_info()\nlogger = logging.get\\_logger(\""diffusers\"")\nlogger.info(\""INFO\"")\nlogger.warning(\""WARN\"")\n```\nAll methods of the logging module are documented below. The main methods are\n[`logging.get\\_verbosity`] to get the current level of verbosity in the logger and\n[`logging.set\\_verbosity`] to set the verbosity to the level of your choice. In order from the least verbose to the most verbose:\n| Method | Integer value | Description |\n|----------------------------------------------------------:|--------------:|----------------------------------------------------:|\n| `diffusers.logging.CRITICAL` or `diffusers.logging.FATAL` | 50 | only report the most critical errors |\n| `diffusers.logging.ERROR` | 40 | only report errors |\n| `diffusers.logging.WARNING` or `diffusers.logging.WARN` | 30 | only report errors and warnings (default) |\n| `diffusers.logging.INFO` | 20 | only report errors, warnings, and basic information |\n| `diffusers.logging.DEBUG` | 10 | report all information |\nBy default, `tqdm` progress bars are displayed during model download. [`logging.disable\\_progress\\_bar`] and [`logging.enable\\_progress\\_bar`] are used to enable or disable this behavior."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Dance Diffusion\n[Dance Diffusion](https://github.com/Harmonai-org/sample-generator) is by Zach Evans. Dance Diffusion is the first in a suite of generative audio tools for producers and musicians released by [Harmonai](https://github.com/Harmonai-org). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## DanceDiffusionPipeline\n[[autodoc]] DanceDiffusionPipeline\n- all\n- \\_\\_call\\_\\_\n## AudioPipelineOutput\n[[autodoc]] pipelines.AudioPipelineOutput""]","The Safety Checker in the Diffusers library is designed to check generated outputs against known hardcoded NSFW content to prevent the generation of harmful content. It helps ensure that the outputs from diffusion models like Stable Diffusion are safe and appropriate. If needed, users can disable the safety checker by passing `None` to the `safety_checker` argument when initializing the pipeline.",The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.
"What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?
","[""!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Interact with Discussions and Pull Requests\nThe `huggingface\\_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood. ## Retrieve Discussions and Pull Requests from the Hub\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(repo\\_id=\""bigscience/bloom\""):\n... print(f\""{discussion.num} - {discussion.title}, pr: {discussion.is\\_pull\\_request}\"")\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer\\_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]\n```\n`HfApi.get\\_repo\\_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(\n..."", ""Firstly, there are a few methods to manage repository creation, deletion, and others:\n```python no-format\nfrom huggingface\\_hub import (\n# User management\nlogin,\nlogout,\nwhoami,\n# Repository creation and management\ncreate\\_repo,\ndelete\\_repo,\nupdate\\_repo\\_visibility,\n# And some methods to retrieve/change information about the content\nlist\\_models,\nlist\\_datasets,\nlist\\_metrics,\nlist\\_repo\\_files,\nupload\\_file,\ndelete\\_file,\n)\n```\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them. The `create\\_repo` method can be used to create a new repository on the hub:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"")\n```\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"", organization=\""huggingface\"")\n```\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization."", ""Once you are satisfied, go to the webpage of your fork on GitHub. Click on \""Pull request\"". Make sure to add the GitHub handle of Patrick\nas one reviewer, so that the Hugging Face team gets notified for future changes. 6. Change the PR into a draft by clicking on \""Convert to draft\"" on the\nright of the GitHub pull request web page. In the following, whenever you have done some progress, don't forget to\ncommit your work and push it to your account so that it shows in the\npull request. Additionally, you should make sure to update your work\nwith the current main from time to time by doing:\ngit fetch upstream\ngit merge upstream/main\nIn general, all questions you might have regarding the model or your\nimplementation should be asked in your PR and discussed/solved in the\nPR. This way, Patrick will always be notified when you are\ncommitting new code or if you have a question. It is often very helpful\nto point Patrick to your added code so that the Hugging\nFace team can efficiently understand your problem or question. To do so, you can go to the \""Files changed\"" tab where you see all of\nyour changes, go to a line regarding which you want to ask a question,\nand click on the \""+\"" symbol to add a comment. Whenever a question or\nproblem has been solved, you can click on the \""Resolve\"" button of the\ncreated comment. In the same way, Patrick will open comments when reviewing\nyour code. We recommend asking most questions on GitHub on your PR."", ""--\ntitle: \""Supercharged Searching on the 🤗 Hub\""\nthumbnail: /blog/assets/48\\_hubsearch/thumbnail.png\nauthors:\n- user: muellerzr\n---\n# Supercharged Searching on the Hugging Face Hub\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/muellerzr/hf-blog-notebooks/blob/main/Searching-the-Hub.ipynb)\nThe `huggingface\\_hub` library is a lightweight interface that provides a programmatic approach to exploring the hosting endpoints Hugging Face provides: models, datasets, and Spaces. Up until now, searching on the Hub through this interface was tricky to pull off, and there were many aspects of it a user had to \""just know\"" and get accustomed to. In this article, we will be looking at a few exciting new features added to `huggingface\\_hub` to help lower that bar and provide users with a friendly API to search for the models and datasets they want to use without leaving their Jupyter or Python interfaces. > Before we begin, if you do not have the latest version of the `huggingface\\_hub` library on your system, please run the following cell:\n```python\n!pip install huggingface\\_hub -U\n```\n## Situating the Problem:\nFirst, let's imagine the scenario you are in. You'd like to find all models hosted on the Hugging Face Hub for Text Classification, were trained on the GLUE dataset, and are compatible with PyTorch."", ""models/datasets/Spaces repos), not arbitrary repos\n## What's next\nOf course, it's only the beginning. We will listen to the community feedback to add new features and improve the community tab in the future. If you have any feedback, you can [join the discussion here](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/1). Today is the best time to join your first discussion and open a PR! 🤗""]",The `HfApi` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.,HfApi
"What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?
","[""Integrate your library with the Hub\nThe Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones already integrated include [spaCy](https://spacy.io/usage/projects#huggingface\\_hub), [AllenNLP](https://allennlp.org/), and [timm](https://rwightman.github.io/pytorch-image-models/), among many others. Integration means users can download and upload files to the Hub directly from your library. We hope you will integrate your library and join us in democratizing artificial intelligence for everyone. Integrating the Hub with your library provides many benefits, including:\n- Free model hosting for you and your users. - Built-in file versioning - even for huge files - made possible by [Git-LFS](https://git-lfs.github.com/). - All public models are powered by the [Inference API](https://huggingface.co/docs/api-inference/index). - In-browser widgets allow users to interact with your hosted models directly. This tutorial will help you integrate the Hub into your library so your users can benefit from all the features offered by the Hub. Before you begin, we recommend you create a [Hugging Face account](https://huggingface.co/join) from which you can manage your repositories and files. If you need help with the integration, feel free to open an [issue](https://github.com/huggingface/huggingface\\_hub/issues/new/choose), and we would be more than happy to help you. ## Installation\n1. Install the `huggingface\\_hub` library with pip in your environment:\n```bash\npython -m pip install huggingface\\_hub\n```\n2. Once you have successfully installed the `huggingface\\_hub` library, log in to your Hugging Face account:\n```bash\nhuggingface-cli login\n```\n```bash\n\\_| \\_| \\_| \\_| \\_|\\_|\\_| \\_|\\_|\\_| \\_|\\_|\\_| \\_| \\_| \\_|\\_|\\_| \\_|\\_|\\_|\\_| \\_|\\_| \\_|\\_|\\_| \\_|\\_|\\_|\\_|\n\\_| \\_| \\_| \\_| \\_| \\_| \\_| \\_|\\_| \\_| \\_| \\_| \\_| \\_| \\_| \\_|\n\\_|\\_|\\_|\\_| \\_| \\_| \\_| \\_|\\_| \\_| \\_|\\_| \\_| \\_| \\_| \\_| \\_| \\_|\\_| \\_|\\_|\\_| \\_|\\_|\\_|\\_| \\_| \\_|\\_|\\_|\n\\_| \\_| \\_| \\_| \\_| \\_| \\_| \\_| \\_| \\_| \\_|\\_| \\_| \\_| \\_| \\_| \\_| \\_| \\_|\n\\_| \\_| \\_|\\_| \\_|\\_|\\_| \\_|\\_|\\_| \\_|\\_|\\_| \\_| \\_| \\_|\\_|\\_| \\_| \\_| \\_| \\_|\\_|\\_| \\_|\\_|\\_|\\_|\nUsername:\nPassword:\n```\n3. Alternatively, if you prefer working from a Jupyter or Colaboratory notebook, login with `notebook\\_login`:\n```python\n>>> from huggingface\\_hub import notebook\\_login\n>>> notebook\\_login()\n```\n`notebook\\_login` will launch a widget in your notebook from which you can enter your Hugging Face credentials."", ""Data is hosted on HuggingFace Datasets."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Adding support for an unsupported architecture\nIf you wish to export a model whose architecture is not already supported by the library, the PR [#813 Adds support for ResNet](https://github.com/huggingface/optimum/pull/813 ) can be used as a reference. You can make sure tests pass for the new `my\\_new\\_modeltype` model type by running:\n```bash\npytest tests/exporters/tflite/test\\_\\*.py -k \""my\\_new\\_modeltype\"" -s --exitfirst\n```"", ""--\ntitle: \""Introducing Skops\""\nthumbnail: /blog/assets/94\\_skops/introducing\\_skops.png\nauthors:\n- user: merve\n- user: adrin\n- user: BenjaminB\n---\n# Introducing Skops\n## Introducing Skops\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others. Let's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production. ```python\n# let's import the libraries first\nimport sklearn\nfrom sklearn.datasets import load\\_breast\\_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model\\_selection import train\\_test\\_split\n# Load the data and split\nX, y = load\\_breast\\_cancer(as\\_frame=True, return\\_X\\_y=True)\nX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(\nX, y, test\\_size=0.3, random\\_state=42\n)\n# Train the model\nmodel = DecisionTreeClassifier().fit(X\\_train, y\\_train)\n```\nYou can use any model filename and serialization method, like `pickle` or `joblib`."", ""### Models\n[Transfer learning](https://www.youtube.com/watch?v=BqqfQnyjmgg&ab\\_channel=HuggingFace) has changed the way companies approach machine learning problems. Traditionally, companies needed to train models from scratch, which requires a lot of time, data, and resources. Now machine learning teams can use a pre-trained model and [fine-tune it for their own use case](https://huggingface.co/course/chapter3/1?fw=pt) in a fast and cost-effective way. This dramatically accelerates the process of getting accurate and performant models. On the Hub, you can find 60,000+ state-of-the-art open source pre-trained models for NLP, computer vision, speech, time-series, biology, reinforcement learning, chemistry and more. You can use the search bar or filter by tasks, libraries, licenses and other tags to find the right model for your particular use case:\n\n60,000+ models available on the Hub\n\nThese models span 180 languages and support up to 25 ML libraries (including Transformers, Keras, spaCy, Timm and others), so there is a lot of flexibility in terms of the type of models, languages and libraries.""]",The name of the new library introduced by Hugging Face for hosting scikit-learn models is Skops.,Skops
"What is the purpose of Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""This, coupled with the offset mapping to get the start and end characters allows us to get the span of texts for each entity."", ""It wraps a 'Hello {name}!' function in an Interface that accepts and returns text."", ""Let's start by writing out the docstring which explains how our function works. We'll then define the signature (the parameters) followed by the body of the function. reverse\\_string.\\_\\_doc\\_\\_ = \\\""Reverses any given string.\\\""\nreverse\\_string.\\_\\_name\\_\\_ ='reverse\\_string'\nreverse\\_string.\\_\\_signature\\_\\_ = inspect.Signature() #<|end|>\n<|user|>\nHere is my attempt:\n<|end|>\n<|assistant|>\nOkay great! Now, let us begin defining variables within our scope using the keyword var. Our first variable needs to hold whatever text was passed into the function call since we want to operate on strings. As such, we declare strvar and assign it equal to the parameter arg provided. strvar := arg\nThen, after declaring our second variable, we initialize it similarly but instead pass in the result of calling strvar's built-in.reversed() method alongside the index operator []. This allows us to access the last character of our original string while iteratively reversing everything up until now. Note that we don't actually need to store anything inside this new variable because it simply holds the same data as strvar without modifying them\""\n```\nGPT4 Evaluation:\n```\n4 7\nAssistant 1's code was not well-structured, and it was difficult to follow the logic."", ""These transformations can also be undetectable with a simple \""print\"". Indeed, keep in mind that for a computer, text is only a succession of 0 and 1 and it happens that different successions of 0 and 1 render the same printed character. The 0s and 1s go in groups of 8 to form a byte. The computer must then decode this sequence of bytes into a sequence of \""code points\"". In our example the 2 bytes are transformed into a single \""code point\"" by UTF-8. The unicode standard then allows us to find the character corresponding to this code point: the c cedilla. Let's repeat the same operation with this new sequence composed of 3 bytes, this time it is transformed into 2 \""code points\"" .... which also correspond to the c cedilla character! It is in fact the composition of the unicode Latin Small Letter Cand the combining cedilla. But it's annoying because what appears to us to be a single character is not at all the same thing for the computer. Fortunately, there are unicode standardization standards known as NFC, NFD, NFKC and NFKD that allow erasing some of these differences. These standards are often used by tokenizers! On all these previous examples, even if the normalizations changed the look of the text, they did not change the content: you could still read \""Hello world, let's normalize this sentence\"". However, you must be aware that some normalizations can be very harmful if they are not adapted to their corpus. For example, if you take the French sentence \""un père indigné\"", which means \""An indignant father\"", and normalize it with the bert-base-uncase tokenizer which removes the accents then the sentence becomes \""un père indigne\"" which means \""An unworthy father\"". If you watch this video to build your own tokenizer, there are no absolute rules to choose or not a normalization for your brand new tokenizer but I advise you to take the time to select them so that they do not make you lose important information.""]",Textual Inversion is a method to personalize text-to-image models like stable diffusion using just 3-5 examples of your own images. It allows users to adapt the model to better understand and generate images based on specific concepts or styles that are represented in the provided examples.,Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.
"What is the recommended multiple of batch size for fp16 data type on an A100 GPU?
","[""For instance, for fp16 data type a multiple of 8 is recommended, unless\nit's an A100 GPU, in which case use multiples of 64. For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup. ## Gradient Accumulation\nThe \\*\\*gradient accumulation\\*\\* method aims to calculate gradients in smaller increments instead of computing them for the\nentire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward\nand backward passes through the model and accumulating the gradients during the process. Once a sufficient number of\ngradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\nbecomes possible to increase the \\*\\*effective batch size\\*\\* beyond the limitations imposed by the GPU's memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can\nslow down the training process. You can enable gradient accumulation by adding the `gradient\\_accumulation\\_steps` argument to [`TrainingArguments`]:\n```py\ntraining\\_args = TrainingArguments(per\\_device\\_train\\_batch\\_size=1, gradient\\_accumulation\\_steps=4, \\*\\*default\\_args)\n```\nIn the above example, your effective batch size becomes 4."", ""The Gaudi runs were performed in mixed precision (bf16/fp32) and the A100 runs in fp16. All runs were \\*distributed\\* runs on \\*8 devices\\*. ### Generating Images from Text with Stable Diffusion\nOne of the main new features of 🤗 Optimum Habana release 1.3 is [the support for Stable Diffusion](https://huggingface.co/docs/optimum/habana/usage\\_guides/stable\\_diffusion). It is now very easy to generate images from text on Gaudi. Unlike with 🤗 Diffusers on GPUs, images are generated by batches. Due to model compilation times, the first two batches will be slower than the following iterations. In this benchmark, these first two iterations were discarded to compute the throughputs for both first-gen Gaudi and Gaudi2. [This script](https://github.com/huggingface/optimum-habana/tree/main/examples/stable-diffusion) was run for a batch size of 8 samples. It uses the [`Habana/stable-diffusion`](https://huggingface.co/Habana/stable-diffusion) Gaudi configuration. The results we got, which are consistent with the numbers published by Habana [here](https://developer.habana.ai/resources/habana-models-performance/), are displayed in the table below. \\*\\*Gaudi2 showcases latencies that are x3.51 faster than first-gen Gaudi (3.25s versus 0.925s) and x2.84 faster than Nvidia A100 (2.63s versus 0.925s).\\*\\* It can also support bigger batch sizes. | | First-gen Gaudi (BS=8) | Gaudi2 (BS=8) | A100 (BS=1) |\n|:---------------:|:----------------------:|:-------------:|:-----------:|\n| Latency (s/img) | 3.25 | 0.925 | 2.63 |\n| Speedup | x1.0 | x3.51 | x1.24 |\n\\*Update: the figures above were updated as SynapseAI 1.10 and Optimum Habana 1.6 bring an additional speedup on first-gen Gaudi and Gaudi2.\\*\n\\*BS\\* is the batch size."", ""Source: Original content from [`sgugger`](https://huggingface.co/sgugger)** |\nAlthough the precision is substantially reduced by reducing the number of bits from 32 to 8, both versions can be used in a variety of situations. Currently one could use [Transformer Engine library](https://github.com/NVIDIA/TransformerEngine) that is also integrated with HF ecosystem through accelerate. The potential floating points that can be represented in the E4M3 format are in the range -448 to 448, whereas in the E5M2 format, as the number of bits of the exponent increases, the range increases to -57344 to 57344 - but with a loss of precision because the number of possible representations remains constant. It has been empirically proven that the E4M3 is best suited for the forward pass, and the second version is best suited for the backward computation\n### FP4 precision in a few words\nThe sign bit represents the sign (+/-), the exponent bits a base two to the power of the integer represented by the bits (e.g."", ""CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.74 GiB free; 9.06 GiB reserved in total by PyTorch)\n2 / 2\nDoesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.24 GiB free; 9.56 GiB reserved in total by PyTorch)\n==================== INFERENCE - MEMORY - RESULT ====================\n--------------------------------------------------------------------------------\nModel Name Batch Size Seq Length Memory in MB\n--------------------------------------------------------------------------------\nReformer-No-Chunk 8 1024 4281\nReformer-No-Chunk 8 2048 7607\nReformer-No-Chunk 8 4096 N/A\nReformer-Chunk 8 1024 4309\nReformer-Chunk 8 2048 7669\nReformer-Chunk 8 4096 N/A\n--------------------------------------------------------------------------------\nInteresting, chunked feed forward layers do not seem to help here at all. The reason is that `config.feed\\_forward\\_size` is not sufficiently large to make a real difference."", ""By default, this is set to `True` which is used to train in mixed Int8/FP16 precision. However, we are interested in memory efficient inference for which we need to use `has\\_fp16\\_weights=False`. 5. Now time to load your model in 8-bit! ```py\nint8\\_model.load\\_state\\_dict(torch.load(\""model.pt\""))\nint8\\_model = int8\\_model.to(0) # Quantization happens here\n```\nNote that the quantization step is done in the second line once the model is set on the GPU. If you print `int8\\_model[0].weight` before calling the `.to` function you get:\n```\nint8\\_model[0].weight\nParameter containing:\ntensor([[ 0.0031, -0.0438, 0.0494, ..., -0.0046, -0.0410, 0.0436],\n[-0.1013, 0.0394, 0.0787, ..., 0.0986, 0.0595, 0.0162],\n[-0.0859, -0.1227, -0.1209, ..., 0.1158, 0.0186, -0.0530],\n...,\n[ 0.0804, 0.0725, 0.0638, ..., -0.0487, -0.0524, -0.1076],\n[-0.0200, -0.0406, 0.0663, ..., 0.0123, 0.0551, -0.0121],\n[-0.0041, 0.0865, -0.0013, ..., -0.0427, -0.0764, 0.1189]],\ndtype=torch.float16)\n```\nWhereas if you print it after the second line's call you get:\n```\nint8\\_model[0].weight\nParameter containing:\ntensor([[ 3, -47, 54, ..., -5, -44, 47],\n[-104, 40, 81, ..., 101, 61, 17],\n[ -89, -127, -125, ..., 120, 19, -55],\n...,\n[ 82, 74, 65, ..., -49, -53, -109],\n[ -21, -42, 68, ..., 13, 57, -12],\n[ -4, 88, -1, ..., -43, -78, 121]],\ndevice='cuda:0', dtype=torch.int8, requires\\_grad=True)\n```\nThe weights values are \""truncated\"" as we have seen when explaining quantization in the previous sections. Also, the values seem to be distributed between [-127, 127]. You might also wonder how to retrieve the FP16 weights in order to perform the outlier MatMul in fp16? You can simply do:\n```py\n(int8\\_model[0].weight.CB \\* int8\\_model[0].weight.SCB) / 127\n```\nAnd you will get:\n```\ntensor([[ 0.0028, -0.0459, 0.0522, ..., -0.0049, -0.0428, 0.0462],\n[-0.0960, 0.0391, 0.0782, ..., 0.0994, 0.0593, 0.0167],\n[-0.0822, -0.1240, -0.1207, ..., 0.1181, 0.0185, -0.0541],\n...,\n[ 0.0757, 0.0723, 0.0628, ..., -0.0482, -0.0516, -0.1072],\n[-0.0194, -0.0410, 0.0657, ..., 0.0128, 0.0554, -0.0118],\n[-0.0037, 0.0859, -0.0010, ..., -0.0423, -0.0759, 0.1190]],\ndevice='cuda:0')\n```\nWhich is close enough to the original FP16 values (2 print outs up)!""]","For fp16 data type on an A100 GPU, it is recommended to use multiples of 64.",64
"How do you run a Gradio Blocks app in reload mode using a Python IDE?
","[""Developing Faster with Auto-Reloading\n\\*\\*Prerequisite\\*\\*: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control). This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks. ## Why Auto-Reloading? When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes. To make it faster and more convenient to write your code, we've made it easier to \""reload\"" your Gradio apps instantly when you are developing in a \\*\\*Python IDE\\*\\* (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \""magic command\"" that allows you to re-run cells faster if you use \\*\\*Jupyter Notebooks\\*\\* (or any similar environment like Colab). This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster. ## Python IDE Reload 🔥\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Markdown(\""# Greetings from Gradio!\"")\ninp = gr.Textbox(placeholder=\""What is your name?\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""Welcome, {x}!\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`."", ""Gradio Demo: colorpicker\\_component\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.ColorPicker()\ndemo.launch()\n```"", ""By default, the Gradio use UTF-8 encoding for scripts. \\*\\*For reload mode\\*\\*, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n1. Configure encoding declaration of python script, for example: `# -\\*- coding: cp1252 -\\*-`\n2. Confirm that your code editor has identified that encoding format. 3. Run like this: `gradio run.py --encoding cp1252`\n🔥 If your application accepts command line arguments, you can pass them in as well. Here's an example:\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# Greetings {args.name}!\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nWhich you could run like this: `gradio run.py --name Gretel`\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code."", ""Gradio Demo: upload\\_button\\_component\\_events\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\nwith gr.Row():\nwith gr.Column():\nupload\\_btn = gr.UploadButton(label=\""Upload Single File\"", file\\_count=\""single\"")\nwith gr.Column():\noutput\\_file\\_1 = gr.File(label=\""Upload Single File Output\"", file\\_count=\""single\"")\nnum\\_load\\_btn\\_1 = gr.Number(label=\""# Load Upload Single File\"", value=0)\noutput\\_click\\_1 = gr.Number(label=\""# Click Upload Single File Output\"", value=0)\nupload\\_btn.upload(lambda s,n: (s, n + 1), [upload\\_btn, num\\_load\\_btn\\_1], [output\\_file\\_1, num\\_load\\_btn\\_1])\nupload\\_btn.click(lambda n: (n + 1), output\\_click\\_1, [output\\_click\\_1])\nwith gr.Row():\nwith gr.Column():\nupload\\_btn\\_multiple = gr.UploadButton(label=\""Upload Multiple Files\"", file\\_count=\""multiple\"")\nwith gr.Column():\noutput\\_file\\_2 = gr.File(label=\""Upload Multiple Files Output\"", file\\_count=\""multiple\"")\nnum\\_load\\_btn\\_2 = gr.Number(label=\""# Load Upload Multiple Files\"", value=0)\noutput\\_click\\_2 = gr.Number(label=\""# Click Upload Multiple Files Output\"", value=0)\nupload\\_btn\\_multiple.upload(lambda s,n: (s, n + 1), [upload\\_btn\\_multiple, num\\_load\\_btn\\_2], [output\\_file\\_2, num\\_load\\_btn\\_2])\nupload\\_btn\\_multiple.click(lambda n: (n + 1), output\\_click\\_2, [output\\_click\\_2])\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: reversible\\_flow\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndef increase(num):\nreturn num + 1\nwith gr.Blocks() as demo:\na = gr.Number(label=\""a\"")\nb = gr.Number(label=\""b\"")\natob = gr.Button(\""a > b\"")\nbtoa = gr.Button(\""b > a\"")\natob.click(increase, a, b)\nbtoa.click(increase, b, a)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```""]","To run a Gradio Blocks app in reload mode using a Python IDE, follow these steps:

1. Write your Gradio app code in a Python file (e.g., `run.py`). Here is an example structure:

   ```python
   import gradio as gr
   with gr.Blocks() as demo:
       gr.Markdown(""# Greetings from Gradio!"")
       inp = gr.Textbox(placeholder=""What is your name?"")
       out = gr.Textbox()
       inp.change(fn=lambda x: f""Welcome, {x}!"", inputs=inp, outputs=out)
   if __name__ == ""__main__"":
       demo.launch()
   ```

2. To enable auto-reloading, simply run your script using the command:

   ```
   gradio run.py
   ```

3. If you are using encoding formats other than UTF-8 (like cp1252), make sure to:
   - Configure the encoding declaration in your Python script, for example: `# -*- coding: cp1252 -*-`
   - Confirm that your code editor recognizes that encoding format.
   - Run your script with the encoding option like this:

   ```
   gradio run.py --encoding cp1252
   ```

With this setup, any changes you make to your `run.py` source code will automatically reload the Gradio app without needing to close and rerun it manually.",Run `gradio run.py` in the terminal.
"How can you install the Hugging Face Unity API in your Unity project?
","[""--\ntitle: \""How to Install and Use the Hugging Face Unity API\""\nthumbnail: /blog/assets/124\\_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n# How to Install and Use the Hugging Face Unity API\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API. ## Installation\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif)\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens). 7. Test the API key by clicking `Test API key` in the API Wizard. 8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API\\_URL` field."", ""| Partially handled by `huggingface\\_hub`. |"", ""Start building with Gradio and Hugging Face 🧱⛏️"", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction"", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Articulation Bodies\nThere are two primary types of articulation supported by Simulate:\n1. `Prismatic` joints: slide along a specified axis,\n2. `Revolute` joints: rotate about a specified axis. \\*Note\\* that these are currently only implemented in \\*\\*Unity\\*\\*. [[autodoc]] ArticulationBodyComponent""]","To install the Hugging Face Unity API in your Unity project, follow these steps:

1. Open your Unity project.
2. Go to `Window` -> `Package Manager`.
3. Click `+` and select `Add Package from git URL`.
4. Enter `https://github.com/huggingface/unity-api.git`.
5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.
6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).
7. Test the API key by clicking `Test API key` in the API Wizard.
8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the URL from the `API_URL` field.","To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`."
"What is the pretraining objective of the Wav2Vec2 context network?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Wav2Vec2\n## Overview\nThe Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. The abstract from the paper is the following:\n\\*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\nrecognition with limited amounts of labeled data.\\*\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). ## Usage tips\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\nusing [`Wav2Vec2CTCTokenizer`]. ## Resources\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Wav2Vec2."", ""[[autodoc]] transformers.VideoMAEForPreTraining\n- forward\n## VideoMAEForVideoClassification\n[[autodoc]] transformers.VideoMAEForVideoClassification\n- forward"", ""The results of this run can be seen [here](https://wandb.ai/patrickvonplaten/pretraining-wav2vec2/reports/Wav2Vec2-Large--VmlldzoxMTAwODM4?accessToken=wm3qzcnldrwsa31tkvf2pdmilw3f63d4twtffs86ou016xjbyilh55uoi3mo1qzc) and the checkpoint pretrained for 120,000 steps can be accessed [here](https://huggingface.co/patrickvonplaten/wav2vec2-large-repro-960h-libri-120k-steps)"", ""```\n\\*\\*Note\\*\\*: It is expected that some weights are newly initialized. Those weights correspond to the newly initialized vocabulary output layer. We now want to make sure that only the adapter weights will be trained and that the rest of the model stays frozen. First, we re-initialize all the adapter weights which can be done with the handy `init\\_adapter\\_layers` method. It is also possible to not re-initilize the adapter weights and continue fine-tuning, but in this case one should make sure to load fitting adapter weights via the [`load\\_adapter(...)` method](https://huggingface.co/docs/transformers/main/en/model\\_doc/wav2vec2#transformers.Wav2Vec2ForCTC.load\\_adapter) before training. Often the vocabulary still will not match the custom training data very well though, so it's usually easier to just re-initialize all adapter layers so that they can be easily fine-tuned. ```python\nmodel.init\\_adapter\\_layers()\n```\nNext, we freeze all weights, \\*\\*but\\*\\* the adapter layers. ```python\nmodel.freeze\\_base\\_model()\nadapter\\_weights = model.\\_get\\_adapters()\nfor param in adapter\\_weights.values():\nparam.requires\\_grad = True\n```\nIn a final step, we define all parameters related to training. To give more explanation on some of the parameters:\n- `group\\_by\\_length` makes training more efficient by grouping training samples of similar input length into one batch. This can significantly speed up training time by heavily reducing the overall number of useless padding tokens that are passed through the model\n- `learning\\_rate` was chosen to be 1e-3 which is a common default value for training with Adam. Other learning rates might work equally well. For more explanations on other parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main\\_classes/trainer.html?highlight=trainer#trainingarguments). To save GPU memory, we enable PyTorch's [gradient checkpointing](https://pytorch.org/docs/stable/checkpoint.html) and also set the loss reduction to \""\\*mean\\*\"". MMS adapter fine-tuning converges extremely fast to very good performance, so even for a dataset as small as 4h we will only train for 4 epochs."", ""Using the Buckwalter format, text is also logged in Arabic abjad. `--target\\_feature\\_extractor\\_sampling\\_rate` resamples audio to target feature extractor's sampling rate (16kHz). `--max\\_duration\\_in\\_seconds=\""15\""` filters out examples whose audio is longer than the specified limit,\nwhich helps with capping GPU memory usage. ### DeepSpeed Integration\nTo learn how to deploy Deepspeed Integration please refer to [this guide](https://huggingface.co/transformers/main/main\\_classes/deepspeed.html#deepspeed-trainer-integration). But to get started quickly all you need is to install:\n```\npip install deepspeed\n```\nand then use the default configuration files in this directory:\n\\* `ds\\_config\\_wav2vec2\\_zero2.json`\n\\* `ds\\_config\\_wav2vec2\\_zero3.json`\nHere are examples of how you can use DeepSpeed:\n(edit the value for `--num\\_gpus` to match the number of GPUs you have)\nZeRO-2:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 2 \\\nrun\\_asr.py \\\n--output\\_dir=output\\_dir --num\\_train\\_epochs=2 --per\\_device\\_train\\_batch\\_size=2 \\\n--per\\_device\\_eval\\_batch\\_size=2 --evaluation\\_strategy=steps --save\\_steps=500 --eval\\_steps=100 \\\n--logging\\_steps=5 --learning\\_rate=5e-4 --warmup\\_steps=3000 \\\n--model\\_name\\_or\\_path=patrickvonplaten/wav2vec2\\_tiny\\_random\\_robust \\\n--dataset\\_name=hf-internal-testing/librispeech\\_asr\\_dummy --dataset\\_config\\_name=clean \\\n--train\\_split\\_name=validation --validation\\_split\\_name=validation --orthography=timit \\\n--preprocessing\\_num\\_workers=1 --group\\_by\\_length --freeze\\_feature\\_extractor --verbose\\_logging \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero2.json\n```\nFor ZeRO-2 with more than 1 gpu you need to use (which is already in the example configuration file):\n```\n\""zero\\_optimization\"": {\n... \""find\\_unused\\_parameters\"": true,\n... }\n```\nZeRO-3:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 2 \\\nrun\\_asr.py \\\n--output\\_dir=output\\_dir --num\\_train\\_epochs=2 --per\\_device\\_train\\_batch\\_size=2 \\\n--per\\_device\\_eval\\_batch\\_size=2 --evaluation\\_strategy=steps --save\\_steps=500 --eval\\_steps=100 \\\n--logging\\_steps=5 --learning\\_rate=5e-4 --warmup\\_steps=3000 \\\n--model\\_name\\_or\\_path=patrickvonplaten/wav2vec2\\_tiny\\_random\\_robust \\\n--dataset\\_name=hf-internal-testing/librispeech\\_asr\\_dummy --dataset\\_config\\_name=clean \\\n--train\\_split\\_name=validation --validation\\_split\\_name=validation --orthography=timit \\\n--preprocessing\\_num\\_workers=1 --group\\_by\\_length --freeze\\_feature\\_extractor --verbose\\_logging \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero3.json\n```\n### Pretraining Wav2Vec2\nThe `run\\_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss objective (see official [paper](https://arxiv.org/abs/2006.11477) for more information). It is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed (please refer to [this guide](https://huggingface.co/transformers/main/main\\_classes/deepspeed.html#deepspeed-trainer-integration) for more information). Here is an example of how you can use DeepSpeed ZeRO-2 to pretrain a small Wav2Vec2 model:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 4 run\\_pretrain.py \\\n--output\\_dir=\""./wav2vec2-base-libri-100h\"" \\\n--num\\_train\\_epochs=\""3\"" \\\n--per\\_device\\_train\\_batch\\_size=\""32\"" \\\n--per\\_device\\_eval\\_batch\\_size=\""32\"" \\\n--gradient\\_accumulation\\_steps=\""2\"" \\\n--save\\_total\\_limit=\""3\"" \\\n--save\\_steps=\""500\"" \\\n--logging\\_steps=\""10\"" \\\n--learning\\_rate=\""5e-4\"" \\\n--weight\\_decay=\""0.01\"" \\\n--warmup\\_steps=\""3000\"" \\\n--model\\_name\\_or\\_path=\""patrickvonplaten/wav2vec2-base-libri-100h\"" \\\n--dataset\\_name=\""librispeech\\_asr\"" \\\n--dataset\\_config\\_name=\""clean\"" \\\n--train\\_split\\_name=\""train.100\"" \\\n--preprocessing\\_num\\_workers=\""4\"" \\\n--max\\_duration\\_in\\_seconds=\""10.0\"" \\\n--group\\_by\\_length \\\n--verbose\\_logging \\\n--fp16 \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero2.json \\\n```\n### Forced Alignment\nCharacter level forced alignment for audio and text pairs with wav2vec2 models finetuned on ASR task for a specific language. Inspired by [this](https://pytorch.org/tutorials/intermediate/forced\\_alignment\\_with\\_torchaudio\\_tutorial.html) Pytorch tutorial. #### Input Formats\nInput format in script.txt Input format in wavs directroy\n0000 sentence1 0000.wav\n0001 sentence2 0001.wav\n#### Output Format\nOutput directory will contain 0000.txt and 0001.txt. Each file will have format like below\nchar score start\\_ms end\\_ms\nh 0.25 1440 1520\n#### Run command\n```\npython alignment.py \\\n--model\\_name=\""arijitx/wav2vec2-xls-r-300m-bengali\"" \\\n--wav\\_dir=\""./wavs\""\n--text\\_file=\""script.txt\"" \\\n--input\\_wavs\\_sr=48000 \\\n--output\\_dir=\""./out\\_alignment\"" \\\n--cuda\n```""]","The pretraining objective of the Wav2Vec2 context network is based on a contrastive loss objective. This approach allows the model to learn powerful representations from speech audio alone, which can then be fine-tuned on transcribed speech. The model masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations that are jointly learned.",The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.
"What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?
","[""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a TensorFlow tensor."", ""## Additional resources\n\\* Transformers.js [repository](https://github.com/xenova/transformers.js)\n\\* Transformers.js [docs](https://huggingface.co/docs/transformers.js)\n\\* Transformers.js [demo](https://xenova.github.io/transformers.js/)"", ""Most parts of the code you will add for the new 🤗 Transformers model already exist\nsomewhere in 🤗 Transformers. Take some time to find similar, already existing models and tokenizers you can copy\nfrom. [grep](https://www.gnu.org/software/grep/) and [rg](https://github.com/BurntSushi/ripgrep) are your\nfriends. Note that it might very well happen that your model's tokenizer is based on one model implementation, and\nyour model's modeling code on another one. \\*E.g.\\* FSMT's modeling code is based on BART, while FSMT's tokenizer code\nis based on XLM. - It's more of an engineering challenge than a scientific challenge. You should spend more time creating an\nefficient debugging environment rather than trying to understand all theoretical aspects of the model in the paper. - Ask for help, when you're stuck! Models are the core component of 🤗 Transformers so we at Hugging Face are more\nthan happy to help you at every step to add your model. Don't hesitate to ask if you notice you are not making\nprogress. In the following, we try to give you a general recipe that we found most useful when porting a model to 🤗 Transformers. The following list is a summary of everything that has to be done to add a model and can be used by you as a To-Do\nList:\n☐ (Optional) Understood the model's theoretical aspects  \n☐ Prepared 🤗 Transformers dev environment  \n☐ Set up debugging environment of the original repository  \n☐ Created script that successfully runs the `forward()` pass using the original repository and checkpoint  \n☐ Successfully added the model skeleton to 🤗 Transformers  \n☐ Successfully converted original checkpoint to 🤗 Transformers checkpoint  \n☐ Successfully ran `forward()` pass in 🤗 Transformers that gives identical output to original checkpoint  \n☐ Finished model tests in 🤗 Transformers  \n☐ Successfully added tokenizer in 🤗 Transformers  \n☐ Run end-to-end integration tests  \n☐ Finished docs  \n☐ Uploaded model weights to the Hub  \n☐ Submitted the pull request  \n☐ (Optional) Added a demo notebook\nTo begin with, we usually recommend starting by getting a good theoretical understanding of `BrandNewBert`."", ""Мы хотим, чтобы Transformers позволил разработчикам, исследователям, студентам, профессорам, инженерам и всем желающим\nсоздавать проекты своей мечты. Чтобы отпраздновать 100 тысяч звезд Transformers, мы решили сделать акцент на сообществе, и создали страницу [awesome-transformers](./awesome-transformers.md), на которой перечислены 100\nневероятных проектов, созданных с помощью transformers. Если вы являетесь владельцем или пользователем проекта, который, по вашему мнению, должен быть включен в этот список, пожалуйста, откройте PR для его добавления! ## Если вы хотите получить индивидуальную поддержку от команды Hugging Face\n[![HuggingFace Expert Acceleration Program](https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png)](https://huggingface.co/support)  \n## Быстрый гайд\nДля использования модели на заданном входе (текст, изображение, звук, ...) мы предоставляем API `pipeline`. Конвейеры объединяют предварительно обученную модель с препроцессингом, который использовался при ее обучении. Вот как можно быстро использовать конвейер для классификации положительных и отрицательных текстов:\n```python\n>>> from transformers import pipeline\n# Выделение конвейера для анализа настроений\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('Мы очень рады представить конвейер в transformers.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```\nВторая строка кода загружает и кэширует предварительно обученную модель, используемую конвейером, а третья оценивает ее на заданном тексте."", ""The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. and achieve state-of-the-art performance in various task. Text is embedding in vector space such that similar text is close and can efficiently be found using cosine similarity. Keywords: Dense vector representations, Text embeddings, Sentence embeddings\n## [ludwig](https://github.com/ludwig-ai/ludwig)\nLudwig is a declarative machine learning framework that makes it easy to define machine learning pipelines using a simple and flexible data-driven configuration system. Ludwig is targeted at a wide variety of AI tasks. It provides a data-driven configuration system, training, prediction, and evaluation scripts, as well as a programmatic API. Keywords: Declarative, Data-driven, ML Framework\n## [InvokeAI](https://github.com/invoke-ai/InvokeAI)\n[InvokeAI](https://github.com/invoke-ai/InvokeAI) is an engine for Stable Diffusion models, aimed at professionals, artists, and enthusiasts. It leverages the latest AI-driven technologies through CLI as well as a WebUI. Keywords: Stable-Diffusion, WebUI, CLI\n## [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP) is an easy-to-use and powerful NLP library particularly targeted at the Chinese languages. It has support for multiple pre-trained model zoos, and supports a wide-range of NLP tasks from research to industrial applications. Keywords: NLP, Chinese, Research, Industry\n## [stanza](https://github.com/stanfordnlp/stanza)\nThe Stanford NLP Group's official Python NLP library. It contains support for running various accurate natural language processing tools on 60+ languages and for accessing the Java Stanford CoreNLP software from Python. Keywords: NLP, Multilingual, CoreNLP\n## [DeepPavlov](https://github.com/deeppavlov/DeepPavlov)\n[DeepPavlov](https://github.com/deeppavlov/DeepPavlov) is an open-source conversational AI library. It is designed for the development of production ready chat-bots and complex conversational systems, as well as research in the area of NLP and, particularly, of dialog systems. Keywords: Conversational, Chatbot, Dialog\n## [alpaca-lora](https://github.com/tloen/alpaca-lora)\nAlpaca-lora contains code for reproducing the Stanford Alpaca results using low-rank adaptation (LoRA). The repository provides training (fine-tuning) as well as generation scripts. Keywords: LoRA, Parameter-efficient fine-tuning\n## [imagen-pytorch](https://github.com/lucidrains/imagen-pytorch)\nAn open-source Implementation of Imagen, Google's closed-source Text-to-Image Neural Network that beats DALL-E2. As of release, it is the new SOTA for text-to-image synthesis. Keywords: Imagen, Text-to-image\n## [adapter-transformers](https://github.com/adapter-hub/adapter-transformers)\n[adapter-transformers](https://github.com/adapter-hub/adapter-transformers) is an extension of HuggingFace's Transformers library, integrating adapters into state-of-the-art language models by incorporating AdapterHub, a central repository for pre-trained adapter modules. It is a drop-in replacement for transformers, which is regularly updated to stay up-to-date with the developments of transformers. Keywords: Adapters, LoRA, Parameter-efficient fine-tuning, Hub\n## [NeMo](https://github.com/NVIDIA/NeMo)\nNVIDIA [NeMo](https://github.com/NVIDIA/NeMo) is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), text-to-speech synthesis (TTS), large language models (LLMs), and natural language processing (NLP). The primary objective of [NeMo](https://github.com/NVIDIA/NeMo) is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new https://developer.nvidia.com/conversational-ai#started. Keywords: Conversational, ASR, TTS, LLMs, NLP\n## [Runhouse](https://github.com/run-house/runhouse)\n[Runhouse](https://github.com/run-house/runhouse) allows to send code and data to any of your compute or data infra, all in Python, and continue to interact with them normally from your existing code and environment. Runhouse developers mention:\n> Think of it as an expansion pack to your Python interpreter that lets it take detours to remote machines or manipulate remote data. Keywords: MLOps, Infrastructure, Data storage, Modeling\n## [MONAI](https://github.com/Project-MONAI/MONAI)\n[MONAI](https://github.com/Project-MONAI/MONAI) is a PyTorch-based, open-source framework for deep learning in healthcare imaging, part of PyTorch Ecosystem. Its ambitions are:\n- developing a community of academic, industrial and clinical researchers collaborating on a common foundation;\n- creating state-of-the-art, end-to-end training workflows for healthcare imaging;\n- providing researchers with the optimized and standardized way to create and evaluate deep learning models. Keywords: Healthcare imaging, Training, Evaluation\n## [simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers)\nSimple Transformers lets you quickly train and evaluate Transformer models.""]",The default checkpoint used by the sentiment analysis pipeline in the Transformers library is `distilbert base uncased finetuned sst2 english`.,distilbert base uncased finetuned sst2 english
"What is the purpose of the notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi""?
","[""Note that while [CUDA graphs](https://developer.nvidia.com/blog/cuda-graphs/) are not currently compatible with model parallelism in DeepSpeed (DeepSpeed v0.8.2, see [here](https://github.com/microsoft/DeepSpeed/blob/v0.8.2/deepspeed/inference/engine.py#L158)), HPU graphs are supported in Habana's DeepSpeed fork. All benchmarks are doing [greedy generation](https://huggingface.co/blog/how-to-generate#greedy-search) of 100 token outputs. The input prompt is:\n> \""DeepSpeed is a machine learning framework\""\nwhich consists of 7 tokens with BLOOM's tokenizer. The results for inference latency are displayed in the table below (the unit is \\*seconds\\*). | Model | Number of devices | Gaudi2 latency (seconds) | A100-80GB latency (seconds) | First-gen Gaudi latency (seconds) |\n|:-----------:|:-----------------:|:-------------------------:|:-----------------:|:----------------------------------:|\n| BLOOMZ | 8 | 3.103 | 4.402 | / |\n| BLOOMZ-7B | 8 | 0.734 | 2.417 | 3.321 |\n| BLOOMZ-7B | 1 | 0.772 | 2.119 | 2.387 |\n\\*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, leading to a\\* x\\*1.42 speedup on BLOOMZ with Gaudi2 compared to A100.\\*\nThe Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. \\*\\*For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB\\*\\*. Smaller checkpoints present interesting results too. \\*\\*Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!\\*\\* It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device. We also ran these models on first-gen Gaudi. While it is slower than Gaudi2, it is interesting from a price perspective as a DL1 instance on AWS costs approximately 13\\$ per hour. Latency for BLOOMZ-7B on first-gen Gaudi is 2.387 seconds. Thus, \\*\\*first-gen Gaudi offers for the 7-billion checkpoint a better price-performance ratio than A100\\*\\* which costs more than 30\\$ per hour! We expect the Habana team will optimize the performance of these models in the upcoming SynapseAI releases. For example, in our last benchmark, we saw that [Gaudi2 performs Stable Diffusion inference 2.2x faster than A100](https://huggingface.co/blog/habana-gaudi-2-benchmark#generating-images-from-text-with-stable-diffusion) and this has since been improved further to 2.37x with the latest optimizations provided by Habana. We will update these numbers as new versions of SynapseAI are released and integrated within Optimum Habana."", ""--\ntitle: 'Pre-Train BERT with Hugging Face Transformers and Habana Gaudi'\nthumbnail: /blog/assets/99\\_pretraining\\_bert/thumbnail.png\nauthors:\n- user: philschmid\n---\n# Pre-Training BERT with Hugging Face Transformers and Habana Gaudi\nIn this Tutorial, you will learn how to pre-train [BERT-base](https://huggingface.co/bert-base-uncased) from scratch using a Habana Gaudi-based [DL1 instance](https://aws.amazon.com/ec2/instance-types/dl1/) on AWS to take advantage of the cost-performance benefits of Gaudi. We will use the Hugging Face [Transformers](https://huggingface.co/docs/transformers), [Optimum Habana](https://huggingface.co/docs/optimum/habana/index) and [Datasets](https://huggingface.co/docs/datasets) libraries to pre-train a BERT-base model using masked-language modeling, one of the two original BERT pre-training tasks. Before we get started, we need to set up the deep learning environment. [View Code](https://github.com/philschmid/deep-learning-habana-huggingface/blob/master/pre-training/pre-training-bert.ipynb)\nYou will learn how to:\n1."", ""#\n# The provided deepspeed config also activates CPU memory offloading, so chances are that if you\n# have a lot of available CPU memory and you don't mind a slowdown you should be able to load a\n# model that doesn't normally fit into a single GPU. If you have enough GPU memory the program will\n# run faster if you don't want offload to CPU - so disable that section then. #\n# To deploy on 1 gpu:\n#\n# deepspeed --num\\_gpus 1 t0.py\n# or:\n# python -m torch.distributed.run --nproc\\_per\\_node=1 t0.py\n#\n# To deploy on 2 gpus:\n#\n# deepspeed --num\\_gpus 2 t0.py\n# or:\n# python -m torch.distributed.run --nproc\\_per\\_node=2 t0.py\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\nfrom transformers.integrations import HfDeepSpeedConfig\nimport deepspeed\nimport os\nimport torch\nos.environ[\""TOKENIZERS\\_PARALLELISM\""] = \""false\"" # To avoid warnings about parallelism in tokenizers\n# distributed setup\nlocal\\_rank = int(os.getenv(\""LOCAL\\_RANK\"", \""0\""))\nworld\\_size = int(os.getenv(\""WORLD\\_SIZE\"", \""1\""))\ntorch.cuda.set\\_device(local\\_rank)\ndeepspeed.init\\_distributed()\nmodel\\_name = \""bigscience/T0\\_3B\""\nconfig = AutoConfig.from\\_pretrained(model\\_name)\nmodel\\_hidden\\_size = config.d\\_model\n# batch size has to be divisible by world\\_size, but can be bigger than world\\_size\ntrain\\_batch\\_size = 1 \\* world\\_size\n# ds\\_config notes\n#\n# - enable bf16 if you use Ampere or higher GPU - this will run in mixed precision and will be\n# faster. #\n# - for older GPUs you can enable fp16, but it'll only work for non-bf16 pretrained models - e.g. # all official t5 models are bf16-pretrained\n#\n# - set offload\\_param.device to \""none\"" or completely remove the `offload\\_param` section if you don't\n# - want CPU offload\n#\n# - if using `offload\\_param` you can manually finetune stage3\\_param\\_persistence\\_threshold to control\n# - which params should remain on gpus - the larger the value the smaller the offload size\n#\n# For indepth info on Deepspeed config see\n# https://huggingface.co/docs/transformers/main/main\\_classes/deepspeed\n# keeping the same format as json for consistency, except it uses lower case for true/false\n# fmt: off\nds\\_config = {\n\""fp16\"": {\n\""enabled\"": False\n},\n\""bf16\"": {\n\""enabled\"": False\n},\n\""zero\\_optimization\"": {\n\""stage\"": 3,\n\""offload\\_param\"": {\n\""device\"": \""cpu\"",\n\""pin\\_memory\"": True\n},\n\""overlap\\_comm\"": True,\n\""contiguous\\_gradients\"": True,\n\""reduce\\_bucket\\_size\"": model\\_hidden\\_size \\* model\\_hidden\\_size,\n\""stage3\\_prefetch\\_bucket\\_size\"": 0.9 \\* model\\_hidden\\_size \\* model\\_hidden\\_size,\n\""stage3\\_param\\_persistence\\_threshold\"": 10 \\* model\\_hidden\\_size\n},\n\""steps\\_per\\_print\"": 2000,\n\""train\\_batch\\_size\"": train\\_batch\\_size,\n\""train\\_micro\\_batch\\_size\\_per\\_gpu\"": 1,\n\""wall\\_clock\\_breakdown\"": False\n}\n# fmt: on\n# next line instructs transformers to partition the model directly over multiple gpus using\n# deepspeed.zero.Init when model's `from\\_pretrained` method is called. #\n# \\*\\*it has to be run before loading the model AutoModelForSeq2SeqLM.from\\_pretrained(model\\_name)\\*\\*\n#\n# otherwise the model will first be loaded normally and only partitioned at forward time which is\n# less efficient and when there is little CPU RAM may fail\ndschf = HfDeepSpeedConfig(ds\\_config) # keep this object alive\n# now a model can be loaded. model = AutoModelForSeq2SeqLM.from\\_pretrained(model\\_name)\n# initialise Deepspeed ZeRO and store only the engine object\nds\\_engine = deepspeed.initialize(model=model, config\\_params=ds\\_config)[0]\nds\\_engine.module.eval() # inference\n# Deepspeed ZeRO can process unrelated inputs on each GPU."", ""HPUs on Gaudi2 or GPUs on A100/H100). This can be done on Gaudi2 using Habana's [media pipeline](https://docs.habana.ai/en/latest/Media\\_Pipeline/index.html). Given a dataset, most dataloaders follow the following recipe:\n1. Fetch data (e.g. where your JPEG images are stored on disk)\n2. The CPU reads encoded images\n3. The CPU decodes images\n4. The CPU applies image transformations to augment images\n5. Finally, images are sent to devices (although this is usually not done by the dataloader itself)\nInstead of doing the whole process on CPU and send ready-to-train data to devices, a more efficient workflow would be to send encoded images to devices first and then perform image decoding and augmentations:\n1. Same as before\n2. Same as before\n3. Encoded images are sent to devices\n4. Devices decode images\n5. Devices apply image transformations to augment images\nThat way we can benefit from the computing power of our devices to speed image decoding and transformations up. Note that there are two caveats to be aware of when doing this:\n- Device memory consumption will increase, so you may have to reduce your batch size if there is not enough free memory."", ""By default, the training script logs outputs to tensorboard. Pass `--report\\_to wandb` to use weights and\nbiases. Gradient accumulation with a smaller batch size can be used to reduce training requirements to ~20 GB VRAM. ```bash\nexport MODEL\\_DIR=\""runwayml/stable-diffusion-v1-5\""\nexport OUTPUT\\_DIR=\""path to save model\""\naccelerate launch train\\_controlnet.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_DIR \\\n--output\\_dir=$OUTPUT\\_DIR \\\n--dataset\\_name=fusing/fill50k \\\n--resolution=512 \\\n--learning\\_rate=1e-5 \\\n--validation\\_image \""./conditioning\\_image\\_1.png\"" \""./conditioning\\_image\\_2.png\"" \\\n--validation\\_prompt \""red circle with blue background\"" \""cyan circle with brown floral background\"" \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=4\n```\n## Training with multiple GPUs\n`accelerate` allows for seamless multi-GPU training. Follow the instructions [here](https://huggingface.co/docs/accelerate/basic\\_tutorials/launch)\nfor running distributed training with `accelerate`. Here is an example command:\n```bash\nexport MODEL\\_DIR=\""runwayml/stable-diffusion-v1-5\""\nexport OUTPUT\\_DIR=\""path to save model\""\naccelerate launch --mixed\\_precision=\""fp16\"" --multi\\_gpu train\\_controlnet.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_DIR \\\n--output\\_dir=$OUTPUT\\_DIR \\\n--dataset\\_name=fusing/fill50k \\\n--resolution=512 \\\n--learning\\_rate=1e-5 \\\n--validation\\_image \""./conditioning\\_image\\_1.png\"" \""./conditioning\\_image\\_2.png\"" \\\n--validation\\_prompt \""red circle with blue background\"" \""cyan circle with brown floral background\"" \\\n--train\\_batch\\_size=4 \\\n--mixed\\_precision=\""fp16\"" \\\n--tracker\\_project\\_name=\""controlnet-demo\"" \\\n--report\\_to=wandb\n```\n## Example results\n#### After 300 steps with batch size 8\n| | |\n|-------------------|:-------------------------:|\n| | red circle with blue background |\n![conditioning image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet\\_training/conditioning\\_image\\_1.png) | ![red circle with blue background](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet\\_training/red\\_circle\\_with\\_blue\\_background\\_300\\_steps.png) |\n| | cyan circle with brown floral background |\n![conditioning image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet\\_training/conditioning\\_image\\_2.png) | ![cyan circle with brown floral background](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet\\_training/cyan\\_circle\\_with\\_brown\\_floral\\_background\\_300\\_steps.png) |\n#### After 6000 steps with batch size 8:\n| | |\n|-------------------|:-------------------------:|\n| | red circle with blue background |\n![conditioning image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet\\_training/conditioning\\_image\\_1.png) | ![red circle with blue background](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet\\_training/red\\_circle\\_with\\_blue\\_background\\_6000\\_steps.png) |\n| | cyan circle with brown floral background |\n![conditioning image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet\\_training/conditioning\\_image\\_2.png) | ![cyan circle with brown floral background](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet\\_training/cyan\\_circle\\_with\\_brown\\_floral\\_background\\_6000\\_steps.png) |\n## Training on a 16 GB GPU\nOptimizations:\n- Gradient checkpointing\n- bitsandbyte's 8-bit optimizer\n[bitandbytes install instructions](https://github.com/TimDettmers/bitsandbytes#requirements--installation). ```bash\nexport MODEL\\_DIR=\""runwayml/stable-diffusion-v1-5\""\nexport OUTPUT\\_DIR=\""path to save model\""\naccelerate launch train\\_controlnet.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_DIR \\\n--output\\_dir=$OUTPUT\\_DIR \\\n--dataset\\_name=fusing/fill50k \\\n--resolution=512 \\\n--learning\\_rate=1e-5 \\\n--validation\\_image \""./conditioning\\_image\\_1.png\"" \""./conditioning\\_image\\_2.png\"" \\\n--validation\\_prompt \""red circle with blue background\"" \""cyan circle with brown floral background\"" \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=4 \\\n--gradient\\_checkpointing \\\n--use\\_8bit\\_adam\n```\n## Training on a 12 GB GPU\nOptimizations:\n- Gradient checkpointing\n- bitsandbyte's 8-bit optimizer\n- xformers\n- set grads to none\n```bash\nexport MODEL\\_DIR=\""runwayml/stable-diffusion-v1-5\""\nexport OUTPUT\\_DIR=\""path to save model\""\naccelerate launch train\\_controlnet.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_DIR \\\n--output\\_dir=$OUTPUT\\_DIR \\\n--dataset\\_name=fusing/fill50k \\\n--resolution=512 \\\n--learning\\_rate=1e-5 \\\n--validation\\_image \""./conditioning\\_image\\_1.png\"" \""./conditioning\\_image\\_2.png\"" \\\n--validation\\_prompt \""red circle with blue background\"" \""cyan circle with brown floral background\"" \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=4 \\\n--gradient\\_checkpointing \\\n--use\\_8bit\\_adam \\\n--enable\\_xformers\\_memory\\_efficient\\_attention \\\n--set\\_grads\\_to\\_none\n```\nWhen using `enable\\_xformers\\_memory\\_efficient\\_attention`, please make sure to install `xformers` by `pip install xformers`. ## Training on an 8 GB GPU\nWe have not exhaustively tested DeepSpeed support for ControlNet.""]","The notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi"" is designed to guide users on how to leverage the DeepSpeed framework for training large-scale models, specifically those with billions of parameters, using Habana Gaudi hardware. It provides instructions and code examples for setting up the training environment, configuring DeepSpeed, and optimizing performance on Gaudi instances. This enables users to efficiently utilize the capabilities of Habana Gaudi for deep learning tasks.",To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.
"What command line module does PyTorch provide to run a script on multiple GPUs?
","[""Just add the flag `--fp16` to your command launching one of the scripts mentioned above! Using mixed precision training usually results in 2x-speedup for training with the same final results (as shown in\n[this table](https://github.com/huggingface/transformers/tree/main/examples/text-classification#mixed-precision-training)\nfor text classification). ## Running on TPUs\nWhen using Tensorflow, TPUs are supported out of the box as a `tf.distribute.Strategy`. When using PyTorch, we support TPUs thanks to `pytorch/xla`. For more context and information on how to setup your TPU environment refer to Google's documentation and to the\nvery detailed [pytorch/xla README](https://github.com/pytorch/xla/blob/master/README.md). In this repo, we provide a very simple launcher script named\n[xla\\_spawn.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/xla\\_spawn.py) that lets you run our\nexample scripts on multiple TPU cores without any boilerplate. Just pass a `--num\\_cores` flag to this script, then your\nregular training script with its arguments (this is similar to the `torch.distributed.launch` helper for\n`torch.distributed`):\n```bash\npython xla\\_spawn.py --num\\_cores num\\_tpu\\_you\\_have \\\npath\\_to\\_script.py \\\n--all\\_arguments\\_of\\_the\\_script\n```\nAs an example, here is how you would fine-tune the BERT large model (with whole word masking) on the text\nclassification MNLI task using the `run\\_glue` script, with 8 TPUs (from this folder):\n```bash\npython xla\\_spawn.py --num\\_cores 8 \\\ntext-classification/run\\_glue.py \\\n--model\\_name\\_or\\_path bert-large-uncased-whole-word-masking \\\n--task\\_name mnli \\\n--do\\_train \\\n--do\\_eval \\\n--max\\_seq\\_length 128 \\\n--per\\_device\\_train\\_batch\\_size 8 \\\n--learning\\_rate 2e-5 \\\n--num\\_train\\_epochs 3.0 \\\n--output\\_dir /tmp/mnli\\_output/\n```\n## Using Accelerate\nMost PyTorch example scripts have a version using the [🤗 Accelerate](https://github.com/huggingface/accelerate) library\nthat exposes the training loop so it's easy for you to customize or tweak them to your needs."", ""This is analogous to the `with\\_indices` parameter. The `with\\_rank` parameter in the mapped function goes after the `index` one if it is already present. ```py\n>>> from multiprocess import set\\_start\\_method\n>>> import torch\n>>> import os\n>>>\n>>> for i in range(torch.cuda.device\\_count()): # send model to every GPU\n... model.to(f\""cuda:{i}\"")\n>>>\n>>> def gpu\\_computation(example, rank):\n... torch.cuda.set\\_device(f\""cuda:{rank}\"") # use one GPU\n... # Your big GPU call goes here, for example\n... inputs = tokenizer(texts, truncation=True, return\\_tensors=\""pt\"").to(f\""cuda:{rank}\"")\n... outputs = model.generate(\\*\\*inputs)\n... example[\""generated\\_text\""] = tokenizer.batch\\_decode(translated\\_tokens, skip\\_special\\_tokens=True)\n... return example\n>>>\n>>> if \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\n... set\\_start\\_method(\""spawn\"")\n... updated\\_dataset = dataset.map(gpu\\_computation, with\\_rank=True, num\\_proc=torch.cuda.device\\_count())\n```\nThe main use-case for rank is to parallelize computation across several GPUs."", ""## PyTorch Distributed\nPyTorch supports [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) which enables data parallelism. To start, create a Python file and import `torch.distributed` and `torch.multiprocessing` to set up the distributed process group and to spawn the processes for inference on each GPU. You should also initialize a [`DiffusionPipeline`]:\n```py\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom diffusers import DiffusionPipeline\nsd = DiffusionPipeline.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n)\n```\nYou'll want to create a function to run inference; [`init\\_process\\_group`](https://pytorch.org/docs/stable/distributed.html?highlight=init\\_process\\_group#torch.distributed.init\\_process\\_group) handles creating a distributed environment with the type of backend to use, the `rank` of the current process, and the `world\\_size` or the number of processes participating. If you're running inference in parallel over 2 GPUs, then the `world\\_size` is 2. Move the [`DiffusionPipeline`] to `rank` and use `get\\_rank` to assign a GPU to each process, where each process handles a different prompt:\n```py\ndef run\\_inference(rank, world\\_size):\ndist.init\\_process\\_group(\""nccl\"", rank=rank, world\\_size=world\\_size)\nsd.to(rank)\nif torch.distributed.get\\_rank() == 0:\nprompt = \""a dog\""\nelif torch.distributed.get\\_rank() == 1:\nprompt = \""a cat\""\nimage = sd(prompt).images[0]\nimage.save(f\""./{'\\_'.join(prompt)}.png\"")\n```\nTo run the distributed inference, call [`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn) to run the `run\\_inference` function on the number of GPUs defined in `world\\_size`:\n```py\ndef main():\nworld\\_size = 2\nmp.spawn(run\\_inference, args=(world\\_size,), nprocs=world\\_size, join=True)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\nmain()\n```\nOnce you've completed the inference script, use the `--nproc\\_per\\_node` argument to specify the number of GPUs to use and call `torchrun` to run the script:\n```bash\ntorchrun run\\_distributed.py --nproc\\_per\\_node=2\n```"", ""This is done very simply using hooks. > [hooks](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register\\_forward\\_hook) are a PyTorch API that adds functions executed just before each forward called\nWe couldn't use this directly since they only support models with regular arguments and no keyword arguments in their forward pass, but we took the same idea. Once the model is loaded, the `dispatch\\_model` function will add hooks to every module and submodule that are executed before and after each forward pass. They will:\n- make sure all the inputs of the module are on the same device as the weights;\n- if the weights have been offloaded to the CPU, move them to GPU 0 before the forward pass and back to the CPU just after;\n- if the weights have been offloaded to disk, load them in RAM then on the GPU 0 before the forward pass and free this memory just after. The whole process is summarized in the following video:\nThis way, your model can be loaded and run even if you don't have enough GPU RAM and CPU RAM. The only thing you need is disk space (and lots of patience!) While this solution is pretty naive if you have multiple GPUs (there is no clever pipeline parallelism involved, just using the GPUs sequentially) it still yields [pretty decent results for BLOOM](https://huggingface.co/blog/bloom-inference-pytorch-scripts). And it allows you to run the model on smaller setups (albeit more slowly)."", ""Scripts\nA train, validation, inference, and checkpoint cleaning script included in the github root folder. Scripts are not currently packaged in the pip release. The training and validation scripts evolved from early versions of the [PyTorch Imagenet Examples](https://github.com/pytorch/examples). I have added significant functionality over time, including CUDA specific performance enhancements based on\n[NVIDIA's APEX Examples](https://github.com/NVIDIA/apex/tree/master/examples). ## Training Script\nThe variety of training args is large and not all combinations of options (or even options) have been fully tested. For the training dataset folder, specify the folder to the base that contains a `train` and `validation` folder. To train an SE-ResNet34 on ImageNet, locally distributed, 4 GPUs, one process per GPU w/ cosine schedule, random-erasing prob of 50% and per-pixel random value:\n`./distributed\\_train.sh 4 /data/imagenet --model seresnet34 --sched cosine --epochs 150 --warmup-epochs 5 --lr 0.4 --reprob 0.5 --remode pixel --batch-size 256 --amp -j 4`\nNOTE: It is recommended to use PyTorch 1.9+ w/ PyTorch native AMP and DDP instead of APEX AMP.""]",PyTorch provides the `torchrun` command line module to run a script on multiple GPUs. You can use it with the `--nproc_per_node` argument to specify the number of GPUs to use.,torchrun
"What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?
","[""--\ntitle: \""Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\""\nthumbnail: /blog/assets/97\\_vision\\_transformers/thumbnail.png\nauthors:\n- user: juliensimon\n---\n# Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\nThis blog post will show how easy it is to fine-tune pre-trained Transformer models for your dataset using the Hugging Face Optimum library on Graphcore Intelligence Processing Units (IPUs). As an example, we will show a step-by-step guide and provide a notebook that takes a large, widely-used chest X-ray dataset and trains a vision transformer (ViT) model. ## Introducing vision transformer (ViT) models\n\nIn 2017 a group of Google AI researchers published a paper introducing the transformer model architecture. Characterised by a novel self-attention mechanism, transformers were proposed as a new and efficient group of models for language applications. Indeed, in the last five years, transformers have seen explosive popularity and are now accepted as the de facto standard for natural language processing (NLP). Transformers for language are perhaps most notably represented by the rapidly evolving GPT and BERT model families. Both can run easily and efficiently on Graphcore IPUs as part of the growing [Hugging Face Optimum Graphcore library](/posts/getting-started-with-hugging-face-transformers-for-ipus-with-optimum)). ![transformers_chrono](https://www.graphcore.ai/hs-fs/hubfs/transformers_chrono.png?width=1024&name=transformers_chrono.png)\n\nA timeline showing releases of prominent transformer language models (credit: Hugging Face)\n\nAn in-depth explainer about the transformer model architecture (with a focus on NLP) can be found [on the Hugging Face website](https://huggingface.co/course/chapter1/4?fw=pt)."", ""Vision Transformers 图像分类\n相关空间：https://huggingface.co/spaces/abidlabs/vision-transformer\n标签：VISION, TRANSFORMERS, HUB\n## 简介\n图像分类是计算机视觉中的重要任务。构建更好的分类器以确定图像中存在的对象是当前研究的热点领域，因为它在从人脸识别到制造质量控制等方面都有应用。\n最先进的图像分类器基于 \\_transformers\\_ 架构，该架构最初在自然语言处理任务中很受欢迎。这种架构通常被称为 vision transformers (ViT)。这些模型非常适合与 Gradio 的\\*图像\\*输入组件一起使用，因此在本教程中，我们将构建一个使用 Gradio 进行图像分类的 Web 演示。我们只需用\\*\\*一行 Python 代码\\*\\*即可构建整个 Web 应用程序，其效果如下（试用一下示例之一！）：\n让我们开始吧！\n### 先决条件\n确保您已经[安装](/getting\\_started)了 `gradio` Python 包。\n## 步骤 1 - 选择 Vision 图像分类模型\n首先，我们需要一个图像分类模型。在本教程中，我们将使用[Hugging Face Model Hub](https://huggingface.co/models?pipeline\\_tag=image-classification)上的一个模型。该 Hub 包含数千个模型，涵盖了多种不同的机器学习任务。\n在左侧边栏中展开 Tasks 类别，并选择我们感兴趣的“Image Classification”作为我们的任务。然后，您将看到 Hub 上为图像分类设计的所有模型。\n在撰写时，最受欢迎的模型是 `google/vit-base-patch16-224`，该模型在分辨率为 224x224 像素的 ImageNet 图像上进行了训练。我们将在演示中使用此模型。\n## 步骤 2 - 使用 Gradio 加载 Vision Transformer 模型\n当使用 Hugging Face Hub 上的模型时，我们无需为演示定义输入或输出组件。同样，我们不需要关心预处理或后处理的细节。所有这些都可以从模型标签中自动推断出来。\n除了导入语句外，我们只需要一行代码即可加载并启动演示。\n我们使用 `gr.Interface.load()` 方法，并传入包含 `huggingface/` 的模型路径，以指定它来自 Hugging Face Hub。\n```python\nimport gradio as gr\ngr.Interface.load(\n\""huggingface/google/vit-base-patch16-224\"",\nexamples=[\""alligator.jpg\"", \""laptop.jpg\""]).launch()\n```\n请注意，我们添加了一个 `examples` 参数，允许我们使用一些预定义的示例预填充我们的界面。\n这将生成以下接口，您可以直接在浏览器中尝试。当您输入图像时，它会自动进行预处理并发送到 Hugging Face Hub API，通过模型处理，并以人类可解释的预测结果返回。尝试上传您自己的图像！\n---\n完成！只需一行代码，您就建立了一个图像分类器的 Web 演示。如果您想与他人分享，请在 `launch()` 接口时设置 `share=True`。"", ""Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. 1. \\*\\*[SwiftFormer](https://huggingface.co/docs/transformers/model\\_doc/swiftformer)\\*\\* (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan. 1. \\*\\*[Swin Transformer](https://huggingface.co/docs/transformers/model\\_doc/swin)\\*\\* (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. 1. \\*\\*[Swin Transformer V2](https://huggingface.co/docs/transformers/model\\_doc/swinv2)\\*\\* (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo. 1. \\*\\*[Swin2SR](https://huggingface.co/docs/transformers/model\\_doc/swin2sr)\\*\\* (from University of Würzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V."", ""Note that one should\nuse [`DeiTImageProcessor`] in order to prepare images for the model. - [BEiT](beit) (BERT pre-training of Image Transformers) by Microsoft Research. BEiT models outperform supervised pre-trained\nvision transformers using a self-supervised method inspired by BERT (masked image modeling) and based on a VQ-VAE. - DINO (a method for self-supervised training of Vision Transformers) by Facebook AI. Vision Transformers trained using\nthe DINO method show very interesting properties not seen with convolutional models. They are capable of segmenting\nobjects, without having ever been trained to do so. DINO checkpoints can be found on the [hub](https://huggingface.co/models?other=dino). - [MAE](vit\\_mae) (Masked Autoencoders) by Facebook AI. By pre-training Vision Transformers to reconstruct pixel values for a high portion\n(75%) of masked patches (using an asymmetric encoder-decoder architecture), the authors show that this simple method outperforms\nsupervised pre-training after fine-tuning."", ""Vision Transformer (ViT)\nThe \\*\\*Vision Transformer\\*\\* is a model for image classification that employs a Transformer-like architecture over patches of the image. This includes the use of [Multi-Head Attention](https://paperswithcode.com/method/multi-head-attention), [Scaled Dot-Product Attention](https://paperswithcode.com/method/scaled) and other architectural features seen in the [Transformer](https://paperswithcode.com/method/transformer) architecture traditionally used for NLP. ## How do I use this model on an image? To load a pretrained model:\n```python\nimport timm\nmodel = timm.create\\_model('vit\\_base\\_patch16\\_224', pretrained=True)\nmodel.eval()\n```\nTo load and preprocess the image:\n```python\nimport urllib\nfrom PIL import Image\nfrom timm.data import resolve\\_data\\_config\nfrom timm.data.transforms\\_factory import create\\_transform\nconfig = resolve\\_data\\_config({}, model=model)\ntransform = create\\_transform(\\*\\*config)\nurl, filename = (\""https://github.com/pytorch/hub/raw/master/images/dog.jpg\"", \""dog.jpg\"")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\nTo get the model predictions:\n```python\nimport torch\nwith torch.no\\_grad():\nout = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n```\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename = (\""https://raw.githubusercontent.com/pytorch/hub/master/imagenet\\_classes.txt\"", \""imagenet\\_classes.txt\"")\nurllib.request.urlretrieve(url, filename)\nwith open(\""imagenet\\_classes.txt\"", \""r\"") as f:\ncategories = [s.strip() for s in f.readlines()]\n# Print top categories per image\ntop5\\_prob, top5\\_catid = torch.topk(probabilities, 5)\nfor i in range(top5\\_prob.size(0)):\nprint(categories[top5\\_catid[i]], top5\\_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\nReplace the model name with the variant you want to use, e.g. `vit\\_base\\_patch16\\_224`. You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature\\_extraction/), just change the name of the model you want to use. ## How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). ```python\nmodel = timm.create\\_model('vit\\_base\\_patch16\\_224', pretrained=True, num\\_classes=NUM\\_FINETUNE\\_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.""]","The most popular vision transformer model on the Hugging Face Model Hub for image classification is `google/vit-base-patch16-224`, which has been trained on ImageNet images with a resolution of 224x224 pixels.",google/vit-base-patch16-224
"What is the command to upload an ESPnet model to a Hugging Face repository?
","[""Using ESPnet at Hugging Face\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks. ## Exploring ESPnet in the Hub\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). All models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more. 2. Metadata tags that help for discoverability and contain information such as license, language and datasets. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png)\n\n## Using existing models\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\_model\\_zoo)). If you're interested in doing inference, different classes for different tasks have a `from\\_pretrained` method that allows loading models from the Hub."", ""Make sure you are logged into Hugging Face before you begin:\n```bash\nhuggingface-cli login\n```\nThen add the `push\\_to\\_hub` argument to the script. This argument will create a repository with your Hugging Face username and the folder name specified in `output\\_dir`. To give your repository a specific name, use the `push\\_to\\_hub\\_model\\_id` argument to add it. The repository will be automatically listed under your namespace. The following example shows how to upload a model with a specific repository name:\n```bash\npython examples/pytorch/summarization/run\\_summarization.py\n--model\\_name\\_or\\_path t5-small \\\n--do\\_train \\\n--do\\_eval \\\n--dataset\\_name cnn\\_dailymail \\\n--dataset\\_config \""3.0.0\"" \\\n--source\\_prefix \""summarize: \"" \\\n--push\\_to\\_hub \\\n--push\\_to\\_hub\\_model\\_id finetuned-t5-cnn\\_dailymail \\\n--output\\_dir /tmp/tst-summarization \\\n--per\\_device\\_train\\_batch\\_size=4 \\\n--per\\_device\\_eval\\_batch\\_size=4 \\\n--overwrite\\_output\\_dir \\\n--predict\\_with\\_generate\n```"", ""Data is hosted on HuggingFace Datasets."", ""4. As soon as your Space is built, Hugging Face will detect that it is associated with the model. A \""Linked Models\"" button should appear in the top right corner of the Space, as shown here:\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/linked-models.png)\n\\*Note\\*: You can also add linked models manually by explicitly updating them in the [README metadata for the Space, as described here](https://huggingface.co/docs/hub/spaces-config-reference). Your Space should appear in the Demo tab next to the paper on ArXiv in a few minutes 🤗"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Adding support for an unsupported architecture\nIf you wish to export a model whose architecture is not already supported by the library, the PR [#813 Adds support for ResNet](https://github.com/huggingface/optimum/pull/813 ) can be used as a reference. You can make sure tests pass for the new `my\\_new\\_modeltype` model type by running:\n```bash\npytest tests/exporters/tflite/test\\_\\*.py -k \""my\\_new\\_modeltype\"" -s --exitfirst\n```""]","To upload an ESPnet model to a Hugging Face repository, you can use the following command with the `push_to_hub` argument:

```bash
python examples/pytorch/summarization/run_summarization.py
--model_name_or_path t5-small \
--do_train \
--do_eval \
--dataset_name cnn_dailymail \
--dataset_config ""3.0.0"" \
--source_prefix ""summarize: "" \
--push_to_hub \
--push_to_hub_model_id finetuned-t5-cnn_dailymail \
--output_dir /tmp/tst-summarization \
--per_device_train_batch_size=4 \
--per_device_eval_batch_size=4 \
--overwrite_output_dir \
--predict_with_generate
```

Make sure you are logged into Hugging Face before you begin by running:

```bash
huggingface-cli login
```",./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo
"What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?
","[""Add custom Dependencies\nInference Endpoints’ base image includes all required libraries to run inference on 🤗 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\\* [customize your inference pipeline](/docs/inference-endpoints/guides/custom\\_handler) and need additional Python dependencies\n\\* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)). To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within. ```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\\* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n\\* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom\\_container) for inference."", ""This is a non-revertible action that will completely remove the endpoint, including its configuration, logs and usage metrics. You cannot restore a deleted Inference Endpoint. ## An end-to-end example\nA typical use case of Inference Endpoints is to process a batch of jobs at once to limit the infrastructure costs. You can automate this process using what we saw in this guide:\n```py\n>>> import asyncio\n>>> from huggingface\\_hub import create\\_inference\\_endpoint\n# Start endpoint + wait until initialized\n>>> endpoint = create\\_inference\\_endpoint(name=\""batch-endpoint\"",...).wait()\n# Run inference\n>>> client = endpoint.client\n>>> results = [client.text\\_generation(...) for job in jobs]\n# Or with asyncio\n>>> async\\_client = endpoint.async\\_client\n>>> results = asyncio.gather(\\*[async\\_client.text\\_generation(...) for job in jobs])\n# Pause endpoint\n>>> endpoint.pause()\n```\nOr if your Inference Endpoint already exists and is paused:\n```py\n>>> import asyncio\n>>> from huggingface\\_hub import get\\_inference\\_endpoint\n# Get endpoint + wait until initialized\n>>> endpoint = get\\_inference\\_endpoint(\""batch-endpoint\"").resume().wait()\n# Run inference\n>>> async\\_client = endpoint.async\\_client\n>>> results = asyncio.gather(\\*[async\\_client.text\\_generation(...) for job in jobs])\n# Pause endpoint\n>>> endpoint.pause()\n```"", ""To do so, you can define them under the `models` and `datasets` keys. In addition to listing the artefacts in the README file, you can also record them in any `.py`, `.ini` or `.html` file as well. We'll parse it auto-magically! Here's an example linking two models from a space:\n```\ntitle: My lovely space\nemoji: 🤗\ncolorFrom: blue\ncolorTo: green\nsdk: docker\npinned: false\nmodels:\n- reach-vb/musicgen-large-fp16-endpoint\n- reach-vb/wav2vec2-large-xls-r-1B-common\\_voice7-lt-ft\n```"", ""You can use `scipy` in Python to write it to a .wav file. ```python\nimport scipy\nimport numpy as np\n# output is [[-0.182352,-0.17802449, ...]]\nscipy.io.wavfile.write(\""musicgen\\_out.wav\"", rate=32000, data=np.array(output[0]))\n```\nAnd voila! Play with the demo below to try the endpoint out. ## Conclusion\nIn this blog post, we have shown how to deploy MusicGen using Inference Endpoints with a custom inference handler. The same technique can be used for any other model in the Hub that does not have an associated pipeline. All you have to do is override the `Endpoint Handler` class in `handler.py`, and add `requirements.txt` to reflect your project's dependencies. ### Read More\n- [Inference Endpoints documentation covering Custom Handler](https://huggingface.co/docs/inference-endpoints/guides/custom\\_handler)"", ""Some of my requests failed because I deliberately omitted the `Content-Type` header. `![](assets/109_inference_endpoints/endpoints06.png)`\nFor additional details, I can check the full logs in the `Logs` tab. ```\n5c7fbb4485cd8w7 2022-10-10T08:19:04.915Z 2022-10-10 08:19:04,915 | INFO | POST / | Duration: 142.76 ms\n5c7fbb4485cd8w7 2022-10-10T08:19:05.860Z 2022-10-10 08:19:05,860 | INFO | POST / | Duration: 148.06 ms\n5c7fbb4485cd8w7 2022-10-10T09:21:39.251Z 2022-10-10 09:21:39,250 | ERROR | Content type \""None\"" not supported. Supported content types are: application/json, text/csv, text/plain, image/png, image/jpeg, image/jpg, image/tiff, image/bmp, image/gif, image/webp, image/x-image, audio/x-flac, audio/flac, audio/mpeg, audio/wave, audio/wav, audio/x-wav, audio/ogg, audio/x-audio, audio/webm, audio/webm;codecs=opus\n5c7fbb4485cd8w7 2022-10-10T09:21:44.114Z 2022-10-10 09:21:44,114 | ERROR | Content type \""None\"" not supported. Supported content types are: application/json, text/csv, text/plain, image/png, image/jpeg, image/jpg, image/tiff, image/bmp, image/gif, image/webp, image/x-image, audio/x-flac, audio/flac, audio/mpeg, audio/wave, audio/wav, audio/x-wav, audio/ogg, audio/x-audio, audio/webm, audio/webm;codecs=opus\n```\nNow, let's increase our security level and deploy a private endpoint. ### Deploying a Private Inference Endpoint\nRepeating the steps above, I select `Private` this time.""]","To install custom Python dependencies for Inference Endpoints, you should add a `requirements.txt` file to your model repository on the Hugging Face Hub. This file should list the Python dependencies you want to install.",requirements.txt
"How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""ote: This is a simplified version of the code needed to create the Stable Diffusion demo."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Safe Stable Diffusion\nSafe Stable Diffusion was proposed in [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://huggingface.co/papers/2211.05105) and mitigates inappropriate degeneration from Stable Diffusion models because they're trained on unfiltered web-crawled datasets. For instance Stable Diffusion may unexpectedly generate nudity, violence, images depicting self-harm, and otherwise offensive content. Safe Stable Diffusion is an extension of Stable Diffusion that drastically reduces this type of content. The abstract from the paper is:\n\\*Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications."", ""They can be found through the following links:\n1. [Stable Diffusion v1.1](https://huggingface.co/CompVis/stable-diffusion-v1-1)\n2. [Stable Diffusion v1.2](https://huggingface.co/CompVis/stable-diffusion-v1-2)\n3. [Stable Diffusion v1.3](https://huggingface.co/CompVis/stable-diffusion-v1-3)\n4. [Stable Diffusion v1.4](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n```python\nfrom diffusers import DiffusionPipeline\nimport matplotlib.pyplot as plt\npipe = DiffusionPipeline.from\\_pretrained('CompVis/stable-diffusion-v1-4', custom\\_pipeline='suvadityamuk/StableDiffusionComparison')\npipe.enable\\_attention\\_slicing()\npipe = pipe.to('cuda')\nprompt = \""an astronaut riding a horse on mars\""\noutput = pipe(prompt)\nplt.subplots(2,2,1)\nplt.imshow(output.images[0])\nplt.title('Stable Diffusion v1.1')\nplt.axis('off')\nplt.subplots(2,2,2)\nplt.imshow(output.images[1])\nplt.title('Stable Diffusion v1.2')\nplt.axis('off')\nplt.subplots(2,2,3)\nplt.imshow(output.images[2])\nplt.title('Stable Diffusion v1.3')\nplt.axis('off')\nplt.subplots(2,2,4)\nplt.imshow(output.images[3])\nplt.title('Stable Diffusion v1.4')\nplt.axis('off')\nplt.show()\n```\nAs a result, you can look at a grid of all 4 generated images being shown together, that captures a difference the advancement of the training between the 4 checkpoints."", ""However, the story is much different for 2D. In this part, we'll talk about how you can use AI to generate 2D Assets. ### Preface\nThis tutorial describes a collaborative process for generating 2D Assets, where Stable Diffusion is incorporated as a tool in a conventional 2D workflow. This is intended for readers with some knowledge of image editing and 2D asset creation but may otherwise be helpful for beginners and experts alike. Requirements:\n- Your preferred image-editing software, such as [Photoshop](https://www.adobe.com/products/photoshop.html) or [GIMP](https://www.gimp.org/) (free). - Stable Diffusion. For instructions on setting up Stable Diffusion, refer to [Part 1](https://huggingface.co/blog/ml-for-games-1#setting-up-stable-diffusion). ### Image2Image\n[Diffusion models](https://en.wikipedia.org/wiki/Diffusion\\_model) such as Stable Diffusion work by reconstructing images from noise, guided by text. Image2Image uses the same process but starts with real images as input rather than noise. This means that the outputs will, to some extent, resemble the input image. An important parameter in Image2Image is \\*\\*denoising strength\\*\\*. This controls the extent to which the model changes the input. A denoising strength of 0 will reproduce the input image exactly, while a denoising strength of 1 will generate a very different image. Another way to think about denoising strength is \\*\\*creativity\\*\\*. The image below demonstrates image-to-image with an input image of a circle and the prompt \""moon\"", at various denoising strengths. ![Denoising Strength Example](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/moons.png)\n\nImage2Image allows Stable Diffusion to be used as a tool, rather than as a replacement for the conventional artistic workflow.""]","To teach new concepts to Stable Diffusion using Textual Inversion, you need just 3-5 examples.",3-5 images
"What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?
","[""In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future. ## Sharded checkpoints\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save\\_pretrained(save\\_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in. You can control the maximum size before sharding with the `max\\_shard\\_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model. ```py\nfrom transformers import AutoModel\nmodel = AutoModel.from\\_pretrained(\""bert-base-cased\"")\n```\nIf you save it using [`~PreTrainedModel.save\\_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n```py\n>>> import os\n>>> import tempfile\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir)\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model.bin']\n```\nNow let's use a maximum shard size of 200MB:\n```py\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model-00001-of-00003.bin', 'pytorch\\_model-00002-of-00003.bin', 'pytorch\\_model-00003-of-00003.bin', 'pytorch\\_model.bin.index.json']\n```\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index."", ""CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.74 GiB free; 9.06 GiB reserved in total by PyTorch)\n2 / 2\nDoesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.24 GiB free; 9.56 GiB reserved in total by PyTorch)\n==================== INFERENCE - MEMORY - RESULT ====================\n--------------------------------------------------------------------------------\nModel Name Batch Size Seq Length Memory in MB\n--------------------------------------------------------------------------------\nReformer-No-Chunk 8 1024 4281\nReformer-No-Chunk 8 2048 7607\nReformer-No-Chunk 8 4096 N/A\nReformer-Chunk 8 1024 4309\nReformer-Chunk 8 2048 7669\nReformer-Chunk 8 4096 N/A\n--------------------------------------------------------------------------------\nInteresting, chunked feed forward layers do not seem to help here at all. The reason is that `config.feed\\_forward\\_size` is not sufficiently large to make a real difference."", ""- Even though the checkpoint is trained on images of specific size, the model will work on images of any size. The smallest supported image size is 32x32. - One can use [`MobileNetV1ImageProcessor`] to prepare images for the model. - The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes). However, the model predicts 1001 classes: the 1000 classes from ImageNet plus an extra “background” class (index 0). - The original TensorFlow checkpoints use different padding rules than PyTorch, requiring the model to determine the padding amount at inference time, since this depends on the input image size. To use native PyTorch padding behavior, create a [`MobileNetV1Config`] with `tf\\_padding = False`. Unsupported features:\n- The [`MobileNetV1Model`] outputs a globally pooled version of the last hidden state. In the original model it is possible to use a 7x7 average pooling layer with stride 2 instead of global pooling. For larger inputs, this gives a pooled output that is larger than 1x1 pixel. The HuggingFace implementation does not support this. - It is currently not possible to specify an `output\\_stride`. For smaller output strides, the original model invokes dilated convolution to prevent the spatial resolution from being reduced further. The output stride of the HuggingFace model is always 32. - The original TensorFlow checkpoints include quantized models."", ""--\ntitle: \""Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\""\nthumbnail: /blog/assets/62\\_pytorch\\_fsdp/fsdp-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\nIn this post we will look at how we can leverage \\*\\*[Accelerate](https://github.com/huggingface/accelerate)\\*\\* Library for training large models which enables users to leverage the latest features of \\*\\*[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)\\*\\*. # Motivation 🤗\n\\*\\*With the ever increasing scale, size and parameters of the Machine Learning (ML) models, ML practitioners are finding it difficult to train or even load such large models on their hardware.\\*\\* On one hand, it has been found that large models learn quickly (data and compute efficient) and are significantly more performant when compared to smaller models [1]; on the other hand, it becomes prohibitive to train such models on most of the available hardware."", ""Alternatively, use 🤗 Accelerate to gain full control over the training loop. Find the 🤗 Accelerate example\n[further down in this guide](#using-accelerate). While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can\nresult in a more pronounced training slowdown. Consider the following example. Let's say, the `per\\_device\\_train\\_batch\\_size=4`\nwithout gradient accumulation hits the GPU's limit. If you would like to train with batches of size 64, do not set the\n`per\\_device\\_train\\_batch\\_size` to 1 and `gradient\\_accumulation\\_steps` to 64. Instead, keep `per\\_device\\_train\\_batch\\_size=4`\nand set `gradient\\_accumulation\\_steps=16`. This results in the same effective batch size while making better use of\nthe available GPU resources. For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\nand [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957). ## Gradient Checkpointing\nSome large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used.""]","In Transformers version 4.18.0, model checkpoints that take more than 10GB of space are automatically sharded into smaller pieces.",10GB
"What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?
","[""As in the examples above, some common steps that may help decide whether and how to apply ML in a way that minimizes bias-related risk include:\n\\* Investigate:\n\\* Reports of bias in the field pre-ML\n\\* At-risk demographic categories for your specific use case\n\\* Examine:\n\\* The impact of your optimization objective on reinforcing biases\n\\* Alternative objectives that favor diversity and positive long-term impacts\n### I am curating/picking a dataset for my ML system, how can I address bias? While training datasets are [not the sole source of bias](https://www.cell.com/patterns/fulltext/S2666-3899(21)00061-1) in the ML development cycle, they do play a significant role. Does your [dataset disproportionately associate](https://aclanthology.org/2020.emnlp-main.23/) biographies of women with life events but those of men with achievements? Those \\*\\*stereotypes\\*\\* are probably going to show up in your full ML system! Does your voice recognition dataset only feature specific accents? Not a good sign for [the inclusivity of technology](https://www.scientificamerican.com/article/speech-recognition-tech-is-yet-another-example-of-bias/) you build with it in terms of \\*\\*disparate performance\\*\\*! Whether you’re curating a dataset for ML applications or selecting a dataset to train an ML model, finding out, mitigating, and [communicating](https://dl.acm.org/doi/10.1145/3479582) to what extent the data exhibits these phenomena are all necessary steps to reducing bias-related risks. You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it comes from, who are the people represented on the data, and what the curation process was. Several frameworks for this reflection and documentation have been proposed such as [Data Statements for NLP](https://direct.mit.edu/tacl/article/doi/10.1162/tacl\\_a\\_00041/43452/Data-Statements-for-Natural-Language-Processing) or [Datasheets for Datasets](https://dl.acm.org/doi/10.1145/3458723). The Hugging Face Hub includes a Dataset Card [template](https://github.com/huggingface/datasets/blob/main/templates/README.md) and [guide](https://github.com/huggingface/datasets/blob/main/templates/README\\_guide.md#dataset-card-creation-guide) inspired by these works; the section on [considerations for using the data](https://github.com/huggingface/datasets/blob/main/templates/README\\_guide.md#considerations-for-using-the-data) is usually a good place to look for information about notable biases if you’re browsing datasets, or to write a paragraph sharing your insights on the topic if you’re sharing a new one. And if you’re looking for more inspiration on what to put there, check out these sections written by Hub users in the [BigLAM organization](https://huggingface.co/biglam) for historical datasets of [legal proceedings](https://huggingface.co/datasets/biglam/old\\_bailey\\_proceedings#social-impact-of-dataset), [image classification](https://huggingface.co/datasets/biglam/brill\\_iconclass#social-impact-of-dataset), and [newspapers](https://huggingface.co/datasets/biglam/bnl\\_newspapers1841-1879#social-impact-of-dataset). ![HF Dataset Card guide for the Social Impact and Bias Sections](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img3.png)\n*[HF Dataset Card guide](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md#social-impact-of-dataset) for the Social Impact and Bias Sections*\n\nWhile describing the origin and context of a dataset is always a good starting point to understand the biases at play, [quantitatively measuring phenomena](https://arxiv.org/abs/2212.05129) that encode those biases can be just as helpful."", ""If you wish to implement your own costumized data loading/sampling, feel free to just make use of the `\""path\""` column instead and disregard the `\""audio\""` column. Training & Evaluation\n---------------------\nThe data is processed so that we are ready to start setting up the\ntraining pipeline. We will make use of 🤗\\'s\n[Trainer](https://huggingface.co/transformers/master/main\\_classes/trainer.html?highlight=trainer)\nfor which we essentially need to do the following:\n- Define a data collator. In contrast to most NLP models, Wav2Vec2 has\na much larger input length than output length. \\*E.g.\\*, a sample of\ninput length 50000 has an output length of no more than 100. Given\nthe large input sizes, it is much more efficient to pad the training\nbatches dynamically meaning that all training samples should only be\npadded to the longest sample in their batch and not the overall\nlongest sample. Therefore, fine-tuning Wav2Vec2 requires a special\npadding data collator, which we will define below\n- Evaluation metric."", ""For full details on this package, please take a look at [this WWDC session](https://developer.apple.com/wwdc23/10047). In the case of Stable Diffusion, we’ll be using \\_6-bit palettization\\_, a type of quantization that compresses model weights from a 16-bit floating-point representation to just 6 bits per parameter. The name “palettization” refers to a technique similar to the one used in computer graphics to work with a limited set of colors: the color table (or “palette”) contains a fixed number of colors, and the colors in the image are replaced with the indexes of the closest colors available in the palette. This immediately provides the benefit of drastically reducing storage size, and thus reducing download time and on-device disk use. ![Illustration of 2-bit palettization. Image credit: Apple WWDC’23 Session 'Use Core ML Tools for machine learning model compression'](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-diffusers-coreml/palettization_illustration.png)\nIllustration of 2-bit palettization. Image credit: Apple WWDC’23 Session *[Use Core ML Tools for machine learning model compression](https://developer.apple.com/wwdc23/10047)*. The compressed 6-bit \\_weights\\_ cannot be used for computation, because they are just indices into a table and no longer represent the magnitude of the original weights. Therefore, Core ML needs to uncompress the palletized weights before use. In previous versions of Core ML, uncompression took place when the model was first loaded from disk, so the amount of memory used was equal to the uncompressed model size. With the new improvements, weights are kept as 6-bit numbers and converted on the fly as inference progresses from layer to layer."", ""after ~20h on 8 A100 GPUs). All the training statistics of the training run are available on [Weights & Biases](https://wandb.ai/lvwerra/trl/runs/ie2h4q8p). ![Per batch reward at each step during training. The model’s performance plateaus after around 1000 steps.](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/wandb\\_reward.png)\n\\*Per batch reward at each step during training. The model’s performance plateaus after around 1000 steps.\\*\nSo what can the model do after training? Let's have a look! ![llama prompt](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/llama\\_prompt.png)\nAlthough we shouldn't trust its advice on LLaMA matters just, yet, the answer looks coherent and even provides a Google link. Let's have a look and some of the training challenges next. ## Challenges, instabilities and workarounds\nTraining LLMs with RL is not always plain sailing. The model we demo today is the result of many experiments, failed runs and hyper-parameter sweeps."", ""For example, a high accuracy might be because a model is doing well, but if the data is unbalanced, it might also be because the model is only accurately labeling the high-frequency class. In such cases, a more detailed analysis of the model's behavior, or the use of a different metric entirely, is necessary to determine how well the model is actually performing. ## Citation(s)\n```bibtex\n@article{scikit-learn,\ntitle={Scikit-learn: Machine Learning in {P}ython},\nauthor={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\nCournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\njournal={Journal of Machine Learning Research},\nvolume={12},\npages={2825--2830},\nyear={2011}\n}\n```\n## Further References""]","Weights and Biases (W&B) is a tool used by data scientists and machine learning scientists to track experiments, visualize results, and manage datasets. It helps in monitoring the training process of machine learning models by providing insights into various metrics, such as loss and accuracy, over time. W&B allows users to log hyperparameters, visualize performance metrics, and compare different runs, making it easier to understand model behavior and optimize performance. Additionally, it facilitates collaboration among team members by sharing results and findings in a structured manner.","To track their machine learning experiments at every stage, from training to production."
"What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?
","[""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\n\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\n\n#### **English** | [简体中文](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [繁體中文](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [한국어](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Español](https://github.com/huggingface/transformers/blob/main/README_es.md) | [日本語](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [हिन्दी](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [Русский](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [Рortuguês](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [తెలుగు](https://github.com/huggingface/transformers//blob/main/README_te.md) |\n\n### State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n\n### \n\n🤗 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio."", ""It uses the Transformers library to set up the model and has two examples."", ""--\ntitle: \""Hugging Face and Graphcore partner for IPU-optimized Transformers\""\nthumbnail: /blog/assets/26\\_graphcore-ipu/thumbnail.png\nauthors:\n- user: sallydoherty\nguest: true\n---\n# Hugging Face and Graphcore partner for IPU-optimized Transformers\n> ##### Speaking at the 2021 AI Hardware Summit, Hugging Face announced the launch of their new Hardware Partner Program, including device-optimized models and software integrations. Here, Graphcore - creators of the Intelligence Processing Unit (IPU) and a founding member of the program – explain how their partnership with Hugging Face will allow developers to easily accelerate their use of state-of-the-art Transformer models. Graphcore and Hugging Face are two companies with a common goal – to make it easier for innovators to harness the power of machine intelligence. Hugging Face’s Hardware Partner Program will allow developers using Graphcore systems to deploy state-of-the-art Transformer models, optimised for our Intelligence Processing Unit (IPU), at production scale, with minimum coding complexity."", ""## Additional resources\n\\* Adapter Transformers [library](https://github.com/adapter-hub/adapter-transformers). \\* Adapter Transformers [docs](https://docs.adapterhub.ml/index.html). \\* Integration with Hub [docs](https://docs.adapterhub.ml/huggingface\\_hub.html)."", ""Please let us know if you have any issue or feature you would like the library to support. For all questions, the [forums](https://discuss.huggingface.co/c/accelerate) is the place to check! For more complex examples in situation, you can look at the official [Transformers examples](https://github.com/huggingface/transformers/tree/master/examples). Each folder contains a `run\\_task\\_no\\_trainer.py` that leverages the Accelerate library!""]","The open-source library created by Hugging Face to simplify Transformer acceleration is called ""Accelerate.""",Optimum
"What parameter is used to ensure that elements in a row have the same height in Gradio?
","[""The Row will wrap if there isn't sufficient space to satisfy all `min\\_width` values. Learn more about Rows in the [docs](https://gradio.app/docs/#row). ## Columns and Nesting\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n$code\\_rows\\_and\\_columns\n$demo\\_rows\\_and\\_columns\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. Learn more about Columns in the [docs](https://gradio.app/docs/#column). # Dimensions\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases. Below is an example illustrating the use of viewport width (vw):\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\nim = gr.ImageEditor(\nwidth=\""50vw\"",\n)\ndemo.launch()\n```\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n```python\nimport gradio as gr\ncss = \""\""\""\n.container {\nheight: 100vh;\n}\n\""\""\""\nwith gr.Blocks(css=css) as demo:\nwith gr.Column(elem\\_classes=[\""container\""]):\nname = gr.Chatbot(value=[[\""1\"", \""2\""]], height=\""70%\"")\ndemo.launch()\n```\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height."", ""Gradio Demo: upload\\_button\n### A simple demo showcasing the upload button used with its `upload` event trigger."", ""his demo identifies if two speakers are the same person using Gradio's Audio and HTML components."", ""`gradio/model3d`\n```html\n```\nBaseModel3D\n```javascript\nexport let value: FileData | null;\nexport let clear\\_color: [number, number, number, number] = [0, 0, 0, 0];\nexport let label = \""\"";\nexport let show\\_label: boolean;\nexport let i18n: I18nFormatter;\nexport let zoom\\_speed = 1;\n// alpha, beta, radius\nexport let camera\\_position: [number | null, number | null, number | null] = [\nnull,\nnull,\nnull\n];\n```\nBaseModel3DUpload\n```javascript\nexport let value: null | FileData;\nexport let clear\\_color: [number, number, number, number] = [0, 0, 0, 0];\nexport let label = \""\"";\nexport let show\\_label: boolean;\nexport let root: string;\nexport let i18n: I18nFormatter;\nexport let zoom\\_speed = 1;\n// alpha, beta, radius\nexport let camera\\_position: [number | null, number | null, number | null] = [\nnull,\nnull,\nnull\n];\n```\nBaseExample\n```javascript\nexport let value: string;\nexport let type: \""gallery\"" | \""table\"";\nexport let selected = false;\n```"", ""`@gradio/theme`\ncss for gradio""]","In Gradio, the `equal_height` parameter is used to ensure that elements in a row have the same height.",equal_height
"What is the command to install the latest version of Optimum with OpenVINO support?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Installation\n🤗 Optimum can be installed using `pip` as follows:\n```bash\npython -m pip install optimum\n```\nIf you'd like to use the accelerator-specific features of 🤗 Optimum, you can install the required dependencies according to the table below:\n| Accelerator | Installation |\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\n| [ONNX runtime](https://onnxruntime.ai/docs/) | `pip install --upgrade-strategy eager install optimum[onnxruntime]`|\n| [Intel Neural Compressor (INC)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) | `pip install --upgrade-strategy eager optimum[neural-compressor]` |\n| [Intel OpenVINO](https://docs.openvino.ai/latest/index.html) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |\n| [AMD Instinct GPUs and Ryzen AI NPU](https://www.amd.com/en/graphics/instinct-server-accelerators) | `pip install --upgrade-strategy eager optimum[amd]` |\n| [Habana Gaudi Processor (HPU)](https://habana.ai/training/) | `pip install --upgrade-strategy eager optimum[habana]` |\n| [FuriosaAI](https://www.furiosa.ai/) | `pip install --upgrade-strategy eager optimum[furiosa]` |\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version. If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you can install the base library from source as follows:\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\nFor the accelerator-specific features, you can install them by appending `optimum[accelerator\\_type]` to the `pip` command, e.g. ```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```"", ""You can find more examples in the 🤗 Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion-xl), and running SDXL in OpenVINO is supported for text-to-image and image-to-image."", ""Optuna Tutorial [[optuna]]\nThe content below comes from [Antonin's Raffin ICRA 2022 presentations](https://araffin.github.io/tools-for-robotic-rl-icra2022/), he's one of the founders of Stable-Baselines and RL-Baselines3-Zoo."", ""​\nQuantization is a process that lowers memory and compute requirements by reducing the bit width of model parameters. Reducing the number of bits means that the resulting model requires less memory at inference time, and that operations like matrix multiplication can be performed faster thanks to integer arithmetic. First, let's create a virtual environment and install all dependencies.​\n```bash\nvirtualenv openvino\nsource openvino/bin/activate\npip install pip --upgrade\npip install optimum[openvino,nncf] torchvision evaluate\n```\nNext, moving to a Python environment, we import the appropriate modules and download the original model as well as its processor. ​\n```python\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\n​\nmodel\\_id = \""juliensimon/autotrain-food101-1471154050\""\nmodel = AutoModelForImageClassification.from\\_pretrained(model\\_id)\nprocessor = AutoImageProcessor.from\\_pretrained(model\\_id)\n```\n​\nPost-training static quantization requires a calibration step where data is fed through the network in order to compute the quantized activation parameters. Here, we take 300 samples from the original dataset to build the calibration dataset. ​\n```python\nfrom optimum.intel.openvino import OVQuantizer\n​\nquantizer = OVQuantizer.from\\_pretrained(model)\ncalibration\\_dataset = quantizer.get\\_calibration\\_dataset(\n\""food101\"",\nnum\\_samples=300,\ndataset\\_split=\""train\"",\n)\n```\nAs usual with image datasets, we need to apply the same image transformations that were used at training time. We use the preprocessing defined in the processor. We also define a data collation function to feed the model batches of properly formatted tensors. ​\n```python\nimport torch\nfrom torchvision.transforms import (\nCenterCrop,\nCompose,\nNormalize,\nResize,\nToTensor,\n)\n​\nnormalize = Normalize(mean=processor.image\\_mean, std=processor.image\\_std)\nsize = processor.size[\""height\""]\n\\_val\\_transforms = Compose(\n[\nResize(size),\nCenterCrop(size),\nToTensor(),\nnormalize,\n]\n)\ndef val\\_transforms(example\\_batch):\nexample\\_batch[\""pixel\\_values\""] = [\\_val\\_transforms(pil\\_img.convert(\""RGB\"")) for pil\\_img in example\\_batch[\""image\""]]\nreturn example\\_batch\n​\ncalibration\\_dataset.set\\_transform(val\\_transforms)\n​\ndef collate\\_fn(examples):\npixel\\_values = torch.stack([example[\""pixel\\_values\""] for example in examples])\nlabels = torch.tensor([example[\""label\""] for example in examples])\nreturn {\""pixel\\_values\"": pixel\\_values, \""labels\"": labels}\n```\nFor our first attempt, we use the default configuration for quantization. You can also specify the number of samples to use during the calibration step, which is by default 300. ```python\nfrom optimum.intel.openvino import OVConfig\n​\nquantization\\_config = OVConfig()\nquantization\\_config.compression[\""initializer\""][\""range\""][\""num\\_init\\_samples\""] = 300\n```\nWe're now ready to quantize the model. The `OVQuantizer.quantize()` method quantizes the model and exports it to the OpenVINO format. The resulting graph is represented with two files: an XML file describing the network topology and a binary file describing the weights. The resulting model can run on any target Intel® device. ```python\nsave\\_dir = \""quantized\\_model\""\n# Apply static quantization and export the resulting quantized model to OpenVINO IR format\nquantizer.quantize(\nquantization\\_config=quantization\\_config,\ncalibration\\_dataset=calibration\\_dataset,\ndata\\_collator=collate\\_fn,\nremove\\_unused\\_columns=False,\nsave\\_directory=save\\_dir,\n)\nprocessor.save\\_pretrained(save\\_dir)\n```\nA minute or two later, the model has been quantized. We can then easily load it with our [`OVModelForXxx`](https://huggingface.co/docs/optimum/intel/inference) classes, the equivalent of the Transformers [`AutoModelForXxx`](https://huggingface.co/docs/transformers/main/en/autoclass\\_tutorial#automodel) classes found in the `transformers` library. Likewise, we can create [pipelines](https://huggingface.co/docs/transformers/main/en/main\\_classes/pipelines) and run inference with [OpenVINO Runtime](https://docs.openvino.ai/latest/openvino\\_docs\\_OV\\_UG\\_OV\\_Runtime\\_User\\_Guide.html). ​\n```python\nfrom transformers import pipeline\nfrom optimum.intel.openvino import OVModelForImageClassification\n​\nov\\_model = OVModelForImageClassification.from\\_pretrained(save\\_dir)\nov\\_pipe = pipeline(\""image-classification\"", model=ov\\_model, image\\_processor=processor)\noutputs = ov\\_pipe(\""http://farm2.staticflickr.com/1375/1394861946\\_171ea43524\\_z.jpg\"")\nprint(outputs)\n```\n​To verify that quantization did not have a negative impact on accuracy, we applied an evaluation step to compare the accuracy of the original model with its quantized counterpart."", ""There are several directions to explore to improve the accuracy of the model including longer training. The validation performance might also be improved through changing optimisers, learning rate, learning rate schedule, loss scaling, or using auto-loss scaling. ## Try Hugging Face Optimum on IPUs for free\n\nIn this post, we have introduced ViT models and have provided a tutorial for training a Hugging Face Optimum model on the IPU using a local dataset. The entire process outlined above can now be run end-to-end within minutes for free, thanks to Graphcore’s [new partnership with Paperspace](/posts/paperspace-graphcore-partner-free-ipus-developers). Launching today, the service will provide access to a selection of Hugging Face Optimum models powered by Graphcore IPUs within Gradient—Paperspace’s web-based Jupyter notebooks. [![run on Gradient](https://www.graphcore.ai/hs-fs/hubfs/gradient-badge-gradient-05-d-05.png?width=200&name=gradient-badge-gradient-05-d-05.png)](https://console.paperspace.com/github/gradient-ai/Graphcore-HuggingFace?machine=Free-IPU-POD16&container=graphcore%2Fpytorch-jupyter%3A2.6.0-ubuntu-20.04-20220804&file=%2Fget-started%2Fwalkthrough.ipynb)\n\nIf you’re interested in trying Hugging Face Optimum with IPUs on Paperspace Gradient including ViT, BERT, RoBERTa and more, you can [sign up here](https://www.paperspace.com/graphcore) and find a getting started guide [here](/posts/getting-started-with-ipus-on-paperspace). ## More Resources for Hugging Face Optimum on IPUs\n\n* [ViT Optimum tutorial code on Graphcore GitHub](https://github.com/graphcore/tutorials/tree/master/tutorials/pytorch/vit_model_training)\n* [Graphcore Hugging Face Models & Datasets](https://huggingface.co/Graphcore)\n* [Optimum Graphcore on GitHub](https://github.com/huggingface/optimum-graphcore)\n\nThis deep dive would not have been possible without extensive support, guidance, and insights from Eva Woodbridge, James Briggs, Jinchen Ge, Alexandre Payot, Thorin Farnsworth, and all others contributing from Graphcore, as well as Jeff Boudier, Julien Simon, and Michael Benayoun from Hugging Face.""]","To install the latest version of Optimum with OpenVINO support, you can use the following command:

```bash
pip install --upgrade-strategy eager optimum[openvino,nncf]
```","pip install --upgrade-strategy eager optimum[""openvino""]"

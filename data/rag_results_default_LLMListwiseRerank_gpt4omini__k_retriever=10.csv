user_input,retrieved_contexts,response,reference
"What architecture is the `tokenizers-linux-x64-musl` binary designed for?
","[""`tokenizers-linux-x64-musl`\nThis is the \\*\\*x86\\_64-unknown-linux-musl\\*\\* binary for `tokenizers`"", ""`tokenizers-android-arm-eabi`\nThis is the \\*\\*armv7-linux-androideabi\\*\\* binary for `tokenizers`"", ""`tokenizers-darwin-arm64`\nThis is the \\*\\*aarch64-apple-darwin\\*\\* binary for `tokenizers`""]",The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.,x86_64-unknown-linux-musl
"What is the purpose of the BLIP-Diffusion model?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# BLIP-Diffusion\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. The abstract from the paper is:\n\\*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).\\*\nThe original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization. `BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## BlipDiffusionPipeline\n[[autodoc]] BlipDiffusionPipeline\n- all\n- \\_\\_call\\_\\_\n## BlipDiffusionControlNetPipeline\n[[autodoc]] BlipDiffusionControlNetPipeline\n- all\n- \\_\\_call\\_\\_"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# BLIP\n## Overview\nThe BLIP model was proposed in [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. BLIP is a model that is able to perform various multi-modal tasks including:\n- Visual Question Answering\n- Image-Text retrieval (Image-text matching)\n- Image Captioning\nThe abstract from the paper is the following:\n\\*Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Text-to-image\nThe Stable Diffusion model was created by researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/), [Runway](https://github.com/runwayml), and [LAION](https://laion.ai/). The [`StableDiffusionPipeline`] is capable of generating photorealistic images given any text input. It's trained on 512x512 images from a subset of the LAION-5B dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs. Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://huggingface.co/papers/2112.10752) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. The abstract from the paper is:\n\\*By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.""]","The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation in text-to-image generation tasks. It creates novel renditions of an input subject based on text prompts while overcoming limitations such as lengthy fine-tuning and difficulties in preserving subject fidelity. BLIP-Diffusion introduces a new multimodal encoder pre-trained to provide subject representation, allowing for efficient fine-tuning and flexible combinations with existing techniques for novel subject-driven generation and editing applications.",The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.
"How can a user claim authorship of a paper on the Hugging Face Hub?
","[""Paper Pages\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png)\n\n## Linking a Paper to a model, dataset or Space\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository's tags. Clicking on the arxiv tag will let you:\n\\* Visit the Paper page. \\* Filter for other models or datasets on the Hub that cite the same paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\n\n## Claiming authorship to a Paper\nThe Hub will attempt to automatically match paper to users based on their email. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png)\n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \""claim authorship\""."", ""| Partially handled by `huggingface\\_hub`. |"", ""Thank you for your contribution! ## Code of conduct\nThis project adheres to the HuggingFace [code of conduct](CODE\\_OF\\_CONDUCT.md). By participating, you are expected to abide by this code.""]","A user can claim authorship of a paper on the Hugging Face Hub by clicking on their name in the corresponding Paper page and then selecting the ""claim authorship"" option. The Hub will also attempt to automatically match papers to users based on their email.","By clicking their name on the corresponding Paper page and clicking ""claim authorship"", then confirming the request in paper settings for admin team validation."
"What is the purpose of the /healthcheck endpoint in the Datasets server API?
","[""Datasets server API - rows endpoint\n> /rows endpoint\n## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### API service\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split"", ""Datasets server admin machine\n> Admin endpoints\n## Configuration\nThe worker can be configured using environment variables. They are grouped by scope. ### Admin service\nSet environment variables to configure the application (`ADMIN\\_` prefix):\n- `ADMIN\\_HF\\_ORGANIZATION`: the huggingface organization from which the authenticated user must be part of in order to access the protected routes, eg. \""huggingface\"". If empty, the authentication is disabled. Defaults to None. - `ADMIN\\_CACHE\\_REPORTS\\_NUM\\_RESULTS`: the number of results in /cache-reports/... endpoints. Defaults to `100`. - `ADMIN\\_CACHE\\_REPORTS\\_WITH\\_CONTENT\\_NUM\\_RESULTS`: the number of results in /cache-reports-with-content/... endpoints. Defaults to `100`. - `ADMIN\\_HF\\_TIMEOUT\\_SECONDS`: the timeout in seconds for the requests to the Hugging Face Hub. Defaults to `0.2` (200 ms). - `ADMIN\\_HF\\_WHOAMI\\_PATH`: the path of the external whoami service, on the hub (see `HF\\_ENDPOINT`), eg. \""/api/whoami-v2\"". Defaults to `/api/whoami-v2`. - `ADMIN\\_MAX\\_AGE`: number of seconds to set in the `max-age` header on technical endpoints. Defaults to `10` (10 seconds). ### Uvicorn\nThe following environment variables are used to configure the Uvicorn server (`ADMIN\\_UVICORN\\_` prefix):\n- `ADMIN\\_UVICORN\\_HOSTNAME`: the hostname. Defaults to `\""localhost\""`. - `ADMIN\\_UVICORN\\_NUM\\_WORKERS`: the number of uvicorn workers. Defaults to `2`. - `ADMIN\\_UVICORN\\_PORT`: the port. Defaults to `8000`. ### Prometheus\n- `PROMETHEUS\\_MULTIPROC\\_DIR`: the directory where the uvicorn workers share their prometheus metrics. See https://github.com/prometheus/client\\_python#multiprocess-mode-eg-gunicorn. Defaults to empty, in which case every worker manages its own metrics, and the /metrics endpoint returns the metrics of a random worker. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nThe admin service provides endpoints:\n- `/healthcheck`\n- `/metrics`: give info about the cache and the queue\n- `/cache-reports{processing\\_step}`: give detailed reports on the content of the cache for a processing step\n- `/cache-reports-with-content{processing\\_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\n- `/force-refresh{processing\\_step}`: force refresh cache entries for the processing step. It's a POST endpoint. Pass the requested parameters, depending on the processing step's input type:\n- `dataset`: `?dataset={dataset}`\n- `config`: `?dataset={dataset}&config={config}`\n- `split`: `?dataset={dataset}&config={config}&split={split}`\n- `/recreate-dataset`: deletes all the cache entries related to a specific dataset, then run all the steps in order. It's a POST endpoint."", ""Datasets server SSE API\n> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache. ## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub\\_cache` field is null for deleted entries, or when the response is an error. The `num\\_rows` value is `0` if it could not be determined. ```typescript\n{\ndataset: string;\nhub\\_cache: null | {\npreview: boolean;\nviewer: boolean;\npartial: boolean;\nnum\\_rows: int;\n};\n}\n```""]",The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the app is running.,Ensure the app is running
"What is the default context window size for Local Attention in the LongT5 model?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# LongT5\n## Overview\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention. The abstract from the paper is the following:\n\\*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.\\*\nThis model was contributed by [stancld](https://huggingface.co/stancld). The original code can be found [here](https://github.com/google-research/longt5). ## Usage tips\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder \\*self-attention\\* layer with efficient either \\*local\\* attention or \\*transient-global\\* (\\*tglobal\\*) attention. - Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`]. - LongT5 model is designed to work efficiently and very well on long-range \\*sequence-to-sequence\\* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens. - For \\*Local Attention\\*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). \\*Local Attention\\* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l\\*r)`. - \\*Transient Global Attention\\* is an extension of the \\*Local Attention\\*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\nalso every global token like in the case of standard global attention (\\*transient\\* represents the fact the global tokens\nare constructed dynamically within each attention operation). As a consequence, \\*TGlobal\\* attention introduces\na few new parameters -- global relative position biases and a layer normalization for global token's embedding."", ""With our sliding window approach, however, there is overlap in\nthe tokens we pass to the model at each iteration. We don't want the log-likelihood for the tokens we're just treating\nas context to be included in our loss, so we can set these targets to `-100` so that they are ignored. The following\nis an example of how we could do this with a stride of `512`. This means that the model will have at least 512 tokens\nfor context when calculating the conditional likelihood of any one token (provided there are 512 preceding tokens\navailable to condition on). ```python\nimport torch\nfrom tqdm import tqdm\nmax\\_length = model.config.n\\_positions\nstride = 512\nseq\\_len = encodings.input\\_ids.size(1)\nnlls = []\nprev\\_end\\_loc = 0\nfor begin\\_loc in tqdm(range(0, seq\\_len, stride)):\nend\\_loc = min(begin\\_loc + max\\_length, seq\\_len)\ntrg\\_len = end\\_loc - prev\\_end\\_loc # may be different from stride on last loop\ninput\\_ids = encodings.input\\_ids[:, begin\\_loc:end\\_loc].to(device)\ntarget\\_ids = input\\_ids.clone()\ntarget\\_ids[:, :-trg\\_len] = -100\nwith torch.no\\_grad():\noutputs = model(input\\_ids, labels=target\\_ids)\n# loss is calculated using CrossEntropyLoss which averages over valid labels\n# N.B. the model only calculates loss over trg\\_len - 1 labels, because it internally shifts the labels\n# to the left by 1. neg\\_log\\_likelihood = outputs.loss\nnlls.append(neg\\_log\\_likelihood)\nprev\\_end\\_loc = end\\_loc\nif end\\_loc == seq\\_len:\nbreak\nppl = torch.exp(torch.stack(nlls).mean())\n```\nRunning this with the stride length equal to the max input length is equivalent to the suboptimal, non-sliding-window\nstrategy we discussed above. The smaller the stride, the more context the model will have in making each prediction,\nand the better the reported perplexity will typically be. When we run the above with `stride = 1024`, i.e. no overlap, the resulting PPL is `19.44`, which is about the same\nas the `19.93` reported in the GPT-2 paper. By using `stride = 512` and thereby employing our striding window\nstrategy, this jumps down to `16.45`. This is not only a more favorable score, but is calculated in a way that is\ncloser to the true autoregressive decomposition of a sequence likelihood."", ""The first drawback of this architecture becomes obvious: Some input vectors have no access to their immediate context, \\*e.g.\\* \\\\(\\mathbf{x}\\_9\\\\) has no access to \\\\(\\mathbf{x}\\_{8}\\\\) and vice-versa in our example. This is problematic because these tokens are not able to learn word representations that take their immediate context into account. A simple remedy is to augment each chunk with `config.local\\_num\\_chunks\\_before`, \\*i.e.\\* \\\\(n\\_{p}\\\\), chunks and `config.local\\_num\\_chunks\\_after`, \\*i.e.\\* \\\\(n\\_{a}\\\\), so that every input vector has at least access to \\\\(n\\_{p}\\\\) previous input vectors and \\\\(n\\_{a}\\\\) following input vectors. This can also be understood as chunking with overlap whereas \\\\(n\\_{p}\\\\) and \\\\(n\\_{a}\\\\) define the amount of overlap each chunk has with all previous chunks and following chunks. We denote this extended local self-attention as follows:\n$$\\mathbf{Z}^{\\text{loc}} = \\left[\\mathbf{Z}\\_{1:l\\_{c}}^{\\text{loc}}, \\ldots, \\mathbf{Z}\\_{(n\\_{c} - 1) \\* l\\_{c} : n\\_{c} \\* l\\_{c}}^{\\text{loc}}\\right], $$\nwith\n$$\\mathbf{Z}\\_{l\\_{c} \\* (i - 1) + 1 : l\\_{c} \\* i}^{\\text{loc}} = \\text{SelfAttn}(\\mathbf{X}\\_{l\\_{c} \\* (i - 1 - n\\_{p}) + 1: l\\_{c} \\* (i + n\\_{a})})\\left[n\\_{p} \\* l\\_{c}: -n\\_{a} \\* l\\_{c}\\right], \\forall i \\in \\{1, \\ldots, n\\_{c} \\}$$\nOkay, this formula looks quite complicated. Let's make it easier. In Reformer's self-attention layers \\\\(n\\_{a}\\\\) is usually set to 0 and \\\\(n\\_{p}\\\\) is set to 1, so let's write down the formula again for \\\\(i = 1\\\\):\n$$\\mathbf{Z}\\_{1:l\\_{c}}^{\\text{loc}} = \\text{SelfAttn}(\\mathbf{X}\\_{-l\\_{c} + 1: l\\_{c}})\\left[l\\_{c}:\\right]$$\nWe notice that we have a circular relationship so that the first segment can attend the last segment as well. Let's illustrate this slightly enhanced local attention again. First, we apply self-attention within each windowed segment and keep only the central output segment. ![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific\\_images/master/reformer\\_benchmark/local\\_attention\\_2.png)\nFinally, the relevant output is concatenated to \\\\(\\mathbf{Z}^{\\text{loc}}\\\\) and looks as follows. ![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific\\_images/master/reformer\\_benchmark/local\\_attention\\_3.png)\nNote that local self-attention is implemented efficiently way so that no output is computed and subsequently \""thrown-out\"" as shown here for illustration purposes by the red cross. It's important to note here that extending the input vectors for each chunked self-attention function allows \\*each\\* single output vector \\\\( \\mathbf{z}\\_{i} \\\\) of this self-attention function to learn better vector representations.""]","The default context window size for Local Attention in the LongT5 model is `r=127`, which means a given token can attend to 127 tokens to the left and 127 tokens to the right of it.",127 tokens
"What method is used to load a checkpoint for a task using `AutoPipeline`?
","[""The `AutoPipeline` class is designed to simplify the variety of pipelines in 🤗 Diffusers. It is a generic, \\*task-first\\* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name. Take a look at the [AutoPipeline](../api/pipelines/auto\\_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting. This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights. ## Choose an AutoPipeline for your task\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nprompt = \""peasant and dragon combat, wood cutting style, viking era, bevel with rune\""\nimage = pipeline(prompt, num\\_inference\\_steps=25).images[0]\nimage\n```\n\n![generated image of peasant fighting dragon in wood cutting style](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png)\n\nUnder the hood, [`AutoPipelineForText2Image`]:\n1. automatically detects a `\""stable-diffusion\""` class from the [`model\\_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model\\_index.json) file\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\""stable-diffusion\""` class name\nLikewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\""stable-diffusion\""` checkpoint from the `model\\_index.json` file and it'll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image:\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\npipeline = AutoPipelineForImage2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"",\ntorch\\_dtype=torch.float16,\nuse\\_safetensors=True,\n).to(\""cuda\"")\nprompt = \""a portrait of a dog wearing a pearl earring\""\nurl = \""https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665\\_Girl\\_with\\_a\\_Pearl\\_Earring.jpg/800px-1665\\_Girl\\_with\\_a\\_Pearl\\_Earring.jpg\""\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\""RGB\"")\nimage.thumbnail((768, 768))\nimage = pipeline(prompt, image, num\\_inference\\_steps=200, strength=0.75, guidance\\_scale=10.5).images[0]\nimage\n```\n\n![generated image of a vermeer portrait of a dog wearing a pearl earring](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png)\n\nAnd if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way:\n```py\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image\nimport torch\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""stabilityai/stable-diffusion-xl-base-1.0\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nimg\\_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\_examples/overture-creations-5sI6fQgYIuo.png\""\nmask\\_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\_examples/overture-creations-5sI6fQgYIuo\\_mask.png\""\ninit\\_image = load\\_image(img\\_url).convert(\""RGB\"")\nmask\\_image = load\\_image(mask\\_url).convert(\""RGB\"")\nprompt = \""A majestic tiger sitting on a bench\""\nimage = pipeline(prompt, image=init\\_image, mask\\_image=mask\\_image, num\\_inference\\_steps=50, strength=0.80).images[0]\nimage\n```\n\n![generated image of a tiger sitting on a bench](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png)\n\nIf you try to load an unsupported checkpoint, it'll throw an error:\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\npipeline = AutoPipelineForImage2Image.from\\_pretrained(\n\""openai/shap-e-img2img\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n)\n\""ValueError: AutoPipeline can't find a pipeline linked to ShapEImg2ImgPipeline for None\""\n```\n## Use multiple pipelines\nFor some workflows or if you're loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory."", ""If you want to use one of the official checkpoints for a task, explore the [gligen](https://huggingface.co/gligen) Hub organizations! [`StableDiffusionGLIGENPipeline`] was contributed by [Nikhil Gajendrakumar](https://github.com/nikhil-masterful) and [`StableDiffusionGLIGENTextImagePipeline`] was contributed by [Nguyễn Công Tú Anh](https://github.com/tuanh123789). ## StableDiffusionGLIGENPipeline\n[[autodoc]] StableDiffusionGLIGENPipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n- enable\\_model\\_cpu\\_offload\n- prepare\\_latents\n- enable\\_fuser\n## StableDiffusionGLIGENTextImagePipeline\n[[autodoc]] StableDiffusionGLIGENTextImagePipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n- enable\\_model\\_cpu\\_offload\n- prepare\\_latents\n- enable\\_fuser\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput"", ""But you can also use regular checkpoints like [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5). Let's compare the results of the two checkpoints. The image on the left is generated from a regular checkpoint, and the image on the right is from an inpaint checkpoint. You'll immediately notice the image on the left is not as clean, and you can still see the outline of the area the model is supposed to inpaint. The image on the right is much cleaner and the inpainted area appears more natural. ```py\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image, make\\_image\\_grid\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, variant=\""fp16\""\n).to(\""cuda\"")\npipeline.enable\\_model\\_cpu\\_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable\\_xformers\\_memory\\_efficient\\_attention()\n# load base and mask image\ninit\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\"")\nmask\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint\\_mask.png\"")\ngenerator = torch.Generator(\""cuda\"").manual\\_seed(92)\nprompt = \""concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\""\nimage = pipeline(prompt=prompt, image=init\\_image, mask\\_image=mask\\_image, generator=generator).images[0]\nmake\\_image\\_grid([init\\_image, image], rows=1, cols=2)\n```\n\n```py\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image, make\\_image\\_grid\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""runwayml/stable-diffusion-inpainting\"", torch\\_dtype=torch.float16, variant=\""fp16\""\n).to(\""cuda\"")\npipeline.enable\\_model\\_cpu\\_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable\\_xformers\\_memory\\_efficient\\_attention()\n# load base and mask image\ninit\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\"")\nmask\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint\\_mask.png\"")\ngenerator = torch.Generator(\""cuda\"").manual\\_seed(92)\nprompt = \""concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\""\nimage = pipeline(prompt=prompt, image=init\\_image, mask\\_image=mask\\_image, generator=generator).images[0]\nmake\\_image\\_grid([init\\_image, image], rows=1, cols=2)\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/non-inpaint-specific.png)\n\nrunwayml/stable-diffusion-v1-5\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint-specific.png)\n\nrunwayml/stable-diffusion-inpainting\n\nHowever, for more basic tasks like erasing an object from an image (like the rocks in the road for example), a regular checkpoint yields pretty good results. There isn't as noticeable of difference between the regular and inpaint checkpoint. ```py\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image, make\\_image\\_grid\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, variant=\""fp16\""\n).to(\""cuda\"")\npipeline.enable\\_model\\_cpu\\_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable\\_xformers\\_memory\\_efficient\\_attention()\n# load base and mask image\ninit\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\"")\nmask\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/road-mask.png\"")\nimage = pipeline(prompt=\""road\"", image=init\\_image, mask\\_image=mask\\_image).images[0]\nmake\\_image\\_grid([init\\_image, image], rows=1, cols=2)\n```\n\n```py\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image, make\\_image\\_grid\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""runwayml/stable-diffusion-inpainting\"", torch\\_dtype=torch.float16, variant=\""fp16\""\n).to(\""cuda\"")\npipeline.enable\\_model\\_cpu\\_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable\\_xformers\\_memory\\_efficient\\_attention()\n# load base and mask image\ninit\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\"")\nmask\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/road-mask.png\"")\nimage = pipeline(prompt=\""road\"", image=init\\_image, mask\\_image=mask\\_image).images[0]\nmake\\_image\\_grid([init\\_image, image], rows=1, cols=2)\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/regular-inpaint-basic.png)\n\nrunwayml/stable-diffusion-v1-5\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/specific-inpaint-basic.png)\n\nrunwayml/stable-diffusion-inpainting\n\nThe trade-off of using a non-inpaint specific checkpoint is the overall image quality may be lower, but it generally tends to preserve the mask area (that is why you can see the mask outline).""]","The method used to load a checkpoint for a task using `AutoPipeline` is to call the appropriate class method for the specific task, such as `AutoPipelineForText2Image`, `AutoPipelineForImage2Image`, or `AutoPipelineForInpainting`. For example, you can use:

```python
pipeline = AutoPipelineForText2Image.from_pretrained(""checkpoint_name"")
```

This method automatically detects the correct pipeline class to use based on the checkpoint provided, making it easier to load the necessary components for the task without needing to know the specific pipeline class name.",from_pretrained()
"What is the purpose of Diffusers library?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# 🧨 Diffusers’ Ethical Guidelines\n## Preamble\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training. Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users’ contributions, and usage of the Diffusers library. The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups."", ""This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\nis very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline. ## Tweakable, contributor-friendly over abstraction\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t\\_repeat\\_yourself). In short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers. Functions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable. \\*\\*However\\*\\*, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions. - Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions. - Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel. At Hugging Face, we call this design the \\*\\*single-file policy\\*\\* which means that almost all of the code of a certain class should be written in a single, self-contained file."", ""--\ntitle: Swift 🧨Diffusers - Fast Stable Diffusion for Mac\nthumbnail: /blog/assets/fast-mac-diffusers/thumbnail.png\nauthors:\n- user: pcuenq\n- user: reach-vb\n---\n# Swift 🧨Diffusers: Fast Stable Diffusion for Mac\nTransform your text into stunning images with ease using Diffusers for Mac, a native app powered by state-of-the-art diffusion models. It leverages a bouquet of SoTA Text-to-Image models contributed by the community to the Hugging Face Hub, and converted to Core ML for blazingly fast performance. Our latest version, 1.1, is now available on the [Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) with significant performance upgrades and user-friendly interface tweaks. It's a solid foundation for future feature updates. Plus, the app is fully open source with a permissive [license](https://github.com/huggingface/swift-coreml-diffusers/blob/main/LICENSE), so you can build on it too! Check out our GitHub repository at https://github.com/huggingface/swift-coreml-diffusers for more information. ![Screenshot showing Diffusers for Mac UI](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-mac-diffusers/UI.png)\n## What exactly is 🧨Diffusers for Mac anyway? The Diffusers app ([App Store](https://apps.apple.com/app/diffusers/id1666309574), [source code](https://github.com/huggingface/swift-coreml-diffusers)) is the Mac counterpart to our [🧨`diffusers` library](https://github.com/huggingface/diffusers). This library is written in Python with PyTorch, and uses a modular design to train and run diffusion models.""]",The purpose of the Diffusers library is to provide pre-trained diffusion models and serve as a modular toolbox for inference and training. It allows users to handle the interaction between different model components and facilitates easier debugging and customization. The library is designed to support various applications of diffusion models in a user-friendly manner.,To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.
"What method does the EulerAncestralDiscreteScheduler use for sampling?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# EulerAncestralDiscreteScheduler\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling\\_euler\\_ancestral\\_discrete.EulerAncestralDiscreteSchedulerOutput"", ""Empirically, DPMSolver sampling with only 20 steps can generate high-quality\nsamples, and it can generate quite good samples even in 10 steps. The original implementation can be found at [LuChengTHU/dpm-solver](https://github.com/LuChengTHU/dpm-solver). ## Tips\nIt is recommended to set `solver\\_order` to 2 for guide sampling, and `solver\\_order=3` for unconditional sampling. Dynamic thresholding from [Imagen](https://huggingface.co/papers/2205.11487) is supported, and for pixel-space\ndiffusion models, you can set both `algorithm\\_type=\""dpmsolver++\""` and `thresholding=True` to use dynamic\nthresholding. This thresholding method is unsuitable for latent-space diffusion models such as\nStable Diffusion. ## DPMSolverSinglestepScheduler\n[[autodoc]] DPMSolverSinglestepScheduler\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling\\_utils.SchedulerOutput"", ""The schedulers in this class are distinguished at a high level by their noise sampling strategy, the type of network and scaling, the training strategy, and how the loss is weighed. The different schedulers in this class, depending on the ordinary differential equations (ODE) solver type, fall into the above taxonomy and provide a good abstraction for the design of the main schedulers implemented in 🤗 Diffusers. The schedulers in this class are given [here](https://github.com/huggingface/diffusers/blob/a69754bb879ed55b9b6dc9dd0b3cf4fa4124c765/src/diffusers/schedulers/scheduling\\_utils.py#L32). ## PushToHubMixin\n[[autodoc]] utils.PushToHubMixin""]",The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling.,Ancestral sampling with Euler method steps.
"What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Image tasks with IDEFICS\n[[open-in-colab]]\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach\nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. For instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can\nsolve image-text tasks with a large multimodal model called IDEFICS. [IDEFICS](../model\\_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198),\na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image\nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content,\ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b)\nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the 🤗 Hub."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# FLAVA\n## Overview\nThe FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022. The paper aims at creating a single unified foundation model which can work across vision, language\nas well as vision-and-language multimodal tasks. The abstract from the paper is the following:\n\\*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety\nof downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal\n(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising\ndirection would be to use a single holistic universal model, as a \""foundation\"", that targets all modalities\nat once -- a true vision and language foundation model should be good at vision tasks, language tasks, and\ncross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate\nimpressive performance on a wide range of 35 tasks spanning these target modalities.\\*\nThis model was contributed by [aps](https://huggingface.co/aps). The original code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava). ## FlavaConfig\n[[autodoc]] FlavaConfig\n## FlavaTextConfig\n[[autodoc]] FlavaTextConfig\n## FlavaImageConfig\n[[autodoc]] FlavaImageConfig\n## FlavaMultimodalConfig\n[[autodoc]] FlavaMultimodalConfig\n## FlavaImageCodebookConfig\n[[autodoc]] FlavaImageCodebookConfig\n## FlavaProcessor\n[[autodoc]] FlavaProcessor\n## FlavaFeatureExtractor\n[[autodoc]] FlavaFeatureExtractor\n## FlavaImageProcessor\n[[autodoc]] FlavaImageProcessor\n- preprocess\n## FlavaForPreTraining\n[[autodoc]] FlavaForPreTraining\n- forward\n## FlavaModel\n[[autodoc]] FlavaModel\n- forward\n- get\\_text\\_features\n- get\\_image\\_features\n## FlavaImageCodebook\n[[autodoc]] FlavaImageCodebook\n- forward\n- get\\_codebook\\_indices\n- get\\_codebook\\_probs\n## FlavaTextModel\n[[autodoc]] FlavaTextModel\n- forward\n## FlavaImageModel\n[[autodoc]] FlavaImageModel\n- forward\n## FlavaMultimodalModel\n[[autodoc]] FlavaMultimodalModel\n- forward"", ""The negative samples are usually randomly sampled from the dataset itself. The MLM and ITM objectives are often combined during the pre-training of multi-modal models. For instance, VisualBERT proposes a BERT-like architecture that uses a pre-trained object detection model, [Faster-RCNN](https://arxiv.org/abs/1506.01497), to detect objects. This model uses a combination of the MLM and ITM objectives during pre-training to implicitly align elements of an input text and regions in an associated input image with self-attention. Another work, FLAVA, consists of an image encoder, a text encoder, and a multi-modal encoder to fuse and align the image and text representations for multi-modal reasoning, all of which are based on transformers. In order to achieve this, FLAVA uses a variety of pre-training objectives: MLM, ITM, as well as Masked-Image Modeling (MIM), and contrastive learning. ### 5) No Training\nFinally, various optimization strategies aim to bridge image and text representations using the pre-trained image and text models or adapt pre-trained multi-modal models to new downstream tasks without additional training. For example, [MaGiC](https://arxiv.org/abs/2205.02655) proposes iterative optimization through a pre-trained autoregressive language model to generate a caption for the input image. To do this, MaGiC computes a CLIP-based “Magic score” using CLIP embeddings of the generated tokens and the input image. ![ASIF](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/asif.png)  \n*Crafting a similarity search space using pre-trained, frozen unimodal image and text encoders ([image source](https://luca.moschella.dev/publication/norelli-asif-2022))*\n\n[ASIF](https://arxiv.org/abs/2210.01738) proposes a simple method to turn pre-trained uni-modal image and text models into a multi-modal model for image captioning using a relatively small multi-modal dataset without additional training. The key intuition behind ASIF is that captions of similar images are also similar to each other.""]",The large multimodal model that can solve image-text tasks and is based on Flamingo is called IDEFICS.,IDEFICS
"What is the purpose of the `gradio.Blocks` API?
","[""Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our \\*\\*newly developed\\*\\* low-level API called `gradio.Blocks`. Now, what's the difference between `Interface` and `Blocks`? - ⚡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs. - 🧱 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \""building blocks\""). ### Why Blocks 🧱?[[why-blocks-]]\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\n- Group together related demos as multiple tabs in one web application\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a dropdown) or its visibility based on user input\nWe will explore all of these concepts below. ### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook. ```py\nimport gradio as gr\ndef flip\\_text(x):\nreturn x[::-1]\ndemo = gr.Blocks()\nwith demo:\ngr.Markdown(\n\""\""\""\n# Flip Text! Start typing below to see the output. \""\""\""\n)\ninput = gr.Textbox(placeholder=\""Flip this text\"")\noutput = gr.Textbox()\ninput.change(fn=flip\\_text, inputs=input, outputs=output)\ndemo.launch()\n```\nThis simple example above introduces 4 concepts that underlie Blocks:\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context."", ""### An Overview of Gradio\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do? #### Chatbots with `gr.ChatInterface`\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast). #### Custom Demos with `gr.Blocks`\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction — still all in Python. You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners). #### The Gradio Python & JavaScript Ecosystem\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\\* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio\\_client`): query any Gradio app programmatically in Python. \\* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript. \\* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \\* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications — for free!"", ""Gradio Blocks Party[[gradio-blocks-party]]\nAlong with the release of the Gradio chapter of the course, Hugging Face hosted a community event on building cool machine learning demos using the new Gradio Blocks feature. You can find all the demos that the community created under the [`Gradio-Blocks`](https://huggingface.co/Gradio-Blocks) organisation on the Hub. Here's a few examples from the winners:\n\\*\\*Natural language to SQL\\*\\*""]","The `gradio.Blocks` API is a low-level API that allows you to have full control over the data flows and layout of your application. It provides flexibility to build complex, multi-step applications by enabling you to:

- Group related demos as multiple tabs in one web application.
- Change the layout of your demo, specifying where inputs and outputs are located.
- Create multi-step interfaces where the output of one model becomes the input to the next model.
- Modify a component's properties or visibility based on user input.

Overall, `gradio.Blocks` is designed for users who need more customization and control compared to the high-level `Interface` class.","The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications."
"What is the purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents""?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# unCLIP\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in 🤗 Diffusers comes from kakaobrain's [karlo](https://github.com/kakaobrain/karlo). The abstract from the paper is following:\n\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\*\nYou can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines."", ""!--Copyright 2023 The GLIGEN Authors and The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# GLIGEN (Grounded Language-to-Image Generation)\nThe GLIGEN model was created by researchers and engineers from [University of Wisconsin-Madison, Columbia University, and Microsoft](https://github.com/gligen/GLIGEN). The [`StableDiffusionGLIGENPipeline`] and [`StableDiffusionGLIGENTextImagePipeline`] can generate photorealistic images conditioned on grounding inputs. Along with text and bounding boxes with [`StableDiffusionGLIGENPipeline`], if input images are given, [`StableDiffusionGLIGENTextImagePipeline`] can insert objects described by text at the region defined by bounding boxes. Otherwise, it'll generate an image described by the caption/prompt and insert objects described by text at the region defined by bounding boxes. It's trained on COCO2014D and COCO2014CD datasets, and the model uses a frozen CLIP ViT-L/14 text encoder to condition itself on grounding inputs. The abstract from the [paper](https://huggingface.co/papers/2301.07093) is:\n\\*Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN’s zeroshot performance on COCO and LVIS outperforms existing supervised layout-to-image baselines by a large margin.\\*\nMake sure to check out the Stable Diffusion [Tips](https://huggingface.co/docs/diffusers/en/api/pipelines/stable\\_diffusion/overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality and how to reuse pipeline components efficiently!"", ""While CLIPSeg is trained on simple object descriptions (e.g., “a car”), its CLIP backbone is pre-trained on engineered text templates (e.g., “an image of a car”, “a photo of a car”) and kept frozen during training. Once the inputs are preprocessed, we can perform inference to get a binary segmentation map of shape (height, width) for each text query. ```py\nimport torch\nwith torch.no\\_grad():\noutputs = model(\\*\\*inputs)\nlogits = outputs.logits\nprint(logits.shape)\n>>> torch.Size([3, 352, 352])\n```\nLet’s visualize the results to see how well CLIPSeg performed (code is adapted from [this post](https://huggingface.co/blog/clipseg-zero-shot)). ```py\nimport matplotlib.pyplot as plt\nlogits = logits.unsqueeze(1)\n\\_, ax = plt.subplots(1, len(texts) + 1, figsize=(3\\*(len(texts) + 1), 12))\n[a.axis('off') for a in ax.flatten()]\nax[0].imshow(image)\n[ax[i+1].imshow(torch.sigmoid(logits[i][0])) for i in range(len(texts))];\n[ax[i+1].text(0, -15, prompt) for i, prompt in enumerate(texts)]\n```\n\n![CLIPSeg results](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/clipseg_result.png)\n\nAmazing, isn’t it?""]","The purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents"" is to leverage robust representations of images learned by contrastive models like CLIP for image generation. The model consists of a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. This approach aims to improve image diversity with minimal loss in photorealism and caption similarity, allowing for variations of an image that preserve its semantics and style while varying non-essential details. Additionally, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion.",The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.
"What command is used to install the requirements for a research project using 🤗 Transformers?
","[""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Research projects\nThis folder contains various research projects using 🤗 Transformers. They are not maintained and require a specific\nversion of 🤗 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice. If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder."", ""ow to add BigBird to 🤗 Transformers? =====================================\nMentor: [Patrick](https://github.com/patrickvonplaten)\nBegin: 12.02.2020\nEstimated End: 19.03.2020\nContributor: [Vasudev](https://github.com/thevasudevgupta)\nAdding a new model is often difficult and requires an in-depth knowledge\nof the 🤗 Transformers library and ideally also of the model's original\nrepository. At Hugging Face, we are trying to empower the community more\nand more to add models independently. The following sections explain in detail how to add BigBird\nto Transformers. You will work closely with Patrick to\nintegrate BigBird into Transformers. By doing so, you will both gain a\ntheoretical and deep practical understanding of BigBird. But more importantly, you will have made a major\nopen-source contribution to Transformers. Along the way, you will:\n- get insights into open-source best practices\n- understand the design principles of one of the most popular NLP\nlibraries\n- learn how to do efficiently test large NLP models\n- learn how to integrate Python utilities like `black`, `ruff`,\n`make fix-copies` into a library to always ensure clean and readable\ncode\nTo start, let's try to get a general overview of the Transformers\nlibrary."", ""\\* [TensorFlow 2.0](https://www.tensorflow.org/install/pip) installation instructions. \\* [Flax](https://flax.readthedocs.io/en/latest/) installation instructions. ## Install with pip\nYou should install 🤗 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies. Start by creating a virtual environment in your project directory:\n```bash\npython -m venv .env\n```\nActivate the virtual environment. On Linux and MacOs:\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n```bash\n.env/Scripts/activate\n```\nNow you're ready to install 🤗 Transformers with the following command:\n```bash\npip install transformers\n```\nFor CPU-support only, you can conveniently install 🤗 Transformers and a deep learning library in one line. For example, install 🤗 Transformers and PyTorch with:\n```bash\npip install 'transformers[torch]'\n```\n🤗 Transformers and TensorFlow 2.0:\n```bash\npip install 'transformers[tf-cpu]'\n```\nM1 / ARM Users\nYou will need to install the following before installing TensorFLow 2.0\n```\nbrew install cmake\nbrew install pkg-config\n```\n🤗 Transformers and Flax:\n```bash\npip install 'transformers[flax]'\n```\nFinally, check if 🤗 Transformers has been properly installed by running the following command. It will download a pretrained model:\n```bash\npython -c \""from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\""\n```\nThen print out the label and score:\n```bash\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```\n## Install from source\nInstall 🤗 Transformers from source with the following command:\n```bash\npip install git+https://github.com/huggingface/transformers\n```\nThis command installs the bleeding edge `main` version rather than the latest `stable` version.""]","The command used to install the requirements for a research project using 🤗 Transformers is:

```
pip install -r requirements.txt
``` 

You should run this command inside the folder of your choice.",pip install -r requirements.txt
"What task does the `roberta-large-mnli` checkpoint perform?
","[""!-- DISABLE-FRONTMATTER-SECTIONS -->\n# End-of-chapter quiz[[end-of-chapter-quiz]]\nThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood. First, though, let's test what you learned in this chapter! ### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform? roberta-large-mnli page.\""\n},\n{\ntext: \""Text classification\"",\nexplain: \""More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) — a task also called *natural language inference*.\"",\ncorrect: true\n},\n{\ntext: \""Text generation\"",\nexplain: \""Look again on the [roberta-large-mnli page](\\\""https://huggingface.co/roberta-large-mnli\\\"").\""\n}\n]}\n/>\n### 2. What will the following code return? ```py\nfrom transformers import pipeline\nner = pipeline(\""ner\"", grouped\\_entities=True)\nner(\""My name is Sylvain and I work at Hugging Face in Brooklyn.\"")\n```\nsentiment-analysis pipeline.\""\n},\n{\ntext: \""It will return a generated text completing this sentence.\"",\nexplain: \""This is incorrect — it would be a `text-generation` pipeline.\"",\n},\n{\ntext: \""It will return the words representing persons, organizations or locations.\"",\nexplain: \""Furthermore, with `grouped_entities=True`, it will group together the words belonging to the same entity, like \\\""Hugging Face\\\"".\"",\ncorrect: true\n}\n]}\n/>\n### 3."", ""- [Question answering task guide](../tasks/question\\_answering)\n\\*\\*Multiple choice\\*\\*\n- [`RobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice.ipynb). - [`TFRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice-tf.ipynb). - [Multiple choice task guide](../tasks/multiple\\_choice)\n## RobertaConfig\n[[autodoc]] RobertaConfig\n## RobertaTokenizer\n[[autodoc]] RobertaTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## RobertaTokenizerFast\n[[autodoc]] RobertaTokenizerFast\n- build\\_inputs\\_with\\_special\\_tokens\n\n## RobertaModel\n[[autodoc]] RobertaModel\n- forward\n## RobertaForCausalLM\n[[autodoc]] RobertaForCausalLM\n- forward\n## RobertaForMaskedLM\n[[autodoc]] RobertaForMaskedLM\n- forward\n## RobertaForSequenceClassification\n[[autodoc]] RobertaForSequenceClassification\n- forward\n## RobertaForMultipleChoice\n[[autodoc]] RobertaForMultipleChoice\n- forward\n## RobertaForTokenClassification\n[[autodoc]] RobertaForTokenClassification\n- forward\n## RobertaForQuestionAnswering\n[[autodoc]] RobertaForQuestionAnswering\n- forward\n\n## TFRobertaModel\n[[autodoc]] TFRobertaModel\n- call\n## TFRobertaForCausalLM\n[[autodoc]] TFRobertaForCausalLM\n- call\n## TFRobertaForMaskedLM\n[[autodoc]] TFRobertaForMaskedLM\n- call\n## TFRobertaForSequenceClassification\n[[autodoc]] TFRobertaForSequenceClassification\n- call\n## TFRobertaForMultipleChoice\n[[autodoc]] TFRobertaForMultipleChoice\n- call\n## TFRobertaForTokenClassification\n[[autodoc]] TFRobertaForTokenClassification\n- call\n## TFRobertaForQuestionAnswering\n[[autodoc]] TFRobertaForQuestionAnswering\n- call\n\n## FlaxRobertaModel\n[[autodoc]] FlaxRobertaModel\n- \\_\\_call\\_\\_\n## FlaxRobertaForCausalLM\n[[autodoc]] FlaxRobertaForCausalLM\n- \\_\\_call\\_\\_\n## FlaxRobertaForMaskedLM\n[[autodoc]] FlaxRobertaForMaskedLM\n- \\_\\_call\\_\\_\n## FlaxRobertaForSequenceClassification\n[[autodoc]] FlaxRobertaForSequenceClassification\n- \\_\\_call\\_\\_\n## FlaxRobertaForMultipleChoice\n[[autodoc]] FlaxRobertaForMultipleChoice\n- \\_\\_call\\_\\_\n## FlaxRobertaForTokenClassification\n[[autodoc]] FlaxRobertaForTokenClassification\n- \\_\\_call\\_\\_\n## FlaxRobertaForQuestionAnswering\n[[autodoc]] FlaxRobertaForQuestionAnswering\n- \\_\\_call\\_\\_"", ""- [LyCORIS checkpoints](https://github.com/KohakuBlueleaf/LyCORIS) aren't fully supported. The [`~loaders.LoraLoaderMixin.load\\_lora\\_weights`] method loads LyCORIS checkpoints with LoRA and LoCon modules, but Hada and LoKR are not supported. Loading a checkpoint from TheLastBen is very similar. For example, to load the [TheLastBen/William\\_Eggleston\\_Style\\_SDXL](https://huggingface.co/TheLastBen/William\\_Eggleston\\_Style\\_SDXL) checkpoint:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\""stabilityai/stable-diffusion-xl-base-1.0\"", torch\\_dtype=torch.float16).to(\""cuda\"")\npipeline.load\\_lora\\_weights(\""TheLastBen/William\\_Eggleston\\_Style\\_SDXL\"", weight\\_name=\""wegg.safetensors\"")\n# use by william eggleston in the prompt to trigger the LoRA\nprompt = \""a house by william eggleston, sunrays, beautiful, sunlight, sunrays, beautiful\""\nimage = pipeline(prompt=prompt).images[0]\nimage\n```\n## IP-Adapter\n[IP-Adapter](https://ip-adapter.github.io/) is an effective and lightweight adapter that adds image prompting capabilities to a diffusion model.""]","The `roberta-large-mnli` checkpoint performs text classification, specifically classifying if two sentences are logically linked across three labels: contradiction, neutral, and entailment. This task is also known as natural language inference.",Text classification
"What service is replacing the Paid tier of the Inference API at Hugging Face?
","[""--\ntitle: Introducing our new pricing\nthumbnail: /blog/assets/114\\_pricing-update/thumbnail.png\nauthors:\n- user: sbrandeis\n- user: pierric\n---\n# Introducing our new pricing\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently. First of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints). Along with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations). Your billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records. \\*\\*TL;DR\\*\\*: \\*\\*At HF we monetize by providing simple access to compute for AI\\*\\*, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub. [Read more](https://huggingface.co/docs/hub/billing) about our pricing and billing system. If you have any questions, feel free to reach out. We welcome your feedback 🔥"", ""Access 🤗 Inference Endpoints\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account. You can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method. There are two pricing plans:\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. - There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested! After you've added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying!"", ""Benefit from higher rate limits, custom accelerated endpoints for the latest models, and early access to features. If you've built some exciting projects with the Inference API or are looking for a model not available in Inference for PROs, please [use this discussion](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/13). [Enterprise users](https://huggingface.co/enterprise) also benefit from PRO Inference API on top of other features, such as SSO. ## FAQ\n\n**Does this affect the free Inference API?**\n\nNo. We still expose thousands of models through free APIs that allow people to prototype and explore model capabilities quickly. **Does this affect Enterprise users?**\n\nUsers with an Enterprise subscription also benefit from accelerated inference API for curated models. **Can I use my own models with PRO Inference API?**\n\nThe free Inference API already supports a wide range of small and medium models from a variety of libraries (such as diffusers, transformers, and sentence transformers). If you have a custom model or custom inference logic, we recommend using [Inference Endpoints](https://ui.endpoints.huggingface.co/catalog).""]",The Paid tier of the Inference API service is being replaced by the new solution called **Inference Endpoints**.,Inference Endpoints
"What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# SqueezeBERT\n## Overview\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers."", ""In the original model it is possible to use an average pooling layer with a fixed 7x7 window and stride 1 instead of global pooling. For inputs that are larger than the recommended image size, this gives a pooled output that is larger than 1x1. The Hugging Face implementation does not support this. - The original TensorFlow checkpoints include quantized models. We do not support these models as they include additional \""FakeQuantization\"" operations to unquantize the weights. - It's common to extract the output from the expansion layers at indices 10 and 13, as well as the output from the final 1x1 convolution layer, for downstream purposes. Using `output\\_hidden\\_states=True` returns the output from all intermediate layers. There is currently no way to limit this to specific layers. - The DeepLabV3+ segmentation head does not use the final convolution layer from the backbone, but this layer gets computed anyway. There is currently no way to tell [`MobileNetV2Model`] up to which layer it should run."", ""EfficientNet (Knapsack Pruned)\n\\*\\*EfficientNet\\*\\* is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a \\*compound coefficient\\*. Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use \\\\( 2^N \\\\) times more computational resources, then we can simply increase the network depth by \\\\( \\alpha ^ N \\\\), width by \\\\( \\beta ^ N \\\\), and image size by \\\\( \\gamma ^ N \\\\), where \\\\( \\alpha, \\beta, \\gamma \\\\) are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\\\( \\phi \\\\) to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of [MobileNetV2](https://paperswithcode.com/method/mobilenetv2), in addition to [squeeze-and-excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block).""]","SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers.",Grouped convolutions
"What type of license is the HuggingFace Team's software distributed under?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction"", ""The ability to use Hugging Face’s comprehensive hub of foundation models means that users can pick the models that best align with their business needs without having to invest in the resources required to train them. This integration is a significant step forward in making foundation models more accessible to enterprises around the world. \\_If you’re interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!\\_"", ""Security\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)""]","The HuggingFace Team's software is distributed under the Apache License, Version 2.0.","Apache License, Version 2.0"
"What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?
","[""It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\n\\*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\*\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A."", ""- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the 🤗 Hugging Face Course. - Check the [Question answering task guide](../tasks/question\\_answering) on how to use the model. \\*\\*Multiple choice\\*\\*\n- [`AlbertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice.ipynb). - [`TFAlbertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice-tf.ipynb). - Check the [Multiple choice task guide](../tasks/multiple\\_choice) on how to use the model. ## AlbertConfig\n[[autodoc]] AlbertConfig\n## AlbertTokenizer\n[[autodoc]] AlbertTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## AlbertTokenizerFast\n[[autodoc]] AlbertTokenizerFast\n## Albert specific outputs\n[[autodoc]] models.albert.modeling\\_albert.AlbertForPreTrainingOutput\n[[autodoc]] models.albert.modeling\\_tf\\_albert.TFAlbertForPreTrainingOutput\n\n## AlbertModel\n[[autodoc]] AlbertModel\n- forward\n## AlbertForPreTraining\n[[autodoc]] AlbertForPreTraining\n- forward\n## AlbertForMaskedLM\n[[autodoc]] AlbertForMaskedLM\n- forward\n## AlbertForSequenceClassification\n[[autodoc]] AlbertForSequenceClassification\n- forward\n## AlbertForMultipleChoice\n[[autodoc]] AlbertForMultipleChoice\n## AlbertForTokenClassification\n[[autodoc]] AlbertForTokenClassification\n- forward\n## AlbertForQuestionAnswering\n[[autodoc]] AlbertForQuestionAnswering\n- forward\n\n## TFAlbertModel\n[[autodoc]] TFAlbertModel\n- call\n## TFAlbertForPreTraining\n[[autodoc]] TFAlbertForPreTraining\n- call\n## TFAlbertForMaskedLM\n[[autodoc]] TFAlbertForMaskedLM\n- call\n## TFAlbertForSequenceClassification\n[[autodoc]] TFAlbertForSequenceClassification\n- call\n## TFAlbertForMultipleChoice\n[[autodoc]] TFAlbertForMultipleChoice\n- call\n## TFAlbertForTokenClassification\n[[autodoc]] TFAlbertForTokenClassification\n- call\n## TFAlbertForQuestionAnswering\n[[autodoc]] TFAlbertForQuestionAnswering\n- call\n\n## FlaxAlbertModel\n[[autodoc]] FlaxAlbertModel\n- \\_\\_call\\_\\_\n## FlaxAlbertForPreTraining\n[[autodoc]] FlaxAlbertForPreTraining\n- \\_\\_call\\_\\_\n## FlaxAlbertForMaskedLM\n[[autodoc]] FlaxAlbertForMaskedLM\n- \\_\\_call\\_\\_\n## FlaxAlbertForSequenceClassification\n[[autodoc]] FlaxAlbertForSequenceClassification\n- \\_\\_call\\_\\_\n## FlaxAlbertForMultipleChoice\n[[autodoc]] FlaxAlbertForMultipleChoice\n- \\_\\_call\\_\\_\n## FlaxAlbertForTokenClassification\n[[autodoc]] FlaxAlbertForTokenClassification\n- \\_\\_call\\_\\_\n## FlaxAlbertForQuestionAnswering\n[[autodoc]] FlaxAlbertForQuestionAnswering\n- \\_\\_call\\_\\_"", ""It means that there is less data to exchange between the peers, which is crucial in our setup, as it significantly speeds up each training iteration. 💡 Want to know more about ALBERT? [Paper](https://arxiv.org/abs/1909.11942)  \n[Transformers doc](https://huggingface.co/transformers/model_doc/albert.html#albert)\n\n### Tokenizer\nThe first brick of our model is called a \\_tokenizer\\_ and takes care of transforming raw text into vocabulary indices. Because we are training a model for Bengali, which is not very similar to English, we need to implement language-specific preprocessing as a part of our tokenizer. We can view it as a sequence of operations:\n1. \\*\\*Normalization:\\*\\* includes all preprocessing operations on raw text data. This was the step at which we have made the most changes, because removing certain details can either change the meaning of the text or leave it the same, depending on the language. For example, the standard ALBERT normalizer removes the accents, while for the Bengali language, we need to keep them, because they contain information about the vowels. As a result, we use the following operations: NMT normalization, NFKC normalization, removal of multiple spaces, homogenization of recurring Unicode characters in the Bengali language, and lowercasing.""]","The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:

1. Splitting the embedding matrix into two smaller matrices.
2. Using repeating layers split among groups.",Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.
"What are the three main steps for fine-tuning a model with the 🤗 Datasets library?
","[""Introduction[[introduction]]\nIn [Chapter 3](/course/chapter3) you got your first taste of the 🤗 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics. But this is just scratching the surface of what 🤗 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\\* What do you do when your dataset is not on the Hub? \\* How can you slice and dice a dataset? (And what if you \\_really\\_ need to use Pandas?)\n\\* What do you do when your dataset is huge and will melt your laptop's RAM? \\* What the heck are \""memory mapping\"" and Apache Arrow? \\* How can you create your own dataset and push it to the Hub? The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!"", ""To verify that all libraries are correctly installed, you can run the following command in a Python shell. It verifies that both `transformers` and `datasets` have been correclty installed. ```python\nfrom transformers import AutoModelForCTC, AutoProcessor\nfrom datasets import load\\_dataset\ndummy\\_dataset = load\\_dataset(\""common\\_voice\"", \""ab\"", split=\""test\"")\nmodel = AutoModelForCTC.from\\_pretrained(\""hf-internal-testing/tiny-random-wav2vec2\"")\nmodel.to(\""cuda\"")\nprocessor = AutoProcessor.from\\_pretrained(\""hf-internal-testing/tiny-random-wav2vec2\"")\ninput\\_values = processor(dummy\\_dataset[0][\""audio\""][\""array\""], return\\_tensors=\""pt\"", sampling\\_rate=16\\_000).input\\_values\ninput\\_values = input\\_values.to(\""cuda\"")\nlogits = model(input\\_values).logits\nassert logits.shape[-1] == 32\n```\n## How to finetune an acoustic model\nIn this section, we show you how to fine-tune a pre-trained [XLS-R Model](https://huggingface.co/docs/transformers/model\\_doc/xls\\_r) on the [Common Voice 7 dataset](https://huggingface.co/datasets/mozilla-foundation/common\\_voice\\_7\\_0)."", ""Considerations for model evaluation\nDeveloping an ML model is rarely a one-shot deal: it often involves multiple stages of defining the model architecture and tuning hyper-parameters before converging on a final set. Responsible model evaluation is a key part of this process, and 🤗 Evaluate is here to help! Here are some things to keep in mind when evaluating your model using the 🤗 Evaluate library:\n## Properly splitting your data\nGood evaluation generally requires three splits of your dataset:\n- \\*\\*train\\*\\*: this is used for training your model. - \\*\\*validation\\*\\*: this is used for validating the model hyperparameters. - \\*\\*test\\*\\*: this is used for evaluating your model. Many of the datasets on the 🤗 Hub are separated into 2 splits: `train` and `validation`; others are split into 3 splits (`train`, `validation` and `test`) -- make sure to use the right split for the right purpose! Some datasets on the 🤗 Hub are already separated into these three splits. However, there are also many that only have a train/validation or only train split. If the dataset you're using doesn't have a predefined train-test split, it is up to you to define which part of the dataset you want to use for training your model and which you want to use for hyperparameter tuning or final evaluation. Training and evaluating on the same split can misrepresent your results! If you overfit on your training data the evaluation results on that split will look great but the model will perform poorly on new data. Depending on the size of the dataset, you can keep anywhere from 10-30% for evaluation and the rest for training, while aiming to set up the test set to reflect the production data as close as possible. Check out [this thread](https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090) for a more in-depth discussion of dataset splitting! ## The impact of class imbalance\nWhile many academic datasets, such as the [IMDb dataset](https://huggingface.co/datasets/imdb) of movie reviews, are perfectly balanced, most real-world datasets are not. In machine learning a \\*balanced dataset\\* corresponds to a datasets where all labels are represented equally. In the case of the IMDb dataset this means that there are as many positive as negative reviews in the dataset. In an imbalanced dataset this is not the case: in fraud detection for example there are usually many more non-fraud cases than fraud cases in the dataset. Having an imbalanced dataset can skew the results of your metrics. Imagine a dataset with 99 \""non-fraud\"" cases and 1 \""fraud\"" case. A simple model that always predicts \""non-fraud\"" cases would give yield a 99% accuracy which might sound good at first until you realize that you will never catch a fraud case.""]","The three main steps for fine-tuning a model with the 🤗 Datasets library are:

1. Load a dataset from the Hugging Face Hub.
2. Preprocess the data with `Dataset.map()`.
3. Load and compute metrics.",1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.
"What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?
","[""It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark. Figure 4. Latency (Batch=1, Physical Cores=2)\n\n---\n## Conclusion\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput. Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens. The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial)\n## Resources\n\\* [Hugging Face Infinity](https://huggingface.co/infinity)\n\\* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\n\\* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i)\n\\* [DistilBERT](https://huggingface.co/docs/transformers/model\\_doc/distilbert)\n\\* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\\* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\n\\* [🤗 Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)"", ""--\ntitle: \""Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\""\nthumbnail: /blog/assets/80\\_intel/01.png\nauthors:\n- user: juliensimon\n---\n# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\n![image](assets/80\\_intel/01.png)\nThe mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption. Today, we're excited to announce that Intel has officially joined our [Hardware Partner Program](https://huggingface.co/hardware). Thanks to the [Optimum](https://github.com/huggingface/optimum-intel) open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers. Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage. Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms. “\\*We’re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.\\*”, says Wei Li, Intel Vice President & General Manager, AI and Analytics. In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs. The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Let’s take a deeper look at it. ## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices."", ""--\ntitle: \""Optimum+ONNX Runtime - Easier, Faster training for your Hugging Face models\""\nthumbnail: /blog/assets/optimum\\_onnxruntime-training/thumbnail.png\nauthors:\n- user: Jingya\n- user: kshama-msft\nguest: true\n- user: askhade\nguest: true\n- user: weicwang\nguest: true\n- user: zhijiang\nguest: true\n---\n# Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models\n## Introduction\nTransformer based models in language, vision and speech are getting larger to support complex multi-modal use cases for the end customer. Increasing model sizes directly impact the resources needed to train these models and scale them as the size increases. Hugging Face and Microsoft’s ONNX Runtime teams are working together to build advancements in finetuning large Language, Speech and Vision models. Hugging Face’s [Optimum library](https://huggingface.co/docs/optimum/index), through its integration with ONNX Runtime for training, provides an open solution to \\_\\_improve training times by 35% or more\\_\\_ for many popular Hugging Face models. We present details of both Hugging Face Optimum and the ONNX Runtime Training ecosystem, with performance numbers highlighting the benefits of using the Optimum library. ## Performance results\nThe chart below shows impressive acceleration \\_\\_from 39% to 130%\\_\\_ for Hugging Face models with Optimum when \\_\\_using ONNX Runtime and DeepSpeed ZeRO Stage 1\\_\\_ for training. The performance measurements were done on selected Hugging Face models with PyTorch as the baseline run, only ONNX Runtime for training as the second run, and ONNX Runtime + DeepSpeed ZeRO Stage 1 as the final run, showing maximum gains.""]",Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.,+800%
"What is the command to upload a spaCy pipeline to the Hugging Face Hub?
","[""Defaults to `hub` in the current working directory. |\n| `--verbose`, `-V` | bool | Output additional info for debugging, e.g. the full generated hub metadata. |\nYou can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`. ```bash\nhuggingface-cli login\npython -m spacy package ./en\\_ner\\_fashion ./output --build wheel\ncd ./output/en\\_ner\\_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en\\_ner\\_fashion-0.0.0-py3-none-any.whl\n```\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card. The command will output two things:\n\\* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en\\_core\\_web\\_sm\n\\* And how to install the pipeline directly from the Hub! ### From a Python script\nYou can use the `push` function from Python. It returns a dictionary containing the `\""url\""` and \""`whl\\_url`\"" of the published model and the wheel file, which you can later install with `pip install`. ```py\nfrom spacy\\_huggingface\\_hub import push\nresult = push(\""./en\\_ner\\_fashion-0.0.0-py3-none-any.whl\"")\nprint(result[\""url\""])\n```\n## Additional resources\n\\* spacy-huggingface-hub [library](https://github.com/explosion/spacy-huggingface-hub)."", ""\\* Launch [blog post](https://huggingface.co/blog/spacy)\n\\* spaCy v 3.1 [Announcement](https://explosion.ai/blog/spacy-v3-1#huggingface-hub)\n\\* spaCy [documentation](https://spacy.io/universe/project/spacy-huggingface-hub/)"", ""The Model Hub\n## What is the Model Hub? The Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing. Download pre-trained models with the [`huggingface\\_hub` client library](https://huggingface.co/docs/huggingface\\_hub/index), with 🤗 [`Transformers`](https://huggingface.co/docs/transformers/index) for fine-tuning and other usages or with any of the over [15 integrated libraries](./models-libraries). You can even leverage the [Inference API](./models-inference) to use models in production settings. You can refer to the following video for a guide on navigating the Model Hub:\nTo learn how to upload models to the Hub, you can refer to the [Repositories Getting Started Guide](./repositories-getting-started).""]","To upload a spaCy pipeline to the Hugging Face Hub, you can use the following command:

```bash
python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
```

Make sure to replace `en_ner_fashion-0.0.0-py3-none-any.whl` with the name of your specific pipeline package.",python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
"What is the time and memory complexity of the Nyströmformer's approximation of self-attention?
","[""--\ntitle: \""Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\""\nthumbnail: /blog/assets/86\\_nystromformer/thumbnail.png\nauthors:\n- user: asi\nguest: true\n---\n# Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\n## Introduction\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. The [Nyströmformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nyströmformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nyström method and how it can be adapted to approximate self-attention. ## Nyström method for matrix approximation\nAt the heart of Nyströmformer is the Nyström method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nyström method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:\n\nRepresenting P as a block matrix\n\nWe now have four submatrices: \\\\(A\\_P, B\\_P, F\\_P,\\\\) and \\\\(C\\_P\\\\), with sizes \\\\(m \\times m, m \\times (n - m), (n - m) \\times m\\\\) and\n\\\\((n - m) \\times (n - m)\\\\) respectively. The \\\\(m\\\\) sampled columns are contained in \\\\(A\\_P\\\\) and \\\\(F\\_P\\\\), whereas the \\\\(m\\\\) sampled rows are contained in \\\\(A\\_P\\\\) and \\\\(B\\_P\\\\). So, the entries of \\\\(A\\_P, B\\_P,\\\\) and \\\\(F\\_P\\\\) are known to us, and we will estimate \\\\(C\\_P\\\\). According to the Nyström method, \\\\(C\\_P\\\\) is given by:\n$$C\\_P = F\\_P A\\_P^+ B\\_P$$\nHere, \\\\(+\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). Thus, the Nyström approximation of \\\\(P, \\hat{P}\\\\) can be written as:\n\nNyström approximation of P\n\nAs shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices."", ""Readers interested in deploying or fine-tuning Nyströmformer for downstream tasks can find the HuggingFace documentation [here](https://huggingface.co/docs/transformers/model\\_doc/nystromformer)."", ""Only at longer sequence lengths of 4096, a slight decrease in memory usage can be seen. Let's see what happens to the memory peak usage if we increase the size of the feed forward layer by a factor of 4 and reduce the number of attention heads also by a factor of 4 so that the feed forward layer becomes the memory bottleneck. ```\nconfig\\_no\\_chunk = ReformerConfig.from\\_pretrained(\""google/reformer-enwik8\"", chunk\\_size\\_feed\\_forward=0, num\\_attention\\_{h}eads=2, feed\\_forward\\_size=16384) # no chuck\nconfig\\_chunk = ReformerConfig.from\\_pretrained(\""google/reformer-enwik8\"", chunk\\_size\\_feed\\_forward=1, num\\_attention\\_{h}eads=2, feed\\_forward\\_size=16384) # feed forward chunk\nbenchmark\\_args = PyTorchBenchmarkArguments(sequence\\_lengths=[1024, 2048, 4096], batch\\_sizes=[8], models=[\""Reformer-No-Chunk\"", \""Reformer-Chunk\""], no\\_speed=True, no\\_env\\_print=True)\nbenchmark = PyTorchBenchmark(configs=[config\\_no\\_chunk, config\\_chunk], args=benchmark\\_args)\nresult = benchmark.run()\n```\n1 / 2\n2 / 2\n==================== INFERENCE - MEMORY - RESULT ====================\n--------------------------------------------------------------------------------\nModel Name Batch Size Seq Length Memory in MB\n--------------------------------------------------------------------------------\nReformer-No-Chunk 8 1024 3743\nReformer-No-Chunk 8 2048 5539\nReformer-No-Chunk 8 4096 9087\nReformer-Chunk 8 1024 2973\nReformer-Chunk 8 2048 3999\nReformer-Chunk 8 4096 6011\n--------------------------------------------------------------------------------\nNow a clear decrease in peak memory usage can be seen for longer input sequences.""]","The Nyströmformer's approximation of self-attention has a time and memory complexity of \\(O(n)\\), where \\(n\\) is the length of the input sequence.",O(n)
"What is the goal of the Named Entity Recognition task in token classification?
","[""p align=\""center\""> ![FSNER LOGO](http://sayef.tech:8082/uploads/FSNER-LOGO-2.png) \n\nImplemented by  [sayef](https://huggingface.co/sayef) . ## Overview\nThe FSNER model was proposed in [Example-Based Named Entity Recognition](https://arxiv.org/abs/2008.10570) by Morteza Ziyadi, Yuting Sun, Abhishek Goswami, Jade Huang, Weizhu Chen. To identify entity spans in a new domain, it uses a train-free few-shot learning approach inspired by question-answering. ## Abstract\n----\n> We present a novel approach to named entity recognition (NER) in the presence of scarce data that we call example-based NER. Our train-free few-shot learning approach takes inspiration from question-answering to identify entity spans in a new and unseen domain. In comparison with the current state-of-the-art, the proposed method performs significantly better, especially when using a low number of support examples. ## Model Training Details\n-----\n| identifier | epochs | datasets |\n| ---------- |:----------:| :-----:|\n| [sayef/fsner-bert-base-uncased](https://huggingface.co/sayef/fsner-bert-base-uncased) | 10 | ontonotes5, conll2003, wnut2017, and fin (Alvarado et al.). |\n## Installation and Example Usage\n------\nYou can use the FSNER model in 3 ways:\n1. Install directly from PyPI: `pip install fsner` and import the model as shown in the code example below\nor\n2. Install from source: `python setup.py install` and import the model as shown in the code example below\nor\n3. Clone repo and change directory to `src` and import the model as shown in the code example below\n```python\nfrom fsner import FSNERModel, FSNERTokenizerUtils\nmodel = FSNERModel(\""sayef/fsner-bert-base-uncased\"")\ntokenizer = FSNERTokenizerUtils(\""sayef/fsner-bert-base-uncased\"")\n# size of query and supports must be the same."", ""Named-Entity Recognition\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert\\_ner\\_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial\\_Analyst\\_AI\nTags: NER, TEXT, HIGHLIGHT\n## Introduction\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \""token\"") into different categories, such as names of people or names of locations, or different parts of speech. For example, given the sentence:\n> Does Chicago have any Pakistani restaurants? A named-entity recognition algorithm may identify:\n- \""Chicago\"" as a \\*\\*location\\*\\*\n- \""Pakistani\"" as an \\*\\*ethnicity\\*\\*\nand so on. Using `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team. Here is an example of a demo that you'll be able to build:\n$demo\\_ner\\_pipeline\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!"", ""These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. Two common types of token classification are:\n\\* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names. \\* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb). ```py\n>>> from transformers import pipeline\n>>> classifier = pipeline(task=\""ner\"")\n>>> preds = classifier(\""Hugging Face is a French company based in New York City.\"")\n>>> preds = [\n... {\n... \""entity\"": pred[\""entity\""],\n... \""score\"": round(pred[\""score\""], 4),\n... \""index\"": pred[\""index\""],\n... \""word\"": pred[\""word\""],\n... \""start\"": pred[\""start\""],\n... \""end\"": pred[\""end\""],\n...""]","The goal of the Named Entity Recognition (NER) task in token classification is to take a sentence and classify every word (or ""token"") into different categories, such as names of people, names of locations, or different parts of speech. For example, in the sentence ""Does Chicago have any Pakistani restaurants?"", a NER algorithm may identify ""Chicago"" as a **location** and ""Pakistani"" as an **ethnicity**.","The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization."
"What is the resolution of images used by the CLIPSeg model?
","[""For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm\\_source=hf&utm\\_medium=blog&utm\\_campaign=clipseg). Before we describe how to do that, let’s first take a look at how CLIPSeg works. ## CLIP: the magic model behind CLIPSeg\n[CLIP](https://huggingface.co/docs/transformers/main/en/model\\_doc/clip), which stands for \\*\\*C\\*\\*ontrastive \\*\\*L\\*\\*anguage–\\*\\*I\\*\\*mage \\*\\*P\\*\\*re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract \\*representation\\* of your input. This abstract representation, also called an \\*embedding\\*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together)."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# CLIPSeg\n## Overview\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke\nand Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero- and one-shot image segmentation."", ""It consists of 2100 images belonging to 21 classes (100 images per class), and each image has 5 captions. The Sydney dataset contains images of Sydney, Australia from Google Earth. It contains 613 images belonging to 7 classes. Images are (500, 500) RGB and provides 5 captions for each image. We used these additional datasets because we were not sure if the RSICD dataset would be large enough to fine-tune CLIP. #### Model\nOur model is just the fine-tuned version of the original CLIP model shown below. Inputs to the model are a batch of captions and a batch of images passed through the CLIP text encoder and image encoder respectively. The training process uses [contrastive learning](https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607) to learn a joint embedding representation of image and captions. In this embedding space, images and their respective captions are pushed close together, as are similar images and similar captions. Conversely, images and captions for different images, or dissimilar images and captions, are likely to be pushed further apart. ![](/blog/assets/30_clip_rsicd/clip_schematic.png)\n*CLIP Training and Inference (Image Credit: CLIP: Connecting Text and Images (https://openai.comclip/))*\n#### Data Augmentation\nIn order to regularize our dataset and prevent overfitting due to the size of the dataset, we used both image and text augmentation.""]",The resolution of images used by the CLIPSeg model is 500 x 500 pixels.,352 x 352 pixels
"What can you use Gradio for?
","[""### What's Next? Keep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features). Or, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/). ## Questions? If you'd like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help. If you like Gradio, please leave us a ⭐ on GitHub! ## Open Source Stack\nGradio is built on top of many wonderful open-source libraries! [![huggingface](readme_files/huggingface_mini.svg)](https://huggingface.co)\n[![python](readme_files/python.svg)](https://www.python.org)\n[![fastapi](readme_files/fastapi.svg)](https://fastapi.tiangolo.com)\n[![encode](readme_files/encode.svg)](https://www.encode.io)\n[![svelte](readme_files/svelte.svg)](https://svelte.dev)\n[![vite](readme_files/vite.svg)](https://vitejs.dev)\n[![pnpm](readme_files/pnpm.svg)](https://pnpm.io)\n[![tailwind](readme_files/tailwind.svg)](https://tailwindcss.com)\n[![storybook](readme_files/storybook.svg)](https://storybook.js.org/)\n[![chromatic](readme_files/chromatic.svg)](https://www.chromatic.com/)\n## License\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository. ## Citation\nAlso check out the paper \\_[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019\\_, and please cite it if you use Gradio in your work."", ""The Gradio Client is mostly used with apps hosted on [Hugging Face Spaces](https://hf.space), but your app can be hosted anywhere, such as your own server. \\*\\*Prequisites\\*\\*: To use the Gradio client, you do \\_not\\_ need to know the `gradio` library in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components. ## Installation\nThe lightweight `@gradio/client` package can be installed from the npm registry with a package manager of your choice and support node version 18 and above:\n```bash\nnpm i @gradio/client\n```\n## Connecting to a running Gradio App\nStart by connecting instantiating a `client` instance and connecting it to a Gradio app that is running on Hugging Face Spaces or generally anywhere on the web. ## Connecting to a Hugging Face Space\n```js\nimport { client } from \""@gradio/client\"";\nconst app = client(\""abidlabs/en2fr\""); // a Space that translates from English to French\n```\nYou can also connect to private Spaces by passing in your HF token with the `hf\\_token` property of the options parameter. You can get your HF token here: https://huggingface.co/settings/tokens\n```js\nimport { client } from \""@gradio/client\"";\nconst app = client(\""abidlabs/my-private-space\"", { hf\\_token=\""hf\\_...\"" })\n```\n## Duplicating a Space for private use\nWhile you can use any public Space as an API, you may get rate limited by Hugging Face if you make too many requests. For unlimited usage of a Space, simply duplicate the Space to create a private Space, and then use it to make as many requests as you'd like! The `@gradio/client` exports another function, `duplicate`, to make this process simple (you'll need to pass in your [Hugging Face token](https://huggingface.co/settings/tokens)). `duplicate` is almost identical to `client`, the only difference is under the hood:\n```js\nimport { client } from \""@gradio/client\"";\nconst response = await fetch(\n\""https://audio-samples.github.io/samples/mp3/blizzard\\_unconditional/sample-0.mp3\""\n);\nconst audio\\_file = await response.blob();\nconst app = await duplicate(\""abidlabs/whisper\"", { hf\\_token: \""hf\\_...\"" });\nconst transcription = app.predict(\""/predict\"", [audio\\_file]);\n```\nIf you have previously duplicated a Space, re-running `duplicate` will \\_not\\_ create a new Space."", ""Gradio Blocks Party[[gradio-blocks-party]]\nAlong with the release of the Gradio chapter of the course, Hugging Face hosted a community event on building cool machine learning demos using the new Gradio Blocks feature. You can find all the demos that the community created under the [`Gradio-Blocks`](https://huggingface.co/Gradio-Blocks) organisation on the Hub. Here's a few examples from the winners:\n\\*\\*Natural language to SQL\\*\\*""]","Gradio can be used for creating and sharing machine learning demos easily. It allows users to build interactive interfaces for their models, enabling others to test and interact with them without needing to write any code. Gradio is particularly useful for:

1. **Demonstrating ML Models**: You can showcase how your machine learning models work through user-friendly interfaces.
2. **Testing Models**: Users can input data and see the model's predictions in real-time.
3. **Sharing Models**: Gradio makes it easy to share your models with others, whether through Hugging Face Spaces or other platforms.
4. **Creating Interactive Demos**: You can build demos that include various input and output components, such as text, images, audio, and more.

Overall, Gradio simplifies the process of sharing and testing machine learning models in an interactive way.","Create a demo for your machine learning model, share your machine learning model with others, and debug your model."
"What TensorFlow API function is used to load a saved tensor file?
","[""Tensorflow API\n[[autodoc]] safetensors.tensorflow.load\\_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save\\_file\n[[autodoc]] safetensors.tensorflow.save"", ""### What is a SavedModel? A SavedModel contains a standalone TensorFlow model, including its weights and its architecture. It does not require the original source of the model to be run, which makes it useful for sharing or deploying with any backend that supports reading a SavedModel such as Java, Go, C++ or JavaScript among others. The internal structure of a SavedModel is represented as such:\n```\nsavedmodel\n/assets\n-> here the needed assets by the model (if any)\n/variables\n-> here the model checkpoints that contains the weights\nsaved\\_model.pb -> protobuf file representing the model graph\n```\n### How to install TensorFlow Serving? There are three ways to install and use TensorFlow Serving:\n- through a Docker container,\n- through an apt package,\n- or using [pip](https://pypi.org/project/pip/). To make things easier and compliant with all the existing OS, we will use Docker in this tutorial. ### How to create a SavedModel? SavedModel is the format expected by TensorFlow Serving. Since Transformers v4.2.0, creating a SavedModel has three additional features:\n1."", ""Why? Because the tokenized array and labels would have to be fully loaded into memory, and because Numpy doesn’t handle “jagged” arrays, so every tokenized sample would have to be padded to the length of the longest sample in the whole dataset. That’s going to make your array even bigger, and all those padding tokens will slow down training too! As a TensorFlow engineer, this is normally where you’d turn to `tf.data` to make a pipeline that will stream the data from storage rather than loading it all into memory. That’s a hassle, though, so we’ve got you. First, let’s use the `map()` method to add the tokenizer columns to the dataset. Remember that our datasets are disc-backed by default - they won’t load into memory until you convert them into arrays! ```py\ndef tokenize\\_dataset(data):\n# Keys of the returned dictionary will be added to the dataset as columns\nreturn tokenizer(data[\""text\""])\ndataset = dataset.map(tokenize\\_dataset)\n```\nNow our dataset has the columns we want, but how do we train on it? Simple - wrap it with a `tf.data.Dataset` and all our problems are solved - data is loaded on-the-fly, and padding is applied only to batches rather than the whole dataset, which means that we need way fewer padding tokens:\n```py\ntf\\_dataset = model.prepare\\_tf\\_dataset(\ndataset,\nbatch\\_size=16,\nshuffle=True\n)\nmodel.fit(tf\\_dataset)\n```\nWhy is [prepare\\_tf\\_dataset()](https://huggingface.co/docs/transformers/main/en/main\\_classes/model#transformers.TFPreTrainedModel.prepare\\_tf\\_dataset) a method on your model?""]",The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`.,safetensors.tensorflow.load_file
"Where can you access the logs of your Endpoints in Hugging Face Endpoints?
","[""Access and view Metrics\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the “Analytics” tab of your Endpoints. ![metric dashboard](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png)\n## Access Metrics via API\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api\\_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints."", ""- \\*\\*Protected\\*\\*: A Protected Endpoint is available from the internet, secured with TLS/SSL, and requires a valid Hugging Face token for authentication. - \\*\\*Private\\*\\* A Private Endpoint is only available through an intra-region secured AWS or Azure PrivateLink connection. Private Endpoints are not accessible from the internet. Public and Protected Endpoints do not require any additional configuration. For Private Endpoints, you need to provide the AWS account ID of the account which also should have access to 🤗 Inference Endpoints. ![endpoint types](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/endpoint_types.png)"", ""For more details, reach out to HF Support to address any questions or concerns at website@huggingface.co""]",You can access the logs of your Endpoints in Hugging Face Endpoints through the UI on the detailed overview in the “Analytics” tab of your Endpoints.,"In the ""Logs"" tab of your Endpoint through the UI."
"What is the latest task added to Hugging Face AutoTrain for Computer Vision?
","[""--\ntitle: Image Classification with AutoTrain\nthumbnail: /blog/assets/105\\_autotrain-image-classification/thumbnail.png\nauthors:\n- user: nimaboscarino\n---\n# Image Classification with AutoTrain\nSo you’ve heard all about the cool things that are happening in the machine learning world, and you want to join in. There’s just one problem – you don’t know how to code! 😱 Or maybe you’re a seasoned software engineer who wants to add some ML to your side-project, but you don’t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. That’s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature we’ve just added, we’re making “no-code” machine learning better than ever. Best of all, you can create your first project for ✨ free! ✨\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with \\*\\*zero\\*\\* configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer 🤯 We’ve been expanding the number of tasks that we support, and we’re proud to announce that \\*\\*you can now use AutoTrain for Computer Vision\\*\\*!"", ""Tasks\n## What's a task? Tasks, or pipeline types, describe the \""shape\"" of each model's API (inputs and outputs) and are used to determine which Inference API and widget we want to display for any given model. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/tasks.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/tasks-dark.png)\n\nThis classification is relatively coarse-grained (you can always add more fine-grained task names in your model tags), so \\*\\*you should rarely have to create a new task\\*\\*. If you want to add support for a new task, this document explains the required steps. ## Overview\nHaving a new task integrated into the Hub means that:\n\\* Users can search for all models – and datasets – of a given task. \\* The Inference API supports the task. \\* Users can try out models directly with the widget. 🏆\nNote that you don't need to implement all the steps by yourself. Adding a new task is a community effort, and multiple people can contribute. 🧑‍🤝‍🧑\nTo begin the process, open a new issue in the [huggingface\\_hub](https://github.com/huggingface/huggingface\\_hub/issues) repository."", ""--\ntitle: Image Similarity with Hugging Face Datasets and Transformers\nthumbnail: /blog/assets/image\\_similarity/thumbnail.png\nauthors:\n- user: sayakpaul\n---\n# Image Similarity with Hugging Face Datasets and Transformers\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_similarity.ipynb)\nIn this post, you'll learn to build an image similarity system with 🤗 Transformers. Finding out the similarity between a query image and potential candidates is an important use case for information retrieval systems, such as reverse image search, for example. All the system is trying to answer is that, given a \\_query\\_ image and a set of \\_candidate\\_ images, which images are the most similar to the query image. We'll leverage the [🤗 `datasets` library](https://huggingface.co/docs/datasets/) as it seamlessly supports parallel processing which will come in handy when building this system. Although the post uses a ViT-based model ([`nateraw/vit-base-beans`](https://huggingface.co/nateraw/vit-base-beans)) and a particular dataset ([Beans](https://huggingface.co/datasets/beans)), it can be extended to use other models supporting vision modality and other image datasets. Some notable models you could try:\n\\* [Swin Transformer](https://huggingface.co/docs/transformers/model\\_doc/swin)\n\\* [ConvNeXT](https://huggingface.co/docs/transformers/model\\_doc/convnext)\n\\* [RegNet](https://huggingface.co/docs/transformers/model\\_doc/regnet)\nAlso, the approach presented in the post can potentially be extended to other modalities as well.""]","The latest task added to Hugging Face AutoTrain for Computer Vision is image classification. With AutoTrain, users can now train models for this task with zero configuration needed.",Image Classification
"What is the default repository type created by the `create_repo` function on Hugging Face Hub?
","[""It's a model repo by default. Parameters:\n- `type`: Type of repo (dataset or space; model by default). - `name`: Name of repo. - `organization`: Name of organization (optional). Payload:\n```js\npayload = {\n\""type\"": \""model\"",\n\""name\"": \""name\"",\n\""organization\"": \""organization\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.delete\\_repo()`. ### PUT /api/repos/{repo\\_type}/{repo\\_id}/settings\nUpdate repo visibility. Payload:\n```js\npayload = {\n\""private\"": \""private\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.update\\_repo\\_visibility()`. ### POST /api/repos/move\nMove a repository (rename within the same namespace or transfer from user to organization). Parameters:\n- `fromRepo`: repo to rename. - `toRepo`: new name of the repo. - `type`: Type of repo (dataset or space; model by default). Payload:\n```js\npayload = {\n\""fromRepo\"" : \""namespace/repo\\_name\"",\n\""toRepo\"" : \""namespace2/repo\\_name2\"",\n\""type\"": \""model\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.move\\_repo()`. ## User API\nThe following endpoint gets information about a user. ### GET /api/whoami-v2\nGet username and organizations the user belongs to. Payload:\n```js\nheaders = { \""authorization\"" : \""Bearer $token\"" }\n```\nThis is equivalent to `huggingface\\_hub.whoami()`."", ""Firstly, there are a few methods to manage repository creation, deletion, and others:\n```python no-format\nfrom huggingface\\_hub import (\n# User management\nlogin,\nlogout,\nwhoami,\n# Repository creation and management\ncreate\\_repo,\ndelete\\_repo,\nupdate\\_repo\\_visibility,\n# And some methods to retrieve/change information about the content\nlist\\_models,\nlist\\_datasets,\nlist\\_metrics,\nlist\\_repo\\_files,\nupload\\_file,\ndelete\\_file,\n)\n```\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them. The `create\\_repo` method can be used to create a new repository on the hub:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"")\n```\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"", organization=\""huggingface\"")\n```\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization."", ""!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# 创建和管理存储库\nHugging Face Hub是一组 Git 存储库。[Git](https://git-scm.com/)是软件开发中广泛使用的工具，可以在协作工作时轻松对项目进行版本控制。本指南将向您展示如何与 Hub 上的存储库进行交互，特别关注以下内容：\n- 创建和删除存储库\n- 管理分支和标签\n- 重命名您的存储库\n- 更新您的存储库可见性\n- 管理存储库的本地副本\n如果您习惯于使用类似于GitLab/GitHub/Bitbucket等平台，您可能首先想到使用 `git`命令行工具来克隆存储库（`git clone`）、提交更改（`git add` , ` git commit`）并推送它们（`git push`）。在使用 Hugging Face Hub 时，这是有效的。然而，软件工程和机器学习并不具有相同的要求和工作流程。模型存储库可能会维护大量模型权重文件以适应不同的框架和工具，因此克隆存储库会导致您维护大量占用空间的本地文件夹。因此，使用我们的自定义HTTP方法可能更有效。您可以阅读我们的[git与HTTP相比较](../concepts/git\\_vs\\_http)解释页面以获取更多详细信息\n如果你想在Hub上创建和管理一个仓库，你的计算机必须处于登录状态。如果尚未登录，请参考[此部分](../quick-start#login)。在本指南的其余部分，我们将假设你的计算机已登录\n## 仓库创建和删除\n第一步是了解如何创建和删除仓库。你只能管理你拥有的仓库（在你的用户名命名空间下）或者你具有写入权限的组织中的仓库\n### 创建一个仓库\n使用 [`create\\_repo`] 创建一个空仓库，并通过 `repo\\_id`参数为其命名 `repo\\_id`是你的命名空间，后面跟着仓库名称：`username\\_or\\_org/repo\\_name`\n运行以下代码，以创建仓库：\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-model\"")\n'https://huggingface.co/lysandre/test-model'\n```\n默认情况下，[`create\\_repo`] 会创建一个模型仓库。但是你可以使用 `repo\\_type`参数来指定其他仓库类型。例如，如果你想创建一个数据集仓库\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-dataset\"", repo\\_type=\""dataset\"")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```\n创建仓库时，你可以使用 `private`参数设置仓库的可见性\n请运行以下代码\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-private\"", private=True)\n```\n如果你想在以后更改仓库的可见性，你可以使用[`update\\_repo\\_visibility`] 函数\n### 删除一个仓库\n使用 [`delete\\_repo`] 删除一个仓库。确保你确实想要删除仓库，因为这是一个不可逆转的过程！做完上述过程后，指定你想要删除的仓库的 `repo\\_id`\n请运行以下代码：\n```py\n>>> delete\\_repo(repo\\_id=\""lysandre/my-corrupted-dataset\"", repo\\_type=\""dataset\"")\n```\n### 克隆一个仓库（仅适用于 Spaces）\n在某些情况下，你可能想要复制别人的仓库并根据自己的用例进行调整。对于 Spaces，你可以使用 [`duplicate\\_space`] 方法来实现。它将复制整个仓库。\n你仍然需要配置自己的设置（硬件和密钥）。查看我们的[管理你的Space指南](./manage-spaces)以获取更多详细信息。\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import duplicate\\_space\n>>> duplicate\\_space(\""multimodalart/dreambooth-training\"", private=False)\nRepoUrl('https://huggingface.co/spaces/nateraw/dreambooth-training',...)\n```\n## 上传和下载文件\n既然您已经创建了您的存储库，您现在也可以推送更改至其中并从中下载文件\n这两个主题有它们自己的指南。请[上传指南](./upload) 和[下载指南](./download)来学习如何使用您的存储库。\n## 分支和标签\nGit存储库通常使用分支来存储同一存储库的不同版本。标签也可以用于标记存储库的特定状态，例如，在发布版本这个情况下。更一般地说，分支和标签被称为[git引用](https://git-scm.com/book/en/v2/Git-Internals-Git-References). ### 创建分支和标签\n你可以使用[`create\\_branch`]和[`create\\_tag`]来创建新的分支和标签:\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import create\\_branch, create\\_tag\n# Create a branch on a Space repo from `main` branch\n>>> create\\_branch(\""Matthijs/speecht5-tts-demo\"", repo\\_type=\""space\"", branch=\""handle-dog-speaker\"")\n# Create a tag on a Dataset repo from `v0.1-release` branch\n>>> create\\_branch(\""bigcode/the-stack\"", repo\\_type=\""dataset\"", revision=\""v0.1-release\"", tag=\""v0.1.1\"", tag\\_message=\""Bump release version.\"")\n```\n同时,你可以以相同的方式使用 [`delete\\_branch`] 和 [`delete\\_tag`] 函数来删除分支或标签\n### 列出所有的分支和标签\n你还可以使用 [`list\\_repo\\_refs`] 列出存储库中的现有 Git 引用\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import list\\_repo\\_refs\n>>> api.list\\_repo\\_refs(\""bigcode/the-stack\"", repo\\_type=\""dataset\"")\nGitRefs(\nbranches=[\nGitRefInfo(name='main', ref='refs/heads/main', target\\_commit='18edc1591d9ce72aa82f56c4431b3c969b210ae3'),\nGitRefInfo(name='v1.1.a1', ref='refs/heads/v1.1.a1', target\\_commit='f9826b862d1567f3822d3d25649b0d6d22ace714')\n],\nconverts=[],\ntags=[\nGitRefInfo(name='v1.0', ref='refs/tags/v1.0', target\\_commit='c37a8cd1e382064d8aced5e05543c5f7753834da')\n]\n)\n```\n## 修改存储库设置\n存储库具有一些可配置的设置。大多数情况下，您通常会在浏览器中的存储库设置页面上手动配置这些设置。要配置存储库，您必须具有对其的写访问权限（拥有它或属于组织）。在本节中，我们将看到您还可以使用 `huggingface\\_hub` 在编程方式上配置的设置。\n一些设置是特定于 Spaces（硬件、环境变量等）的。要配置这些设置，请参考我们的[管理Spaces](../guides/manage-spaces)指南。\n### 更新可见性\n一个存储库可以是公共的或私有的。私有存储库仅对您或存储库所在组织的成员可见。\n请运行以下代码将存储库更改为私有：\n```py\n>>> from huggingface\\_hub import update\\_repo\\_visibility\n>>> update\\_repo\\_visibility(repo\\_id=repo\\_id, private=True)\n```\n### 重命名您的存储库\n您可以使用 [`move\\_repo`] 在 Hub 上重命名您的存储库。使用这种方法，您还可以将存储库从一个用户移动到一个组织。在这样做时，有一些[限制](https://hf.co/docs/hub/repositories-settings#renaming-or-transferring-a-repo)需要注意。例如，您不能将存储库转移到另一个用户。\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import move\\_repo\n>>> move\\_repo(from\\_id=\""Wauplin/cool-model\"", to\\_id=\""huggingface/cool-model\"")\n```\n## 管理存储库的本地副本\n上述所有操作都可以通过HTTP请求完成。然而，在某些情况下，您可能希望在本地拥有存储库的副本，并使用您熟悉的Git命令与之交互。\n[`Repository`] 类允许您使用类似于Git命令的函数与Hub上的文件和存储库进行交互。它是对Git和Git-LFS方法的包装，以使用您已经了解和喜爱的Git命令。在开始之前，请确保已安装Git-LFS（请参阅[此处](https://git-lfs.github.com/)获取安装说明）。\n### 使用本地存储库\n使用本地存储库路径实例化一个 [`Repository`] 对象：\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(local\\_dir=\""//\"")\n```\n### 克隆\n`clone\\_from`参数将一个存储库从Hugging Face存储库ID克隆到由 `local\\_dir`参数指定的本地目录：\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(local\\_dir=\""w2v2\"", clone\\_from=\""facebook/wav2vec2-large-960h-lv60\"")\n```\n`clone\\_from`还可以使用URL克隆存储库：\n请运行以下代码：\n```py\n>>> repo = Repository(local\\_dir=\""huggingface-hub\"", clone\\_from=\""https://huggingface.co/facebook/wav2vec2-large-960h-lv60\"")\n```\n你可以将`clone\\_from`参数与[`create\\_repo`]结合使用，以创建并克隆一个存储库：\n请运行以下代码：\n```py\n>>> repo\\_url = create\\_repo(repo\\_id=\""repo\\_name\"")\n>>> repo = Repository(local\\_dir=\""repo\\_local\\_path\"", clone\\_from=repo\\_url)\n```\n当你克隆一个存储库时，通过在克隆时指定`git\\_user`和`git\\_email`参数，你还可以为克隆的存储库配置Git用户名和电子邮件。当用户提交到该存储库时，Git将知道提交的作者是谁。\n请运行以下代码：\n```py\n>>> repo = Repository(\n...""]",The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository.,model
"How many splits does the ""duorc"" dataset have?
","[""List splits and configurations\nDatasets typically have splits and may also have configurations. A \\_split\\_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A \\_configuration\\_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you're interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load\\_hub)! This guide shows you how to use Datasets Server's `/splits` endpoint to retrieve a dataset's splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\nThe `/splits` endpoint accepts the dataset name as its query parameter:\n\n```python\nimport requests\nheaders = {\""Authorization\"": f\""Bearer {API\\_TOKEN}\""}\nAPI\\_URL = \""https://datasets-server.huggingface.co/splits?dataset=duorc\""\ndef query():\nresponse = requests.get(API\\_URL, headers=headers)\nreturn response.json()\ndata = query()\n```\n\n```js\nimport fetch from \""node-fetch\"";\nasync function query(data) {\nconst response = await fetch(\n\""https://datasets-server.huggingface.co/splits?dataset=duorc\"",\n{\nheaders: { Authorization: `Bearer ${API\\_TOKEN}` },\nmethod: \""GET\""\n}\n);\nconst result = await response.json();\nreturn result;\n}\nquery().then((response) => {\nconsole.log(JSON.stringify(response));\n});\n```\n\n```curl\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\n-X GET \\\n-H \""Authorization: Bearer ${API\\_TOKEN}\""\n```\nThe endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n```json\n{\n\""splits\"": [\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""test\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""test\"" }\n],\n\""pending\"": [],\n\""failed\"": []\n}\n```"", ""- The slice of `rows` of a dataset and the content contained in each column of a specific row. For example, here are the `features` and the slice of `rows` of the `duorc`/`SelfRC` train split from 150 to 151:\n```json\n// https://datasets-server.huggingface.co/rows?dataset=duorc&config=SelfRC&split=train&offset=150&length=2\n{\n\""features\"": [\n{\n\""feature\\_idx\"": 0,\n\""name\"": \""plot\\_id\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 1,\n\""name\"": \""plot\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 2,\n\""name\"": \""title\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 3,\n\""name\"": \""question\\_id\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 4,\n\""name\"": \""question\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 5,\n\""name\"": \""answers\"",\n\""type\"": {\n\""feature\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" },\n\""\\_type\"": \""Sequence\""\n}\n},\n{\n\""feature\\_idx\"": 6,\n\""name\"": \""no\\_answer\"",\n\""type\"": { \""dtype\"": \""bool\"", \""\\_type\"": \""Value\"" }\n}\n],\n\""rows\"": [\n{\n\""row\\_idx\"": 150,\n\""row\"": {\n\""plot\\_id\"": \""/m/03wj\\_q\"",\n\""plot\"": \""The film is centered on Mortal Kombat, a fighting tournament between the representatives of the realms of Earth and Outworld conceived by the Elder Gods amid looming invasion of the Earth by Outworld."", ""If your dataset splits aren't loading as expected, it may be due to an incorrect pattern. ### Split name keywords\nThere are several ways to name splits. Validation splits are sometimes called \""dev\"", and test splits may be referred to as \""eval\"". These other split names are also supported, and the following keywords are equivalent:\n- train, training\n- validation, valid, val, dev\n- test, testing, eval, evaluation\nThe structure below is a valid repository:\n```\nmy\\_dataset\\_repository/\n├── README.md\n└── data/\n├── training.csv\n├── eval.csv\n└── valid.csv\n```\n### Multiple files per split\nIf one of your splits comprises several files, 🤗 Datasets can still infer whether it is the train, validation, and test split from the file name. For example, if your train and test splits span several files:\n```\nmy\\_dataset\\_repository/\n├── README.md\n├── train\\_0.csv\n├── train\\_1.csv\n├── train\\_2.csv\n├── train\\_3.csv\n├── test\\_0.csv\n└── test\\_1.csv\n```\nMake sure all the files of your `train` set have \\*train\\* in their names (same for test and validation). Even if you add a prefix or suffix to `train` in the file name (like `my\\_train\\_file\\_00001.csv` for example),\n🤗 Datasets can still infer the appropriate split.""]","The ""duorc"" dataset has six splits.",Six
"What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Fully Sharded Data Parallel\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or \\*rank\\*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class. Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer. ```bash\npip install accelerate\n```\n## FSDP configuration\nTo start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package\\_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`. ```bash\naccelerate config\n```\nWhen you run `accelerate config`, you'll be prompted with a series of options to configure your training environment."", ""This section covers some of the most important FSDP options. To learn more about the other available FSDP options, take a look at the [fsdp\\_config](https://huggingface.co/docs/transformers/main\\_classes/trainer#transformers.TrainingArguments.fsdp\\_config) parameters. ### Sharding strategy\nFSDP offers a number of sharding strategies to select from:\n\\* `FULL\\_SHARD` - shards model parameters, gradients and optimizer states across workers; select `1` for this option\n\\* `SHARD\\_GRAD\\_OP`- shard gradients and optimizer states across workers; select `2` for this option\n\\* `NO\\_SHARD` - don't shard anything (this is equivalent to DDP); select `3` for this option\n\\* `HYBRID\\_SHARD` - shard model parameters, gradients and optimizer states within each worker where each worker also has a full copy; select `4` for this option\n\\* `HYBRID\\_SHARD\\_ZERO2` - shard gradients and optimizer states within each worker where each worker also has a full copy; select `5` for this option\nThis is enabled by the `fsdp\\_sharding\\_strategy` flag."", ""4. 3D parallelism [3]: Employs Data Parallelism using ZERO + Tensor Parallelism + Pipeline Parallelism to train humongous models in the order of 100s of Billions of parameters. For instance, BigScience 176B parameters Language Model employ this [6]. In this post we will look at Data Parallelism using ZeRO and more specifically the latest PyTorch feature \\*\\*[FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)\\*\\*. \\*\\*[DeepSpeed](https://github.com/microsoft/deepspeed)\\*\\* and \\*\\*[FairScale](https://github.com/facebookresearch/fairscale/)\\*\\* have implemented the core ideas of the ZERO paper. These have already been integrated in `transformers` Trainer and accompanied by great blog [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale) [10]. PyTorch recently upstreamed the Fairscale FSDP into PyTorch Distributed with additional optimizations. # Accelerate 🚀: Leverage PyTorch FSDP without any code changes\nWe will look at the task of Causal Language Modelling using GPT-2 Large (762M) and XL (1.5B) model variants. Below is the code for pre-training GPT-2 model. It is similar to the official causal language modeling example [here](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run\\_clm\\_no\\_trainer.py) with the addition of 2 arguments `n\\_train` (2000) and `n\\_val` (500) to prevent preprocessing/training on entire data in order to perform quick proof of concept benchmarks. [run\\_clm\\_no\\_trainer.py](./assets/62_pytorch_fsdp/run_clm_no_trainer.py)\nSample FSDP config after running the command `accelerate config`:\n```bash\ncompute\\_environment: LOCAL\\_MACHINE\ndeepspeed\\_config: {}\ndistributed\\_type: FSDP\nfsdp\\_config:\nmin\\_num\\_params: 2000\noffload\\_params: false\nsharding\\_strategy: 1\nmachine\\_rank: 0\nmain\\_process\\_ip: null\nmain\\_process\\_port: null\nmain\\_training\\_function: main\nmixed\\_precision: 'no'\nnum\\_machines: 1\nnum\\_processes: 2\nuse\\_cpu: false\n```\n## Multi-GPU FSDP\nHere, we experiment on the Single-Node Multi-GPU setting.""]","Fully Sharded Data Parallel (FSDP) is a data parallel method that shards a model's parameters, gradients, and optimizer states across the number of available GPUs (also called workers or rank). The purpose of FSDP is to reduce memory usage by not replicating the entire model on each GPU, which improves GPU memory efficiency. This allows for training much larger models on fewer GPUs compared to traditional methods like DistributedDataParallel (DDP). FSDP is integrated with the Accelerate library, making it easier to manage training in distributed environments.","FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes."
"What file format is used to save and store PyTorch model weights more securely than `.bin` files?
","[""Convert weights to safetensors\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`. The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository. For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights. Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space."", ""On CPU, if the file is already in cache, then it can\ntruly be zero-copy, whereas on GPU there is not such disk cache, so a copy is always required\nbut you can bypass allocating all the tensors on CPU at any given point. SafeTensors is not zero-copy for the header. The choice of JSON is pretty arbitrary, but since deserialization is <<< of the time required to load the actual tensor data and is readable I went that way, (also space is <<< to the tensor data). - Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the\nmoment. - Order: 'C' or row-major. This seems to have won. We can add that information later if needed. - Stride: No striding, all tensors need to be packed before being serialized. I have yet to see a case where it seems useful to have a strided tensor stored in serialized format. ### Benefits\nSince we can invent a new format we can propose additional benefits:\n- Prevent DOS attacks: We can craft the format in such a way that it's almost\nimpossible to use malicious files to DOS attack a user. Currently, there's a limit\non the size of the header of 100MB to prevent parsing extremely large JSON. Also when reading the file, there's a guarantee that addresses in the file\ndo not overlap in any way, meaning when you're loading a file you should never\nexceed the size of the file in memory\n- Faster load: PyTorch seems to be the fastest file to load out in the major\nML formats. However, it does seem to have an extra copy on CPU, which we\ncan bypass in this lib by using `torch.UntypedStorage.from\\_file`. Currently, CPU loading times are extremely fast with this lib compared to pickle. GPU loading times are as fast or faster than PyTorch equivalent. Loading first on CPU with memmapping with torch, and then moving all tensors to GPU seems\nto be faster too somehow (similar behavior in torch pickle)\n- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to\nload only part of the tensors on the various models. For\n[BLOOM](https://huggingface.co/bigscience/bloom) using this format enabled\nto load the model on 8 GPUs from 10mn with regular PyTorch weights down to 45s."", ""Pickle Scanning\nPickle is a widely used serialization format in ML. Most notably, it is the default format for PyTorch model weights. There are dangerous arbitrary code execution attacks that can be perpetrated when you load a pickle file. We suggest loading models from users and organizations you trust, relying on signed commits, and/or loading models from TF or Jax formats with the `from\\_tf=True` auto-conversion mechanism. We also alleviate this issue by displaying/\""vetting\"" the list of imports in any pickled file, directly on the Hub. Finally, we are experimenting with a new, simple serialization format for weights called [`safetensors`](https://github.com/huggingface/safetensors). ## What is a pickle? From the [official docs](https://docs.python.org/3/library/pickle.html) :\n> The `pickle` module implements binary protocols for serializing and de-serializing a Python object structure. What this means is that pickle is a serializing protocol, something you use to efficiently share data amongst parties. We call a pickle the binary file that was generated while pickling. At its core, the pickle is basically a stack of instructions or opcodes. As you probably have guessed, it’s not human readable. The opcodes are generated when pickling and read sequentially at unpickling. Based on the opcode, a given action is executed. Here’s a small example:\n```python\nimport pickle\nimport pickletools\nvar = \""data I want to share with a friend\""\n# store the pickle data in a file named 'payload.pkl'\nwith open('payload.pkl', 'wb') as f:\npickle.dump(var, f)\n# disassemble the pickle\n# and print the instructions to the command line\nwith open('payload.pkl', 'rb') as f:\npickletools.dis(f)\n```\nWhen you run this, it will create a pickle file and print the following instructions in your terminal:\n```python\n0: \\x80 PROTO 4\n2: \\x95 FRAME 48\n11: \\x8c SHORT\\_BINUNICODE 'data I want to share with a friend'\n57: \\x94 MEMOIZE (as 0)\n58: .""]",The more secure file format used to save and store PyTorch model weights is `.safetensors`.,`.safetensors`
"What type of security certification does Hugging Face have?
","[""Security\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)"", ""| Version | Supported |\n|---------|--------------------|\n| 1.x.x | :white\\_check\\_mark: |\n| 2.x.x | :white\\_check\\_mark: |\n## Reporting a Vulnerability\nTo report a security vulnerability, please contact: security@huggingface.co"", ""The certification process\nThe certification process is \\*\\*completely free\\*\\*:\n- To get a \\*certificate of completion\\*: you need \\*\\*to pass 80% of the assignments\\*\\*. - To get a \\*certificate of excellence\\*: you need \\*\\*to pass 100% of the assignments\\*\\*. There's \\*\\*no deadlines, the course is self-paced\\*\\*. ![Course certification](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit0/certification.jpg)\nWhen we say pass, \\*\\*we mean that your model must be pushed to the Hub and get a result equal or above the minimal requirement\\*\\*. To check your progression and which unit you passed/not passed: https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course\nNow that you're ready for the certification process, you need to:\n1. Go here: https://huggingface.co/spaces/huggingface-projects/Deep-RL-Course-Certification/\n2. Type your \\*hugging face username\\*, your \\*first name\\*, \\*last name\\*\n3. Click on \""Generate my certificate\"". - If you passed 80% of the assignments, \\*\\*congratulations\\*\\* you've just got the certificate of completion. - If you passed 100% of the assignments, \\*\\*congratulations\\*\\* you've just got the excellence certificate. - If you are below 80%, don't be discouraged!""]","Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses.",SOC2 Type 2 certified
"What do RAG models combine to generate outputs?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# RAG\n\n[![Models](https://img.shields.io/badge/All_model_pages-rag-blueviolet)](https://huggingface.co/models?filter=rag)\n\n## Overview\nRetrieval-augmented generation (\""RAG\"") models combine the powers of pretrained dense retrieval (DPR) and\nsequence-to-sequence models."", ""If this path already exists, the script will use saved predictions to calculate metrics. Add `--recalculate` parameter to force the script to perform inference from scratch. An example e2e evaluation run could look as follows:\n```bash\npython examples/research\\_projects/rag/eval\\_rag.py \\\n--model\\_name\\_or\\_path facebook/rag-sequence-nq \\\n--model\\_type rag\\_sequence \\\n--evaluation\\_set path/to/test.source \\\n--gold\\_data\\_path path/to/gold\\_data \\\n--predictions\\_path path/to/e2e\\_preds.txt \\\n--eval\\_mode e2e \\\n--gold\\_data\\_mode qa \\\n--n\\_docs 5 \\ # You can experiment with retrieving different number of documents at evaluation time\n--print\\_predictions \\\n--recalculate \\ # adding this parameter will force recalculating predictions even if predictions\\_path already exists\n```\n# Use your own knowledge source\nBy default, RAG uses the English Wikipedia as a knowledge source, known as the 'wiki\\_dpr' dataset. With `use\\_custom\\_knowledge\\_dataset.py` you can build your own knowledge source, \\*e.g.\\* for RAG. For instance, if documents are serialized as tab-separated csv files with the columns \""title\"" and \""text\"", one can use `use\\_own\\_knowledge\\_dataset.py` as follows:\n```bash\npython examples/research\\_projects/rag/use\\_own\\_knowledge\\_dataset.py \\\n--csv\\_path path/to/my\\_csv \\\n--output\\_dir path/to/my\\_knowledge\\_dataset \\\n```\nThe created outputs in `path/to/my\\_knowledge\\_dataset` can then be used to finetune RAG as follows:\n```bash\npython examples/research\\_projects/rag/finetune\\_rag.py \\\n--data\\_dir $DATA\\_DIR \\\n--output\\_dir $OUTPUT\\_DIR \\\n--model\\_name\\_or\\_path $MODEL\\_NAME\\_OR\\_PATH \\\n--model\\_type rag\\_sequence \\\n--fp16 \\\n--gpus 8\n--index\\_name custom\n--passages\\_path path/to/data/my\\_knowledge\\_dataset\n--index\\_path path/to/my\\_knowledge\\_dataset\\_hnsw\\_index.faiss\n```"", ""Each expert specializes in a\ndifferent subset of the data, and a gating network determines the appropriate\nexpert to use for a given input. This approach allows the model to adapt to\ncomplex, non-linear relationships in the data and improve overall performance. >\n### Using Text Generation Inference\n\\*\\*[Text Generation Inference](https://github.com/huggingface/text-generation-inference)\\*\\* is a production-ready inference container developed by Hugging Face to enable easy deployment of large language models. It has features such as continuous batching, token streaming, tensor parallelism for fast inference on multiple GPUs, and production-ready logging and tracing. You can deploy Mixtral on Hugging Face's [Inference Endpoints](https://ui.endpoints.huggingface.co/new?repository=mistralai%2FMixtral-8x7B-Instruct-v0.1&vendor=aws&region=us-east-1&accelerator=gpu&instance\\_size=2xlarge&task=text-generation&no\\_suggested\\_compute=true&tgi=true&tgi\\_max\\_batch\\_total\\_tokens=1024000&tgi\\_max\\_total\\_tokens=32000), which uses Text Generation Inference as the backend. To deploy a Mixtral model, go to the [model page](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) and click on the [Deploy -> Inference Endpoints](https://ui.endpoints.huggingface.co/new?repository=meta-llama/Llama-2-7b-hf) widget.""]",RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs.,Pretrained dense retrieval (DPR) and sequence-to-sequence models.
"What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# MarkupLM\n## Overview\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm). The model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838\\_From\\_one\\_tree\\_to\\_a\\_forest\\_a\\_unified\\_solution\\_for\\_structured\\_web\\_data\\_extraction), a dataset\nfor information extraction from web pages (basically named-entity recogntion on web pages)\nThe abstract from the paper is the following:\n\\*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\ntasks. The pre-trained model and code will be publicly available.\\*\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm). ## Usage tips\n- In addition to `input\\_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath\\_tags\\_seq` and `xpath\\_subs\\_seq`. These are the XPATH tags and subscripts respectively for each token in the input sequence. - One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info. ![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg)\n MarkupLM architecture. Taken from the [original paper.](https://arxiv.org/abs/2110.08518) \n## Usage: MarkupLMProcessor\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input\\_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks. ```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\nfeature\\_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from\\_pretrained(\""microsoft/markuplm-base\"")\nprocessor = MarkupLMProcessor(feature\\_extractor, tokenizer)\n```\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input\\_ids`, `attention\\_mask`, `token\\_type\\_ids`, `xpath\\_subs\\_seq`, `xpath\\_tags\\_seq`. Optionally, one can provide node labels to the processor, which are turned into token-level `labels`. [`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Utilities for `FeatureExtractors`\nThis page lists all the utility functions that can be used by the audio [`FeatureExtractor`] in order to compute special features from a raw audio using common algorithms such as \\*Short Time Fourier Transform\\* or \\*log mel spectrogram\\*. Most of those are only useful if you are studying the code of the audio processors in the library. ## Audio Transformations\n[[autodoc]] audio\\_utils.hertz\\_to\\_mel\n[[autodoc]] audio\\_utils.mel\\_to\\_hertz\n[[autodoc]] audio\\_utils.mel\\_filter\\_bank\n[[autodoc]] audio\\_utils.optimal\\_fft\\_length\n[[autodoc]] audio\\_utils.window\\_function\n[[autodoc]] audio\\_utils.spectrogram\n[[autodoc]] audio\\_utils.power\\_to\\_db\n[[autodoc]] audio\\_utils.amplitude\\_to\\_db"", ""In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs). \\*\\*Use case 1: web page classification (training, inference) + token classification (inference), parse\\_html = True\\*\\*\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML. ```python\n>>> from transformers import MarkupLMProcessor\n>>> processor = MarkupLMProcessor.from\\_pretrained(\""microsoft/markuplm-base\"")\n>>> html\\_string = \""\""\""\n... ... ... ... Hello world\n... ... ... # Welcome\n\n... Here is my website. ... ... \""\""\""\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\n>>> encoding = processor(html\\_string, return\\_tensors=\""pt\"")\n>>> print(encoding.keys())\ndict\\_keys(['input\\_ids', 'token\\_type\\_ids', 'attention\\_mask', 'xpath\\_tags\\_seq', 'xpath\\_subs\\_seq'])\n```\n\\*\\*Use case 2: web page classification (training, inference) + token classification (inference), parse\\_html=False\\*\\*\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse\\_html` to `False`. ```python\n>>> from transformers import MarkupLMProcessor\n>>> processor = MarkupLMProcessor.from\\_pretrained(\""microsoft/markuplm-base\"")\n>>> processor.parse\\_html = False\n>>> nodes = [\""hello\"", \""world\"", \""how\"", \""are\""]\n>>> xpaths = [\""/html/body/div/li[1]/div/span\"", \""/html/body/div/li[1]/div/span\"", \""html/body\"", \""html/body/div\""]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return\\_tensors=\""pt\"")\n>>> print(encoding.keys())\ndict\\_keys(['input\\_ids', 'token\\_type\\_ids', 'attention\\_mask', 'xpath\\_tags\\_seq', 'xpath\\_subs\\_seq'])\n```\n\\*\\*Use case 3: token classification (training), parse\\_html=False\\*\\*\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\ncorresponding node labels in order to train a model.""]","MarkupLMFeatureExtractor uses Beautiful Soup, a Python library for pulling data out of HTML and XML files, to extract data from HTML and XML documents.",Beautiful Soup
"What is the file size limit for syncing to HF Spaces without using Git-LFS?
","[""Managing Spaces with CircleCI Workflows\nYou can keep your app in sync with your GitHub repository with a \\*\\*CircleCI workflow\\*\\*. [CircleCI](https://circleci.com) is a continuous integration and continuous delivery (CI/CD) platform that helps automate the software development process. A [CircleCI workflow](https://circleci.com/docs/workflows/) is a set of automated tasks defined in a configuration file, orchestrated by CircleCI, to streamline the process of building, testing, and deploying software applications. \\*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\\*\nFirst, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository. ```bash\ngit remote add space https://huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME\n```\nThen force push to sync everything for the first time:\n```bash\ngit push --force space main\n```\nNext, set up a [CircleCI workflow](https://circleci.com/docs/workflows/) to push your `main` git branch to Spaces. In the example below:\n\\* Replace `HF\\_USERNAME` with your username and `SPACE\\_NAME` with your Space name. \\* [Create a context in CircleCI](https://circleci.com/docs/contexts/) and add an env variable into it called \\*HF\\_PERSONAL\\_TOKEN\\* (you can give it any name, use the key you create in place of HF\\_PERSONAL\\_TOKEN) and the value as your Hugging Face API token. You can find your Hugging Face API token under \\*\\*API Tokens\\*\\* on [your Hugging Face profile](https://huggingface.co/settings/tokens). ```yaml\nversion: 2.1\nworkflows:\nmain:\njobs:\n- sync-to-huggingface:\ncontext:\n- HuggingFace\nfilters:\nbranches:\nonly:\n- main\njobs:\nsync-to-huggingface:\ndocker:\n- image: alpine\nresource\\_class: small\nsteps:\n- run:\nname: install git\ncommand: apk update && apk add openssh-client git\n- checkout\n- run:\nname: push to Huggingface hub\ncommand: |\ngit config user.email \""\""\ngit config user.name \""\""\ngit push -f https://HF\\_USERNAME:${HF\\_PERSONAL\\_TOKEN}@huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME main\n```"", ""There are a few reasons for this:\n- Uploading and downloading smaller files is much easier both for you and the other users. Connection issues can always\nhappen when streaming data and smaller files avoid resuming from the beginning in case of errors. - Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\nleading to a slower download speed. In all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size. - \\*\\*Number of commits\\*\\*: There is no hard limit for the total number of commits on your repo history. However, from\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a\nfresh start using `huggingface\\_hub`'s [`super\\_squash\\_history`](https://huggingface.co/docs/huggingface\\_hub/main/en/package\\_reference/hf\\_api#huggingface\\_hub.HfApi.super\\_squash\\_history)."", ""{/if}\n✏️ When creating the repository from the web interface, the \\*.gitattributes\\* file is automatically set up to consider files with certain extensions, such as \\*.bin\\* and \\*.h5\\*, as large files, and git-lfs will track them with no necessary setup on your side. We can now go ahead and proceed like we would usually do with traditional Git repositories. We can add all the files to Git's staging environment using the `git add` command:\n```bash\ngit add . ```\nWe can then have a look at the files that are currently staged:\n```bash\ngit status\n```\n{#if fw === 'pt'}\n```bash\nOn branch main\nYour branch is up to date with 'origin/main'. Changes to be committed:\n(use \""git restore --staged ...\"" to unstage)\nmodified: .gitattributes\nnew file: config.json\nnew file: pytorch\\_model.bin\nnew file: sentencepiece.bpe.model\nnew file: special\\_tokens\\_map.json\nnew file: tokenizer.json\nnew file: tokenizer\\_config.json\n```\n{:else}\n```bash\nOn branch main\nYour branch is up to date with 'origin/main'. Changes to be committed:\n(use \""git restore --staged ...\"" to unstage)\nmodified: .gitattributes\nnew file: config.json\nnew file: sentencepiece.bpe.model\nnew file: special\\_tokens\\_map.json\nnew file: tf\\_model.h5\nnew file: tokenizer.json\nnew file: tokenizer\\_config.json\n```\n{/if}\nSimilarly, we can make sure that git-lfs is tracking the correct files by using its `status` command:\n```bash\ngit lfs status\n```\n{#if fw === 'pt'}\n```bash\nOn branch main\nObjects to be pushed to origin/main:\nObjects to be committed:\nconfig.json (Git: bc20ff2)\npytorch\\_model.bin (LFS: 35686c2)\nsentencepiece.bpe.model (LFS: 988bc5a)\nspecial\\_tokens\\_map.json (Git: cb23931)\ntokenizer.json (Git: 851ff3e)\ntokenizer\\_config.json (Git: f0f7783)\nObjects not staged for commit:\n```\nWe can see that all files have `Git` as a handler, except \\*pytorch\\_model.bin\\* and \\*sentencepiece.bpe.model\\*, which have `LFS`. Great!""]","The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. If you have files larger than this, you will need to use Git-LFS or review your files and check your history.",10MB
"What is the title of the paper introducing the ByT5 model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ByT5\n## Overview\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, Colin Raffel.""]","The title of the paper introducing the ByT5 model is ""ByT5: Towards a token-free future with pre-trained byte-to-byte models.""",ByT5: Towards a token-free future with pre-trained byte-to-byte models
"What is the dimension of the feature vector for the base BERT model?
","[""n this video, we'll study the encoder architecture. An example of a popular encoder-only architecture is BERT, which is the most popular model of its kind. Let's first start by understanding how it works. We'll use a small example, using three words. We use these as inputs, and pass them through the encoder. We retrieve a numerical representation of each word. Here, for example, the encoder converts the three words “Welcome to NYC” in these three sequences of numbers. The encoder outputs exactly one sequence of numbers per input word. This numerical representation can also be called a \""Feature vector\"", or \""Feature tensor\"". Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \""to\"", isn't the representation of only the \""to\"" word. It also takes into account the words around it, which we call the “context”.As in, it looks to the left context, the word on the left of the one we're studying (here the word \""Welcome\"") and the context on the right (here the word \""NYC\"") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \""meaning\"" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence."", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# BERT\n\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\n\n## Overview\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It's a\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\n\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\narchitecture modifications.\\*\n\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\*\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf)."", ""|\n| Transformer Layers: | Number of Transformer blocks. A transformer block transforms a sequence of word representations to a sequence of contextualized words (numbered representations). |\n| Hidden Size: | Layers of mathematical functions, located between the input and output, that assign weights (to words) to produce a desired result. |\n| Attention Heads: | The size of a Transformer block. |\n| Processing: | Type of processing unit used to train the model. |\n| Length of Training: | Time it took to train the model. Here’s how many of the above ML architecture parts BERTbase and BERTlarge has:\n| | Transformer Layers | Hidden Size | Attention Heads | Parameters | Processing | Length of Training |\n|-----------|--------------------|-------------|-----------------|------------|------------|--------------------|\n| BERTbase | 12 | 768 | 12 | 110M | 4 TPUs | 4 days |\n| BERTlarge | 24 | 1024 | 16 | 340M | 16 TPUs | 4 days |\nLet’s take a look at how BERTlarge’s additional layers, attention heads, and parameters have increased its performance across NLP tasks.""]",The dimension of the feature vector for the base BERT model is 768.,768
"What special identifier does the WordPiece Model use for continuing subwords?
","[""WordPiece tokenization[[wordpiece-tokenization]]\nWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It's very similar to BPE in terms of the training, but the actual tokenization is done differently. 💡 This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm. ## Training algorithm[[training-algorithm]]\n⚠️ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate. Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\""word\""` gets split like this:\n```\nw ##o ##r ##d\n```\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix. Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\n$$\\mathrm{score} = (\\mathrm{freq\\\\_of\\\\_pair}) / (\\mathrm{freq\\\\_of\\\\_first\\\\_element} \\times \\mathrm{freq\\\\_of\\\\_second\\\\_element})$$\nBy dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary."", ""This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |\n## Post-Processors\nAfter the whole pipeline, we sometimes want to insert some special\ntokens before feed a tokenized string into a model like \""[CLS] My\nhorse is amazing [SEP]\"". The `PostProcessor` is the component doing\njust that. | Name | Description | Example |\n| :--- | :--- | :--- |\n| TemplateProcessing | Let’s you easily template the post processing, adding special tokens, and specifying the `type\\_id` for each sequence/special token. The template is given two strings representing the single sequence and the pair of sequences, as well as a set of special tokens to use. | Example, when specifying a template with these values:  \n\n* single: `\""[CLS] $A [SEP]\""`\n* pair: `\""[CLS] $A [SEP] $B [SEP]\""`\n* special tokens:\n  + `\""[CLS]\""`\n  + `\""[SEP]\""`\n\n Input: `(\""I like this\"", \""but not this\"")`   \n Output: `\""[CLS] I like this [SEP] but not this [SEP]\""` |\n## Decoders\nThe Decoder knows how to go from the IDs used by the Tokenizer, back to\na readable piece of text. Some `Normalizer` and `PreTokenizer` use\nspecial characters or identifiers that need to be reverted for example. | Name | Description |\n| :--- | :--- |\n| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTokenizer encodes at the byte-level, using a set of visible Unicode characters to represent each byte, so we need a Decoder to revert this process and get something readable again. |\n| Metaspace | Reverts the Metaspace PreTokenizer. This PreTokenizer uses a special identifer `▁` to identify whitespaces, and so this Decoder helps with decoding these. |\n| WordPiece | Reverts the WordPiece Model. This model uses a special identifier `##` for continuing subwords, and so this Decoder helps with decoding these. |"", ""We attack here the big part: the design of our tokenizer with the tokenizers library. We start by initializing a tokenizer instance with a WordPiece model because it is the model used by BERT. Then we can define our normalizer. We will define it as a succession of 2 normalizations used to clean up characters not visible in the text, 1 lowercasing normalization and 2 normalizations used to remove accents. For the pre-tokenization, we will chain two pre\\_tokenizer. The first one separating the text at the level of spaces and the second one isolating the punctuation marks. Now, we can define the trainer that will allow us to train the WordPiece model chosen at the beginning. To carry out the training, we will have to choose a vocabulary size, here we choose twenty-five thousand and also announce the special tokens that we absolutely want to add to our vocabulary. In one line of code, we can train our WordPiece model using the iterator we defined earlier. Once the model has been trained, we can retrieve the ids of the special class and separation tokens because we will need them to post-process our sequence. Thanks to the TemplateProcessing class, we can add the CLS token at the beginning of each sequence and the SEP token at the end of the sequence and between two sentences if we tokenize a text pair.""]",The WordPiece Model uses the special identifier `##` for continuing subwords.,##
"What is the purpose of the 🧨 Diffusers tutorials?
","[""If you liked this topic and want to learn more, we recommend the following resources:\n- Our [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). - The [Getting Started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers\\_intro.ipynb) notebook, that gives a broader overview on Diffusion systems. - The [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) blog post. - Our [code in GitHub](https://github.com/huggingface/diffusers) where we'd be more than happy if you leave a ⭐ if `diffusers` is useful to you! ### Citation:\n```\n@article{patil2022stable,\nauthor = {Patil, Suraj and Cuenca, Pedro and Lambert, Nathan and von Platen, Patrick},\ntitle = {Stable Diffusion with 🧨 Diffusers},\njournal = {Hugging Face Blog},\nyear = {2022},\nnote = {[https://huggingface.co/blog/rlhf](https://huggingface.co/blog/stable\\_diffusion)},\n}\n```"", ""Overview\nThese examples show how to run [Diffuser](https://arxiv.org/abs/2205.09991) in Diffusers. There are two ways to use the script, `run\\_diffuser\\_locomotion.py`. The key option is a change of the variable `n\\_guide\\_steps`. When `n\\_guide\\_steps=0`, the trajectories are sampled from the diffusion model, but not fine-tuned to maximize reward in the environment. By default, `n\\_guide\\_steps=2` to match the original implementation. You will need some RL specific requirements to run the examples:\n```\npip install -f https://download.pytorch.org/whl/torch\\_stable.html \\\nfree-mujoco-py \\\neinops \\\ngym==0.24.1 \\\nprotobuf==3.20.1 \\\ngit+https://github.com/rail-berkeley/d4rl.git \\\nmediapy \\\nPillow==9.0.0\n```"", ""Models\nFor more detail on the models, please refer to the [docs](https://huggingface.co/docs/diffusers/api/models/overview).""]","The purpose of the 🧨 Diffusers tutorials is to provide users with guidance on how to effectively use the Diffusers library for various applications, particularly in the context of diffusion models. These tutorials cover different aspects of the library, including setup, usage, and examples, helping users to understand and implement diffusion systems in their projects.",To provide a gentle introduction to diffusion models and help understand the library fundamentals.
"What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?
","[""Using Flagging\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\nTags: FLAGGING, DATA\n## Introduction\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \""hard\"" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust. Gradio simplifies the collection of this data by including a \\*\\*Flag\\*\\* button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`. ## The \\*\\*Flag\\*\\* button in `gradio.Interface`\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked \\*\\*Flag\\*\\*. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file. There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail. - `allow\\_flagging`: this parameter can be set to either `\""manual\""` (default), `\""auto\""`, or `\""never\""`. - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked. - `auto`: users will not see a button to flag, but every sample will be flagged automatically. - `never`: users will not see a button to flag, and no sample will be flagged. - `flagging\\_options`: this parameter can be either `None` (default) or a list of strings. - If `None`, then the user simply clicks on the \\*\\*Flag\\*\\* button and no additional options are shown. - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\""Incorrect\"", \""Ambiguous\""]`, then buttons labeled \\*\\*Flag as Incorrect\\*\\* and \\*\\*Flag as Ambiguous\\*\\* appear. This only applies if `allow\\_flagging` is `\""manual\""`. - The chosen option is then logged along with the input and output. - `flagging\\_dir`: this parameter takes a string. - It represents what to name the directory where flagged data is stored. - `flagging\\_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n- Using this parameter allows you to write custom code that gets run when the flag button is clicked\n- By default, this is set to an instance of `gr.CSVLogger`\n- One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset.""]","The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `""manual""`.","""manual"""
"Where can the full code for the Stable Diffusion demo be found?
","[""--\ntitle: 🧨 Stable Diffusion in JAX / Flax ! thumbnail: /blog/assets/108\\_stable\\_diffusion\\_jax/thumbnail.png\nauthors:\n- user: pcuenq\n- user: patrickvonplaten\n---\n# 🧨 Stable Diffusion in JAX / Flax ! [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_jax_how_to.ipynb)\n# \\*\\*Stable Diffusion in JAX / Flax\\*\\* 🚀\n🤗 Hugging Face [Diffusers](https://github.com/huggingface/diffusers) supports Flax since version `0.5.1`! This allows for super fast inference on Google TPUs, such as those available in Colab, Kaggle or Google Cloud Platform. This post shows how to run inference using JAX / Flax. If you want more details about how Stable Diffusion works or want to run it in GPU, please refer to [this Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). If you want to follow along, click the button above to open this post as a Colab notebook. First, make sure you are using a TPU backend. If you are running this notebook in Colab, select `Runtime` in the menu above, then select the option \""Change runtime type\"" and then select `TPU` under the `Hardware accelerator` setting. Note that JAX is not exclusive to TPUs, but it shines on that hardware because each TPU server has 8 TPU accelerators working in parallel. ## Setup\n``` python\nimport jax\nnum\\_devices = jax.device\\_count()\ndevice\\_type = jax.devices()[0].device\\_kind\nprint(f\""Found {num\\_devices} JAX devices of type {device\\_type}.\"")\nassert \""TPU\"" in device\\_type, \""Available device is not a TPU, please select TPU from Edit > Notebook settings > Hardware accelerator\""\n```\n\\*Output\\*:\n```bash\nFound 8 JAX devices of type TPU v2."", ""Free solutions include many [spaces](https://huggingface.co/spaces) here on 🤗 Hugging Face, such as the [Stable Diffusion 2.1 Demo](https://huggingface.co/spaces/stabilityai/stable-diffusion) or the [camemduru webui](https://huggingface.co/spaces/camenduru/webui). You can find a list of additional online services [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services). You can even use 🤗 [Diffusers](https://huggingface.co/docs/diffusers/index) to write your own free solution! You can find a simple code example to get started [here](https://colab.research.google.com/drive/1HebngGyjKj7nLdXfj6Qi0N1nh7WvD74z?usp=sharing). \\*Note:\\* Parts of this series will use advanced features such as image2image, which may not be available on all online services. ### Generating Concept Art \nLet's generate some concept art. The steps are simple:\n1. Type what you want. 2. Click generate. ![Stable Diffusion Demo Space](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/sd-demo.png)\nBut, how do you get the results you actually want?"", ""ote: This is a simplified version of the code needed to create the Stable Diffusion demo.""]",The full code for the Stable Diffusion demo can be found in the [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_jax_how_to.ipynb).,https://hf.co/spaces/stabilityai/stable-diffusion/tree/main
"What transformation does the FNet model use to replace the self-attention layer in a BERT model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# FNet\n## Overview\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:\n\\*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \""mix\"" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \""efficient\"" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.\\*\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f\\_net). ## Usage tips\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\nsequence length for fine-tuning and inference. ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## FNetConfig\n[[autodoc]] FNetConfig\n## FNetTokenizer\n[[autodoc]] FNetTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## FNetTokenizerFast\n[[autodoc]] FNetTokenizerFast\n## FNetModel\n[[autodoc]] FNetModel\n- forward\n## FNetForPreTraining\n[[autodoc]] FNetForPreTraining\n- forward\n## FNetForMaskedLM\n[[autodoc]] FNetForMaskedLM\n- forward\n## FNetForNextSentencePrediction\n[[autodoc]] FNetForNextSentencePrediction\n- forward\n## FNetForSequenceClassification\n[[autodoc]] FNetForSequenceClassification\n- forward\n## FNetForMultipleChoice\n[[autodoc]] FNetForMultipleChoice\n- forward\n## FNetForTokenClassification\n[[autodoc]] FNetForTokenClassification\n- forward\n## FNetForQuestionAnswering\n[[autodoc]] FNetForQuestionAnswering\n- forward"", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# DeBERTa\n## Overview\nThe DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's\nBERT model released in 2018 and Facebook's RoBERTa model released in 2019. It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in\nRoBERTa. The abstract from the paper is the following:\n\\*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\nlanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\ndisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\ndisentangled attention mechanism, where each word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed using disentangled matrices on their\ncontents and relative positions."", ""The original code can be found [here](https://github.com/google-research/bert). ## Usage tips\n- BERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\nthe left. - BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is\nefficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. - Corrupts the inputs by using random masking, more precisely, during pretraining, a given percentage of tokens (usually 15%) is masked by:\n\\* a special mask token with probability 0.8\n\\* a random token different from the one masked with probability 0.1\n\\* the same token with probability 0.1\n- The model must predict the original sentence, but has a second objective: inputs are two sentences A and B (with a separation token in between). With probability 50%, the sentences are consecutive in the corpus, in the remaining 50% they are not related. The model has to predict if the sentences are consecutive or not. ## Resources\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with BERT.""]","The FNet model replaces the self-attention layer in a BERT model with a Fourier transform, which returns only the real parts of the transform.",Fourier transform
"What type of test should typically accompany a bug fix in Gradio's testing strategy?
","[""Test Strategy\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to. This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Objectives\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n1. Ensure that the Gradio library functions as we expect it to. 2. Enable the maintenance team to quickly identify both the presence and source of defects. 3. Prevent regressions, i.e. if we fix something it should stay fixed. 4. Improve the quality of the codebase in order to ease maintenance efforts. 5. Reduce the amount of manual testing required. ## Scope\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas. We should not focus on code coverage but on test coverage following the below criteria:\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n## Types of testing\nOur tests will broadly fall into one of three categories:\n- Static Quality checks\n- Dynamic 'Code' tests\n- Dynamic Functional tests\n### Static Quality checks\nStatic quality checks are generally very fast to run and do not require building the code base."", ""Tests in this category could be browser-based end-to-end tests, accessibility tests, or performance tests. They are sometimes called acceptance tests. ## Testing tools\nWe currently use the following tools:\n### Static quality checks\n- Python type-checking (python)\n- Black linting (python)\n- ruff formatting (python)\n- prettier formatting (javascript/svelte)\n- TypeScript type-checking (javascript/svelte)\n- eslint linting (javascript/svelte) [in progress]\n### Dynamic code tests\n- pytest (python unit and integration tests)\n- vitest (node-based unit and integration tests)\n- playwright (browser-based unit and integration tests)\n### Functional/acceptance tests\n- playwright (full end to end testing)\n- chromatic (visual testing) [in progress]\n- Accessibility testing [to do]\n## Supported environments and versions\nAll operating systems refer to the current runner variants supported by GitHub actions. All unspecified version segments (`x`) refer to latest. | Software | Version(s) | Operating System(s) |\n| -------- | --------------------- | --------------------------------- |\n| Python | `3.8.x` | `ubuntu-latest`, `windows-latest` |\n| Node | `18.x.x` | `ubuntu-latest` |\n| Browser | `playwright-chrome-x` | `ubuntu-latest` |\n## Test execution\nTests need to be executed in a number of environments and at different stages of the development cycle in order to be useful. The requirements for tests are as follows:\n- \\*\\*Locally\\*\\*: it is important that developers can easily run most tests locally to ensure a passing suite before making a PR. There are some exceptions to this, certain tests may require access to secret values which we cannot make available to all possible contributors for practical security reasons. It is reasonable that it isn't possible to run these tests but they should be disabled by default when running locally. - \\*\\*CI\\*\\* - It is \\_critical\\_ that all tests run successfully in CI with no exceptions. Not every test is required to pass to satisfy CI checks for practical reasons but it is required that all tests should run in CI and notify us if something unexpected happens in order for the development team to take appropriate action. For instructions on how to write and run tests see the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Managing defects\nAs we formalise our testing strategy and bring / keep our test up to standard, it is important that we have some principles on managing defects as they occur/ are reported. For now we can have one very simple rule:\n- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should \\_typically\\_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule."", ""gradio\n## 4.11.0\n### Features\n- [#6842](https://github.com/gradio-app/gradio/pull/6842) [`846d52d`](https://github.com/gradio-app/gradio/commit/846d52d1c92d429077382ce494eea27fd062d9f6) - Fix md highlight. Thanks [@pngwn](https://github.com/pngwn)! - [#6831](https://github.com/gradio-app/gradio/pull/6831) [`f3abde8`](https://github.com/gradio-app/gradio/commit/f3abde80884d96ad69b825020c46486d9dd5cac5) - Add an option to enable header links for markdown. Thanks [@pngwn](https://github.com/pngwn)! - [#6814](https://github.com/gradio-app/gradio/pull/6814) [`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d) - Refactor queue so that there are separate queues for each concurrency id. Thanks [@aliabid94](https://github.com/aliabid94)! - [#6809](https://github.com/gradio-app/gradio/pull/6809) [`1401d99`](https://github.com/gradio-app/gradio/commit/1401d99ade46d87da75b5f5808a3354c49f1d1ea) - Fix `ImageEditor` interaction story. Thanks [@hannahblair](https://github.com/hannahblair)! - [#6803](https://github.com/gradio-app/gradio/pull/6803) [`77c9003`](https://github.com/gradio-app/gradio/commit/77c900311e2ba37b8f849ce088ceb48aa196af18) - Fixes issue 5781: Enables specifying a caching directory for Examples. Thanks [@cswamy](https://github.com/cswamy)! - [#6823](https://github.com/gradio-app/gradio/pull/6823) [`67a2b7f`](https://github.com/gradio-app/gradio/commit/67a2b7f12cb06355fcc41e40d47e8b2ad211d7d1) - Fixed duplicate word (\""this this\""). Thanks [@Cassini-chris](https://github.com/Cassini-chris)!""]",Every bug fix in Gradio's testing strategy should typically be accompanied by a dynamic code test that failed before the fix and passes afterwards.,Dynamic code test
"How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?
","[""Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of 🤗 Accelerate is:\n- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible. - to have the same functions work for any distributed setup, so only have to learn one API. ### How does it work? To see how the library works in practice, let's have a look at each line of code we need to add to a training loop. ```python\naccelerator = Accelerator()\n```\nOn top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script. ```python\nmodel, optim, data = accelerator.prepare(model, optim, data)\n```\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torch.nn.Module`), optimizers (`torch.optim.Optimizer`) and dataloaders (`torch.data.dataloader.DataLoader`). #### Model\nModel preparation include wrapping it in the proper container (for instance `DistributedDataParallel`) and putting it on the proper device. Like with a regular distributed training, you will need to unwrap your model for saving, or to access its specific methods, which can be done with `accelerator.unwrap\\_model(model)`. #### Optimizer\nThe optimizer is also wrapped in a special container that will perform the necessary operations in the step to make mixed precision work."", ""Alternatively, use 🤗 Accelerate to gain full control over the training loop. Find the 🤗 Accelerate example\n[further down in this guide](#using-accelerate). While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can\nresult in a more pronounced training slowdown. Consider the following example. Let's say, the `per\\_device\\_train\\_batch\\_size=4`\nwithout gradient accumulation hits the GPU's limit. If you would like to train with batches of size 64, do not set the\n`per\\_device\\_train\\_batch\\_size` to 1 and `gradient\\_accumulation\\_steps` to 64. Instead, keep `per\\_device\\_train\\_batch\\_size=4`\nand set `gradient\\_accumulation\\_steps=16`. This results in the same effective batch size while making better use of\nthe available GPU resources. For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\nand [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957). ## Gradient Checkpointing\nSome large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used."", ""# Initialize accelerator and tensorboard logging\n... accelerator = Accelerator(\n... mixed\\_precision=config.mixed\\_precision,\n... gradient\\_accumulation\\_steps=config.gradient\\_accumulation\\_steps,\n... log\\_with=\""tensorboard\"",\n... project\\_dir=os.path.join(config.output\\_dir, \""logs\""),\n... )\n... if accelerator.is\\_main\\_process:\n... if config.output\\_dir is not None:\n... os.makedirs(config.output\\_dir, exist\\_ok=True)\n... if config.push\\_to\\_hub:\n... repo\\_id = create\\_repo(\n... repo\\_id=config.hub\\_model\\_id or Path(config.output\\_dir).name, exist\\_ok=True\n... ).repo\\_id\n... accelerator.init\\_trackers(\""train\\_example\"")\n... # Prepare everything\n... # There is no specific order to remember, you just need to unpack the\n... # objects in the same order you gave them to the prepare method. ... model, optimizer, train\\_dataloader, lr\\_scheduler = accelerator.prepare(\n... model, optimizer, train\\_dataloader, lr\\_scheduler\n... )\n... global\\_step = 0\n... # Now you train the model\n... for epoch in range(config.num\\_epochs):\n... progress\\_bar = tqdm(total=len(train\\_dataloader), disable=not accelerator.is\\_local\\_main\\_process)\n... progress\\_bar.set\\_description(f\""Epoch {epoch}\"")\n...""]","You can force mixed precision training when initializing the Accelerator in 🤗 Accelerate by passing the argument `fp16=True` to the `Accelerator()` initialization. Here is an example:

```python
accelerator = Accelerator(fp16=True)
```",By passing `fp16=True` to the Accelerator init.
"What is the purpose of tokenizers in the NLP pipeline?
","[""FrameworkSwitchCourse {fw} />\n# Tokenizers[[tokenizers]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. In NLP tasks, the data that is generally processed is raw text. Here's an example of such text:\n```\nJim Henson was a puppeteer\n```\nHowever, models can only process numbers, so we need to find a way to convert the raw text to numbers. That's what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation — that is, the one that makes the most sense to the model — and, if possible, the smallest representation. Let's take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization. ## Word-based[[word-based]]\nThe first type of tokenizer that comes to mind is \\_word-based\\_. It's generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:\n\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg)\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg)\n\nThere are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python's `split()` function:\n```py\ntokenized\\_text = \""Jim Henson was a puppeteer\"".split()\nprint(tokenized\\_text)\n```\n```python out\n['Jim', 'Henson', 'was', 'a', 'puppeteer']\n```\nThere are also variations of word tokenizers that have extra rules for punctuation."", ""p align=\""center\"">\n\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\n\n![Build](https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg)\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\n[![Doc](https://docs.rs/tokenizers/badge.svg)](https://docs.rs/tokenizers/)\n\nThe core of `tokenizers`, written in Rust. Provides an implementation of today's most used tokenizers, with a focus on performance and\nversatility. ## What is a Tokenizer\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`. The various steps of the pipeline are:\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\nthe [unicode normalization standards](https://unicode.org/reports/tr15/#Norm\\_Forms), such as `NFD` or `NFKC`. More details about how to use the `Normalizers` are available on the\n[Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\nsplitting text is simply on whitespace. 3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\n`BPE` or `WordPiece`."", ""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a PyTorch tensor.""]","The purpose of tokenizers in the NLP pipeline is to translate text into data that can be processed by the model. Since models can only process numbers, tokenizers convert raw text inputs into numerical data. This process involves finding the most meaningful representation of the text, which allows the model to understand and work with the data effectively.",To translate text into data that can be processed by the model.
"What is the purpose of the Safety Checker in the Diffusers library?
","[""We will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback. ## Scope\nThe Diffusers community will apply the following ethical guidelines to the project’s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. ## Ethical guidelines\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - \\*\\*Transparency\\*\\*: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions. - \\*\\*Consistency\\*\\*: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - \\*\\*Simplicity\\*\\*: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project’s goals lean and coherent. - \\*\\*Accessibility\\*\\*: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community. - \\*\\*Reproducibility\\*\\*: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library. - \\*\\*Responsibility\\*\\*: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers. ## Examples of implementations: Safety features and Mechanisms\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology."", ""This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\nis very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline. ## Tweakable, contributor-friendly over abstraction\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t\\_repeat\\_yourself). In short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers. Functions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable. \\*\\*However\\*\\*, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions. - Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions. - Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel. At Hugging Face, we call this design the \\*\\*single-file policy\\*\\* which means that almost all of the code of a certain class should be written in a single, self-contained file."", ""❤️\n🧪 Many of the SDXL ControlNet checkpoints are experimental, and there is a lot of room for improvement. Feel free to open an [Issue](https://github.com/huggingface/diffusers/issues/new/choose) and leave us feedback on how we can improve! Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## StableDiffusionXLControlNetXSPipeline\n[[autodoc]] StableDiffusionXLControlNetXSPipeline\n- all\n- \\_\\_call\\_\\_\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput""]","The Safety Checker in the Diffusers library is designed to help mitigate potential ethical and social risks associated with diffusion technology. It serves as a mechanism to ensure that the generated content adheres to safety standards and guidelines, promoting responsible use of the technology. The Safety Checker aims to enhance user trust and safety by filtering out harmful or inappropriate outputs from the models.",The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.
"What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?
","[""!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Interact with Discussions and Pull Requests\nThe `huggingface\\_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood. ## Retrieve Discussions and Pull Requests from the Hub\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(repo\\_id=\""bigscience/bloom\""):\n... print(f\""{discussion.num} - {discussion.title}, pr: {discussion.is\\_pull\\_request}\"")\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer\\_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]\n```\n`HfApi.get\\_repo\\_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(\n..."", ""Pull requests and Discussions\nHub Pull requests and Discussions allow users to do community contributions to repositories. Pull requests and discussions work the same for all the repo types. At a high level, the aim is to build a simpler version of other git hosts' (like GitHub's) PRs and Issues:\n- no forks are involved: contributors push to a special `ref` branch directly on the source repo. - there's no hard distinction between discussions and PRs: they are essentially the same so they are displayed in the same lists. - they are streamlined for ML (i.e. models/datasets/spaces repos), not arbitrary repos. \\_Note, Pull Requests and discussions can be enabled or disabled from the [repository settings](./repositories-settings#disabling-discussions-pull-requests)\\_\n## List\nBy going to the community tab in any repository, you can see all Discussions and Pull requests. You can also filter to only see the ones that are open. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list-dark.png)\n\n## View\nThe Discussion page allows you to see the comments from different users. If it's a Pull Request, you can see all the changes by going to the Files changed tab. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-view.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-view-dark.png)\n\n## Editing a Discussion / Pull request title\nIf you opened a PR or discussion, are the author of the repository, or have write access to it, you can edit the discussion title by clicking on the pencil button. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-edit-title.PNG)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-edit-title-dark.PNG)\n\n## Pin a Discussion / Pull Request\nIf you have write access to a repository, you can pin discussions and Pull Requests. Pinned discussions appear at the top of all the discussions. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pin.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pin-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pinned.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pinned-dark.png)\n\n## Lock a Discussion / Pull Request\nIf you have write access to a repository, you can lock discussions or Pull Requests. Once a discussion is locked, previous comments are still visible and users won't be able to add new comments. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-lock.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-lock-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-locked.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-locked-dark.png)\n\n## Comment edition and moderation\nIf you wrote a comment or have write access to the repository, you can edit the content of the comment from the contextual menu in the top-right corner of the comment box. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-edit.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-edit-dark.png)\n\nOnce the comment has been edited, a new link will appear above the comment. This link shows the edit history. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-link.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-link-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-history.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-history-dark.png)\n\nYou can also hide a comment. Hiding a comment is irreversible, and nobody will be able to see its content nor edit it anymore. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-hidden.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-hidden-dark.png)\n\nRead also [moderation](./moderation) to see how to report an abusive comment. ## Can I use Markdown and LaTeX in my comments and discussions? Yes! You can use Markdown to add formatting to your comments. Additionally, you can utilize LaTeX for mathematical typesetting, your formulas will be rendered with [KaTeX](https://katex.org/) before being parsed in Markdown."", ""This library allows users to interact with the Hub from Python. 1. Begin by installing the library:\n```bash\npip install huggingface\\_hub\n```\n2. To upload a dataset on the Hub in Python, you need to log in to your Hugging Face account:\n```bash\nhuggingface-cli login\n```\n3. Use the [`push\\_to\\_hub()`](https://huggingface.co/docs/datasets/main/en/package\\_reference/main\\_classes#datasets.DatasetDict.push\\_to\\_hub) function to help you add, commit, and push a file to your repository:\n```py\n>>> from datasets import load\\_dataset\n>>> dataset = load\\_dataset(\""stevhliu/demo\"")\n# dataset = dataset.map(...) # do all your processing here\n>>> dataset.push\\_to\\_hub(\""stevhliu/processed\\_demo\"")\n```\nTo set your dataset as private, set the `private` parameter to `True`. This parameter will only work if you are creating a repository for the first time. ```py\n>>> dataset.push\\_to\\_hub(\""stevhliu/private\\_processed\\_demo\"", private=True)\n```\nTo add a new configuration (or subset) to a dataset or to add a new split (train/validation/test), please refer to the [`Dataset.push\\_to\\_hub`] documentation.""]",The `HfApi` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.,HfApi
"What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?
","[""--\ntitle: \""Introducing Skops\""\nthumbnail: /blog/assets/94\\_skops/introducing\\_skops.png\nauthors:\n- user: merve\n- user: adrin\n- user: BenjaminB\n---\n# Introducing Skops\n## Introducing Skops\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others. Let's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production. ```python\n# let's import the libraries first\nimport sklearn\nfrom sklearn.datasets import load\\_breast\\_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model\\_selection import train\\_test\\_split\n# Load the data and split\nX, y = load\\_breast\\_cancer(as\\_frame=True, return\\_X\\_y=True)\nX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(\nX, y, test\\_size=0.3, random\\_state=42\n)\n# Train the model\nmodel = DecisionTreeClassifier().fit(X\\_train, y\\_train)\n```\nYou can use any model filename and serialization method, like `pickle` or `joblib`.""]",The new library introduced by Hugging Face for hosting scikit-learn models is called Skops.,Skops
"What is the purpose of Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""🎉 To learn more about how to use your new model, the following guides may be helpful:\n- Learn how to [load Textual Inversion embeddings](../using-diffusers/loading\\_adapters) and also use them as negative embeddings. - Learn how to use [Textual Inversion](textual\\_inversion\\_inference) for inference with Stable Diffusion 1/2 and Stable Diffusion XL."", ""However, we observed that fine-tuning the encoder produces better results. We experimented with this approach after seeing it used in other Dreambooth implementations, and the results are striking! Frozen text encoder\n![Frozen text encoder](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/f\\_froxen\\_encoder.jpg)\nFine-tuned text encoder\n![Fine-tuned text encoder](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/g\\_unfrozen\\_encoder.jpg)\nFine-tuning the text encoder produces the best results, especially with faces. It generates more realistic images, it's less prone to overfitting and it also achieves better prompt interpretability, being able to handle more complex prompts. ## Epilogue: Textual Inversion + Dreambooth\nWe also ran a final experiment where we combined [Textual Inversion](https://textual-inversion.github.io) with Dreambooth. Both techniques have a similar goal, but their approaches are different. In this experiment we first ran textual inversion for 2000 steps. From that model, we then ran Dreambooth for an additional 500 steps using a learning rate of `1e-6`. These are the results:\n![Textual Inversion + Dreambooth](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/h\\_textual\\_inversion\\_dreambooth.jpg)\nWe think the results are much better than doing plain Dreambooth but not as good as when we fine-tune the whole text encoder. It seems to copy the style of the training images a bit more, so it could be overfitting to them. We didn't explore this combination further, but it could be an interesting alternative to improve Dreambooth and still fit the process in a 16GB GPU. Feel free to explore and tell us about your results!""]","The purpose of Textual Inversion is to personalize text-to-image models, like stable diffusion, using just 3-5 examples of your own images. It allows users to adapt these models to generate images that reflect specific concepts or styles based on the provided examples. This method enhances the model's ability to interpret prompts and produce more relevant and realistic images.",Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.
"What is the recommended multiple of batch size for fp16 data type on an A100 GPU?
","[""For instance, for fp16 data type a multiple of 8 is recommended, unless\nit's an A100 GPU, in which case use multiples of 64. For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup. ## Gradient Accumulation\nThe \\*\\*gradient accumulation\\*\\* method aims to calculate gradients in smaller increments instead of computing them for the\nentire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward\nand backward passes through the model and accumulating the gradients during the process. Once a sufficient number of\ngradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\nbecomes possible to increase the \\*\\*effective batch size\\*\\* beyond the limitations imposed by the GPU's memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can\nslow down the training process. You can enable gradient accumulation by adding the `gradient\\_accumulation\\_steps` argument to [`TrainingArguments`]:\n```py\ntraining\\_args = TrainingArguments(per\\_device\\_train\\_batch\\_size=1, gradient\\_accumulation\\_steps=4, \\*\\*default\\_args)\n```\nIn the above example, your effective batch size becomes 4."", ""It was run on a single NVIDIA A100-SXM4-80GB GPU with a prompt length of 512. The model we used was `meta-llama/Llama-2-13b-hf`. with batch size = 1:\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None |None|None |None |26.0 |36.958 |27.058 |29152.98 |\n|gptq |False |4 |128 |exllama|36.2 |33.711 |29.663 |10484.34 |\n|bitsandbytes|None |4|None |None |37.64 |52.00 |19.23 |11018.36 |\nwith batch size = 16:\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None |None|None |None |26.0 |69.94 |228.76 |53986.51 |\n|gptq |False |4 |128 |exllama|36.2 |95.41 |167.68 |34777.04 |\n|bitsandbytes|None |4|None |None |37.64 |113.98 |140.38 |35532.37 |\nFrom the benchmark, we can see that bitsandbyes and GPTQ are equivalent, with GPTQ being slightly faster for large batch size."", ""The impact of `fp16` on latency is less marked with `batch\\_size = 1`, but here it is of enormous interest as it can reduce latency by almost half, and almost double throughput! # Concluding remarks\nThis blog post showcased a few simple optimization tricks bundled in the 🤗 ecosystem. Using anyone of these techniques, or a combination of all three, can greatly improve Bark inference speed and memory footprint. \\* You can use the large version of Bark without any performance degradation and a footprint of just 2GB instead of 5GB, 15% faster, \\*\\*using 🤗 Better Transformer and CPU offload\\*\\*. \\* Do you prefer high throughput? \\*\\*Batch by 8 with 🤗 Better Transformer and half-precision\\*\\*. \\* You can get the best of both worlds by using \\*\\*fp16, 🤗 Better Transformer and CPU offload\\*\\*!""]","For fp16 data type on an A100 GPU, it is recommended to use multiples of 64.",64
"How do you run a Gradio Blocks app in reload mode using a Python IDE?
","[""Developing Faster with Auto-Reloading\n\\*\\*Prerequisite\\*\\*: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control). This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks. ## Why Auto-Reloading? When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes. To make it faster and more convenient to write your code, we've made it easier to \""reload\"" your Gradio apps instantly when you are developing in a \\*\\*Python IDE\\*\\* (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \""magic command\"" that allows you to re-run cells faster if you use \\*\\*Jupyter Notebooks\\*\\* (or any similar environment like Colab). This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster. ## Python IDE Reload 🔥\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Markdown(\""# Greetings from Gradio!\"")\ninp = gr.Textbox(placeholder=\""What is your name?\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""Welcome, {x}!\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`."", ""Instead of doing this, you can run your code in \\*\\*reload mode\\*\\* by changing 1 word: `python` to `gradio`:\nIn the terminal, run `gradio run.py`. That's it! Now, you'll see that after you'll see something like this:\n```bash\nWatching: '/Users/freddy/sources/gradio/gradio', '/Users/freddy/sources/gradio/demo/'\nRunning on local URL: http://127.0.0.1:7860\n```\nThe important part here is the line that says `Watching...` What's happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically 🥳\n⚠️ Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show\\_error` in `launch()` will not be reflected in the app. There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n```python\nimport gradio as gr\nwith gr.Blocks() as my\\_demo:\ngr.Markdown(\""# Greetings from Gradio!\"")\ninp = gr.Textbox(placeholder=\""What is your name?\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""Welcome, {x}!\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\nmy\\_demo.launch()\n```\nThen you would launch it in reload mode like this: `gradio run.py my\\_demo`."", ""WARNING: The --reload flag should not be used in production on Windows. ```\n这里最重要的一行是 `正在观察 ...`。这里发生的情况是 Gradio 将观察 `run.py` 文件所在的目录，如果文件发生更改，它将自动为您重新运行文件。因此，您只需专注于编写代码，Gradio 演示将自动刷新 🥳\n⚠️ 警告：`gradio` 命令不会检测传递给 `launch()` 方法的参数，因为在重新加载模式下从未调用 `launch()` 方法。例如，设置 `launch()` 中的 `auth` 或 `show\\_error` 不会在应用程序中反映出来。\n当您使用重新加载模式时，请记住一件重要的事情：Gradio 专门查找名为 `demo` 的 Gradio Blocks/Interface 演示。如果您将演示命名为其他名称，您需要在代码中的第二个参数中传入演示的 FastAPI 应用程序的名称。对于 Gradio 演示，可以使用 `.app` 属性访问 FastAPI 应用程序。因此，如果您的 `run.py` 文件如下所示：\n```python\nimport gradio as gr\nwith gr.Blocks() as my\\_demo:\ngr.Markdown(\""# 来自Gradio的问候！\"")\ninp = gr.Textbox(placeholder=\""您叫什么名字？\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""欢迎，{x}！\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\nmy\\_demo.launch()\n```\n那么您可以这样启动它：`gradio run.py my\\_demo.app`。\nGradio默认使用UTF-8编码格式。对于\\*\\*重新加载模式\\*\\*，如果你的脚本使用的是除UTF-8以外的编码（如GBK）：\n1. 在Python脚本的编码声明处指定你想要的编码格式，如：`# -\\*- coding: gbk -\\*-`\n2. 确保你的代码编辑器识别到该格式。\n3. 执行：`gradio run.py --encoding gbk`\n🔥 如果您的应用程序接受命令行参数，您也可以传递它们。下面是一个例子：\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# 欢迎 {args.name}！\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\n您可以像这样运行它：`gradio run.py --name Gretel`\n作为一个小提示，只要更改了 `run.py` 源代码或 Gradio 源代码，自动重新加载就会发生。这意味着如果您决定[为 Gradio 做贡献](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md)，这将非常有用 ✅\n## Jupyter Notebook 魔法命令🔮\n如果您使用 Jupyter Notebooks（或 Colab Notebooks 等）进行开发，我们也为您提供了一个解决方案！\n我们开发了一个 \\*\\*magic command 魔法命令\\*\\*，可以为您创建和运行一个 Blocks 演示。要使用此功能，在笔记本顶部加载 gradio 扩展：\n`%load\\_ext gradio`\n然后，在您正在开发 Gradio 演示的单元格中，只需在顶部写入魔法命令\\*\\*`%%blocks`\\*\\*，然后像平常一样编写布局和组件：\n```py\n%%blocks\nimport gradio as gr\ngr.Markdown(\""# 来自Gradio的问候！\"")\ninp = gr.Textbox(placeholder=\""您叫什么名字？\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""欢迎，{x}！\"",\ninputs=inp,\noutputs=out)\n```\n请注意：\n- 您不需要放置样板代码 `with gr.Blocks() as demo:` 和 `demo.launch()` — Gradio 会自动为您完成！\n- 每次重新运行单元格时，Gradio 都将在相同的端口上重新启动您的应用程序，并使用相同的底层网络服务器。这意味着您将比正常重新运行单元格更快地看到变化。\n下面是在 Jupyter Notebook 中的示例：\n![](https://i.ibb.co/nrszFws/Blocks.gif)\n🪄这在 colab 笔记本中也适用！[这是一个 colab 笔记本](https://colab.research.google.com/drive/1jUlX1w7JqckRHVE-nbDyMPyZ7fYD8488?authuser=1#scrollTo=zxHYjbCTTz\\_5)，您可以在其中看到 Blocks 魔法效果。尝试进行一些更改并重新运行带有 Gradio 代码的单元格！\nNotebook Magic 现在是作者构建 Gradio 演示的首选方式。无论您如何编写 Python 代码，我们都希望这两种方法都能为您提供更好的 Gradio 开发体验。\n---\n## 下一步\n既然您已经了解了如何使用 Gradio 快速开发，请开始构建自己的应用程序吧！\n如果你正在寻找灵感，请尝试浏览其他人用 Gradio 构建的演示，[浏览 Hugging Face Spaces](http://hf.space/) 🤗""]","To run a Gradio Blocks app in reload mode using a Python IDE, you need to change the command you use to launch your app. Instead of running your script with `python run.py`, you should use the command `gradio run.py`. 

Here’s a brief overview of the steps:

1. Make sure your code is structured properly. For example, your `run.py` file might look like this:

   ```python
   import gradio as gr
   with gr.Blocks() as demo:
       gr.Markdown(""# Greetings from Gradio!"")
       inp = gr.Textbox(placeholder=""What is your name?"")
       out = gr.Textbox()
       inp.change(fn=lambda x: f""Welcome, {x}!"", inputs=inp, outputs=out)
   if __name__ == ""__main__"":
       demo.launch()
   ```

2. In the terminal, run the command `gradio run.py`. 

3. You will see output indicating that Gradio is watching for changes in your directory, and it will automatically rerun the app whenever you make changes to your code.

Note: If your demo is named something other than `demo`, you will need to specify the name of your demo as the second parameter, like this: `gradio run.py my_demo`.",Run `gradio run.py` in the terminal.
"How can you install the Hugging Face Unity API in your Unity project?
","[""--\ntitle: \""How to Install and Use the Hugging Face Unity API\""\nthumbnail: /blog/assets/124\\_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n# How to Install and Use the Hugging Face Unity API\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API. ## Installation\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif)\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens). 7. Test the API key by clicking `Test API key` in the API Wizard. 8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API\\_URL` field."", ""### 4. Speech Recognition\nNext, we'll want to use the Hugging Face Unity API to run speech recognition on our encoded audio. To do so, we'll create a `SendRecording()` method:\n```\nusing HuggingFace.API;\nprivate void SendRecording() {\nHuggingFaceAPI.AutomaticSpeechRecognition(bytes, response => {\ntext.color = Color.white;\ntext.text = response;\n}, error => {\ntext.color = Color.red;\ntext.text = error;\n});\n}\n```\nThis will send the encoded audio to the API, displaying the response in white if successful, otherwise the error message in red. Don't forget to call `SendRecording()` at the end of the `StopRecording()` method:\n```\nprivate void StopRecording() {\n/\\* other code \\*/\nSendRecording();\n}\n```\n### 5. Final Touches\nFinally, let's improve the UX of this demo a bit using button interactability and status messages. The Start and Stop buttons should only be interactable when appropriate, i.e. when a recording is ready to be started/stopped. Then, set the response text to a simple status message while recording or waiting for the API. The finished script should look something like this:\n```\nusing System.IO;\nusing HuggingFace.API;\nusing TMPro;\nusing UnityEngine;\nusing UnityEngine.UI;\npublic class SpeechRecognitionTest : MonoBehaviour {\n[SerializeField] private Button startButton;\n[SerializeField] private Button stopButton;\n[SerializeField] private TextMeshProUGUI text;\nprivate AudioClip clip;\nprivate byte[] bytes;\nprivate bool recording;\nprivate void Start() {\nstartButton.onClick.AddListener(StartRecording);\nstopButton.onClick.AddListener(StopRecording);\nstopButton.interactable = false;\n}\nprivate void Update() {\nif (recording && Microphone.GetPosition(null) >= clip.samples) {\nStopRecording();\n}\n}\nprivate void StartRecording() {\ntext.color = Color.white;\ntext.text = \""Recording...\"";\nstartButton.interactable = false;\nstopButton.interactable = true;\nclip = Microphone.Start(null, false, 10, 44100);\nrecording = true;\n}\nprivate void StopRecording() {\nvar position = Microphone.GetPosition(null);\nMicrophone.End(null);\nvar samples = new float[position \\* clip.channels];\nclip.GetData(samples, 0);\nbytes = EncodeAsWAV(samples, clip.frequency, clip.channels);\nrecording = false;\nSendRecording();\n}\nprivate void SendRecording() {\ntext.color = Color.yellow;\ntext.text = \""Sending...\"";\nstopButton.interactable = false;\nHuggingFaceAPI.AutomaticSpeechRecognition(bytes, response => {\ntext.color = Color.white;\ntext.text = response;\nstartButton.interactable = true;\n}, error => {\ntext.color = Color.red;\ntext.text = error;\nstartButton.interactable = true;\n});\n}\nprivate byte[] EncodeAsWAV(float[] samples, int frequency, int channels) {\nusing (var memoryStream = new MemoryStream(44 + samples.Length \\* 2)) {\nusing (var writer = new BinaryWriter(memoryStream)) {\nwriter.Write(\""RIFF\"".ToCharArray());\nwriter.Write(36 + samples.Length \\* 2);\nwriter.Write(\""WAVE\"".ToCharArray());\nwriter.Write(\""fmt \"".ToCharArray());\nwriter.Write(16);\nwriter.Write((ushort)1);\nwriter.Write((ushort)channels);\nwriter.Write(frequency);\nwriter.Write(frequency \\* channels \\* 2);\nwriter.Write((ushort)(channels \\* 2));\nwriter.Write((ushort)16);\nwriter.Write(\""data\"".ToCharArray());\nwriter.Write(samples.Length \\* 2);\nforeach (var sample in samples) {\nwriter.Write((short)(sample \\* short.MaxValue));\n}\n}\nreturn memoryStream.ToArray();\n}\n}\n}\n```\nCongratulations, you can now use state-of-the-art Speech Recognition in Unity! If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!"", ""Congratulations! Refresh your Space. You should now be able to play your game in a Hugging Face Space. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!""]","To install the Hugging Face Unity API in your Unity project, follow these steps:

1. Open your Unity project.
2. Go to `Window` -> `Package Manager`.
3. Click `+` and select `Add Package from git URL`.
4. Enter `https://github.com/huggingface/unity-api.git`.
5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.
6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).
7. Test the API key by clicking `Test API key` in the API Wizard.
8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.","To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`."
"What is the pretraining objective of the Wav2Vec2 context network?
","[""Wav2Vec2 Contrastive Loss PreTraining examples\nThe following example showcases how to pretrain a wav2vec2 model using the JAX/Flax backend. Pretraining Wav2Vec2 is rather complex, so it is highly recommended to read the\n[official paper](https://arxiv.org/abs/2006.11477). JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU. Models written in JAX/Flax are \\*\\*immutable\\*\\* and updated in a purely functional\nway which enables simple and efficient model parallelism. `run\\_wav2vec2\\_pretrain\\_flax.py` is a lightweight example of how to download and preprocess a dataset from the 🤗 Datasets library or use your own files (jsonlines or csv), then pretrain the wav2vec2 architectures above on it. For custom datasets in `jsonlines` format please see: [the Datasets documentation](https://huggingface.co/docs/datasets/loading\\_datasets#json-files) and you also will find examples of these below. Let's start by creating a model repository to save the trained model and logs. Here we call the model `\""wav2vec2-base-robust\""`, but you can change the model name as you like. You can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\nyou are logged in) or via the command line:\n```\nhuggingface-cli repo create wav2vec2-base-robust\n```\nNext we clone the model repository to add the tokenizer and model files. ```\ngit clone https://huggingface.co//wav2vec2-base-robust\n```\nTo ensure that all tensorboard traces will be uploaded correctly, we need to\ntrack them. You can run the following command inside your model repo to do so. ```\ncd wav2vec2-base-robust\ngit lfs track \""\\*tfevents\\*\""\n```\nGreat, we have set up our model repository. During training, we will automatically\npush the training logs and model weights to the repo. Next, let's add a symbolic link to the `run\\_wav2vec2\\_pretrain\\_flax`. ```bash\nexport MODEL\\_DIR=\""./wav2vec2-base-robust\""\nln -s ~/transformers/examples/research\\_projects/jax-projects/wav2vec2/run\\_wav2vec2\\_pretrain\\_flax.py ./\n```\n### Create the model configuration\nLet's first create the model configuration and store it in the model repository."", ""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Wav2Vec2\n## Overview\nThe Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. The abstract from the paper is the following:\n\\*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\nrecognition with limited amounts of labeled data.\\*\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). ## Usage tips\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\nusing [`Wav2Vec2CTCTokenizer`]. ## Resources\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Wav2Vec2."", ""Using the Buckwalter format, text is also logged in Arabic abjad. `--target\\_feature\\_extractor\\_sampling\\_rate` resamples audio to target feature extractor's sampling rate (16kHz). `--max\\_duration\\_in\\_seconds=\""15\""` filters out examples whose audio is longer than the specified limit,\nwhich helps with capping GPU memory usage. ### DeepSpeed Integration\nTo learn how to deploy Deepspeed Integration please refer to [this guide](https://huggingface.co/transformers/main/main\\_classes/deepspeed.html#deepspeed-trainer-integration). But to get started quickly all you need is to install:\n```\npip install deepspeed\n```\nand then use the default configuration files in this directory:\n\\* `ds\\_config\\_wav2vec2\\_zero2.json`\n\\* `ds\\_config\\_wav2vec2\\_zero3.json`\nHere are examples of how you can use DeepSpeed:\n(edit the value for `--num\\_gpus` to match the number of GPUs you have)\nZeRO-2:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 2 \\\nrun\\_asr.py \\\n--output\\_dir=output\\_dir --num\\_train\\_epochs=2 --per\\_device\\_train\\_batch\\_size=2 \\\n--per\\_device\\_eval\\_batch\\_size=2 --evaluation\\_strategy=steps --save\\_steps=500 --eval\\_steps=100 \\\n--logging\\_steps=5 --learning\\_rate=5e-4 --warmup\\_steps=3000 \\\n--model\\_name\\_or\\_path=patrickvonplaten/wav2vec2\\_tiny\\_random\\_robust \\\n--dataset\\_name=hf-internal-testing/librispeech\\_asr\\_dummy --dataset\\_config\\_name=clean \\\n--train\\_split\\_name=validation --validation\\_split\\_name=validation --orthography=timit \\\n--preprocessing\\_num\\_workers=1 --group\\_by\\_length --freeze\\_feature\\_extractor --verbose\\_logging \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero2.json\n```\nFor ZeRO-2 with more than 1 gpu you need to use (which is already in the example configuration file):\n```\n\""zero\\_optimization\"": {\n... \""find\\_unused\\_parameters\"": true,\n... }\n```\nZeRO-3:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 2 \\\nrun\\_asr.py \\\n--output\\_dir=output\\_dir --num\\_train\\_epochs=2 --per\\_device\\_train\\_batch\\_size=2 \\\n--per\\_device\\_eval\\_batch\\_size=2 --evaluation\\_strategy=steps --save\\_steps=500 --eval\\_steps=100 \\\n--logging\\_steps=5 --learning\\_rate=5e-4 --warmup\\_steps=3000 \\\n--model\\_name\\_or\\_path=patrickvonplaten/wav2vec2\\_tiny\\_random\\_robust \\\n--dataset\\_name=hf-internal-testing/librispeech\\_asr\\_dummy --dataset\\_config\\_name=clean \\\n--train\\_split\\_name=validation --validation\\_split\\_name=validation --orthography=timit \\\n--preprocessing\\_num\\_workers=1 --group\\_by\\_length --freeze\\_feature\\_extractor --verbose\\_logging \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero3.json\n```\n### Pretraining Wav2Vec2\nThe `run\\_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss objective (see official [paper](https://arxiv.org/abs/2006.11477) for more information). It is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed (please refer to [this guide](https://huggingface.co/transformers/main/main\\_classes/deepspeed.html#deepspeed-trainer-integration) for more information). Here is an example of how you can use DeepSpeed ZeRO-2 to pretrain a small Wav2Vec2 model:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 4 run\\_pretrain.py \\\n--output\\_dir=\""./wav2vec2-base-libri-100h\"" \\\n--num\\_train\\_epochs=\""3\"" \\\n--per\\_device\\_train\\_batch\\_size=\""32\"" \\\n--per\\_device\\_eval\\_batch\\_size=\""32\"" \\\n--gradient\\_accumulation\\_steps=\""2\"" \\\n--save\\_total\\_limit=\""3\"" \\\n--save\\_steps=\""500\"" \\\n--logging\\_steps=\""10\"" \\\n--learning\\_rate=\""5e-4\"" \\\n--weight\\_decay=\""0.01\"" \\\n--warmup\\_steps=\""3000\"" \\\n--model\\_name\\_or\\_path=\""patrickvonplaten/wav2vec2-base-libri-100h\"" \\\n--dataset\\_name=\""librispeech\\_asr\"" \\\n--dataset\\_config\\_name=\""clean\"" \\\n--train\\_split\\_name=\""train.100\"" \\\n--preprocessing\\_num\\_workers=\""4\"" \\\n--max\\_duration\\_in\\_seconds=\""10.0\"" \\\n--group\\_by\\_length \\\n--verbose\\_logging \\\n--fp16 \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero2.json \\\n```\n### Forced Alignment\nCharacter level forced alignment for audio and text pairs with wav2vec2 models finetuned on ASR task for a specific language. Inspired by [this](https://pytorch.org/tutorials/intermediate/forced\\_alignment\\_with\\_torchaudio\\_tutorial.html) Pytorch tutorial. #### Input Formats\nInput format in script.txt Input format in wavs directroy\n0000 sentence1 0000.wav\n0001 sentence2 0001.wav\n#### Output Format\nOutput directory will contain 0000.txt and 0001.txt. Each file will have format like below\nchar score start\\_ms end\\_ms\nh 0.25 1440 1520\n#### Run command\n```\npython alignment.py \\\n--model\\_name=\""arijitx/wav2vec2-xls-r-300m-bengali\"" \\\n--wav\\_dir=\""./wavs\""\n--text\\_file=\""script.txt\"" \\\n--input\\_wavs\\_sr=48000 \\\n--output\\_dir=\""./out\\_alignment\"" \\\n--cuda\n```""]","The pretraining objective of the Wav2Vec2 context network is based on a contrastive loss. This involves learning powerful representations from speech audio alone, followed by fine-tuning on transcribed speech. The model masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations, which are jointly learned. For more detailed information, you can refer to the official paper.",The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.
"What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?
","[""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a TensorFlow tensor."", ""Мы хотим, чтобы Transformers позволил разработчикам, исследователям, студентам, профессорам, инженерам и всем желающим\nсоздавать проекты своей мечты. Чтобы отпраздновать 100 тысяч звезд Transformers, мы решили сделать акцент на сообществе, и создали страницу [awesome-transformers](./awesome-transformers.md), на которой перечислены 100\nневероятных проектов, созданных с помощью transformers. Если вы являетесь владельцем или пользователем проекта, который, по вашему мнению, должен быть включен в этот список, пожалуйста, откройте PR для его добавления! ## Если вы хотите получить индивидуальную поддержку от команды Hugging Face\n[![HuggingFace Expert Acceleration Program](https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png)](https://huggingface.co/support)  \n## Быстрый гайд\nДля использования модели на заданном входе (текст, изображение, звук, ...) мы предоставляем API `pipeline`. Конвейеры объединяют предварительно обученную модель с препроцессингом, который использовался при ее обучении. Вот как можно быстро использовать конвейер для классификации положительных и отрицательных текстов:\n```python\n>>> from transformers import pipeline\n# Выделение конвейера для анализа настроений\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('Мы очень рады представить конвейер в transformers.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```\nВторая строка кода загружает и кэширует предварительно обученную модель, используемую конвейером, а третья оценивает ее на заданном тексте."", ""For instance, if the review was negative, I could create a feature which would contact the customer, apologize for the poor quality of service, and inform him/her that our support team will contact him/her as soon as possible to assist him and hopefully fix the problem. Since I don't plan to get more than 2,000 requests per month, I didn't impose any performance constraints regarding the time and the scalability. ## The Transformers library\nI was a bit confused at the beginning when I downloaded the .h5 file. I thought it would be compatible with `tensorflow.keras.models.load\\_model`, but this wasn't the case. After a few minutes of research I was able to figure out that the file was a weights checkpoint rather than a Keras model. After that, I tried out the API that Hugging Face offers and read a bit more about the pipeline feature they offer. Since the results of the API & the pipeline were great, I decided that I could serve the model through the pipeline on my own server. Below is the [official example](https://github.com/huggingface/transformers#quick-tour) from the Transformers GitHub page. ```python\nfrom transformers import pipeline\n# Allocate a pipeline for sentiment-analysis\nclassifier = pipeline('sentiment-analysis')\nclassifier('We are very happy to include pipeline into the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9978193640708923}]\n```\n## Deploy transformers to Google Cloud\n> GCP is chosen as it is the cloud environment I am using in my personal organization.""]",The default checkpoint used by the sentiment analysis pipeline in the Transformers library is `distilbert base uncased finetuned sst2 english`.,distilbert base uncased finetuned sst2 english
"What is the purpose of the notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi""?
","[""Note that while [CUDA graphs](https://developer.nvidia.com/blog/cuda-graphs/) are not currently compatible with model parallelism in DeepSpeed (DeepSpeed v0.8.2, see [here](https://github.com/microsoft/DeepSpeed/blob/v0.8.2/deepspeed/inference/engine.py#L158)), HPU graphs are supported in Habana's DeepSpeed fork. All benchmarks are doing [greedy generation](https://huggingface.co/blog/how-to-generate#greedy-search) of 100 token outputs. The input prompt is:\n> \""DeepSpeed is a machine learning framework\""\nwhich consists of 7 tokens with BLOOM's tokenizer. The results for inference latency are displayed in the table below (the unit is \\*seconds\\*). | Model | Number of devices | Gaudi2 latency (seconds) | A100-80GB latency (seconds) | First-gen Gaudi latency (seconds) |\n|:-----------:|:-----------------:|:-------------------------:|:-----------------:|:----------------------------------:|\n| BLOOMZ | 8 | 3.103 | 4.402 | / |\n| BLOOMZ-7B | 8 | 0.734 | 2.417 | 3.321 |\n| BLOOMZ-7B | 1 | 0.772 | 2.119 | 2.387 |\n\\*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, leading to a\\* x\\*1.42 speedup on BLOOMZ with Gaudi2 compared to A100.\\*\nThe Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. \\*\\*For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB\\*\\*. Smaller checkpoints present interesting results too. \\*\\*Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!\\*\\* It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device. We also ran these models on first-gen Gaudi. While it is slower than Gaudi2, it is interesting from a price perspective as a DL1 instance on AWS costs approximately 13\\$ per hour. Latency for BLOOMZ-7B on first-gen Gaudi is 2.387 seconds. Thus, \\*\\*first-gen Gaudi offers for the 7-billion checkpoint a better price-performance ratio than A100\\*\\* which costs more than 30\\$ per hour! We expect the Habana team will optimize the performance of these models in the upcoming SynapseAI releases. For example, in our last benchmark, we saw that [Gaudi2 performs Stable Diffusion inference 2.2x faster than A100](https://huggingface.co/blog/habana-gaudi-2-benchmark#generating-images-from-text-with-stable-diffusion) and this has since been improved further to 2.37x with the latest optimizations provided by Habana. We will update these numbers as new versions of SynapseAI are released and integrated within Optimum Habana."", ""Why would you want to use DeepSpeed with just one GPU? 1. It has a ZeRO-offload feature which can delegate some computations and memory to the host's CPU and RAM, and thus\nleave more GPU resources for model's needs - e.g. larger batch size, or enabling a fitting of a very big model which\nnormally won't fit. 2. It provides a smart GPU memory management system, that minimizes memory fragmentation, which again allows you to fit\nbigger models and data batches. While we are going to discuss the configuration in details next, the key to getting a huge improvement on a single GPU\nwith DeepSpeed is to have at least the following configuration in the configuration file:\n```json\n{\n\""zero\\_optimization\"": {\n\""stage\"": 2,\n\""offload\\_optimizer\"": {\n\""device\"": \""cpu\"",\n\""pin\\_memory\"": true\n},\n\""allgather\\_partitions\"": true,\n\""allgather\\_bucket\\_size\"": 2e8,\n\""reduce\\_scatter\"": true,\n\""reduce\\_bucket\\_size\"": 2e8,\n\""overlap\\_comm\"": true,\n\""contiguous\\_gradients\"": true\n}\n}\n```\nwhich enables optimizer offload and some other important features."", ""--\ntitle: \""Accelerate Large Model Training using DeepSpeed\""\nthumbnail: /blog/assets/83\\_accelerate\\_deepspeed/deepspeed-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n# Accelerate Large Model Training using DeepSpeed\nIn this post we will look at how we can leverage the \\*\\*[Accelerate](https://github.com/huggingface/accelerate)\\*\\* library for training large models which enables users to leverage the ZeRO features of \\*\\*[DeeSpeed](https://www.deepspeed.ai)\\*\\*. # Motivation 🤗\n\\*\\*Tired of Out of Memory (OOM) errors while trying to train large models? We've got you covered. Large models are very performant [1] but difficult to train with the available hardware. To get the most of the available hardware for training large models one can leverage Data Parallelism using ZeRO - Zero Redundancy Optimizer [2]\\*\\*. Below is a short description of Data Parallelism using ZeRO with diagram from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)\n![ZeRO Data Parallelism](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero.png)\n(Source: [link](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/))\na.""]","The notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi"" is designed to guide users on leveraging the DeepSpeed framework for training large models, specifically those with billions of parameters, on Habana Gaudi hardware. It provides practical examples and configurations to optimize the training process, taking advantage of features like ZeRO (Zero Redundancy Optimizer) to efficiently manage memory and computational resources. This enables users to effectively utilize the capabilities of Habana Gaudi for large-scale model training.",To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.
"What command line module does PyTorch provide to run a script on multiple GPUs?
","[""\\* correct / len(test\\_loader.dataset)}')\n```\nTypically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook. However, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through \\*distributed\\* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\n## PyTorch Distributed Data Parallelism\nAs the name implies, `torch.distributed` is meant to work on \\*distributed\\* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both. To convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp\\_tutorial.html)\nFirst a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through\n> Note: for this section of the tutorial it should be assumed these are sent in python script files. Later on a launcher using Accelerate will be discussed that removes this necessity\n```python\nimport os\nimport torch.distributed as dist\ndef setup(rank, world\\_size):\n\""Sets up the process group and configuration for PyTorch Distributed Data Parallelism\""\nos.environ[\""MASTER\\_ADDR\""] = 'localhost'\nos.environ[\""MASTER\\_PORT\""] = \""12355\""\n# Initialize the process group\ndist.init\\_process\\_group(\""gloo\"", rank=rank, world\\_size=world\\_size)\ndef cleanup():\n\""Cleans up the distributed environment\""\ndist.destroy\\_process\\_group()\n```\nThe last piece of the puzzle is \\*how do I send my data and model to another GPU?\\*\nThis is where the `DistributedDataParallel` module comes into play. It will copy your model onto each GPU, and when `loss.backward()` is called the backpropagation is performed and the resulting gradients across all these copies of the model will be averaged/reduced. This ensures each device has the same weights post the optimizer step. Below is an example of our training setup, refactored as a function, with this capability:\n> Note: Here rank is the overall rank of the current GPU compared to all the other GPUs available, meaning they have a rank of `0 -> n-1`\n```python\nfrom torch.nn.parallel import DistributedDataParallel as DDP\ndef train(model, rank, world\\_size):\nsetup(rank, world\\_size)\nmodel = model.to(rank)\nddp\\_model = DDP(model, device\\_ids=[rank])\noptimizer = optim.AdamW(ddp\\_model.parameters(), lr=1e-3)\n# Train for one epoch\nmodel.train()\nfor batch\\_idx, (data, target) in enumerate(train\\_loader):\ndata, target = data.to(device), target.to(device)\noutput = model(data)\nloss = F.nll\\_loss(output, target)\nloss.backward()\noptimizer.step()\noptimizer.zero\\_grad()\ncleanup()\n```\nThe optimizer needs to be declared based on the model \\*on the specific device\\* (so `ddp\\_model` and not `model`) for all of the gradients to properly be calculated. Lastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\n```bash\ntorchrun --nproc\\_per\\_node=2 --nnodes=1 example\\_script.py\n```\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch."", "". ! - \\; \\: \\\"" “ % ‘ ” � \\\n--fp16 \\\n--group\\_by\\_length \\\n--push\\_to\\_hub \\\n--do\\_train --do\\_eval\n```\nOn a single V100 GPU, this script should run in \\*ca.\\* 1 hour 20 minutes and yield a CTC loss of \\*\\*0.39\\*\\* and word error rate\nof \\*\\*0.35\\*\\*. ### Multi GPU CTC\nThe following command shows how to fine-tune [XLSR-Wav2Vec2](https://huggingface.co/transformers/main/model\\_doc/xlsr\\_wav2vec2.html) on [Common Voice](https://huggingface.co/datasets/common\\_voice) using 8 GPUs in half-precision. ```bash\ntorchrun \\\n--nproc\\_per\\_node 8 run\\_speech\\_recognition\\_ctc.py \\\n--dataset\\_name=\""common\\_voice\"" \\\n--model\\_name\\_or\\_path=\""facebook/wav2vec2-large-xlsr-53\"" \\\n--dataset\\_config\\_name=\""tr\"" \\\n--output\\_dir=\""./wav2vec2-common\\_voice-tr-demo-dist\"" \\\n--overwrite\\_output\\_dir \\\n--num\\_train\\_epochs=\""15\"" \\\n--per\\_device\\_train\\_batch\\_size=\""4\"" \\\n--learning\\_rate=\""3e-4\"" \\\n--warmup\\_steps=\""500\"" \\\n--evaluation\\_strategy=\""steps\"" \\\n--text\\_column\\_name=\""sentence\"" \\\n--length\\_column\\_name=\""input\\_length\"" \\\n--save\\_steps=\""400\"" \\\n--eval\\_steps=\""100\"" \\\n--logging\\_steps=\""1\"" \\\n--layerdrop=\""0.0\"" \\\n--save\\_total\\_limit=\""3\"" \\\n--freeze\\_feature\\_encoder \\\n--gradient\\_checkpointing \\\n--chars\\_to\\_ignore , ?"", ""The file naming is up to you. It's recommended to use DeepSpeed's `add\\_config\\_arguments` utility to add the necessary command line arguments to your code. For more information please see [DeepSpeed's Argument Parsing](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) doc. You can use a launcher of your choice here. You can continue using the pytorch launcher:\n```bash\ntorch.distributed.run --nproc\\_per\\_node=2 your\\_program.py  --deepspeed ds\\_config.json\n```\nor use the launcher provided by `deepspeed`:\n```bash\ndeepspeed --num\\_gpus=2 your\\_program.py  --deepspeed ds\\_config.json\n```\nAs you can see the arguments aren't the same, but for most needs either of them works. The\nfull details on how to configure various nodes and GPUs can be found [here](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node). When you use the `deepspeed` launcher and you want to use all available gpus you can just omit the `--num\\_gpus` flag. Here is an example of running `run\\_translation.py` under DeepSpeed deploying all available GPUs:\n```bash\ndeepspeed examples/pytorch/translation/run\\_translation.py \\\n--deepspeed tests/deepspeed/ds\\_config\\_zero3.json \\\n--model\\_name\\_or\\_path t5-small --per\\_device\\_train\\_batch\\_size 1 \\\n--output\\_dir output\\_dir --overwrite\\_output\\_dir --fp16 \\\n--do\\_train --max\\_train\\_samples 500 --num\\_train\\_epochs 1 \\\n--dataset\\_name wmt16 --dataset\\_config \""ro-en\"" \\\n--source\\_lang en --target\\_lang ro\n```\nNote that in the DeepSpeed documentation you are likely to see `--deepspeed --deepspeed\\_config ds\\_config.json` - i.e. two DeepSpeed-related arguments, but for the sake of simplicity, and since there are already so many arguments to deal\nwith, we combined the two into a single argument. For some practical usage examples, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400). ### Deployment with one GPU\nTo deploy DeepSpeed with one GPU adjust the [`Trainer`] command line arguments as follows:\n```bash\ndeepspeed --num\\_gpus=1 examples/pytorch/translation/run\\_translation.py \\\n--deepspeed tests/deepspeed/ds\\_config\\_zero2.json \\\n--model\\_name\\_or\\_path t5-small --per\\_device\\_train\\_batch\\_size 1 \\\n--output\\_dir output\\_dir --overwrite\\_output\\_dir --fp16 \\\n--do\\_train --max\\_train\\_samples 500 --num\\_train\\_epochs 1 \\\n--dataset\\_name wmt16 --dataset\\_config \""ro-en\"" \\\n--source\\_lang en --target\\_lang ro\n```\nThis is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly to use just one GPU via\n`--num\\_gpus=1`. By default, DeepSpeed deploys all GPUs it can see on the given node. If you have only 1 GPU to start\nwith, then you don't need this argument. The following [documentation](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) discusses the launcher options.""]","PyTorch provides the `torchrun` command line module to run a script on multiple GPUs. You can use it by passing in the number of nodes and the script to run, for example:

```bash
torchrun --nproc_per_node=2 --nnodes=1 example_script.py
```",torchrun
"What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?
","[""Vision Transformers 图像分类\n相关空间：https://huggingface.co/spaces/abidlabs/vision-transformer\n标签：VISION, TRANSFORMERS, HUB\n## 简介\n图像分类是计算机视觉中的重要任务。构建更好的分类器以确定图像中存在的对象是当前研究的热点领域，因为它在从人脸识别到制造质量控制等方面都有应用。\n最先进的图像分类器基于 \\_transformers\\_ 架构，该架构最初在自然语言处理任务中很受欢迎。这种架构通常被称为 vision transformers (ViT)。这些模型非常适合与 Gradio 的\\*图像\\*输入组件一起使用，因此在本教程中，我们将构建一个使用 Gradio 进行图像分类的 Web 演示。我们只需用\\*\\*一行 Python 代码\\*\\*即可构建整个 Web 应用程序，其效果如下（试用一下示例之一！）：\n让我们开始吧！\n### 先决条件\n确保您已经[安装](/getting\\_started)了 `gradio` Python 包。\n## 步骤 1 - 选择 Vision 图像分类模型\n首先，我们需要一个图像分类模型。在本教程中，我们将使用[Hugging Face Model Hub](https://huggingface.co/models?pipeline\\_tag=image-classification)上的一个模型。该 Hub 包含数千个模型，涵盖了多种不同的机器学习任务。\n在左侧边栏中展开 Tasks 类别，并选择我们感兴趣的“Image Classification”作为我们的任务。然后，您将看到 Hub 上为图像分类设计的所有模型。\n在撰写时，最受欢迎的模型是 `google/vit-base-patch16-224`，该模型在分辨率为 224x224 像素的 ImageNet 图像上进行了训练。我们将在演示中使用此模型。\n## 步骤 2 - 使用 Gradio 加载 Vision Transformer 模型\n当使用 Hugging Face Hub 上的模型时，我们无需为演示定义输入或输出组件。同样，我们不需要关心预处理或后处理的细节。所有这些都可以从模型标签中自动推断出来。\n除了导入语句外，我们只需要一行代码即可加载并启动演示。\n我们使用 `gr.Interface.load()` 方法，并传入包含 `huggingface/` 的模型路径，以指定它来自 Hugging Face Hub。\n```python\nimport gradio as gr\ngr.Interface.load(\n\""huggingface/google/vit-base-patch16-224\"",\nexamples=[\""alligator.jpg\"", \""laptop.jpg\""]).launch()\n```\n请注意，我们添加了一个 `examples` 参数，允许我们使用一些预定义的示例预填充我们的界面。\n这将生成以下接口，您可以直接在浏览器中尝试。当您输入图像时，它会自动进行预处理并发送到 Hugging Face Hub API，通过模型处理，并以人类可解释的预测结果返回。尝试上传您自己的图像！\n---\n完成！只需一行代码，您就建立了一个图像分类器的 Web 演示。如果您想与他人分享，请在 `launch()` 接口时设置 `share=True`。"", ""--\ntitle: \""Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\""\nthumbnail: /blog/assets/97\\_vision\\_transformers/thumbnail.png\nauthors:\n- user: juliensimon\n---\n# Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\nThis blog post will show how easy it is to fine-tune pre-trained Transformer models for your dataset using the Hugging Face Optimum library on Graphcore Intelligence Processing Units (IPUs). As an example, we will show a step-by-step guide and provide a notebook that takes a large, widely-used chest X-ray dataset and trains a vision transformer (ViT) model. ## Introducing vision transformer (ViT) models\n\nIn 2017 a group of Google AI researchers published a paper introducing the transformer model architecture. Characterised by a novel self-attention mechanism, transformers were proposed as a new and efficient group of models for language applications. Indeed, in the last five years, transformers have seen explosive popularity and are now accepted as the de facto standard for natural language processing (NLP). Transformers for language are perhaps most notably represented by the rapidly evolving GPT and BERT model families. Both can run easily and efficiently on Graphcore IPUs as part of the growing [Hugging Face Optimum Graphcore library](/posts/getting-started-with-hugging-face-transformers-for-ipus-with-optimum)). ![transformers_chrono](https://www.graphcore.ai/hs-fs/hubfs/transformers_chrono.png?width=1024&name=transformers_chrono.png)\n\nA timeline showing releases of prominent transformer language models (credit: Hugging Face)\n\nAn in-depth explainer about the transformer model architecture (with a focus on NLP) can be found [on the Hugging Face website](https://huggingface.co/course/chapter1/4?fw=pt)."", ""For Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage. AutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate 🤗 Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details. ## The technical philosophy\nIn this section, we wanted to share our philosophy behind adding support for Computer Vision in 🤗 Transformers so that the community is aware of the design choices specific to this area. Even though Transformers started with NLP, we support multiple modalities today, for example – vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:\n- Easy model download with a single line of code with `from\\_pretrained()`\n- Easy model upload with `push\\_to\\_hub()`\n- Support for loading huge checkpoints with efficient checkpoint sharding techniques\n- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))\n- Initialization from model configurations\n- Support for both PyTorch and TensorFlow (non-exhaustive)\n- and many more\nUnlike tokenizers, we have preprocessors (such as [this](https://huggingface.co/docs/transformers/model\\_doc/vit#transformers.ViTImageProcessor)) that take care of preparing data for the vision models. We have worked hard to ensure the user experience of using a vision model still feels easy and similar:\n```py\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nimport torch\nfrom datasets import load\\_dataset\ndataset = load\\_dataset(\""huggingface/cats-image\"")\nimage = dataset[\""test\""][\""image\""][0]\nimage\\_processor  = ViTImageProcessor.from\\_pretrained(\""google/vit-base-patch16-224\"")\nmodel = ViTForImageClassification.from\\_pretrained(\""google/vit-base-patch16-224\"")\ninputs = image\\_processor(image, return\\_tensors=\""pt\"")\nwith torch.no\\_grad():\nlogits = model(\\*\\*inputs).logits\n# model predicts one of the 1000 ImageNet classes\npredicted\\_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted\\_label])\n# Egyptian cat\n```\nEven for a difficult task like object detection, the user experience doesn’t change very much:\n```py\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\nfrom PIL import Image\nimport requests\nurl = \""http://images.cocodataset.org/val2017/000000039769.jpg\""\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\\_processor = AutoImageProcessor.from\\_pretrained(\""microsoft/conditional-detr-resnet-50\"")\nmodel = AutoModelForObjectDetection.from\\_pretrained(\""microsoft/conditional-detr-resnet-50\"")\ninputs = image\\_processor(images=image, return\\_tensors=\""pt\"")\noutputs = model(\\*\\*inputs)\n# convert outputs (bounding boxes and class logits) to COCO API\ntarget\\_sizes = torch.tensor([image.size[::-1]])\nresults = image\\_processor.post\\_process\\_object\\_detection(\noutputs, threshold=0.5, target\\_sizes=target\\_sizes\n)[0]\nfor score, label, box in zip(results[\""scores\""], results[\""labels\""], results[\""boxes\""]):\nbox = [round(i, 2) for i in box.tolist()]\nprint(\nf\""Detected {model.config.id2label[label.item()]} with confidence \""\nf\""{round(score.item(), 3)} at location {box}\""\n)\n```\nLeads to:\n```bash\nDetected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\nDetected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\nDetected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\nDetected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\nDetected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\n```\n## Zero-shot models for vision\nThere’s been a surge of models that reformulate core vision tasks like segmentation and detection in interesting ways and introduce even more flexibility.""]",最受欢迎的 Vision Transformer 模型是 `google/vit-base-patch16-224`，该模型在分辨率为 224x224 像素的 ImageNet 图像上进行了训练。,google/vit-base-patch16-224
"What is the command to upload an ESPnet model to a Hugging Face repository?
","[""Using ESPnet at Hugging Face\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks. ## Exploring ESPnet in the Hub\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). All models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more. 2. Metadata tags that help for discoverability and contain information such as license, language and datasets. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png)\n\n## Using existing models\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\_model\\_zoo)). If you're interested in doing inference, different classes for different tasks have a `from\\_pretrained` method that allows loading models from the Hub."", ""- Option1: Pushing the model to Hugging Face Hub\n```python\nmodel.push\\_to\\_hub(\nf\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\""/\"", \""\\_\""),\ntoken = \""hf\\_...\""\n)\n```\ntoken (`bool` or `str`, \\*optional\\*):\n`token` is to be used for HTTP Bearer authorization when accessing remote files. If `True`, will use the token generated\nwhen running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo\\_url`\nis not specified. Or you can get your token from https://huggingface.co/settings/token\n```\n- Or save model locally\n```python\npeft\\_model\\_id = f\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\""/\"", \""\\_\"")\nmodel.save\\_pretrained(peft\\_model\\_id)\n```\n```python\n# saving model\npeft\\_model\\_id = f\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\n\""/\"", \""\\_\""\n)\nmodel.save\\_pretrained(peft\\_model\\_id)\n```\n```python\nckpt = f\""{peft\\_model\\_id}/adapter\\_model.bin\""\n!du -h $ckpt\n```\n```python\nfrom peft import PeftModel, PeftConfig\npeft\\_model\\_id = f\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\n\""/\"", \""\\_\""\n)\nconfig = PeftConfig.from\\_pretrained(peft\\_model\\_id)\nmodel = AutoModelForCausalLM.from\\_pretrained(config.base\\_model\\_name\\_or\\_path)\nmodel = PeftModel.from\\_pretrained(model, peft\\_model\\_id)\n```\n```python\nmodel.to(device)\nmodel.eval()\ni = 4\ninputs = tokenizer(f'{text\\_column} : {dataset[\""test\""][i][\""Tweet text\""]} Label : ', return\\_tensors=\""pt\"")\nprint(dataset[\""test\""][i][\""Tweet text\""])\nprint(inputs)\nwith torch.no\\_grad():\ninputs = {k: v.to(device) for k, v in inputs.items()}\noutputs = model.generate(\ninput\\_ids=inputs[\""input\\_ids\""], attention\\_mask=inputs[\""attention\\_mask\""], max\\_new\\_tokens=10, eos\\_token\\_id=3\n)\nprint(outputs)\nprint(tokenizer.batch\\_decode(outputs.detach().cpu().numpy(), skip\\_special\\_tokens=True))\n```"", ""FrameworkSwitchCourse {fw} />\n# Sharing pretrained models[[sharing-pretrained-models]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nIn the steps below, we'll take a look at the easiest ways to share pretrained models to the 🤗 Hub. There are tools and utilities available that make it simple to share and update models directly on the Hub, which we will explore below. We encourage all users that train models to contribute by sharing them with the community — sharing models, even when trained on very specific datasets, will help others, saving them time and compute resources and providing access to useful trained artifacts. In turn, you can benefit from the work that others have done! There are three ways to go about creating new model repositories:\n- Using the `push\\_to\\_hub` API\n- Using the `huggingface\\_hub` Python library\n- Using the web interface\nOnce you've created a repository, you can upload files to it via git and git-lfs. We'll walk you through creating model repositories and uploading files to them in the following sections. ## Using the `push\\_to\\_hub` API[[using-the-pushtohub-api]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nThe simplest way to upload files to the Hub is by leveraging the `push\\_to\\_hub` API.""]","The command to upload an ESPnet model to a Hugging Face repository is as follows:

```python
model.push_to_hub(
    f""{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}"".replace(""/"", ""_""),
    token = ""hf_...""
)
```

In this command, `token` is optional and is used for HTTP Bearer authorization when accessing remote files. If `True`, it will use the token generated when running `huggingface-cli login`.",./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo
"What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?
","[""Add custom Dependencies\nInference Endpoints’ base image includes all required libraries to run inference on 🤗 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\\* [customize your inference pipeline](/docs/inference-endpoints/guides/custom\\_handler) and need additional Python dependencies\n\\* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)). To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within. ```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\\* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n\\* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom\\_container) for inference."", ""It contains information about a deployed `InferenceEndpoint`, including its configuration and current state. Once deployed, you can run inference on the Endpoint using the [`InferenceEndpoint.client`] and [`InferenceEndpoint.async\\_client`] properties that respectively return an [`InferenceClient`] and an [`AsyncInferenceClient`] object. [[autodoc]] InferenceEndpoint\n- from\\_raw\n- client\n- async\\_client\n- all\n## InferenceEndpointStatus\n[[autodoc]] InferenceEndpointStatus\n## InferenceEndpointType\n[[autodoc]] InferenceEndpointType\n## InferenceEndpointError\n[[autodoc]] InferenceEndpointError"", ""### 6. Deploy your Custom Handler as an Inference Endpoint\nThe last step is to deploy your Custom Handler as an Inference Endpoint. You can deploy your Custom Handler like you would a regular Inference Endpoint. Add your repository, select your cloud and region, your instance and security setting, and deploy. When creating your Endpoint, the Inference Endpoint Service will check for an available and valid `handler.py`, and will use it for serving requests no matter which “Task” you select. \\_Note: In your [Inference Endpoints dashboard](https://ui.endpoints.huggingface.co/), the Task for this Endpoint should now be set to Custom\\_""]","To install custom Python dependencies for Inference Endpoints, you should add a `requirements.txt` file to your model repository on the Hugging Face Hub. This file should list the Python dependencies you want to install.",requirements.txt
"How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Textual inversion\n[[open-in-colab]]\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer). This guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text\\_inversion) training guide. Import the necessary libraries:\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import make\\_image\\_grid\n```\n## Stable Diffusion 1 and 2\nPick a Stable Diffusion checkpoint and a pre-learned concept from the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer):\n```py\npretrained\\_model\\_name\\_or\\_path = \""runwayml/stable-diffusion-v1-5\""\nrepo\\_id\\_embeds = \""sd-concepts-library/cat-toy\""\n```\nNow you can load a pipeline, and pass the pre-learned concept to it:\n```py\npipeline = StableDiffusionPipeline.from\\_pretrained(\npretrained\\_model\\_name\\_or\\_path, torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\npipeline.load\\_textual\\_inversion(repo\\_id\\_embeds)\n```\nCreate a prompt with the pre-learned concept by using the special placeholder token ``, and choose the number of samples and rows of images you'd like to generate:\n```py\nprompt = \""a grafitti in a favela wall with a  on it\""\nnum\\_samples\\_per\\_row = 2\nnum\\_rows = 2\n```\nThen run the pipeline (feel free to adjust the parameters like `num\\_inference\\_steps` and `guidance\\_scale` to see how they affect image quality), save the generated images and visualize them with the helper function you created at the beginning:\n```py\nall\\_images = []\nfor \\_ in range(num\\_rows):\nimages = pipeline(prompt, num\\_images\\_per\\_prompt=num\\_samples\\_per\\_row, num\\_inference\\_steps=50, guidance\\_scale=7.5).images\nall\\_images.extend(images)\ngrid = make\\_image\\_grid(all\\_images, num\\_rows, num\\_samples\\_per\\_row)\ngrid\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/textual_inversion_inference.png)\n\n## Stable Diffusion XL\nStable Diffusion XL (SDXL) can also use textual inversion vectors for inference. In contrast to Stable Diffusion 1 and 2, SDXL has two text encoders so you'll need two textual inversion embeddings - one for each text encoder model. Let's download the SDXL textual inversion embeddings and have a closer look at it's structure:\n```py\nfrom huggingface\\_hub import hf\\_hub\\_download\nfrom safetensors.torch import load\\_file\nfile = hf\\_hub\\_download(\""dn118/unaestheticXL\"", filename=\""unaestheticXLv31.safetensors\"")\nstate\\_dict = load\\_file(file)\nstate\\_dict\n```\n```\n{'clip\\_g': tensor([[ 0.0077, -0.0112, 0.0065, ..., 0.0195, 0.0159, 0.0275],\n...,\n[-0.0170, 0.0213, 0.0143, ..., -0.0302, -0.0240, -0.0362]],\n'clip\\_l': tensor([[ 0.0023, 0.0192, 0.0213, ..., -0.0385, 0.0048, -0.0011],\n...,\n[ 0.0475, -0.0508, -0.0145, ..., 0.0070, -0.0089, -0.0163]],\n```\nThere are two tensors, `\""clip\\_g\""` and `\""clip\\_l\""`. `\""clip\\_g\""` corresponds to the bigger text encoder in SDXL and refers to\n`pipe.text\\_encoder\\_2` and `\""clip\\_l\""` refers to `pipe.text\\_encoder`. Now you can load each tensor separately by passing them along with the correct text encoder and tokenizer\nto [`~loaders.TextualInversionLoaderMixin.load\\_textual\\_inversion`]:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipe = AutoPipelineForText2Image.from\\_pretrained(\""stabilityai/stable-diffusion-xl-base-1.0\"", variant=\""fp16\"", torch\\_dtype=torch.float16)\npipe.to(\""cuda\"")\npipe.load\\_textual\\_inversion(state\\_dict[\""clip\\_g\""], token=\""unaestheticXLv31\"", text\\_encoder=pipe.text\\_encoder\\_2, tokenizer=pipe.tokenizer\\_2)\npipe.load\\_textual\\_inversion(state\\_dict[\""clip\\_l\""], token=\""unaestheticXLv31\"", text\\_encoder=pipe.text\\_encoder, tokenizer=pipe.tokenizer)\n# the embedding should be used as a negative embedding, so we pass it as a negative prompt\ngenerator = torch.Generator().manual\\_seed(33)\nimage = pipe(\""a woman standing in front of a mountain\"", negative\\_prompt=\""unaestheticXLv31\"", generator=generator).images[0]\nimage\n```""]","To teach new concepts to Stable Diffusion using Textual Inversion, you need just 3-5 example images.",3-5 images
"What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?
","[""In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future. ## Sharded checkpoints\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save\\_pretrained(save\\_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in. You can control the maximum size before sharding with the `max\\_shard\\_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model. ```py\nfrom transformers import AutoModel\nmodel = AutoModel.from\\_pretrained(\""bert-base-cased\"")\n```\nIf you save it using [`~PreTrainedModel.save\\_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n```py\n>>> import os\n>>> import tempfile\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir)\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model.bin']\n```\nNow let's use a maximum shard size of 200MB:\n```py\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model-00001-of-00003.bin', 'pytorch\\_model-00002-of-00003.bin', 'pytorch\\_model-00003-of-00003.bin', 'pytorch\\_model.bin.index.json']\n```\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index."", ""- Even though the checkpoint is trained on images of specific size, the model will work on images of any size. The smallest supported image size is 32x32. - One can use [`MobileNetV1ImageProcessor`] to prepare images for the model. - The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes). However, the model predicts 1001 classes: the 1000 classes from ImageNet plus an extra “background” class (index 0). - The original TensorFlow checkpoints use different padding rules than PyTorch, requiring the model to determine the padding amount at inference time, since this depends on the input image size. To use native PyTorch padding behavior, create a [`MobileNetV1Config`] with `tf\\_padding = False`. Unsupported features:\n- The [`MobileNetV1Model`] outputs a globally pooled version of the last hidden state. In the original model it is possible to use a 7x7 average pooling layer with stride 2 instead of global pooling. For larger inputs, this gives a pooled output that is larger than 1x1 pixel. The HuggingFace implementation does not support this. - It is currently not possible to specify an `output\\_stride`. For smaller output strides, the original model invokes dilated convolution to prevent the spatial resolution from being reduced further. The output stride of the HuggingFace model is always 32. - The original TensorFlow checkpoints include quantized models."", ""Alternatively, use 🤗 Accelerate to gain full control over the training loop. Find the 🤗 Accelerate example\n[further down in this guide](#using-accelerate). While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can\nresult in a more pronounced training slowdown. Consider the following example. Let's say, the `per\\_device\\_train\\_batch\\_size=4`\nwithout gradient accumulation hits the GPU's limit. If you would like to train with batches of size 64, do not set the\n`per\\_device\\_train\\_batch\\_size` to 1 and `gradient\\_accumulation\\_steps` to 64. Instead, keep `per\\_device\\_train\\_batch\\_size=4`\nand set `gradient\\_accumulation\\_steps=16`. This results in the same effective batch size while making better use of\nthe available GPU resources. For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\nand [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957). ## Gradient Checkpointing\nSome large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used.""]","In Transformers version 4.18.0, the maximum size of a model checkpoint before it is automatically sharded is 10GB.",10GB
"What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?
","[""Whether and to what extent bias in the system concretely affects people ultimately depends on what the system is used for. As such, the first place developers can work to mitigate bias is when deciding how ML fits in their system, e.g., by deciding what optimization objective it will use. For example, let’s go back to one of the first highly-publicized cases of a Machine Learning system used in production for algorithmic content recommendation. From 2006 to 2009, Netflix ran the [Netflix Prize](https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/The-Netflix-Prize-Bennett.pdf), a competition with a 1M$ cash prize challenging teams around the world to develop ML systems to accurately predict a user’s rating for a new movie based on their past ratings. The [winning submission](https://www.asc.ohio-state.edu/statistics/dmsl/GrandPrize2009\\_BPC\\_BigChaos.pdf) improved the RMSE (Root-mean-square-error) of predictions on unseen user-movie pairs by over 10% over Netflix’s own CineMatch algorithm, meaning it got much better at predicting how users would rate a new movie based on their history."", ""As in the examples above, some common steps that may help decide whether and how to apply ML in a way that minimizes bias-related risk include:\n\\* Investigate:\n\\* Reports of bias in the field pre-ML\n\\* At-risk demographic categories for your specific use case\n\\* Examine:\n\\* The impact of your optimization objective on reinforcing biases\n\\* Alternative objectives that favor diversity and positive long-term impacts\n### I am curating/picking a dataset for my ML system, how can I address bias? While training datasets are [not the sole source of bias](https://www.cell.com/patterns/fulltext/S2666-3899(21)00061-1) in the ML development cycle, they do play a significant role. Does your [dataset disproportionately associate](https://aclanthology.org/2020.emnlp-main.23/) biographies of women with life events but those of men with achievements? Those \\*\\*stereotypes\\*\\* are probably going to show up in your full ML system! Does your voice recognition dataset only feature specific accents? Not a good sign for [the inclusivity of technology](https://www.scientificamerican.com/article/speech-recognition-tech-is-yet-another-example-of-bias/) you build with it in terms of \\*\\*disparate performance\\*\\*! Whether you’re curating a dataset for ML applications or selecting a dataset to train an ML model, finding out, mitigating, and [communicating](https://dl.acm.org/doi/10.1145/3479582) to what extent the data exhibits these phenomena are all necessary steps to reducing bias-related risks. You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it comes from, who are the people represented on the data, and what the curation process was. Several frameworks for this reflection and documentation have been proposed such as [Data Statements for NLP](https://direct.mit.edu/tacl/article/doi/10.1162/tacl\\_a\\_00041/43452/Data-Statements-for-Natural-Language-Processing) or [Datasheets for Datasets](https://dl.acm.org/doi/10.1145/3458723). The Hugging Face Hub includes a Dataset Card [template](https://github.com/huggingface/datasets/blob/main/templates/README.md) and [guide](https://github.com/huggingface/datasets/blob/main/templates/README\\_guide.md#dataset-card-creation-guide) inspired by these works; the section on [considerations for using the data](https://github.com/huggingface/datasets/blob/main/templates/README\\_guide.md#considerations-for-using-the-data) is usually a good place to look for information about notable biases if you’re browsing datasets, or to write a paragraph sharing your insights on the topic if you’re sharing a new one. And if you’re looking for more inspiration on what to put there, check out these sections written by Hub users in the [BigLAM organization](https://huggingface.co/biglam) for historical datasets of [legal proceedings](https://huggingface.co/datasets/biglam/old\\_bailey\\_proceedings#social-impact-of-dataset), [image classification](https://huggingface.co/datasets/biglam/brill\\_iconclass#social-impact-of-dataset), and [newspapers](https://huggingface.co/datasets/biglam/bnl\\_newspapers1841-1879#social-impact-of-dataset). ![HF Dataset Card guide for the Social Impact and Bias Sections](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img3.png)\n*[HF Dataset Card guide](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md#social-impact-of-dataset) for the Social Impact and Bias Sections*\n\nWhile describing the origin and context of a dataset is always a good starting point to understand the biases at play, [quantitatively measuring phenomena](https://arxiv.org/abs/2212.05129) that encode those biases can be just as helpful."", ""in real time via an API or in batch jobs that run once a day. Memory footprint refers to the size of the model weights and how much hardware memory they occupy. If a model is too large to fit on a single GPU or CPU, then it has to be split over multiple ones, which can be more or less difficult depending on the model architecture and the deployment method. When doing online model evaluation, there is often a trade-off to be done between inference speed and accuracy or precision, whereas this is less the case for offline evaluation. ## Limitations and bias\nAll models and all metrics have their limitations and biases, which depend on the way in which they were trained, the data that was used, and their intended uses. It is important to measure and communicate these limitations clearly to prevent misuse and unintended impacts, for instance via [model cards](https://huggingface.co/course/chapter4/4?fw=pt) which document the training and evaluation process. Measuring biases can be done by evaluating models on datasets such as [Wino Bias](https://huggingface.co/datasets/wino\\_bias) or [MD Gender Bias](https://huggingface.co/datasets/md\\_gender\\_bias), and by doing [Interactive Error Analyis](https://huggingface.co/spaces/nazneen/error-analysis) to try to identify which subsets of the evaluation dataset a model performs poorly on. We are currently working on additional measurements that can be used to quantify different dimensions of bias in both models and datasets -- stay tuned for more documentation on this topic!""]","I'm unable to provide information on Weights and Biases (W&B) as it is not related to HuggingFace or gradio-app. If you have questions specifically about HuggingFace or gradio-app, feel free to ask!","To track their machine learning experiments at every stage, from training to production."
"What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\n🤗 Optimum provides an API called BetterTransformer, a fast path of standard PyTorch Transformer APIs to benefit from interesting speedups on CPU & GPU through sparsity and fused kernels as Flash Attention. For now, BetterTransformer supports the fastpath from the native [`nn.TransformerEncoderLayer`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) as well as Flash Attention and Memory-Efficient Attention from [`torch.nn.functional.scaled\\_dot\\_product\\_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled\\_dot\\_product\\_attention.html)."", ""--\ntitle: \""Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\""\nthumbnail: /blog/assets/80\\_intel/01.png\nauthors:\n- user: juliensimon\n---\n# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\n![image](assets/80\\_intel/01.png)\nThe mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption. Today, we're excited to announce that Intel has officially joined our [Hardware Partner Program](https://huggingface.co/hardware). Thanks to the [Optimum](https://github.com/huggingface/optimum-intel) open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers. Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage. Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms. “\\*We’re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.\\*”, says Wei Li, Intel Vice President & General Manager, AI and Analytics. In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs. The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Let’s take a deeper look at it. ## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices."", ""--\ntitle: \""How we sped up transformer inference 100x for 🤗 API customers\""\nthumbnail: /blog/assets/09\\_accelerated\\_inference/thumbnail.png\n---\n# How we sped up transformer inference 100x for 🤗 API customers\n🤗 Transformers has become the default library for data scientists all around the world to explore state of the art NLP models and build new NLP features. With over 5,000 pre-trained and fine-tuned models available, in over 250 languages, it is a rich playground, easily accessible whichever framework you are working in. While experimenting with models in 🤗 Transformers is easy, deploying these large models into production with maximum performance, and managing them into an architecture that scales with usage is a \\*\\*hard engineering challenge\\*\\* for any Machine Learning Engineer. This 100x performance gain and built-in scalability is why subscribers of our hosted [Accelerated Inference API](https://huggingface.co/pricing) chose to build their NLP features on top of it. To get to the \\*\\*last 10x of performance\\*\\* boost, the optimizations need to be low-level, specific to the model, and to the target hardware. This post shares some of our approaches squeezing every drop of compute juice for our customers. 🍋\n## Getting to the first 10x speedup\nThe first leg of the optimization journey is the most accessible, all about using the best combination of techniques offered by the [Hugging Face libraries](https://github.com/huggingface/), independent of the target hardware. We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main\\_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:\n-| Naive version | Optimized version |\n-|:---------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------:|\n-|![](/blog/assets/09\\_accelerated\\_inference/unoptimized\\_graph.png)|![](/blog/assets/09\\_accelerated\\_inference/optimized\\_graph.png)|\nTokenization is often a bottleneck for efficiency during inference. We use the most efficient methods from the [🤗 Tokenizers](https://github.com/huggingface/tokenizers/) library, leveraging the Rust implementation of the model tokenizer in combination with smart caching to get up to 10x speedup for the overall latency. Leveraging the latest features of the Hugging Face libraries, we achieve a reliable 10x speed up compared to an out-of-box deployment for a given model/hardware pair. As new releases of Transformers and Tokenizers typically ship every month, our API customers do not need to constantly adapt to new optimization opportunities, their models just keep running faster.""]",The name of the open-source library created by Hugging Face to simplify Transformer acceleration is Optimum.,Optimum
"What parameter is used to ensure that elements in a row have the same height in Gradio?
","[""The Row will wrap if there isn't sufficient space to satisfy all `min\\_width` values. Learn more about Rows in the [docs](https://gradio.app/docs/#row). ## Columns and Nesting\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n$code\\_rows\\_and\\_columns\n$demo\\_rows\\_and\\_columns\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. Learn more about Columns in the [docs](https://gradio.app/docs/#column). # Dimensions\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases. Below is an example illustrating the use of viewport width (vw):\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\nim = gr.ImageEditor(\nwidth=\""50vw\"",\n)\ndemo.launch()\n```\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n```python\nimport gradio as gr\ncss = \""\""\""\n.container {\nheight: 100vh;\n}\n\""\""\""\nwith gr.Blocks(css=css) as demo:\nwith gr.Column(elem\\_classes=[\""container\""]):\nname = gr.Chatbot(value=[[\""1\"", \""2\""]], height=\""70%\"")\ndemo.launch()\n```\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height."", ""@gradio/row\n## 0.1.1\n### Features\n- [#6399](https://github.com/gradio-app/gradio/pull/6399) [`053bec9`](https://github.com/gradio-app/gradio/commit/053bec98be1127e083414024e02cf0bebb0b5142) - Improve CSS token documentation in Storybook. Thanks [@hannahblair](https://github.com/hannahblair)! ## 0.1.0\n### Features\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Publish all components to npm. Thanks [@pngwn](https://github.com/pngwn)! ## 0.1.0-beta.2\n### Features\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676`](https://github.com/gradio-app/gradio/commit/83e947676d327ca2ab6ae2a2d710c78961c771a0) - Format js in v4 branch. Thanks [@freddyaboulton](https://github.com/freddyaboulton)! ## 0.1.0-beta.1\n### Features\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-app/gradio/commit/319c30f3fccf23bfe1da6c9b132a6a99d59652f7) - rererefactor frontend files. Thanks [@pngwn](https://github.com/pngwn)! ## 0.0.2-beta.0\n### Features\n- [#5648](https://github.com/gradio-app/gradio/pull/5648) [`c573e2339`](https://github.com/gradio-app/gradio/commit/c573e2339b86c85b378dc349de5e9223a3c3b04a) - Publish all components to npm. Thanks [@freddyaboulton](https://github.com/freddyaboulton)!"", ""\\* `scale` and `min\\_width` allow Gradio app developers to control how much space your component takes up in the UI. \\* `loading\\_status` is used to display a loading status over the component when it is the output of an event. \\* `mode` is how the parent Gradio app tells your component whether the `interactive` or `static` version should be displayed. \\* `gradio`: The `gradio` object is created by the parent Gradio app. It stores some application-level configuration that will be useful in your component, like internationalization. You must use it to dispatch events from your component. A minimal `Index.svelte` file would look like:\n```typescript\n\n{#if loading\\_status}\n{/if}\n\n{value}\n\n```\n## The Example.svelte file\nThe `Example.svelte` file should expose the following props:\n```typescript\nexport let value: string;\nexport let type: \""gallery\"" | \""table\"";\nexport let selected = false;\nexport let samples\\_dir: string;\nexport let index: number;\n```\n\\* `value`: The example value that should be displayed.""]","In Gradio, the `min_width` parameter is used to ensure that elements in a row have the same height. This parameter allows you to control the minimum width of the components, which helps in maintaining a consistent height across the elements in a row.",equal_height
"What is the command to install the latest version of Optimum with OpenVINO support?
","[""--\ntitle: \""Accelerate your models with 🤗 Optimum Intel and OpenVINO\""\nthumbnail: /blog/assets/113\\_openvino/thumbnail.png\nauthors:\n- user: echarlaix\n- user: juliensimon\n---\n# Accelerate your models with 🤗 Optimum Intel and OpenVINO\n![image](assets/113\\_openvino/thumbnail.png)\nLast July, we [announced](https://huggingface.co/blog/intel) that Intel and Hugging Face would collaborate on building state-of-the-art yet simple hardware acceleration tools for Transformer models. ​\nToday, we are very happy to announce that we added Intel [OpenVINO](https://docs.openvino.ai/latest/index.html) to [Optimum Intel](https://github.com/huggingface/optimum-intel). You can now easily perform inference with OpenVINO Runtime on a variety of Intel processors ([see](https://docs.openvino.ai/latest/openvino\\_docs\\_OV\\_UG\\_supported\\_plugins\\_Supported\\_Devices.html) the full list of supported devices) using Transformers models which can be hosted either on the Hugging Face hub or locally. You can also quantize your model with the OpenVINO Neural Network Compression Framework ([NNCF](https://github.com/openvinotoolkit/nncf)), and reduce its size and prediction latency in near minutes. ​\nThis first release is based on OpenVINO 2022.2 and enables inference for a large quantity of PyTorch models using our [`OVModels`](https://huggingface.co/docs/optimum/intel/inference). Post-training static quantization and quantization aware training can be applied on many encoder models (BERT, DistilBERT, etc.). More encoder models will be supported in the upcoming OpenVINO release. Currently the quantization of Encoder Decoder models is not enabled, however this restriction should be lifted with our integration of the next OpenVINO release. ​Let us show you how to get started in minutes!​\n## Quantizing a Vision Transformer with Optimum Intel and OpenVINO\n​\nIn this example, we will run post-training static quantization on a Vision Transformer (ViT) [model](https://huggingface.co/juliensimon/autotrain-food101-1471154050) fine-tuned for image classification on the [food101](https://huggingface.co/datasets/food101) dataset."", ""As demonstrated by this [Intel Space](https://huggingface.co/spaces/Intel/Stable-Diffusion-Side-by-Side), the same code runs on a previous generation Intel Xeon (code name Ice Lake) in about 45 seconds. Out of the box, we can see that Sapphire Rapids CPUs are quite faster without any code change! Now, let's accelerate! ## Optimum Intel and OpenVINO\n[Optimum Intel](https://huggingface.co/docs/optimum/intel/index) accelerates end-to-end pipelines on Intel architectures. Its API is extremely similar to the vanilla [Diffusers](https://huggingface.co/docs/diffusers/index) API, making it trivial to adapt existing code. Optimum Intel supports [OpenVINO](https://docs.openvino.ai/latest/index.html), an Intel open-source toolkit for high-performance inference. Optimum Intel and OpenVINO can be installed as follows:\n```\npip install optimum[openvino]\n```\nStarting from the code above, we only need to replace `StableDiffusionPipeline` with `OVStableDiffusionPipeline`. To load a PyTorch model and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model. ```python\nfrom optimum.intel.openvino import OVStableDiffusionPipeline\n... ov\\_pipe = OVStableDiffusionPipeline.from\\_pretrained(model\\_id, export=True)\nlatency = elapsed\\_time(ov\\_pipe, prompt)\nprint(latency)\n# Don't forget to save the exported model\nov\\_pipe.save\\_pretrained(\""./openvino\"")\n```\nOpenVINO automatically optimizes the model for the `bfloat16` format. Thanks to this, the average latency is now \\*\\*16.7 seconds\\*\\*, a sweet 2x speedup. The pipeline above support dynamic input shapes, with no restriction on the number of images or their resolution. With Stable Diffusion, your application is usually restricted to one (or a few) different output resolutions, such as 512x512, or 256x256."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Installation\n🤗 Optimum can be installed using `pip` as follows:\n```bash\npython -m pip install optimum\n```\nIf you'd like to use the accelerator-specific features of 🤗 Optimum, you can install the required dependencies according to the table below:\n| Accelerator | Installation |\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\n| [ONNX runtime](https://onnxruntime.ai/docs/) | `pip install --upgrade-strategy eager install optimum[onnxruntime]`|\n| [Intel Neural Compressor (INC)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) | `pip install --upgrade-strategy eager optimum[neural-compressor]` |\n| [Intel OpenVINO](https://docs.openvino.ai/latest/index.html) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |\n| [AMD Instinct GPUs and Ryzen AI NPU](https://www.amd.com/en/graphics/instinct-server-accelerators) | `pip install --upgrade-strategy eager optimum[amd]` |\n| [Habana Gaudi Processor (HPU)](https://habana.ai/training/) | `pip install --upgrade-strategy eager optimum[habana]` |\n| [FuriosaAI](https://www.furiosa.ai/) | `pip install --upgrade-strategy eager optimum[furiosa]` |\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version. If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you can install the base library from source as follows:\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\nFor the accelerator-specific features, you can install them by appending `optimum[accelerator\\_type]` to the `pip` command, e.g. ```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```""]","The command to install the latest version of Optimum with OpenVINO support is:

```bash
pip install optimum[openvino]
```","pip install --upgrade-strategy eager optimum[""openvino""]"

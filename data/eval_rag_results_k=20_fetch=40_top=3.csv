user_input,retrieved_contexts,response,reference
Who proposed the hybrid Vision Transformer model?,"[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Hybrid Vision Transformer (ViT Hybrid)\n## Overview\nThe hybrid Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\nat Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, Neil Houlsby. It's the first paper that successfully trains a Transformer encoder on ImageNet, attaining\nvery good results compared to familiar convolutional architectures. ViT hybrid is a slight variant of the [plain Vision Transformer](vit),\nby leveraging a convolutional backbone (specifically, [BiT](bit)) whose features are used as initial \""tokens\"" for the Transformer."", ""The abstract from the paper is the following:\n\\*Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across several mobile vision tasks, including classification and detection. Though these models have fewer parameters, they have high latency as compared to convolutional neural network-based models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention (MHA) in transformers, which requires O(k2) time complexity with respect to the number of tokens (or patches) k. Moreover, MHA requires costly operations (e.g., batch-wise matrix multiplication) for computing self-attention, impacting latency on resource-constrained devices. This paper introduces a separable self-attention method with linear complexity, i.e. O(k). A simple yet effective characteristic of the proposed method is that it uses element-wise operations for computing self-attention, making it a good choice for resource-constrained devices. The improved model, MobileViTV2, is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection. With about three million parameters, MobileViTV2 achieves a top-1 accuracy of 75.6% on the ImageNet dataset, outperforming MobileViT by about 1% while running 3.2√ó faster on a mobile device.\\*\nThis model was contributed by [shehan97](https://huggingface.co/shehan97). The original code can be found [here](https://github.com/apple/ml-cvnets). ## Usage tips\n- MobileViTV2 is more like a CNN than a Transformer model."", ""With so many Transformer variants available, it can be easy to miss the bigger picture. What all these models have in common is they're based on the original Transformer architecture. Some models only use the encoder or decoder, while others use both. This provides a useful taxonomy to categorize and examine the high-level differences within models in the Transformer family, and it'll help you understand Transformers you haven't encountered before. If you aren't familiar with the original Transformer model or need a refresher, check out the [How do Transformers work](https://huggingface.co/course/chapter1/4?fw=pt) chapter from the Hugging Face course.## Computer vision\n### Convolutional network\nFor a long time, convolutional networks (CNNs) were the dominant paradigm for computer vision tasks until the [Vision Transformer](https://arxiv.org/abs/2010.11929) demonstrated its scalability and efficiency. Even then, some of a CNN's best qualities, like translation invariance, are so powerful (especially for certain tasks) that some Transformers incorporate convolutions in their architecture. [ConvNeXt](model\\_doc/convnext) flipped this exchange around and incorporated design choices from Transformers to modernize a CNN. For example, ConvNeXt uses non-overlapping sliding windows to patchify an image and a larger kernel to increase its global receptive field. ConvNeXt also makes several layer design choices to be more memory-efficient and improve performance, so it competes favorably with Transformers! ### Encoder[[cv-encoder]]\nThe [Vision Transformer (ViT)](model\\_doc/vit) opened the door to computer vision tasks without convolutions. ViT uses a standard Transformer encoder, but its main breakthrough was how it treated an image. It splits an image into fixed-size patches and uses them to create an embedding, just like how a sentence is split into tokens. ViT capitalized on the Transformers' efficient architecture to demonstrate competitive results with the CNNs at the time while requiring fewer resources to train. ViT was soon followed by other vision models that could also handle dense vision tasks like segmentation as well as detection. One of these models is the [Swin](model\\_doc/swin) Transformer. It builds hierarchical feature maps (like a CNN üëÄ and unlike ViT) from smaller-sized patches and merges them with neighboring patches in deeper layers. Attention is only computed within a local window, and the window is shifted between attention layers to create connections to help the model learn better. Since the Swin Transformer can produce hierarchical feature maps, it is a good candidate for dense prediction tasks like segmentation and detection. The [SegFormer](model\\_doc/segformer) also uses a Transformer encoder to build hierarchical feature maps, but it adds a simple multilayer perceptron (MLP) decoder on top to combine all the feature maps and make a prediction. Other vision models, like BeIT and ViTMAE, drew inspiration from BERT's pretraining objective. [BeIT](model\\_doc/beit) is pretrained by \\*masked image modeling (MIM)\\*; the image patches are randomly masked, and the image is also tokenized into visual tokens. BeIT is trained to predict the visual tokens corresponding to the masked patches. [ViTMAE](model\\_doc/vitmae) has a similar pretraining objective, except it must predict the pixels instead of visual tokens.""]","The hybrid Vision Transformer (ViT) model was proposed by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.","Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby."
Who are the authors of the Longformer paper?,"[""The Authors' code can be found [here](https://github.com/allenai/longformer). ## Usage tips\n- Since the Longformer is based on RoBERTa, it doesn't have `token\\_type\\_ids`. You don't need to indicate which\ntoken belongs to which segment. Just separate your segments with the separation token `tokenizer.sep\\_token` (or\n``). - A transformer model replacing the attention matrices by sparse matrices to go faster. Often, the local context (e.g., what are the two tokens left and right?) is enough to take action for a given token. Some preselected input tokens are still given global attention, but the attention matrix has way less parameters, resulting in a speed-up. See the local attention section for more information. ## Longformer Self Attention\nLongformer self attention employs self attention on both a \""local\"" context and a \""global\"" context. Most tokens only\nattend \""locally\"" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\n`config.attention\\_window`. Note that `config.attention\\_window` can be of type `List` to define a\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \""globally\"" to all other tokens, as it is\nconventionally done for all tokens in `BertSelfAttention`. Note that \""locally\"" and \""globally\"" attending tokens are projected by different query, key and value matrices. Also note\nthat every \""locally\"" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \""globally\""\nattending tokens so that global attention is \\*symmetric\\*. The user can define which tokens attend \""locally\"" and which tokens attend \""globally\"" by setting the tensor\n`global\\_attention\\_mask` at run-time appropriately. All Longformer models employ the following logic for\n`global\\_attention\\_mask`:\n- 0: the token attends \""locally\"",\n- 1: the token attends \""globally\"". For more information please also refer to [`~LongformerModel.forward`] method. Using Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n\\_s \\times n\\_s)\\\\) to\n\\\\(\\mathcal{O}(n\\_s \\times w)\\\\), with \\\\(n\\_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize."", ""The abstract from the paper is the following:\n\\*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\ncommunity. To address this limitation, we propose Nystr√∂mformer -- a model that exhibits favorable scalability as a\nfunction of sequence length. Our idea is based on adapting the Nystr√∂m method to approximate standard self-attention\nwith O(n) complexity. The scalability of Nystr√∂mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\nsequence length, and find that our Nystr√∂mformer performs comparably, or in a few cases, even slightly better, than\nstandard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr√∂mformer performs\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.\\*\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/Nystromformer). ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## NystromformerConfig\n[[autodoc]] NystromformerConfig\n## NystromformerModel\n[[autodoc]] NystromformerModel\n- forward\n## NystromformerForMaskedLM\n[[autodoc]] NystromformerForMaskedLM\n- forward\n## NystromformerForSequenceClassification\n[[autodoc]] NystromformerForSequenceClassification\n- forward\n## NystromformerForMultipleChoice\n[[autodoc]] NystromformerForMultipleChoice\n- forward\n## NystromformerForTokenClassification\n[[autodoc]] NystromformerForTokenClassification\n- forward\n## NystromformerForQuestionAnswering\n[[autodoc]] NystromformerForQuestionAnswering\n- forward"", ""It is assumed that the number of \""globally\"" attending tokens is insignificant as compared to the number of\n\""locally\"" attending tokens. For more information, please refer to the official [paper](https://arxiv.org/pdf/2004.05150.pdf). ## Training\n[`LongformerForMaskedLM`] is trained the exact same way [`RobertaForMaskedLM`] is\ntrained and should be used as follows:\n```python\ninput\\_ids = tokenizer.encode(\""This is a sentence from [MASK] training data\"", return\\_tensors=\""pt\"")\nmlm\\_labels = tokenizer.encode(\""This is a sentence from the training data\"", return\\_tensors=\""pt\"")\nloss = model(input\\_ids, labels=input\\_ids, masked\\_lm\\_labels=mlm\\_labels)[0]\n```\n## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## LongformerConfig\n[[autodoc]] LongformerConfig\n## LongformerTokenizer\n[[autodoc]] LongformerTokenizer\n## LongformerTokenizerFast\n[[autodoc]] LongformerTokenizerFast\n## Longformer specific outputs\n[[autodoc]] models.longformer.modeling\\_longformer.LongformerBaseModelOutput\n[[autodoc]] models.longformer.modeling\\_longformer.LongformerBaseModelOutputWithPooling\n[[autodoc]] models.longformer.modeling\\_longformer.LongformerMaskedLMOutput\n[[autodoc]] models.longformer.modeling\\_longformer.LongformerQuestionAnsweringModelOutput\n[[autodoc]] models.longformer.modeling\\_longformer.LongformerSequenceClassifierOutput\n[[autodoc]] models.longformer.modeling\\_longformer.LongformerMultipleChoiceModelOutput\n[[autodoc]] models.longformer.modeling\\_longformer.LongformerTokenClassifierOutput\n[[autodoc]] models.longformer.modeling\\_tf\\_longformer.TFLongformerBaseModelOutput\n[[autodoc]] models.longformer.modeling\\_tf\\_longformer.TFLongformerBaseModelOutputWithPooling\n[[autodoc]] models.longformer.modeling\\_tf\\_longformer.TFLongformerMaskedLMOutput\n[[autodoc]] models.longformer.modeling\\_tf\\_longformer.TFLongformerQuestionAnsweringModelOutput\n[[autodoc]] models.longformer.modeling\\_tf\\_longformer.TFLongformerSequenceClassifierOutput\n[[autodoc]] models.longformer.modeling\\_tf\\_longformer.TFLongformerMultipleChoiceModelOutput\n[[autodoc]] models.longformer.modeling\\_tf\\_longformer.TFLongformerTokenClassifierOutput\n\n## LongformerModel\n[[autodoc]] LongformerModel\n- forward\n## LongformerForMaskedLM\n[[autodoc]] LongformerForMaskedLM\n- forward\n## LongformerForSequenceClassification\n[[autodoc]] LongformerForSequenceClassification\n- forward\n## LongformerForMultipleChoice\n[[autodoc]] LongformerForMultipleChoice\n- forward\n## LongformerForTokenClassification\n[[autodoc]] LongformerForTokenClassification\n- forward\n## LongformerForQuestionAnswering\n[[autodoc]] LongformerForQuestionAnswering\n- forward\n\n## TFLongformerModel\n[[autodoc]] TFLongformerModel\n- call\n## TFLongformerForMaskedLM\n[[autodoc]] TFLongformerForMaskedLM\n- call\n## TFLongformerForQuestionAnswering\n[[autodoc]] TFLongformerForQuestionAnswering\n- call\n## TFLongformerForSequenceClassification\n[[autodoc]] TFLongformerForSequenceClassification\n- call\n## TFLongformerForTokenClassification\n[[autodoc]] TFLongformerForTokenClassification\n- call\n## TFLongformerForMultipleChoice\n[[autodoc]] TFLongformerForMultipleChoice\n- call""]","The authors of the Longformer paper are Iz Beltagy, Matthew E. Peters, and Arman Cohan.","Iz Beltagy, Matthew E. Peters, Arman Cohan"
How many layers does the distilbert-base-uncased model have?,"[""The model has 6 layers, 768 dimension and 12 heads, totalizing 66M parameters. - `distilbert-base-uncased-distilled-squad`: A finetuned version of `distilbert-base-uncased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 86.9 on the dev set (for comparison, Bert `bert-base-uncased` version reaches a 88.5 F1 score). - `distilbert-base-cased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-cased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 65M parameters. - `distilbert-base-cased-distilled-squad`: A finetuned version of `distilbert-base-cased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 87.1 on the dev set (for comparison, Bert `bert-base-cased` version reaches a 88.7 F1 score). - `distilbert-base-german-cased`: DistilBERT German language model pretrained on 1/2 of the data used to pretrain Bert using distillation with the supervision of the `bert-base-german-dbmdz-cased` version of German DBMDZ Bert. For NER tasks the model reaches a F1 score of 83.49 on the CoNLL-2003 test set (for comparison, `bert-base-german-dbmdz-cased` reaches a 84.52 F1 score), and a F1 score of 85.23 on the GermEval 2014 test set (`bert-base-german-dbmdz-cased` reaches a 86.89 F1 score). - `distilgpt2`: DistilGPT2 English language model pretrained with the supervision of `gpt2` (the smallest version of GPT2) on [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), a reproduction of OpenAI's WebText dataset. The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 124M parameters for GPT2). On average, DistilGPT2 is two times faster than GPT2. - `distilroberta-base`: DistilRoBERTa English language model pretrained with the supervision of `roberta-base` solely on [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), a reproduction of OpenAI's WebText dataset (it is ~4 times less training data than the teacher RoBERTa). The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base). On average DistilRoBERTa is twice as fast as Roberta-base. - `distilbert-base-multilingual-cased`: DistilmBERT multilingual model pretrained with the supervision of `bert-base-multilingual-cased` on the concatenation of Wikipedia in 104 different languages. The model supports the 104 languages listed [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages). The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters (compared to 177M parameters for mBERT-base). On average DistilmBERT is twice as fast as mBERT-base. Using DistilBERT is very similar to using BERT. DistilBERT share the same tokenizer as BERT's `bert-base-uncased` even though we provide a link to this tokenizer under the `DistilBertTokenizer` name to have a consistent naming between the library models. ```python\ntokenizer = DistilBertTokenizer.from\\_pretrained('distilbert-base-cased')\nmodel = DistilBertModel.from\\_pretrained('distilbert-base-cased')\ninput\\_ids = torch.tensor(tokenizer.encode(\""Hello, my dog is cute\"")).unsqueeze(0)\noutputs = model(input\\_ids)\nlast\\_hidden\\_states = outputs[0] # The last hidden-state is the first element of the output tuple\n```\nSimilarly, using the other Distil\\* models simply consists in calling the base classes with a different pretrained checkpoint:\n- DistilBERT uncased: `model = DistilBertModel.from\\_pretrained('distilbert-base-uncased')`\n- DistilGPT2: `model = GPT2Model.from\\_pretrained('distilgpt2')`\n- DistilRoBERTa: `model = RobertaModel.from\\_pretrained('distilroberta-base')`\n- DistilmBERT: `model = DistilBertModel.from\\_pretrained('distilbert-base-multilingual-cased')`\n## How to train Distil\\*\nIn the following, we will explain how you can train DistilBERT."", ""This is\nalso a seeting where we only have `K=4` possible labels. The higher the number of classes for a given task, the more\ndrastic the speedup will be, since the zero-shot teacher's complexity scales linearly with the number of classes. Since we secretly have access to ground truth labels for AG's news, we can evaluate the accuracy of each model. The\noriginal zero-shot model `roberta-large-mnli` gets an accuracy of 69.3% on the held-out test set. After training a\nstudent on the unlabeled training set, the distilled model gets a similar score of 70.4%. Lastly, you can share the distilled model with the community and/or use it with our inference API by [uploading it\nto the ü§ó Hub](https://huggingface.co/transformers/model\\_sharing.html). We've uploaded the distilled model from this\nexample at\n[joeddav/distilbert-base-uncased-agnews-student](https://huggingface.co/joeddav/distilbert-base-uncased-agnews-student)."", ""In our experiments, we initialized our model from a few layers of the teacher (Bert) itself! Please refer to `scripts/extract.py` and `scripts/extract\\_distilbert.py` to create a valid initialization checkpoint and use `--student\\_pretrained\\_weights` argument to use this initialization for the distilled training! Happy distillation! ## Citation\nIf you find the resource useful, you should cite the following paper:\n```\n@inproceedings{sanh2019distilbert,\ntitle={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\nauthor={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\nbooktitle={NeurIPS EMC^2 Workshop},\nyear={2019}\n}\n```""]",The `distilbert-base-uncased` model has 6 layers.,6
What is the style of TPU access when using Google Colab?,"[""### What kinds of TPU are available? New users are often very confused by the range of TPUs, and the different ways to access them. The first key distinction to understand is the difference between \\*\\*TPU Nodes\\*\\* and \\*\\*TPU VMs.\\*\\*\nWhen you use a \\*\\*TPU Node\\*\\*, you are effectively indirectly accessing a remote TPU. You will need a separate VM, which will initialize your network and data pipeline and then forward them to the remote node. When you use a TPU on Google Colab, you are accessing it in the \\*\\*TPU Node\\*\\* style. Using TPU Nodes can have some quite unexpected behaviour for people who aren‚Äôt used to them! In particular, because the TPU is located on a physically different system to the machine you‚Äôre running your Python code on, your data cannot be local to your machine - any data pipeline that loads from your machine‚Äôs internal storage will totally fail! Instead, data must be stored in Google Cloud Storage where your data pipeline can still access it, even when the pipeline is running on the remote TPU node. If you can fit all your data in memory as `np.ndarray` or `tf.Tensor`, then you can `fit()` on that data even when using Colab or a TPU Node, without needing to upload it to Google Cloud Storage. \\*\\*ü§óSpecific Hugging Face Tipü§ó:\\*\\* The methods `Dataset.to\\_tf\\_dataset()` and its higher-level wrapper `model.prepare\\_tf\\_dataset()` , which you will see throughout our TF code examples, will both fail on a TPU Node. The reason for this is that even though they create a `tf.data.Dataset` it is not a ‚Äúpure‚Äù `tf.data` pipeline and uses `tf.numpy\\_function` or `Dataset.from\\_generator()` to stream data from the underlying HuggingFace `Dataset`. This HuggingFace `Dataset` is backed by data that is on a local disc and which the remote TPU Node will not be able to read. The second way to access a TPU is via a \\*\\*TPU VM.\\*\\* When using a TPU VM, you connect directly to the machine that the TPU is attached to, much like training on a GPU VM. TPU VMs are generally easier to work with, particularly when it comes to your data pipeline. All of the above warnings do not apply to TPU VMs! This is an opinionated document, so here‚Äôs our opinion: \\*\\*Avoid using TPU Node if possible.\\*\\* It is more confusing and more difficult to debug than TPU VMs. It is also likely to be unsupported in future - Google‚Äôs latest TPU, TPUv4, can only be accessed as a TPU VM, which suggests that TPU Nodes are increasingly going to become a ‚Äúlegacy‚Äù access method. However, we understand that the only free TPU access is on Colab and Kaggle Kernels, which uses TPU Node - so we‚Äôll try to explain how to handle it if you have to! Check the [TPU example notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu\\_training-tf.ipynb) for code samples that explain this in more detail. ### What sizes of TPU are available? A single TPU (a v2-8/v3-8/v4-8) runs 8 replicas. TPUs exist in \\*\\*pods\\*\\* that can run hundreds or thousands of replicas simultaneously. When you use more than a single TPU but less than a whole pod (for example, a v3-32), your TPU fleet is referred to as a \\*\\*pod slice.\\*\\*\nWhen you access a free TPU via Colab, you generally get a single v2-8 TPU."", ""Here is the demo for running T5-11B. The T5-11B model checkpoint is in FP32 which uses 42GB of memory and does not fit on Google Colab. With our 8-bit modules it only uses 11GB and fits easily:\n[![Open In Colab: T5-11b demo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing)\nOr this demo for BLOOM-3B:\n[![Open In Colab: BLOOM-3b demo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/HuggingFace\\_int8\\_demo.ipynb)\n## Scope of improvements\nThis approach, in our opinion, greatly improves access to very large models. With no performance degradation, it enables users with less compute to access models that were previously inaccessible. We've found several areas for improvement that can be worked on in the future to make this method even better for large models! ### Faster inference speed for smaller models\nAs we have seen in the [the benchmarking section](#is-it-faster-than-native-models), we could improve the runtime speed for small model (<=6B parameters) by a factor of almost 2x. However, while the inference speed is robust for large models like BLOOM-176B there are still improvements to be had for small models."", ""Introduction[[introduction]]\nWelcome to the Hugging Face course! This introduction will guide you through setting up a working environment. If you're just starting the course, we recommend you first take a look at [Chapter 1](/course/chapter1), then come back and set up your environment so you can try the code yourself. All the libraries that we'll be using in this course are available as Python packages, so here we'll show you how to set up a Python environment and install the specific libraries you'll need. We'll cover two ways of setting up your working environment, using a Colab notebook or a Python virtual environment. Feel free to choose the one that resonates with you the most. For beginners, we strongly recommend that you get started by using a Colab notebook. Note that we will not be covering the Windows system. If you're running on Windows, we recommend following along using a Colab notebook. If you're using a Linux distribution or macOS, you can use either approach described here. Most of the course relies on you having a Hugging Face account. We recommend creating one now: [create an account](https://huggingface.co/join). ## Using a Google Colab notebook[[using-a-google-colab-notebook]]\nUsing a Colab notebook is the simplest possible setup; boot up a notebook in your browser and get straight to coding! If you're not familiar with Colab, we recommend you start by following the [introduction](https://colab.research.google.com/notebooks/intro.ipynb). Colab allows you to use some accelerating hardware, like GPUs or TPUs, and it is free for smaller workloads. Once you're comfortable moving around in Colab, create a new notebook and get started with the setup:\n\n![An empty colab notebook](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter0/new_colab.png)\n\nThe next step is to install the libraries that we'll be using in this course. We'll use `pip` for the installation, which is the package manager for Python. In notebooks, you can run system commands by preceding them with the `!` character, so you can install the ü§ó Transformers library as follows:\n```\n!pip install transformers\n```\nYou can make sure the package was correctly installed by importing it within your Python runtime:\n```\nimport transformers\n```\n\n![A gif showing the result of the two commands above: installation and import](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter0/install.gif)\n\nThis installs a very light version of ü§ó Transformers. In particular, no specific machine learning frameworks (like PyTorch or TensorFlow) are installed. Since we'll be using a lot of different features of the library, we recommend installing the development version, which comes with all the required dependencies for pretty much any imaginable use case:\n```\n!pip install transformers[sentencepiece]\n```\nThis will take a bit of time, but then you'll be ready to go for the rest of the course!""]","When using Google Colab, you are accessing a TPU in the **TPU Node** style. This means you are indirectly accessing a remote TPU, and you will need a separate VM to initialize your network and data pipeline, which then forwards them to the remote node.",TPU Node style.
What is the default label inferred by Gradio for the input parameter in the GUI?,"[""Gradio Components: The Key Concepts\nIn this section, we discuss a few important concepts when it comes to components in Gradio. It's important to understand these concepts when developing your own component. Otherwise, your component may behave very different to other Gradio components! Tip: You can skip this section if you are familiar with the internals of the Gradio library, such as each component's preprocess and postprocess methods. ## Interactive vs Static\nEvery component in Gradio comes in a `static` variant, and most come in an `interactive` version as well. The `static` version is used when a component is displaying a value, and the user can \\*\\*NOT\\*\\* change that value by interacting with it. The `interactive` version is used when the user is able to change the value by interacting with the Gradio UI. Let's see some examples:\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Textbox(value=\""Hello\"", interactive=True)\ngr.Textbox(value=\""Hello\"", interactive=False)\ndemo.launch()\n```\nThis will display two textboxes. The only difference: you'll be able to edit the value of the Gradio component on top, and you won't be able to edit the variant on the bottom (i.e. the textbox will be disabled). Perhaps a more interesting example is with the `Image` component:\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Image(interactive=True)\ngr.Image(interactive=False)\ndemo.launch()\n```\nThe interactive version of the component is much more complex -- you can upload images or snap a picture from your webcam -- while the static version can only be used to display images. Not every component has a distinct interactive version. For example, the `gr.AnnotatedImage` only appears as a static version since there's no way to interactively change the value of the annotations or the image. ### What you need to remember\n\\* Gradio will use the interactive version (if available) of a component if that component is used as the \\*\\*input\\*\\* to any event; otherwise, the static version will be used. \\* When you design custom components, you \\*\\*must\\*\\* accept the boolean interactive keyword in the constructor of your Python class. In the frontend, you \\*\\*may\\*\\* accept the `interactive` property, a `bool` which represents whether the component should be static or interactive. If you do not use this property in the frontend, the component will appear the same in interactive or static mode. ## The value and how it is preprocessed/postprocessed\nThe most important attribute of a component is its `value`."", ""Note: you can pass in a default value to the state parameter,\nwhich is used as the initial value of the state. ### Using interpretation to understand predictions[[using-interpretation-to-understand-predictions]]\nMost machine learning models are black boxes and the internal logic of the function is hidden from the end user. To encourage transparency, we've made it very easy to add interpretation to your model by simply setting the interpretation keyword in the Interface class to default. This allows your users to understand what parts of the input are responsible for the output. Take a look at the simple interface below which shows an image classifier that also includes interpretation:\n```py\nimport requests\nimport tensorflow as tf\nimport gradio as gr\ninception\\_net = tf.keras.applications.MobileNetV2() # load the model\n# Download human-readable labels for ImageNet. response = requests.get(\""https://git.io/JJkYN\"")\nlabels = response.text.split(\""\\n\"")\ndef classify\\_image(inp):\ninp = inp.reshape((-1, 224, 224, 3))\ninp = tf.keras.applications.mobilenet\\_v2.preprocess\\_input(inp)\nprediction = inception\\_net.predict(inp).flatten()\nreturn {labels[i]: float(prediction[i]) for i in range(1000)}\nimage = gr.Image(shape=(224, 224))\nlabel = gr.Label(num\\_top\\_classes=3)\ntitle = \""Gradio Image Classifiction + Interpretation Example\""\ngr.Interface(\nfn=classify\\_image, inputs=image, outputs=label, interpretation=\""default\"", title=title\n).launch()\n```\nTest the interpretation function by submitting an input then clicking Interpret under the output component."", ""Besides the default interpretation method Gradio provides, you can also specify `shap` for the `interpretation` parameter and set the `num\\_shap` parameter. This uses Shapley-based interpretation, which you can read more about [here](https://christophm.github.io/interpretable-ml-book/shap.html). Lastly, you can also pass in your own interpretation function into the `interpretation` parameter. See an example in Gradio's getting started page [here](https://gradio.app/getting\\_started/). This wraps up our deep dive into the `Interface` class of Gradio. As we've seen, this class makes it simple to create machine learning demos in a few lines of Python code. However, sometimes you'll want to customise your demo by changing the layout or chaining multiple prediction functions together. Wouldn't it be nice if we could somehow split the `Interface` into customizable \""blocks\""? Fortunately, there is! That's the topic of the final section.""]","The default label inferred by Gradio for the input parameter in the GUI is ""Input"".",name
What does the 'beta_start' parameter value mean in DDPMScheduler?,"[""This happens according to a \\*\\*variance schedule\\*\\*. The original DDPM authors employed a linear schedule:\n> We set the forward process variances to constants\nincreasing linearly from \\\\(\\beta\\_1 = 10^{‚àí4}\\\\)\nto \\\\(\\beta\\_T = 0.02\\\\). However, it was shown in ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)) that better results can be achieved when employing a cosine schedule. Below, we define various schedules for the \\\\(T\\\\) timesteps (we'll choose one later on). ```python\ndef cosine\\_beta\\_schedule(timesteps, s=0.008):\n\""\""\""\ncosine schedule as proposed in https://arxiv.org/abs/2102.09672\n\""\""\""\nsteps = timesteps + 1\nx = torch.linspace(0, timesteps, steps)\nalphas\\_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) \\* torch.pi \\* 0.5) \\*\\* 2\nalphas\\_cumprod = alphas\\_cumprod / alphas\\_cumprod[0]\nbetas = 1 - (alphas\\_cumprod[1:] / alphas\\_cumprod[:-1])\nreturn torch.clip(betas, 0.0001, 0.9999)\ndef linear\\_beta\\_schedule(timesteps):\nbeta\\_start = 0.0001\nbeta\\_end = 0.02\nreturn torch.linspace(beta\\_start, beta\\_end, timesteps)\ndef quadratic\\_beta\\_schedule(timesteps):\nbeta\\_start = 0.0001\nbeta\\_end = 0.02\nreturn torch.linspace(beta\\_start\\*\\*0.5, beta\\_end\\*\\*0.5, timesteps) \\*\\* 2\ndef sigmoid\\_beta\\_schedule(timesteps):\nbeta\\_start = 0.0001\nbeta\\_end = 0.02\nbetas = torch.linspace(-6, 6, timesteps)\nreturn torch.sigmoid(betas) \\* (beta\\_end - beta\\_start) + beta\\_start\n```\nTo start with, let's use the linear schedule for \\\\(T=300\\\\) time steps and define the various variables from the \\\\(\\beta\\_t\\\\) which we will need, such as the cumulative product of the variances \\\\(\\bar{\\alpha}\\_t\\\\)."", ""However, this is not the only task scheduling library available. Feel free to use whatever you are comfortable with. The function to back up our data will look like this:\n```python\nfrom apscheduler.schedulers.background import BackgroundScheduler\ndef backup\\_db():\nshutil.copyfile(DB\\_FILE, \""./data/reviews.db\"")\ndb = sqlite3.connect(DB\\_FILE)\nreviews = db.execute(\""SELECT \\* FROM reviews\"").fetchall()\npd.DataFrame(reviews).to\\_csv(\""./data/reviews.csv\"", index=False)\nprint(\""updating db\"")\nrepo.push\\_to\\_hub(blocking=False, commit\\_message=f\""Updating data at {datetime.datetime.now()}\"")\nscheduler = BackgroundScheduler()\nscheduler.add\\_job(func=backup\\_db, trigger=\""interval\"", seconds=60)\nscheduler.start()\n```\n## Step 4 (Bonus) - Deployment to HuggingFace Spaces\nYou can use the HuggingFace [Spaces](https://huggingface.co/spaces) platform to deploy this application for free ‚ú®\nIf you haven't used Spaces before, follow the previous guide [here](/using\\_hugging\\_face\\_integrations). You will have to use the `HUB\\_TOKEN` environment variable as a secret in the Guides."", ""## Tips\nThe paper [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://huggingface.co/papers/2305.08891) claims that a mismatch between the training and inference settings leads to suboptimal inference generation results for Stable Diffusion. To fix this, the authors propose:\nüß™ This is an experimental feature! 1. rescale the noise schedule to enforce zero terminal signal-to-noise ratio (SNR)\n```py\npipe.scheduler = DDIMScheduler.from\\_config(pipe.scheduler.config, rescale\\_betas\\_zero\\_snr=True)\n```\n2. train a model with `v\\_prediction` (add the following argument to the [train\\_text\\_to\\_image.py](https://github.com/huggingface/diffusers/blob/main/examples/text\\_to\\_image/train\\_text\\_to\\_image.py) or [train\\_text\\_to\\_image\\_lora.py](https://github.com/huggingface/diffusers/blob/main/examples/text\\_to\\_image/train\\_text\\_to\\_image\\_lora.py) scripts)\n```bash\n--prediction\\_type=\""v\\_prediction\""\n```\n3. change the sampler to always start from the last timestep\n```py\npipe.scheduler = DDIMScheduler.from\\_config(pipe.scheduler.config, timestep\\_spacing=\""trailing\"")\n```\n4. rescale classifier-free guidance to prevent over-exposure\n```py\nimage = pipe(prompt, guidance\\_rescale=0.7).images[0]\n```\nFor example:\n```py\nfrom diffusers import DiffusionPipeline, DDIMScheduler\nimport torch\npipe = DiffusionPipeline.from\\_pretrained(\""ptx0/pseudo-journey-v2\"", torch\\_dtype=torch.float16)\npipe.scheduler = DDIMScheduler.from\\_config(\npipe.scheduler.config, rescale\\_betas\\_zero\\_snr=True, timestep\\_spacing=\""trailing\""\n)\npipe.to(\""cuda\"")\nprompt = \""A lion in galaxies, spirals, nebulae, stars, smoke, iridescent, intricate detail, octane render, 8k\""\nimage = pipe(prompt, guidance\\_rescale=0.7).images[0]\nimage\n```\n## DDIMScheduler\n[[autodoc]] DDIMScheduler\n## DDIMSchedulerOutput\n[[autodoc]] schedulers.scheduling\\_ddim.DDIMSchedulerOutput""]","The 'beta_start' parameter value in the DDPMScheduler represents the initial variance for the forward diffusion process in the Denoising Diffusion Probabilistic Models (DDPM). It is the starting point for the variance schedule, which defines how the noise is added to the data over the timesteps. In the provided code, 'beta_start' is set to 0.0001, indicating that this is the initial level of noise introduced at the beginning of the diffusion process.",0.0001
How do you register a custom Resnet model to the auto classes in Transformers?,"[""## Registering a model with custom code to the auto classes\nIf you are writing a library that extends ü§ó Transformers, you may want to extend the auto classes to include your own\nmodel. This is different from pushing the code to the Hub in the sense that users will need to import your library to\nget the custom models (contrarily to automatically downloading the model code from the Hub). As long as your config has a `model\\_type` attribute that is different from existing model types, and that your model\nclasses have the right `config\\_class` attributes, you can just add them to the auto classes like this:\n```py\nfrom transformers import AutoConfig, AutoModel, AutoModelForImageClassification\nAutoConfig.register(\""resnet\"", ResnetConfig)\nAutoModel.register(ResnetConfig, ResnetModel)\nAutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification)\n```\nNote that the first argument used when registering your custom config to [`AutoConfig`] needs to match the `model\\_type`\nof your custom config, and the first argument used when registering your custom models to any auto model class needs\nto match the `config\\_class` of those models."", ""## Writing a custom configuration\nBefore we dive into the model, let's first write its configuration. The configuration of a model is an object that\nwill contain all the necessary information to build the model. As we will see in the next section, the model can only\ntake a `config` to be initialized, so we really need that object to be as complete as possible. In our example, we will take a couple of arguments of the ResNet class that we might want to tweak. Different\nconfigurations will then give us the different types of ResNets that are possible. We then just store those arguments,\nafter checking the validity of a few of them. ```python\nfrom transformers import PretrainedConfig\nfrom typing import List\nclass ResnetConfig(PretrainedConfig):\nmodel\\_type = \""resnet\""\ndef \\_\\_init\\_\\_(\nself,\nblock\\_type=\""bottleneck\"",\nlayers: List[int] = [3, 4, 6, 3],\nnum\\_classes: int = 1000,\ninput\\_channels: int = 3,\ncardinality: int = 1,\nbase\\_width: int = 64,\nstem\\_width: int = 64,\nstem\\_type: str = \""\"",\navg\\_down: bool = False,\n\\*\\*kwargs,\n):\nif block\\_type not in [\""basic\"", \""bottleneck\""]:\nraise ValueError(f\""`block\\_type` must be 'basic' or bottleneck', got {block\\_type}.\"")\nif stem\\_type not in [\""\"", \""deep\"", \""deep-tiered\""]:\nraise ValueError(f\""`stem\\_type` must be '', 'deep' or 'deep-tiered', got {stem\\_type}.\"")\nself.block\\_type = block\\_type\nself.layers = layers\nself.num\\_classes = num\\_classes\nself.input\\_channels = input\\_channels\nself.cardinality = cardinality\nself.base\\_width = base\\_width\nself.stem\\_width = stem\\_width\nself.stem\\_type = stem\\_type\nself.avg\\_down = avg\\_down\nsuper().\\_\\_init\\_\\_(\\*\\*kwargs)\n```\nThe three important things to remember when writing you own configuration are the following:\n- you have to inherit from `PretrainedConfig`,\n- the `\\_\\_init\\_\\_` of your `PretrainedConfig` must accept any kwargs,\n- those `kwargs` need to be passed to the superclass `\\_\\_init\\_\\_`. The inheritance is to make sure you get all the functionality from the ü§ó Transformers library, while the two other\nconstraints come from the fact a `PretrainedConfig` has more fields than the ones you are setting. When reloading a\nconfig with the `from\\_pretrained` method, those fields need to be accepted by your config and then sent to the\nsuperclass. Defining a `model\\_type` for your configuration (here `model\\_type=\""resnet\""`) is not mandatory, unless you want to\nregister your model with the auto classes (see last section). With this done, you can easily create and save your configuration like you would do with any other model config of the\nlibrary. Here is how we can create a resnet50d config and save it:\n```py\nresnet50d\\_config = ResnetConfig(block\\_type=\""bottleneck\"", stem\\_width=32, stem\\_type=\""deep\"", avg\\_down=True)\nresnet50d\\_config.save\\_pretrained(\""custom-resnet\"")\n```\nThis will save a file named `config.json` inside the folder `custom-resnet`. You can then reload your config with the\n`from\\_pretrained` method:\n```py\nresnet50d\\_config = ResnetConfig.from\\_pretrained(\""custom-resnet\"")\n```\nYou can also use any other method of the [`PretrainedConfig`] class, like [`~PretrainedConfig.push\\_to\\_hub`] to\ndirectly upload your config to the Hub. ## Writing a custom model\nNow that we have our ResNet configuration, we can go on writing the model. We will actually write two: one that\nextracts the hidden features from a batch of images (like [`BertModel`]) and one that is suitable for image\nclassification (like [`BertForSequenceClassification`]). As we mentioned before, we'll only write a loose wrapper of the model to keep it simple for this example. The only\nthing we need to do before writing this class is a map between the block types and actual block classes. Then the\nmodel is defined from the configuration by passing everything to the `ResNet` class:\n```py\nfrom transformers import PreTrainedModel\nfrom timm.models.resnet import BasicBlock, Bottleneck, ResNet\nfrom .configuration\\_resnet import ResnetConfig\nBLOCK\\_MAPPING = {\""basic\"": BasicBlock, \""bottleneck\"": Bottleneck}\nclass ResnetModel(PreTrainedModel):\nconfig\\_class = ResnetConfig\ndef \\_\\_init\\_\\_(self, config):\nsuper().\\_\\_init\\_\\_(config)\nblock\\_layer = BLOCK\\_MAPPING[config.block\\_type]\nself.model = ResNet(\nblock\\_layer,\nconfig.layers,\nnum\\_classes=config.num\\_classes,\nin\\_chans=config.input\\_channels,\ncardinality=config.cardinality,\nbase\\_width=config.base\\_width,\nstem\\_width=config.stem\\_width,\nstem\\_type=config.stem\\_type,\navg\\_down=config.avg\\_down,\n)\ndef forward(self, tensor):\nreturn self.model.forward\\_features(tensor)\n```\nFor the model that will classify images, we just change the forward method:\n```py\nimport torch\nclass ResnetModelForImageClassification(PreTrainedModel):\nconfig\\_class = ResnetConfig\ndef \\_\\_init\\_\\_(self, config):\nsuper().\\_\\_init\\_\\_(config)\nblock\\_layer = BLOCK\\_MAPPING[config.block\\_type]\nself.model = ResNet(\nblock\\_layer,\nconfig.layers,\nnum\\_classes=config.num\\_classes,\nin\\_chans=config.input\\_channels,\ncardinality=config.cardinality,\nbase\\_width=config.base\\_width,\nstem\\_width=config.stem\\_width,\nstem\\_type=config.stem\\_type,\navg\\_down=config.avg\\_down,\n)\ndef forward(self, tensor, labels=None):\nlogits = self.model(tensor)\nif labels is not None:\nloss = torch.nn.cross\\_entropy(logits, labels)\nreturn {\""loss\"": loss, \""logits\"": logits}\nreturn {\""logits\"": logits}\n```\nIn both cases, notice how we inherit from `PreTrainedModel` and call the superclass initialization with the `config`\n(a bit like when you write a regular `torch.nn.Module`)."", ""You can check the result\nin this [model repo](https://huggingface.co/sgugger/custom-resnet50d). See the [sharing tutorial](model\\_sharing) for more information on the push to Hub method. ## Using a model with custom code\nYou can use any configuration, model or tokenizer with custom code files in its repository with the auto-classes and\nthe `from\\_pretrained` method. All files and code uploaded to the Hub are scanned for malware (refer to the [Hub security](https://huggingface.co/docs/hub/security#malware-scanning) documentation for more information), but you should still\nreview the model code and author to avoid executing malicious code on your machine. Set `trust\\_remote\\_code=True` to use\na model with custom code:\n```py\nfrom transformers import AutoModelForImageClassification\nmodel = AutoModelForImageClassification.from\\_pretrained(\""sgugger/custom-resnet50d\"", trust\\_remote\\_code=True)\n```\nIt is also strongly encouraged to pass a commit hash as a `revision` to make sure the author of the models did not\nupdate the code with some malicious new lines (unless you fully trust the authors of the models). ```py\ncommit\\_hash = \""ed94a7c6247d8aedce4647f00f20de6875b5b292\""\nmodel = AutoModelForImageClassification.from\\_pretrained(\n\""sgugger/custom-resnet50d\"", trust\\_remote\\_code=True, revision=commit\\_hash\n)\n```\nNote that when browsing the commit history of the model repo on the Hub, there is a button to easily copy the commit\nhash of any commit.""]","To register a custom ResNet model to the auto classes in Transformers, you need to follow these steps:

1. Ensure that your configuration has a `model_type` attribute that is different from existing model types.
2. Make sure your model classes have the correct `config_class` attributes.
3. Use the `register` method to add your custom configuration and models to the auto classes.

Here is an example of how to do this:

```python
from transformers import AutoConfig, AutoModel, AutoModelForImageClassification

# Register the custom configuration
AutoConfig.register(""resnet"", ResnetConfig)

# Register the custom models
AutoModel.register(ResnetConfig, ResnetModel)
AutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification)
```

In this code:
- The first argument used when registering your custom config to `AutoConfig` needs to match the `model_type` of your custom config.
- The first argument used when registering your custom models to any auto model class needs to match the `config_class` of those models.","Use AutoConfig.register(""resnet"", ResnetConfig), AutoModel.register(ResnetConfig, ResnetModel), and AutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification) to register the custom Resnet model to the auto classes."
Who is the author of the paper 'Learning Transferable Visual Models From Natural Language Supervision'?,"[""It can be\ninstructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing\nfor the task, similarly to the zero-shot capabilities of GPT-2 and 3. The abstract from the paper is the following:\n\\*State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This\nrestricted form of supervision limits their generality and usability since additional labeled data is needed to specify\nany other visual concept. Learning directly from raw text about images is a promising alternative which leverages a\nmuch broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes\nwith which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400\nmillion (image, text) pairs collected from the internet. After pre-training, natural language is used to reference\nlearned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study\nthe performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks\nsuch as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need\nfor any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot\nwithout needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained\nmodel weights at this https URL.\\*\nThis model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/openai/CLIP). ## Usage tips and example\nCLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image\nclassification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text\nfeatures."", ""The model would have separate or fused encoders for vision and language to achieve this understanding. But these inputs and outputs can take several forms. Below we give some examples:\n- Image retrieval from natural language text. - Phrase grounding, i.e., performing object detection from an input image and natural language phrase (example: A \\*\\*young person\\*\\* swings a \\*\\*bat\\*\\*). - Visual question answering, i.e., finding answers from an input image and a question in natural language. - Generate a caption for a given image. This can also take the form of conditional text generation, where you'd start with a natural language prompt and an image. - Detection of hate speech from social media content involving both images and text modalities. ## Learning Strategies\nA vision-language model typically consists of 3 key elements: an image encoder, a text encoder, and a strategy to fuse information from the two encoders. These key elements are tightly coupled together as the loss functions are designed around both the model architecture and the learning strategy. While vision-language model research is hardly a new research area, the design of such models has changed tremendously over the years. Whereas earlier research adopted hand-crafted image descriptors and pre-trained word vectors or the frequency-based TF-IDF features, the latest research predominantly adopts image and text encoders with [transformer](https://arxiv.org/abs/1706.03762) architectures to separately or jointly learn image and text features. These models are pre-trained with strategic pre-training objectives that enable various downstream tasks. In this section, we'll discuss some of the typical pre-training objectives and strategies for vision-language models that have been shown to perform well regarding their transfer performance. We'll also touch upon additional interesting things that are either specific to these objectives or can be used as general components for pre-training. We‚Äôll cover the following themes in the pre-training objectives:\n- \\*\\*Contrastive Learning:\\*\\* Aligning images and texts to a joint feature space in a contrastive manner\n- \\*\\*PrefixLM:\\*\\* Jointly learning image and text embeddings by using images as a prefix to a language model\n- \\*\\*Multi-modal Fusing with Cross Attention:\\*\\* Fusing visual information into layers of a language model with a cross-attention mechanism\n- \\*\\*MLM / ITM:\\*\\* Aligning parts of images with text with masked-language modeling and image-text matching objectives\n- \\*\\*No Training:\\*\\* Using stand-alone vision and language models via iterative optimization\nNote that this section is a non-exhaustive list, and there are various other approaches, as well as hybrid strategies such as [Unified-IO](https://arxiv.org/abs/2206.08916). For a more comprehensive review of multi-modal models, refer to [this work.](https://arxiv.org/abs/2210.09263)\n### 1) Contrastive Learning\n\n![Contrastive Learning](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/contrastive_learning.png)  \n*Contrastive pre-training and zero-shot image classification as shown [here](https://openai.com/blog/clip).*\n\nContrastive learning is a commonly used pre-training objective for vision models and has proven to be a highly effective pre-training objective for vision-language models as well."", ""--\ntitle: \""A Dive into Vision-Language Models\""\nthumbnail: /blog//assets/128\\_vision\\_language\\_pretraining/thumbnail.png\nauthors:\n- user: adirik\n- user: sayakpaul\n---\n# A Dive into Vision-Language Models\nHuman learning is inherently multi-modal as jointly leveraging multiple senses helps us understand and analyze new information better. Unsurprisingly, recent advances in multi-modal learning take inspiration from the effectiveness of this process to create models that can process and link information using various modalities such as image, video, text, audio, body gestures, facial expressions, and physiological signals. Since 2021, we‚Äôve seen an increased interest in models that combine vision and language modalities (also called joint vision-language models), such as [OpenAI‚Äôs CLIP](https://openai.com/blog/clip/). Joint vision-language models have shown particularly impressive capabilities in very challenging tasks such as image captioning, text-guided image generation and manipulation, and visual question-answering. This field continues to evolve, and so does its effectiveness in improving zero-shot generalization leading to various practical use cases. In this blog post, we'll introduce joint vision-language models focusing on how they're trained.""]",The paper 'Learning Transferable Visual Models From Natural Language Supervision' was contributed by [valhalla](https://huggingface.co/valhalla).,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever."
What is the purpose of local attention in Longformer?,"[""The Authors' code can be found [here](https://github.com/allenai/longformer). ## Usage tips\n- Since the Longformer is based on RoBERTa, it doesn't have `token\\_type\\_ids`. You don't need to indicate which\ntoken belongs to which segment. Just separate your segments with the separation token `tokenizer.sep\\_token` (or\n``). - A transformer model replacing the attention matrices by sparse matrices to go faster. Often, the local context (e.g., what are the two tokens left and right?) is enough to take action for a given token. Some preselected input tokens are still given global attention, but the attention matrix has way less parameters, resulting in a speed-up. See the local attention section for more information. ## Longformer Self Attention\nLongformer self attention employs self attention on both a \""local\"" context and a \""global\"" context. Most tokens only\nattend \""locally\"" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\n`config.attention\\_window`. Note that `config.attention\\_window` can be of type `List` to define a\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \""globally\"" to all other tokens, as it is\nconventionally done for all tokens in `BertSelfAttention`. Note that \""locally\"" and \""globally\"" attending tokens are projected by different query, key and value matrices. Also note\nthat every \""locally\"" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \""globally\""\nattending tokens so that global attention is \\*symmetric\\*. The user can define which tokens attend \""locally\"" and which tokens attend \""globally\"" by setting the tensor\n`global\\_attention\\_mask` at run-time appropriately. All Longformer models employ the following logic for\n`global\\_attention\\_mask`:\n- 0: the token attends \""locally\"",\n- 1: the token attends \""globally\"". For more information please also refer to [`~LongformerModel.forward`] method. Using Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n\\_s \\times n\\_s)\\\\) to\n\\\\(\\mathcal{O}(n\\_s \\times w)\\\\), with \\\\(n\\_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize."", ""This can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion, especially with a small number of layers. Growing the receptive field has been shown to improve quality significantly (Salimans et al.). Doing so, however, comes at a significant cost in number of parameters and consequently computational performance and can make training such models more challenging. >\n> ... self-attention can achieve a better balance in the trade-off between the virtually unlimited receptive field of the necessarily sequential PixelRNN and the limited receptive field of the much more parallelizable PixelCNN and its various extensions. [Image Transformer](https://arxiv.org/abs/1802.05751) uses transformers by restricting self attention over local neighborhoods of pixels. [Taming Transformers](https://arxiv.org/abs/2012.09841) and [DALL-E 1](https://arxiv.org/abs/2102.12092) combine convolutions and transformers. Both train a VQ-VAE to learn a discrete latent space, and then a transformer is trained in the compressed latent space. The transformer context is global but masked, because attention is provided over all previously predicted latent pixels, but the model is still AR so attention cannot be provided over not yet predicted pixels. [ImageBART](https://arxiv.org/abs/2108.08827) combines convolutions, transformers, and diffusion processes."", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# LED\n## Overview\nThe LED model was proposed in [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz\nBeltagy, Matthew E. Peters, Arman Cohan. The abstract from the paper is the following:\n\\*Transformer-based models are unable to process long sequences due to their self-attention operation, which scales\nquadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or\nlonger. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we\nevaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8.""]","The purpose of local attention in Longformer is to allow each token to attend to a limited context of tokens around it, specifically to its \(\frac{1}{2} w\) previous tokens and \(\frac{1}{2} w\) succeeding tokens, where \(w\) is the window length defined in `config.attention_window`. This mechanism helps to reduce the memory and time complexity of the self-attention operation from \(\mathcal{O}(n_s \times n_s)\) to \(\mathcal{O}(n_s \times w)\), making it more efficient for processing long sequences. Local attention is particularly useful because it captures the immediate context of tokens, which is often sufficient for many tasks, while still allowing a few selected tokens to attend globally to all other tokens.","To take action for a given token using the local context, such as the two tokens to the left and right, and to build a representation of the whole sentence by stacking attention layers with a small window."
What library is used to import the text encoder?,"[""Quilter ist der Apostel der Mittelschicht und wir freuen uns, sein Evangelium willkommen hei√üen zu k√∂nnen. ```\n## Training\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other encoder-decoder model on a dataset of (speech, text) pairs. As you can see, only 2 inputs are required for the model in order to compute a loss: `input\\_values` (which are the\nspeech inputs) and `labels` (which are the `input\\_ids` of the encoded target sequence). ```python\n>>> from transformers import AutoTokenizer, AutoFeatureExtractor, SpeechEncoderDecoderModel\n>>> from datasets import load\\_dataset\n>>> encoder\\_id = \""facebook/wav2vec2-base-960h\"" # acoustic model encoder\n>>> decoder\\_id = \""bert-base-uncased\"" # text decoder\n>>> feature\\_extractor = AutoFeatureExtractor.from\\_pretrained(encoder\\_id)\n>>> tokenizer = AutoTokenizer.from\\_pretrained(decoder\\_id)\n>>> # Combine pre-trained encoder and pre-trained decoder to form a Seq2Seq model\n>>> model = SpeechEncoderDecoderModel.from\\_encoder\\_decoder\\_pretrained(encoder\\_id, decoder\\_id)\n>>> model.config.decoder\\_start\\_token\\_id = tokenizer.cls\\_token\\_id\n>>> model.config.pad\\_token\\_id = tokenizer.pad\\_token\\_id\n>>> # load an audio input and pre-process (normalise mean/std to 0/1)\n>>> ds = load\\_dataset(\""hf-internal-testing/librispeech\\_asr\\_dummy\"", \""clean\"", split=\""validation\"")\n>>> input\\_values = feature\\_extractor(ds[0][\""audio\""][\""array\""], return\\_tensors=\""pt\"").input\\_values\n>>> # load its corresponding transcription and tokenize to generate labels\n>>> labels = tokenizer(ds[0][\""text\""], return\\_tensors=\""pt\"").input\\_ids\n>>> # the forward function automatically creates the correct decoder\\_input\\_ids\n>>> loss = model(input\\_values=input\\_values, labels=labels).loss\n>>> loss.backward()\n```\n## SpeechEncoderDecoderConfig\n[[autodoc]] SpeechEncoderDecoderConfig\n## SpeechEncoderDecoderModel\n[[autodoc]] SpeechEncoderDecoderModel\n- forward\n- from\\_encoder\\_decoder\\_pretrained\n## FlaxSpeechEncoderDecoderModel\n[[autodoc]] FlaxSpeechEncoderDecoderModel\n- \\_\\_call\\_\\_\n- from\\_encoder\\_decoder\\_pretrained"", ""As we've seen before, this means that the resulting representation of a word has been affected by other words in the sequence. We won't dive into the specifics here, but we'll offer some further readings if you want to get a better understanding at what happens under the hood. So when should one use an encoder? Encoders can be used as standalone models in a wide variety of tasks. For example BERT, arguably the most famous transformer model, is a standalone encoder model and at the time of release, beat the state of the art in many sequence classification tasks, question answering tasks, and masked language modeling, to only cite a few. The idea is that encoders are very powerful at extracting vectors that carry meaningful information about a sequence. This vector can then be handled down the road by additional layers of neurons to make sense of them. Let's take a look at some examples where encoders really shine. First of all, Masked Language Modeling, or MLM. It's the task of predicting a hidden word in a sequence of words. Here, for example, we have hidden the word between \""My\"" and \""is\"". This is one of the objectives with which BERT was trained: it was trained to predict hidden words in a sequence. Encoders shine in this scenario in particular, as bidirectional information is crucial here. If we didn't have the words on the right (is, Sylvain, and the dot), then there is very little chance that BERT would have been able to identify \""name\"" as the correct word. The encoder needs to have a good understanding of the sequence in order to predict a masked word, as even if the text is grammatically correct, It does not necessarily make sense in the context of the sequence. As mentioned earlier, encoders are good at doing sequence classification. Sentiment analysis is an example of a sequence classification task. The model's aim is to identify the sentiment of a sequence ‚Äì it can range from giving a sequence a rating from one to five stars if doing review analysis, to giving a positive or negative rating to a sequence, which is what is shown here. For example here, given the two sequences, we use the model to compute a prediction and to classify the sequences among these two classes: positive and negative. While the two sequences are very similar, containing the same words, the meaning is different ‚Äì and the encoder model is able to grasp that difference."", ""Text Summarization with Pretrained Encoders\nThis folder contains part of the code necessary to reproduce the results on abstractive summarization from the article [Text Summarization with Pretrained Encoders](https://arxiv.org/pdf/1908.08345.pdf) by [Yang Liu](https://nlp-yang.github.io/) and [Mirella Lapata](https://homepages.inf.ed.ac.uk/mlap/). It can also be used to summarize any document. The original code can be found on the Yang Liu's [github repository](https://github.com/nlpyang/PreSumm). The model is loaded with the pre-trained weights for the abstractive summarization model trained on the CNN/Daily Mail dataset with an extractive and then abstractive tasks. ## Setup\n```\ngit clone https://github.com/huggingface/transformers && cd transformers\npip install . pip install nltk py-rouge\ncd examples/seq2seq/bertabs\n```\n## Reproduce the authors' ROUGE score\nTo be able to reproduce the authors' results on the CNN/Daily Mail dataset you first need to download both CNN and Daily Mail datasets [from Kyunghyun Cho's website](https://cs.nyu.edu/~kcho/DMQA/) (the links next to \""Stories\"") in the same folder.""]","The library used to import the text encoder is `transformers`. Specifically, you can use `AutoTokenizer` from the `transformers` library to import the text encoder.",Transformers
What license is used for TheBloke/Chronohermes-Grad-L2-13B-GPTQ?,"[""More interestingly, the degradation is minimal! ## Conclusion and final words\nIn this blogpost, we compared bitsandbytes and GPTQ quantization across multiple setups. We saw that bitsandbytes is better suited for fine-tuning while GPTQ is better for generation. From this observation, one way to get better merged models would be to:\n- (1) quantize the base model using bitsandbytes (zero-shot quantization)\n- (2) add and fine-tune the adapters\n- (3) merge the trained adapters on top of the base model or the [dequantized model](https://github.com/huggingface/peft/pull/851/files) ! - (4) quantize the merged model using GPTQ and use it for deployment\nWe hope that this overview will make it easier for everyone to use LLMs in their applications and usecases, and we are looking forward to seeing what you will build with it! ## Acknowledgements\nWe would like to thank [Ilyas](https://huggingface.co/IlyasMoutawwakil), [Cl√©mentine](https://huggingface.co/clefourrier) and [Felix](https://huggingface.co/fxmarty) for their help on the benchmarking."", ""‚ÄºÔ∏è Commercial use:\nFalcon 180b can be commercially used but under very restrictive conditions, excluding any \""hosting use\"". We recommend to check the [license](https://huggingface.co/spaces/tiiuae/falcon-180b-license/blob/main/LICENSE.txt) and consult your legal team if you are interested in using it for commercial purposes. ## How good is Falcon 180B? Falcon 180B is the best openly released LLM today, outperforming Llama 2 70B and OpenAI‚Äôs GPT-3.5 on MMLU, and is on par with Google's PaLM 2-Large on HellaSwag, LAMBADA, WebQuestions, Winogrande, PIQA, ARC, BoolQ, CB, COPA, RTE, WiC, WSC, ReCoRD. Falcon 180B typically sits somewhere between GPT 3.5 and GPT4 depending on the evaluation benchmark and further finetuning from the community will be very interesting to follow now that it's openly released. ![Palm 2 comparison](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/162\\_falcon\\_180b/palm2\\_480.jpg)\nWith 68.74 on the Hugging Face Leaderboard, Falcon 180B is the highest-scoring openly released pre-trained LLM, surpassing Meta‚Äôs LLaMA 2 (67.35). | Model | Size | Leaderboard score | Commercial use or license | Pretraining length |\n| ------- | ---- | ----------------- | ------------------------- | ------------------ |\n| Falcon | 180B | 68.74 | üü† | 3,500B |\n| Llama 2 | 70B | 67.35 | üü† | 2,000B |\n| LLaMA | 65B | 64.23 | üî¥ | 1,400B |\n| Falcon | 40B | 61.48 | üü¢ | 1,000B |\n| MPT | 30B | 56.15 | üü¢ | 1,000B |\n![open\\_llm\\_leaderboard.png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/162\\_falcon\\_180b/open\\_llm\\_leaderboard.jpg)\nThe quantized Falcon models preserve similar metrics across benchmarks."", ""If you want to learn more about Llama 2 check out this [blog post](https://huggingface.co/blog/llama2). ### What is GPTQ? GPTQ is a post-training quantziation method to compress LLMs, like GPT. GPTQ compresses GPT (decoder) models by reducing the number of bits needed to store each weight in the model, from 32 bits down to just 3-4 bits. This means the model takes up much less memory and can run on less Hardware, e.g. Single GPU for 13B Llama2 models. GPTQ analyzes each layer of the model separately and approximates the weights to preserve the overall accuracy. If you want to learn more and how to use it, check out [Optimize open LLMs using GPTQ and Hugging Face Optimum](https://www.philschmid.de/gptq-llama). ## Benchmark\nTo benchmark the real-world performance of Llama 2, we tested 3 model sizes (7B, 13B, 70B parameters) on four different instance types with four different load levels, resulting in 60 different configurations:\n- Models: We evaluated all currently available model sizes, including 7B, 13B, and 70B.""]",The license for TheBloke/Chronohermes-Grad-L2-13B-GPTQ can be found on its Hugging Face model page. Please check the specific model's page for detailed licensing information.,llama-2-community-license
What is a potential legal issue with using outputs from language models?,"[""The prompts are passed through the initial language model to generate new text. Human annotators are used to rank the generated text outputs from the LM. One may initially think that humans should apply a scalar score directly to each piece of text in order to generate a reward model, but this is difficult to do in practice. The differing values of humans cause these scores to be uncalibrated and noisy. Instead, rankings are used to compare the outputs of multiple models and create a much better regularized dataset. There are multiple methods for ranking the text. One method that has been successful is to have users compare generated text from two language models conditioned on the same prompt. By comparing model outputs in head-to-head matchups, an [Elo](https://en.wikipedia.org/wiki/Elo\\_rating\\_system) system can be used to generate a ranking of the models and outputs relative to each-other. These different methods of ranking are normalized into a scalar reward signal for training. An interesting artifact of this process is that the successful RLHF systems to date have used reward language models with varying sizes relative to the text generation (e.g. OpenAI 175B LM, 6B reward model, Anthropic used LM and reward models from 10B to 52B, DeepMind uses 70B Chinchilla models for both LM and reward)."", ""4. \\*\\*Refine the content.\\*\\* I continue to refine the generated content to better fit the game. ![ChatGPT for Story #4](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/chatgpt4.png)\n\nI'm happy with this result. So, should I use it directly? Maybe. Since this is a free game being developed for an AI tutorial, probably. However, it may not be straightforward for commercial products, having potential unintended legal, ethical, and commercial ramifications. > ‚ö†Ô∏è \\*\\*Limitation:\\*\\* Using outputs from language models directly may have unintended legal, ethical, and commercial ramifications. Some potential unintended ramifications of using outputs directly are as follows:\n- Legal: The legal landscape surrounding Generative AI is currently very unclear, with several ongoing lawsuits. - Ethical: Language models can produce plagiarized or biased outputs. For more information, check out the [Ethics and Society Newsletter](https://huggingface.co/blog/ethics-soc-2). - Commercial: [Some](https://www.searchenginejournal.com/google-says-ai-generated-content-is-against-guidelines/444916/) sources have stated that AI-generated content may be deprioritized by search engines. This [may not](https://seo.ai/blog/google-is-not-against-ai-content) be the case for most non-spam content, but is worth considering. Tools such as [AI Content Detector](https://writer.com/ai-content-detector/) can be used to check whether content may be detected as AI-generated. There is ongoing research on language model [watermarking](https://arxiv.org/abs/2301.10226) which may mark text as AI-generated. Given these limitations, the safest approach may be to use language models like ChatGPT for brainstorming but write the final content by hand."", ""n this video we take a look at the data processing necessary to train causal language models. Causal Language Modeling is the task of predicting the next token based on the previous token. Another term for Causal Language Modeling is Autoregressive Modeling. In the example that you see here the next token could for example be NLP or machine learning. A popular example of a Causal Language Model is the GPT family of models. To train such models such as GPT-2 we usually start with a large corpus of text files. These files can webpages scraped from the internet such as the Common Crawl dataset or they can be Python files from GitHub like you can see here. As a first step we need to tokenize these files such that we can feed them through a model. Here we show the tokenized texts as bars of various length illustrating the different sequence lengths. Normally, the text files come in various sizes and which results in various sequence length of the tokenized texts. Transformer models have a limited context length and depending on the data source it is possible that the tokenized texts are much longer than this context length. In this case we could just truncate the sequence to the context length but this would mean that we loose everything after the context length. Using the return overflowing tokens flag in the we can use the tokenizer to create chunks with each one being the size of the context length. Sometimes it can happen that the last chunk is too short if there aren‚Äôt enough tokens to fill it. In this case we would like to remove it. With the return\\_length keyword we also get the length of each chunk from the tokenizer. This function shows all the steps necessary to prepare the dataset. First we tokenize the dataset with the flags I just mentioned. Then we go through each chunk and if its length matches the context length we add it to the inputs we return. We can apply this function to the whole dataset and we make sure to use batches and remove the existing columns. We need to remove columns because we can create multiple samples per text and the shapes in the dataset would not match.""]","A potential legal issue with using outputs from language models is that the legal landscape surrounding Generative AI is currently very unclear, with several ongoing lawsuits. This uncertainty may lead to legal ramifications when using AI-generated content directly.","The legal landscape surrounding Generative AI is currently very unclear, with several ongoing lawsuits."
What is the filename used when downloading the model from the Hugging Face Hub?,"[""n this video, we're going to understand how to manage a model repository on the HuggingFace model hub. In order to handle a repository, you should first have a Hugging Face account. A link to create a new account is available in the description. Once you are logged in, you can create a new repository by clicking on the \""New model\"" option. You should be facing a similar modal to the following. In the \""Owner\"" input, you can put either your own namespace or any of your organisations namespaces. The model name is the model identifier that will then be used to identify your model on your chosen namespace. The final choice is between public and private. Public models are accessible by anyone. This is the recommended, free option, as this makes your model easily accessible and shareable. The owners of your namespace are the only ones who can update and change your model. A more advanced option is the private option. In this case, only the owners of your namespace will have visibility over your model. Other users won't know it exists and will not be able to use it. Let's create a dummy model to play with. Once your model is created, comes the management of that model! Three tabs are available to you. You're facing the first one, which is the model card page; this is the page used to showcase your model to the world."", ""!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Manage `huggingface\\_hub` cache-system\n## Understand caching\nThe Hugging Face Hub cache-system is designed to be the central cache shared across libraries\nthat depend on the Hub. It has been updated in v0.8.0 to prevent re-downloading same files\nbetween revisions. The caching system is designed as follows:\n```\n‚îú‚îÄ \n‚îú‚îÄ \n‚îú‚îÄ \n```\nThe `` is usually your user's home directory. However, it is customizable with the `cache\\_dir` argument on all methods, or by specifying either `HF\\_HOME` or `HF\\_HUB\\_CACHE` environment variable. Models, datasets and spaces share a common root. Each of these repositories contains the\nrepository type, the namespace (organization or username) if it exists and the\nrepository name:\n```\n‚îú‚îÄ models--julien-c--EsperBERTo-small\n‚îú‚îÄ models--lysandrejik--arxiv-nlp\n‚îú‚îÄ models--bert-base-cased\n‚îú‚îÄ datasets--glue\n‚îú‚îÄ datasets--huggingface--DataMeasurementsFiles\n‚îú‚îÄ spaces--dalle-mini--dalle-mini\n```\nIt is within these folders that all files will now be downloaded from the Hub. Caching ensures that\na file isn't downloaded twice if it already exists and wasn't updated; but if it was updated,\nand you're asking for the latest file, then it will download the latest file (while keeping\nthe previous file intact in case you need it again). In order to achieve this, all folders contain the same skeleton:\n```\n‚îú‚îÄ datasets--glue\n‚îÇ ‚îú‚îÄ refs\n‚îÇ ‚îú‚îÄ blobs\n‚îÇ ‚îú‚îÄ snapshots\n... ```\nEach folder is designed to contain the following:\n### Refs\nThe `refs` folder contains files which indicates the latest revision of the given reference. For example,\nif we have previously fetched a file from the `main` branch of a repository, the `refs`\nfolder will contain a file named `main`, which will itself contain the commit identifier of the current head. If the latest commit of `main` has `aaaaaa` as identifier, then it will contain `aaaaaa`. If that same branch gets updated with a new commit, that has `bbbbbb` as an identifier, then\nre-downloading a file from that reference will update the `refs/main` file to contain `bbbbbb`. ### Blobs\nThe `blobs` folder contains the actual files that we have downloaded. The name of each file is their hash. ### Snapshots\nThe `snapshots` folder contains symlinks to the blobs mentioned above. It is itself made up of several folders:\none per known revision! In the explanation above, we had initially fetched a file from the `aaaaaa` revision, before fetching a file from\nthe `bbbbbb` revision. In this situation, we would now have two folders in the `snapshots` folder: `aaaaaa`\nand `bbbbbb`. In each of these folders, live symlinks that have the names of the files that we have downloaded. For example,\nif we had downloaded the `README.md` file at revision `aaaaaa`, we would have the following path:\n```\n//snapshots/aaaaaa/README.md\n```\nThat `README.md` file is actually a symlink linking to the blob that has the hash of the file. By creating the skeleton this way we open the mechanism to file sharing: if the same file was fetched in\nrevision `bbbbbb`, it would have the same hash and the file would not need to be re-downloaded. ### .no\\_exist (advanced)\nIn addition to the `blobs`, `refs` and `snapshots` folders, you might also find a `.no\\_exist` folder\nin your cache. This folder keeps track of files that you've tried to download once but don't exist\non the Hub. Its structure is the same as the `snapshots` folder with 1 subfolder per known revision:\n```\n//.no\\_exist/aaaaaa/config\\_that\\_does\\_not\\_exist.json\n```\nUnlike the `snapshots` folder, files are simple empty files (no symlinks). In this example,\nthe file `\""config\\_that\\_does\\_not\\_exist.json\""` does not exist on the Hub for the revision `\""aaaaaa\""`. As it only stores empty files, this folder is neglectable is term of disk usage. So now you might wonder, why is this information even relevant? In some cases, a framework tries to load optional files for a model. Saving the non-existence\nof optional files makes it faster to load a model as it saves 1 HTTP call per possible optional file. This is for example the case in `transformers` where each tokenizer can support additional files. The first time you load the tokenizer on your machine, it will cache which optional files exists (and\nwhich doesn't) to make the loading time faster for the next initializations. To test if a file is cached locally (without making any HTTP request), you can use the [`try\\_to\\_load\\_from\\_cache`]\nhelper. It will either return the filepath (if exists and cached), the object `\\_CACHED\\_NO\\_EXIST` (if non-existence\nis cached) or `None` (if we don't know). ```python\nfrom huggingface\\_hub import try\\_to\\_load\\_from\\_cache, \\_CACHED\\_NO\\_EXIST\nfilepath = try\\_to\\_load\\_from\\_cache()\nif isinstance(filepath, str):\n# file exists and is cached\n... elif filepath is \\_CACHED\\_NO\\_EXIST:\n# non-existence of file is cached\n... else:\n# file is not cached\n... ```\n### In practice\nIn practice, your cache should look like the following tree:\n```text\n[ 96] . ‚îî‚îÄ‚îÄ [ 160] models--julien-c--EsperBERTo-small\n‚îú‚îÄ‚îÄ [ 160] blobs\n‚îÇ ‚îú‚îÄ‚îÄ [321M] 403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\n‚îÇ ‚îú‚îÄ‚îÄ [ 398] 7cb18dc9bafbfcf74629a4b760af1b160957a83e\n‚îÇ ‚îî‚îÄ‚îÄ [1.4K] d7edf6bd2a681fb0175f7735299831ee1b22b812\n‚îú‚îÄ‚îÄ [ 96] refs\n‚îÇ ‚îî‚îÄ‚îÄ [ 40] main\n‚îî‚îÄ‚îÄ [ 128] snapshots\n‚îú‚îÄ‚îÄ [ 128] 2439f60ef33a0d46d85da5001d52aeda5b00ce9f\n‚îÇ ‚îú‚îÄ‚îÄ [ 52] README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812\n‚îÇ ‚îî‚îÄ‚îÄ [ 76] pytorch\\_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\n‚îî‚îÄ‚îÄ [ 128] bbc77c8132af1cc5cf678da3f1ddf2de43606d48\n‚îú‚îÄ‚îÄ [ 52] README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e\n‚îî‚îÄ‚îÄ [ 76] pytorch\\_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\n```\n### Limitations\nIn order to have an efficient cache-system, `huggingface-hub` uses symlinks."", ""Models Download Stats\n## How are download stats generated for models? Counting the number of downloads for models is not a trivial task as a single model repository might contain multiple files, including multiple model weight files (e.g., with sharded models), and different formats depending on the library. To avoid double counting downloads (e.g., counting a single download of a model as multiple downloads), the Hub uses a set of query files that are employed for download counting. No information is sent from the user, and no additional calls are made for this. The count is done server-side as we serve files for downloads. Every HTTP request to these files, including `GET` and `HEAD` will be counted as a download. By default, when no library is specified, the Hub uses `config.json` as the default query file. Otherwise, the query file depends on each library, and the Hub might examine files such as `pytorch\\_model.bin` and `adapter\\_config.json`. ## Which are the query files for different libraries? By default, the Hub looks at `config.json`, `config.yaml`, `hyperparams.yaml`, and `meta.yaml`. For the following set of libraries, there are specific query files\n```json\n{\n\""adapter-transformers\"": {\nfilter: [\n{\nterm: { path: \""adapter\\_config.json\"" },\n},\n],\n},\n\""asteroid\"": {\nfilter: [\n{\nterm: { path: \""pytorch\\_model.bin\"" },\n},\n],\n},\n\""flair\"": {\nfilter: [\n{\nterm: { path: \""pytorch\\_model.bin\"" },\n},\n],\n},\n\""keras\"": {\nfilter: [\n{\nterm: { path: \""saved\\_model.pb\"" },\n},\n],\n},\n\""ml-agents\"": {\nfilter: [\n{\nwildcard: { path: \""\\*.onnx\"" },\n},\n],\n},\n\""nemo\"": {\nfilter: [\n{\nwildcard: { path: \""\\*.nemo\"" },\n},\n],\n},\n\""open\\_clip\"": {\nfilter: [\n{\nwildcard: { path: \""\\*pytorch\\_model.bin\"" },\n},\n],\n},\n\""sample-factory\"": {\nfilter: [\n{\nterm: { path: \""cfg.json\"" },\n},\n],\n},\n\""paddlenlp\"": {\nfilter: [\n{\nterm: { path: \""model\\_config.json\"" },\n},\n],\n},\n\""speechbrain\"": {\nfilter: [\n{\nterm: { path: \""hyperparams.yaml\"" },\n},\n],\n},\n\""sklearn\"": {\nfilter: [\n{\nterm: { path: \""sklearn\\_model.joblib\"" },\n},\n],\n},\n\""spacy\"": {\nfilter: [\n{\nwildcard: { path: \""\\*.whl\"" },\n},\n],\n},\n\""stanza\"": {\nfilter: [\n{\nterm: { path: \""models/default.zip\"" },\n},\n],\n},\n\""stable-baselines3\"": {\nfilter: [\n{\nwildcard: { path: \""\\*.zip\"" },\n},\n],\n},\n\""timm\"": {\nfilter: [\n{\nterms: { path: [\""pytorch\\_model.bin\"", \""model.safetensors\""] },\n},\n],\n},\n\""diffusers\"": {\n/// Filter out nested safetensors and pickle weights to avoid double counting downloads from the diffusers lib\nmust\\_not: [\n{\nwildcard: { path: \""\\*/\\*.safetensors\"" },\n},\n{\nwildcard: { path: \""\\*/\\*.bin\"" },\n},\n],\n/// Include documents that match at least one of the following rules\nshould: [\n/// Downloaded from diffusers lib\n{\nterm: { path: \""model\\_index.json\"" },\n},\n/// Direct downloads (LoRa, Auto1111 and others)\n{\nwildcard: { path: \""\\*.safetensors\"" },\n},\n{\nwildcard: { path: \""\\*.ckpt\"" },\n},\n{\nwildcard: { path: \""\\*.bin\"" },\n},\n],\nminimum\\_should\\_match: 1,\n},\n\""peft\"": {\nfilter: [\n{\nterm: { path: \""adapter\\_config.json\"" },\n},\n],\n}\n}\n```""]","The filename used when downloading the model from the Hugging Face Hub is typically determined by the specific query files associated with the library being used. By default, the Hub looks at `config.json`, `config.yaml`, `hyperparams.yaml`, and `meta.yaml`. For different libraries, there are specific query files that may include filenames like `pytorch_model.bin`, `adapter_config.json`, `saved_model.pb`, and others, depending on the library in question.",q-learning.pkl
What method is used by ViTHybridImageProcessor?,"[""It does not work on sequence data but on batches of images. Unlike ViT, there are no embeddings. The backbone model outputs a feature map. - One can use [`MobileViTImageProcessor`] to prepare images for the model. Note that if you do your own preprocessing, the pretrained checkpoints expect images to be in BGR pixel order (not RGB). - The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes). - The segmentation model uses a [DeepLabV3](https://arxiv.org/abs/1706.05587) head. The available semantic segmentation checkpoints are pre-trained on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/). ## MobileViTV2Config\n[[autodoc]] MobileViTV2Config\n## MobileViTV2Model\n[[autodoc]] MobileViTV2Model\n- forward\n## MobileViTV2ForImageClassification\n[[autodoc]] MobileViTV2ForImageClassification\n- forward\n## MobileViTV2ForSemanticSegmentation\n[[autodoc]] MobileViTV2ForSemanticSegmentation\n- forward"", ""We will use the same model checkpoint for this. Let's initialize the model and the processor. ```python\nfrom transformers import Swin2SRForImageSuperResolution, Swin2SRImageProcessor\nmodel = Swin2SRForImageSuperResolution.from\\_pretrained(\""caidas/swin2SR-lightweight-x2-64\"").to(device)\nprocessor = Swin2SRImageProcessor(\""caidas/swin2SR-lightweight-x2-64\"")\n```\n`pipeline` abstracts away the preprocessing and postprocessing steps that we have to do ourselves, so let's preprocess the image. We will pass the image to the processor and then move the pixel values to GPU. ```python\npixel\\_values = processor(image, return\\_tensors=\""pt\"").pixel\\_values\nprint(pixel\\_values.shape)\npixel\\_values = pixel\\_values.to(device)\n```\nWe can now infer the image by passing pixel values to the model. ```python\nimport torch\nwith torch.no\\_grad():\noutputs = model(pixel\\_values)\n```\nOutput is an object of type `ImageSuperResolutionOutput` that looks like below üëá\n```\n(loss=None, reconstruction=tensor([[[[0.8270, 0.8269, 0.8275, ..., 0.7463, 0.7446, 0.7453],\n[0.8287, 0.8278, 0.8283, ..., 0.7451, 0.7448, 0.7457],\n[0.8280, 0.8273, 0.8269, ..., 0.7447, 0.7446, 0.7452],\n...,\n[0.5923, 0.5933, 0.5924, ..., 0.0697, 0.0695, 0.0706],\n[0.5926, 0.5932, 0.5926, ..., 0.0673, 0.0687, 0.0705],\n[0.5927, 0.5914, 0.5922, ..., 0.0664, 0.0694, 0.0718]]]],\ndevice='cuda:0'), hidden\\_states=None, attentions=None)\n```\nWe need to get the `reconstruction` and post-process it for visualization."", ""So, it's a good idea to ship an ML model that already has these supports. It also helps in reducing training/serving skew. For our model we need:\n\\* Data normalization, resizing, and transposition as the preprocessing ops. \\* Mapping the predicted logits to ImageNet-1k classes as the post-processing ops. ```python\nprocessor = ViTImageProcessor()\nprocessor\n```\n```python\nCONCRETE\\_INPUT = \""pixel\\_values\""\nSIZE = processor.size[\""height\""]\nINPUT\\_SHAPE = (SIZE, SIZE, 3)\n```\n```python\ndef normalize\\_img(img, mean=processor.image\\_mean, std=processor.image\\_std):\n# Scale to the value range of [0, 1] first and then normalize. img = img / 255\nmean = tf.constant(mean)\nstd = tf.constant(std)\nreturn (img - mean) / std\ndef preprocess(string\\_input):\ndecoded = tf.io.decode\\_jpeg(string\\_input, channels=3)\nresized = tf.image.resize(decoded, size=(SIZE, SIZE))\nnormalized = normalize\\_img(resized)\nnormalized = tf.transpose(normalized, (2, 0, 1)) # Since HF models are channel-first. return normalized\n@tf.function(input\\_signature=[tf.TensorSpec([None], tf.string)])\ndef preprocess\\_fn(string\\_input):\ndecoded\\_images = tf.map\\_fn(\npreprocess, string\\_input, fn\\_output\\_signature=tf.float32,\n)\nreturn {CONCRETE\\_INPUT: decoded\\_images}\ndef model\\_exporter(model: tf.keras.Model):\nm\\_call = tf.function(model.call).get\\_concrete\\_function(\ntf.TensorSpec(\nshape=[None, 3, SIZE, SIZE], dtype=tf.float32, name=CONCRETE\\_INPUT\n)\n)\n@tf.function(input\\_signature=[tf.TensorSpec([None], tf.string)])\ndef serving\\_fn(string\\_input):\nlabels = tf.constant(\nlist(model.config.id2label.values()), dtype=tf.string\n)\nimages = preprocess\\_fn(string\\_input)\npredictions = m\\_call(\\*\\*images)\nindices = tf.argmax(predictions.logits, axis=1)\npred\\_source = tf.gather(params=labels, indices=indices)\nprobs = tf.nn.softmax(predictions.logits, axis=1)\npred\\_confidence = tf.reduce\\_max(probs, axis=1)\nreturn {\""label\"": pred\\_source, \""confidence\"": pred\\_confidence}\nreturn serving\\_fn\n```\n```python\n# To deploy the model on Vertex AI we must have the model in a storage bucket. tf.saved\\_model.save(\nmodel,\nos.path.join(GCS\\_BUCKET, LOCAL\\_MODEL\\_DIR),\nsignatures={\""serving\\_default\"": model\\_exporter(model)},\n)\n```\n\\*\\*Notes on making the model accept string inputs\\*\\*:\nWhen dealing with images via REST or gRPC requests the size of the request payload can easily spiral up depending on the resolution of the images being passed.""]","The `ViTHybridImageProcessor` uses the `__call__` method to preprocess images for the model. This method handles the necessary transformations, including normalization and resizing, to prepare the images for input into the ViT model.",preprocess
Who contributed the code component for syntax highlighting in Gradio?,"[""Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling. ##### Various performance improvements\nThese improvements will be particularly beneficial to large applications. - Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast. - Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%). - Corrected an issue that was causing markdown to re-render infinitely. - Ensured that the `gr.3DModel` does re-render prematurely. Thanks [@pngwn](https://github.com/pngwn)! ## 0.0.2\n### Fixes\n- [#4997](https://github.com/gradio-app/gradio/pull/4997) [`41c83070`](https://github.com/gradio-app/gradio/commit/41c83070b01632084e7d29123048a96c1e261407) - Add CSS resets and specifiers to play nice with HF blog. Thanks [@aliabid94](https://github.com/aliabid94)!"", ""Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling. ##### Various performance improvements\nThese improvements will be particularly beneficial to large applications. - Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast. - Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%). - Corrected an issue that was causing markdown to re-render infinitely. - Ensured that the `gr.3DModel` does re-render prematurely. Thanks [@pngwn](https://github.com/pngwn)! ### Features\n- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app/gradio/commit/fbdad78af4c47454cbb570f88cc14bf4479bbceb) - Lazy load interactive or static variants of a component individually, rather than loading both variants regardless."", ""Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling. ##### Various performance improvements\nThese improvements will be particularly beneficial to large applications. - Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast. - Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%). - Corrected an issue that was causing markdown to re-render infinitely. - Ensured that the `gr.3DModel` does re-render prematurely. Thanks [@pngwn](https://github.com/pngwn)! ## 0.0.2\n### Patch Changes\n- Updated dependencies [[`41c83070`](https://github.com/gradio-app/gradio/commit/41c83070b01632084e7d29123048a96c1e261407)]:\n- @gradio/theme@0.0.2""]",The code component for syntax highlighting in Gradio was contributed by [@pngwn](https://github.com/pngwn).,@pngwn
"Where can you find the Apache License, Version 2.0?","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Using Sample Factory for high throughput training\nUnder construction üöß."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction"", ""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Legacy examples\nThis folder contains examples which are not actively maintained (mostly contributed by the community). Using these examples together with a recent version of the library usually requires to make small (sometimes big) adaptations to get the scripts working.""]","You can find the Apache License, Version 2.0 at the following URL: http://www.apache.org/licenses/LICENSE-2.0.",http://www.apache.org/licenses/LICENSE-2.0
What tool can be used for sentiment analysis in NLP?,"[""### Finding suitable datasets\nHaving decided on the task, next, we should find the data the model will be trained on. This is usually more important for the performance of your use case than picking the right model architecture. Keep in mind that a model is \\*\\*only as good as the data it has been trained on\\*\\*. Thus, we should be very careful when curating and/or selecting the dataset. Since we consider the hypothetical use case of \\*filtering out unsatisfied messages\\*, let's look into what datasets are available. For your real-world use case, it is \\*\\*very likely\\*\\* that you have internal data that best represents the actual data your NLP system is supposed to handle. Therefore, you should use such internal data to train your NLP system. It can nevertheless be helpful to also include publicly available data to improve the generalizability of your model. Let's take a look at all available Datasets on the [Hugging Face Hub](https://huggingface.co/datasets). On the left side, you can filter the datasets according to \\*Task Categories\\* as well as \\*Tasks\\* which are more specific. Our use case corresponds to \\*Text Classification\\* -> \\*Sentiment Analysis\\* so let's select [these filters](https://huggingface.co/datasets?task\\_categories=task\\_categories:text-classification&task\\_ids=task\\_ids:sentiment-classification&sort=downloads). We are left with \\*ca.\\* 80 datasets at the time of writing this notebook. Two aspects should be evaluated when picking a dataset:\n- \\*\\*Quality\\*\\*: Is the dataset of high quality? More specifically: Does the data correspond to the data you expect to deal with in your use case? Is the data diverse, unbiased, ...? - \\*\\*Size\\*\\*: How big is the dataset? Usually, one can safely say the bigger the dataset, the better. It's quite tricky to evaluate whether a dataset is of high quality efficiently, and it's even more challenging to know whether and how the dataset is biased. An efficient and reasonable heuristic for high quality is to look at the download statistics."", ""üí•\nAre you interested in doing more? As a next step, you could use a second [text classifier](https://huggingface.co/tasks/text-classification) to classify each tweet by their theme or topic. This way, each tweet will be labeled with both sentiment and topic, and you can get more granular insights (e.g. are users praising how easy to use is Notion but are complaining about their pricing or customer support?). ## How to do Twitter sentiment analysis without coding? To get started with sentiment analysis, you don't need to be a developer or know how to code. ü§Ø\nThere are some amazing no-code solutions that will enable you to easily do sentiment analysis in just a few minutes. In this section, you will use [Zapier](https://zapier.com/), a no-code tool that enables users to connect 5,000+ apps with an easy to use user interface. You will create a [Zap](https://zapier.com/help/create/basics/create-zaps), that is triggered whenever someone mentions Notion on Twitter. Then the Zap will use the [Inference API](https://huggingface.co/inference-api) to analyze the tweet with a sentiment analysis model and finally it will save the results on Google Sheets:\n1."", ""Remove any unwanted object, defect, people from your pictures or erase and replace anything on your pictures. Keywords: inpainting, SD, Stable Diffusion\n## [flair](https://github.com/flairNLP/flair)\nFLAIR is a powerful PyTorch NLP framework, convering several important tasks: NER, sentiment-analysis, part-of-speech tagging, text and document embeddings, among other things. Keywords: NLP, text embedding, document embedding, biomedical, NER, PoS, sentiment-analysis\n## [mindsdb](https://github.com/mindsdb/mindsdb)\nMindsDB is a low-code ML platform, which automates and integrates several ML frameworks into the data stack as \""AI Tables\"" to streamline the integration of AI into applications, making it accessible to developers of all skill levels. Keywords: Database, low-code, AI table\n## [langchain](https://github.com/hwchase17/langchain)\n[langchain](https://github.com/hwchase17/langchain) is aimed at assisting in the development of apps merging both LLMs and other sources of knowledge. The library allows chaining calls to applications, creating a sequence across many tools. Keywords: LLMs, Large Language Models, Agents, Chains\n## [LlamaIndex](https://github.com/jerryjliu/llama\\_index)\n[LlamaIndex](https://github.com/jerryjliu/llama\\_index) is a project that provides a central interface to connect your LLM's with external data. It provides various kinds of indices and retreival mechanisms to perform different LLM tasks and obtain knowledge-augmented results. Keywords: LLMs, Large Language Models, Data Retrieval, Indices, Knowledge Augmentation\n## [ParlAI](https://github.com/facebookresearch/ParlAI)\n[ParlAI](https://github.com/facebookresearch/ParlAI) is a python framework for sharing, training and testing dialogue models, from open-domain chitchat, to task-oriented dialogue, to visual question answering. It provides more than 100 datasets under the same API, a large zoo of pretrained models, a set of agents, and has several integrations. Keywords: Dialogue, Chatbots, VQA, Datasets, Agents\n## [sentence-transformers](https://github.com/UKPLab/sentence-transformers)\nThis framework provides an easy method to compute dense vector representations for sentences, paragraphs, and images.""]","For sentiment analysis in NLP, you can use the Inference API provided by Hugging Face. This API allows you to analyze text using sentiment analysis models without needing to code. Additionally, you can explore various datasets available on the Hugging Face Hub that are specifically tailored for sentiment analysis tasks.",transformers pipeline
Who are the authors of the paper associated with Graphormer?,"[""!--Copyright 2022 The HuggingFace Team and Microsoft. All rights reserved. Licensed under the MIT License; you may not use this file except in compliance with\nthe License. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Graphormer\n## Overview\nThe Graphormer model was proposed in [Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234) by\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen and Tie-Yan Liu. It is a Graph Transformer model, modified to allow computations on graphs instead of text sequences by generating embeddings and features of interest during preprocessing and collation, then using a modified attention. The abstract from the paper is the following:\n\\*The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.\\*\nThis model was contributed by [clefourrier](https://huggingface.co/clefourrier). The original code can be found [here](https://github.com/microsoft/Graphormer). ## Usage tips\nThis model will not work well on large graphs (more than 100 nodes/edges), as it will make the memory explode. You can reduce the batch size, increase your RAM, or decrease the `UNREACHABLE\\_NODE\\_DISTANCE` parameter in algos\\_graphormer.pyx, but it will be hard to go above 700 nodes/edges. This model does not use a tokenizer, but instead a special collator during training."", ""1. \\*\\*[GPTBigCode](https://huggingface.co/docs/transformers/model\\_doc/gpt\\_bigcode)\\*\\* (BigCode ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc√≠a del R√≠o, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.Ïùò [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988)ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§. 1. \\*\\*[GPTSAN-japanese](https://huggingface.co/docs/transformers/model\\_doc/gptsan-japanese)\\*\\* released in the repository [tanreinama/GPTSAN](https://github.com/tanreinama/GPTSAN/blob/main/report/model.md) by Toshiyuki Sakamoto(tanreinama). 1. \\*\\*[Graphormer](https://huggingface.co/docs/transformers/model\\_doc/graphormer)\\*\\* (from Microsoft) Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu Ïùò [Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§. 1. \\*\\*[GroupViT](https://huggingface.co/docs/transformers/model\\_doc/groupvit)\\*\\* (UCSD, NVIDIA ÏóêÏÑú) Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang Ïùò [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§. 1. \\*\\*[HerBERT](https://huggingface.co/docs/transformers/model\\_doc/herbert)\\*\\* (Allegro.pl, AGH University of Science and Technology ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.Ïùò [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf)ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§. 1. \\*\\*[Hubert](https://huggingface.co/docs/transformers/model\\_doc/hubert)\\*\\* (Facebook ÏóêÏÑú) Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed Ïùò [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§. 1. \\*\\*[I-BERT](https://huggingface.co/docs/transformers/model\\_doc/ibert)\\*\\* (Berkeley ÏóêÏÑú) Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer Ïùò [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§."", ""\\*\\*[BigBird-Pegasus](https://huggingface.co/docs/transformers/model\\_doc/bigbird\\_pegasus)\\*\\* (‡§ó‡•Ç‡§ó‡§≤ ‡§∞‡§ø‡§∏‡§∞‡•ç‡§ö ‡§∏‡•á) ‡§∏‡§æ‡§• ‡§µ‡§æ‡§≤‡§æ ‡§™‡•á‡§™‡§∞ [‡§¨‡§ø‡§ó ‡§¨‡§∞‡•ç‡§°: ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡•â‡§∞‡•ç‡§Æ‡§∞‡•ç‡§∏ ‡§´‡•â‡§∞ ‡§≤‡•â‡§®‡•ç‡§ó‡§∞ ‡§∏‡•Ä‡§ï‡•ç‡§µ‡•á‡§Ç‡§∏](https://arxiv .org/abs/2007.14062) ‡§Æ‡§Ç‡§ú‡§º‡§ø‡§≤ ‡§ú‡§º‡§π‡•Ä‡§∞, ‡§ó‡•Å‡§∞‡•Å ‡§ó‡•Å‡§∞‡•Å‡§ó‡§£‡•á‡§∂, ‡§Ö‡§µ‡§ø‡§®‡§æ‡§µ‡§æ ‡§¶‡•Å‡§¨‡•á, ‡§ú‡•ã‡§∂‡•Å‡§Ü ‡§Ü‡§á‡§Ç‡§∏‡•ç‡§≤‡•Ä, ‡§ï‡•ç‡§∞‡§ø‡§∏ ‡§Ö‡§≤‡•ç‡§¨‡§∞‡•ç‡§ü‡•Ä, ‡§∏‡•à‡§Ç‡§ü‡§ø‡§Ø‡§æ‡§ó‡•ã ‡§ì‡§Ç‡§ü‡§æ‡§®‡•ã‡§®, ‡§´‡§ø‡§≤‡§ø‡§™ ‡§´‡§æ‡§Æ, ‡§Ö‡§®‡§ø‡§∞‡•Å‡§¶‡•ç‡§ß ‡§∞‡§æ‡§µ‡•Å‡§≤‡§æ, ‡§ï‡§ø‡§´‡§º‡§æ‡§® ‡§µ‡§æ‡§Ç‡§ó, ‡§≤‡•Ä ‡§Ø‡§æ‡§Ç‡§ó, ‡§Ö‡§Æ‡§∞ ‡§Ö‡§π‡§Æ‡§¶ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡•§\n1. \\*\\*[BigBird-RoBERTa](https://huggingface.co/docs/transformers/model\\_doc/big\\_bird)\\*\\* (‡§ó‡•Ç‡§ó‡§≤ ‡§∞‡§ø‡§∏‡§∞‡•ç‡§ö ‡§∏‡•á) ‡§∏‡§æ‡§• ‡§Æ‡•á‡§Ç ‡§™‡•á‡§™‡§∞ [‡§¨‡§ø‡§ó ‡§¨‡§∞‡•ç‡§°: ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡•â‡§∞‡•ç‡§Æ‡§∞‡•ç‡§∏ ‡§´‡•â‡§∞ ‡§≤‡•â‡§®‡•ç‡§ó‡§∞ ‡§∏‡•Ä‡§ï‡•ç‡§µ‡•á‡§Ç‡§∏](https://arxiv.org/abs/2007.14062) ‡§Æ‡§Ç‡§ú‡§º‡§ø‡§≤ ‡§ú‡§º‡§π‡•Ä‡§∞, ‡§ó‡•Å‡§∞‡•Å ‡§ó‡•Å‡§∞‡•Å‡§ó‡§£‡•á‡§∂, ‡§Ö‡§µ‡§ø‡§®‡§æ‡§µ‡§æ ‡§¶‡•Å‡§¨‡•á, ‡§ú‡•ã‡§∂‡•Å‡§Ü ‡§Ü‡§á‡§Ç‡§∏‡•ç‡§≤‡•Ä, ‡§ï‡•ç‡§∞‡§ø‡§∏ ‡§Ö‡§≤‡•ç‡§¨‡§∞‡•ç‡§ü‡•Ä, ‡§∏‡•à‡§Ç‡§ü‡§ø‡§Ø‡§æ‡§ó‡•ã ‡§ì‡§Ç‡§ü‡§æ‡§®‡§®, ‡§´‡§ø‡§≤‡§ø‡§™ ‡§´‡§æ‡§Æ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ , ‡§Ö‡§®‡§ø‡§∞‡•Å‡§¶‡•ç‡§ß ‡§∞‡§æ‡§µ‡•Å‡§≤‡§æ, ‡§ï‡§ø‡§´‡§º‡§æ‡§® ‡§µ‡§æ‡§Ç‡§ó, ‡§≤‡•Ä ‡§Ø‡§æ‡§Ç‡§ó, ‡§Ö‡§Æ‡§∞ ‡§Ö‡§π‡§Æ‡§¶ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§™‡•ã‡§∏‡•ç‡§ü ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ‡•§\n1. \\*\\*[BioGpt](https://huggingface.co/docs/transformers/model\\_doc/biogpt)\\*\\* (from Microsoft Research AI4Science) released with the paper [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu.""]","The authors of the paper associated with Graphormer are Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu.","Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu."
What function does Accelerate provide to determine a device map?,"[""upercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on which you can run your training: it could be on CPU, GPUs, TPUs. Distributed on one machine with several devices, or several machines (often called nodes) each with multiple devices. On top of that there are new tweaks to make your training faster or more memory efficient, like mixed precision and DeepSpeed. Each of those setups or training tweaks, requires you to change the code of your training loop in one way or another and to learn a new API. All those setups are handled by the Trainer API, and there are several third-party libraries that can also help you with that. The problem with those is that they can feel like a black box and that it might not be easy to implement the tweak to the training loop you need. Accelerate has been designed specifically to let you retain full control over your training loop and be as non-intrusive as possible. With just four lines to add to your training loop (here shown on the code of the training loop from the \""Raw training loop\"" video), Accelerate will handle all the setups and training tweaks mentioned on the first slide."", ""Now let's talk about Accelerate, a library aimed to make this process more seameless and also help with a few best practices\n## ü§ó Accelerate\n[Accelerate](https://huggingface.co/docs/accelerate) is a library designed to allow you to perform what we just did above, without needing to modify your code greatly. On top of this, the data pipeline innate to Accelerate can also improve performance to your code as well. First, let's wrap all of the above code we just performed into a single function, to help us visualize the difference:\n```python\ndef train\\_ddp(rank, world\\_size):\nsetup(rank, world\\_size)\n# Build DataLoaders\ntransform = transforms.Compose([\ntransforms.ToTensor(),\ntransforms.Normalize((0.1307), (0.3081))\n])\ntrain\\_dset = datasets.MNIST('data', train=True, download=True, transform=transform)\ntest\\_dset = datasets.MNIST('data', train=False, transform=transform)\ntrain\\_loader = torch.utils.data.DataLoader(train\\_dset, shuffle=True, batch\\_size=64)\ntest\\_loader = torch.utils.data.DataLoader(test\\_dset, shuffle=False, batch\\_size=64)\n# Build model\nmodel = model.to(rank)\nddp\\_model = DDP(model, device\\_ids=[rank])\n# Build optimizer\noptimizer = optim.AdamW(ddp\\_model.parameters(), lr=1e-3)\n# Train for a single epoch\nmodel.train()\nfor batch\\_idx, (data, target) in enumerate(train\\_loader):\ndata, target = data.to(device), target.to(device)\noutput = model(data)\nloss = F.nll\\_loss(output, target)\nloss.backward()\noptimizer.step()\noptimizer.zero\\_grad()\n# Evaluate\nmodel.eval()\ncorrect = 0\nwith torch.no\\_grad():\nfor data, target in test\\_loader:\ndata, target = data.to(device), target.to(device)\noutput = model(data)\npred = output.argmax(dim=1, keepdim=True)\ncorrect += pred.eq(target.view\\_as(pred)).sum().item()\nprint(f'Accuracy: {100."", ""The attribute `has\\_fp16\\_weights` has to be set to `False` in order to directly load the weights in `int8` together with the quantization statistics. We also discard the replacement for some modules (here the `lm\\_head`) since we want to keep the latest in their native precision for more precise and stable results. But it isn't over yet! The function above is executed under the `init\\_empty\\_weights` context manager which means that the new model will be still in the `meta` device. For models that are initialized under this context manager, `accelerate` will manually load the parameters of each module and move them to the correct devices. In `bitsandbytes`, setting a `Linear8bitLt` module's device is a crucial step (if you are curious, you can check the code snippet [here](https://github.com/TimDettmers/bitsandbytes/blob/bd515328d70f344f935075f359c5aefc616878d5/bitsandbytes/nn/modules.py#L94)) as we have seen in our toy script. Here the quantization step fails when calling it twice. We had to come up with an implementation of `accelerate`'s `set\\_module\\_tensor\\_to\\_device` function (termed as `set\\_module\\_8bit\\_tensor\\_to\\_device`) to make sure we don't call it twice. Let's discuss this in detail in the section below! ### Be very careful on how to set devices with `accelerate`\nHere we played a very delicate balancing act with the `accelerate` library! Once you load your model and set it on the correct devices, sometimes you still need to call `set\\_module\\_tensor\\_to\\_device` to dispatch the model with hooks on all devices.""]",Accelerate provides the function `infer_auto_device_map` to determine a device map. This function helps in automatically assigning model layers to the appropriate devices based on the available hardware resources.,infer_auto_device_map
Why is the __call__ method decorated with torch.no_grad?,"[""The behavior is fully analogous to [PyTorch's `to` method](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to). - [`\\_\\_call\\_\\_`] method to use the pipeline in inference. `\\_\\_call\\_\\_` defines inference logic of the pipeline and should ideally encompass all aspects of it, from pre-processing to forwarding tensors to the different models and schedulers, as well as post-processing. The API of the `\\_\\_call\\_\\_` method can strongly vary from pipeline to pipeline. \\*E.g.\\* a text-to-image pipeline, such as [`StableDiffusionPipeline`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\_diffusion/pipeline\\_stable\\_diffusion.py) should accept among other things the text prompt to generate the image. A pure image generation pipeline, such as [DDPMPipeline](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/ddpm) on the other hand can be run without providing any inputs. To better understand what inputs can be adapted for\neach pipeline, one should look directly into the respective pipeline. \\*\\*Note\\*\\*: All pipelines have PyTorch's autograd disabled by decorating the `\\_\\_call\\_\\_` method with a [`torch.no\\_grad`](https://pytorch.org/docs/stable/generated/torch.no\\_grad.html) decorator because pipelines should\nnot be used for training. If you want to store the gradients during the forward pass, we recommend writing your own pipeline, see also our [community-examples](https://github.com/huggingface/diffusers/tree/main/examples/community)\n## Contribution\nWe are more than happy about any contribution to the officially supported pipelines ü§ó."", ""This is analogous to the `with\\_indices` parameter. The `with\\_rank` parameter in the mapped function goes after the `index` one if it is already present. ```py\n>>> from multiprocess import set\\_start\\_method\n>>> import torch\n>>> import os\n>>>\n>>> for i in range(torch.cuda.device\\_count()): # send model to every GPU\n... model.to(f\""cuda:{i}\"")\n>>>\n>>> def gpu\\_computation(example, rank):\n... torch.cuda.set\\_device(f\""cuda:{rank}\"") # use one GPU\n... # Your big GPU call goes here, for example\n... inputs = tokenizer(texts, truncation=True, return\\_tensors=\""pt\"").to(f\""cuda:{rank}\"")\n... outputs = model.generate(\\*\\*inputs)\n... example[\""generated\\_text\""] = tokenizer.batch\\_decode(translated\\_tokens, skip\\_special\\_tokens=True)\n... return example\n>>>\n>>> if \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\n... set\\_start\\_method(\""spawn\"")\n... updated\\_dataset = dataset.map(gpu\\_computation, with\\_rank=True, num\\_proc=torch.cuda.device\\_count())\n```\nThe main use-case for rank is to parallelize computation across several GPUs."", ""### Inference from a training checkpoint\nYou can also perform inference from one of the complete checkpoint saved during the training process, if you used the `--checkpointing\\_steps` argument. TODO. ## Set grads to none\nTo save even more memory, pass the `--set\\_grads\\_to\\_none` argument to the script. This will set grads to None instead of zero. However, be aware that it changes certain behaviors, so if you start experiencing any problems, remove this argument. More info: https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero\\_grad.html\n## Experimental results\nYou can refer to [our webpage](https://www.cs.cmu.edu/~custom-diffusion/) that discusses our experiments in detail. We also released a more extensive dataset of 101 concepts for evaluating model customization methods. For more details please refer to our [dataset webpage](https://www.cs.cmu.edu/~custom-diffusion/dataset.html).""]","The `__call__` method is decorated with `torch.no_grad` because pipelines should not be used for training. This decorator disables PyTorch's autograd, which means that gradients will not be computed during the forward pass. This is important for inference, as it saves memory and improves performance by avoiding unnecessary gradient calculations. If you want to store the gradients during the forward pass, it is recommended to write your own pipeline.",Because pipelines should not be used for training.
What is the latency in milliseconds with fp16 optimization?,"[""The impact of `fp16` on latency is less marked with `batch\\_size = 1`, but here it is of enormous interest as it can reduce latency by almost half, and almost double throughput! # Concluding remarks\nThis blog post showcased a few simple optimization tricks bundled in the ü§ó ecosystem. Using anyone of these techniques, or a combination of all three, can greatly improve Bark inference speed and memory footprint. \\* You can use the large version of Bark without any performance degradation and a footprint of just 2GB instead of 5GB, 15% faster, \\*\\*using ü§ó Better Transformer and CPU offload\\*\\*. \\* Do you prefer high throughput? \\*\\*Batch by 8 with ü§ó Better Transformer and half-precision\\*\\*. \\* You can get the best of both worlds by using \\*\\*fp16, ü§ó Better Transformer and CPU offload\\*\\*!"", ""Also, it is important to note that tuning OpenMP library will only work within software that uses the OpenMP API internally. More specially, now only PyTorch and TorchScript really make usage of OpenMP and thus benefit from OpenMP backend tuning. This also explains why we reported latencies only for these two frameworks. ## Automatic Performances Tuning: Bayesian Optimization with Intel SigOpt\nAs mentioned above, many knobs can be tweaked to improve latency and throughput on Intel CPUs, but because there are many, tuning all of them to get optimal performance can be cumbersome. For instance, in our experiments, the following knobs were tuned:\n- The number of cores: although using as many cores as you have is often a good idea, it does not always provide the best performance because it also means more communication between the different threads. On top of that, having better performance with fewer cores can be very useful as it allows to run multiple instances at the same time, resulting in both better latency and throughput. - The memory allocator: which memory allocator out of the default malloc, Google's tcmalloc and Facebook's jemalloc provides the best performance? - The parallelism library: which parallelism library out of GNU OpenMP and Intel OpenMP provides the best performance? - Transparent Huge Pages: does enabling Transparent Huge Pages (THP) on the system provide better performance? - KMP block time parameter: sets the time, in milliseconds, that a thread should wait, after completing the execution of a parallel region, before sleeping. Of course, the brute force approach, consisting of trying out all the possibilities will provide the best knob values to use to get optimal performance but,\nthe size of the search space being `N x 3 x 2 x 2 x 2 = 24N`, it can take a lot of time: on a machine with 80 physical cores, this means trying out at most `24 x 80 = 1920` different setups! üò±\nFortunately, Intel's [SigOpt](https://sigopt.com/), through Bayesian optimization, allows us to make these tuning experiments both faster and more convenient to analyse, while providing similar performance than the brute force approach. When we analyse the relative difference between the absolute best latency and what SigOpt provides, we observe that although it is often not as good as brute force (except for sequence length = 512 in that specific case),\nit gives very close performance, with \\*\\*8.6%\\*\\* being the biggest gap on this figure. |  |  |\n| --- | --- |\n| Figure 23. Absolute best latency found by SigOpt automatic tuning vs brute force | Figure 24. Relative best latency found by SigOpt automatic tuning vs brute force |\n\nSigOpt is also very useful for analysis: it provides a lot of figures and valuable information."", ""First Token Latency (also known as Time to First Token or prefill latency) measures how long you wait from the time you enter your prompt to the time you begin receiving your output, so this metric can tell you how responsive the model will feel. Optimum-NVIDIA delivers up to 3.3x faster First Token Latency compared to stock transformers:\n\n![](assets/optimum_nvidia/first_token_latency.svg)\n\nFigure 1. Time it takes to generate the first token (ms)\n\nThroughput, on the other hand, measures how fast the model can generate tokens and is particularly relevant when you want to batch generations together. While there are a few ways to calculate throughput, we adopted a standard method to divide the end-to-end latency by the total sequence length, including both input and output tokens summed over all batches. Optimum-NVIDIA delivers up to 28x better throughput compared to stock transformers:\n\n![](assets/optimum_nvidia/throughput.svg)\n\nFigure 2. Throughput (token / second)\n\nInitial evaluations of the [recently announced NVIDIA H200 Tensor Core GPU](https://www.nvidia.com/en-us/data-center/h200/) show up to an additional 2x boost in throughput for LLaMA models compared to an NVIDIA H100 Tensor Core GPU. As H200 GPUs become more readily available, we will share performance data for Optimum-NVIDIA running on them. ### Next steps\nOptimum-NVIDIA currently provides peak performance for the LLaMAForCausalLM architecture + task, so any [LLaMA-based model](https://huggingface.co/models?other=llama,llama2), including fine-tuned versions, should work with Optimum-NVIDIA out of the box today. We are actively expanding support to include other text generation model architectures and tasks, all from within Hugging Face. We continue to push the boundaries of performance and plan to incorporate cutting-edge optimization techniques like In-Flight Batching to improve throughput when streaming prompts and INT4 quantization to run even bigger models on a single GPU. Give it a try: we are releasing the [Optimum-NVIDIA repository](https://github.com/huggingface/optimum-nvidia) with instructions on how to get started.""]","The provided context does not specify the exact latency in milliseconds with fp16 optimization. It mentions that using fp16 can significantly reduce latency, but specific numerical values are not given. For detailed performance metrics, you may want to refer to the relevant benchmarks or documentation on HuggingFace's resources.",10.32
How can you share a Gradio demo publicly?,"[""Gradio, check![[gradio-check]]\nThis wraps up the chapter on building cool ML demos with Gradio - we hope you enjoyed it! To recap, in this chapter we learned:\n- How to create Gradio demos with the high-level `Interface` API, and how to configure different input and output modalities. - Different ways to share Gradio demos, through temporary links and hosting on [Hugging Face Spaces](https://huggingface.co/spaces). - How to integrate Gradio demos with models and Spaces on the Hugging Face Hub. - Advanced features like storing state in a demo or providing authentication. - How to have full control of the data flow and layout of your demo with Gradio Blocks. If you'd like to test your understanding of the concepts covered in this chapter, check out the quiz in the next section! ## Where to next?[[where-to-next]]\nIf you want to learn more about Gradio you can\n- Take a look at [Demos](https://github.com/gradio-app/gradio/tree/main/demo) in the repo, there are quite a lot of examples there. - See the [Guides](https://gradio.app/guides/) page, where you can find guides about cool and advanced features."", ""Sharing demos with others[[sharing-demos-with-others]]\nNow that you've built a demo, you'll probably want to share it with others. Gradio demos\ncan be shared in two ways: using a \\*\\*\\*temporary share link\\*\\*\\* or \\*\\*\\*permanent hosting on Spaces\\*\\*\\*. We'll cover both of these approaches shortly. But before you share your demo, you may want to polish it up üíÖ. ### Polishing your Gradio demo:[[polishing-your-gradio-demo]]\n\n![Overview of a gradio interface](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter9/gradio-demo-overview.png)\n![Overview of a gradio interface](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter9/gradio-demo-overview-dark.png)\n\nTo add additional content to your demo, the `Interface` class supports some optional parameters:\n- `title`: you can give a title to your demo, which appears \\_above\\_ the input and output components. - `description`: you can give a description (in text, Markdown, or HTML) for the interface, which appears above the input and output components and below the title."", ""ÂàÜ‰∫´ÊÇ®ÁöÑÂ∫îÁî®\nÂ¶Ç‰ΩïÂàÜ‰∫´ÊÇ®ÁöÑ Gradio Â∫îÁî®Ôºö\n1. [‰ΩøÁî® share ÂèÇÊï∞ÂàÜ‰∫´ÊºîÁ§∫](#sharing-demos)\n2. [Âú® HF Spaces ‰∏äÊâòÁÆ°](#hosting-on-hf-spaces)\n3. [ÂµåÂÖ•ÊâòÁÆ°ÁöÑÁ©∫Èó¥](#embedding-hosted-spaces)\n4. [‰ΩøÁî® Web ÁªÑ‰ª∂ÂµåÂÖ•](#embedding-with-web-components)\n5. [‰ΩøÁî® API È°µÈù¢](#api-page)\n6. [Âú®È°µÈù¢‰∏äÊ∑ªÂä†Ë∫´‰ªΩÈ™åËØÅ](#authentication)\n7. [ËÆøÈóÆÁΩëÁªúËØ∑Ê±Ç](#accessing-the-network-request-directly)\n8. [Âú® FastAPI ‰∏≠ÊåÇËΩΩ](#mounting-within-another-fastapi-app)\n9. [ÂÆâÂÖ®ÊÄß](#security-and-file-access)\n## ÂàÜ‰∫´ÊºîÁ§∫\nÈÄöËøáÂú® `launch()` ÊñπÊ≥ï‰∏≠ËÆæÁΩÆ `share=True`ÔºåÂèØ‰ª•ËΩªÊùæÂÖ¨ÂºÄÂàÜ‰∫´ Gradio ÊºîÁ§∫„ÄÇÂ∞±ÂÉèËøôÊ†∑Ôºö\n```python\ndemo.launch(share=True)\n```\nËøôÂ∞ÜÁîüÊàê‰∏Ä‰∏™ÂÖ¨ÂºÄÁöÑÂèØÂàÜ‰∫´ÈìæÊé•ÔºåÊÇ®ÂèØ‰ª•Â∞ÜÂÖ∂ÂèëÈÄÅÁªô‰ªª‰Ωï‰∫∫ÔºÅÂΩìÊÇ®ÂèëÈÄÅÊ≠§ÈìæÊé•Êó∂ÔºåÂØπÊñπÁî®Êà∑ÂèØ‰ª•Âú®ÂÖ∂ÊµèËßàÂô®‰∏≠Â∞ùËØïÊ®°Âûã„ÄÇÂõ†‰∏∫Â§ÑÁêÜËøáÁ®ãÂèëÁîüÂú®ÊÇ®ÁöÑËÆæÂ§á‰∏äÔºàÂè™Ë¶ÅÊÇ®ÁöÑËÆæÂ§á‰øùÊåÅÂºÄÂêØÔºÅÔºâÔºåÊÇ®‰∏çÂøÖÊãÖÂøÉ‰ªª‰ΩïÊâìÂåÖ‰æùËµñÈ°πÁöÑÈóÆÈ¢ò„ÄÇ‰∏Ä‰∏™ÂàÜ‰∫´ÈìæÊé•ÈÄöÂ∏∏ÁúãËµ∑Êù•ÂÉèËøôÊ†∑Ôºö\\*\\*XXXXX.gradio.app\\*\\*„ÄÇÂ∞ΩÁÆ°ÈìæÊé•ÊòØÈÄöËøá Gradio URL Êèê‰æõÁöÑÔºå‰ΩÜÊàë‰ª¨Âè™ÊòØÊÇ®Êú¨Âú∞ÊúçÂä°Âô®ÁöÑ‰ª£ÁêÜÔºåÂπ∂‰∏ç‰ºöÂ≠òÂÇ®ÈÄöËøáÊÇ®ÁöÑÂ∫îÁî®ÂèëÈÄÅÁöÑ‰ªª‰ΩïÊï∞ÊçÆ„ÄÇ\n‰ΩÜËØ∑ËÆ∞‰ΩèÔºåËøô‰∫õÈìæÊé•ÂèØ‰ª•Ë¢´ÂÖ¨ÂºÄËÆøÈóÆÔºåËøôÊÑèÂë≥ÁùÄ‰ªª‰Ωï‰∫∫ÈÉΩÂèØ‰ª•‰ΩøÁî®ÊÇ®ÁöÑÊ®°ÂûãËøõË°åÈ¢ÑÊµãÔºÅÂõ†Ê≠§ÔºåËØ∑Á°Æ‰øù‰∏çË¶ÅÈÄöËøáÊÇ®ÁºñÂÜôÁöÑÂáΩÊï∞ÂÖ¨ÂºÄ‰ªª‰ΩïÊïèÊÑü‰ø°ÊÅØÔºå‰πü‰∏çË¶ÅÂÖÅËÆ∏Âú®ÊÇ®ÁöÑËÆæÂ§á‰∏äËøõË°å‰ªª‰ΩïÂÖ≥ÈîÆÊõ¥Êîπ„ÄÇÂ¶ÇÊûúÊÇ®ËÆæÁΩÆ `share=False`ÔºàÈªòËÆ§ÂÄºÔºåÂú® colab Á¨îËÆ∞Êú¨‰∏≠Èô§Â§ñÔºâÔºåÂàôÂè™ÂàõÂª∫‰∏Ä‰∏™Êú¨Âú∞ÈìæÊé•ÔºåÂèØ‰ª•ÈÄöËøá[Á´ØÂè£ËΩ¨Âèë](https://www.ssh.com/ssh/tunneling/example)‰∏éÁâπÂÆöÁî®Êà∑ÂÖ±‰∫´„ÄÇ\n![](/assets/guides/sharing.svg)\nÂàÜ‰∫´ÈìæÊé•Âú® 72 Â∞èÊó∂ÂêéËøáÊúü„ÄÇ\n## Âú® HF Spaces ‰∏äÊâòÁÆ°\nÂ¶ÇÊûúÊÇ®ÊÉ≥Âú®‰∫íËÅîÁΩë‰∏äËé∑ÂæóÊÇ®ÁöÑ Gradio ÊºîÁ§∫ÁöÑÊ∞∏‰πÖÈìæÊé•ÔºåËØ∑‰ΩøÁî® Hugging Face Spaces„ÄÇ [Hugging Face Spaces](http://huggingface.co/spaces/) Êèê‰æõ‰∫ÜÂÖçË¥πÊâòÁÆ°ÊÇ®ÁöÑÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÁöÑÂü∫Á°ÄËÆæÊñΩÔºÅ\nÂú®ÊÇ®ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂÖçË¥πÁöÑ Hugging Face Ë¥¶Êà∑ÂêéÔºåÊúâ‰∏âÁßçÊñπÊ≥ïÂèØ‰ª•Â∞ÜÊÇ®ÁöÑ Gradio Â∫îÁî®ÈÉ®ÁΩ≤Âà∞ Hugging Face SpacesÔºö\n1.""]","Ë¶ÅÂÖ¨ÂºÄÂàÜ‰∫´ Gradio ÊºîÁ§∫ÔºåÊÇ®ÂèØ‰ª•ÈÄöËøáÂú® `launch()` ÊñπÊ≥ï‰∏≠ËÆæÁΩÆ `share=True` Êù•ËΩªÊùæÂÆûÁé∞„ÄÇÁ§∫‰æãÂ¶Ç‰∏ãÔºö

```python
demo.launch(share=True)
```

ËøôÂ∞ÜÁîüÊàê‰∏Ä‰∏™ÂÖ¨ÂºÄÁöÑÂèØÂàÜ‰∫´ÈìæÊé•ÔºåÊÇ®ÂèØ‰ª•Â∞ÜÂÖ∂ÂèëÈÄÅÁªô‰ªª‰Ωï‰∫∫„ÄÇËØ∑Ê≥®ÊÑèÔºåËøô‰∫õÈìæÊé•ÂèØ‰ª•Ë¢´ÂÖ¨ÂºÄËÆøÈóÆÔºåÊÑèÂë≥ÁùÄ‰ªª‰Ωï‰∫∫ÈÉΩÂèØ‰ª•‰ΩøÁî®ÊÇ®ÁöÑÊ®°ÂûãËøõË°åÈ¢ÑÊµã„ÄÇÂõ†Ê≠§ÔºåËØ∑Á°Æ‰øù‰∏çË¶ÅÈÄöËøáÊÇ®ÁºñÂÜôÁöÑÂáΩÊï∞ÂÖ¨ÂºÄ‰ªª‰ΩïÊïèÊÑü‰ø°ÊÅØ„ÄÇÂàÜ‰∫´ÈìæÊé•Âú® 72 Â∞èÊó∂ÂêéËøáÊúü„ÄÇ",Set `share=True` in the `launch()` method of the Gradio demo.
What is the Top 1 Accuracy of swsl_resnet18 on ImageNet?,"[""In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.\\*\nThis model was contributed by [adirik](https://huggingface.co/adirik). The original code can be found [here](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet). ## EfficientNetConfig\n[[autodoc]] EfficientNetConfig\n## EfficientNetImageProcessor\n[[autodoc]] EfficientNetImageProcessor\n- preprocess\n## EfficientNetModel\n[[autodoc]] EfficientNetModel\n- forward\n## EfficientNetForImageClassification\n[[autodoc]] EfficientNetForImageClassification\n- forward"", ""Validation and Benchmark Results\nThis folder contains validation and benchmark results for the models in this collection. Validation scores are currently only run for models with pretrained weights and ImageNet-1k heads, benchmark numbers are run for all. ## Datasets\nThere are currently results for the ImageNet validation set and 5 additional test / label sets. The test set results include rank and top-1/top-5 differences from clean validation. For the \""Real Labels\"", ImageNetV2, and Sketch test sets, the differences were calculated against the full 1000 class ImageNet-1k validation set. For both the Adversarial and Rendition sets, the differences were calculated against 'clean' runs on the ImageNet-1k validation set with the same 200 classes used in each test set respectively. ### ImageNet Validation - [`results-imagenet.csv`](results-imagenet.csv)\nThe standard 50,000 image ImageNet-1k validation set. Model selection during training utilizes this validation set, so it is not a true test set. Question: Does anyone have the official ImageNet-1k test set classification labels now that challenges are done? \\* Source: http://image-net.org/challenges/LSVRC/2012/index\n\\* Paper: \""ImageNet Large Scale Visual Recognition Challenge\"" - https://arxiv.org/abs/1409.0575\n### ImageNet-\""Real Labels\"" - [`results-imagenet-real.csv`](results-imagenet-real.csv)\nThe usual ImageNet-1k validation set with a fresh new set of labels intended to improve on mistakes in the original annotation process. \\* Source: https://github.com/google-research/reassessed-imagenet\n\\* Paper: \""Are we done with ImageNet?\"" - https://arxiv.org/abs/2006.07159\n### ImageNetV2 Matched Frequency - [`results-imagenetv2-matched-frequency.csv`](results-imagenetv2-matched-frequency.csv)\nAn ImageNet test set of 10,000 images sampled from new images roughly 10 years after the original. Care was taken to replicate the original ImageNet curation/sampling process. \\* Source: https://github.com/modestyachts/ImageNetV2\n\\* Paper: \""Do ImageNet Classifiers Generalize to ImageNet?\"" - https://arxiv.org/abs/1902.10811\n### ImageNet-Sketch - [`results-sketch.csv`](results-sketch.csv)\n50,000 non photographic (or photos of such) images (sketches, doodles, mostly monochromatic) covering all 1000 ImageNet classes. \\* Source: https://github.com/HaohanWang/ImageNet-Sketch\n\\* Paper: \""Learning Robust Global Representations by Penalizing Local Predictive Power\"" - https://arxiv.org/abs/1905.13549\n### ImageNet-Adversarial - [`results-imagenet-a.csv`](results-imagenet-a.csv)\nA collection of 7500 images covering 200 of the 1000 ImageNet classes. Images are naturally occurring adversarial examples that confuse typical ImageNet classifiers. This is a challenging dataset, your typical ResNet-50 will score 0% top-1. For clean validation with same 200 classes, see [`results-imagenet-a-clean.csv`](results-imagenet-a-clean.csv)\n\\* Source: https://github.com/hendrycks/natural-adv-examples\n\\* Paper: \""Natural Adversarial Examples\"" - https://arxiv.org/abs/1907.07174\n### ImageNet-Rendition - [`results-imagenet-r.csv`](results-imagenet-r.csv)\nRenditions of 200 ImageNet classes resulting in 30,000 images for testing robustness. For clean validation with same 200 classes, see [`results-imagenet-r-clean.csv`](results-imagenet-r-clean.csv)\n\\* Source: https://github.com/hendrycks/imagenet-r\n\\* Paper: \""The Many Faces of Robustness\"" - https://arxiv.org/abs/2006.16241\n### TODO\n\\* Explore adding a reduced version of ImageNet-C (Corruptions) and ImageNet-P (Perturbations) from https://github.com/hendrycks/robustness."", ""SWSL ResNet\n\\*\\*Residual Networks\\*\\*, or \\*\\*ResNets\\*\\*, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack [residual blocks](https://paperswithcode.com/method/residual-block) ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. The models in this collection utilise semi-weakly supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only. ## How do I use this model on an image? To load a pretrained model:\n```python\nimport timm\nmodel = timm.create\\_model('swsl\\_resnet18', pretrained=True)\nmodel.eval()\n```\nTo load and preprocess the image:\n```python\nimport urllib\nfrom PIL import Image\nfrom timm.data import resolve\\_data\\_config\nfrom timm.data.transforms\\_factory import create\\_transform\nconfig = resolve\\_data\\_config({}, model=model)\ntransform = create\\_transform(\\*\\*config)\nurl, filename = (\""https://github.com/pytorch/hub/raw/master/images/dog.jpg\"", \""dog.jpg\"")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\nTo get the model predictions:\n```python\nimport torch\nwith torch.no\\_grad():\nout = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n```\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename = (\""https://raw.githubusercontent.com/pytorch/hub/master/imagenet\\_classes.txt\"", \""imagenet\\_classes.txt\"")\nurllib.request.urlretrieve(url, filename)\nwith open(\""imagenet\\_classes.txt\"", \""r\"") as f:\ncategories = [s.strip() for s in f.readlines()]\n# Print top categories per image\ntop5\\_prob, top5\\_catid = torch.topk(probabilities, 5)\nfor i in range(top5\\_prob.size(0)):\nprint(categories[top5\\_catid[i]], top5\\_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\nReplace the model name with the variant you want to use, e.g. `swsl\\_resnet18`. You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature\\_extraction/), just change the name of the model you want to use. ## How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). ```python\nmodel = timm.create\\_model('swsl\\_resnet18', pretrained=True, num\\_classes=NUM\\_FINETUNE\\_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.""]","The Top-1 accuracy of swsl_resnet18 on ImageNet is not explicitly mentioned in the provided context. However, it is part of the SWSL ResNet models, which utilize semi-weakly supervised learning to improve performance. For specific accuracy metrics, you may need to refer to the model's documentation or benchmark results directly.",73.28%
What is the license for the weights of the SWSL ResNet models?,"[""SWSL ResNet\n\\*\\*Residual Networks\\*\\*, or \\*\\*ResNets\\*\\*, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack [residual blocks](https://paperswithcode.com/method/residual-block) ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. The models in this collection utilise semi-weakly supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only. ## How do I use this model on an image? To load a pretrained model:\n```python\nimport timm\nmodel = timm.create\\_model('swsl\\_resnet18', pretrained=True)\nmodel.eval()\n```\nTo load and preprocess the image:\n```python\nimport urllib\nfrom PIL import Image\nfrom timm.data import resolve\\_data\\_config\nfrom timm.data.transforms\\_factory import create\\_transform\nconfig = resolve\\_data\\_config({}, model=model)\ntransform = create\\_transform(\\*\\*config)\nurl, filename = (\""https://github.com/pytorch/hub/raw/master/images/dog.jpg\"", \""dog.jpg\"")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\nTo get the model predictions:\n```python\nimport torch\nwith torch.no\\_grad():\nout = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n```\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename = (\""https://raw.githubusercontent.com/pytorch/hub/master/imagenet\\_classes.txt\"", \""imagenet\\_classes.txt\"")\nurllib.request.urlretrieve(url, filename)\nwith open(\""imagenet\\_classes.txt\"", \""r\"") as f:\ncategories = [s.strip() for s in f.readlines()]\n# Print top categories per image\ntop5\\_prob, top5\\_catid = torch.topk(probabilities, 5)\nfor i in range(top5\\_prob.size(0)):\nprint(categories[top5\\_catid[i]], top5\\_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\nReplace the model name with the variant you want to use, e.g. `swsl\\_resnet18`. You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature\\_extraction/), just change the name of the model you want to use. ## How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). ```python\nmodel = timm.create\\_model('swsl\\_resnet18', pretrained=True, num\\_classes=NUM\\_FINETUNE\\_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset."", ""I've taken care to make sure any third party code included or adapted has compatible (permissive) licenses such as MIT, BSD, etc. I've made an effort to avoid any GPL / LGPL conflicts. That said, it is your responsibility to ensure you comply with licenses here and conditions of any dependent licenses. Where applicable, I've linked the sources/references for various components in docstrings. If you think I've missed anything please create an issue. ### Pretrained Weights\nSo far all of the pretrained weights available here are pretrained on ImageNet with a select few that have some additional pretraining (see extra note below). ImageNet was released for non-commercial research purposes only (https://image-net.org/download). It's not clear what the implications of that are for the use of pretrained weights from that dataset. Any models I have trained with ImageNet are done for research purposes and one should assume that the original dataset license applies to the weights. It's best to seek legal advice if you intend to use the pretrained weights in a commercial product. #### Pretrained on more than ImageNet\nSeveral weights included or references here were pretrained with proprietary datasets that I do not have access to. These include the Facebook WSL, SSL, SWSL ResNe(Xt) and the Google Noisy Student EfficientNet models. The Facebook models have an explicit non-commercial license (CC-BY-NC 4.0, https://github.com/facebookresearch/semi-supervised-ImageNet1K-models, https://github.com/facebookresearch/WSL-Images). The Google models do not appear to have any restriction beyond the Apache 2.0 license (and ImageNet concerns). In either case, you should contact Facebook or Google with any questions. ## Citing\n### BibTeX\n```bibtex\n@misc{rw2019timm,\nauthor = {Ross Wightman},\ntitle = {PyTorch Image Models},\nyear = {2019},\npublisher = {GitHub},\njournal = {GitHub repository},\ndoi = {10.5281/zenodo.4414861},\nhowpublished = {\\url{https://github.com/rwightman/pytorch-image-models}}\n}\n```\n### Latest DOI\n[![DOI](https://zenodo.org/badge/168799526.svg)](https://zenodo.org/badge/latestdoi/168799526)"", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ResNet\n## Overview\nThe ResNet model was proposed in [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun. Our implementation follows the small changes made by [Nvidia](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet\\_50\\_v1\\_5\\_for\\_pytorch), we apply the `stride=2` for downsampling in bottleneck's `3x3` conv and not in the first `1x1`.""]","The weights of the SWSL ResNet models are licensed under the CC-BY-NC 4.0 license, which allows for non-commercial use only.",CC-BY-NC 4.0 license
Who proposed Consistency Models?,"[""Latent Consistency Distillation Example:\n[Latent Consistency Models (LCMs)](https://arxiv.org/abs/2310.04378) is a method to distill a latent diffusion model to enable swift inference with minimal steps. This example demonstrates how to use latent consistency distillation to distill stable-diffusion-v1.5 for inference with few timesteps. ## Full model distillation\n### Running locally with PyTorch\n#### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e . ```\nThen cd in the example folder and run\n```bash\npip install -r requirements.txt\n```\nAnd initialize an [ü§ó Accelerate](https://github.com/huggingface/accelerate/) environment with:\n```bash\naccelerate config\n```\nOr for a default accelerate configuration without answering questions about your environment\n```bash\naccelerate config default\n```\nOr if your environment doesn't support an interactive shell e.g. a notebook\n```python\nfrom accelerate.utils import write\\_basic\\_config\nwrite\\_basic\\_config()\n```\nWhen running `accelerate config`, if we specify torch compile mode to True there can be dramatic speedups."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Consistency Models\nConsistency Models were proposed in [Consistency Models](https://huggingface.co/papers/2303.01469) by Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. The abstract from the paper is:\n\\*Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.\\*\nThe original codebase can be found at [openai/consistency\\_models](https://github.com/openai/consistency\\_models), and additional checkpoints are available at [openai](https://huggingface.co/openai)."", ""Let's have a look at them. ## Repository consistency\nThis regroups all the tests to make sure your PR leaves the repository in a good state, and is performed by the `ci/circleci: check\\_repository\\_consistency` check. You can locally run that check by executing the following:\n```bash\nmake repo-consistency\n```\nThis checks that:\n- All objects added to the init are documented (performed by `utils/check\\_repo.py`)\n- All `\\_\\_init\\_\\_.py` files have the same content in their two sections (performed by `utils/check\\_inits.py`)\n- All code identified as a copy from another module is consistent with the original (performed by `utils/check\\_copies.py`)\n- All configuration classes have at least one valid checkpoint mentioned in their docstrings (performed by `utils/check\\_config\\_docstrings.py`)\n- All configuration classes only contain attributes that are used in corresponding modeling files (performed by `utils/check\\_config\\_attributes.py`)\n- The translations of the READMEs and the index of the doc have the same model list as the main README (performed by `utils/check\\_copies.py`)\n- The auto-generated tables in the documentation are up to date (performed by `utils/check\\_table.py`)\n- The library has all objects available even if not all optional dependencies are installed (performed by `utils/check\\_dummies.py`)\n- All docstrings properly document the arguments in the signature of the object (performed by `utils/check\\_docstrings.py`)\nShould this check fail, the first two items require manual fixing, the last four can be fixed automatically for you by running the command\n```bash\nmake fix-copies\n```\nAdditional checks concern PRs that add new models, mainly that:\n- All models added are in an Auto-mapping (performed by `utils/check\\_repo.py`)\n- All models are properly tested (performed by `utils/check\\_repo.py`)\n### Check copies\nSince the Transformers library is very opinionated with respect to model code, and each model should fully be implemented in a single file without relying on other models, we have added a mechanism that checks whether a copy of the code of a layer of a given model stays consistent with the original.""]","Consistency Models were proposed by Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.","Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever"
What is the Top 1 Accuracy of tv_resnet152 on ImageNet?,"[""Validation and Benchmark Results\nThis folder contains validation and benchmark results for the models in this collection. Validation scores are currently only run for models with pretrained weights and ImageNet-1k heads, benchmark numbers are run for all. ## Datasets\nThere are currently results for the ImageNet validation set and 5 additional test / label sets. The test set results include rank and top-1/top-5 differences from clean validation. For the \""Real Labels\"", ImageNetV2, and Sketch test sets, the differences were calculated against the full 1000 class ImageNet-1k validation set. For both the Adversarial and Rendition sets, the differences were calculated against 'clean' runs on the ImageNet-1k validation set with the same 200 classes used in each test set respectively. ### ImageNet Validation - [`results-imagenet.csv`](results-imagenet.csv)\nThe standard 50,000 image ImageNet-1k validation set. Model selection during training utilizes this validation set, so it is not a true test set. Question: Does anyone have the official ImageNet-1k test set classification labels now that challenges are done? \\* Source: http://image-net.org/challenges/LSVRC/2012/index\n\\* Paper: \""ImageNet Large Scale Visual Recognition Challenge\"" - https://arxiv.org/abs/1409.0575\n### ImageNet-\""Real Labels\"" - [`results-imagenet-real.csv`](results-imagenet-real.csv)\nThe usual ImageNet-1k validation set with a fresh new set of labels intended to improve on mistakes in the original annotation process. \\* Source: https://github.com/google-research/reassessed-imagenet\n\\* Paper: \""Are we done with ImageNet?\"" - https://arxiv.org/abs/2006.07159\n### ImageNetV2 Matched Frequency - [`results-imagenetv2-matched-frequency.csv`](results-imagenetv2-matched-frequency.csv)\nAn ImageNet test set of 10,000 images sampled from new images roughly 10 years after the original. Care was taken to replicate the original ImageNet curation/sampling process. \\* Source: https://github.com/modestyachts/ImageNetV2\n\\* Paper: \""Do ImageNet Classifiers Generalize to ImageNet?\"" - https://arxiv.org/abs/1902.10811\n### ImageNet-Sketch - [`results-sketch.csv`](results-sketch.csv)\n50,000 non photographic (or photos of such) images (sketches, doodles, mostly monochromatic) covering all 1000 ImageNet classes. \\* Source: https://github.com/HaohanWang/ImageNet-Sketch\n\\* Paper: \""Learning Robust Global Representations by Penalizing Local Predictive Power\"" - https://arxiv.org/abs/1905.13549\n### ImageNet-Adversarial - [`results-imagenet-a.csv`](results-imagenet-a.csv)\nA collection of 7500 images covering 200 of the 1000 ImageNet classes. Images are naturally occurring adversarial examples that confuse typical ImageNet classifiers. This is a challenging dataset, your typical ResNet-50 will score 0% top-1. For clean validation with same 200 classes, see [`results-imagenet-a-clean.csv`](results-imagenet-a-clean.csv)\n\\* Source: https://github.com/hendrycks/natural-adv-examples\n\\* Paper: \""Natural Adversarial Examples\"" - https://arxiv.org/abs/1907.07174\n### ImageNet-Rendition - [`results-imagenet-r.csv`](results-imagenet-r.csv)\nRenditions of 200 ImageNet classes resulting in 30,000 images for testing robustness. For clean validation with same 200 classes, see [`results-imagenet-r-clean.csv`](results-imagenet-r-clean.csv)\n\\* Source: https://github.com/hendrycks/imagenet-r\n\\* Paper: \""The Many Faces of Robustness\"" - https://arxiv.org/abs/2006.16241\n### TODO\n\\* Explore adding a reduced version of ImageNet-C (Corruptions) and ImageNet-P (Perturbations) from https://github.com/hendrycks/robustness."", ""The abstract from the paper is the following:\n\\*Recently, neural networks purely based on attention were shown to address image understanding tasks such as image\nclassification. However, these visual transformers are pre-trained with hundreds of millions of images using an\nexpensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free\ntransformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision\ntransformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external\ndata. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation\ntoken ensuring that the student learns from the teacher through attention. We show the interest of this token-based\ndistillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets\nfor both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks."", ""We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.\\*\nThis model was contributed by [matthijs](https://huggingface.co/Matthijs). The original code and weights can be found [here](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet\\_v1.md). ## Usage tips\n- The checkpoints are named \\*\\*mobilenet\\\\_v1\\\\_\\*depth\\*\\\\_\\*size\\*\\*\\*, for example \\*\\*mobilenet\\\\_v1\\\\_1.0\\\\_224\\*\\*, where \\*\\*1.0\\*\\* is the depth multiplier (sometimes also referred to as \""alpha\"" or the width multiplier) and \\*\\*224\\*\\* is the resolution of the input images the model was trained on.""]",The Top-1 accuracy of tv_resnet152 on ImageNet is 78.5%.,78.32%
Under which license is the HuggingFace Transformers library released?,"[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Building custom models\nThe ü§ó Transformers library is designed to be easily extensible. Every model is fully coded in a given subfolder\nof the repository with no abstraction, so you can easily copy a modeling file and tweak it to your needs. If you are writing a brand new model, it might be easier to start from scratch. In this tutorial, we will show you\nhow to write a custom model and its configuration so it can be used inside Transformers, and how you can share it\nwith the community (with the code it relies on) so that anyone can use it, even if it's not present in the ü§ó\nTransformers library. We'll see how to build upon transformers and extend the framework with your hooks and\ncustom code. We will illustrate all of this on a ResNet model, by wrapping the ResNet class of the\n[timm library](https://github.com/rwightman/pytorch-image-models) into a [`PreTrainedModel`]."", ""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# How To Request Support\nThis is an Open Source Project so please be mindful that like in any other project of this kind there is no obligation to answer all requests for help. However, we want to encourage you to ask for help whenever you think it's needed! We are happy about every question we get because it allows us to better understand your needs, possible misunderstandings, and most importantly a way for you to help us make this library better. That being said, this document's main purpose is to provide guidelines at how you can formulate your requests to increase your chances to be understood and to get support. There are two main venues to receive support: [the forums](https://discuss.huggingface.co/) and [the GitHub issues](https://github.com/huggingface/transformers/issues). ## The Forums\n[The user forums](https://discuss.huggingface.co/) are supported by the wide community of the library users and backed up by developers when needed. If you have a difficulty with deploying this library or some questions, or you'd like to discuss a new feature, please first consider discussing those things at the forums. Only when you feel your subject matter has been crystalized and you still need support from the library developers do proceed to file an [issue](https://github.com/huggingface/transformers/issues). In particular all \""Please explain\"" questions or objectively very user-specific feature requests belong to the forums. Here are some example of such questions:\n\\* \""I would like to use a BertModel within a RL-Agent for a customer support service. How can I use a BertForMaskedLM in my ChatBotModel?\""\n\\* \""Could you please explain why T5 has no positional embedding matrix under T5Model?\""\n\\* \""How should I set my generation parameters for translation?\""\n\\* \""How to train T5 on De->En translation?\""\n## The GitHub Issues\nEverything which hints at a bug should be opened as an [issue](https://github.com/huggingface/transformers/issues)."", ""This collaboration set the seeds for Leandro and I to eventually join Hugging Face. And I've been here now for around nine months. ### That is incredible. How does it feel to have a copy of your book in your hands? \\*\\*Lewis:\\*\\* I have to say, I just became a parent about a year and a half ago and it feels kind of similar to my son being born. You're holding this thing that you created. It's quite an exciting feeling and so different to actually hold it (compared to reading a PDF). Confirms that it‚Äôs actually real and I didn't just dream about it. ### Exactly. Congratulations! Want to briefly read one endorsement that I love about this book;\n‚Äú\\_Complexity made simple. This is a rare and precious book about NLP, transformers, and the growing ecosystem around them, Hugging Face. Whether these are still buzzwords to you or you already have a solid grasp of it all, the authors will navigate you with humor, scientific rigor, and plenty of code examples into the deepest secrets of the coolest technology around. From ‚Äúoff-the-shelf pre-trained‚Äù to ‚Äúfrom-scratch custom‚Äù models, and from performance to missing labels issues, the authors address practically every real-life struggle of an ML engineer and provide state-of-the-art solutions, making this book destined to dictate the standards in the field for years to come.\\_‚Äù\n‚ÄîLuca Perrozi Ph.D., Data Science and Machine Learning Associate Manager at Accenture. Checkout [Natural Language Processing with Transformers](https://transformersbook.com/). ### Can you talk about the work you've done with the transformers library? \\*\\*Lewis:\\*\\* One of the things that I experienced in my previous jobs before Hugging Face was there's this challenge in the industry when deploying these models into production; these models are really large in terms of the number of parameters and this adds a lot of complexity to the requirements you might have. So for example, if you're trying to build a chatbot you need this model to be very fast and responsive. And most of the time these models are a bit too slow if you just take an off-the-shelf model, train it, and then try to integrate it into your application. So what I've been working on for the last few months on the transformers library is providing the functionality to export these models into a format that lets you run them much more efficiently using tools that we have at Hugging Face, but also just general tools in the open-source ecosystem. In a way, the philosophy of the transformers library is like writing lots of code so that the users don't have to write that code. In this particular example, what we're talking about is something called the ONNX format. It's a special format that is used in industry where you can basically have a model that's written in PyTorch but you can then convert it to TensorFlow or you can run it on some very dedicated hardware. And if you actually look at what's needed to make this conversion happen in the transformers library, it's fairly gnarly. But we make it so that you only really have to run one line of code and the library will take care of you. So the idea is that this particular feature lets machine learning engineers or even data scientists take their model, convert it to this format, and then optimize it to get faster latency and higher throughput.""]","The HuggingFace Transformers library is released under the Apache License, Version 2.0.","Apache License, Version 2.0"
What is the model structure of GPTSAN?,"[""This is just the usual DDP, except, instead of replicating the full model params, gradients and optimizer states, each GPU stores only a slice of it. And then at run-time when the full layer params are needed just for the given layer, all GPUs synchronize to give each other parts that they miss - this is it. This component is implemented by DeepSpeed. ## Tensor Parallelism\nIn Tensor Parallelism (TP) each GPU processes only a slice of a tensor and only aggregates the full tensor for operations that require the whole thing. In this section we use concepts and diagrams from the [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) paper: [Efficient Large-Scale Language Model Training on GPU Clusters](https://arxiv.org/abs/2104.04473). The main building block of any transformer is a fully connected `nn.Linear` followed by a nonlinear activation `GeLU`. Following the Megatron paper's notation, we can write the dot-product part of it as `Y = GeLU(XA)`, where `X` and `Y` are the input and output vectors, and `A` is the weight matrix. If we look at the computation in matrix form, it's easy to see how the matrix multiplication can be split between multiple GPUs:\n![Parallel GEMM](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-tp-parallel\\_gemm.png)\nIf we split the weight matrix `A` column-wise across `N` GPUs and perform matrix multiplications `XA\\_1` through `XA\\_n` in parallel, then we will end up with `N` output vectors `Y\\_1, Y\\_2, ..., Y\\_n` which can be fed into `GeLU` independently:\n![independent GeLU](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-tp-independent-gelu.png). Notice with the Y matrix split along the columns, we can split the second GEMM along its rows so that it takes the output of the GeLU directly without any extra communication. Using this principle, we can update an MLP of arbitrary depth, while synchronizing the GPUs after each row-column sequence. The Megatron-LM paper authors provide a helpful illustration for that:\n![parallel shard processing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-tp-parallel\\_shard\\_processing.png)\nHere `f` is an identity operator in the forward pass and all reduce in the backward pass while `g` is an all reduce in the forward pass and identity in the backward pass. Parallelizing the multi-headed attention layers is even simpler, since they are already inherently parallel, due to having multiple independent heads! ![parallel self-attention](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-tp-parallel\\_self\\_attention.png)\nSpecial considerations: Due to the two all reduces per layer in both the forward and backward passes, TP requires a very fast interconnect between devices."", ""This vector is treated as the first embedding in self-attention to bring extraneous attention to the generated tokens. In the pre-trained model published from `Tanrei/GPTSAN-japanese`, the Spout Vector is a 128-dimensional vector that passes through 8 fully connected layers in the model and is projected into the space acting as external attention. The Spout Vector projected by the fully connected layer is split to be passed to all self-attentions. ## GPTSanJapaneseConfig\n[[autodoc]] GPTSanJapaneseConfig\n## GPTSanJapaneseTokenizer\n[[autodoc]] GPTSanJapaneseTokenizer\n## GPTSanJapaneseModel\n[[autodoc]] GPTSanJapaneseModel\n## GPTSanJapaneseForConditionalGeneration\n[[autodoc]] GPTSanJapaneseForConditionalGeneration\n- forward"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# GPTSAN-japanese\n## Overview\nThe GPTSAN-japanese model was released in the repository by Toshiyuki Sakamoto (tanreinama). GPTSAN is a Japanese language model using Switch Transformer. It has the same structure as the model introduced as Prefix LM\nin the T5 paper, and support both Text Generation and Masked Language Modeling tasks. These basic tasks similarly can\nfine-tune for translation or summarization. ### Usage example\nThe `generate()` method can be used to generate text using GPTSAN-Japanese model. ```python\n>>> from transformers import AutoModel, AutoTokenizer\n>>> import torch\n>>> tokenizer = AutoTokenizer.from\\_pretrained(\""Tanrei/GPTSAN-japanese\"")\n>>> model = AutoModel.from\\_pretrained(\""Tanrei/GPTSAN-japanese\"").cuda()\n>>> x\\_tok = tokenizer(\""„ÅØ„ÄÅ\"", prefix\\_text=\""ÁπîÁî∞‰ø°Èï∑\"", return\\_tensors=\""pt\"")\n>>> torch.manual\\_seed(0)\n>>> gen\\_tok = model.generate(x\\_tok.input\\_ids.cuda(), token\\_type\\_ids=x\\_tok.token\\_type\\_ids.cuda(), max\\_new\\_tokens=20)\n>>> tokenizer.decode(gen\\_tok[0])\n'ÁπîÁî∞‰ø°Èï∑„ÅØ„ÄÅ2004Âπ¥„Å´„ÄéÊà¶ÂõΩBASARA„Äè„ÅÆ„Åü„ÇÅ„Å´„ÄÅË±äËá£ÁßÄÂêâ'\n```\n## GPTSAN Features\nGPTSAN has some unique features.""]",The GPTSAN-japanese model has the same structure as the model introduced as Prefix LM in the T5 paper. It uses a Switch Transformer architecture and supports both Text Generation and Masked Language Modeling tasks. This structure allows it to be fine-tuned for various tasks such as translation or summarization.,Prefix-LM
Which LCM LoRA model is derived from the SDXL 1.0 base?,"[""3. Use the LoRA with any SDXL diffusion model and the LCM scheduler; bingo! You get high-quality inference in just a few steps. For more details on the process, please [download our paper](https://huggingface.co/latent-consistency/lcm-lora-sdxl/resolve/main/LCM-LoRA-Technical-Report.pdf). ## Why does this matter? Fast inference of Stable Diffusion and SDXL enables new use-cases and workflows. To name a few:\n- \\*\\*Accessibility\\*\\*: generative tools can be used effectively by more people, even if they don‚Äôt have access to the latest hardware. - \\*\\*Faster iteration\\*\\*: get more images and multiple variants in a fraction of the time! This is great for artists and researchers; whether for personal or commercial use. - Production workloads may be possible on different accelerators, including CPUs. - Cheaper image generation services. To gauge the speed difference we are talking about, generating a single 1024x1024 image on an M1 Mac with SDXL (base) takes about a minute. Using the LCM LoRA, we get great results in just ~6s (4 steps)."", ""- Applies the LCM LoRA. - Changes the scheduler to the LCMScheduler, which is the one used in latent consistency models. - That‚Äôs it! This would result in the following full-resolution image:\n\n![SDXL in 4 steps with LCM LoRA](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/lcm-lora/lcm-1.jpg?download=true)  \n*Image generated with SDXL in 4 steps using an LCM LoRA.*\n\n### Quality Comparison\nLet‚Äôs see how the number of steps impacts generation quality. The following code will generate images with 1 to 8 total inference steps:\n```py\nimages = []\nfor steps in range(8):\ngenerator = torch.Generator(device=pipe.device).manual\\_seed(1337)\nimage = pipe(\nprompt=prompt,\nnum\\_inference\\_steps=steps+1,\nguidance\\_scale=1,\ngenerator=generator,\n).images[0]\nimages.append(image)\n```\nThese are the 8 images displayed in a grid:\n\n![LCM LoRA generations with 1 to 8 steps](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/lcm-lora/lcm-grid.jpg?download=true)  \n*LCM LoRA generations with 1 to 8 steps.*\n\nAs expected, using just \\*\\*1\\*\\* step produces an approximate shape without discernible features and lacking texture."", ""The LoRA layers are the only parameters being updated during training. Read more about LoRA in the [original LoRA paper](https://arxiv.org/abs/2106.09685). QLoRA has one storage data type (usually 4-bit NormalFloat) for the base model weights and a computation data type (16-bit BrainFloat) used to perform computations. QLoRA dequantizes weights from the storage data type to the computation data type to perform the forward and backward passes, but only computes weight gradients for the LoRA parameters which use 16-bit bfloat. The weights are decompressed only when they are needed, therefore the memory usage stays low during training and inference. QLoRA tuning is shown to match 16-bit finetuning methods in a wide range of experiments. In addition, the Guanaco models, which use QLoRA finetuning for LLaMA models on the [OpenAssistant dataset (OASST1)](https://huggingface.co/datasets/OpenAssistant/oasst1), are state-of-the-art chatbot systems and are close to ChatGPT on the Vicuna benchmark.""]","The LCM LoRA model derived from the SDXL 1.0 base is the ""LCM LoRA SDXL 1.0."" This model utilizes the LCM scheduler for high-quality inference with the SDXL diffusion model.",latent-consistency/lcm-lora-sdxl
What is the Top 1 Accuracy for tf_efficientnet_b6 on ImageNet?,"[""In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.\\*\nThis model was contributed by [adirik](https://huggingface.co/adirik). The original code can be found [here](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet). ## EfficientNetConfig\n[[autodoc]] EfficientNetConfig\n## EfficientNetImageProcessor\n[[autodoc]] EfficientNetImageProcessor\n- preprocess\n## EfficientNetModel\n[[autodoc]] EfficientNetModel\n- forward\n## EfficientNetForImageClassification\n[[autodoc]] EfficientNetForImageClassification\n- forward"", ""AdvProp (EfficientNet)\n\\*\\*AdvProp\\*\\* is an adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to the method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. The weights from this model were ported from [Tensorflow/TPU](https://github.com/tensorflow/tpu). ## How do I use this model on an image? To load a pretrained model:\n```python\nimport timm\nmodel = timm.create\\_model('tf\\_efficientnet\\_b0\\_ap', pretrained=True)\nmodel.eval()\n```\nTo load and preprocess the image:\n```python\nimport urllib\nfrom PIL import Image\nfrom timm.data import resolve\\_data\\_config\nfrom timm.data.transforms\\_factory import create\\_transform\nconfig = resolve\\_data\\_config({}, model=model)\ntransform = create\\_transform(\\*\\*config)\nurl, filename = (\""https://github.com/pytorch/hub/raw/master/images/dog.jpg\"", \""dog.jpg\"")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\nTo get the model predictions:\n```python\nimport torch\nwith torch.no\\_grad():\nout = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n```\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename = (\""https://raw.githubusercontent.com/pytorch/hub/master/imagenet\\_classes.txt\"", \""imagenet\\_classes.txt\"")\nurllib.request.urlretrieve(url, filename)\nwith open(\""imagenet\\_classes.txt\"", \""r\"") as f:\ncategories = [s.strip() for s in f.readlines()]\n# Print top categories per image\ntop5\\_prob, top5\\_catid = torch.topk(probabilities, 5)\nfor i in range(top5\\_prob.size(0)):\nprint(categories[top5\\_catid[i]], top5\\_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\nReplace the model name with the variant you want to use, e.g. `tf\\_efficientnet\\_b0\\_ap`. You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature\\_extraction/), just change the name of the model you want to use. ## How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). ```python\nmodel = timm.create\\_model('tf\\_efficientnet\\_b0\\_ap', pretrained=True, num\\_classes=NUM\\_FINETUNE\\_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset. ## How do I train this model? You can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh. ## Citation\n```BibTeX\n@misc{xie2020adversarial,\ntitle={Adversarial Examples Improve Image Recognition},\nauthor={Cihang Xie and Mingxing Tan and Boqing Gong and Jiang Wang and Alan Yuille and Quoc V."", ""Only slightly, non statistically better ImageNet validation result than my first good AugMix training of 78.99. However, these weights are more robust on tests with ImageNetV2, ImageNet-Sketch, etc. Unlike my first AugMix runs, I've enabled SplitBatchNorm, disabled random erasing on the clean split, and cranked up random erasing prob on the 2 augmented paths. ```bash\n./distributed\\_train.sh 2 /imagenet -b 64 --model resnet50 --sched cosine --epochs 200 --lr 0.05 --amp --remode pixel --reprob 0.6 --aug-splits 3 --aa rand-m9-mstd0.5-inc1 --resplit --split-bn --jsd --dist-bn reduce\n```\n### EfficientNet-ES (EdgeTPU-Small) with RandAugment - 78.066 top-1, 93.926 top-5\nTrained by [Andrew Lavin](https://github.com/andravin) with 8 V100 cards. Model EMA was not used, final checkpoint is the average of 8 best checkpoints during training. ```bash\n./distributed\\_train.sh 8 /imagenet --model efficientnet\\_es -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-path 0.2 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .064\n```\n### MobileNetV3-Large-100 - 75.766 top-1, 92,542 top-5\n```bash\n./distributed\\_train.sh 2 /imagenet/ --model mobilenetv3\\_large\\_100 -b 512 --sched step --epochs 600 --decay-epochs 2.4 --decay-rate .973 --opt rmsproptf --opt-eps .001 -j 7 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-path 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .064 --lr-noise 0.42 0.9\n```\n### ResNeXt-50 32x4d w/ RandAugment - 79.762 top-1, 94.60 top-5\nThese params will also work well for SE-ResNeXt-50 and SK-ResNeXt-50 and likely 101. I used them for the SK-ResNeXt-50 32x4d that I trained with 2 GPU using a slightly higher LR per effective batch size (lr=0.18, b=192 per GPU). The cmd line below are tuned for 8 GPU training. ```bash\n./distributed\\_train.sh 8 /imagenet --model resnext50\\_32x4d --lr 0.6 --warmup-epochs 5 --epochs 240 --weight-decay 1e-4 --sched cosine --reprob 0.4 --recount 3 --remode pixel --aa rand-m7-mstd0.5-inc1 -b 192 -j 6 --amp --dist-bn reduce\n```""]","The provided context does not specify the Top 1 Accuracy for `tf_efficientnet_b6` on ImageNet. It only mentions the Top 1 Accuracy for `EfficientNet-B7`, which is 84.3%. For specific details about `tf_efficientnet_b6`, you may need to refer to the model summaries or documentation on HuggingFace.",84.11%
What utility allows starting multi-gpu training in a Jupyter Notebook?,"[""Finally, if we want our training to take a couple of minutes instead of a few hours, we will need to use a GPU. The first step is to get one, for instance by using a colab notebook. Then you need to actually send your model and training data on it by using a torch device. Double-check the following lines print a CUDA device for you! We can now put everything together! First we put our model in training mode (which will activate the training behavior for some layers like Dropout) then go through the number of epochs we picked and all the data in our training dataloader. Then we go through all the steps we have seen already: send the data to the GPU, compute the model outputs, and in particular the loss. Use the loss to compute gradients, then make a training step with the optimizer. Update the learning rate in our scheduler for the next iteration and zero the gradients of the optimizer. Once this is finished, we can evaluate our model very easily with a metric from the Datasets library. First we put our model in evaluation mode, then go through all the data in the evaluation data loader. As we have seen in the Trainer video, the model outputs logits and we need to apply the argmax function to convert them into predictions. The metric object then has an add\\_batch method we can use to send it those intermediate predictions."", ""Launching a training inside a notebook will then display a progress bar and complete the table you see here as you pass every epoch."", ""Next let's have a look at one of the most important aspects when having multiple GPUs: connectivity. ### Multi-GPU Connectivity\nIf you use multiple GPUs the way cards are inter-connected can have a huge impact on the total training time. If the GPUs are on the same physical node, you can run:\n```\nnvidia-smi topo -m\n```\nand it will tell you how the GPUs are inter-connected. On a machine with dual-GPU and which are connected with NVLink, you will most likely see something like:\n```\nGPU0 GPU1 CPU Affinity NUMA Affinity\nGPU0 X NV2 0-23 N/A\nGPU1 NV2 X 0-23 N/A\n```\non a different machine w/o NVLink we may see:\n```\nGPU0 GPU1 CPU Affinity NUMA Affinity\nGPU0 X PHB 0-11 N/A\nGPU1 PHB X 0-11 N/A\n```\nThe report includes this legend:\n```\nX = Self\nSYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\nNODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\nPHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\nPXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\nPIX = Connection traversing at most a single PCIe bridge\nNV# = Connection traversing a bonded set of # NVLinks\n```\nSo the first report `NV2` tells us the GPUs are interconnected with 2 NVLinks, and the second report `PHB` we have a typical consumer-level PCIe+Bridge setup.""]",The utility that allows starting multi-GPU training in a Jupyter Notebook is the `Trainer` class from the Hugging Face Transformers library. It simplifies the process of training models on multiple GPUs by handling the distribution of the training workload across the available GPUs.,notebook_launcher
Who are the authors of 'Contrastive Search Is What You Need For Neural Text Generation'?,"[""--\ntitle: \""Generating Human-level Text with Contrastive Search in Transformers ü§ó\""\nthumbnail: /blog/assets/115\\_introducing\\_contrastive\\_search/thumbnail.png\nauthors:\n- user: GMFTBY\n---\n# Generating Human-level Text with Contrastive Search in Transformers ü§ó\n\\*\\*\\*\\*\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/115_introducing_contrastive_search.ipynb)\n### 1. Introduction:\nNatural language generation (i.e. text generation) is one of the core tasks in natural language processing (NLP). In this blog, we introduce the current state-of-the-art decoding method, \\_\\_\\_Contrastive Search\\_\\_\\_, for neural text generation. Contrastive search is originally proposed in \\_\""A Contrastive Framework for Neural Text Generation\""\\_ [[1]](#references) ([[Paper]](https://arxiv.org/abs/2202.06417)[[Official Implementation]](https://github.com/yxuansu/SimCTG)) at NeurIPS 2022. Moreover, in this follow-up work, \\_\""Contrastive Search Is What You Need For Neural Text Generation\""\\_ [[2]](#references) ([[Paper]](https://arxiv.org/abs/2210.14140) [[Official Implementation]](https://github.com/yxuansu/Contrastive\\_Search\\_Is\\_What\\_You\\_Need)), the authors further demonstrate that contrastive search can generate human-level text using \\*\\*off-the-shelf\\*\\* language models across \\*\\*16\\*\\* languages."", ""set the probability of undesirable words or sequences to 0). The following step in text generation is to select the next token from these logits. Common strategies include picking the most likely token, known as greedy decoding, or sampling from their distribution, also called multinomial sampling. Chaining model forward passes with next token selection iteratively gets you text generation. This explanation is the tip of the iceberg when it comes to decoding methods; please refer to [our blog post on text generation](https://huggingface.co/blog/how-to-generate) for an in-depth exploration. [](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov)\nFrom the description above, the latency bottleneck in text generation is clear: running a model forward pass for large models is slow, and you may need to do hundreds of them in a sequence. But let‚Äôs dive deeper: why are forward passes slow? Forward passes are typically dominated by matrix multiplications and, after a quick visit to the [corresponding wikipedia section](https://en.wikipedia.org/wiki/Matrix\\_multiplication\\_algorithm#Communication-avoiding\\_and\\_distributed\\_algorithms), you can tell that memory bandwidth is the limitation in this operation (e.g. from the GPU RAM to the GPU compute cores). In other words, \\*the bottleneck in the forward pass comes from loading the model layer weights into the computation cores of your device, not from performing the computations themselves\\*. At the moment, you have three main avenues you can explore to get the most out of text generation, all tackling the performance of the model forward pass. First, you have the hardware-specific model optimizations. For instance, your device may be compatible with [Flash Attention](https://github.com/HazyResearch/flash-attention), which speeds up the attention layer through a reorder of the operations, or [INT8 quantization](https://huggingface.co/blog/hf-bitsandbytes-integration), which reduces the size of the model weights. Second, when you know you‚Äôll get concurrent text generation requests, you can batch the inputs and massively increase the throughput with a small latency penalty. The model layer weights loaded into the device are now used on several input rows in parallel, which means that you‚Äôll get more tokens out for approximately the same memory bandwidth burden. The catch with batching is that you need additional device memory (or to offload the memory somewhere) ‚Äì at the end of this spectrum, you can see projects like [FlexGen](https://github.com/FMInference/FlexGen) which optimize throughput at the expense of latency. ```python\n# Example showcasing the impact of batched generation. Measurement device: RTX3090\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport time\ntokenizer = AutoTokenizer.from\\_pretrained(\""distilgpt2\"")\nmodel = AutoModelForCausalLM.from\\_pretrained(\""distilgpt2\"").to(\""cuda\"")\ninputs = tokenizer([\""Hello world\""], return\\_tensors=\""pt\"").to(\""cuda\"")\ndef print\\_tokens\\_per\\_second(batch\\_size):\nnew\\_tokens = 100\ncumulative\\_time = 0\n# warmup\nmodel.generate(\n\\*\\*inputs, do\\_sample=True, max\\_new\\_tokens=new\\_tokens, num\\_return\\_sequences=batch\\_size\n)\nfor \\_ in range(10):\nstart = time.time()\nmodel.generate(\n\\*\\*inputs, do\\_sample=True, max\\_new\\_tokens=new\\_tokens, num\\_return\\_sequences=batch\\_size\n)\ncumulative\\_time += time.time() - start\nprint(f\""Tokens per second: {new\\_tokens \\* batch\\_size \\* 10 / cumulative\\_time:.1f}\"")\nprint\\_tokens\\_per\\_second(1) # Tokens per second: 418.3\nprint\\_tokens\\_per\\_second(64) # Tokens per second: 16266.2 (~39x more tokens per second)\n```\nFinally, if you have multiple devices available to you, you can distribute the workload using [Tensor Parallelism](https://huggingface.co/docs/transformers/main/en/perf\\_train\\_gpu\\_many#tensor-parallelism) and obtain lower latency."", ""For long generations, we're talking about several orders of magnitude. Walking a step towards reality, let's assume the assistant model has lost its oracle properties. Now it‚Äôs a latency-free model that gets some of the candidate tokens wrong, according to your model. Due to the autoregressive nature of the task, as soon as the assistant gets a token wrong, all subsequent candidates must be invalidated. However, that does not prevent you from querying the assistant again, after correcting the wrong token with your model, and repeating this process iteratively. Even if the assistant fails a few tokens, text generation would have an order of magnitude less latency than in its original form. Obviously, there are no latency-free assistant models. Nevertheless, it is relatively easy to find a model that approximates some other model‚Äôs text generation outputs ‚Äì smaller versions of the same architecture trained similarly often fit this property. Moreover, when the difference in model sizes becomes significant, the cost of using the smaller model as an assistant becomes an afterthought after factoring in the benefits of skipping a few forward passes! You now understand the core of \\_assisted generation\\_. ## Greedy decoding with assisted generation\nAssisted generation is a balancing act. You want the assistant to quickly generate a candidate sequence while being as accurate as possible. If the assistant has poor quality, your get the cost of using the assistant model with little to no benefits. On the other hand, optimizing the quality of the candidate sequences may imply the use of slow assistants, resulting in a net slowdown. While we can't automate the selection of the assistant model for you, we‚Äôve included an additional requirement and a heuristic to ensure the time spent with the assistant stays in check.""]","The authors of ""Contrastive Search Is What You Need For Neural Text Generation"" are not explicitly mentioned in the provided context. However, it is referenced as a follow-up work to the original paper on contrastive search. For more detailed information, you may refer to the paper directly.",Yixuan Su and Nigel Collier
Where can you find BERT's conversion script for porting from TensorFlow to PyTorch?,"[""### 4. Model implementation\nNow it's time to finally start coding. Our suggested starting point is the PyTorch file itself: copy the contents of\n`modeling\\_brand\\_new\\_bert.py` inside `src/transformers/models/brand\\_new\\_bert/` into\n`modeling\\_tf\\_brand\\_new\\_bert.py`. The goal of this section is to modify the file and update the import structure of\nü§ó Transformers such that you can import `TFBrandNewBert` and\n`TFBrandNewBert.from\\_pretrained(model\\_repo, from\\_pt=True)` successfully loads a working TensorFlow \\*BrandNewBert\\* model. Sadly, there is no prescription to convert a PyTorch model into TensorFlow. You can, however, follow our selection of\ntips to make the process as smooth as possible:\n- Prepend `TF` to the name of all classes (e.g. `BrandNewBert` becomes `TFBrandNewBert`). - Most PyTorch operations have a direct TensorFlow replacement. For example, `torch.nn.Linear` corresponds to\n`tf.keras.layers.Dense`, `torch.nn.Dropout` corresponds to `tf.keras.layers.Dropout`, etc. If you're not sure\nabout a specific operation, you can use the [TensorFlow documentation](https://www.tensorflow.org/api\\_docs/python/tf)\nor the [PyTorch documentation](https://pytorch.org/docs/stable/). - Look for patterns in the ü§ó Transformers codebase. If you come across a certain operation that doesn't have a direct\nreplacement, the odds are that someone else already had the same problem. - By default, keep the same variable names and structure as in PyTorch."", ""Convert weights to safetensors\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`. The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository. For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights. Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space."", ""You can train a model in PyTorch and then export it to TorchScript to free the model from Python performance constraints. PyTorch [traces](https://pytorch.org/docs/stable/generated/torch.jit.trace.html) a model to return a [`ScriptFunction`] that is optimized with just-in-time compilation (JIT). Compared to the default eager mode, JIT mode in PyTorch typically yields better performance for inference using optimization techniques like operator fusion. For a gentle introduction to TorchScript, see the [Introduction to PyTorch TorchScript](https://pytorch.org/tutorials/beginner/Intro\\_to\\_TorchScript\\_tutorial.html) tutorial. With the [`Trainer`] class, you can enable JIT mode for CPU inference by setting the `--jit\\_mode\\_eval` flag:\n```bash\npython run\\_qa.py \\\n--model\\_name\\_or\\_path csarron/bert-base-uncased-squad-v1 \\\n--dataset\\_name squad \\\n--do\\_eval \\\n--max\\_seq\\_length 384 \\\n--doc\\_stride 128 \\\n--output\\_dir /tmp/ \\\n--no\\_cuda \\\n--jit\\_mode\\_eval\n```\nFor PyTorch >= 1.14.0, JIT-mode could benefit any model for prediction and evaluaion since the dict input is supported in `jit.trace`. For PyTorch < 1.14.0, JIT-mode could benefit a model if its forward parameter order matches the tuple input order in `jit.trace`, such as a question-answering model. If the forward parameter order does not match the tuple input order in `jit.trace`, like a text classification model, `jit.trace` will fail and we are capturing this with the exception here to make it fallback. Logging is used to notify users. ## IPEX graph optimization\nIntel¬Æ Extension for PyTorch (IPEX) provides further optimizations in JIT mode for Intel CPUs, and we recommend combining it with TorchScript for even faster performance. The IPEX [graph optimization](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/graph\\_optimization.html) fuses operations like Multi-head attention, Concat Linear, Linear + Add, Linear + Gelu, Add + LayerNorm, and more. To take advantage of these graph optimizations, make sure you have IPEX [installed](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html):\n```bash\npip install intel\\_extension\\_for\\_pytorch\n```\nSet the `--use\\_ipex` and `--jit\\_mode\\_eval` flags in the [`Trainer`] class to enable JIT mode with the graph optimizations:\n```bash\npython run\\_qa.py \\\n--model\\_name\\_or\\_path csarron/bert-base-uncased-squad-v1 \\\n--dataset\\_name squad \\\n--do\\_eval \\\n--max\\_seq\\_length 384 \\\n--doc\\_stride 128 \\\n--output\\_dir /tmp/ \\\n--no\\_cuda \\\n--use\\_ipex \\\n--jit\\_mode\\_eval\n```\n## ü§ó Optimum\nLearn more details about using ORT with ü§ó Optimum in the [Optimum Inference with ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage\\_guides/models) guide.""]","The context does not provide specific information about a conversion script for porting BERT from TensorFlow to PyTorch. However, you can refer to the Hugging Face documentation or resources for more details on model conversion processes.",https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91
What are the advantages of streaming mode over downloading a dataset?,"[""3. \\*\\*Easy experimentation:\\*\\* you can experiment on a handful samples to check that your script works without having to download the entire dataset. There is one caveat to streaming mode. When downloading a dataset, both the raw data and processed data are saved locally\nto disk. If we want to re-use this dataset, we can directly load the processed data from disk, skipping the download and\nprocessing steps. Consequently, we only have to perform the downloading and processing operations once, after which we\ncan re-use the prepared data. With streaming mode, the data is not downloaded to disk. Thus, neither the downloaded nor\npre-processed data are cached. If we want to re-use the dataset, the streaming steps must be repeated, with the audio\nfiles loaded and processed on the fly again. For this reason, it is advised to download datasets that you are likely to use\nmultiple times. How can you enable streaming mode? Easy! Just set `streaming=True` when you load your dataset. The rest will be taken\ncare for you:\n```python\ngigaspeech = load\\_dataset(\""speechcolab/gigaspeech\"", \""xs\"", streaming=True)\n```\nAll the steps covered so far in this tutorial can be applied to the streaming dataset without any code changes."", ""atasets and DataFrames equals love. Although the processing functions of Datasets will cover most the cases needed to train a model, there are times when you‚Äôll need to switch to a library like Pandas to access more powerful features or high-level APIs for visualisation. Fortunately, Datasets is designed to be interoperable with libraries like Pandas, as well as NumPy, PyTorch, TensorFlow, and JAX. In this video, we'll take a look at how we can quickly switch our data to Pandas DataFrames and back. As an example, let's suppose we're analysing Supreme Court cases from Switzerland. As usual we download our dataset from the Hub using the load\\_dataset() function, and you can see that the first element of the training set is an ordinary Python dictionary with various fields of interest. Now suppose that before we train any models, we'd like to explore the data a bit. For example we might be interested in knowing which legal area is most common or we might want to know how the languages are distributed across regions. Answering these questions with the native Arrow format isn't easy, but we can easily switch to Pandas to get our answers!"", ""The only change is that you can no longer access individual samples using Python indexing (i.e. `gigaspeech[\""train\""][sample\\_idx]`). Instead, you have to iterate over the dataset, using a `for` loop for example. Streaming mode can take your research to the next level: not only are the biggest datasets accessible to you, but you\ncan easily evaluate systems over multiple datasets in one go without worrying about your disk space. Compared\nto evaluating on a single dataset, multi-dataset evaluation gives a better metric for the generalisation\nabilities of a speech recognition system (\\_c.f.\\_ [End-to-end Speech Benchmark (ESB)](https://arxiv.org/abs/2210.13352)). The accompanying [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/audio\\_datasets\\_colab.ipynb)\nprovides an example for evaluating the Whisper model on eight English speech recognition datasets in one script using\nstreaming mode. ## A Tour of Audio Datasets on The Hub\nThis Section serves as a reference guide for the most popular speech recognition, speech translation and audio\nclassification datasets on the Hugging Face Hub. We can apply everything that we've covered for the GigaSpeech dataset\nto any of the datasets on the Hub. All we have to do is switch the dataset identifier in the `load\\_dataset` function.""]","Streaming mode offers several advantages over downloading a dataset:

1. **Easy Experimentation**: You can experiment on a handful of samples to check that your script works without having to download the entire dataset.

2. **No Disk Space Usage**: Since the data is not downloaded to disk, you do not need to worry about disk space, making it easier to access large datasets.

3. **On-the-Fly Processing**: Data is loaded and processed on the fly, which can be beneficial for real-time applications or when working with very large datasets.

4. **Multi-Dataset Evaluation**: Streaming mode allows you to easily evaluate systems over multiple datasets in one go, providing a better metric for the generalization abilities of a speech recognition system.

5. **No Caching Required**: Unlike downloaded datasets, which save both raw and processed data locally, streaming mode does not cache data, meaning you can always access the most up-to-date version of the dataset.

To enable streaming mode, you simply set `streaming=True` when loading your dataset.",1. Disk space: No local disk space is required as data is loaded to memory one-by-one. 2. Download and processing time: Allows immediate use of the dataset without waiting for full download. 3. Easy experimentation: Enables testing on a few samples without downloading the entire dataset.
What teacher model is used with frugalscore_tiny_bert-base_mover-score?,"[""The inputs it takes are:\n`predictions`: a list of strings representing the predictions to score. `references`: a list of string representing the references for each prediction. Its optional arguments are:\n`batch\\_size`: the batch size for predictions (default value is `32`). `max\\_length`: the maximum sequence length (default value is `128`). `device`: either \""gpu\"" or \""cpu\"" (default value is `None`). ```python\n>>> results = frugalscore.compute(predictions=['hello there', 'huggingface'], references=['hello world', 'hugging face'], batch\\_size=16, max\\_length=64, device=\""gpu\"")\n```\n## Output values\nThe output of FrugalScore is a dictionary with the list of scores for each prediction-reference pair:\n```python\n{'scores': [0.6307541, 0.6449357]}\n```\n### Values from popular papers\nThe [original FrugalScore paper](https://arxiv.org/abs/2110.08559) reported that FrugalScore-Tiny retains 97.7/94.7% of the original performance compared to [BertScore](https://huggingface.co/metrics/bertscore) while running 54 times faster and having 84 times less parameters. ## Examples\nMaximal values (exact match between `references` and `predictions`):\n```python\n>>> frugalscore = evaluate.load(\""frugalscore\"")\n>>> results = frugalscore.compute(predictions=['hello world'], references=['hello world'])\n>>> print(results)\n{'scores': [0.9891098]}\n```\nPartial values:\n```python\n>>> frugalscore = evaluate.load(\""frugalscore\"")\n>>> results = frugalscore.compute(predictions=['hello world'], references=['hugging face'])\n>>> print(results)\n{'scores': [0.42482382]}\n```\n## Limitations and bias\nFrugalScore is based on [BertScore](https://huggingface.co/metrics/bertscore) and [MoverScore](https://arxiv.org/abs/1909.02622), and the models used are based on the original models used for these scores. The full list of available models for FrugalScore is:\n| FrugalScore | Student | Teacher | Method |\n|----------------------------------------------------|-------------|----------------|------------|\n| [moussaKam/frugalscore\\_tiny\\_bert-base\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_tiny\\_bert-base\\_bert-score) | BERT-tiny | BERT-Base | BERTScore |\n| [moussaKam/frugalscore\\_small\\_bert-base\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_small\\_bert-base\\_bert-score) | BERT-small | BERT-Base | BERTScore |\n| [moussaKam/frugalscore\\_medium\\_bert-base\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_medium\\_bert-base\\_bert-score) | BERT-medium | BERT-Base | BERTScore |\n| [moussaKam/frugalscore\\_tiny\\_roberta\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_tiny\\_roberta\\_bert-score) | BERT-tiny | RoBERTa-Large | BERTScore |\n| [moussaKam/frugalscore\\_small\\_roberta\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_small\\_roberta\\_bert-score) | BERT-small | RoBERTa-Large | BERTScore |\n| [moussaKam/frugalscore\\_medium\\_roberta\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_medium\\_roberta\\_bert-score) | BERT-medium | RoBERTa-Large | BERTScore |\n| [moussaKam/frugalscore\\_tiny\\_deberta\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_tiny\\_deberta\\_bert-score) | BERT-tiny | DeBERTa-XLarge | BERTScore |\n| [moussaKam/frugalscore\\_small\\_deberta\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_small\\_deberta\\_bert-score) | BERT-small | DeBERTa-XLarge | BERTScore |\n| [moussaKam/frugalscore\\_medium\\_deberta\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_medium\\_deberta\\_bert-score) | BERT-medium | DeBERTa-XLarge | BERTScore |\n| [moussaKam/frugalscore\\_tiny\\_bert-base\\_mover-score](https://huggingface.co/moussaKam/frugalscore\\_tiny\\_bert-base\\_mover-score) | BERT-tiny | BERT-Base | MoverScore |\n| [moussaKam/frugalscore\\_small\\_bert-base\\_mover-score](https://huggingface.co/moussaKam/frugalscore\\_small\\_bert-base\\_mover-score) | BERT-small | BERT-Base | MoverScore |\n| [moussaKam/frugalscore\\_medium\\_bert-base\\_mover-score](https://huggingface.co/moussaKam/frugalscore\\_medium\\_bert-base\\_mover-score) | BERT-medium | BERT-Base | MoverScore |\nDepending on the size of the model picked, the loading time will vary: the `tiny` models will load very quickly, whereas the `medium` ones can take several minutes, depending on your Internet connection. ## Citation\n```bibtex\n@article{eddine2021frugalscore,\ntitle={FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metrics for Automatic Text Generation},\nauthor={Eddine, Moussa Kamal and Shang, Guokan and Tixier, Antoine J-P and Vazirgiannis, Michalis},\njournal={arXiv preprint arXiv:2110.08559},\nyear={2021}\n}\n```\n## Further References\n- [Original FrugalScore code](https://github.com/moussaKam/FrugalScore)\n- [FrugalScore paper](https://arxiv.org/abs/2110.08559)"", ""Metric Card for BERT Score\n## Metric description\nBERTScore is an automatic evaluation metric for text generation that computes a similarity score for each token in the candidate sentence with each token in the reference sentence. It leverages the pre-trained contextual embeddings from [BERT](https://huggingface.co/bert-base-uncased) models and matches words in candidate and reference sentences by cosine similarity. Moreover, BERTScore computes precision, recall, and F1 measure, which can be useful for evaluating different language generation tasks. ## How to use\nBERTScore takes 3 mandatory arguments : `predictions` (a list of string of candidate sentences), `references` (a list of strings or list of list of strings of reference sentences) and either `lang` (a string of two letters indicating the language of the sentences, in [ISO 639-1 format](https://en.wikipedia.org/wiki/List\\_of\\_ISO\\_639-1\\_codes)) or `model\\_type` (a string specififying which model to use, according to the BERT specification). The default behavior of the metric is to use the suggested model for the target language when one is specified, otherwise to use the `model\\_type` indicated. ```python\nfrom datasets import load\\_metric\nbertscore = load\\_metric(\""bertscore\"")\npredictions = [\""hello there\"", \""general kenobi\""]\nreferences = [\""hello there\"", \""general kenobi\""]\nresults = bertscore.compute(predictions=predictions, references=references, lang=\""en\"")\n```\nBERTScore also accepts multiple optional arguments:\n`num\\_layers` (int): The layer of representation to use."", ""the contradiction score. This is sometimes called \""multi-class multi-label\"" classification. - `--temperature` (default: `1.0`): The temperature applied to the softmax of the teacher model predictions. A\nhigher temperature results in a student with smoother (lower confidence) predictions than the teacher while a value\n`<1` resultings in a higher-confidence, peaked distribution. The default `1.0` is equivalent to no smoothing. - `--teacher\\_batch\\_size` (default: `32`): The batch size used for generating a single set of teacher predictions. Does not affect training. Use `--per\\_device\\_train\\_batch\\_size` to change the training batch size. Any of the arguments in the ü§ó Trainer's\n[`TrainingArguments`](https://huggingface.co/transformers/main\\_classes/trainer.html?#trainingarguments) can also be\nmodified, such as `--learning\\_rate`, `--fp16`, `--no\\_cuda`, `--warmup\\_steps`, etc. Run `python distill\\_classifier.py\n-h` for a full list of available arguments or consult the [Trainer\ndocumentation](https://huggingface.co/transformers/main\\_classes/trainer.html#trainingarguments).""]",The teacher model used with `frugalscore_tiny_bert-base_mover-score` is BERT-Base.,BERT-Base
What class is used to compile the model to run on IPUs?,"[""Here, we show the transformed images. Our dataset is now ready to be used. ## Preparing the model\n\nTo train a model on the IPU we need to import it from Hugging Face Hub and define a trainer using the IPUTrainer class. The IPUTrainer class takes the same arguments as the original [Transformer Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) and works in tandem with the IPUConfig object which specifies the behaviour for compilation and execution on the IPU. Now we import the ViT model from Hugging Face. To use this model on the IPU we need to load the IPU configuration, `IPUConfig`, which gives control to all the parameters specific to Graphcore IPUs (existing IPU configs [can be found here](https://huggingface.co/Graphcore)). We are going to use `Graphcore/vit-base-ipu`. Let's set our training hyperparameters using `IPUTrainingArguments`. This subclasses the Hugging Face `TrainingArguments` class, adding parameters specific to the IPU and its execution characteristics. ## Implementing a custom performance metric for evaluation\n\nThe performance of multi-label classification models can be assessed using the area under the ROC (receiver operating characteristic) curve (AUC\\_ROC)."", ""The Stable Diffusion model is distributed under the CreateML OpenRail-M license. It's an open license that claims no rights on the outputs you generate and prohibits you from deliberately producing illegal or harmful content. The [model card](https://huggingface.co/CompVis/stable-diffusion-v1-4) provides more details, so take a moment to read them and consider carefully whether you accept the license. If you do, you need to be a registered user in the Hub and use an access token for the code to work. You have two options to provide your access token:\n- Use the `huggingface-cli login` command-line tool in your terminal and paste your token when prompted. It will be saved in a file in your computer. - Or use `notebook\\_login()` in a notebook, which does the same thing. The following cell will present a login interface unless you've already authenticated before in this computer. You'll need to paste your access token. ``` python\nif not (Path.home()/'.huggingface'/'token').exists(): notebook\\_login()\n```\nTPU devices support `bfloat16`, an efficient half-float type. We'll use it for our tests, but you can also use `float32` to use full precision instead. ``` python\ndtype = jnp.bfloat16\n```\nFlax is a functional framework, so models are stateless and parameters are stored outside them. Loading the pre-trained Flax pipeline will return both the pipeline itself and the model weights (or parameters). We are using a `bf16` version of the weights, which leads to type warnings that you can safely ignore. ``` python\npipeline, params = FlaxStableDiffusionPipeline.from\\_pretrained(\n\""CompVis/stable-diffusion-v1-4\"",\nrevision=\""bf16\"",\ndtype=dtype,\n)\n```\n## Inference\nSince TPUs usually have 8 devices working in parallel, we'll replicate our prompt as many times as devices we have."", ""\""distilbert-base-uncased-finetuned-sst-2-english\"",\n... export=True,\n... provider=\""ROCMExecutionProvider\"",\n... )\n```\nThe model can then be used with the common ü§ó Transformers API for inference and evaluation, such as [pipelines](https://huggingface.co/docs/optimum/onnxruntime/usage\\_guides/pipelines). When using Transformers pipeline, note that the `device` argument should be set to perform pre- and post-processing on GPU, following the example below:\n```python\n>>> from optimum.pipelines import pipeline\n>>> from transformers import AutoTokenizer\n>>> tokenizer = AutoTokenizer.from\\_pretrained(\""distilbert-base-uncased-finetuned-sst-2-english\"")\n>>> pipe = pipeline(task=\""text-classification\"", model=ort\\_model, tokenizer=tokenizer, device=\""cuda:0\"")\n>>> result = pipe(\""Both the music and visual were astounding, not to mention the actors performance.\"")\n>>> print(result) # doctest: +IGNORE\\_RESULT\n# printing: [{'label': 'POSITIVE', 'score': 0.9997727274894c714}]\n```\nAdditionally, you can pass the session option `log\\_severity\\_level = 0` (verbose), to check whether all nodes are indeed placed on the ROCM execution provider or not:\n```python\n>>> import onnxruntime\n>>> session\\_options = onnxruntime.SessionOptions()\n>>> session\\_options.log\\_severity\\_level = 0\n>>> ort\\_model = ORTModelForSequenceClassification.from\\_pretrained(\n...""]",The class used to compile the model to run on IPUs is the `IPUTrainer` class.,IPUTrainer
What is the default activation function used in DistilBertConfig?,"[""All attributes are customizable, creating space for experimentation. For example, you can customize a default model to:\n- Try a different activation function with the `activation` parameter. - Use a higher dropout ratio for the attention probabilities with the `attention\\_dropout` parameter. ```py\n>>> my\\_config = DistilBertConfig(activation=\""relu\"", attention\\_dropout=0.4)\n>>> print(my\\_config)\nDistilBertConfig {\n\""activation\"": \""relu\"",\n\""attention\\_dropout\"": 0.4,\n\""dim\"": 768,\n\""dropout\"": 0.1,\n\""hidden\\_dim\"": 3072,\n\""initializer\\_range\"": 0.02,\n\""max\\_position\\_embeddings\"": 512,\n\""model\\_type\"": \""distilbert\"",\n\""n\\_heads\"": 12,\n\""n\\_layers\"": 6,\n\""pad\\_token\\_id\"": 0,\n\""qa\\_dropout\"": 0.1,\n\""seq\\_classif\\_dropout\"": 0.2,\n\""sinusoidal\\_pos\\_embds\"": false,\n\""transformers\\_version\"": \""4.16.2\"",\n\""vocab\\_size\"": 30522\n}\n```\nPretrained model attributes can be modified in the [`~PretrainedConfig.from\\_pretrained`] function:\n```py\n>>> my\\_config = DistilBertConfig.from\\_pretrained(\""distilbert-base-uncased\"", activation=\""relu\"", attention\\_dropout=0.4)\n```\nOnce you are satisfied with your model configuration, you can save it with [`~PretrainedConfig.save\\_pretrained`]. Your configuration file is stored as a JSON file in the specified save directory:\n```py\n>>> my\\_config.save\\_pretrained(save\\_directory=\""./your\\_model\\_save\\_path\"")\n```\nTo reuse the configuration file, load it with [`~PretrainedConfig.from\\_pretrained`]:\n```py\n>>> my\\_config = DistilBertConfig.from\\_pretrained(\""./your\\_model\\_save\\_path/config.json\"")\n```\nYou can also save your configuration file as a dictionary or even just the difference between your custom configuration attributes and the default configuration attributes!"", ""We'll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task). {/if}\nIn the code sample above we didn't use `BertConfig`, and instead loaded a pretrained model via the `bert-base-cased` identifier. This is a model checkpoint that was trained by the authors of BERT themselves; you can find more details about it in its [model card](https://huggingface.co/bert-base-cased). This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results. The weights have been downloaded and cached (so future calls to the `from\\_pretrained()` method won't re-download them) in the cache folder, which defaults to \\*~/.cache/huggingface/transformers\\*. You can customize your cache folder by setting the `HF\\_HOME` environment variable."", ""## Combining DistilBERT and Flash Attention 2\nFirst, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature. ```bash\npip install -U flash-attn --no-build-isolation\n```\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of flash-attn repository. Make also sure to load your model in half-precision (e.g. `torch.float16`)\nTo load and run a model using Flash Attention 2, refer to the snippet below:\n```python\n>>> import torch\n>>> from transformers import AutoTokenizer, AutoModel\n>>> device = \""cuda\"" # the device to load the model onto\n>>> tokenizer = AutoTokenizer.from\\_pretrained('distilbert-base-uncased')\n>>> model = AutoModel.from\\_pretrained(\""distilbert-base-uncased\"", torch\\_dtype=torch.float16, attn\\_implementation=\""flash\\_attention\\_2\"")\n>>> text = \""Replace me by any text you'd like.\""\n>>> encoded\\_input = tokenizer(text, return\\_tensors='pt').to(device)\n>>> model.to(device)\n>>> output = model(\\*\\*encoded\\_input)\n```\n## DistilBertConfig\n[[autodoc]] DistilBertConfig\n## DistilBertTokenizer\n[[autodoc]] DistilBertTokenizer\n## DistilBertTokenizerFast\n[[autodoc]] DistilBertTokenizerFast\n\n## DistilBertModel\n[[autodoc]] DistilBertModel\n- forward\n## DistilBertForMaskedLM\n[[autodoc]] DistilBertForMaskedLM\n- forward\n## DistilBertForSequenceClassification\n[[autodoc]] DistilBertForSequenceClassification\n- forward\n## DistilBertForMultipleChoice\n[[autodoc]] DistilBertForMultipleChoice\n- forward\n## DistilBertForTokenClassification\n[[autodoc]] DistilBertForTokenClassification\n- forward\n## DistilBertForQuestionAnswering\n[[autodoc]] DistilBertForQuestionAnswering\n- forward\n\n## TFDistilBertModel\n[[autodoc]] TFDistilBertModel\n- call\n## TFDistilBertForMaskedLM\n[[autodoc]] TFDistilBertForMaskedLM\n- call\n## TFDistilBertForSequenceClassification\n[[autodoc]] TFDistilBertForSequenceClassification\n- call\n## TFDistilBertForMultipleChoice\n[[autodoc]] TFDistilBertForMultipleChoice\n- call\n## TFDistilBertForTokenClassification\n[[autodoc]] TFDistilBertForTokenClassification\n- call\n## TFDistilBertForQuestionAnswering\n[[autodoc]] TFDistilBertForQuestionAnswering\n- call\n\n## FlaxDistilBertModel\n[[autodoc]] FlaxDistilBertModel\n- \\_\\_call\\_\\_\n## FlaxDistilBertForMaskedLM\n[[autodoc]] FlaxDistilBertForMaskedLM\n- \\_\\_call\\_\\_\n## FlaxDistilBertForSequenceClassification\n[[autodoc]] FlaxDistilBertForSequenceClassification\n- \\_\\_call\\_\\_\n## FlaxDistilBertForMultipleChoice\n[[autodoc]] FlaxDistilBertForMultipleChoice\n- \\_\\_call\\_\\_\n## FlaxDistilBertForTokenClassification\n[[autodoc]] FlaxDistilBertForTokenClassification\n- \\_\\_call\\_\\_\n## FlaxDistilBertForQuestionAnswering\n[[autodoc]] FlaxDistilBertForQuestionAnswering\n- \\_\\_call\\_\\_""]","The default activation function used in `DistilBertConfig` is ""gelu"".",gelu
What is the top 1 accuracy of mobilenetv3_large_100 on ImageNet?,"[""`mobilenetv2\\_100`. You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the [timm feature extraction examples](../feature\\_extraction), just change the name of the model you want to use. ## How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). ```py\n>>> model = timm.create\\_model('mobilenetv2\\_100', pretrained=True, num\\_classes=NUM\\_FINETUNE\\_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset. ## How do I train this model? You can follow the [timm recipe scripts](../scripts) for training a new model afresh. ## Citation\n```BibTeX\n@article{DBLP:journals/corr/abs-1801-04381,\nauthor = {Mark Sandler and\nAndrew G. Howard and\nMenglong Zhu and\nAndrey Zhmoginov and\nLiang{-}Chieh Chen},\ntitle = {Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification,\nDetection and Segmentation},\njournal = {CoRR},\nvolume = {abs/1801.04381},\nyear = {2018},\nurl = {http://arxiv.org/abs/1801.04381},\narchivePrefix = {arXiv},\neprint = {1801.04381},\ntimestamp = {Tue, 12 Jan 2021 15:30:06 +0100},\nbiburl = {https://dblp.org/rec/journals/corr/abs-1801-04381.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```"", ""MobileNet v2\n\\*\\*MobileNetV2\\*\\* is a convolutional neural network architecture that seeks to perform well on mobile devices. It is based on an [inverted residual structure](https://paperswithcode.com/method/inverted-residual-block) where the residual connections are between the bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. As a whole, the architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters, followed by 19 residual bottleneck layers. ## How do I use this model on an image? To load a pretrained model:\n```py\n>>> import timm\n>>> model = timm.create\\_model('mobilenetv2\\_100', pretrained=True)\n>>> model.eval()\n```\nTo load and preprocess the image:\n```py\n>>> import urllib\n>>> from PIL import Image\n>>> from timm.data import resolve\\_data\\_config\n>>> from timm.data.transforms\\_factory import create\\_transform\n>>> config = resolve\\_data\\_config({}, model=model)\n>>> transform = create\\_transform(\\*\\*config)\n>>> url, filename = (\""https://github.com/pytorch/hub/raw/master/images/dog.jpg\"", \""dog.jpg\"")\n>>> urllib.request.urlretrieve(url, filename)\n>>> img = Image.open(filename).convert('RGB')\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\nTo get the model predictions:\n```py\n>>> import torch\n>>> with torch.no\\_grad():\n... out = model(tensor)\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\n>>> print(probabilities.shape)\n>>> # prints: torch.Size([1000])\n```\nTo get the top-5 predictions class names:\n```py\n>>> # Get imagenet class mappings\n>>> url, filename = (\""https://raw.githubusercontent.com/pytorch/hub/master/imagenet\\_classes.txt\"", \""imagenet\\_classes.txt\"")\n>>> urllib.request.urlretrieve(url, filename)\n>>> with open(\""imagenet\\_classes.txt\"", \""r\"") as f:\n... categories = [s.strip() for s in f.readlines()]\n>>> # Print top categories per image\n>>> top5\\_prob, top5\\_catid = torch.topk(probabilities, 5)\n>>> for i in range(top5\\_prob.size(0)):\n... print(categories[top5\\_catid[i]], top5\\_prob[i].item())\n>>> # prints class names and probabilities like:\n>>> # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\nReplace the model name with the variant you want to use, e.g."", ""Validation and Benchmark Results\nThis folder contains validation and benchmark results for the models in this collection. Validation scores are currently only run for models with pretrained weights and ImageNet-1k heads, benchmark numbers are run for all. ## Datasets\nThere are currently results for the ImageNet validation set and 5 additional test / label sets. The test set results include rank and top-1/top-5 differences from clean validation. For the \""Real Labels\"", ImageNetV2, and Sketch test sets, the differences were calculated against the full 1000 class ImageNet-1k validation set. For both the Adversarial and Rendition sets, the differences were calculated against 'clean' runs on the ImageNet-1k validation set with the same 200 classes used in each test set respectively. ### ImageNet Validation - [`results-imagenet.csv`](results-imagenet.csv)\nThe standard 50,000 image ImageNet-1k validation set. Model selection during training utilizes this validation set, so it is not a true test set. Question: Does anyone have the official ImageNet-1k test set classification labels now that challenges are done? \\* Source: http://image-net.org/challenges/LSVRC/2012/index\n\\* Paper: \""ImageNet Large Scale Visual Recognition Challenge\"" - https://arxiv.org/abs/1409.0575\n### ImageNet-\""Real Labels\"" - [`results-imagenet-real.csv`](results-imagenet-real.csv)\nThe usual ImageNet-1k validation set with a fresh new set of labels intended to improve on mistakes in the original annotation process. \\* Source: https://github.com/google-research/reassessed-imagenet\n\\* Paper: \""Are we done with ImageNet?\"" - https://arxiv.org/abs/2006.07159\n### ImageNetV2 Matched Frequency - [`results-imagenetv2-matched-frequency.csv`](results-imagenetv2-matched-frequency.csv)\nAn ImageNet test set of 10,000 images sampled from new images roughly 10 years after the original. Care was taken to replicate the original ImageNet curation/sampling process. \\* Source: https://github.com/modestyachts/ImageNetV2\n\\* Paper: \""Do ImageNet Classifiers Generalize to ImageNet?\"" - https://arxiv.org/abs/1902.10811\n### ImageNet-Sketch - [`results-sketch.csv`](results-sketch.csv)\n50,000 non photographic (or photos of such) images (sketches, doodles, mostly monochromatic) covering all 1000 ImageNet classes. \\* Source: https://github.com/HaohanWang/ImageNet-Sketch\n\\* Paper: \""Learning Robust Global Representations by Penalizing Local Predictive Power\"" - https://arxiv.org/abs/1905.13549\n### ImageNet-Adversarial - [`results-imagenet-a.csv`](results-imagenet-a.csv)\nA collection of 7500 images covering 200 of the 1000 ImageNet classes. Images are naturally occurring adversarial examples that confuse typical ImageNet classifiers. This is a challenging dataset, your typical ResNet-50 will score 0% top-1. For clean validation with same 200 classes, see [`results-imagenet-a-clean.csv`](results-imagenet-a-clean.csv)\n\\* Source: https://github.com/hendrycks/natural-adv-examples\n\\* Paper: \""Natural Adversarial Examples\"" - https://arxiv.org/abs/1907.07174\n### ImageNet-Rendition - [`results-imagenet-r.csv`](results-imagenet-r.csv)\nRenditions of 200 ImageNet classes resulting in 30,000 images for testing robustness. For clean validation with same 200 classes, see [`results-imagenet-r-clean.csv`](results-imagenet-r-clean.csv)\n\\* Source: https://github.com/hendrycks/imagenet-r\n\\* Paper: \""The Many Faces of Robustness\"" - https://arxiv.org/abs/2006.16241\n### TODO\n\\* Explore adding a reduced version of ImageNet-C (Corruptions) and ImageNet-P (Perturbations) from https://github.com/hendrycks/robustness.""]",The top-1 accuracy of `mobilenetv3_large_100` on ImageNet is 75.2%.,75.77%
What version of Python is required to contribute to ü§ó Transformers?,"[""The `main` version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn't been rolled out yet. However, this means the `main` version may not always be stable. We strive to keep the `main` version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an [Issue](https://github.com/huggingface/transformers/issues) so we can fix it even sooner! Check if ü§ó Transformers has been properly installed by running the following command:\n```bash\npython -c \""from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\""\n```\n## Editable install\nYou will need an editable install if you'd like to:\n\\* Use the `main` version of the source code. \\* Contribute to ü§ó Transformers and need to test changes in the code. Clone the repository and install ü§ó Transformers with the following commands:\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e . ```\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`. You must keep the `transformers` folder if you want to keep using the library. Now you can easily update your clone to the latest version of ü§ó Transformers with the following command:\n```bash\ncd ~/transformers/\ngit pull\n```\nYour Python environment will find the `main` version of ü§ó Transformers on the next run. ## Install with conda\nInstall from the conda channel `huggingface`:\n```bash\nconda install -c huggingface transformers\n```\n## Cache setup\nPretrained models are downloaded and locally cached at: `~/.cache/huggingface/hub`."", ""This was an\nincredible effort led by many different people. To maximize the GPU during training, several solutions were explored\nand in the end, [Megatron-Deepspeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed) was chosen to train the end model. This meant that the code as-is wasn't necessarily compatible with the `transformers`\nlibrary. # Porting to transformers\nBecause of the original training code, we set out to do something which we regularly\ndo: port an existing model to `transformers`. The goal was to extract from the\ntraining code the relevant parts and implement it within `transformers`. This effort was tackled by [Younes](/ybelkada). This is by no means a small effort as it took almost a month and [200 commits](https://github.com/huggingface/transformers/pull/17474/commits) to get there. There are several things to note that will come back later:\nWe needed to have smaller models [bigscience/bigscience-small-testing](https://huggingface.co/bigscience/bigscience-small-testing) and [bigscience/bloom-560m](https://huggingface.co/bigscience/bloom-560m)."", ""- Ask for help when you're stuck! The ü§ó Transformers team is here to help, and we've probably found solutions to the same\nproblems you're facing. Here's an overview of the steps needed to add a TensorFlow model architecture:\n1. Select the model you wish to convert\n2. Prepare transformers dev environment\n3. (Optional) Understand theoretical aspects and the existing implementation\n4. Implement the model architecture\n5. Implement model tests\n6. Submit the pull request\n7. (Optional) Build demos and share with the world\n### 1.-3. Prepare your model contribution\n\\*\\*1. Select the model you wish to convert\\*\\*\nLet's start off with the basics: the first thing you need to know is the architecture you want to convert. If you\ndon't have your eyes set on a specific architecture, asking the ü§ó Transformers team for suggestions is a great way to\nmaximize your impact - we will guide you towards the most prominent architectures that are missing on the TensorFlow\nside. If the specific model you want to use with TensorFlow already has a TensorFlow architecture implementation in\nü§ó Transformers but is lacking weights, feel free to jump straight into the\n[weight conversion section](#adding-tensorflow-weights-to-hub)\nof this page. For simplicity, the remainder of this guide assumes you've decided to contribute with the TensorFlow version of\n\\*BrandNewBert\\* (the same example as in the [guide](add\\_new\\_model) to add a new model from scratch). Before starting the work on a TensorFlow model architecture, double-check that there is no ongoing effort to do so. You can search for `BrandNewBert` on the\n[pull request GitHub page](https://github.com/huggingface/transformers/pulls?q=is%3Apr) to confirm that there is no\nTensorFlow-related pull request. \\*\\*2. Prepare transformers dev environment\\*\\*\nHaving selected the model architecture, open a draft PR to signal your intention to work on it. Follow the\ninstructions below to set up your environment and open a draft PR.""]","To contribute to ü§ó Transformers, you should use Python 3.7 or higher.",Python 3.8 or above
What is the default value of HF_INFERENCE_ENDPOINT?,"[""Some of my requests failed because I deliberately omitted the `Content-Type` header. `![](assets/109_inference_endpoints/endpoints06.png)`\nFor additional details, I can check the full logs in the `Logs` tab. ```\n5c7fbb4485cd8w7 2022-10-10T08:19:04.915Z 2022-10-10 08:19:04,915 | INFO | POST / | Duration: 142.76 ms\n5c7fbb4485cd8w7 2022-10-10T08:19:05.860Z 2022-10-10 08:19:05,860 | INFO | POST / | Duration: 148.06 ms\n5c7fbb4485cd8w7 2022-10-10T09:21:39.251Z 2022-10-10 09:21:39,250 | ERROR | Content type \""None\"" not supported. Supported content types are: application/json, text/csv, text/plain, image/png, image/jpeg, image/jpg, image/tiff, image/bmp, image/gif, image/webp, image/x-image, audio/x-flac, audio/flac, audio/mpeg, audio/wave, audio/wav, audio/x-wav, audio/ogg, audio/x-audio, audio/webm, audio/webm;codecs=opus\n5c7fbb4485cd8w7 2022-10-10T09:21:44.114Z 2022-10-10 09:21:44,114 | ERROR | Content type \""None\"" not supported. Supported content types are: application/json, text/csv, text/plain, image/png, image/jpeg, image/jpg, image/tiff, image/bmp, image/gif, image/webp, image/x-image, audio/x-flac, audio/flac, audio/mpeg, audio/wave, audio/wav, audio/x-wav, audio/ogg, audio/x-audio, audio/webm, audio/webm;codecs=opus\n```\nNow, let's increase our security level and deploy a private endpoint. ### Deploying a Private Inference Endpoint\nRepeating the steps above, I select `Private` this time."", ""Hugging Face Inference Endpoints documentation\n## Setup\n```bash\npip install hf-doc-builder==0.4.0 watchdog --upgrade\n```\n## Local Development\n```bash\ndoc-builder preview endpoints docs/source/ --not\\_python\\_module\n```\n## Build Docs\n```bash\ndoc-builder build endpoints docs/source/ --build\\_dir build/ --not\\_python\\_module\n```\n## Add assets/Images\nAdding images/assets is only possible through `https://` links meaning you need to use `https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/` prefix. example\n```bash\n![Login](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/0_login.png)\n```\n## Generate API Reference\n1. Copy openapi spec from `https://api.endpoints.huggingface.cloud/api-doc/openapi.json`\n2. create markdown `widdershins --environment env.json openapi.json -o myOutput.md`\n3. copy into `api\\_reference.mdx`"", ""Create an Endpoint\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. ## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\n![select repository](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png)\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know. ![select region](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png)\n## 3. Define the [Security Level](security) for the Endpoint:\n![define security](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png)\n## 4. Create your Endpoint by clicking \\*\\*Create Endpoint\\*\\*. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account. ![create endpoint](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png)\n## 5.""]",The default value of `HF_INFERENCE_ENDPOINT` is not explicitly mentioned in the provided context. Please refer to the relevant documentation or resources for specific details regarding default values.,"""https://api-inference.huggingface.com"""
What is the name of the ResNet variant that uses squeeze-and-excitation blocks?,"[""This is generally known as \""ResNet v1.5\"". ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision. The abstract from the paper is the following:\n\\*Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\\*\nThe figure below illustrates the architecture of ResNet."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# ControlNet\nThe ControlNet model was introduced in [Adding Conditional Control to Text-to-Image Diffusion Models](https://huggingface.co/papers/2302.05543) by Lvmin Zhang, Anyi Rao, Maneesh Agrawala. It provides a greater degree of control over text-to-image generation by conditioning the model on additional inputs such as edge maps, depth maps, segmentation maps, and keypoints for pose detection. The abstract from the paper is:\n\\*We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \""zero convolutions\"" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning."", ""DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.5% box AP in COCO object detection,\n1.3% mask AP in COCO instance segmentation, and 1.1% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.2 PQ)\nand ADE20K (48.5 PQ), and instance segmentation model on Cityscapes (44.5 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.2 mIoU),\nand ranks second on Cityscapes (84.5 mIoU) (no extra data). \\*\n![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dilated-neighborhood-attention-pattern.jpg)\n Neighborhood Attention with different dilation values. Taken from the [original paper](https://arxiv.org/abs/2209.15001). This model was contributed by [Ali Hassani](https://huggingface.co/alihassanijr). The original code can be found [here](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer). ## Usage tips\nDiNAT can be used as a \\*backbone\\*. When `output\\_hidden\\_states = True`,\nit will output both `hidden\\_states` and `reshaped\\_hidden\\_states`. The `reshaped\\_hidden\\_states` have a shape of `(batch, num\\_channels, height, width)` rather than `(batch\\_size, height, width, num\\_channels)`. Notes:\n- DiNAT depends on [NATTEN](https://github.com/SHI-Labs/NATTEN/)'s implementation of Neighborhood Attention and Dilated Neighborhood Attention.""]",The ResNet variant that uses squeeze-and-excitation blocks is called ResNet-SE.,SE ResNet
What Python package needs to be installed for the Gradio demo?,"[""Building your first demo[[building-your-first-demo]]\nLet's start by installing Gradio! Since it is a Python package, simply run:\n`$ pip install gradio `\nYou can run Gradio anywhere, be it from your favourite Python IDE, to Jupyter notebooks or even in Google Colab ü§Ø! So install Gradio wherever you run Python! Let's get started with a simple ‚ÄúHello World‚Äù example to get familiar with the Gradio syntax:\n```py\nimport gradio as gr\ndef greet(name):\nreturn \""Hello \"" + name\ndemo = gr.Interface(fn=greet, inputs=\""text\"", outputs=\""text\"")\ndemo.launch()\n```\nLet's walk through the code above:\n- First, we define a function called `greet()`. In this case, it is a simple function that adds \""Hello\"" before your name, but it can be \\*any\\* Python function in general. For example, in machine learning applications, this function would \\*call a model to make a prediction\\* on an input and return the output. - Then, we create a Gradio `Interface` with three arguments, `fn`, `inputs`, and `outputs`. These arguments define the prediction function, as well as the \\_type\\_ of input and output components we would like. In our case, both components are simple text boxes. - We then call the `launch()` method on the `Interface` that we created. If you run this code, the interface below will appear automatically within a Jupyter/Colab notebook, or pop in a browser on \\*\\*[http://localhost:7860](http://localhost:7860/)\\*\\* if running from a script. Try using this GUI right now with your own name or some other input! You'll notice that in this GUI, Gradio automatically inferred the name of the input parameter (`name`)\nand applied it as a label on top of the textbox."", ""Gradio Demo: fake\\_diffusion\\_with\\_gif\n```\n!pip install -q gradio\n```\n```\n# Downloading files from the demo repo\nimport os\n!wget -q https://github.com/gradio-app/gradio/raw/main/demo/fake\\_diffusion\\_with\\_gif/image.gif\n```\n```\nimport gradio as gr\nimport numpy as np\nimport time\nimport os\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\ndef create\\_gif(images):\npil\\_images = []\nfor image in images:\nif isinstance(image, str):\nresponse = requests.get(image)\nimage = Image.open(BytesIO(response.content))\nelse:\nimage = Image.fromarray((image \\* 255).astype(np.uint8))\npil\\_images.append(image)\nfp\\_out = os.path.join(os.path.abspath(''), \""image.gif\"")\nimg = pil\\_images.pop(0)\nimg.save(fp=fp\\_out, format='GIF', append\\_images=pil\\_images,\nsave\\_all=True, duration=400, loop=0)\nreturn fp\\_out\ndef fake\\_diffusion(steps):\nimages = []\nfor \\_ in range(steps):\ntime.sleep(1)\nimage = np.random.random((600, 600, 3))\nimages.append(image)\nyield image, gr.Image(visible=False)\ntime.sleep(1)\nimage = \""https://gradio-builds.s3.amazonaws.com/diffusion\\_image/cute\\_dog.jpg\""\nimages.append(image)\ngif\\_path = create\\_gif(images)\nyield image, gr.Image(value=gif\\_path, visible=True)\ndemo = gr.Interface(fake\\_diffusion,\ninputs=gr.Slider(1, 10, 3),\noutputs=[\""image\"", gr.Image(label=\""All Images\"", visible=False)])\ndemo.queue()\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: main\\_note\n```\n!pip install -q gradio scipy numpy matplotlib\n```\n```\n# Downloading files from the demo repo\nimport os\nos.mkdir('audio')\n!wget -q -O audio/cantina.wav https://github.com/gradio-app/gradio/raw/main/demo/main\\_note/audio/cantina.wav\n!wget -q -O audio/recording1.wav https://github.com/gradio-app/gradio/raw/main/demo/main\\_note/audio/recording1.wav\n```\n```\nfrom math import log2, pow\nimport os\nimport numpy as np\nfrom scipy.fftpack import fft\nimport gradio as gr\nA4 = 440\nC0 = A4 \\* pow(2, -4.75)\nname = [\""C\"", \""C#\"", \""D\"", \""D#\"", \""E\"", \""F\"", \""F#\"", \""G\"", \""G#\"", \""A\"", \""A#\"", \""B\""]\ndef get\\_pitch(freq):\nh = round(12 \\* log2(freq / C0))\nn = h % 12\nreturn name[n]\ndef main\\_note(audio):\nrate, y = audio\nif len(y.shape) == 2:\ny = y.T[0]\nN = len(y)\nT = 1.0 / rate\nyf = fft(y)\nyf2 = 2.0 / N \\* np.abs(yf[0 : N // 2])\nxf = np.linspace(0.0, 1.0 / (2.0 \\* T), N // 2)\nvolume\\_per\\_pitch = {}\ntotal\\_volume = np.sum(yf2)\nfor freq, volume in zip(xf, yf2):\nif freq == 0:\ncontinue\npitch = get\\_pitch(freq)\nif pitch not in volume\\_per\\_pitch:\nvolume\\_per\\_pitch[pitch] = 0\nvolume\\_per\\_pitch[pitch] += 1.0 \\* volume / total\\_volume\nvolume\\_per\\_pitch = {k: float(v) for k, v in volume\\_per\\_pitch.items()}\nreturn volume\\_per\\_pitch\ndemo = gr.Interface(\nmain\\_note,\ngr.Audio(sources=[\""microphone\""]),\ngr.Label(num\\_top\\_classes=4),\nexamples=[\n[os.path.join(os.path.abspath(''),\""audio/recording1.wav\"")],\n[os.path.join(os.path.abspath(''),\""audio/cantina.wav\"")],\n],\n)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```""]","You need to install the Gradio package by running the following command:

```
$ pip install gradio
```",gradio
What does ZeRO-powered data parallelism store on each GPU?,"[""Varuna further attempts to improve the\nschedule by using simulations to discover the most efficient scheduling. OSLO has pipeline parallelism implementation based on the Transformers without `nn.Sequential` conversion. ## Tensor Parallelism\nIn Tensor Parallelism, each GPU processes a slice of a tensor and only aggregates the full tensor for operations requiring it. To describe this method, this section of the guide relies on the concepts and diagrams from the [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)\npaper: [Efficient Large-Scale Language Model Training on GPU Clusters](https://arxiv.org/abs/2104.04473). The main building block of any transformer is a fully connected `nn.Linear` followed by a nonlinear activation `GeLU`. The dot dot-product part of it, following the Megatron's paper notation, can be written as `Y = GeLU(XA)`, where `X` is\nan input vector, `Y` is the output vector, and `A` is the weight matrix. If we look at the computation in matrix form, you can see how the matrix multiplication can be split between multiple GPUs:\n\n![Parallel GEMM](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-tp-parallel_gemm.png)\n\nIf we split the weight matrix `A` column-wise across `N` GPUs and perform matrix multiplications `XA\\_1` through `XA\\_n` in parallel,\nthen we will end up with `N` output vectors `Y\\_1, Y\\_2, ..., Y\\_n` which can be fed into `GeLU` independently:\n\n![Independent GeLU](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-tp-independent-gelu.png)\n\nUsing this principle, we can update a multi-layer perceptron of arbitrary depth, without the need for any synchronization\nbetween GPUs until the very end, where we need to reconstruct the output vector from shards. The Megatron-LM paper authors\nprovide a helpful illustration for that:\n\n![Parallel shard processing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-tp-parallel_shard_processing.png)\n\nParallelizing the multi-headed attention layers is even simpler, since they are already inherently parallel, due to having\nmultiple independent heads! ![Parallel self-attention](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-tp-parallel_self_attention.png)\n\nSpecial considerations: TP requires very fast network, and therefore it's not advisable to do TP across more than one node. Practically, if a node has 4 GPUs, the highest TP degree is therefore 4. If you need a TP degree of 8, you need to use\nnodes that have at least 8 GPUs."", ""Why would you want to use DeepSpeed with just one GPU? 1. It has a ZeRO-offload feature which can delegate some computations and memory to the host's CPU and RAM, and thus\nleave more GPU resources for model's needs - e.g. larger batch size, or enabling a fitting of a very big model which\nnormally won't fit. 2. It provides a smart GPU memory management system, that minimizes memory fragmentation, which again allows you to fit\nbigger models and data batches. While we are going to discuss the configuration in details next, the key to getting a huge improvement on a single GPU\nwith DeepSpeed is to have at least the following configuration in the configuration file:\n```json\n{\n\""zero\\_optimization\"": {\n\""stage\"": 2,\n\""offload\\_optimizer\"": {\n\""device\"": \""cpu\"",\n\""pin\\_memory\"": true\n},\n\""allgather\\_partitions\"": true,\n\""allgather\\_bucket\\_size\"": 2e8,\n\""reduce\\_scatter\"": true,\n\""reduce\\_bucket\\_size\"": 2e8,\n\""overlap\\_comm\"": true,\n\""contiguous\\_gradients\"": true\n}\n}\n```\nwhich enables optimizer offload and some other important features."", ""Additionally DeepSpeed is currently developing a related product called Deepspeed-Inference which has no relationship\nto the ZeRO technology, but instead uses tensor parallelism to scale models that can't fit onto a single GPU. This is a\nwork in progress and we will provide the integration once that product is complete. ### Memory Requirements\nSince Deepspeed ZeRO can offload memory to CPU (and NVMe) the framework provides utils that allow one to tell how much CPU and GPU memory will be needed depending on the number of GPUs being used. Let's estimate how much memory is needed to finetune \""bigscience/T0\\_3B\"" on a single GPU:\n```bash\n$ python -c 'from transformers import AutoModel; \\\nfrom deepspeed.runtime.zero.stage3 import estimate\\_zero3\\_model\\_states\\_mem\\_needs\\_all\\_live; \\\nmodel = AutoModel.from\\_pretrained(\""bigscience/T0\\_3B\""); \\\nestimate\\_zero3\\_model\\_states\\_mem\\_needs\\_all\\_live(model, num\\_gpus\\_per\\_node=1, num\\_nodes=1)'\n[...]\nEstimated memory needed for params, optim states and gradients for a:\nHW: Setup with 1 node, 1 GPU per node.""]","ZeRO-powered data parallelism stores the following on each GPU:

1. **Model Parameters**: Each GPU holds a shard of the model parameters.
2. **Optimizer States**: Each GPU maintains a shard of the optimizer states.
3. **Gradients**: Each GPU keeps a shard of the gradients.

This approach allows for efficient memory usage and enables training larger models by distributing the memory requirements across multiple GPUs.","A slice of the model parameters, gradients, and optimizer states."
What is the license URL for TheBloke/Llama2-22B-Daydreamer-v3-GGUF?,"[""There are 2 ways of defining a license tag. Either the license is one of the officially-supported licenses. In this case, simply defining it as a string in the metadata is enough:\n```yaml\n# Example from https://huggingface.co/codellama/CodeLlama-34b-hf\n---\nlicense: llama2\n---\n```\nOtherwise, the license is considered as `other`. In that case, we can set a custom name and a URL to the said license. Here is an example of how it looks like:\n```yaml\n# Example from https://huggingface.co/coqui/XTTS-v1\n---\nlicense: other\nlicense\\_name: coqui-public-model-license\nlicense\\_link: https://coqui.ai/cpml\n---\n```\nThis challenge aims to improve the completeness of this metadata on the Hub, which will ultimately benefit all users. ## How to contribute? How to do it in practice? That's simple! We have listed models below that have no `license` in their metadata but still contain a LICENSE file in the repo. Those models require an update in their metadata to describe which license is attached. We only selected the models that have at least 1 download and 1 like to filter out unrelevant models. For each model, the workflow looks like this:\n1. Choose a model in the list below. We suggest focusing on the most downloaded or liked models first. 2. \\*\\*Check that the model doesn't already have a PR to update the license\\*\\*. Maybe another contributor already checked it!"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Open-Llama\nThis model is in maintenance mode only, we don't accept any new PRs changing its code. If you run into any issues running this model, please reinstall the last version that supported this model: v4.31.0. You can do so by running the following command: `pip install -U transformers==4.31.0`. This model differs from the [OpenLLaMA models](https://huggingface.co/models?search=openllama) on the Hugging Face Hub, which primarily use the [LLaMA](llama) architecture. ## Overview\nThe Open-Llama model was proposed in the open source Open-Llama project by community developer s-JoL. The model is mainly based on LLaMA with some modifications, incorporating memory-efficient attention from Xformers, stable embedding from Bloom, and shared input-output embedding from PaLM. And the model is pre-trained on both Chinese and English, which gives it better performance on Chinese language tasks."", ""--\ntitle: \""Open-Source Text Generation & LLM Ecosystem at Hugging Face\""\nthumbnail: /blog/assets/os\\_llms/thumbnail.png\nauthors:\n- user: merve\n---\n# Open-Source Text Generation & LLM Ecosystem at Hugging Face\n[Updated on July 24, 2023: Added Llama 2.]\nText generation and conversational technologies have been around for ages. Earlier challenges in working with these technologies were controlling both the coherence and diversity of the text through inference parameters and discriminative biases. More coherent outputs were less creative and closer to the original training data and sounded less human. Recent developments overcame these challenges, and user-friendly UIs enabled everyone to try these models out. Services like ChatGPT have recently put the spotlight on powerful models like GPT-4 and caused an explosion of open-source alternatives like Llama to go mainstream. We think these technologies will be around for a long time and become more and more integrated into everyday products. This post is divided into the following sections:\n1. [Brief background on text generation](#brief-background-on-text-generation)\n2. [Licensing](#licensing)\n3. [Tools in the Hugging Face Ecosystem for LLM Serving](#tools-in-the-hugging-face-ecosystem-for-llm-serving)\n4. [Parameter Efficient Fine Tuning (PEFT)](#parameter-efficient-fine-tuning-peft)\n## Brief Background on Text Generation\nText generation models are essentially trained with the objective of completing an incomplete text or generating text from scratch as a response to a given instruction or question.""]",The license URL for TheBloke/Llama2-22B-Daydreamer-v3-GGUF is not provided in the context. Please check the model's repository for the specific license details.,https://huggingface.co/TheBloke/Llama2-22B-Daydreamer-v3-GGUF/blob/main/LICENSE.txt
What is the recommended scheduler for the IP-Adapter face model?,"[""Using Adapter Transformers at Hugging Face\n`adapter-transformers` is a library that extends ü§ó `transformers` by allowing to integrate, train and use Adapters and other efficient fine-tuning methods. The library is fully compatible with ü§ó `transformers`. Adapters are small learnt layers inserted within each layer of a pre-trained model. You can learn more about this in the [original paper](https://arxiv.org/abs/2007.07779). ## Exploring adapter-transformers in the Hub\nYou can find over a hundred `adapter-transformer` models by filtering at the left of the [models page](https://huggingface.co/models?library=adapter-transformers&sort=downloads). Some adapter models can be found in the Adapter Hub [repository](https://github.com/adapter-hub/hub). Models from both sources are then aggregated in the [AdapterHub](https://adapterhub.ml/explore/). ## Using existing models\nFor a full guide on loading pre-trained adapters, we recommend checking out the [official guide](https://docs.adapterhub.ml/loading.html). As a brief summary, once you load a model with the usual `\\*Model` classes from ü§ó`transformers`, you can use the `load\\_adapter` method to load and activate the Adapter (remember `adapter-transformers` extends ü§ó`transformers`.). ```py\nfrom transformers import AutoModelWithHeads\nmodel = AutoModelWithHeads.from\\_pretrained(\""bert-base-uncased\"")\nadapter\\_name = model.load\\_adapter(\""AdapterHub/bert-base-uncased-pf-imdb\"", source=\""hf\"")\nmodel.active\\_adapters = adapter\\_name\n```\nYou can also use `list\\_adapters` to find all Adapter Models programmatically\n```py\nfrom transformers import list\\_adapters\n# source can be \""ah\"" (AdapterHub), \""hf\"" (hf.co) or None (for both, default)\nadapter\\_infos = list\\_adapters(source=\""hf\"", model\\_name=\""bert-base-uncased\"")\n```\nIf you want to see how to load a specific model, you can click `Use in Adapter Transformers` and you will be given a working snippet that you can load it! ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-adapter_transformers_snippet1.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-adapter_transformers-snippet1-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-adapter_transformers_snippet2.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-adapter_transformers-snippet2-dark.png)\n\n## Sharing your models\nFor a full guide on sharing models with `adapter-transformers`, we recommend checking out the [official guide](https://docs.adapterhub.ml/huggingface\\_hub.html#uploading-to-the-hub). You can share your Adapter by using the `push\\_adapter\\_to\\_hub` method from a model that already contains an adapter. ```py\nmodel.push\\_adapter\\_to\\_hub(\n\""my-awesome-adapter\"",\n\""awesome\\_adapter\"",\nadapterhub\\_tag=\""sentiment/imdb\"",\ndatasets\\_tag=\""imdb\""\n)\n```\nThis command creates a repository with an automatically generated model card and all necessary metadata."", ""--\ntitle: \""Efficient Controllable Generation for SDXL with T2I-Adapters\""\nthumbnail: /blog/assets/t2i-sdxl-adapters/thumbnail.png\nauthors:\n- user: Adapter\nguest: true\n- user: valhalla\n- user: sayakpaul\n- user: Xintao\nguest: true\n- user: hysts\n---\n# Efficient Controllable Generation for SDXL with T2I-Adapters\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/t2i-adapters-sdxl/hf_tencent.png)\n\n[T2I-Adapter](https://huggingface.co/papers/2302.08453) is an efficient plug-and-play model that provides extra guidance to pre-trained text-to-image models while freezing the original large text-to-image models. T2I-Adapter aligns internal knowledge in T2I models with external control signals. We can train various adapters according to different conditions and achieve rich control and editing effects. As a contemporaneous work, [ControlNet](https://hf.co/papers/2302.05543) has a similar function and is widely used. However, it can be \\*\\*computationally expensive\\*\\* to run. This is because, during each denoising step of the reverse diffusion process, both the ControlNet and UNet need to be run. In addition, ControlNet emphasizes the importance of copying the UNet encoder as a control model, resulting in a larger parameter number. Thus, the generation is bottlenecked by the size of the ControlNet (the larger, the slower the process becomes). T2I-Adapters provide a competitive advantage to ControlNets in this matter. T2I-Adapters are smaller in size, and unlike ControlNets, T2I-Adapters are run just once for the entire course of the denoising process. | \\*\\*Model Type\\*\\* | \\*\\*Model Parameters\\*\\* | \\*\\*Storage (fp16)\\*\\* |\n| --- | --- | --- |\n| [ControlNet-SDXL](https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0) | 1251 M | 2.5 GB |\n| [ControlLoRA](https://huggingface.co/stabilityai/control-lora) (with rank 128) | 197.78 M (84.19% reduction) | 396 MB (84.53% reduction) |\n| [T2I-Adapter-SDXL](https://huggingface.co/TencentARC/t2i-adapter-canny-sdxl-1.0) | 79 M (\\*\\*\\_93.69% reduction\\_\\*\\*) | 158 MB (\\*\\*\\_94% reduction\\_\\*\\*) |\nOver the past few weeks, the Diffusers team and the T2I-Adapter authors have been collaborating to bring the support of T2I-Adapters for [Stable Diffusion XL (SDXL)](https://huggingface.co/papers/2307.01952) in [`diffusers`](https://github.com/huggingface/diffusers). In this blog post, we share our findings from training T2I-Adapters on SDXL from scratch, some appealing results, and, of course, the T2I-Adapter checkpoints on various conditionings (sketch, canny, lineart, depth, and openpose)! ![Collage of the results](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/t2i-adapters-sdxl/results\\_collage.png)\nCompared to previous versions of T2I-Adapter (SD-1.4/1.5), [T2I-Adapter-SDXL](https://github.com/TencentARC/T2I-Adapter) still uses the original recipe, driving 2.6B SDXL with a 79M Adapter! T2I-Adapter-SDXL maintains powerful control capabilities while inheriting the high-quality generation of SDXL! ## Training T2I-Adapter-SDXL with `diffusers`\nWe built our training script on [this official example](https://github.com/huggingface/diffusers/blob/main/examples/t2i\\_adapter/README\\_sdxl.md) provided by `diffusers`. Most of the T2I-Adapter models we mention in this blog post were trained on 3M high-resolution image-text pairs from LAION-Aesthetics V2 with the following settings:\n- Training steps: 20000-35000\n- Batch size: Data parallel with a single GPU batch size of 16 for a total batch size of 128."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# IP-Adapter\n[IP-Adapter](https://hf.co/papers/2308.06721) is a lightweight adapter that enables prompting a diffusion model with an image. This method decouples the cross-attention layers of the image and text features. The image features are generated from an image encoder. Files generated from IP-Adapter are only ~100MBs. Learn how to load an IP-Adapter checkpoint and image in the [IP-Adapter](../../using-diffusers/loading\\_adapters#ip-adapter) loading guide. ## IPAdapterMixin\n[[autodoc]] loaders.ip\\_adapter.IPAdapterMixin""]",The information regarding the recommended scheduler for the IP-Adapter face model is not provided in the context. Please refer to the official documentation or resources related to IP-Adapter for specific details on the recommended scheduler.,DDIMScheduler and EulerDiscreteScheduler
What is one advantage of using a generator of lists of texts in `train_new_from_iterator()`?,"[""Training from memory\nIn the [Quicktour](quicktour), we saw how to build and train a\ntokenizer using text files, but we can actually use any Python Iterator. In this section we'll see a few different ways of training our\ntokenizer. For all the examples listed below, we'll use the same [`~tokenizers.Tokenizer`] and\n[`~tokenizers.trainers.Trainer`], built as\nfollowing:\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_tutorial\\_train\\_from\\_iterators.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START init\\_tokenizer\\_trainer\"",\n\""end-before\"": \""END init\\_tokenizer\\_trainer\"",\n\""dedent\"": 8}\nThis tokenizer is based on the [`~tokenizers.models.Unigram`] model. It\ntakes care of normalizing the input using the NFKC Unicode normalization\nmethod, and uses a [`~tokenizers.pre\\_tokenizers.ByteLevel`] pre-tokenizer with the corresponding decoder. For more information on the components used here, you can check\n[here](components). ## The most basic way\nAs you probably guessed already, the easiest way to train our tokenizer\nis by using a `List`{.interpreted-text role=\""obj\""}:\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_tutorial\\_train\\_from\\_iterators.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START train\\_basic\"",\n\""end-before\"": \""END train\\_basic\"",\n\""dedent\"": 8}\nEasy, right?"", ""Here is how it can be done with code, with one loop to concatenate all the texts and another one to chunk it. Notice how it reduces the number of samples in our dataset here, there must have been quite a few short entries! Once this is done, the masking is the easy part. There is a data collator designed specifically for this in the Transformers library. You can use it directly in the Trainer, or when converting your datasets to tensorflow datasets before doing Keras.fit, with the to\\_tf\\_dataset method."", ""You can use anything working as an iterator here, be it a\n`List`{.interpreted-text role=\""obj\""}, `Tuple`{.interpreted-text\nrole=\""obj\""}, or a `np.Array`{.interpreted-text role=\""obj\""}. Anything\nworks as long as it provides strings. ## Using the ü§ó Datasets library\nAn awesome way to access one of the many datasets that exist out there\nis by using the ü§ó Datasets library. For more information about it, you\nshould check [the official documentation\nhere](https://huggingface.co/docs/datasets/). Let's start by loading our dataset:\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_tutorial\\_train\\_from\\_iterators.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START load\\_dataset\"",\n\""end-before\"": \""END load\\_dataset\"",\n\""dedent\"": 8}\nThe next step is to build an iterator over this dataset. The easiest way\nto do this is probably by using a generator:\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_tutorial\\_train\\_from\\_iterators.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START def\\_batch\\_iterator\"",\n\""end-before\"": \""END def\\_batch\\_iterator\"",\n\""dedent\"": 8}\nAs you can see here, for improved efficiency we can actually provide a\nbatch of examples used to train, instead of iterating over them one by\none. By doing so, we can expect performances very similar to those we\ngot while training directly from files. With our iterator ready, we just need to launch the training. In order\nto improve the look of our progress bars, we can specify the total\nlength of the dataset:\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_tutorial\\_train\\_from\\_iterators.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START train\\_datasets\"",\n\""end-before\"": \""END train\\_datasets\"",\n\""dedent\"": 8}\nAnd that's it! ## Using gzip files\nSince gzip files in Python can be used as iterators, it is extremely\nsimple to train on such files:\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_tutorial\\_train\\_from\\_iterators.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START single\\_gzip\"",\n\""end-before\"": \""END single\\_gzip\"",\n\""dedent\"": 8}\nNow if we wanted to train from multiple gzip files, it wouldn't be much\nharder:\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_tutorial\\_train\\_from\\_iterators.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START multi\\_gzip\"",\n\""end-before\"": \""END multi\\_gzip\"",\n\""dedent\"": 8}\nAnd voil√†!""]","One advantage of using a generator of lists of texts in `train_new_from_iterator()` is that it allows for improved efficiency by providing a batch of examples used for training, rather than iterating over them one by one. This can lead to performance that is very similar to training directly from files.",You will avoid loading the whole dataset into memory at once.
What does the stride argument control in the tokenizer?,"[""he tokenization pipeline involves several steps that convert raw text into numbers. In this video, we will see what happens during the pre-tokenization step. The pre-tokenization operation is the operation performed after the normalization of the text and before the application of the tokenization algorithm. This step consists in applying rules that do not need to be learned to perform a first division of the text. Let's look at how several tokenizers pre\\_tokenize this example. The gpt 2 pretokenization divides the text on spaces and some punctuation - but the apostrophe is not a division criterion for example. We also notice that spaces have been replaced by a capital G with a dot above. Albert's pre-tokenization divides the text at the level of spaces, adds a space at the beginning of the sentence and replaces spaces with a special underscore. Finally, Bert's pre-tokenization divides the text at the level of punctuation and spaces. Unlike the previous tokenizers, spaces are not transformed and integrated to the tokens produced with this pre-tokenizer. Through these 3 examples, we could observe the two main types of operations brought by the pre-tokenization: some changes on the text and the division of the string into tokens that can be associated to words. Finally, the \""backend\\_tokenizer\"" of the fast tokenizers also allows to test the pre-tokenization operation very easily thanks to its \""pre\\_tokenize\\_str\"" method. We notice that the output of this operation is composed of both tokens and offsets which allow to link the token to its position in the text given in input of the method."", ""et's take a look at subword-based tokenization. Understanding why subword-based tokenization is interesting requires understanding the flaws of word-based and character-based tokenization. If you haven't seen the first videos on word-based and character-based tokenization, we recommend you check them out before looking at this video. Subword-tokenization lies in between character-based and word-based tokenization algorithms. The idea is to find a middle ground between very large vocabularies, large quantity of out-of-vocabulary tokens, loss of meaning across very similar words, for word-based tokenizers, and very long sequences, less meaningful individual tokens for character-based tokenizers. These algorithms rely on the following principle: frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. An example is the word dog: we would like to have our tokenizer to have a single ID for the word dog, rather than splitting it into characters: d, o, and g. However, when encountering the word dogs, we would like our tokenizer to understand that at the root, this is still the word dog, with an added s while slightly changes the meaning while keeping the original idea. Another example is a complex word like tokenization, which can be split into meaningful subwords. The root of the word is token, and ization completes the root to give it a slightly different meaning. It makes sense to split the word into two: token, as the root of the word (labeled as the \""start\"" of the word). ization as additional information (labeled as a \""completion\"" of the word)."", ""n this video we will see together what is the purpose of training a tokenizer, what are the key steps to follow and what is the easiest way to do it. You will ask yourself the question \""Should I train a new tokenizer?\"" when you plan to train a new model from scratch. A trained tokenizer would not be suitable for your corpus if your corpus is in a different language, uses new characters such as accents or upper cased letters, has a specific vocabulary, for example medical or legal, or uses a different style, a language from another century for instance. For example, if I take the tokenizer trained for the bert-base-uncased model and ignore its normalization step then we can see that the tokenization operation on the English sentence \""here is a sentence adapted to our tokenizer\"" produces a rather satisfactory list of tokens in the sense that this sentence of 8 words is tokenized into 9 tokens. On the other hand if we use this same tokenizer on a sentence in Bengali, we see that either a word is divided into many sub tokens or that the tokenizer does not know one of the unicode characters and returns only an unknown token. The fact that a \""common\"" word is split into many tokens can be problematic because language models can only handle a sequence of tokens of limited length. A tokenizer that excessively splits your initial text may even impact the performance of your model. Unknown tokens are also problematic because the model will not be able to extract any information from the \""unknown\"" part of the text. In this other example, we can see that the tokenizer replaces words containing characters with accents and capital letters with unknown tokens. Finally, if we use again this tokenizer to tokenize medical vocabulary we see again that a single word is divided into many sub tokens: 4 for \""paracetamol\"" and \""pharyngitis\"".""]","The stride argument in the tokenizer controls the overlap between the input sequences when tokenizing long texts. It allows you to specify how much of the previous sequence should be included in the next tokenization step. This is particularly useful when dealing with long texts that exceed the maximum token limit, as it helps to ensure that important context is not lost between segments. By adjusting the stride, you can manage the trade-off between context preservation and the number of tokens generated.",The stride argument controls the number of overlapping tokens.
What is the role of machine learning in predictive healthcare networks?,"[""However, companies can create the required foundation for successful machine learning projects with a thorough review of accurate, comprehensive, and constant training data. #### \\*\\*4. Where do you see ML having the biggest impact in the next 5-10 years?\\*\\*\nIn the next 5-10 years, ML will have the biggest impact on transforming the healthcare sector. \\*\\*Networked hospitals and connected care:\\*\\*\nWith predictive care, command centers are all set to analyze clinical and location data to monitor supply and demand across healthcare networks in real-time. With ML, healthcare professionals will be able to spot high-risk patients more quickly and efficiently, thus removing bottlenecks in the system. You can check the spread of contractible diseases faster, take better measures to manage epidemics, identify at-risk patients more accurately, especially for genetic diseases, and more. \\*\\*Better staff and patient experiences:\\*\\*\nPredictive healthcare networks are expected to reduce wait times, improve staff workflows, and take on the ever-growing administrative burden. By learning from every patient, diagnosis, and procedure, ML is expected to create experiences that adapt to hospital staff as well as the patient. This improves health outcomes and reduces clinician shortages and burnout while enabling the system to be financially sustainable. ---\nü§ó Thank you for joining us in this second installment of ML Director Insights. Stay tuned for more insights from ML Directors in Finance, Healthcare and e-Commerce. Big thanks to Omar Rahman, Cao (Danica) Xiao, Raphael Cohen, and Martin Ostrovsky for their brilliant insights and participation in this piece. We look forward to watching each of your continued successes and will be cheering you on each step of the way. üéâ\nIf you or your team are interested in accelerating your ML roadmap with Hugging Face Experts please visit [hf.co/support](https://huggingface.co/support?utm\\_source=article&utm\\_medium=blog&utm\\_campaign=ml\\_director\\_insights\\_2) to learn more."", ""We're using machine learning, specifically natural language processing (NLP), to help identify which of these thousands of text messages that we see daily are ones that we can automatically handle. ### What challenges were you facing before the Expert Acceleration Program? Our rule-based system caught about 80% of our inbound text messages, but we wanted to do much better. We knew that a statistical machine learning approach would be the only way to improve our parsing. When we looked around for what tools we could leverage, we found the language models on Hugging Face would be a great place to start. Even though Larry and I have backgrounds in machine learning and NLP, we were worried that we weren't formulating our problem perfectly, using the best model or neural network architecture for our particular use case and training data. ### How did you leverage the Expert Acceleration Program? The Hugging Face team really helped us in all aspects of implementing our NLP solution for this particular problem."", ""He is an active researcher and is currently interested in various information extraction problems and domain adaptation techniques in NLP. He has a track record of formulating and applying machine learning to various use cases. He actively participates in the program committee of different top tier conference venues in machine learning. \\*\\*Fun Fact:\\*\\* Debanjan played cycle polo at the world's oldest polo club (the Calcutta Polo Club) when he was a kid. \\*\\*Moody's Analytics:\\*\\* Provides financial intelligence and analytical tools supporting our clients‚Äô growth, efficiency and risk management objectives. #### \\*\\*1. How has ML made a positive impact on finance?\\*\\*\nMachine learning (ML) has made a significant positive impact in the finance industry in many ways. For example, it has helped in combating financial crimes and identifying fraudulent transactions. Machine learning has been a crucial tool in applications such as Know Your Customer (KYC) screening and Anti Money Laundering (AML). With an increase in AML fines by financial institutions worldwide, ever changing realm of sanctions, and greater complexity in money laundering, banks are increasing their investments in KYC and AML technologies, many of which are powered by ML.""]","Machine learning (ML) plays a crucial role in predictive healthcare networks by enabling healthcare professionals to analyze clinical and location data in real-time. This allows for the quick identification of high-risk patients, which helps to remove bottlenecks in the healthcare system. ML facilitates faster tracking of contractible diseases, better management of epidemics, and more accurate identification of at-risk patients, particularly for genetic diseases. Additionally, predictive healthcare networks powered by ML are expected to reduce wait times, improve staff workflows, and alleviate the administrative burden, ultimately enhancing health outcomes and supporting the financial sustainability of healthcare systems.","Machine learning helps spot high-risk patients more quickly and efficiently, checks the spread of contractible diseases faster, manages epidemics better, identifies at-risk patients more accurately, and creates experiences that adapt to both hospital staff and patients."
What is the size of the Parquet file for the train split of the rotten_tomatoes dataset?,"[""You can even add a prefix or suffix to `train` in the file name (like `my\\_train\\_file\\_00001.csv` for example). For convenience, you can also place your data files into different directories. In this case, the split name is inferred from the directory name. ```\nmy\\_dataset\\_repository/\n‚îú‚îÄ‚îÄ README.md\n‚îî‚îÄ‚îÄ data/\n‚îú‚îÄ‚îÄ train/\n‚îÇ ‚îú‚îÄ‚îÄ shard\\_0.csv\n‚îÇ ‚îú‚îÄ‚îÄ shard\\_1.csv\n‚îÇ ‚îú‚îÄ‚îÄ shard\\_2.csv\n‚îÇ ‚îî‚îÄ‚îÄ shard\\_3.csv\n‚îî‚îÄ‚îÄ test/\n‚îú‚îÄ‚îÄ shard\\_0.csv\n‚îî‚îÄ‚îÄ shard\\_1.csv\n```\n### Custom split name\nIf your dataset splits have custom names that aren't `train`, `test`, or `validation`, then you can name your data files like `data/-xxxxx-of-xxxxx.csv`. Here is an example with three splits, `train`, `test`, and `random`:\n```\nmy\\_dataset\\_repository/\n‚îú‚îÄ‚îÄ README.md\n‚îî‚îÄ‚îÄ data/\n‚îú‚îÄ‚îÄ train-00000-of-00003.csv\n‚îú‚îÄ‚îÄ train-00001-of-00003.csv\n‚îú‚îÄ‚îÄ train-00002-of-00003.csv\n‚îú‚îÄ‚îÄ test-00000-of-00001.csv\n‚îú‚îÄ‚îÄ random-00000-of-00003.csv\n‚îú‚îÄ‚îÄ random-00001-of-00003.csv\n‚îî‚îÄ‚îÄ random-00002-of-00003.csv\n```"", ""The final training corpus has a size of 3 GB, which is still small ‚Äì for your model, you will get better results the more data you can get to pretrain on. ## 2. Train a tokenizer\nWe choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let‚Äôs arbitrarily pick its size to be 52,000. We recommend training a byte-level BPE (rather than let‚Äôs say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `` tokens!). ```python\n#! pip install tokenizers\nfrom pathlib import Path\nfrom tokenizers import ByteLevelBPETokenizer\npaths = [str(x) for x in Path(\""./eo\\_data/\"").glob(\""\\*\\*/\\*.txt\"")]\n# Initialize a tokenizer\ntokenizer = ByteLevelBPETokenizer()\n# Customize training\ntokenizer.train(files=paths, vocab\\_size=52\\_000, min\\_frequency=2, special\\_tokens=[\n\""~~\"",\n\""\"",\n\""~~\"",\n\""\"",\n\""\"",\n])\n# Save files to disk\ntokenizer.save\\_model(\"".\"", \""esperberto\"")\n```\nAnd here‚Äôs a slightly accelerated capture of the output:\n![tokenizers](assets/01\\_how-to-train/tokenizers-fast.gif)\nOn our dataset, training took about ~5 minutes."", ""### Download and extract the data. It consists of two compressed folders: one with images, and the other‚Äîwith associated image captions. Note that the compressed images folder is 13GB in size. ```bash\nwget http://images.cocodataset.org/annotations/annotations\\_trainval2014.zip\nwget http://images.cocodataset.org/zips/train2014.zip\nunzip annotations\\_trainval2014.zip\nunzip train2014.zip\nmkdir coco\\_dataset\nmv train2014 coco\\_dataset/\nmv annotations coco\\_dataset/\n```\n### Prepare dataset files and split the dataset. ```python\nimport json\nimport collections\nimages\\_dir = \""coco\\_dataset/train2014\""\nannotation\\_file = \""coco\\_dataset/annotations/captions\\_train2014.json\""\nwith open(annotation\\_file, \""r\"") as f:\nannotations = json.load(f)[\""annotations\""]\nimage\\_path\\_to\\_caption = collections.defaultdict(list)\nfor element in annotations:\ncaption = f\""{element['caption'].lower().rstrip('.')}\""\nimage\\_path = images\\_dir + \""/COCO\\_train2014\\_\"" + \""%012d.jpg\"" % (element[\""image\\_id\""])\nimage\\_path\\_to\\_caption[image\\_path].append(caption)\nlines = []\nfor image\\_path, captions in image\\_path\\_to\\_caption.items():\nlines.append(json.dumps({\""image\\_path\"": image\\_path, \""captions\"": captions}))\ntrain\\_lines = lines[:-8000]\nvalid\\_line = lines[-8000:]\nwith open(\""coco\\_dataset/train\\_dataset.json\"", \""w\"") as f:\nf.write(\""\\n\"".join(train\\_lines))\nwith open(\""coco\\_dataset/valid\\_dataset.json\"", \""w\"") as f:\nf.write(\""\\n\"".join(valid\\_line))\n```\n> Note: The data loading and processing part of this script can still be improved for maximum performance.""]",I don't have specific information about the size of the Parquet file for the train split of the rotten_tomatoes dataset. You may want to check the dataset's documentation or repository for that information.,698845
What are the two types of guides for contributing to Gradio?,"[""Contributing to Gradio\n![GitHub issues by-label](https://img.shields.io/github/issues/gradio-app/gradio/good%20first%20issue?color=fe7c01&link=https%3A%2F%2Fgithub.com%2Fgradio-app%2Fgradio%2Fissues%3Fq%3Dis%253Aopen%2Bis%253Aissue%2Blabel%253A%2522good%2Bfirst%2Bissue%2522)\nMore than 200 awesome developers have contributed to the `gradio` library, and we'd be thrilled if you would like to be the next contributor! Prerequisites:\n- [Python 3.8+](https://www.python.org/downloads/)\n- [Node.js v16.14+](https://nodejs.dev/en/download/package-manager/) (only needed if you are making changes to the frontend)\n- [pnpm 8.1+](https://pnpm.io/8.x/installation) (only needed if you are making changes to the frontend)\n## üè° Setup for local development\nThere are a few ways to install and run Gradio. ### üõ†Ô∏è Install Gradio locally from `main`\n- Clone this repo\n- Navigate to the repo directory and (from the root directory) run\n```bash\nbash scripts/install\\_gradio.sh\n```\n- Build the front end\n```\nbash scripts/build\\_frontend.sh\n```\n- Install development requirements\n(Note that it is highly recommended to use a virtual environment running \\*\\*Python 3.9\\*\\* since the versions of Gradio's dependencies are pinned)\n```\nbash scripts/install\\_test\\_requirements.sh\n```\nIf you have a different Python version and conflicting packages during the installation, please first run:\n```\nbash scripts/create\\_test\\_requirements.sh\n```\n### üì¶ Using dev containers\nYou can alternatively use dev containers. This is supported on all platforms (macOS/Windows/Linux), as well as on GitHub Codespaces. Prerequisites:\n- An editor which supports dev containers, like VS Code\n- Docker support on the host computer:\n- macOS: [Docker Desktop 2.0+](https://www.docker.com/products/docker-desktop/)\n- Windows: [Docker Desktop 2.0+](https://www.docker.com/products/docker-desktop/)\n- Linux: [Docker CE/EE 18.06+](https://docs.docker.com/get-docker/) and [Docker Compose 1.21+](https://docs.docker.com/compose/install/)\n- If using VS Code, the [Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) extension\nSteps:\n- Clone repository\n- Open it in your editor\n- For VS Code, execute `Dev Containers: Reopen in container` command\nFor detailed instructions, please see the [Dev Containers tutorial](https://code.visualstudio.com/docs/devcontainers/tutorial). ## üß± Structure of the Repository\nIf you're a newcomer to Gradio, we recommend getting familiar with the overall structure of the repository so that you can focus on the part of the source code you'd like to contribute to. - `/gradio`: contains the Python source code for the library\n- `/gradio/interface.py`: contains the Python source code for the core `Interface` class\n- `/gradio/blocks.py`: contains the Python source code for the core `Blocks` class\n- `/gradio/components/`: the directory that contains the Python source code for all of the Gradio components. - `/js`: contains the HTML/JS/CSS source code for the library ([start here for frontend changes](/js/README.md))\n- `/js/\\_website`: contains the code for the Gradio website (www.gradio.app). See the README in the `/js/\\_website` folder for more details\n- `/test`: contains Python unit tests for the library\n- `/demo`: contains demos that are used in the documentation, you can find `Gradio` examples over here. ## üöÄ Run a Gradio app\nYou can get started by creating an `app.py` file in the root:\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Button()\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nthen run:\n```\ngradio app.py\n```\nThis will start the backend server in reload mode, which will watch for changes in the `gradio` folder and reload the app if changes are made."", ""- @MohamedAliRashad for making their first contribution to gradio! ## 3.15.0\n### New Features:\nGradio's newest plotting component `gr.LinePlot`! üìà\nWith this component you can easily create time series visualizations with customizable\nappearance for your demos and dashboards ... all without having to know an external plotting library. For an example of the api see below:\n```python\ngr.LinePlot(stocks,\nx=\""date\"",\ny=\""price\"",\ncolor=\""symbol\"",\ncolor_legend_position=\""bottom\"",\nwidth=600, height=400, title=\""Stock Prices\"")\n```\n![image](https://user-images.githubusercontent.com/41651716/208711646-81ae3745-149b-46a3-babd-0569aecdd409.png)\nBy [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2807](https://github.com/gradio-app/gradio/pull/2807)\n### Bug Fixes:\n- Fixed bug where the `examples_per_page` parameter of the `Examples` component was not passed to the internal `Dataset` component by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2861](https://github.com/gradio-app/gradio/pull/2861)\n- Fixes loading Spaces that have components with default values by [@abidlabs](https://github.com/abidlabs) in [PR 2855](https://github.com/gradio-app/gradio/pull/2855)\n- Fixes flagging when `allow_flagging=\""auto\""` in `gr.Interface()` by [@abidlabs](https://github.com/abidlabs) in [PR 2695](https://github.com/gradio-app/gradio/pull/2695)\n- Fixed bug where passing a non-list value to `gr.CheckboxGroup` would crash the entire app by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2866](https://github.com/gradio-app/gradio/pull/2866)\n### Documentation Changes:\n- Added a Guide on using BigQuery with Gradio's `DataFrame` and `ScatterPlot` component,\nby [@abidlabs](https://github.com/abidlabs) in [PR 2794](https://github.com/gradio-app/gradio/pull/2794)\n### Testing and Infrastructure Changes:\nNo changes to highlight."", ""How to add support for more languages\nWe would love to support more languages for Gradio üåé\nTo add your language, do the following steps:\n1. Create a new json file in this directory\n2. Name the file after the language code (Here's a list: http://4umi.com/web/html/languagecodes.php)\n3. Please provide clear and complete translations. Take a look at the [`en.json`](https://github.com/gradio-app/gradio/blob/master/js/app/public/lang/en.json) file for the corresponding English text. That's it!""]","The two types of guides for contributing to Gradio are:

1. **Setup for local development**: This guide provides instructions on how to install and run Gradio locally, including prerequisites and steps for installation.

2. **How to add support for more languages**: This guide outlines the steps to add support for additional languages in Gradio, including creating a new JSON file and providing translations.",Use cases and Feature explanation.
What method is called to compute gradients in the training pipeline?,"[""Finally, if we want our training to take a couple of minutes instead of a few hours, we will need to use a GPU. The first step is to get one, for instance by using a colab notebook. Then you need to actually send your model and training data on it by using a torch device. Double-check the following lines print a CUDA device for you! We can now put everything together! First we put our model in training mode (which will activate the training behavior for some layers like Dropout) then go through the number of epochs we picked and all the data in our training dataloader. Then we go through all the steps we have seen already: send the data to the GPU, compute the model outputs, and in particular the loss. Use the loss to compute gradients, then make a training step with the optimizer. Update the learning rate in our scheduler for the next iteration and zero the gradients of the optimizer. Once this is finished, we can evaluate our model very easily with a metric from the Datasets library. First we put our model in evaluation mode, then go through all the data in the evaluation data loader. As we have seen in the Trainer video, the model outputs logits and we need to apply the argmax function to convert them into predictions. The metric object then has an add\\_batch method we can use to send it those intermediate predictions."", ""Along with complete equivalence to regular distributed training and easy scalability, this method also has the benefit of built-in fault tolerance, which we illustrate below. Let's consider a couple of potential failure cases that we might encounter throughout a collaborative experiment. By far, the most frequent scenario is that one or several peers disconnect from the training procedure: they might have an unstable connection or simply want to use their GPUs for something else. In this case, we only suffer a minor setback of training: the contribution of these peers gets deducted from the currently accumulated batch size, but other participants will compensate for that with their gradients. Also, if more peers join, the target batch size will simply be reached faster, and our training procedure will naturally speed up. You can see a demonstration of this in the video:### Adaptive averaging\nNow that we have discussed the overall training procedure, there remains one more question: how do we actually aggregate the gradients of participants?"", ""For instance, for fp16 data type a multiple of 8 is recommended, unless\nit's an A100 GPU, in which case use multiples of 64. For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup. ## Gradient Accumulation\nThe \\*\\*gradient accumulation\\*\\* method aims to calculate gradients in smaller increments instead of computing them for the\nentire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward\nand backward passes through the model and accumulating the gradients during the process. Once a sufficient number of\ngradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\nbecomes possible to increase the \\*\\*effective batch size\\*\\* beyond the limitations imposed by the GPU's memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can\nslow down the training process. You can enable gradient accumulation by adding the `gradient\\_accumulation\\_steps` argument to [`TrainingArguments`]:\n```py\ntraining\\_args = TrainingArguments(per\\_device\\_train\\_batch\\_size=1, gradient\\_accumulation\\_steps=4, \\*\\*default\\_args)\n```\nIn the above example, your effective batch size becomes 4.""]","In the training pipeline, gradients are computed using the backward pass through the model. After calculating the loss, you use the loss to compute gradients, which is typically done by calling the `backward()` method on the loss tensor. This process allows the model to update its parameters based on the computed gradients during the optimization step.",backward()
Who created the Falcon language models?,"[""They are made available under the Apache 2.0 license. Falcon's architecture is modern and optimized for inference, with multi-query attention and support for efficient\nattention variants like `FlashAttention`. Both 'base' models trained only as causal language models as well as\n'instruct' models that have received further fine-tuning are available. Falcon models are (as of 2023) some of the largest and most powerful open-source language models,\nand consistently rank highly in the [OpenLLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open\\_llm\\_leaderboard). ## Converting custom checkpoints\nFalcon models were initially added to the Hugging Face Hub as custom code checkpoints. However, Falcon is now fully\nsupported in the Transformers library. If you fine-tuned a model from a custom code checkpoint, we recommend converting\nyour checkpoint to the new in-library format, as this should give significant improvements to stability and\nperformance, especially for generation, as well as removing the need to use `trust\\_remote\\_code=True`! You can convert custom code checkpoints to full Transformers checkpoints using the `convert\\_custom\\_code\\_checkpoint.py`\nscript located in the\n[Falcon model directory](https://github.com/huggingface/transformers/tree/main/src/transformers/models/falcon)\nof the Transformers library. To use this script, simply call it with\n`python convert\\_custom\\_code\\_checkpoint.py --checkpoint\\_dir my\\_model`. This will convert your checkpoint in-place, and\nyou can immediately load it from the directory afterwards with e.g. `from\\_pretrained()`. If your model hasn't been\nuploaded to the Hub, we recommend making a backup before attempting the conversion, just in case! ## FalconConfig\n[[autodoc]] FalconConfig\n- all\n## FalconModel\n[[autodoc]] FalconModel\n- forward\n## FalconForCausalLM\n[[autodoc]] FalconForCausalLM\n- forward\n## FalconForSequenceClassification\n[[autodoc]] FalconForSequenceClassification\n- forward\n## FalconForTokenClassification\n[[autodoc]] FalconForTokenClassification\n- forward\n## FalconForQuestionAnswering\n[[autodoc]] FalconForQuestionAnswering\n- forward"", ""The Falcon models, data, and training process were detailed in a technical report and a [later research paper](https://huggingface.co/papers/2311.16867). Where previous models were public about their data, from then on, following releases gave close to no information about what was used to train the models, and their efforts cannot be reproduced - however, they provide starting points for the community through the weights released. Early in the summer came the [X-Gen](https://huggingface.co/papers/2309.03450) [models](https://huggingface.co/Salesforce/xgen-7b-4k-base) from Salesforce, 7B parameters models trained on 1.5T tokens of \""natural language and code\"", in several steps, following a data scheduling system (not all data is introduced at the same time to the model). X-Gen was a bit over-shadowed by the much visible new [LLaMA-2](https://huggingface.co/papers/2307.09288) family from Meta, a range of [7 to 70B models](https://huggingface.co/meta-llama/Llama-2-7b) trained on 2T tokens \""from publicly available sources\"", with a permissive community license and an extensive process of finetuning from human-preferences (RLHF), so-called alignment procedure. A couple of months later, the first [model](https://huggingface.co/mistralai/Mistral-7B-v0.1) from the newly created startup Mistral, the so-called [Mistral-7B](https://huggingface.co/papers/2310.06825) was released, trained on an undisclosed number of tokens from data \""extracted from the open Web\"". The end of 2023 was busy with model releases with a second larger model from Mistral (Mixtral 8x7B), a first impressive [model](https://huggingface.co/Deci/DeciLM-7B) from Deci.AI called [DeciLM](https://deci.ai/blog/introducing-DeciLM-7B-the-fastest-and-most-accurate-7b-large-language-model-to-date) as well as a larger merge of models from upstage, [SOLAR](https://huggingface.co/upstage/SOLAR-10.7B-v1.0) also trained on undisclosed amount and sources of data."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Falcon\n## Overview\nFalcon is a class of causal decoder-only models built by [TII](https://www.tii.ae/). The largest Falcon checkpoints\nhave been trained on >=1T tokens of text, with a particular emphasis on the [RefinedWeb](https://arxiv.org/abs/2306.01116)\ncorpus.""]",The Falcon language models were created by TII (Technology Innovation Institute).,Technology Innovation Institute

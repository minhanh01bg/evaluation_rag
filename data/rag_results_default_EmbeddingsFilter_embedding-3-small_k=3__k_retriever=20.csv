user_input,retrieved_contexts,response,reference
"What architecture is the `tokenizers-linux-x64-musl` binary designed for?
","[""`tokenizers-linux-x64-musl`\nThis is the \\*\\*x86\\_64-unknown-linux-musl\\*\\* binary for `tokenizers`"", ""`tokenizers-linux-x64-gnu`\nThis is the \\*\\*x86\\_64-unknown-linux-gnu\\*\\* binary for `tokenizers`"", ""This is more robust than preparing the pattern yourself: it avoids pitfalls, such as token glueing, that are very hard to debug. To see how much CPU and GPU memory you need for this model or others, try [this calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage) which can help determine that value. The LLaMA tokenizer is a BPE model based on [sentencepiece](https://github.com/google/sentencepiece). One quirk of sentencepiece is that when decoding a sequence, if the first token is the start of the word (e.g. \""Banana\""), the tokenizer does not prepend the prefix space to the string. Code Llama has the same architecture as the `Llama2` models, refer to [Llama2's documentation page](llama2) for the API reference. Find Code Llama tokenizer reference below. ## CodeLlamaTokenizer\n[[autodoc]] CodeLlamaTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## CodeLlamaTokenizerFast\n[[autodoc]] CodeLlamaTokenizerFast\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- update\\_post\\_processor\n- save\\_vocabulary""]",The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.,x86_64-unknown-linux-musl
"What is the purpose of the BLIP-Diffusion model?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# BLIP-Diffusion\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. The abstract from the paper is:\n\\*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).\\*\nThe original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization. `BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## BlipDiffusionPipeline\n[[autodoc]] BlipDiffusionPipeline\n- all\n- \\_\\_call\\_\\_\n## BlipDiffusionControlNetPipeline\n[[autodoc]] BlipDiffusionControlNetPipeline\n- all\n- \\_\\_call\\_\\_"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# BLIP\n## Overview\nThe BLIP model was proposed in [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. BLIP is a model that is able to perform various multi-modal tasks including:\n- Visual Question Answering\n- Image-Text retrieval (Image-text matching)\n- Image Captioning\nThe abstract from the paper is the following:\n\\*Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks."", ""--\ntitle: Stable Diffusion with ðŸ§¨ Diffusers\nthumbnail: /blog/assets/98\\_stable\\_diffusion/thumbnail.png\nauthors:\n- user: valhalla\n- user: pcuenq\n- user: natolambert\n- user: patrickvonplaten\n---\n# Stable Diffusion with ðŸ§¨ Diffusers\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb)\n# \\*\\*Stable Diffusion\\*\\* ðŸŽ¨\n\\*...using ðŸ§¨ Diffusers\\*\nStable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). It is trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database. \\*LAION-5B\\* is the largest, freely accessible multi-modal dataset that currently exists. In this post, we want to show how to use Stable Diffusion with the [ðŸ§¨ Diffusers library](https://github.com/huggingface/diffusers), explain how the model works and finally dive a bit deeper into how `diffusers` allows\none to customize the image generation pipeline. \\*\\*Note\\*\\*: It is highly recommended to have a basic understanding of how diffusion models work. If diffusion\nmodels are completely new to you, we recommend reading one of the following blog posts:\n- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n- [Getting started with ðŸ§¨ Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers\\_intro.ipynb)\nNow, let's get started by generating some images ðŸŽ¨. ## Running Stable Diffusion\n### License\nBefore using the model, you need to accept the model [license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) in order to download and use the weights.""]","The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation in text-to-image generation tasks. It creates novel renditions of an input subject based on text prompts while overcoming limitations such as lengthy fine-tuning and difficulties in preserving subject fidelity. BLIP-Diffusion introduces a new multimodal encoder pre-trained to provide subject representation, allowing for efficient fine-tuning and flexible combinations with existing techniques for novel subject-driven generation and editing applications.",The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.
"How can a user claim authorship of a paper on the Hugging Face Hub?
","[""Paper Pages\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png)\n\n## Linking a Paper to a model, dataset or Space\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository's tags. Clicking on the arxiv tag will let you:\n\\* Visit the Paper page. \\* Filter for other models or datasets on the Hub that cite the same paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\n\n## Claiming authorship to a Paper\nThe Hub will attempt to automatically match paper to users based on their email. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png)\n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \""claim authorship\""."", ""This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings-dark.png)\n\n## Frequently Asked Questions\n### Can I control which Paper pages show in my profile? Yes! You can visit your Papers in [settings](https://huggingface.co/settings/papers), where you will see a list of verified papers. There, you can click the \""Show on profile\"" checkbox to hide/show it in your profile. ### Do you support ACL anthology? We're starting with Arxiv as it accounts for 95% of the paper URLs Hugging Face users have linked in their repos organically. We'll check how this evolve and potentially extend to other paper hosts in the future. ### Can I have a Paper page even if I have no model/dataset/Space? Yes. You can go to [the main Papers page](https://huggingface.co/papers), click search and write the name of the paper or the full Arxiv id. If the paper does not exist, you will get an option to index it. You can also just visit the page `hf.co/papers/xxxx.yyyyy` replacing with the arxiv id of the paper you wish to index."", ""![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\n\nRead more about paper pages [here](./paper-pages).""]","A user can claim authorship of a paper on the Hugging Face Hub by following these steps:

1. If the paper is not linked to the user's account, the user can click on their name in the corresponding Paper page.
2. Then, they should click ""claim authorship.""
3. This action will automatically redirect the user to their paper settings, where they can confirm the request.
4. The admin team will validate the request soon. Once confirmed, the Paper page will show as verified.","By clicking their name on the corresponding Paper page and clicking ""claim authorship"", then confirming the request in paper settings for admin team validation."
"What is the purpose of the /healthcheck endpoint in the Datasets server API?
","[""Datasets server API - rows endpoint\n> /rows endpoint\n## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### API service\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split"", ""Datasets server SSE API\n> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache. ## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub\\_cache` field is null for deleted entries, or when the response is an error. The `num\\_rows` value is `0` if it could not be determined. ```typescript\n{\ndataset: string;\nhub\\_cache: null | {\npreview: boolean;\nviewer: boolean;\npartial: boolean;\nnum\\_rows: int;\n};\n}\n```"", ""Datasets server API\n> API on ðŸ¤— datasets\n## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### API service\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /webhook: Add, update or remove a dataset\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first\\_rows) for a dataset split\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset""]",The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the app is running.,Ensure the app is running
"What is the default context window size for Local Attention in the LongT5 model?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# LongT5\n## Overview\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention. The abstract from the paper is the following:\n\\*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.\\*\nThis model was contributed by [stancld](https://huggingface.co/stancld). The original code can be found [here](https://github.com/google-research/longt5). ## Usage tips\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder \\*self-attention\\* layer with efficient either \\*local\\* attention or \\*transient-global\\* (\\*tglobal\\*) attention. - Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`]. - LongT5 model is designed to work efficiently and very well on long-range \\*sequence-to-sequence\\* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens. - For \\*Local Attention\\*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). \\*Local Attention\\* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l\\*r)`. - \\*Transient Global Attention\\* is an extension of the \\*Local Attention\\*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\nalso every global token like in the case of standard global attention (\\*transient\\* represents the fact the global tokens\nare constructed dynamically within each attention operation). As a consequence, \\*TGlobal\\* attention introduces\na few new parameters -- global relative position biases and a layer normalization for global token's embedding."", ""With our sliding window approach, however, there is overlap in\nthe tokens we pass to the model at each iteration. We don't want the log-likelihood for the tokens we're just treating\nas context to be included in our loss, so we can set these targets to `-100` so that they are ignored. The following\nis an example of how we could do this with a stride of `512`. This means that the model will have at least 512 tokens\nfor context when calculating the conditional likelihood of any one token (provided there are 512 preceding tokens\navailable to condition on). ```python\nimport torch\nfrom tqdm import tqdm\nmax\\_length = model.config.n\\_positions\nstride = 512\nseq\\_len = encodings.input\\_ids.size(1)\nnlls = []\nprev\\_end\\_loc = 0\nfor begin\\_loc in tqdm(range(0, seq\\_len, stride)):\nend\\_loc = min(begin\\_loc + max\\_length, seq\\_len)\ntrg\\_len = end\\_loc - prev\\_end\\_loc # may be different from stride on last loop\ninput\\_ids = encodings.input\\_ids[:, begin\\_loc:end\\_loc].to(device)\ntarget\\_ids = input\\_ids.clone()\ntarget\\_ids[:, :-trg\\_len] = -100\nwith torch.no\\_grad():\noutputs = model(input\\_ids, labels=target\\_ids)\n# loss is calculated using CrossEntropyLoss which averages over valid labels\n# N.B. the model only calculates loss over trg\\_len - 1 labels, because it internally shifts the labels\n# to the left by 1. neg\\_log\\_likelihood = outputs.loss\nnlls.append(neg\\_log\\_likelihood)\nprev\\_end\\_loc = end\\_loc\nif end\\_loc == seq\\_len:\nbreak\nppl = torch.exp(torch.stack(nlls).mean())\n```\nRunning this with the stride length equal to the max input length is equivalent to the suboptimal, non-sliding-window\nstrategy we discussed above. The smaller the stride, the more context the model will have in making each prediction,\nand the better the reported perplexity will typically be. When we run the above with `stride = 1024`, i.e. no overlap, the resulting PPL is `19.44`, which is about the same\nas the `19.93` reported in the GPT-2 paper. By using `stride = 512` and thereby employing our striding window\nstrategy, this jumps down to `16.45`. This is not only a more favorable score, but is calculated in a way that is\ncloser to the true autoregressive decomposition of a sequence likelihood."", ""![](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/mixtral-7b-inference-large-seqlen.png)\n\n### Sliding window Attention\nThe current implementation supports the sliding window attention mechanism and memory efficient cache management. To enable sliding window attention, just make sure to have a `flash-attn` version that is compatible with sliding window attention (`>=2.3.0`). The Flash Attention-2 model uses also a more memory efficient cache slicing mechanism - as recommended per the official implementation of Mistral model that use rolling cache mechanism we keep the cache size fixed (`self.config.sliding\\_window`), support batched generation only for `padding\\_side=\""left\""` and use the absolute position of the current token to compute the positional embedding. ## The Mistral Team\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed. ## MixtralConfig\n[[autodoc]] MixtralConfig\n## MixtralModel\n[[autodoc]] MixtralModel\n- forward\n## MixtralForCausalLM\n[[autodoc]] MixtralForCausalLM\n- forward\n## MixtralForSequenceClassification\n[[autodoc]] MixtralForSequenceClassification\n- forward""]","The default context window size for Local Attention in the LongT5 model is `r=127`, which means a given token can attend to 127 tokens to the left and 127 tokens to the right of it.",127 tokens
"What method is used to load a checkpoint for a task using `AutoPipeline`?
","[""The `AutoPipeline` class is designed to simplify the variety of pipelines in ðŸ¤— Diffusers. It is a generic, \\*task-first\\* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name. Take a look at the [AutoPipeline](../api/pipelines/auto\\_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting. This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights. ## Choose an AutoPipeline for your task\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nprompt = \""peasant and dragon combat, wood cutting style, viking era, bevel with rune\""\nimage = pipeline(prompt, num\\_inference\\_steps=25).images[0]\nimage\n```\n\n![generated image of peasant fighting dragon in wood cutting style](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png)\n\nUnder the hood, [`AutoPipelineForText2Image`]:\n1. automatically detects a `\""stable-diffusion\""` class from the [`model\\_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model\\_index.json) file\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\""stable-diffusion\""` class name\nLikewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\""stable-diffusion\""` checkpoint from the `model\\_index.json` file and it'll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image:\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\npipeline = AutoPipelineForImage2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"",\ntorch\\_dtype=torch.float16,\nuse\\_safetensors=True,\n).to(\""cuda\"")\nprompt = \""a portrait of a dog wearing a pearl earring\""\nurl = \""https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665\\_Girl\\_with\\_a\\_Pearl\\_Earring.jpg/800px-1665\\_Girl\\_with\\_a\\_Pearl\\_Earring.jpg\""\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\""RGB\"")\nimage.thumbnail((768, 768))\nimage = pipeline(prompt, image, num\\_inference\\_steps=200, strength=0.75, guidance\\_scale=10.5).images[0]\nimage\n```\n\n![generated image of a vermeer portrait of a dog wearing a pearl earring](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png)\n\nAnd if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way:\n```py\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image\nimport torch\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""stabilityai/stable-diffusion-xl-base-1.0\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nimg\\_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\_examples/overture-creations-5sI6fQgYIuo.png\""\nmask\\_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\_examples/overture-creations-5sI6fQgYIuo\\_mask.png\""\ninit\\_image = load\\_image(img\\_url).convert(\""RGB\"")\nmask\\_image = load\\_image(mask\\_url).convert(\""RGB\"")\nprompt = \""A majestic tiger sitting on a bench\""\nimage = pipeline(prompt, image=init\\_image, mask\\_image=mask\\_image, num\\_inference\\_steps=50, strength=0.80).images[0]\nimage\n```\n\n![generated image of a tiger sitting on a bench](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png)\n\nIf you try to load an unsupported checkpoint, it'll throw an error:\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\npipeline = AutoPipelineForImage2Image.from\\_pretrained(\n\""openai/shap-e-img2img\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n)\n\""ValueError: AutoPipeline can't find a pipeline linked to ShapEImg2ImgPipeline for None\""\n```\n## Use multiple pipelines\nFor some workflows or if you're loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# AutoPipeline\n`AutoPipeline` is designed to:\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from\\_pretrained()` method. To seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from\\_pipe()` method to transfer the components from the original pipeline to the new one. ```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nprompt = \""Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\""\nimage = pipeline(prompt, num\\_inference\\_steps=25).images[0]\n```\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API! `AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n- [Stable Diffusion](./stable\\_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable\\_diffusion/stable\\_diffusion\\_xl)\n- [DeepFloyd IF](./deepfloyd\\_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky\\_v22)\n## AutoPipelineForText2Image\n[[autodoc]] AutoPipelineForText2Image\n- all\n- from\\_pretrained\n- from\\_pipe\n## AutoPipelineForImage2Image\n[[autodoc]] AutoPipelineForImage2Image\n- all\n- from\\_pretrained\n- from\\_pipe\n## AutoPipelineForInpainting\n[[autodoc]] AutoPipelineForInpainting\n- all\n- from\\_pretrained\n- from\\_pipe"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# AutoPipeline\nðŸ¤— Diffusers is able to complete many different tasks, and you can often reuse the same pretrained weights for multiple tasks such as text-to-image, image-to-image, and inpainting. If you're new to the library and diffusion models though, it may be difficult to know which pipeline to use for a task. For example, if you're using the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint for text-to-image, you might not know that you could also use it for image-to-image and inpainting by loading the checkpoint with the [`StableDiffusionImg2ImgPipeline`] and [`StableDiffusionInpaintPipeline`] classes respectively.""]",The method used to load a checkpoint for a task using `AutoPipeline` is the `from_pretrained()` method. This method automatically retrieves the relevant pipeline given the name or path to the pretrained weights.,from_pretrained()
"What is the purpose of Diffusers library?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\nWelcome to ðŸ§¨ Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how ðŸ§¨ Diffusers is meant to be used. You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want. After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications. Feel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers! Let's start diffusing!"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n## Preamble\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training. Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library. The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Philosophy\nðŸ§¨ Diffusers provides \\*\\*state-of-the-art\\*\\* pretrained diffusion models across multiple modalities. Its purpose is to serve as a \\*\\*modular toolbox\\*\\* for both inference and training. We aim at building a library that stands the test of time and therefore take API design very seriously. In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch's Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let's go over the most important ones:\n## Usability over Performance\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library. - Diffusers aims to be a \\*\\*light-weight\\*\\* package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages. - Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired. ## Simple over easy\nAs PyTorch states, \\*\\*explicit is better than implicit\\*\\* and \\*\\*simple is better than complex\\*\\*.""]","The purpose of the ðŸ§¨ Diffusers library is to provide state-of-the-art pretrained diffusion models across multiple modalities and to serve as a modular toolbox for both inference and training. It aims to build a library that stands the test of time, focusing on usability and simplicity while being a natural extension of PyTorch.",To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.
"What method does the EulerAncestralDiscreteScheduler use for sampling?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# EulerAncestralDiscreteScheduler\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling\\_euler\\_ancestral\\_discrete.EulerAncestralDiscreteSchedulerOutput"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# EulerDiscreteScheduler\nThe Euler scheduler (Algorithm 2) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\_diffusion/sampling.py#L51) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerDiscreteScheduler\n[[autodoc]] EulerDiscreteScheduler\n## EulerDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling\\_euler\\_discrete.EulerDiscreteSchedulerOutput""]",The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling.,Ancestral sampling with Euler method steps.
"What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Image tasks with IDEFICS\n[[open-in-colab]]\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach\nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. For instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can\nsolve image-text tasks with a large multimodal model called IDEFICS. [IDEFICS](../model\\_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198),\na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image\nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content,\ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b)\nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the ðŸ¤— Hub."", ""Models such as [VisualGPT](https://arxiv.org/abs/2102.10407), [VC-GPT](https://arxiv.org/abs/2201.12723), and [Flamingo](https://arxiv.org/abs/2204.14198) use this pre-training strategy and are trained on image captioning and visual question-answering tasks. The main goal of such models is to balance the mixture of text generation capacity and visual information efficiently, which is highly important in the absence of large multi-modal datasets. Models such as VisualGPT use a visual encoder to embed images and feed the visual embeddings to the cross-attention layers of a pre-trained language decoder module to generate plausible captions. A more recent work, [FIBER](http://arxiv.org/abs/2206.07643), inserts cross-attention layers with a gating mechanism into both vision and language backbones, for more efficient multi-modal fusing and enables various other downstream tasks, such as image-text retrieval and open vocabulary object detection. ### 4) Masked-Language Modeling / Image-Text Matching\nAnother line of vision-language models uses a combination of Masked-Language Modeling (MLM) and Image-Text Matching (ITM) objectives to align specific parts of images with text and enable various downstream tasks such as visual question answering, visual commonsense reasoning, text-based image retrieval, and text-guided object detection. Models that follow this pre-training setup include [VisualBERT](https://arxiv.org/abs/1908.03557), [FLAVA](https://arxiv.org/abs/2112.04482), [ViLBERT](https://arxiv.org/abs/1908.02265), [LXMERT](https://arxiv.org/abs/1908.07490) and [BridgeTower](https://arxiv.org/abs/2206.08657). ![MLM / ITM](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/mlm_itm.png)  \n *Aligning parts of images with text ([image source](https://arxiv.org/abs/1908.02265))*\n\nLetâ€™s break down what MLM and ITM objectives mean. Given a partially masked caption, the MLM objective is to predict the masked words based on the corresponding image. Note that the MLM objective requires either using a richly annotated multi-modal dataset with bounding boxes or using an object detection model to generate object region proposals for parts of the input text. For the ITM objective, given an image and caption pair, the task is to predict whether the caption matches the image or not."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# FLAVA\n## Overview\nThe FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022. The paper aims at creating a single unified foundation model which can work across vision, language\nas well as vision-and-language multimodal tasks. The abstract from the paper is the following:\n\\*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety\nof downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal\n(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising\ndirection would be to use a single holistic universal model, as a \""foundation\"", that targets all modalities\nat once -- a true vision and language foundation model should be good at vision tasks, language tasks, and\ncross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate\nimpressive performance on a wide range of 35 tasks spanning these target modalities.\\*\nThis model was contributed by [aps](https://huggingface.co/aps). The original code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava). ## FlavaConfig\n[[autodoc]] FlavaConfig\n## FlavaTextConfig\n[[autodoc]] FlavaTextConfig\n## FlavaImageConfig\n[[autodoc]] FlavaImageConfig\n## FlavaMultimodalConfig\n[[autodoc]] FlavaMultimodalConfig\n## FlavaImageCodebookConfig\n[[autodoc]] FlavaImageCodebookConfig\n## FlavaProcessor\n[[autodoc]] FlavaProcessor\n## FlavaFeatureExtractor\n[[autodoc]] FlavaFeatureExtractor\n## FlavaImageProcessor\n[[autodoc]] FlavaImageProcessor\n- preprocess\n## FlavaForPreTraining\n[[autodoc]] FlavaForPreTraining\n- forward\n## FlavaModel\n[[autodoc]] FlavaModel\n- forward\n- get\\_text\\_features\n- get\\_image\\_features\n## FlavaImageCodebook\n[[autodoc]] FlavaImageCodebook\n- forward\n- get\\_codebook\\_indices\n- get\\_codebook\\_probs\n## FlavaTextModel\n[[autodoc]] FlavaTextModel\n- forward\n## FlavaImageModel\n[[autodoc]] FlavaImageModel\n- forward\n## FlavaMultimodalModel\n[[autodoc]] FlavaMultimodalModel\n- forward""]",The large multimodal model that can solve image-text tasks and is based on Flamingo is called IDEFICS.,IDEFICS
"What is the purpose of the `gradio.Blocks` API?
","[""Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our \\*\\*newly developed\\*\\* low-level API called `gradio.Blocks`. Now, what's the difference between `Interface` and `Blocks`? - âš¡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs. - ðŸ§± `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \""building blocks\""). ### Why Blocks ðŸ§±?[[why-blocks-]]\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\n- Group together related demos as multiple tabs in one web application\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a dropdown) or its visibility based on user input\nWe will explore all of these concepts below. ### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook. ```py\nimport gradio as gr\ndef flip\\_text(x):\nreturn x[::-1]\ndemo = gr.Blocks()\nwith demo:\ngr.Markdown(\n\""\""\""\n# Flip Text! Start typing below to see the output. \""\""\""\n)\ninput = gr.Textbox(placeholder=\""Flip this text\"")\noutput = gr.Textbox()\ninput.change(fn=flip\\_text, inputs=input, outputs=output)\ndemo.launch()\n```\nThis simple example above introduces 4 concepts that underlie Blocks:\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context."", ""## An Overview of Gradio\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do? ### Chatbots with `gr.ChatInterface`\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight to [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast). ### Custom Demos with `gr.Blocks`\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction â€” still all in Python. You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners). ### The Gradio Python & JavaScript Ecosystem\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\\* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio\\_client`): query any Gradio app programmatically in Python. \\* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript. \\* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \\* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications â€” for free!"", ""All components are now keyboard navigable and include better colors to be usable by a wider audience. ![](https://media0.giphy.com/media/Kv1bAN7MX3ya5krkEU/giphy.gif)\n\\*\\*3. Server Side Events\\*\\*:\nGradio's built-in queuing system is now the default for every Gradio app. We now use Server Side Events instead of Websockets for the queue. SSE means everything is served over HTTP and has better device support and better scaling than websockets. ![](https://i.imgur.com/ewUIuUc.png)\n\\*\\*4. Custom Share Servers\\*\\*:\nGradio share links can now run on custom domains. You can now set up your own server to serve Gradio share links. To get started, [read our guide here](https://github.com/huggingface/frp/). ![](https://i.imgur.com/VFWVsqn.png)\n5. We now support adding arbitrary JS to your apps using the `js` parameter in Blocks, and arbitrary modifications to the  of your app using the `head` parameter in Blocks\n6. We no longer expose a user's working directory by default when you release a Gradio app.""]","The `gradio.Blocks` API is a low-level API that allows you to have full control over the data flows and layout of your application. It provides flexibility to build complex, multi-step applications by enabling you to:

- Group related demos as multiple tabs in one web application.
- Change the layout of your demo, specifying where the inputs and outputs are located.
- Create multi-step interfaces, where the output of one model becomes the input to the next model, or have more flexible data flows in general.
- Modify a component's properties (e.g., choices in a dropdown) or its visibility based on user input.

Overall, `gradio.Blocks` allows for more customized and intricate applications compared to the high-level `Interface` class.","The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications."
"What is the purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents""?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# unCLIP\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in ðŸ¤— Diffusers comes from kakaobrain's [karlo](https://github.com/kakaobrain/karlo). The abstract from the paper is following:\n\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\*\nYou can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Prior Transformer\nThe Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process. The abstract from the paper is:\n\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\*\n## PriorTransformer\n[[autodoc]] PriorTransformer\n## PriorTransformerOutput\n[[autodoc]] models.prior\\_transformer.PriorTransformerOutput"", ""While CLIPSeg is trained on simple object descriptions (e.g., â€œa carâ€), its CLIP backbone is pre-trained on engineered text templates (e.g., â€œan image of a carâ€, â€œa photo of a carâ€) and kept frozen during training. Once the inputs are preprocessed, we can perform inference to get a binary segmentation map of shape (height, width) for each text query. ```py\nimport torch\nwith torch.no\\_grad():\noutputs = model(\\*\\*inputs)\nlogits = outputs.logits\nprint(logits.shape)\n>>> torch.Size([3, 352, 352])\n```\nLetâ€™s visualize the results to see how well CLIPSeg performed (code is adapted from [this post](https://huggingface.co/blog/clipseg-zero-shot)). ```py\nimport matplotlib.pyplot as plt\nlogits = logits.unsqueeze(1)\n\\_, ax = plt.subplots(1, len(texts) + 1, figsize=(3\\*(len(texts) + 1), 12))\n[a.axis('off') for a in ax.flatten()]\nax[0].imshow(image)\n[ax[i+1].imshow(torch.sigmoid(logits[i][0])) for i in range(len(texts))];\n[ax[i+1].text(0, -15, prompt) for i, prompt in enumerate(texts)]\n```\n\n![CLIPSeg results](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/clipseg_result.png)\n\nAmazing, isnâ€™t it?""]","The purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents"" is to leverage robust representations of images learned by contrastive models like CLIP for image generation. The model consists of a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. This approach aims to improve image diversity with minimal loss in photorealism and caption similarity, allowing for variations of an image that preserve its semantics and style while varying non-essential details. Additionally, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion.",The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.
"What command is used to install the requirements for a research project using ðŸ¤— Transformers?
","[""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Research projects\nThis folder contains various research projects using ðŸ¤— Transformers. They are not maintained and require a specific\nversion of ðŸ¤— Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice. If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder."", ""ow to add BigBird to ðŸ¤— Transformers? =====================================\nMentor: [Patrick](https://github.com/patrickvonplaten)\nBegin: 12.02.2020\nEstimated End: 19.03.2020\nContributor: [Vasudev](https://github.com/thevasudevgupta)\nAdding a new model is often difficult and requires an in-depth knowledge\nof the ðŸ¤— Transformers library and ideally also of the model's original\nrepository. At Hugging Face, we are trying to empower the community more\nand more to add models independently. The following sections explain in detail how to add BigBird\nto Transformers. You will work closely with Patrick to\nintegrate BigBird into Transformers. By doing so, you will both gain a\ntheoretical and deep practical understanding of BigBird. But more importantly, you will have made a major\nopen-source contribution to Transformers. Along the way, you will:\n- get insights into open-source best practices\n- understand the design principles of one of the most popular NLP\nlibraries\n- learn how to do efficiently test large NLP models\n- learn how to integrate Python utilities like `black`, `ruff`,\n`make fix-copies` into a library to always ensure clean and readable\ncode\nTo start, let's try to get a general overview of the Transformers\nlibrary."", ""\\* [TensorFlow 2.0](https://www.tensorflow.org/install/pip) installation instructions. \\* [Flax](https://flax.readthedocs.io/en/latest/) installation instructions. ## Install with pip\nYou should install ðŸ¤— Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies. Start by creating a virtual environment in your project directory:\n```bash\npython -m venv .env\n```\nActivate the virtual environment. On Linux and MacOs:\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n```bash\n.env/Scripts/activate\n```\nNow you're ready to install ðŸ¤— Transformers with the following command:\n```bash\npip install transformers\n```\nFor CPU-support only, you can conveniently install ðŸ¤— Transformers and a deep learning library in one line. For example, install ðŸ¤— Transformers and PyTorch with:\n```bash\npip install 'transformers[torch]'\n```\nðŸ¤— Transformers and TensorFlow 2.0:\n```bash\npip install 'transformers[tf-cpu]'\n```\nM1 / ARM Users\nYou will need to install the following before installing TensorFLow 2.0\n```\nbrew install cmake\nbrew install pkg-config\n```\nðŸ¤— Transformers and Flax:\n```bash\npip install 'transformers[flax]'\n```\nFinally, check if ðŸ¤— Transformers has been properly installed by running the following command. It will download a pretrained model:\n```bash\npython -c \""from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\""\n```\nThen print out the label and score:\n```bash\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```\n## Install from source\nInstall ðŸ¤— Transformers from source with the following command:\n```bash\npip install git+https://github.com/huggingface/transformers\n```\nThis command installs the bleeding edge `main` version rather than the latest `stable` version.""]","The command used to install the requirements for a research project using ðŸ¤— Transformers is:

```
pip install -r requirements.txt
``` 

You should run this command inside the folder of your choice.",pip install -r requirements.txt
"What task does the `roberta-large-mnli` checkpoint perform?
","[""!-- DISABLE-FRONTMATTER-SECTIONS -->\n# End-of-chapter quiz[[end-of-chapter-quiz]]\nThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood. First, though, let's test what you learned in this chapter! ### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform? roberta-large-mnli page.\""\n},\n{\ntext: \""Text classification\"",\nexplain: \""More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) â€” a task also called *natural language inference*.\"",\ncorrect: true\n},\n{\ntext: \""Text generation\"",\nexplain: \""Look again on the [roberta-large-mnli page](\\\""https://huggingface.co/roberta-large-mnli\\\"").\""\n}\n]}\n/>\n### 2. What will the following code return? ```py\nfrom transformers import pipeline\nner = pipeline(\""ner\"", grouped\\_entities=True)\nner(\""My name is Sylvain and I work at Hugging Face in Brooklyn.\"")\n```\nsentiment-analysis pipeline.\""\n},\n{\ntext: \""It will return a generated text completing this sentence.\"",\nexplain: \""This is incorrect â€” it would be a `text-generation` pipeline.\"",\n},\n{\ntext: \""It will return the words representing persons, organizations or locations.\"",\nexplain: \""Furthermore, with `grouped_entities=True`, it will group together the words belonging to the same entity, like \\\""Hugging Face\\\"".\"",\ncorrect: true\n}\n]}\n/>\n### 3."", ""Refer to this page for usage examples. ## Resources\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with RoBERTa. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. - A blog on [Getting Started with Sentiment Analysis on Twitter](https://huggingface.co/blog/sentiment-analysis-twitter) using RoBERTa and the [Inference API](https://huggingface.co/inference-api). - A blog on [Opinion Classification with Kili and Hugging Face AutoTrain](https://huggingface.co/blog/opinion-classification-with-kili) using RoBERTa. - A notebook on how to [finetune RoBERTa for sentiment analysis](https://colab.research.google.com/github/DhavalTaunk08/NLP\\_scripts/blob/master/sentiment\\_analysis\\_using\\_roberta.ipynb). ðŸŒŽ\n- [`RobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\_classification.ipynb). - [`TFRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\_classification-tf.ipynb). - [`FlaxRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\_classification\\_flax.ipynb). - [Text classification task guide](../tasks/sequence\\_classification)\n- [`RobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token\\_classification.ipynb). - [`TFRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token\\_classification-tf.ipynb). - [`FlaxRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification). - [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the ðŸ¤— Hugging Face Course. - [Token classification task guide](../tasks/token\\_classification)\n- A blog on [How to train a new language model from scratch using Transformers and Tokenizers](https://huggingface.co/blog/how-to-train) with RoBERTa. - [`RobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language\\_modeling.ipynb). - [`TFRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run\\_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language\\_modeling-tf.ipynb). - [`FlaxRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked\\_language\\_modeling\\_flax.ipynb)."", ""- [Question answering task guide](../tasks/question\\_answering)\n\\*\\*Multiple choice\\*\\*\n- [`RobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice.ipynb). - [`TFRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice-tf.ipynb). - [Multiple choice task guide](../tasks/multiple\\_choice)\n## RobertaConfig\n[[autodoc]] RobertaConfig\n## RobertaTokenizer\n[[autodoc]] RobertaTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## RobertaTokenizerFast\n[[autodoc]] RobertaTokenizerFast\n- build\\_inputs\\_with\\_special\\_tokens\n\n## RobertaModel\n[[autodoc]] RobertaModel\n- forward\n## RobertaForCausalLM\n[[autodoc]] RobertaForCausalLM\n- forward\n## RobertaForMaskedLM\n[[autodoc]] RobertaForMaskedLM\n- forward\n## RobertaForSequenceClassification\n[[autodoc]] RobertaForSequenceClassification\n- forward\n## RobertaForMultipleChoice\n[[autodoc]] RobertaForMultipleChoice\n- forward\n## RobertaForTokenClassification\n[[autodoc]] RobertaForTokenClassification\n- forward\n## RobertaForQuestionAnswering\n[[autodoc]] RobertaForQuestionAnswering\n- forward\n\n## TFRobertaModel\n[[autodoc]] TFRobertaModel\n- call\n## TFRobertaForCausalLM\n[[autodoc]] TFRobertaForCausalLM\n- call\n## TFRobertaForMaskedLM\n[[autodoc]] TFRobertaForMaskedLM\n- call\n## TFRobertaForSequenceClassification\n[[autodoc]] TFRobertaForSequenceClassification\n- call\n## TFRobertaForMultipleChoice\n[[autodoc]] TFRobertaForMultipleChoice\n- call\n## TFRobertaForTokenClassification\n[[autodoc]] TFRobertaForTokenClassification\n- call\n## TFRobertaForQuestionAnswering\n[[autodoc]] TFRobertaForQuestionAnswering\n- call\n\n## FlaxRobertaModel\n[[autodoc]] FlaxRobertaModel\n- \\_\\_call\\_\\_\n## FlaxRobertaForCausalLM\n[[autodoc]] FlaxRobertaForCausalLM\n- \\_\\_call\\_\\_\n## FlaxRobertaForMaskedLM\n[[autodoc]] FlaxRobertaForMaskedLM\n- \\_\\_call\\_\\_\n## FlaxRobertaForSequenceClassification\n[[autodoc]] FlaxRobertaForSequenceClassification\n- \\_\\_call\\_\\_\n## FlaxRobertaForMultipleChoice\n[[autodoc]] FlaxRobertaForMultipleChoice\n- \\_\\_call\\_\\_\n## FlaxRobertaForTokenClassification\n[[autodoc]] FlaxRobertaForTokenClassification\n- \\_\\_call\\_\\_\n## FlaxRobertaForQuestionAnswering\n[[autodoc]] FlaxRobertaForQuestionAnswering\n- \\_\\_call\\_\\_""]","The `roberta-large-mnli` checkpoint performs text classification, specifically classifying if two sentences are logically linked across three labels: contradiction, neutral, and entailment. This task is also known as natural language inference.",Text classification
"What service is replacing the Paid tier of the Inference API at Hugging Face?
","[""--\ntitle: Introducing our new pricing\nthumbnail: /blog/assets/114\\_pricing-update/thumbnail.png\nauthors:\n- user: sbrandeis\n- user: pierric\n---\n# Introducing our new pricing\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently. First of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints). Along with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations). Your billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records. \\*\\*TL;DR\\*\\*: \\*\\*At HF we monetize by providing simple access to compute for AI\\*\\*, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub. [Read more](https://huggingface.co/docs/hub/billing) about our pricing and billing system. If you have any questions, feel free to reach out. We welcome your feedback ðŸ”¥"", ""Benefit from higher rate limits, custom accelerated endpoints for the latest models, and early access to features. If you've built some exciting projects with the Inference API or are looking for a model not available in Inference for PROs, please [use this discussion](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/13). [Enterprise users](https://huggingface.co/enterprise) also benefit from PRO Inference API on top of other features, such as SSO. ## FAQ\n\n**Does this affect the free Inference API?**\n\nNo. We still expose thousands of models through free APIs that allow people to prototype and explore model capabilities quickly. **Does this affect Enterprise users?**\n\nUsers with an Enterprise subscription also benefit from accelerated inference API for curated models. **Can I use my own models with PRO Inference API?**\n\nThe free Inference API already supports a wide range of small and medium models from a variety of libraries (such as diffusers, transformers, and sentence transformers). If you have a custom model or custom inference logic, we recommend using [Inference Endpoints](https://ui.endpoints.huggingface.co/catalog)."", ""Access ðŸ¤— Inference Endpoints\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account. You can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method. There are two pricing plans:\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. - There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested! After you've added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying!""]",The Paid tier of the Inference API service is being replaced by the new solution called **Inference Endpoints**.,Inference Endpoints
"What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# SqueezeBERT\n## Overview\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers."", ""The abstract from the paper is the following:\n\\*Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets,\nlarge computing systems, and better neural network models, natural language processing (NLP) technology has made\nsignificant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant\nopportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. In particular, we\nconsider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today's\nhighly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with\nBERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. In this work, we observe that methods\nsuch as grouped convolutions have yielded significant speedups for computer vision networks, but many of these\ntechniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\nset. The SqueezeBERT code will be released.\\*\nThis model was contributed by [forresti](https://huggingface.co/forresti). ## Usage tips\n- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\nrather than the left. - SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\nefficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\nwith a causal language modeling (CLM) objective are better in that regard. - For best results when finetuning on sequence classification tasks, it is recommended to start with the\n\\*squeezebert/squeezebert-mnli-headless\\* checkpoint. ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## SqueezeBertConfig\n[[autodoc]] SqueezeBertConfig\n## SqueezeBertTokenizer\n[[autodoc]] SqueezeBertTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## SqueezeBertTokenizerFast\n[[autodoc]] SqueezeBertTokenizerFast\n## SqueezeBertModel\n[[autodoc]] SqueezeBertModel\n## SqueezeBertForMaskedLM\n[[autodoc]] SqueezeBertForMaskedLM\n## SqueezeBertForSequenceClassification\n[[autodoc]] SqueezeBertForSequenceClassification\n## SqueezeBertForMultipleChoice\n[[autodoc]] SqueezeBertForMultipleChoice\n## SqueezeBertForTokenClassification\n[[autodoc]] SqueezeBertForTokenClassification\n## SqueezeBertForQuestionAnswering\n[[autodoc]] SqueezeBertForQuestionAnswering"", ""In the original model it is possible to use an average pooling layer with a fixed 7x7 window and stride 1 instead of global pooling. For inputs that are larger than the recommended image size, this gives a pooled output that is larger than 1x1. The Hugging Face implementation does not support this. - The original TensorFlow checkpoints include quantized models. We do not support these models as they include additional \""FakeQuantization\"" operations to unquantize the weights. - It's common to extract the output from the expansion layers at indices 10 and 13, as well as the output from the final 1x1 convolution layer, for downstream purposes. Using `output\\_hidden\\_states=True` returns the output from all intermediate layers. There is currently no way to limit this to specific layers. - The DeepLabV3+ segmentation head does not use the final convolution layer from the backbone, but this layer gets computed anyway. There is currently no way to tell [`MobileNetV2Model`] up to which layer it should run.""]","SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers.",Grouped convolutions
"What type of license is the HuggingFace Team's software distributed under?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction"", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Scenes\nUnder construction ðŸš§."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction""]","The HuggingFace Team's software is distributed under the Apache License, Version 2.0.","Apache License, Version 2.0"
"What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?
","[""It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\n\\*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\*\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A."", ""However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model\\_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model\\_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model\\_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix. ### Decoder[[nlp-decoder]]\n[GPT-2](model\\_doc/gpt2) is a decoder-only Transformer that predicts the next word in the sequence."", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ALBERT\n\n[![Models](https://img.shields.io/badge/All_model_pages-albert-blueviolet)](https://huggingface.co/models?filter=albert)\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/albert-base-v2)\n\n## Overview\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\nRadu Soricut.""]","The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:

1. Splitting the embedding matrix into two smaller matrices.
2. Using repeating layers split among groups.",Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.
"What are the three main steps for fine-tuning a model with the ðŸ¤— Datasets library?
","[""Introduction[[introduction]]\nIn [Chapter 3](/course/chapter3) you got your first taste of the ðŸ¤— Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics. But this is just scratching the surface of what ðŸ¤— Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\\* What do you do when your dataset is not on the Hub? \\* How can you slice and dice a dataset? (And what if you \\_really\\_ need to use Pandas?)\n\\* What do you do when your dataset is huge and will melt your laptop's RAM? \\* What the heck are \""memory mapping\"" and Apache Arrow? \\* How can you create your own dataset and push it to the Hub? The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!"", ""To verify that all libraries are correctly installed, you can run the following command in a Python shell. It verifies that both `transformers` and `datasets` have been correclty installed. ```python\nfrom transformers import AutoModelForCTC, AutoProcessor\nfrom datasets import load\\_dataset\ndummy\\_dataset = load\\_dataset(\""common\\_voice\"", \""ab\"", split=\""test\"")\nmodel = AutoModelForCTC.from\\_pretrained(\""hf-internal-testing/tiny-random-wav2vec2\"")\nmodel.to(\""cuda\"")\nprocessor = AutoProcessor.from\\_pretrained(\""hf-internal-testing/tiny-random-wav2vec2\"")\ninput\\_values = processor(dummy\\_dataset[0][\""audio\""][\""array\""], return\\_tensors=\""pt\"", sampling\\_rate=16\\_000).input\\_values\ninput\\_values = input\\_values.to(\""cuda\"")\nlogits = model(input\\_values).logits\nassert logits.shape[-1] == 32\n```\n## How to finetune an acoustic model\nIn this section, we show you how to fine-tune a pre-trained [XLS-R Model](https://huggingface.co/docs/transformers/model\\_doc/xls\\_r) on the [Common Voice 7 dataset](https://huggingface.co/datasets/mozilla-foundation/common\\_voice\\_7\\_0)."", ""Considerations for model evaluation\nDeveloping an ML model is rarely a one-shot deal: it often involves multiple stages of defining the model architecture and tuning hyper-parameters before converging on a final set. Responsible model evaluation is a key part of this process, and ðŸ¤— Evaluate is here to help! Here are some things to keep in mind when evaluating your model using the ðŸ¤— Evaluate library:\n## Properly splitting your data\nGood evaluation generally requires three splits of your dataset:\n- \\*\\*train\\*\\*: this is used for training your model. - \\*\\*validation\\*\\*: this is used for validating the model hyperparameters. - \\*\\*test\\*\\*: this is used for evaluating your model. Many of the datasets on the ðŸ¤— Hub are separated into 2 splits: `train` and `validation`; others are split into 3 splits (`train`, `validation` and `test`) -- make sure to use the right split for the right purpose! Some datasets on the ðŸ¤— Hub are already separated into these three splits. However, there are also many that only have a train/validation or only train split. If the dataset you're using doesn't have a predefined train-test split, it is up to you to define which part of the dataset you want to use for training your model and which you want to use for hyperparameter tuning or final evaluation. Training and evaluating on the same split can misrepresent your results! If you overfit on your training data the evaluation results on that split will look great but the model will perform poorly on new data. Depending on the size of the dataset, you can keep anywhere from 10-30% for evaluation and the rest for training, while aiming to set up the test set to reflect the production data as close as possible. Check out [this thread](https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090) for a more in-depth discussion of dataset splitting! ## The impact of class imbalance\nWhile many academic datasets, such as the [IMDb dataset](https://huggingface.co/datasets/imdb) of movie reviews, are perfectly balanced, most real-world datasets are not. In machine learning a \\*balanced dataset\\* corresponds to a datasets where all labels are represented equally. In the case of the IMDb dataset this means that there are as many positive as negative reviews in the dataset. In an imbalanced dataset this is not the case: in fraud detection for example there are usually many more non-fraud cases than fraud cases in the dataset. Having an imbalanced dataset can skew the results of your metrics. Imagine a dataset with 99 \""non-fraud\"" cases and 1 \""fraud\"" case. A simple model that always predicts \""non-fraud\"" cases would give yield a 99% accuracy which might sound good at first until you realize that you will never catch a fraud case.""]","The three main steps for fine-tuning a model with the ðŸ¤— Datasets library are:

1. Load a dataset from the Hugging Face Hub.
2. Preprocess the data with `Dataset.map()`.
3. Load and compute metrics.",1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.
"What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?
","[""It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark. Figure 4. Latency (Batch=1, Physical Cores=2)\n\n---\n## Conclusion\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput. Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens. The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial)\n## Resources\n\\* [Hugging Face Infinity](https://huggingface.co/infinity)\n\\* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\n\\* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i)\n\\* [DistilBERT](https://huggingface.co/docs/transformers/model\\_doc/distilbert)\n\\* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\\* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\n\\* [ðŸ¤— Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)"", ""In addition to superior performance for machine learning workloads, the Intel Ice Lake C6i instances offer great cost-performance and are our recommendation to deploy Infinity on Amazon Web Services. To learn more, visit the [EC2 C6i instance](https://aws.amazon.com/ec2/instance-types/c6i) page. ### Methodologies\nWhen it comes to benchmarking BERT-like models, two metrics are most adopted:\n\\* \\*\\*Latency\\*\\*: Time it takes for a single prediction of the model (pre-process, prediction, post-process)\n\\* \\*\\*Throughput\\*\\*: Number of executions performed in a fixed amount of time for one benchmark configuration, respecting Physical CPU cores, Sequence Length, and Batch Size\nThese two metrics will be used to benchmark Hugging Face Infinity across different setups to understand the benefits and tradeoffs in this blog post. ---\n## Results\nTo run the benchmark, we created an infinity container for the [EC2 C6i instance](https://aws.amazon.com/ec2/instance-types/c6i) (Ice-lake) and optimized a [DistilBERT](https://huggingface.co/docs/transformers/model\\_doc/distilbert) model for sequence classification using Infinity Multiverse. This ice-lake optimized Infinity Container can achieve up to 34% better latency & throughput compared to existing cascade-lake-based instances, and up to 800% better latency & throughput compared to vanilla transformers running on ice-lake. The Benchmark we created consists of 192 different experiments and configurations. We ran experiments for:\n\\* Physical CPU cores: 1, 2, 4, 8\n\\* Sequence length: 8, 16, 32, 64, 128, 256, 384, 512\n\\* Batch\\_size: 1, 2, 4, 8, 16, 32\nIn each experiment, we collect numbers for:\n\\* Throughput (requests per second)\n\\* Latency (min, max, avg, p90, p95, p99)\nYou can find the full data of the benchmark in this google spreadsheet: [ðŸ¤— Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing). In this blog post, we will highlight a few results of the benchmark including the best latency and throughput configurations. In addition to this, we deployed the [DistilBERT](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion) model we used for the benchmark as an API endpoint on 2 physical cores."", ""You can test it and get a feeling for the performance of Infinity. Below you will find a `curl` command on how to send a request to the hosted endpoint. The API returns a `x-compute-time` HTTP Header, which contains the duration of the end-to-end pipeline. ```bash\ncurl --request POST `-i` \\\n--url https://infinity.huggingface.co/cpu/distilbert-base-uncased-emotion \\\n--header 'Content-Type: application/json' \\\n--data '{\""inputs\"":\""I like you. I love you\""}'\n```\n### Throughput\nBelow you can find the throughput comparison for running infinity on 2 physical cores with batch size 1, compared with vanilla transformers. Figure 3. Throughput: Infinity vs Transformers\n\n| Sequence Length | Infinity | Transformers | improvement |\n|-----------------|-------------|--------------|-------------|\n| 8 | 248 req/sec | 49 req/sec | +506% |\n| 16 | 212 req/sec | 50 req/sec | +424% |\n| 32 | 150 req/sec | 40 req/sec | +375% |\n| 64 | 97 req/sec | 28 req/sec | +346% |\n| 128 | 55 req/sec | 18 req/sec | +305% |\n| 256 | 27 req/sec | 9 req/sec | +300% |\n| 384 | 17 req/sec | 5 req/sec | +340% |\n| 512 | 12 req/sec | 4 req/sec | +300% |\n### Latency\nBelow, you can find the latency results for an experiment running Hugging Face Infinity on 2 Physical Cores with Batch Size 1.""]",Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.,+800%
"What is the command to upload a spaCy pipeline to the Hugging Face Hub?
","[""Defaults to `hub` in the current working directory. |\n| `--verbose`, `-V` | bool | Output additional info for debugging, e.g. the full generated hub metadata. |\nYou can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`. ```bash\nhuggingface-cli login\npython -m spacy package ./en\\_ner\\_fashion ./output --build wheel\ncd ./output/en\\_ner\\_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en\\_ner\\_fashion-0.0.0-py3-none-any.whl\n```\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card. The command will output two things:\n\\* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en\\_core\\_web\\_sm\n\\* And how to install the pipeline directly from the Hub! ### From a Python script\nYou can use the `push` function from Python. It returns a dictionary containing the `\""url\""` and \""`whl\\_url`\"" of the published model and the wheel file, which you can later install with `pip install`. ```py\nfrom spacy\\_huggingface\\_hub import push\nresult = push(\""./en\\_ner\\_fashion-0.0.0-py3-none-any.whl\"")\nprint(result[\""url\""])\n```\n## Additional resources\n\\* spacy-huggingface-hub [library](https://github.com/explosion/spacy-huggingface-hub)."", ""Using spaCy at Hugging Face\n`spaCy` is a popular library for advanced Natural Language Processing used widely across industry. `spaCy` makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text. ## Exploring spaCy models in the Hub\nThe official models from `spaCy` 3.3 are in the `spaCy` [Organization Page](https://huggingface.co/spacy). Anyone in the community can also share their `spaCy` models, which you can find by filtering at the left of the [models page](https://huggingface.co/models?library=spacy). All models on the Hub come up with useful features\n1. An automatically generated model card with label scheme, metrics, components, and more. 2. An evaluation sections at top right where you can look at the metrics. 3. Metadata tags that help for discoverability and contain information such as license and language. 4. An interactive widget you can use to play out with the model directly in the browser\n5. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_widget.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_widget-dark.png)\n\n## Using existing models\nAll `spaCy` models from the Hub can be directly installed using pip install. ```bash\npip install https://huggingface.co/spacy/en\\_core\\_web\\_sm/resolve/main/en\\_core\\_web\\_sm-any-py3-none-any.whl\n```\nTo find the link of interest, you can go to a repository with a `spaCy` model. When you open the repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model! ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_snippet.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_snippet-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_snippet2.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_snippet2-dark.png)\n\nOnce installed, you can load the model as any spaCy pipeline. ```python\n# Using spacy.load(). import spacy\nnlp = spacy.load(\""en\\_core\\_web\\_sm\"")\n# Importing as module. import en\\_core\\_web\\_sm\nnlp = en\\_core\\_web\\_sm.load()\n```\n## Sharing your models\n### Using the spaCy CLI (recommended)\nThe `spacy-huggingface-hub` library extends `spaCy` native CLI so people can easily push their packaged models to the Hub."", ""--\ntitle: \""Welcome spaCy to the Hugging Face Hub\""\nthumbnail: /blog/assets/23\\_spacy/thumbnail.png\nauthors:\n- user: osanseviero\n- user: ines\n---\n# Welcome spaCy to the Hugging Face Hub\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text. Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production! ### Finding models\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy. ### Widgets\nThis integration includes support for NER widgets, so all models with a NER component will have this out of the box! Coming soon there will be support for text classification and POS. [`spacy/en_core_web_sm`](/spacy/en_core_web_sm)\n\nHosted inference API\n\nToken Classification\n\nCompute\n\nThis model is currently loaded and running on the Inference API. JSON Output \nMaximize\n\n### Using existing models\nAll models from the Hub can be directly installed using `pip install`. ```bash\npip install https://huggingface.co/spacy/en\\_core\\_web\\_sm/resolve/main/en\\_core\\_web\\_sm-any-py3-none-any.whl\n```\n```python\n# Using spacy.load(). import spacy\nnlp = spacy.load(\""en\\_core\\_web\\_sm\"")\n# Importing as module. import en\\_core\\_web\\_sm\nnlp = en\\_core\\_web\\_sm.load()\n```\nWhen you open a repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model!""]","The command to upload a spaCy pipeline to the Hugging Face Hub is as follows:

```bash
huggingface-cli login
python -m spacy package ./en_ner_fashion ./output --build wheel
cd ./output/en_ner_fashion-0.0.0/dist
python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
```

This sequence of commands logs in to Hugging Face, packages the spaCy model, and then pushes it to the Hub.",python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
"What is the time and memory complexity of the NystrÃ¶mformer's approximation of self-attention?
","[""--\ntitle: \""NystrÃ¶mformer: Approximating self-attention in linear time and memory via the NystrÃ¶m method\""\nthumbnail: /blog/assets/86\\_nystromformer/thumbnail.png\nauthors:\n- user: asi\nguest: true\n---\n# NystrÃ¶mformer: Approximating self-attention in linear time and memory via the NystrÃ¶m method\n## Introduction\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. The [NystrÃ¶mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. NystrÃ¶mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the NystrÃ¶m method and how it can be adapted to approximate self-attention. ## NystrÃ¶m method for matrix approximation\nAt the heart of NystrÃ¶mformer is the NystrÃ¶m method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the NystrÃ¶m method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:\n\nRepresenting P as a block matrix\n\nWe now have four submatrices: \\\\(A\\_P, B\\_P, F\\_P,\\\\) and \\\\(C\\_P\\\\), with sizes \\\\(m \\times m, m \\times (n - m), (n - m) \\times m\\\\) and\n\\\\((n - m) \\times (n - m)\\\\) respectively. The \\\\(m\\\\) sampled columns are contained in \\\\(A\\_P\\\\) and \\\\(F\\_P\\\\), whereas the \\\\(m\\\\) sampled rows are contained in \\\\(A\\_P\\\\) and \\\\(B\\_P\\\\). So, the entries of \\\\(A\\_P, B\\_P,\\\\) and \\\\(F\\_P\\\\) are known to us, and we will estimate \\\\(C\\_P\\\\). According to the NystrÃ¶m method, \\\\(C\\_P\\\\) is given by:\n$$C\\_P = F\\_P A\\_P^+ B\\_P$$\nHere, \\\\(+\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). Thus, the NystrÃ¶m approximation of \\\\(P, \\hat{P}\\\\) can be written as:\n\nNystrÃ¶m approximation of P\n\nAs shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices."", ""The abstract from the paper is the following:\n\\*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\ncommunity. To address this limitation, we propose NystrÃ¶mformer -- a model that exhibits favorable scalability as a\nfunction of sequence length. Our idea is based on adapting the NystrÃ¶m method to approximate standard self-attention\nwith O(n) complexity. The scalability of NystrÃ¶mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\nsequence length, and find that our NystrÃ¶mformer performs comparably, or in a few cases, even slightly better, than\nstandard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, NystrÃ¶mformer performs\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.\\*\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/Nystromformer). ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## NystromformerConfig\n[[autodoc]] NystromformerConfig\n## NystromformerModel\n[[autodoc]] NystromformerModel\n- forward\n## NystromformerForMaskedLM\n[[autodoc]] NystromformerForMaskedLM\n- forward\n## NystromformerForSequenceClassification\n[[autodoc]] NystromformerForSequenceClassification\n- forward\n## NystromformerForMultipleChoice\n[[autodoc]] NystromformerForMultipleChoice\n- forward\n## NystromformerForTokenClassification\n[[autodoc]] NystromformerForTokenClassification\n- forward\n## NystromformerForQuestionAnswering\n[[autodoc]] NystromformerForQuestionAnswering\n- forward"", ""Also, notice that there is a DConv box. This corresponds to a skip connection added to the values using a 1D depthwise convolution. ## How is NystrÃ¶mformer implemented? The original implementation of NystrÃ¶mformer can be found [here](https://github.com/mlpen/Nystromformer) and the HuggingFace implementation can be found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/nystromformer/modeling\\_nystromformer.py). Let's take a look at a few lines of code (with some comments added) from the HuggingFace implementation. Note that some details such as normalization, attention masking, and depthwise convolution are avoided for simplicity. ```python\nkey\\_layer = self.transpose\\_for\\_scores(self.key(hidden\\_states)) # K\nvalue\\_layer = self.transpose\\_for\\_scores(self.value(hidden\\_states)) # V\nquery\\_layer = self.transpose\\_for\\_scores(mixed\\_query\\_layer) # Q\nq\\_landmarks = query\\_layer.reshape(\n-1,\nself.num\\_attention\\_heads,\nself.num\\_landmarks,\nself.seq\\_len // self.num\\_landmarks,\nself.attention\\_head\\_size,\n).mean(dim=-2) # \\tilde{Q}\nk\\_landmarks = key\\_layer.reshape(\n-1,\nself.num\\_attention\\_heads,\nself.num\\_landmarks,\nself.seq\\_len // self.num\\_landmarks,\nself.attention\\_head\\_size,\n).mean(dim=-2) # \\tilde{K}\nkernel\\_1 = torch.nn.functional.softmax(torch.matmul(query\\_layer, k\\_landmarks.transpose(-1, -2)), dim=-1) # \\tilde{F}\nkernel\\_2 = torch.nn.functional.softmax(torch.matmul(q\\_landmarks, k\\_landmarks.transpose(-1, -2)), dim=-1) # \\tilde{A} before pseudo-inverse\nattention\\_scores = torch.matmul(q\\_landmarks, key\\_layer.transpose(-1, -2)) # \\tilde{B} before softmax\nkernel\\_3 = nn.functional.softmax(attention\\_scores, dim=-1) # \\tilde{B}\nattention\\_probs = torch.matmul(kernel\\_1, self.iterative\\_inv(kernel\\_2)) # \\tilde{F} \\* \\tilde{A}\nnew\\_value\\_layer = torch.matmul(kernel\\_3, value\\_layer) # \\tilde{B} \\* V\ncontext\\_layer = torch.matmul(attention\\_probs, new\\_value\\_layer) # \\tilde{F} \\* \\tilde{A} \\* \\tilde{B} \\* V\n```\n## Using NystrÃ¶mformer with HuggingFace\nNystrÃ¶mformer for Masked Language Modeling (MLM) is available on HuggingFace. Currently, there are 4 checkpoints, corresponding to various sequence lengths: [`nystromformer-512`](https://huggingface.co/uw-madison/nystromformer-512), [`nystromformer-1024`](https://huggingface.co/uw-madison/nystromformer-1024), [`nystromformer-2048`](https://huggingface.co/uw-madison/nystromformer-2048), and [`nystromformer-4096`](https://huggingface.co/uw-madison/nystromformer-4096). The number of landmarks, \\\\(m\\\\), can be controlled using the `num\\_landmarks` parameter in the [`NystromformerConfig`](https://huggingface.co/docs/transformers/v4.18.0/en/model\\_doc/nystromformer#transformers.NystromformerConfig). Let's take a look at a minimal example of NystrÃ¶mformer for MLM:\n```python\nfrom transformers import AutoTokenizer, NystromformerForMaskedLM\nimport torch\ntokenizer = AutoTokenizer.from\\_pretrained(\""uw-madison/nystromformer-512\"")\nmodel = NystromformerForMaskedLM.from\\_pretrained(\""uw-madison/nystromformer-512\"")\ninputs = tokenizer(\""Paris is the [MASK] of France.\"", return\\_tensors=\""pt\"")\nwith torch.no\\_grad():\nlogits = model(\\*\\*inputs).logits\n# retrieve index of [MASK]\nmask\\_token\\_index = (inputs.input\\_ids == tokenizer.mask\\_token\\_id)[0].nonzero(as\\_tuple=True)[0]\npredicted\\_token\\_id = logits[0, mask\\_token\\_index].argmax(axis=-1)\ntokenizer.decode(predicted\\_token\\_id)\n```\n\nOutput:\n----------------------------------------------------------------------------------------------------\ncapital\n\nAlternatively, we can use the [pipeline API](https://huggingface.co/docs/transformers/main\\_classes/pipelines) (which handles all the complexity for us):\n```python\nfrom transformers import pipeline\nunmasker = pipeline('fill-mask', model='uw-madison/nystromformer-512')\nunmasker(\""Paris is the [MASK] of France.\"")\n```\n\nOutput:\n----------------------------------------------------------------------------------------------------\n[{'score': 0.829957902431488,\n'token': 1030,\n'token\\_str': 'capital',\n'sequence': 'paris is the capital of france.'},\n{'score': 0.022157637402415276,\n'token': 16081,\n'token\\_str': 'birthplace',\n'sequence': 'paris is the birthplace of france.'},\n{'score': 0.01904447190463543,\n'token': 197,\n'token\\_str': 'name',\n'sequence': 'paris is the name of france.'},\n{'score': 0.017583081498742104,\n'token': 1107,\n'token\\_str': 'kingdom',\n'sequence': 'paris is the kingdom of france.'},\n{'score': 0.005948934704065323,\n'token': 148,\n'token\\_str': 'city',\n'sequence': 'paris is the city of france.'}]\n\n## Conclusion\nNystrÃ¶mformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other linear self-attention schemes. In this blog post, we went over a high-level overview of the NystrÃ¶m method and how it can be leveraged for self-attention.""]","The NystrÃ¶mformer's approximation of self-attention has a time and memory complexity of \(O(n)\), where \(n\) is the length of the input sequence. This is an improvement over the standard self-attention mechanism, which has a complexity of \(O(n^2)\).",O(n)
"What is the goal of the Named Entity Recognition task in token classification?
","[""p align=\""center\""> ![FSNER LOGO](http://sayef.tech:8082/uploads/FSNER-LOGO-2.png) \n\nImplemented by  [sayef](https://huggingface.co/sayef) . ## Overview\nThe FSNER model was proposed in [Example-Based Named Entity Recognition](https://arxiv.org/abs/2008.10570) by Morteza Ziyadi, Yuting Sun, Abhishek Goswami, Jade Huang, Weizhu Chen. To identify entity spans in a new domain, it uses a train-free few-shot learning approach inspired by question-answering. ## Abstract\n----\n> We present a novel approach to named entity recognition (NER) in the presence of scarce data that we call example-based NER. Our train-free few-shot learning approach takes inspiration from question-answering to identify entity spans in a new and unseen domain. In comparison with the current state-of-the-art, the proposed method performs significantly better, especially when using a low number of support examples. ## Model Training Details\n-----\n| identifier | epochs | datasets |\n| ---------- |:----------:| :-----:|\n| [sayef/fsner-bert-base-uncased](https://huggingface.co/sayef/fsner-bert-base-uncased) | 10 | ontonotes5, conll2003, wnut2017, and fin (Alvarado et al.). |\n## Installation and Example Usage\n------\nYou can use the FSNER model in 3 ways:\n1. Install directly from PyPI: `pip install fsner` and import the model as shown in the code example below\nor\n2. Install from source: `python setup.py install` and import the model as shown in the code example below\nor\n3. Clone repo and change directory to `src` and import the model as shown in the code example below\n```python\nfrom fsner import FSNERModel, FSNERTokenizerUtils\nmodel = FSNERModel(\""sayef/fsner-bert-base-uncased\"")\ntokenizer = FSNERTokenizerUtils(\""sayef/fsner-bert-base-uncased\"")\n# size of query and supports must be the same."", ""Named-Entity Recognition\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert\\_ner\\_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial\\_Analyst\\_AI\nTags: NER, TEXT, HIGHLIGHT\n## Introduction\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \""token\"") into different categories, such as names of people or names of locations, or different parts of speech. For example, given the sentence:\n> Does Chicago have any Pakistani restaurants? A named-entity recognition algorithm may identify:\n- \""Chicago\"" as a \\*\\*location\\*\\*\n- \""Pakistani\"" as an \\*\\*ethnicity\\*\\*\nand so on. Using `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team. Here is an example of a demo that you'll be able to build:\n$demo\\_ner\\_pipeline\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!"", ""ote: the following transcripts are associated with Merve Noyan's videos in the Hugging Face Tasks playlist: https://www.youtube.com/playlist?list=PLo2EIpI\\_JMQtyEr-sLJSy5\\_SnLCb4vtQf\nToken Classification video\nWelcome to the Hugging Face tasks series! In this video weâ€™ll take a look at the token classification task. Token classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging. Letâ€™s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity. Another token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech. Token classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score.""]","The goal of the Named Entity Recognition (NER) task in token classification is to find the entities in a piece of text, such as names of people, locations, or organizations. This task is formulated as labeling each token with one class for each entity, and another class for tokens that have no entity.","The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization."
"What is the resolution of images used by the CLIPSeg model?
","[""For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm\\_source=hf&utm\\_medium=blog&utm\\_campaign=clipseg). Before we describe how to do that, letâ€™s first take a look at how CLIPSeg works. ## CLIP: the magic model behind CLIPSeg\n[CLIP](https://huggingface.co/docs/transformers/main/en/model\\_doc/clip), which stands for \\*\\*C\\*\\*ontrastive \\*\\*L\\*\\*anguageâ€“\\*\\*I\\*\\*mage \\*\\*P\\*\\*re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract \\*representation\\* of your input. This abstract representation, also called an \\*embedding\\*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together)."", ""It allows a computer to not only know what is in an image (classification), where objects are in the image (detection), but also what the outlines of those objects are. Knowing the outlines of objects is essential in fields such as robotics and autonomous driving. For example, a robot has to know the shape of an object to grab it correctly. Segmentation can also be combined with [image inpainting](https://t.co/5q8YHSOfx7) to allow users to describe which part of the image they want to replace. One limitation of most image segmentation models is that they only work with a fixed list of categories. For example, you cannot simply use a segmentation model trained on oranges to segment apples. To teach the segmentation model an additional category, you have to label data of the new category and train a new model, which can be costly and time-consuming. But what if there was a model that can already segment almost any kind of object, without any further training? Thatâ€™s exactly what [CLIPSeg](https://arxiv.org/abs/2112.10003), a zero-shot segmentation model, achieves. Currently, CLIPSeg still has its limitations."", ""Now you see why CLIP was truly a breakthrough in machine learning. The reason why CLIP works so well is that the model was trained on a huge dataset of images with text captions. The dataset contained a whopping 400 million image-text pairs taken from the internet. These images contain a wide variety of objects and concepts, and CLIP is great at creating a representation for each of them. ## CLIPSeg: image segmentation with CLIP\n[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model that uses CLIP representations to create image segmentation masks. It was published by Timo LÃ¼ddecke and Alexander Ecker. They achieved zero-shot image segmentation by training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesnâ€™t only use the final CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of CLIP. [Source](https://arxiv.org/abs/2112.10003)\n\nThe decoder is trained on the [PhraseCut dataset](https://arxiv.org/abs/2008.01187), which contains over 340,000 phrases with corresponding image segmentation masks.""]","The CLIPSeg model uses images of 352 x 352 pixels, which results in relatively low-resolution outputs.",352 x 352 pixels
"What can you use Gradio for?
","[""Key Features\nLet's go through some of the key features of Gradio. This guide is intended to be a high-level overview of various things that you should be aware of as you build your demo. Where appropriate, we link to more detailed guides on specific topics. 1. [Components](#components)\n2. [Queuing](#queuing)\n3. [Streaming outputs](#streaming-outputs)\n4. [Streaming inputs](#streaming-inputs)\n5. [Alert modals](#alert-modals)\n6. [Styling](#styling)\n7. [Progress bars](#progress-bars)\n8. [Batch functions](#batch-functions)\n## Components\nGradio includes more than 30 pre-built components (as well as many user-built \\_custom components\\_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on. Each component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[\""webcam\"", \""upload\""]`). \\*\\*Static and Interactive Components\\*\\*\nEvery component has a \\_static\\_ version that is designed to \\*display\\* data, and most components also have an \\_interactive\\_ version designed to let users input or modify the data. Typically, you don't need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports. \\*\\*Preprocessing and Postprocessing\\*\\*\nWhen a component is used as an input, Gradio automatically handles the \\_preprocessing\\_ needed to convert the data from a type sent by the user's browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array). Similarly, when a component is used as an output, Gradio automatically handles the \\_postprocessing\\_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (a gallery of images). Consider an example demo with three input components (`gr.Textbox`, `gr.Number`, and `gr.Image`) and two outputs (`gr.Number` and `gr.Gallery`) that serve as a UI for your image-to-image generation model. Below is a diagram of what our preprocessing will send to the model and what our postprocessing will require from it. ![](https://github.com/gradio-app/gradio/blob/main/guides/assets/dataflow.svg?raw=true)\nIn this image, the following preprocessing steps happen to send the data from the browser to your function:\n\\* The text in the textbox is converted to a Python `str` (essentially no preprocessing)\n\\* The number in the number input in converted to a Python `float` (essentially no preprocessing)\n\\* Most importantly, ihe image supplied by the user is converted to a `numpy.array` representation of the RGB values in the image\nImages are converted to NumPy arrays because they are a common format for machine learning workflows. You can control the \\_preprocessing\\_ using the component's parameters when constructing the component. For example, if you instantiate the `Image` component with the following parameters, it will preprocess the image to the `PIL` format instead:\n```py\nimg = gr.Image(type=\""pil\"")\n```\nPostprocessing is even simpler! Gradio automatically recognizes the format of the returned data (e.g. does the user's function return a `numpy` array or a `str` filepath for the `gr.Image` component?) and postprocesses it appropriately into a format that can be displayed by the browser. So in the image above, the following postprocessing steps happen to send the data returned from a user's function to the browser:\n\\* The `float` is displayed as a number and displayed directly to the user\n\\* The list of string filepaths (`list[str]`) is interpreted as a list of image filepaths and displayed as a gallery in the browser\nTake a look at the [Docs](https://gradio.app/docs) to see all the parameters for each Gradio component."", ""Gradio, check![[gradio-check]]\nThis wraps up the chapter on building cool ML demos with Gradio - we hope you enjoyed it! To recap, in this chapter we learned:\n- How to create Gradio demos with the high-level `Interface` API, and how to configure different input and output modalities. - Different ways to share Gradio demos, through temporary links and hosting on [Hugging Face Spaces](https://huggingface.co/spaces). - How to integrate Gradio demos with models and Spaces on the Hugging Face Hub. - Advanced features like storing state in a demo or providing authentication. - How to have full control of the data flow and layout of your demo with Gradio Blocks. If you'd like to test your understanding of the concepts covered in this chapter, check out the quiz in the next section! ## Where to next?[[where-to-next]]\nIf you want to learn more about Gradio you can\n- Take a look at [Demos](https://github.com/gradio-app/gradio/tree/main/demo) in the repo, there are quite a lot of examples there. - See the [Guides](https://gradio.app/guides/) page, where you can find guides about cool and advanced features."", ""### An Overview of Gradio\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do? #### Chatbots with `gr.ChatInterface`\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast). #### Custom Demos with `gr.Blocks`\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction â€” still all in Python. You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners). #### The Gradio Python & JavaScript Ecosystem\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\\* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio\\_client`): query any Gradio app programmatically in Python. \\* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript. \\* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \\* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications â€” for free!""]","Gradio can be used for creating interactive demos for machine learning models and applications. Some specific uses include:

1. **Building Demos**: Quickly create demos using the high-level `Interface` API, allowing you to configure different input and output modalities.

2. **Chatbots**: Use the `gr.ChatInterface` class to create chatbot UIs easily.

3. **Custom Applications**: Design more complex web applications with flexible layouts and data flows using the `gr.Blocks` class.

4. **Integration with Models**: Integrate Gradio demos with models and Spaces on the Hugging Face Hub.

5. **Sharing Demos**: Share your Gradio demos through temporary links or host them on Hugging Face Spaces.

6. **Advanced Features**: Utilize features like storing state in a demo, providing authentication, and controlling data flow and layout.

Gradio is versatile and can be used for a wide range of machine learning applications, making it easier to showcase and interact with models.","Create a demo for your machine learning model, share your machine learning model with others, and debug your model."
"What TensorFlow API function is used to load a saved tensor file?
","[""Tensorflow API\n[[autodoc]] safetensors.tensorflow.load\\_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save\\_file\n[[autodoc]] safetensors.tensorflow.save"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Load safetensors\n[[open-in-colab]]\n[safetensors](https://github.com/huggingface/safetensors) is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or \\*pickled\\* into a `.bin` file with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights. This guide will show you how you load `.safetensor` files, and how to convert Stable Diffusion model weights stored in other formats to `.safetensor`. Before you start, make sure you have safetensors installed:\n```py\n# uncomment to install the necessary libraries in Colab\n#!pip install safetensors\n```\nIf you look at the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main) repository, you'll see weights inside the `text\\_encoder`, `unet` and `vae` subfolders are stored in the `.safetensors` format. By default, ðŸ¤— Diffusers automatically loads these `.safetensors` files from their subfolders if they're available in the model repository. For more explicit control, you can optionally set `use\\_safetensors=True` (if `safetensors` is not installed, you'll get an error message asking you to install it):\n```py\nfrom diffusers import DiffusionPipeline\npipeline = DiffusionPipeline.from\\_pretrained(\""runwayml/stable-diffusion-v1-5\"", use\\_safetensors=True)\n```\nHowever, model weights are not necessarily stored in separate subfolders like in the example above. Sometimes, all the weights are stored in a single `.safetensors` file. In this case, if the weights are Stable Diffusion weights, you can load the file directly with the [`~diffusers.loaders.FromSingleFileMixin.from\\_single\\_file`] method:\n```py\nfrom diffusers import StableDiffusionPipeline\npipeline = StableDiffusionPipeline.from\\_single\\_file(\n\""https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\""\n)\n```\n## Convert to safetensors\nNot all weights on the Hub are available in the `.safetensors` format, and you may encounter weights stored as `.bin`."", ""!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-logo-light.svg)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-logo-dark.svg)\n\n# Safetensors\nSafetensors is a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy). Safetensors is really [fast ðŸš€](./speed). ## Installation\nwith pip:\n```\npip install safetensors\n```\nwith conda:\n```\nconda install -c huggingface safetensors\n```\n## Usage\n### Load tensors\n```python\nfrom safetensors import safe\\_open\ntensors = {}\nwith safe\\_open(\""model.safetensors\"", framework=\""pt\"", device=0) as f:\nfor k in f.keys():\ntensors[k] = f.get\\_tensor(k)\n```\nLoading only part of the tensors (interesting when running on multiple GPU)\n```python\nfrom safetensors import safe\\_open\ntensors = {}\nwith safe\\_open(\""model.safetensors\"", framework=\""pt\"", device=0) as f:\ntensor\\_slice = f.get\\_slice(\""embedding\"")\nvocab\\_size, hidden\\_dim = tensor\\_slice.get\\_shape()\ntensor = tensor\\_slice[:, :hidden\\_dim]\n```\n### Save tensors\n```python\nimport torch\nfrom safetensors.torch import save\\_file\ntensors = {\n\""embedding\"": torch.zeros((2, 2)),\n\""attention\"": torch.zeros((2, 3))\n}\nsave\\_file(tensors, \""model.safetensors\"")\n```\n## Format\nLet's say you have safetensors file named `model.safetensors`, then `model.safetensors` will have the following internal format:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-format.svg)\n\n## Featured Projects\nSafetensors is being used widely at leading AI enterprises, such as [Hugging Face](https://huggingface.co/), [EleutherAI](https://www.eleuther.ai/), and [StabilityAI](https://stability.ai/). Here is a non-exhaustive list of projects that are using safetensors:\n\\* [huggingface/transformers](https://github.com/huggingface/transformers)\n\\* [AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)\n\\* [Llama-cpp](https://github.com/ggerganov/llama.cpp/blob/e6a46b0ed1884c77267dc70693183e3b7164e0e0/convert.py#L537)\n\\* [microsoft/TaskMatrix](https://github.com/microsoft/TaskMatrix)\n\\* [hpcaitech/ColossalAI](https://github.com/hpcaitech/ColossalAI)\n\\* [huggingface/pytorch-image-models](https://github.com/huggingface/pytorch-image-models)\n\\* [CivitAI](https://civitai.com/)\n\\* [huggingface/diffusers](https://github.com/huggingface/diffusers)\n\\* [coreylowman/dfdx](https://github.com/coreylowman/dfdx)\n\\* [invoke-ai/InvokeAI](https://github.com/invoke-ai/InvokeAI)\n\\* [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)\n\\* [Sanster/lama-cleaner](https://github.com/Sanster/lama-cleaner)\n\\* [PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n\\* [AIGC-Audio/AudioGPT](https://github.com/AIGC-Audio/AudioGPT)\n\\* [brycedrennan/imaginAIry](https://github.com/brycedrennan/imaginAIry)\n\\* [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)\n\\* [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE)\n\\* [alvarobartt/safejax](https://github.com/alvarobartt/safejax)\n\\* [MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)\n\\* [LaurentMazare/tch-rs](https://github.com/LaurentMazare/tch-rs)\n\\* [chainyo/tensorshare](https://github.com/chainyo/tensorshare)""]",The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`.,safetensors.tensorflow.load_file
"Where can you access the logs of your Endpoints in Hugging Face Endpoints?
","[""Access and read Logs\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the â€œLogsâ€ tab of your Endpoint. You will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference. ![select logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png)\nThe Container Logs are only available when your Endpoint is in the â€œRunningâ€ state. \\_Note: If your Endpoint creation is in the â€œFailedâ€ state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc.\\_\n\\*\\*Build Logs:\\*\\*\n![build logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png)\n\\*\\*Container Logs:\\*\\*\n![container logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png)"", ""Access and view Metrics\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the â€œAnalyticsâ€ tab of your Endpoints. ![metric dashboard](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png)\n## Access Metrics via API\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api\\_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints."", ""Once I'm done testing, I delete the endpoints that I've created to avoid unwanted charges. I also delete the VPC Endpoint in the AWS console. Hugging Face customers are already using Inference Endpoints. For example, [Phamily](https://phamily.com/), the #1 in-house chronic care management & proactive care platform, [told us](https://www.youtube.com/watch?v=20C9X5OYO2Q) that Inference Endpoints is helping them simplify and accelerate HIPAA-compliant Transformer deployments. ### Now it's your turn! Thanks to Inference Endpoints, you can deploy production-grade, scalable, secure endpoints in minutes, in just a few clicks. Why don't you [give it a try](https://ui.endpoints.huggingface.co/new)? We have plenty of ideas to make the service even better, and we'd love to hear your feedback in the [Hugging Face forum](https://discuss.huggingface.co/). Thank you for reading and have fun with Inference Endpoints!""]","You can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the â€œLogsâ€ tab of your Endpoint. You will have access to the build logs of your Image artifacts as well as the Container Logs during inference. The Container Logs are only available when your Endpoint is in the â€œRunningâ€ state. If your Endpoint creation is in the â€œFailedâ€ state, you can check the Build Logs to see the reason for the failure.","In the ""Logs"" tab of your Endpoint through the UI."
"What is the latest task added to Hugging Face AutoTrain for Computer Vision?
","[""--\ntitle: Image Classification with AutoTrain\nthumbnail: /blog/assets/105\\_autotrain-image-classification/thumbnail.png\nauthors:\n- user: nimaboscarino\n---\n# Image Classification with AutoTrain\nSo youâ€™ve heard all about the cool things that are happening in the machine learning world, and you want to join in. Thereâ€™s just one problem â€“ you donâ€™t know how to code! ðŸ˜± Or maybe youâ€™re a seasoned software engineer who wants to add some ML to your side-project, but you donâ€™t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. Thatâ€™s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature weâ€™ve just added, weâ€™re making â€œno-codeâ€ machine learning better than ever. Best of all, you can create your first project for âœ¨ free! âœ¨\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with \\*\\*zero\\*\\* configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer ðŸ¤¯ Weâ€™ve been expanding the number of tasks that we support, and weâ€™re proud to announce that \\*\\*you can now use AutoTrain for Computer Vision\\*\\*!"", ""For Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage. AutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate ðŸ¤— Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details. ## The technical philosophy\nIn this section, we wanted to share our philosophy behind adding support for Computer Vision in ðŸ¤— Transformers so that the community is aware of the design choices specific to this area. Even though Transformers started with NLP, we support multiple modalities today, for example â€“ vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:\n- Easy model download with a single line of code with `from\\_pretrained()`\n- Easy model upload with `push\\_to\\_hub()`\n- Support for loading huge checkpoints with efficient checkpoint sharding techniques\n- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))\n- Initialization from model configurations\n- Support for both PyTorch and TensorFlow (non-exhaustive)\n- and many more\nUnlike tokenizers, we have preprocessors (such as [this](https://huggingface.co/docs/transformers/model\\_doc/vit#transformers.ViTImageProcessor)) that take care of preparing data for the vision models. We have worked hard to ensure the user experience of using a vision model still feels easy and similar:\n```py\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nimport torch\nfrom datasets import load\\_dataset\ndataset = load\\_dataset(\""huggingface/cats-image\"")\nimage = dataset[\""test\""][\""image\""][0]\nimage\\_processor  = ViTImageProcessor.from\\_pretrained(\""google/vit-base-patch16-224\"")\nmodel = ViTForImageClassification.from\\_pretrained(\""google/vit-base-patch16-224\"")\ninputs = image\\_processor(image, return\\_tensors=\""pt\"")\nwith torch.no\\_grad():\nlogits = model(\\*\\*inputs).logits\n# model predicts one of the 1000 ImageNet classes\npredicted\\_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted\\_label])\n# Egyptian cat\n```\nEven for a difficult task like object detection, the user experience doesnâ€™t change very much:\n```py\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\nfrom PIL import Image\nimport requests\nurl = \""http://images.cocodataset.org/val2017/000000039769.jpg\""\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\\_processor = AutoImageProcessor.from\\_pretrained(\""microsoft/conditional-detr-resnet-50\"")\nmodel = AutoModelForObjectDetection.from\\_pretrained(\""microsoft/conditional-detr-resnet-50\"")\ninputs = image\\_processor(images=image, return\\_tensors=\""pt\"")\noutputs = model(\\*\\*inputs)\n# convert outputs (bounding boxes and class logits) to COCO API\ntarget\\_sizes = torch.tensor([image.size[::-1]])\nresults = image\\_processor.post\\_process\\_object\\_detection(\noutputs, threshold=0.5, target\\_sizes=target\\_sizes\n)[0]\nfor score, label, box in zip(results[\""scores\""], results[\""labels\""], results[\""boxes\""]):\nbox = [round(i, 2) for i in box.tolist()]\nprint(\nf\""Detected {model.config.id2label[label.item()]} with confidence \""\nf\""{round(score.item(), 3)} at location {box}\""\n)\n```\nLeads to:\n```bash\nDetected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\nDetected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\nDetected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\nDetected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\nDetected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\n```\n## Zero-shot models for vision\nThereâ€™s been a surge of models that reformulate core vision tasks like segmentation and detection in interesting ways and introduce even more flexibility."", ""--\ntitle: The State of Computer Vision at Hugging Face ðŸ¤—\nthumbnail: /blog/assets/cv\\_state/thumbnail.png\nauthors:\n- user: sayakpaul\n---\n# The State of Computer Vision at Hugging Face ðŸ¤—\nAt Hugging Face, we pride ourselves on democratizing the field of artificial intelligence together with the community. As a part of that mission, we began focusing our efforts on computer vision over the last year. What started as a [PR for having Vision Transformers (ViT) in ðŸ¤— Transformers](https://github.com/huggingface/transformers/pull/10950) has now grown into something much bigger â€“ 8 core vision tasks, over 3000 models, and over 100 datasets on the Hugging Face Hub. A lot of exciting things have happened since ViTs joined the Hub. In this blog post, weâ€™ll summarize what went down and whatâ€™s coming to support the continuous progress of Computer Vision from the ðŸ¤— ecosystem. Here is a list of things weâ€™ll cover:\n- [Supported vision tasks and Pipelines](#support-for-pipelines)\n- [Training your own vision models](#training-your-own-models)\n- [Integration with `timm`](#ðŸ¤—-ðŸ¤-timm)\n- [Diffusers](#ðŸ§¨-diffusers)\n- [Support for third-party libraries](#support-for-third-party-libraries)\n- [Deployment](#deployment)\n- and much more! ## Enabling the community: One task at a time ðŸ‘\nThe Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:\n- Image classification\n- Image segmentation\n- (Zero-shot) object detection\n- Video classification\n- Depth estimation\n- Image-to-image synthesis\n- Unconditional image generation\n- Zero-shot image classification\nEach of these tasks comes with at least 10 model checkpoints on the Hub for you to explore. Furthermore, we support [tasks](https://huggingface.co/tasks) that lie at the intersection of vision and language such as:\n- Image-to-text (image captioning, OCR)\n- Text-to-image\n- Document question-answering\n- Visual question-answering\nThese tasks entail not only state-of-the-art Transformer-based architectures such as [ViT](https://huggingface.co/docs/transformers/model\\_doc/vit), [Swin](https://huggingface.co/docs/transformers/model\\_doc/swin), [DETR](https://huggingface.co/docs/transformers/model\\_doc/detr) but also \\*pure convolutional\\* architectures like [ConvNeXt](https://huggingface.co/docs/transformers/model\\_doc/convnext), [ResNet](https://huggingface.co/docs/transformers/model\\_doc/resnet), [RegNet](https://huggingface.co/docs/transformers/model\\_doc/regnet), and more! Architectures like ResNets are still very much relevant for a myriad of industrial use cases and hence the support of these non-Transformer architectures in ðŸ¤— Transformers. Itâ€™s also important to note that the models on the Hub are not just from the Transformers library but also from other third-party libraries. For example, even though we support tasks like unconditional image generation on the Hub, we donâ€™t have any models supporting that task in Transformers yet (such as [this](https://huggingface.co/ceyda/butterfly\\_cropped\\_uniq1K\\_512)). Supporting all ML tasks, whether they are solved with Transformers or a third-party library is a part of our mission to foster a collaborative open-source Machine Learning ecosystem. ## Support for Pipelines\nWe developed [Pipelines](https://huggingface.co/docs/transformers/main/en/main\\_classes/pipelines) to equip practitioners with the tools they need to easily incorporate machine learning into their toolbox. They provide an easy way to perform inference on a given input with respect to a task. We have support for [seven vision tasks](https://huggingface.co/docs/transformers/main/en/main\\_classes/pipelines#computer-vision) in Pipelines. Here is an example of using Pipelines for depth estimation:\n```py\nfrom transformers import pipeline\ndepth\\_estimator = pipeline(task=\""depth-estimation\"", model=\""Intel/dpt-large\"")\noutput = depth\\_estimator(\""http://images.cocodataset.org/val2017/000000039769.jpg\"")\n# This is a tensor with the values being the depth expressed\n# in meters for each pixel\noutput[\""depth\""]\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/cv_state/depth_estimation_output.png)\n\nThe interface remains the same even for tasks like visual question-answering:\n```py\nfrom transformers import pipeline\noracle = pipeline(model=\""dandelin/vilt-b32-finetuned-vqa\"")\nimage\\_url = \""https://huggingface.co/datasets/mishig/sample\\_images/resolve/main/tiger.jpg\""\noracle(question=\""What's the animal doing?\"", image=image\\_url, top\\_k=1)\n# [{'score': 0.778620, 'answer': 'laying down'}]\n```\n## Training your own models\nWhile being able to use a model for off-the-shelf inference is a great way to get started, fine-tuning is where the community gets the most benefits.""]",The latest task added to Hugging Face AutoTrain for Computer Vision is **image classification**.,Image Classification
"What is the default repository type created by the `create_repo` function on Hugging Face Hub?
","[""Firstly, there are a few methods to manage repository creation, deletion, and others:\n```python no-format\nfrom huggingface\\_hub import (\n# User management\nlogin,\nlogout,\nwhoami,\n# Repository creation and management\ncreate\\_repo,\ndelete\\_repo,\nupdate\\_repo\\_visibility,\n# And some methods to retrieve/change information about the content\nlist\\_models,\nlist\\_datasets,\nlist\\_metrics,\nlist\\_repo\\_files,\nupload\\_file,\ndelete\\_file,\n)\n```\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them. The `create\\_repo` method can be used to create a new repository on the hub:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"")\n```\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"", organization=\""huggingface\"")\n```\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization."", ""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# åˆ›å»ºå’Œç®¡ç†å­˜å‚¨åº“\nHugging Face Hubæ˜¯ä¸€ç»„ Git å­˜å‚¨åº“ã€‚[Git](https://git-scm.com/)æ˜¯è½¯ä»¶å¼€å‘ä¸­å¹¿æ³›ä½¿ç”¨çš„å·¥å…·ï¼Œå¯ä»¥åœ¨åä½œå·¥ä½œæ—¶è½»æ¾å¯¹é¡¹ç›®è¿›è¡Œç‰ˆæœ¬æŽ§åˆ¶ã€‚æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä¸Ž Hub ä¸Šçš„å­˜å‚¨åº“è¿›è¡Œäº¤äº’ï¼Œç‰¹åˆ«å…³æ³¨ä»¥ä¸‹å†…å®¹ï¼š\n- åˆ›å»ºå’Œåˆ é™¤å­˜å‚¨åº“\n- ç®¡ç†åˆ†æ”¯å’Œæ ‡ç­¾\n- é‡å‘½åæ‚¨çš„å­˜å‚¨åº“\n- æ›´æ–°æ‚¨çš„å­˜å‚¨åº“å¯è§æ€§\n- ç®¡ç†å­˜å‚¨åº“çš„æœ¬åœ°å‰¯æœ¬\nå¦‚æžœæ‚¨ä¹ æƒ¯äºŽä½¿ç”¨ç±»ä¼¼äºŽGitLab/GitHub/Bitbucketç­‰å¹³å°ï¼Œæ‚¨å¯èƒ½é¦–å…ˆæƒ³åˆ°ä½¿ç”¨ `git`å‘½ä»¤è¡Œå·¥å…·æ¥å…‹éš†å­˜å‚¨åº“ï¼ˆ`git clone`ï¼‰ã€æäº¤æ›´æ”¹ï¼ˆ`git add` , ` git commit`ï¼‰å¹¶æŽ¨é€å®ƒä»¬ï¼ˆ`git push`ï¼‰ã€‚åœ¨ä½¿ç”¨ Hugging Face Hub æ—¶ï¼Œè¿™æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œè½¯ä»¶å·¥ç¨‹å’Œæœºå™¨å­¦ä¹ å¹¶ä¸å…·æœ‰ç›¸åŒçš„è¦æ±‚å’Œå·¥ä½œæµç¨‹ã€‚æ¨¡åž‹å­˜å‚¨åº“å¯èƒ½ä¼šç»´æŠ¤å¤§é‡æ¨¡åž‹æƒé‡æ–‡ä»¶ä»¥é€‚åº”ä¸åŒçš„æ¡†æž¶å’Œå·¥å…·ï¼Œå› æ­¤å…‹éš†å­˜å‚¨åº“ä¼šå¯¼è‡´æ‚¨ç»´æŠ¤å¤§é‡å ç”¨ç©ºé—´çš„æœ¬åœ°æ–‡ä»¶å¤¹ã€‚å› æ­¤ï¼Œä½¿ç”¨æˆ‘ä»¬çš„è‡ªå®šä¹‰HTTPæ–¹æ³•å¯èƒ½æ›´æœ‰æ•ˆã€‚æ‚¨å¯ä»¥é˜…è¯»æˆ‘ä»¬çš„[gitä¸ŽHTTPç›¸æ¯”è¾ƒ](../concepts/git\\_vs\\_http)è§£é‡Šé¡µé¢ä»¥èŽ·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯\nå¦‚æžœä½ æƒ³åœ¨Hubä¸Šåˆ›å»ºå’Œç®¡ç†ä¸€ä¸ªä»“åº“ï¼Œä½ çš„è®¡ç®—æœºå¿…é¡»å¤„äºŽç™»å½•çŠ¶æ€ã€‚å¦‚æžœå°šæœªç™»å½•ï¼Œè¯·å‚è€ƒ[æ­¤éƒ¨åˆ†](../quick-start#login)ã€‚åœ¨æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å‡è®¾ä½ çš„è®¡ç®—æœºå·²ç™»å½•\n## ä»“åº“åˆ›å»ºå’Œåˆ é™¤\nç¬¬ä¸€æ­¥æ˜¯äº†è§£å¦‚ä½•åˆ›å»ºå’Œåˆ é™¤ä»“åº“ã€‚ä½ åªèƒ½ç®¡ç†ä½ æ‹¥æœ‰çš„ä»“åº“ï¼ˆåœ¨ä½ çš„ç”¨æˆ·åå‘½åç©ºé—´ä¸‹ï¼‰æˆ–è€…ä½ å…·æœ‰å†™å…¥æƒé™çš„ç»„ç»‡ä¸­çš„ä»“åº“\n### åˆ›å»ºä¸€ä¸ªä»“åº“\nä½¿ç”¨ [`create\\_repo`] åˆ›å»ºä¸€ä¸ªç©ºä»“åº“ï¼Œå¹¶é€šè¿‡ `repo\\_id`å‚æ•°ä¸ºå…¶å‘½å `repo\\_id`æ˜¯ä½ çš„å‘½åç©ºé—´ï¼ŒåŽé¢è·Ÿç€ä»“åº“åç§°ï¼š`username\\_or\\_org/repo\\_name`\nè¿è¡Œä»¥ä¸‹ä»£ç ï¼Œä»¥åˆ›å»ºä»“åº“ï¼š\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-model\"")\n'https://huggingface.co/lysandre/test-model'\n```\né»˜è®¤æƒ…å†µä¸‹ï¼Œ[`create\\_repo`] ä¼šåˆ›å»ºä¸€ä¸ªæ¨¡åž‹ä»“åº“ã€‚ä½†æ˜¯ä½ å¯ä»¥ä½¿ç”¨ `repo\\_type`å‚æ•°æ¥æŒ‡å®šå…¶ä»–ä»“åº“ç±»åž‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœä½ æƒ³åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ä»“åº“\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-dataset\"", repo\\_type=\""dataset\"")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```\nåˆ›å»ºä»“åº“æ—¶ï¼Œä½ å¯ä»¥ä½¿ç”¨ `private`å‚æ•°è®¾ç½®ä»“åº“çš„å¯è§æ€§\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç \n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-private\"", private=True)\n```\nå¦‚æžœä½ æƒ³åœ¨ä»¥åŽæ›´æ”¹ä»“åº“çš„å¯è§æ€§ï¼Œä½ å¯ä»¥ä½¿ç”¨[`update\\_repo\\_visibility`] å‡½æ•°\n### åˆ é™¤ä¸€ä¸ªä»“åº“\nä½¿ç”¨ [`delete\\_repo`] åˆ é™¤ä¸€ä¸ªä»“åº“ã€‚ç¡®ä¿ä½ ç¡®å®žæƒ³è¦åˆ é™¤ä»“åº“ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªä¸å¯é€†è½¬çš„è¿‡ç¨‹ï¼åšå®Œä¸Šè¿°è¿‡ç¨‹åŽï¼ŒæŒ‡å®šä½ æƒ³è¦åˆ é™¤çš„ä»“åº“çš„ `repo\\_id`\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> delete\\_repo(repo\\_id=\""lysandre/my-corrupted-dataset\"", repo\\_type=\""dataset\"")\n```\n### å…‹éš†ä¸€ä¸ªä»“åº“ï¼ˆä»…é€‚ç”¨äºŽ Spacesï¼‰\nåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½æƒ³è¦å¤åˆ¶åˆ«äººçš„ä»“åº“å¹¶æ ¹æ®è‡ªå·±çš„ç”¨ä¾‹è¿›è¡Œè°ƒæ•´ã€‚å¯¹äºŽ Spacesï¼Œä½ å¯ä»¥ä½¿ç”¨ [`duplicate\\_space`] æ–¹æ³•æ¥å®žçŽ°ã€‚å®ƒå°†å¤åˆ¶æ•´ä¸ªä»“åº“ã€‚\nä½ ä»ç„¶éœ€è¦é…ç½®è‡ªå·±çš„è®¾ç½®ï¼ˆç¡¬ä»¶å’Œå¯†é’¥ï¼‰ã€‚æŸ¥çœ‹æˆ‘ä»¬çš„[ç®¡ç†ä½ çš„SpaceæŒ‡å—](./manage-spaces)ä»¥èŽ·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import duplicate\\_space\n>>> duplicate\\_space(\""multimodalart/dreambooth-training\"", private=False)\nRepoUrl('https://huggingface.co/spaces/nateraw/dreambooth-training',...)\n```\n## ä¸Šä¼ å’Œä¸‹è½½æ–‡ä»¶\næ—¢ç„¶æ‚¨å·²ç»åˆ›å»ºäº†æ‚¨çš„å­˜å‚¨åº“ï¼Œæ‚¨çŽ°åœ¨ä¹Ÿå¯ä»¥æŽ¨é€æ›´æ”¹è‡³å…¶ä¸­å¹¶ä»Žä¸­ä¸‹è½½æ–‡ä»¶\nè¿™ä¸¤ä¸ªä¸»é¢˜æœ‰å®ƒä»¬è‡ªå·±çš„æŒ‡å—ã€‚è¯·[ä¸Šä¼ æŒ‡å—](./upload) å’Œ[ä¸‹è½½æŒ‡å—](./download)æ¥å­¦ä¹ å¦‚ä½•ä½¿ç”¨æ‚¨çš„å­˜å‚¨åº“ã€‚\n## åˆ†æ”¯å’Œæ ‡ç­¾\nGitå­˜å‚¨åº“é€šå¸¸ä½¿ç”¨åˆ†æ”¯æ¥å­˜å‚¨åŒä¸€å­˜å‚¨åº“çš„ä¸åŒç‰ˆæœ¬ã€‚æ ‡ç­¾ä¹Ÿå¯ä»¥ç”¨äºŽæ ‡è®°å­˜å‚¨åº“çš„ç‰¹å®šçŠ¶æ€ï¼Œä¾‹å¦‚ï¼Œåœ¨å‘å¸ƒç‰ˆæœ¬è¿™ä¸ªæƒ…å†µä¸‹ã€‚æ›´ä¸€èˆ¬åœ°è¯´ï¼Œåˆ†æ”¯å’Œæ ‡ç­¾è¢«ç§°ä¸º[gitå¼•ç”¨](https://git-scm.com/book/en/v2/Git-Internals-Git-References). ### åˆ›å»ºåˆ†æ”¯å’Œæ ‡ç­¾\nä½ å¯ä»¥ä½¿ç”¨[`create\\_branch`]å’Œ[`create\\_tag`]æ¥åˆ›å»ºæ–°çš„åˆ†æ”¯å’Œæ ‡ç­¾:\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import create\\_branch, create\\_tag\n# Create a branch on a Space repo from `main` branch\n>>> create\\_branch(\""Matthijs/speecht5-tts-demo\"", repo\\_type=\""space\"", branch=\""handle-dog-speaker\"")\n# Create a tag on a Dataset repo from `v0.1-release` branch\n>>> create\\_branch(\""bigcode/the-stack\"", repo\\_type=\""dataset\"", revision=\""v0.1-release\"", tag=\""v0.1.1\"", tag\\_message=\""Bump release version.\"")\n```\nåŒæ—¶,ä½ å¯ä»¥ä»¥ç›¸åŒçš„æ–¹å¼ä½¿ç”¨ [`delete\\_branch`] å’Œ [`delete\\_tag`] å‡½æ•°æ¥åˆ é™¤åˆ†æ”¯æˆ–æ ‡ç­¾\n### åˆ—å‡ºæ‰€æœ‰çš„åˆ†æ”¯å’Œæ ‡ç­¾\nä½ è¿˜å¯ä»¥ä½¿ç”¨ [`list\\_repo\\_refs`] åˆ—å‡ºå­˜å‚¨åº“ä¸­çš„çŽ°æœ‰ Git å¼•ç”¨\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import list\\_repo\\_refs\n>>> api.list\\_repo\\_refs(\""bigcode/the-stack\"", repo\\_type=\""dataset\"")\nGitRefs(\nbranches=[\nGitRefInfo(name='main', ref='refs/heads/main', target\\_commit='18edc1591d9ce72aa82f56c4431b3c969b210ae3'),\nGitRefInfo(name='v1.1.a1', ref='refs/heads/v1.1.a1', target\\_commit='f9826b862d1567f3822d3d25649b0d6d22ace714')\n],\nconverts=[],\ntags=[\nGitRefInfo(name='v1.0', ref='refs/tags/v1.0', target\\_commit='c37a8cd1e382064d8aced5e05543c5f7753834da')\n]\n)\n```\n## ä¿®æ”¹å­˜å‚¨åº“è®¾ç½®\nå­˜å‚¨åº“å…·æœ‰ä¸€äº›å¯é…ç½®çš„è®¾ç½®ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ‚¨é€šå¸¸ä¼šåœ¨æµè§ˆå™¨ä¸­çš„å­˜å‚¨åº“è®¾ç½®é¡µé¢ä¸Šæ‰‹åŠ¨é…ç½®è¿™äº›è®¾ç½®ã€‚è¦é…ç½®å­˜å‚¨åº“ï¼Œæ‚¨å¿…é¡»å…·æœ‰å¯¹å…¶çš„å†™è®¿é—®æƒé™ï¼ˆæ‹¥æœ‰å®ƒæˆ–å±žäºŽç»„ç»‡ï¼‰ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ `huggingface\\_hub` åœ¨ç¼–ç¨‹æ–¹å¼ä¸Šé…ç½®çš„è®¾ç½®ã€‚\nä¸€äº›è®¾ç½®æ˜¯ç‰¹å®šäºŽ Spacesï¼ˆç¡¬ä»¶ã€çŽ¯å¢ƒå˜é‡ç­‰ï¼‰çš„ã€‚è¦é…ç½®è¿™äº›è®¾ç½®ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„[ç®¡ç†Spaces](../guides/manage-spaces)æŒ‡å—ã€‚\n### æ›´æ–°å¯è§æ€§\nä¸€ä¸ªå­˜å‚¨åº“å¯ä»¥æ˜¯å…¬å…±çš„æˆ–ç§æœ‰çš„ã€‚ç§æœ‰å­˜å‚¨åº“ä»…å¯¹æ‚¨æˆ–å­˜å‚¨åº“æ‰€åœ¨ç»„ç»‡çš„æˆå‘˜å¯è§ã€‚\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç å°†å­˜å‚¨åº“æ›´æ”¹ä¸ºç§æœ‰ï¼š\n```py\n>>> from huggingface\\_hub import update\\_repo\\_visibility\n>>> update\\_repo\\_visibility(repo\\_id=repo\\_id, private=True)\n```\n### é‡å‘½åæ‚¨çš„å­˜å‚¨åº“\næ‚¨å¯ä»¥ä½¿ç”¨ [`move\\_repo`] åœ¨ Hub ä¸Šé‡å‘½åæ‚¨çš„å­˜å‚¨åº“ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæ‚¨è¿˜å¯ä»¥å°†å­˜å‚¨åº“ä»Žä¸€ä¸ªç”¨æˆ·ç§»åŠ¨åˆ°ä¸€ä¸ªç»„ç»‡ã€‚åœ¨è¿™æ ·åšæ—¶ï¼Œæœ‰ä¸€äº›[é™åˆ¶](https://hf.co/docs/hub/repositories-settings#renaming-or-transferring-a-repo)éœ€è¦æ³¨æ„ã€‚ä¾‹å¦‚ï¼Œæ‚¨ä¸èƒ½å°†å­˜å‚¨åº“è½¬ç§»åˆ°å¦ä¸€ä¸ªç”¨æˆ·ã€‚\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import move\\_repo\n>>> move\\_repo(from\\_id=\""Wauplin/cool-model\"", to\\_id=\""huggingface/cool-model\"")\n```\n## ç®¡ç†å­˜å‚¨åº“çš„æœ¬åœ°å‰¯æœ¬\nä¸Šè¿°æ‰€æœ‰æ“ä½œéƒ½å¯ä»¥é€šè¿‡HTTPè¯·æ±‚å®Œæˆã€‚ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½å¸Œæœ›åœ¨æœ¬åœ°æ‹¥æœ‰å­˜å‚¨åº“çš„å‰¯æœ¬ï¼Œå¹¶ä½¿ç”¨æ‚¨ç†Ÿæ‚‰çš„Gitå‘½ä»¤ä¸Žä¹‹äº¤äº’ã€‚\n[`Repository`] ç±»å…è®¸æ‚¨ä½¿ç”¨ç±»ä¼¼äºŽGitå‘½ä»¤çš„å‡½æ•°ä¸ŽHubä¸Šçš„æ–‡ä»¶å’Œå­˜å‚¨åº“è¿›è¡Œäº¤äº’ã€‚å®ƒæ˜¯å¯¹Gitå’ŒGit-LFSæ–¹æ³•çš„åŒ…è£…ï¼Œä»¥ä½¿ç”¨æ‚¨å·²ç»äº†è§£å’Œå–œçˆ±çš„Gitå‘½ä»¤ã€‚åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…Git-LFSï¼ˆè¯·å‚é˜…[æ­¤å¤„](https://git-lfs.github.com/)èŽ·å–å®‰è£…è¯´æ˜Žï¼‰ã€‚\n### ä½¿ç”¨æœ¬åœ°å­˜å‚¨åº“\nä½¿ç”¨æœ¬åœ°å­˜å‚¨åº“è·¯å¾„å®žä¾‹åŒ–ä¸€ä¸ª [`Repository`] å¯¹è±¡ï¼š\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(local\\_dir=\""//\"")\n```\n### å…‹éš†\n`clone\\_from`å‚æ•°å°†ä¸€ä¸ªå­˜å‚¨åº“ä»ŽHugging Faceå­˜å‚¨åº“IDå…‹éš†åˆ°ç”± `local\\_dir`å‚æ•°æŒ‡å®šçš„æœ¬åœ°ç›®å½•ï¼š\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(local\\_dir=\""w2v2\"", clone\\_from=\""facebook/wav2vec2-large-960h-lv60\"")\n```\n`clone\\_from`è¿˜å¯ä»¥ä½¿ç”¨URLå…‹éš†å­˜å‚¨åº“ï¼š\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> repo = Repository(local\\_dir=\""huggingface-hub\"", clone\\_from=\""https://huggingface.co/facebook/wav2vec2-large-960h-lv60\"")\n```\nä½ å¯ä»¥å°†`clone\\_from`å‚æ•°ä¸Ž[`create\\_repo`]ç»“åˆä½¿ç”¨ï¼Œä»¥åˆ›å»ºå¹¶å…‹éš†ä¸€ä¸ªå­˜å‚¨åº“ï¼š\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> repo\\_url = create\\_repo(repo\\_id=\""repo\\_name\"")\n>>> repo = Repository(local\\_dir=\""repo\\_local\\_path\"", clone\\_from=repo\\_url)\n```\nå½“ä½ å…‹éš†ä¸€ä¸ªå­˜å‚¨åº“æ—¶ï¼Œé€šè¿‡åœ¨å…‹éš†æ—¶æŒ‡å®š`git\\_user`å’Œ`git\\_email`å‚æ•°ï¼Œä½ è¿˜å¯ä»¥ä¸ºå…‹éš†çš„å­˜å‚¨åº“é…ç½®Gitç”¨æˆ·åå’Œç”µå­é‚®ä»¶ã€‚å½“ç”¨æˆ·æäº¤åˆ°è¯¥å­˜å‚¨åº“æ—¶ï¼ŒGitå°†çŸ¥é“æäº¤çš„ä½œè€…æ˜¯è°ã€‚\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> repo = Repository(\n..."", ""There are three kinds of repositories on the Hub, and in this guide you'll be creating a \\*\\*model repository\\*\\* for demonstration purposes. For information on creating and managing models, datasets, and Spaces, refer to their respective documentation. 1. To create a new repository, visit [huggingface.co/new](http://huggingface.co/new):\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new_repo.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new_repo-dark.png)\n\n2. Specify the owner of the repository: this can be either you or any of the organizations youâ€™re affiliated with. 3. Enter your modelâ€™s name. This will also be the name of the repository. 4. Specify whether you want your model to be public or private. 5. Specify the license. You can leave the \\*License\\* field blank for now. To learn about licenses, visit the [\\*\\*Licenses\\*\\*](repositories-licenses) documentation. After creating your model repository, you should see a page like this:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/empty_repo.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/empty_repo-dark.png)\n\nNote that the Hub prompts you to create a \\*Model Card\\*, which you can learn about in the [\\*\\*Model Cards documentation\\*\\*](./model-cards). Including a Model Card in your model repo is best practice, but since we're only making a test repo at the moment we can skip this. ## Adding files to a repository (Web UI)\nTo add files to your repository via the web UI, start by selecting the \\*\\*Files\\*\\* tab, navigating to the desired directory, and then clicking \\*\\*Add file\\*\\*. You'll be given the option to create a new file or upload a file directly from your computer. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repositories-add_file.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repositories-add_file-dark.png)\n\n### Creating a new file\nChoosing to create a new file will take you to the following editor screen, where you can choose a name for your file, add content, and save your file with a message that summarizes your changes. Instead of directly committing the new file to your repo's `main` branch, you can select `Open as a pull request` to create a [Pull Request](./repositories-pull-requests-discussions). ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repositories-create_file.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repositories-create_file-dark.png)\n\n### Uploading a file\nIf you choose \\_Upload file\\_ you'll be able to choose a local file to upload, along with a message summarizing your changes to the repo. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repositories-upload_file.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repositories-upload_file-dark.png)\n\nAs with creating new files, you can select `Open as a pull request` to create a [Pull Request](./repositories-pull-requests-discussions) instead of adding your changes directly to the `main` branch of your repo. ## Adding files to a repository (terminal)[[terminal]]\n### Cloning repositories\nDownloading repositories to your local machine is called \\*cloning\\*. You can use the following commands to load your repo and navigate to it:\n```bash\ngit clone https://huggingface.co//\ncd \n```\nYou can clone over SSH with the following command:\n```bash\ngit clone git@hf.co:/\ncd \n```\nYou'll need to add your SSH public key to [your user settings](https://huggingface.co/settings/keys) to push changes or access private repositories. ### Set up\nNow's the time, you can add any files you want to the repository! ðŸ”¥\nDo you have files larger than 10MB? Those files should be tracked with `git-lfs`, which you can initialize with:\n```bash\ngit lfs install\n```\nNote that if your files are larger than \\*\\*5GB\\*\\* you'll also need to run:\n```bash\nhuggingface-cli lfs-enable-largefiles . ```\nWhen you use Hugging Face to create a repository, Hugging Face automatically provides a list of common file extensions for common Machine Learning large files in the `.gitattributes` file, which `git-lfs` uses to efficiently track changes to your large files.""]",The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository.,model
"How many splits does the ""duorc"" dataset have?
","[""List splits and configurations\nDatasets typically have splits and may also have configurations. A \\_split\\_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A \\_configuration\\_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you're interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load\\_hub)! This guide shows you how to use Datasets Server's `/splits` endpoint to retrieve a dataset's splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\nThe `/splits` endpoint accepts the dataset name as its query parameter:\n\n```python\nimport requests\nheaders = {\""Authorization\"": f\""Bearer {API\\_TOKEN}\""}\nAPI\\_URL = \""https://datasets-server.huggingface.co/splits?dataset=duorc\""\ndef query():\nresponse = requests.get(API\\_URL, headers=headers)\nreturn response.json()\ndata = query()\n```\n\n```js\nimport fetch from \""node-fetch\"";\nasync function query(data) {\nconst response = await fetch(\n\""https://datasets-server.huggingface.co/splits?dataset=duorc\"",\n{\nheaders: { Authorization: `Bearer ${API\\_TOKEN}` },\nmethod: \""GET\""\n}\n);\nconst result = await response.json();\nreturn result;\n}\nquery().then((response) => {\nconsole.log(JSON.stringify(response));\n});\n```\n\n```curl\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\n-X GET \\\n-H \""Authorization: Bearer ${API\\_TOKEN}\""\n```\nThe endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n```json\n{\n\""splits\"": [\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""test\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""test\"" }\n],\n\""pending\"": [],\n\""failed\"": []\n}\n```"", ""Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package\\_reference/main\\_classes#datasets.DatasetInfo) object of the `datasets` library. ```json\n{\n\""dataset\\_info\"": {\n\""description\"": \""DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\n\"",\n\""citation\"": \""@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = {2018}\\n}\\n\"",\n\""homepage\"": \""https://duorc.github.io/\"",\n\""license\"": \""https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\"",\n\""features\"": {\n\""plot\\_id\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""plot\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""title\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""question\\_id\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""question\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""answers\"": {\n\""feature\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""\\_type\"": \""Sequence\""\n},\n\""no\\_answer\"": {\n\""dtype\"": \""bool\"",\n\""\\_type\"": \""Value\""\n}\n},\n\""builder\\_name\"": \""duorc\"",\n\""config\\_name\"": \""SelfRC\"",\n\""version\"": {\n\""version\\_str\"": \""1.0.0\"",\n\""major\"": 1,\n\""minor\"": 0,\n\""patch\"": 0\n},\n\""splits\"": {\n\""train\"": {\n\""name\"": \""train\"",\n\""num\\_bytes\"": 239852729,\n\""num\\_examples\"": 60721,\n\""dataset\\_name\"": \""duorc\""\n},\n\""validation\"": {\n\""name\"": \""validation\"",\n\""num\\_bytes\"": 51662519,\n\""num\\_examples\"": 12961,\n\""dataset\\_name\"": \""duorc\""\n},\n\""test\"": {\n\""name\"": \""test\"",\n\""num\\_bytes\"": 49142710,\n\""num\\_examples\"": 12559,\n\""dataset\\_name\"": \""duorc\""\n}\n},\n\""download\\_checksums\"": {\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_train.json\"": {\n\""num\\_bytes\"": 24388192,\n\""checksum\"": null\n},\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_dev.json\"": {\n\""num\\_bytes\"": 5051240,\n\""checksum\"": null\n},\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_test.json\"": {\n\""num\\_bytes\"": 5023228,\n\""checksum\"": null\n}\n},\n\""download\\_size\"": 34462660,\n\""dataset\\_size\"": 340657958,\n\""size\\_in\\_bytes\"": 375120618\n}\n}\n```"", ""It provides the number of rows, the number of colums (where applicable) and the size in bytes for the different forms of the data: original files, size in memory (RAM) and auto-converted parquet files. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has 187.213 rows along all its configurations and splits, for a total of 97MB. ```json\n{\n\""size\"": {\n\""dataset\"": {\n\""dataset\"": \""duorc\"",\n\""num\\_bytes\\_original\\_files\"": 97383710,\n\""num\\_bytes\\_parquet\\_files\"": 58710973,\n\""num\\_bytes\\_memory\"": 1059067116,\n\""num\\_rows\"": 187213\n},\n\""configs\"": [\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""num\\_bytes\\_original\\_files\"": 62921050,\n\""num\\_bytes\\_parquet\\_files\"": 37709127,\n\""num\\_bytes\\_memory\"": 718409158,\n\""num\\_rows\"": 100972,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""num\\_bytes\\_original\\_files\"": 34462660,\n\""num\\_bytes\\_parquet\\_files\"": 21001846,\n\""num\\_bytes\\_memory\"": 340657958,\n\""num\\_rows\"": 86241,\n\""num\\_columns\"": 7\n}\n],\n\""splits\"": [\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""train\"",\n\""num\\_bytes\\_parquet\\_files\"": 26005668,\n\""num\\_bytes\\_memory\"": 496682909,\n\""num\\_rows\"": 69524,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""validation\"",\n\""num\\_bytes\\_parquet\\_files\"": 5566868,\n\""num\\_bytes\\_memory\"": 106510489,\n\""num\\_rows\"": 15591,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""test\"",\n\""num\\_bytes\\_parquet\\_files\"": 6136591,\n\""num\\_bytes\\_memory\"": 115215760,\n\""num\\_rows\"": 15857,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""train\"",\n\""num\\_bytes\\_parquet\\_files\"": 14851720,\n\""num\\_bytes\\_memory\"": 239852729,\n\""num\\_rows\"": 60721,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""validation\"",\n\""num\\_bytes\\_parquet\\_files\"": 3114390,\n\""num\\_bytes\\_memory\"": 51662519,\n\""num\\_rows\"": 12961,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""test\"",\n\""num\\_bytes\\_parquet\\_files\"": 3035736,\n\""num\\_bytes\\_memory\"": 49142710,\n\""num\\_rows\"": 12559,\n\""num\\_columns\"": 7\n}\n]\n},\n\""pending\"": [],\n\""failed\"": [],\n\""partial\"": false\n}\n```\nIf the size has `partial: true` it means that the actual size of the dataset couldn't been determined because it's too big. In that case the number of rows and bytes can be inferior to the actual numbers.""]","The ""duorc"" dataset has six splits.",Six
"What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Fully Sharded Data Parallel\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or \\*rank\\*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class. Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer. ```bash\npip install accelerate\n```\n## FSDP configuration\nTo start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package\\_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`. ```bash\naccelerate config\n```\nWhen you run `accelerate config`, you'll be prompted with a series of options to configure your training environment."", ""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Fully Sharded Data Parallel\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes. Currently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates. FSDP is supported in ðŸ¤— Accelerate, and you can use it with ðŸ¤— PEFT. This guide will help you learn how to use our FSDP [training script](https://github.com/huggingface/peft/blob/main/examples/conditional\\_generation/peft\\_lora\\_seq2seq\\_accelerate\\_fsdp.py). You'll configure the script to train a large model for conditional generation. ## Configuration\nBegin by running the following command to [create a FSDP configuration file](https://huggingface.co/docs/accelerate/main/en/usage\\_guides/fsdp) with ðŸ¤— Accelerate. Use the `--config\\_file` flag to save the configuration file to a specific location, otherwise it is saved as a `default\\_config.yaml` file in the ðŸ¤— Accelerate cache. The configuration file is used to set the default options when you launch the training script. ```bash\naccelerate config --config\\_file fsdp\\_config.yaml\n```\nYou'll be asked a few questions about your setup, and configure the following arguments. For this example, make sure you fully shard the model parameters, gradients, optimizer states, leverage the CPU for offloading, and wrap model layers based on the Transformer layer class name. ```bash\n`Sharding Strategy`: [1] FULL\\_SHARD (shards optimizer states, gradients and parameters), [2] SHARD\\_GRAD\\_OP (shards optimizer states and gradients), [3] NO\\_SHARD\n`Offload Params`: Decides Whether to offload parameters and gradients to CPU\n`Auto Wrap Policy`: [1] TRANSFORMER\\_BASED\\_WRAP, [2] SIZE\\_BASED\\_WRAP, [3] NO\\_WRAP\n`Transformer Layer Class to Wrap`: When using `TRANSFORMER\\_BASED\\_WRAP`, user specifies comma-separated string of transformer layer class names (case-sensitive) to wrap ,e.g,\n`BertLayer`, `GPTJBlock`, `T5Block`, `BertLayer,BertEmbeddings,BertSelfOutput`... `Min Num Params`: minimum number of parameters when using `SIZE\\_BASED\\_WRAP`\n`Backward Prefetch`: [1] BACKWARD\\_PRE, [2] BACKWARD\\_POST, [3] NO\\_PREFETCH\n`State Dict Type`: [1] FULL\\_STATE\\_DICT, [2] LOCAL\\_STATE\\_DICT, [3] SHARDED\\_STATE\\_DICT\n```\nFor example, your FSDP configuration file may look like the following:\n```yaml\ncommand\\_file: null\ncommands: null\ncompute\\_environment: LOCAL\\_MACHINE\ndeepspeed\\_config: {}\ndistributed\\_type: FSDP\ndowncast\\_bf16: 'no'\ndynamo\\_backend: 'NO'\nfsdp\\_config:\nfsdp\\_auto\\_wrap\\_policy: TRANSFORMER\\_BASED\\_WRAP\nfsdp\\_backward\\_prefetch\\_policy: BACKWARD\\_PRE\nfsdp\\_offload\\_params: true\nfsdp\\_sharding\\_strategy: 1\nfsdp\\_state\\_dict\\_type: FULL\\_STATE\\_DICT\nfsdp\\_transformer\\_layer\\_cls\\_to\\_wrap: T5Block\ngpu\\_ids: null\nmachine\\_rank: 0\nmain\\_process\\_ip: null\nmain\\_process\\_port: null\nmain\\_training\\_function: main\nmegatron\\_lm\\_config: {}\nmixed\\_precision: 'no'\nnum\\_machines: 1\nnum\\_processes: 2\nrdzv\\_backend: static\nsame\\_network: true\ntpu\\_name: null\ntpu\\_zone: null\nuse\\_cpu: false\n```\n## The important parts\nLet's dig a bit deeper into the training script to understand how it works."", ""--\ntitle: \""Fine-tuning Llama 2 70B using PyTorch FSDP\""\nthumbnail: /blog/assets/160\\_fsdp\\_llama/thumbnail.jpg\nauthors:\n- user: smangrul\n- user: sgugger\n- user: lewtun\n- user: philschmid\n---\n# Fine-tuning Llama 2 70B using PyTorch FSDP\n## Introduction\nIn this blog post, we will look at how to fine-tune Llama 2 70B using PyTorch FSDP and related best practices. We will be leveraging Hugging Face Transformers, Accelerate and TRL. We will also learn how to use Accelerate with SLURM. Fully Sharded Data Parallelism (FSDP) is a paradigm in which the optimizer states, gradients and parameters are sharded across devices. During the forward pass, each FSDP unit performs an \\_all-gather operation\\_ to get the complete weights, computation is performed followed by discarding the shards from other devices. After the forward pass, the loss is computed followed by the backward pass. In the backward pass, each FSDP unit performs an all-gather operation to get the complete weights, with computation performed to get the local gradients. These local gradients are averaged and sharded across the devices via a \\_reduce-scatter operation\\_ so that each device can update the parameters of its shard. For more information on what PyTorch FSDP is, please refer to this blog post: [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface.co/blog/pytorch-fsdp). ![FSDP Workflow](./assets/62\\_pytorch\\_fsdp/FSDP\\_workflow.png)\n(Source: [link](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/))\n## Hardware Used\nNumber of nodes: 2. Minimum required is 1. Number of GPUs per node: 8\nGPU type: A100\nGPU memory: 80GB\nintra-node connection: NVLink\nRAM per node: 1TB\nCPU cores per node: 96\ninter-node connection: Elastic Fabric Adapter\n## Challenges with fine-tuning LLaMa 70B\nWe encountered three main challenges when trying to fine-tune LLaMa 70B with FSDP:\n1. FSDP wraps the model after loading the pre-trained model. If each process/rank within a node loads the Llama-70B model, it would require 70\\\\*4\\\\*8 GB ~ 2TB of CPU RAM, where 4 is the number of bytes per parameter and 8 is the number of GPUs on each node.""]","Fully Sharded Data Parallel (FSDP) is a data parallel method designed for distributed training of large pretrained models, allowing for efficient sharding of a model's parameters, gradients, and optimizer states across multiple GPUs. The primary purpose of FSDP is to improve memory efficiency, enabling the training of much larger models on fewer GPUs by reducing the memory usage associated with model replication. This allows for scaling training to larger batch or model sizes, making it particularly useful for training models with up to 1 trillion parameters.","FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes."
"What file format is used to save and store PyTorch model weights more securely than `.bin` files?
","[""There are several reasons for using safetensors:\n- Safety is the number one reason for using safetensors. As open-source and model distribution grows, it is important to be able to trust the model weights you downloaded don't contain any malicious code. The current size of the header in safetensors prevents parsing extremely large JSON files. - Loading speed between switching models is another reason to use safetensors, which performs zero-copy of the tensors. It is especially fast compared to `pickle` if you're loading the weights to CPU (the default case), and just as fast if not faster when directly loading the weights to GPU. You'll only notice the performance difference if the model is already loaded, and not if you're downloading the weights or loading the model for the first time. The time it takes to load the entire pipeline:\n```py\nfrom diffusers import StableDiffusionPipeline\npipeline = StableDiffusionPipeline.from\\_pretrained(\""stabilityai/stable-diffusion-2-1\"", use\\_safetensors=True)\n\""Loaded in safetensors 0:00:02.033658\""\n\""Loaded in PyTorch 0:00:02.663379\""\n```\nBut the actual time it takes to load 500MB of the model weights is only:\n```bash\nsafetensors: 3.4873ms\nPyTorch: 172.7537ms\n```\n- Lazy loading is also supported in safetensors, which is useful in distributed settings to only load some of the tensors. This format allowed the [BLOOM](https://huggingface.co/bigscience/bloom) model to be loaded in 45 seconds on 8 GPUs instead of 10 minutes with regular PyTorch weights."", ""We are also taking steps to make sure the existing PyTorch files are not malicious, but the best we can do is flag suspicious-looking files. Of course, there are other file formats out there, but\nnone seemed to meet the full set of [ideal requirements](https://github.com/huggingface/safetensors#yet-another-format-) our team identified. In addition to being safe, `safetensors` allows lazy loading and generally faster loads (around 100x faster on CPU). Lazy loading means loading only part of a tensor in an efficient manner. This particular feature enables arbitrary sharding with efficient inference libraries, such as [text-generation-inference](https://github.com/huggingface/text-generation-inference), to load LLMs (such as LLaMA, StarCoder, etc.) on various types of hardware\nwith maximum efficiency. Because it loads so fast and is framework agnostic, we can even use the format\nto load models from the same file in PyTorch or TensorFlow. ## The security audit\nSince `safetensors` main asset is providing safety guarantees, we wanted to make sure\nit actually delivered. That's why Hugging Face, EleutherAI, and Stability AI teamed up to get an external\nsecurity audit to confirm it. Important findings:\n- No critical security flaw leading to arbitrary code execution was found. - Some imprecisions in the spec format were detected and fixed. - Some missing validation allowed [polyglot files](https://en.wikipedia.org/wiki/Polyglot\\_(computing)), which was fixed. - Lots of improvements to the test suite were proposed and implemented. In the name of openness and transparency, all companies agreed to make the report\nfully public. [Full report](https://huggingface.co/datasets/safetensors/trail\\_of\\_bits\\_audit\\_repot/resolve/main/SOW-TrailofBits-EleutherAI\\_HuggingFace-v1.2.pdf)\nOne import thing to note is that the library is written in Rust."", ""Convert weights to safetensors\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`. The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository. For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights. Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.""]",The file format used to save and store PyTorch model weights more securely than `.bin` files is `.safetensors`.,`.safetensors`
"What type of security certification does Hugging Face have?
","[""Security\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)"", ""Security & Compliance\nðŸ¤— Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place. ## Data Security/Privacy\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit. We also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet. Hugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan. ## Model Security/Privacy:\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub. ## Inference Endpoints and Hub Security\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\n## Inference Endpoint Security level\nWe currently offer three types of endpoints, in order or increasing security level:\n- \\*\\*Public\\*\\*: A Public Endpoint is available from the internet, secured with TLS/SSL, and requires no authentication."", ""Security Policy\n## Reporting a Vulnerability\nðŸ¤— We have our bug bounty program set up with HackerOne. Please feel free to submit vulnerability reports to our private program at https://hackerone.com/hugging\\_face. Note that you'll need to be invited to our program, so send us a quick email at security@huggingface.co if you've found a vulnerability.""]","Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses.",SOC2 Type 2 certified
"What do RAG models combine to generate outputs?
","[""RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate\noutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\nboth retrieval and generation to adapt to downstream tasks. It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, Douwe Kiela. The abstract from the paper is the following:\n\\*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve\nstate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely\nmanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind\ntask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge\nremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametric\nmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a\ngeneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) â€” models which combine pre-trained\nparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a\npre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages\nacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our\nmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,\noutperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation\ntasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art\nparametric-only seq2seq baseline.\\*\nThis model was contributed by [ola13](https://huggingface.co/ola13). ## Usage tips\nRetrieval-augmented generation (\""RAG\"") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq\nmodules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt\nto downstream tasks. ## RagConfig\n[[autodoc]] RagConfig\n## RagTokenizer\n[[autodoc]] RagTokenizer\n## Rag specific outputs\n[[autodoc]] models.rag.modeling\\_rag.RetrievAugLMMarginOutput\n[[autodoc]] models.rag.modeling\\_rag.RetrievAugLMOutput\n## RagRetriever\n[[autodoc]] RagRetriever\n\n## RagModel\n[[autodoc]] RagModel\n- forward\n## RagSequenceForGeneration\n[[autodoc]] RagSequenceForGeneration\n- forward\n- generate\n## RagTokenForGeneration\n[[autodoc]] RagTokenForGeneration\n- forward\n- generate\n\n## TFRagModel\n[[autodoc]] TFRagModel\n- call\n## TFRagSequenceForGeneration\n[[autodoc]] TFRagSequenceForGeneration\n- call\n- generate\n## TFRagTokenForGeneration\n[[autodoc]] TFRagTokenForGeneration\n- call\n- generate"", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# RAG\n\n[![Models](https://img.shields.io/badge/All_model_pages-rag-blueviolet)](https://huggingface.co/models?filter=rag)\n\n## Overview\nRetrieval-augmented generation (\""RAG\"") models combine the powers of pretrained dense retrieval (DPR) and\nsequence-to-sequence models."", ""This information retrieval step allows [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) to make use of multiple sources of knowledge -- those that are baked into the model parameters and the information that is contained in the contextual passages, allowing it to outperform other state-of-the-art models in tasks like question answering. You can try it for yourself using this [demo provided by Huggingface](https://huggingface.co/rag/)! ### Scaling up fine-tuning\nThis retrieval of contextual documents is crucial for RAG's state-of-the-art results but introduces an extra layer of complexity. When scaling up the training process via a data-parallel training routine, a naive implementation of the document lookup can become a bottleneck for training. Further, the \\*\\*document index\\*\\* used in the retrieval component is often quite large, making it infeasible for each training worker to load its own replicated copy of the index. The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) communication package for the document retrieval portion.""]","RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence (Seq2Seq) models to generate outputs. They retrieve documents, pass them to a Seq2Seq model, and then marginalize to produce the final outputs.",Pretrained dense retrieval (DPR) and sequence-to-sequence models.
"What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# MarkupLM\n## Overview\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm). The model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838\\_From\\_one\\_tree\\_to\\_a\\_forest\\_a\\_unified\\_solution\\_for\\_structured\\_web\\_data\\_extraction), a dataset\nfor information extraction from web pages (basically named-entity recogntion on web pages)\nThe abstract from the paper is the following:\n\\*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\ntasks. The pre-trained model and code will be publicly available.\\*\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm). ## Usage tips\n- In addition to `input\\_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath\\_tags\\_seq` and `xpath\\_subs\\_seq`. These are the XPATH tags and subscripts respectively for each token in the input sequence. - One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info. ![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg)\n MarkupLM architecture. Taken from the [original paper.](https://arxiv.org/abs/2110.08518) \n## Usage: MarkupLMProcessor\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input\\_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks. ```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\nfeature\\_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from\\_pretrained(\""microsoft/markuplm-base\"")\nprocessor = MarkupLMProcessor(feature\\_extractor, tokenizer)\n```\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input\\_ids`, `attention\\_mask`, `token\\_type\\_ids`, `xpath\\_subs\\_seq`, `xpath\\_tags\\_seq`. Optionally, one can provide node labels to the processor, which are turned into token-level `labels`. [`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]."", ""In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs). \\*\\*Use case 1: web page classification (training, inference) + token classification (inference), parse\\_html = True\\*\\*\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML. ```python\n>>> from transformers import MarkupLMProcessor\n>>> processor = MarkupLMProcessor.from\\_pretrained(\""microsoft/markuplm-base\"")\n>>> html\\_string = \""\""\""\n... ... ... ... Hello world\n... ... ... # Welcome\n\n... Here is my website. ... ... \""\""\""\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\n>>> encoding = processor(html\\_string, return\\_tensors=\""pt\"")\n>>> print(encoding.keys())\ndict\\_keys(['input\\_ids', 'token\\_type\\_ids', 'attention\\_mask', 'xpath\\_tags\\_seq', 'xpath\\_subs\\_seq'])\n```\n\\*\\*Use case 2: web page classification (training, inference) + token classification (inference), parse\\_html=False\\*\\*\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse\\_html` to `False`. ```python\n>>> from transformers import MarkupLMProcessor\n>>> processor = MarkupLMProcessor.from\\_pretrained(\""microsoft/markuplm-base\"")\n>>> processor.parse\\_html = False\n>>> nodes = [\""hello\"", \""world\"", \""how\"", \""are\""]\n>>> xpaths = [\""/html/body/div/li[1]/div/span\"", \""/html/body/div/li[1]/div/span\"", \""html/body\"", \""html/body/div\""]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return\\_tensors=\""pt\"")\n>>> print(encoding.keys())\ndict\\_keys(['input\\_ids', 'token\\_type\\_ids', 'attention\\_mask', 'xpath\\_tags\\_seq', 'xpath\\_subs\\_seq'])\n```\n\\*\\*Use case 3: token classification (training), parse\\_html=False\\*\\*\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\ncorresponding node labels in order to train a model."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Utilities for `FeatureExtractors`\nThis page lists all the utility functions that can be used by the audio [`FeatureExtractor`] in order to compute special features from a raw audio using common algorithms such as \\*Short Time Fourier Transform\\* or \\*log mel spectrogram\\*. Most of those are only useful if you are studying the code of the audio processors in the library. ## Audio Transformations\n[[autodoc]] audio\\_utils.hertz\\_to\\_mel\n[[autodoc]] audio\\_utils.mel\\_to\\_hertz\n[[autodoc]] audio\\_utils.mel\\_filter\\_bank\n[[autodoc]] audio\\_utils.optimal\\_fft\\_length\n[[autodoc]] audio\\_utils.window\\_function\n[[autodoc]] audio\\_utils.spectrogram\n[[autodoc]] audio\\_utils.power\\_to\\_db\n[[autodoc]] audio\\_utils.amplitude\\_to\\_db""]","MarkupLMFeatureExtractor uses Beautiful Soup, a Python library for pulling data out of HTML and XML files, to extract data from HTML and XML documents.",Beautiful Soup
"What is the file size limit for syncing to HF Spaces without using Git-LFS?
","[""Managing Spaces with CircleCI Workflows\nYou can keep your app in sync with your GitHub repository with a \\*\\*CircleCI workflow\\*\\*. [CircleCI](https://circleci.com) is a continuous integration and continuous delivery (CI/CD) platform that helps automate the software development process. A [CircleCI workflow](https://circleci.com/docs/workflows/) is a set of automated tasks defined in a configuration file, orchestrated by CircleCI, to streamline the process of building, testing, and deploying software applications. \\*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\\*\nFirst, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository. ```bash\ngit remote add space https://huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME\n```\nThen force push to sync everything for the first time:\n```bash\ngit push --force space main\n```\nNext, set up a [CircleCI workflow](https://circleci.com/docs/workflows/) to push your `main` git branch to Spaces. In the example below:\n\\* Replace `HF\\_USERNAME` with your username and `SPACE\\_NAME` with your Space name. \\* [Create a context in CircleCI](https://circleci.com/docs/contexts/) and add an env variable into it called \\*HF\\_PERSONAL\\_TOKEN\\* (you can give it any name, use the key you create in place of HF\\_PERSONAL\\_TOKEN) and the value as your Hugging Face API token. You can find your Hugging Face API token under \\*\\*API Tokens\\*\\* on [your Hugging Face profile](https://huggingface.co/settings/tokens). ```yaml\nversion: 2.1\nworkflows:\nmain:\njobs:\n- sync-to-huggingface:\ncontext:\n- HuggingFace\nfilters:\nbranches:\nonly:\n- main\njobs:\nsync-to-huggingface:\ndocker:\n- image: alpine\nresource\\_class: small\nsteps:\n- run:\nname: install git\ncommand: apk update && apk add openssh-client git\n- checkout\n- run:\nname: push to Huggingface hub\ncommand: |\ngit config user.email \""\""\ngit config user.name \""\""\ngit push -f https://HF\\_USERNAME:${HF\\_PERSONAL\\_TOKEN}@huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME main\n```"", ""Managing Spaces with Github Actions\nYou can keep your app in sync with your GitHub repository with \\*\\*Github Actions\\*\\*. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup. First, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository. ```bash\ngit remote add space https://huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME\n```\nThen force push to sync everything for the first time:\n```bash\ngit push --force space main\n```\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:\n\\* Replace `HF\\_USERNAME` with your username and `SPACE\\_NAME` with your Space name. \\* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF\\_TOKEN`. You can find your Hugging Face API token under \\*\\*API Tokens\\*\\* on your Hugging Face profile. ```yaml\nname: Sync to Hugging Face hub\non:\npush:\nbranches: [main]\n# to run this workflow manually from the Actions tab\nworkflow\\_dispatch:\njobs:\nsync-to-hub:\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@v3\nwith:\nfetch-depth: 0\nlfs: true\n- name: Push to hub\nenv:\nHF\\_TOKEN: ${{ secrets.HF\\_TOKEN }}\nrun: git push https://HF\\_USERNAME:$HF\\_TOKEN@huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME main\n```\nFinally, create an Action that automatically checks the file size of any new pull request:\n```yaml\nname: Check file size\non: # or directly `on: [push]` to run the action on every push on any branch\npull\\_request:\nbranches: [main]\n# to run this workflow manually from the Actions tab\nworkflow\\_dispatch:\njobs:\nsync-to-hub:\nruns-on: ubuntu-latest\nsteps:\n- name: Check large files\nuses: ActionsDesk/lfs-warning@v2.0\nwith:\nfilesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\n```"", ""There are a few reasons for this:\n- Uploading and downloading smaller files is much easier both for you and the other users. Connection issues can always\nhappen when streaming data and smaller files avoid resuming from the beginning in case of errors. - Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\nleading to a slower download speed. In all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size. - \\*\\*Number of commits\\*\\*: There is no hard limit for the total number of commits on your repo history. However, from\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a\nfresh start using `huggingface\\_hub`'s [`super\\_squash\\_history`](https://huggingface.co/docs/huggingface\\_hub/main/en/package\\_reference/hf\\_api#huggingface\\_hub.HfApi.super\\_squash\\_history).""]",The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. Files larger than this require Git-LFS.,10MB
"What is the title of the paper introducing the ByT5 model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ByT5\n## Overview\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, Colin Raffel."", ""The abstract from the paper is the following:\n\\*Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from\nthe model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they\ncan process text in any language out of the box, they are more robust to noise, and they minimize technical debt by\nremoving complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token\nsequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of\noperating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with\nminimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count,\ntraining FLOPs, and inference speed, and show that byte-level models are competitive with their token-level\ncounterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on\ntasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of\npre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our\nexperiments.\\*\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/byt5). ByT5's architecture is based on the T5v1.1 model, refer to [T5v1.1's documentation page](t5v1.1) for the API reference. They\nonly differ in how inputs should be prepared for the model, see the code examples below. Since ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix. ## Usage example\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\n```python\n>>> from transformers import T5ForConditionalGeneration\n>>> import torch\n>>> model = T5ForConditionalGeneration.from\\_pretrained(\""google/byt5-small\"")\n>>> num\\_special\\_tokens = 3\n>>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5. >>> # => Need to shift utf-8 character encodings by 3 before passing ids to model. >>> input\\_ids = torch.tensor([list(\""Life is like a box of chocolates.\"".encode(\""utf-8\""))]) + num\\_special\\_tokens\n>>> labels = torch.tensor([list(\""La vie est comme une boÃ®te de chocolat.\"".encode(\""utf-8\""))]) + num\\_special\\_tokens\n>>> loss = model(input\\_ids, labels=labels).loss\n>>> loss.item()\n2.66\n```\nFor batched inference and training it is however recommended to make use of the tokenizer:\n```python\n>>> from transformers import T5ForConditionalGeneration, AutoTokenizer\n>>> model = T5ForConditionalGeneration.from\\_pretrained(\""google/byt5-small\"")\n>>> tokenizer = AutoTokenizer.from\\_pretrained(\""google/byt5-small\"")\n>>> model\\_inputs = tokenizer(\n... [\""Life is like a box of chocolates.\"", \""Today is Monday.\""], padding=\""longest\"", return\\_tensors=\""pt\""\n...""]","The title of the paper introducing the ByT5 model is ""ByT5: Towards a token-free future with pre-trained byte-to-byte models.""",ByT5: Towards a token-free future with pre-trained byte-to-byte models
"What is the dimension of the feature vector for the base BERT model?
","[""n this video, we'll study the encoder architecture. An example of a popular encoder-only architecture is BERT, which is the most popular model of its kind. Let's first start by understanding how it works. We'll use a small example, using three words. We use these as inputs, and pass them through the encoder. We retrieve a numerical representation of each word. Here, for example, the encoder converts the three words â€œWelcome to NYCâ€ in these three sequences of numbers. The encoder outputs exactly one sequence of numbers per input word. This numerical representation can also be called a \""Feature vector\"", or \""Feature tensor\"". Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \""to\"", isn't the representation of only the \""to\"" word. It also takes into account the words around it, which we call the â€œcontextâ€.As in, it looks to the left context, the word on the left of the one we're studying (here the word \""Welcome\"") and the context on the right (here the word \""NYC\"") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \""meaning\"" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence."", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# BERT\n\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\n\n## Overview\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It's a\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\n\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\narchitecture modifications.\\*\n\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\*\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf)."", ""---\n## 2. How does BERT Work? BERT works by leveraging the following:\n### 2.1 Large amounts of training data\nA massive dataset of 3.3 Billion words has contributed to BERTâ€™s continued success. BERT was specifically trained on Wikipedia (\\~2.5B words) and Googleâ€™s BooksCorpus (\\~800M words). These large informational datasets contributed to BERTâ€™s deep knowledge not only of the English language but also of our world! ðŸš€\nTraining on a dataset this large takes a long time. BERTâ€™s training was made possible thanks to the novel Transformer architecture and sped up by using TPUs (Tensor Processing Units - Googleâ€™s custom circuit built specifically for large ML models). â€”64 TPUs trained BERT over the course of 4 days. \\*\\*Note:\\*\\* Demand for smaller BERT models is increasing in order to use BERT within smaller computational environments (like cell phones and personal computers). [23 smaller BERT models were released in March 2020](https://github.com/google-research/bert). [DistilBERT](https://huggingface.co/docs/transformers/model\\_doc/distilbert) offers a lighter version of BERT; runs 60% faster while maintaining over 95% of BERTâ€™s performance. ### 2.2 What is a Masked Language Model? MLM enables/enforces bidirectional learning from text by masking (hiding) a word in a sentence and forcing BERT to bidirectionally use the words on either side of the covered word to predict the masked word.""]",The dimension of the feature vector for the base BERT model is 768.,768
"What special identifier does the WordPiece Model use for continuing subwords?
","[""WordPiece tokenization[[wordpiece-tokenization]]\nWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It's very similar to BPE in terms of the training, but the actual tokenization is done differently. ðŸ’¡ This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm. ## Training algorithm[[training-algorithm]]\nâš ï¸ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate. Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\""word\""` gets split like this:\n```\nw ##o ##r ##d\n```\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix. Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\n$$\\mathrm{score} = (\\mathrm{freq\\\\_of\\\\_pair}) / (\\mathrm{freq\\\\_of\\\\_first\\\\_element} \\times \\mathrm{freq\\\\_of\\\\_second\\\\_element})$$\nBy dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary."", ""For instance,\nthe word `\""bug\""` would be tokenized to `[\""b\"", \""ug\""]` but `\""mug\""` would be tokenized as `[\""\"", \""ug\""]` since\nthe symbol `\""m\""` is not in the base vocabulary. In general, single letters such as `\""m\""` are not replaced by the\n`\""\""` symbol because the training data usually includes at least one occurrence of each letter, but it is likely\nto happen for very special characters like emojis. As mentioned earlier, the vocabulary size, \\*i.e.\\* the base vocabulary size + the number of merges, is a hyperparameter\nto choose. For instance [GPT](model\\_doc/gpt) has a vocabulary size of 40,478 since they have 478 base characters\nand chose to stop training after 40,000 merges. #### Byte-level BPE\nA base vocabulary that includes all possible base characters can be quite large if \\*e.g.\\* all unicode characters are\nconsidered as base characters. To have a better base vocabulary, [GPT-2](https://cdn.openai.com/better-language-models/language\\_models\\_are\\_unsupervised\\_multitask\\_learners.pdf) uses bytes\nas the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that\nevery base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2's\ntokenizer can tokenize every text without the need for the  symbol. [GPT-2](model\\_doc/gpt) has a vocabulary\nsize of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned\nwith 50,000 merges. ### WordPiece\nWordPiece is the subword tokenization algorithm used for [BERT](model\\_doc/bert), [DistilBERT](model\\_doc/distilbert), and [Electra](model\\_doc/electra). The algorithm was outlined in [Japanese and Korean\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary. So what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is\nequivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by\nits second symbol is the greatest among all symbol pairs. \\*E.g.\\* `\""u\""`, followed by `\""g\""` would have only been\nmerged if the probability of `\""ug\""` divided by `\""u\""`, `\""g\""` would have been greater than for any other symbol\npair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it \\_loses\\_ by merging two symbols\nto ensure it's \\_worth it\\_. ### Unigram\nUnigram is a subword tokenization algorithm introduced in [Subword Regularization: Improving Neural Network Translation\nModels with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). In contrast to BPE or\nWordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each\nsymbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and\nthe most common substrings. Unigram is not used directly for any of the models in the transformers, but it's used in\nconjunction with [SentencePiece](#sentencepiece). At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training\ndata given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm\ncomputes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then\nremoves p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, \\*i.e.\\* those\nsymbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has\nreached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized. Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of\ntokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:\n```\n[\""b\"", \""g\"", \""h\"", \""n\"", \""p\"", \""s\"", \""u\"", \""ug\"", \""un\"", \""hug\""],\n```\n`\""hugs\""` could be tokenized both as `[\""hug\"", \""s\""]`, `[\""h\"", \""ug\"", \""s\""]` or `[\""h\"", \""u\"", \""g\"", \""s\""]`. So which one\nto choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that\nthe probability of each possible tokenization can be computed after training."", ""et's see together what is the training strategy of the WordPiece algorithm and how it performs the tokenization of a text once trained WordPiece is a tokenization algorithm introduced by Google. It is used for example by Bert. To our knowledge, the code of Word Pieces has not been open sourced, so we base our explanations on our own interpretation of the published literature. What is the training strategy of WordPiece? Similarly to the BPE algorithm, WordPiece starts by establishing an initial vocabulary composed of elementary units and then increases this vocabulary to the desired size. To build the initial vocabulary, we divide each word in the training corpus into the sequence of letters that make it up. As you can see, there is a small subtlety: we add a 2 hashtags in front of the letters that do not start a word. By keeping only one occurrence per elementary unit we now have our initial vocabulary. We will list all the existing pairs in our corpus. Once we have this list, we will calculate a score for each of these pairs. As for the BPE algorithm, we will select the pair with the highest score. Taking for example the first pair composed of H and U. The score of a pair is simply equal to the frequency of appearance of the pair divided by the product of the frequency of appearance of the first token by the frequency of appearance of the second token. Thus at a fixed frequency of appearance of the pair, if the subparts of the pair are very frequent in the corpus then this score will be decreased. In our example, the pair \""hu\"" appears 4 times, the letter \""h\"" 4 times and the letter u 4 times.""]","The WordPiece model uses a prefix of `##` to identify continuing subwords. For example, a word like `""word""` would be tokenized as `[""w"", ""##o"", ""##r"", ""##d""]`, where the `##` indicates that the characters following it are part of the same word.",##
"What is the purpose of the ðŸ§¨ Diffusers tutorials?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\nWelcome to ðŸ§¨ Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how ðŸ§¨ Diffusers is meant to be used. You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want. After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications. Feel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers! Let's start diffusing!"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n\n![](https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg)\n\n# Diffusers\nðŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, ðŸ¤— Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). The library has three main components:\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in ðŸ¤— Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve. - Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality. - Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. [Tutorials\n\nLearn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you're using ðŸ¤— Diffusers for the first time!](./tutorials/tutorial_overview)\n[How-to guides\n\nPractical guides for helping you load pipelines, models, and schedulers. You'll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.](./using-diffusers/loading_overview)\n[Conceptual guides\n\nUnderstand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.](./conceptual/philosophy)\n[Reference\n\nTechnical descriptions of how ðŸ¤— Diffusers classes and methods work.](./api/models/overview)"", ""Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). ðŸ¤— Diffusers offers three core components:\n- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code. - Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality. - Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. ## Installation\nWe recommend installing ðŸ¤— Diffusers in a virtual environment from PyPI or Conda. For more details about installing [PyTorch](https://pytorch.org/get-started/locally/) and [Flax](https://flax.readthedocs.io/en/latest/#installation), please refer to their official documentation. ### PyTorch\nWith `pip` (official package):\n```bash\npip install --upgrade diffusers[torch]\n```\nWith `conda` (maintained by the community):\n```sh\nconda install -c conda-forge diffusers\n```\n### Flax\nWith `pip` (official package):\n```bash\npip install --upgrade diffusers[flax]\n```\n### Apple Silicon (M1/M2) support\nPlease refer to the [How to use Stable Diffusion in Apple Silicon](https://huggingface.co/docs/diffusers/optimization/mps) guide. ## Quickstart\nGenerating outputs is super easy with ðŸ¤— Diffusers. To generate an image from text, use the `from\\_pretrained` method to load any pretrained diffusion model (browse the [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) for 16000+ checkpoints):\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\npipeline = DiffusionPipeline.from\\_pretrained(\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16)\npipeline.to(\""cuda\"")\npipeline(\""An image of a squirrel in Picasso style\"").images[0]\n```\nYou can also dig into the models and schedulers toolbox to build your own diffusion system:\n```python\nfrom diffusers import DDPMScheduler, UNet2DModel\nfrom PIL import Image\nimport torch\nscheduler = DDPMScheduler.from\\_pretrained(\""google/ddpm-cat-256\"")\nmodel = UNet2DModel.from\\_pretrained(\""google/ddpm-cat-256\"").to(\""cuda\"")\nscheduler.set\\_timesteps(50)\nsample\\_size = model.config.sample\\_size\nnoise = torch.randn((1, 3, sample\\_size, sample\\_size), device=\""cuda\"")\ninput = noise\nfor t in scheduler.timesteps:\nwith torch.no\\_grad():\nnoisy\\_residual = model(input, t).sample\nprev\\_noisy\\_sample = scheduler.step(noisy\\_residual, t, input).prev\\_sample\ninput = prev\\_noisy\\_sample\nimage = (input / 2 + 0.5).clamp(0, 1)\nimage = image.cpu().permute(0, 2, 3, 1).numpy()[0]\nimage = Image.fromarray((image \\* 255).round().astype(\""uint8\""))\nimage\n```\nCheck out the [Quickstart](https://huggingface.co/docs/diffusers/quicktour) to launch your diffusion journey today! ## How to navigate the documentation\n| \\*\\*Documentation\\*\\* | \\*\\*What can I learn?\\*\\* |\n|---------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Tutorial](https://huggingface.co/docs/diffusers/tutorials/tutorial\\_overview) | A basic crash course for learning how to use the library's most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model. |\n| [Loading](https://huggingface.co/docs/diffusers/using-diffusers/loading\\_overview) | Guides for how to load and configure all the components (pipelines, models, and schedulers) of the library, as well as how to use different schedulers. |\n| [Pipelines for inference](https://huggingface.co/docs/diffusers/using-diffusers/pipeline\\_overview) | Guides for how to use pipelines for different inference tasks, batched generation, controlling generated outputs and randomness, and how to contribute a pipeline to the library.""]","The purpose of the ðŸ§¨ Diffusers tutorials is to provide a gentle introduction to diffusion models and help users understand the library fundamentals. These beginner-friendly tutorials are designed to teach users how to use a pipeline for inference to rapidly generate outputs, as well as how to deconstruct that pipeline to understand how to use the library as a modular toolbox for building their own diffusion systems. After completing the tutorials, users will have gained the necessary skills to start exploring the library on their own and apply it to their own projects and applications.",To provide a gentle introduction to diffusion models and help understand the library fundamentals.
"What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?
","[""Using Flagging\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\nTags: FLAGGING, DATA\n## Introduction\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \""hard\"" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust. Gradio simplifies the collection of this data by including a \\*\\*Flag\\*\\* button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`. ## The \\*\\*Flag\\*\\* button in `gradio.Interface`\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked \\*\\*Flag\\*\\*. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file. There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail. - `allow\\_flagging`: this parameter can be set to either `\""manual\""` (default), `\""auto\""`, or `\""never\""`. - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked. - `auto`: users will not see a button to flag, but every sample will be flagged automatically. - `never`: users will not see a button to flag, and no sample will be flagged. - `flagging\\_options`: this parameter can be either `None` (default) or a list of strings. - If `None`, then the user simply clicks on the \\*\\*Flag\\*\\* button and no additional options are shown. - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\""Incorrect\"", \""Ambiguous\""]`, then buttons labeled \\*\\*Flag as Incorrect\\*\\* and \\*\\*Flag as Ambiguous\\*\\* appear. This only applies if `allow\\_flagging` is `\""manual\""`. - The chosen option is then logged along with the input and output. - `flagging\\_dir`: this parameter takes a string. - It represents what to name the directory where flagged data is stored. - `flagging\\_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n- Using this parameter allows you to write custom code that gets run when the flag button is clicked\n- By default, this is set to an instance of `gr.CSVLogger`\n- One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset."", ""(See more below.)\n## What happens to flagged data? Within the directory provided by the `flagging\\_dir` argument, a CSV file will log the flagged data. Here's an example: The code below creates the calculator interface embedded below it:\n```python\nimport gradio as gr\ndef calculator(num1, operation, num2):\nif operation == \""add\"":\nreturn num1 + num2\nelif operation == \""subtract\"":\nreturn num1 - num2\nelif operation == \""multiply\"":\nreturn num1 \\* num2\nelif operation == \""divide\"":\nreturn num1 / num2\niface = gr.Interface(\ncalculator,\n[\""number\"", gr.Radio([\""add\"", \""subtract\"", \""multiply\"", \""divide\""]), \""number\""],\n\""number\"",\nallow\\_flagging=\""manual\""\n)\niface.launch()\n```\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged. ```directory\n+-- flagged/\n| +-- logs.csv\n```\n\\_flagged/logs.csv\\_\n```csv\nnum1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n```\nIf the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure. ```directory\n+-- flagged/\n| +-- logs.csv\n| +-- image/\n| | +-- 0.png\n| | +-- 1.png\n| +-- Output/\n| | +-- 0.png\n| | +-- 1.png\n```\n\\_flagged/logs.csv\\_\n```csv\nim,Output timestamp\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\n```\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging\\_options` argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV. If we go back to the calculator example, the following code will create the interface embedded below it. ```python\niface = gr.Interface(\ncalculator,\n[\""number\"", gr.Radio([\""add\"", \""subtract\"", \""multiply\"", \""divide\""]), \""number\""],\n\""number\"",\nallow\\_flagging=\""manual\"",\nflagging\\_options=[\""wrong sign\"", \""off by one\"", \""other\""]\n)\niface.launch()\n```\nWhen users click the flag button, the csv file will now include a column indicating the selected option. \\_flagged/logs.csv\\_\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```\n## The HuggingFaceDatasetSaver Callback\nSometimes, saving the data to a local CSV file doesn't make sense."", ""ä½¿ç”¨æ ‡è®°\nç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\næ ‡ç­¾ï¼šæ ‡è®°ï¼Œæ•°æ®\n## ç®€ä»‹\nå½“æ‚¨æ¼”ç¤ºä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡åž‹æ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æ”¶é›†è¯•ç”¨æ¨¡åž‹çš„ç”¨æˆ·çš„æ•°æ®ï¼Œç‰¹åˆ«æ˜¯æ¨¡åž‹è¡Œä¸ºä¸å¦‚é¢„æœŸçš„æ•°æ®ç‚¹ã€‚æ•èŽ·è¿™äº›â€œå›°éš¾â€æ•°æ®ç‚¹æ˜¯æœ‰ä»·å€¼çš„ï¼Œå› ä¸ºå®ƒå…è®¸æ‚¨æ”¹è¿›æœºå™¨å­¦ä¹ æ¨¡åž‹å¹¶ä½¿å…¶æ›´å¯é å’Œç¨³å¥ã€‚\nGradio é€šè¿‡åœ¨æ¯ä¸ªâ€œç•Œé¢â€ä¸­åŒ…å«ä¸€ä¸ª\\*\\*æ ‡è®°\\*\\*æŒ‰é’®æ¥ç®€åŒ–è¿™äº›æ•°æ®çš„æ”¶é›†ã€‚è¿™ä½¿å¾—ç”¨æˆ·æˆ–æµ‹è¯•äººå‘˜å¯ä»¥è½»æ¾åœ°å°†æ•°æ®å‘é€å›žè¿è¡Œæ¼”ç¤ºçš„æœºå™¨ã€‚æ ·æœ¬ä¼šä¿å­˜åœ¨ä¸€ä¸ª CSV æ—¥å¿—æ–‡ä»¶ä¸­ï¼ˆé»˜è®¤æƒ…å†µä¸‹ï¼‰ã€‚å¦‚æžœæ¼”ç¤ºæ¶‰åŠå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘æˆ–å…¶ä»–ç±»åž‹çš„æ–‡ä»¶ï¼Œåˆ™è¿™äº›æ–‡ä»¶ä¼šå•ç‹¬ä¿å­˜åœ¨ä¸€ä¸ªå¹¶è¡Œç›®å½•ä¸­ï¼Œå¹¶ä¸”è¿™äº›æ–‡ä»¶çš„è·¯å¾„ä¼šä¿å­˜åœ¨ CSV æ–‡ä»¶ä¸­ã€‚\n## åœ¨ `gradio.Interface` ä¸­ä½¿ç”¨\\*\\*æ ‡è®°\\*\\*æŒ‰é’®\nä½¿ç”¨ Gradio çš„ `Interface` è¿›è¡Œæ ‡è®°ç‰¹åˆ«ç®€å•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨è¾“å‡ºç»„ä»¶ä¸‹æ–¹æœ‰ä¸€ä¸ªæ ‡è®°ä¸º\\*\\*æ ‡è®°\\*\\*çš„æŒ‰é’®ã€‚å½“ç”¨æˆ·æµ‹è¯•æ‚¨çš„æ¨¡åž‹æ—¶ï¼Œå¦‚æžœçœ‹åˆ°æœ‰è¶£çš„è¾“å‡ºï¼Œä»–ä»¬å¯ä»¥ç‚¹å‡»æ ‡è®°æŒ‰é’®å°†è¾“å…¥å’Œè¾“å‡ºæ•°æ®å‘é€å›žè¿è¡Œæ¼”ç¤ºçš„æœºå™¨ã€‚æ ·æœ¬ä¼šä¿å­˜åœ¨ä¸€ä¸ª CSV æ—¥å¿—æ–‡ä»¶ä¸­ï¼ˆé»˜è®¤æƒ…å†µä¸‹ï¼‰ã€‚å¦‚æžœæ¼”ç¤ºæ¶‰åŠå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘æˆ–å…¶ä»–ç±»åž‹çš„æ–‡ä»¶ï¼Œåˆ™è¿™äº›æ–‡ä»¶ä¼šå•ç‹¬ä¿å­˜åœ¨ä¸€ä¸ªå¹¶è¡Œç›®å½•ä¸­ï¼Œå¹¶ä¸”è¿™äº›æ–‡ä»¶çš„è·¯å¾„ä¼šä¿å­˜åœ¨ CSV æ–‡ä»¶ä¸­ã€‚\nåœ¨ `gradio.Interface` ä¸­æœ‰[å››ä¸ªå‚æ•°](https://gradio.app/docs/#interface-header)æŽ§åˆ¶æ ‡è®°çš„å·¥ä½œæ–¹å¼ã€‚æˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»å®ƒä»¬ã€‚\n- `allow\\_flagging`ï¼šæ­¤å‚æ•°å¯ä»¥è®¾ç½®ä¸º `\""manual\""`ï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œ`\""auto\""` æˆ– `\""never\""`ã€‚\n- `manual`ï¼šç”¨æˆ·å°†çœ‹åˆ°ä¸€ä¸ªæ ‡è®°æŒ‰é’®ï¼Œåªæœ‰åœ¨ç‚¹å‡»æŒ‰é’®æ—¶æ ·æœ¬æ‰ä¼šè¢«æ ‡è®°ã€‚\n- `auto`ï¼šç”¨æˆ·å°†ä¸ä¼šçœ‹åˆ°ä¸€ä¸ªæ ‡è®°æŒ‰é’®ï¼Œä½†æ¯ä¸ªæ ·æœ¬éƒ½ä¼šè‡ªåŠ¨è¢«æ ‡è®°ã€‚\n- `never`ï¼šç”¨æˆ·å°†ä¸ä¼šçœ‹åˆ°ä¸€ä¸ªæ ‡è®°æŒ‰é’®ï¼Œå¹¶ä¸”ä¸ä¼šæ ‡è®°ä»»ä½•æ ·æœ¬ã€‚\n- `flagging\\_options`ï¼šæ­¤å‚æ•°å¯ä»¥æ˜¯ `None`ï¼ˆé»˜è®¤å€¼ï¼‰æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ã€‚\n- å¦‚æžœæ˜¯ `None`ï¼Œåˆ™ç”¨æˆ·åªéœ€ç‚¹å‡»\\*\\*æ ‡è®°\\*\\*æŒ‰é’®ï¼Œä¸ä¼šæ˜¾ç¤ºå…¶ä»–é€‰é¡¹ã€‚\n- å¦‚æžœæä¾›äº†ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œåˆ™ç”¨æˆ·ä¼šçœ‹åˆ°å¤šä¸ªæŒ‰é’®ï¼Œå¯¹åº”äºŽæä¾›çš„æ¯ä¸ªå­—ç¬¦ä¸²ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæ­¤å‚æ•°çš„å€¼ä¸º`[\"" é”™è¯¯ \"", \"" æ¨¡ç³Š \""]`ï¼Œåˆ™ä¼šæ˜¾ç¤ºæ ‡è®°ä¸º\\*\\*æ ‡è®°ä¸ºé”™è¯¯\\*\\*å’Œ\\*\\*æ ‡è®°ä¸ºæ¨¡ç³Š\\*\\*çš„æŒ‰é’®ã€‚è¿™ä»…é€‚ç”¨äºŽ `allow\\_flagging` ä¸º `\""manual\""` çš„æƒ…å†µã€‚\n- æ‰€é€‰é€‰é¡¹å°†ä¸Žè¾“å…¥å’Œè¾“å‡ºä¸€èµ·è®°å½•ã€‚\n- `flagging\\_dir`ï¼šæ­¤å‚æ•°æŽ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚\n- å®ƒè¡¨ç¤ºæ ‡è®°æ•°æ®å­˜å‚¨çš„ç›®å½•åç§°ã€‚\n- `flagging\\_callback`ï¼šæ­¤å‚æ•°æŽ¥å— `FlaggingCallback` ç±»çš„å­ç±»çš„å®žä¾‹\n- ä½¿ç”¨æ­¤å‚æ•°å…è®¸æ‚¨ç¼–å†™åœ¨ç‚¹å‡»æ ‡è®°æŒ‰é’®æ—¶è¿è¡Œçš„è‡ªå®šä¹‰ä»£ç \n- é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒè®¾ç½®ä¸º `gr.CSVLogger` çš„ä¸€ä¸ªå®žä¾‹\n- ä¸€ä¸ªç¤ºä¾‹æ˜¯å°†å…¶è®¾ç½®ä¸º `gr.HuggingFaceDatasetSaver` çš„ä¸€ä¸ªå®žä¾‹ï¼Œè¿™æ ·æ‚¨å¯ä»¥å°†ä»»ä½•æ ‡è®°çš„æ•°æ®å¯¼å…¥åˆ° HuggingFace æ•°æ®é›†ä¸­ï¼ˆå‚è§ä¸‹æ–‡ï¼‰ã€‚\n## æ ‡è®°çš„æ•°æ®ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ\nåœ¨ `flagging\\_dir` å‚æ•°æä¾›çš„ç›®å½•ä¸­ï¼Œå°†è®°å½•æ ‡è®°çš„æ•°æ®çš„ CSV æ–‡ä»¶ã€‚\nä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼šä¸‹é¢çš„ä»£ç åˆ›å»ºäº†åµŒå…¥å…¶ä¸­çš„è®¡ç®—å™¨ç•Œé¢ï¼š\n```python\nimport gradio as gr\ndef calculator(num1, operation, num2):\nif operation == \""add\"":\nreturn num1 + num2\nelif operation == \""subtract\"":\nreturn num1 - num2\nelif operation == \""multiply\"":\nreturn num1 \\* num2\nelif operation == \""divide\"":\nreturn num1 / num2\niface = gr.Interface(\ncalculator,\n[\""number\"", gr.Radio([\""add\"", \""subtract\"", \""multiply\"", \""divide\""]), \""number\""],\n\""number\"",\nallow\\_flagging=\""manual\""\n)\niface.launch()\n```\nå½“æ‚¨ç‚¹å‡»ä¸Šé¢çš„æ ‡è®°æŒ‰é’®æ—¶ï¼Œå¯åŠ¨ç•Œé¢çš„ç›®å½•å°†åŒ…æ‹¬ä¸€ä¸ªæ–°çš„æ ‡è®°å­æ–‡ä»¶å¤¹ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ª CSV æ–‡ä»¶ã€‚è¯¥ CSV æ–‡ä»¶åŒ…æ‹¬æ‰€æœ‰è¢«æ ‡è®°çš„æ•°æ®ã€‚\n```directory\n+-- flagged/\n| +-- logs.csv\n```\n\\_flagged/logs.csv\\_\n```csv\nnum1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n```\nå¦‚æžœç•Œé¢æ¶‰åŠæ–‡ä»¶æ•°æ®ï¼Œä¾‹å¦‚å›¾åƒå’ŒéŸ³é¢‘ç»„ä»¶ï¼Œè¿˜å°†åˆ›å»ºæ–‡ä»¶å¤¹æ¥å­˜å‚¨è¿™äº›æ ‡è®°çš„æ•°æ®ã€‚ä¾‹å¦‚ï¼Œå°† `image` è¾“å…¥åˆ° `image` è¾“å‡ºç•Œé¢å°†åˆ›å»ºä»¥ä¸‹ç»“æž„ã€‚\n```directory\n+-- flagged/\n| +-- logs.csv\n| +-- image/\n| | +-- 0.png\n| | +-- 1.png\n| +-- Output/\n| | +-- 0.png\n| | +-- 1.png\n```\n\\_flagged/logs.csv\\_\n```csv\nim,Output timestamp\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\n```\nå¦‚æžœæ‚¨å¸Œæœ›ç”¨æˆ·ä¸ºæ ‡è®°æä¾›ä¸€ä¸ªåŽŸå› ï¼Œæ‚¨å¯ä»¥å°†å­—ç¬¦ä¸²åˆ—è¡¨ä¼ é€’ç»™ Interface çš„ `flagging\\_options` å‚æ•°ã€‚ç”¨æˆ·åœ¨æ ‡è®°æ—¶å¿…é¡»é€‰æ‹©å…¶ä¸­ä¸€é¡¹ï¼Œé€‰é¡¹å°†ä½œä¸ºé™„åŠ åˆ—ä¿å­˜åœ¨ CSV æ–‡ä»¶ä¸­ã€‚\nå¦‚æžœæˆ‘ä»¬å›žåˆ°è®¡ç®—å™¨ç¤ºä¾‹ï¼Œä¸‹é¢çš„ä»£ç å°†åˆ›å»ºåµŒå…¥å…¶ä¸­çš„ç•Œé¢ã€‚\n```python\niface = gr.Interface(\ncalculator,\n[\""number\"", gr.Radio([\""add\"", \""subtract\"", \""multiply\"", \""divide\""]), \""number\""],\n\""number\"",\nallow\\_flagging=\""manual\"",\nflagging\\_options=[\""wrong sign\"", \""off by one\"", \""other\""]\n)\niface.launch()\n```\nå½“ç”¨æˆ·ç‚¹å‡»æ ‡è®°æŒ‰é’®æ—¶ï¼ŒCSV æ–‡ä»¶çŽ°åœ¨å°†åŒ…æ‹¬æŒ‡ç¤ºæ‰€é€‰é€‰é¡¹çš„åˆ—ã€‚\n\\_flagged/logs.csv\\_\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```\n## HuggingFaceDatasetSaver å›žè°ƒ\næœ‰æ—¶ï¼Œå°†æ•°æ®ä¿å­˜åˆ°æœ¬åœ° CSV æ–‡ä»¶æ˜¯ä¸åˆç†çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨ Hugging Face Spaces ä¸Š\nï¼Œå¼€å‘è€…é€šå¸¸æ— æ³•è®¿é—®æ‰˜ç®¡ Gradio æ¼”ç¤ºçš„åº•å±‚ä¸´æ—¶æœºå™¨ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨ Hugging Face Space ä¸­å…³é—­æ ‡è®°çš„åŽŸå› ã€‚ç„¶è€Œï¼Œ\næ‚¨å¯èƒ½å¸Œæœ›å¯¹æ ‡è®°çš„æ•°æ®åšå…¶ä»–å¤„ç†ã€‚\nyou may want to do something else with the flagged data. é€šè¿‡ `flagging\\_callback` å‚æ•°ï¼Œæˆ‘ä»¬ä½¿è¿™å˜å¾—éžå¸¸ç®€å•ã€‚\nä¾‹å¦‚ï¼Œä¸‹é¢æˆ‘ä»¬å°†ä¼šå°†æ ‡è®°çš„æ•°æ®ä»Žæˆ‘ä»¬çš„è®¡ç®—å™¨ç¤ºä¾‹å¯¼å…¥åˆ° Hugging Face æ•°æ®é›†ä¸­ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥æž„å»ºä¸€ä¸ªâ€œä¼—åŒ…â€æ•°æ®é›†ï¼š\n```python\nimport os\nHF\\_TOKEN = os.getenv('HF\\_TOKEN')\nhf\\_writer = gr.HuggingFaceDatasetSaver(HF\\_TOKEN, \""crowdsourced-calculator-demo\"")\niface = gr.Interface(\ncalculator,\n[\""number\"", gr.Radio([\""add\"", \""subtract\"", \""multiply\"", \""divide\""]), \""number\""],\n\""number\"",\ndescription=\""Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\"",\nallow\\_flagging=\""manual\"",\nflagging\\_options=[\""wrong sign\"", \""off by one\"", \""other\""],\nflagging\\_callback=hf\\_writer\n)\niface.launch()\n```\næ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„ Hugging Face ä»¤ç‰Œå’Œ\nè¦ä¿å­˜æ ·æœ¬çš„æ•°æ®é›†çš„åç§°ï¼Œå®šä¹‰äº†æˆ‘ä»¬è‡ªå·±çš„\n`gradio.HuggingFaceDatasetSaver` çš„å®žä¾‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°† `allow\\_flagging=\""manual\""` è®¾ç½®ä¸ºäº†\nï¼Œå› ä¸ºåœ¨ Hugging Face Spaces ä¸­ï¼Œ`allow\\_flagging` é»˜è®¤è®¾ç½®ä¸º `\""never\""`ã€‚è¿™æ˜¯æˆ‘ä»¬çš„æ¼”ç¤ºï¼š\næ‚¨çŽ°åœ¨å¯ä»¥åœ¨è¿™ä¸ª[å…¬å…±çš„ Hugging Face æ•°æ®é›†](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)ä¸­çœ‹åˆ°ä¸Šé¢æ ‡è®°çš„æ‰€æœ‰ç¤ºä¾‹ã€‚\n![flagging callback hf](/assets/guides/flagging-callback-hf.png)\næˆ‘ä»¬åˆ›å»ºäº† `gradio.HuggingFaceDatasetSaver` ç±»ï¼Œä½†åªè¦å®ƒç»§æ‰¿è‡ª[æ­¤æ–‡ä»¶](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py)ä¸­å®šä¹‰çš„ `FlaggingCallback`ï¼Œæ‚¨å¯ä»¥ä¼ é€’è‡ªå·±çš„è‡ªå®šä¹‰ç±»ã€‚å¦‚æžœæ‚¨åˆ›å»ºäº†ä¸€ä¸ªå¾ˆæ£’çš„å›žè°ƒï¼Œè¯·å°†å…¶è´¡çŒ®ç»™è¯¥å­˜å‚¨åº“ï¼\n## ä½¿ç”¨ Blocks è¿›è¡Œæ ‡è®°\nå¦‚æžœæ‚¨æ­£åœ¨ä½¿ç”¨ `gradio.Blocks`ï¼Œåˆè¯¥æ€Žä¹ˆåŠžå‘¢ï¼Ÿä¸€æ–¹é¢ï¼Œä½¿ç”¨ Blocks æ‚¨æ‹¥æœ‰æ›´å¤šçš„çµæ´»æ€§\n--æ‚¨å¯ä»¥ç¼–å†™ä»»ä½•æ‚¨æƒ³åœ¨æŒ‰é’®è¢«ç‚¹å‡»æ—¶è¿è¡Œçš„ Python ä»£ç ï¼Œ\nå¹¶ä½¿ç”¨ Blocks ä¸­çš„å†…ç½®äº‹ä»¶åˆ†é…å®ƒã€‚\nåŒæ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ›ä½¿ç”¨çŽ°æœ‰çš„ `FlaggingCallback` æ¥é¿å…ç¼–å†™é¢å¤–çš„ä»£ç ã€‚\nè¿™éœ€è¦ä¸¤ä¸ªæ­¥éª¤ï¼š\n1. æ‚¨å¿…é¡»åœ¨ä»£ç ä¸­çš„æŸä¸ªä½ç½®è¿è¡Œæ‚¨çš„å›žè°ƒçš„ `.setup()` æ–¹æ³•\nåœ¨ç¬¬ä¸€æ¬¡æ ‡è®°æ•°æ®ä¹‹å‰\n2.""]","The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `""manual""`.","""manual"""
"Where can the full code for the Stable Diffusion demo be found?
","[""--\ntitle: ðŸ§¨ Stable Diffusion in JAX / Flax ! thumbnail: /blog/assets/108\\_stable\\_diffusion\\_jax/thumbnail.png\nauthors:\n- user: pcuenq\n- user: patrickvonplaten\n---\n# ðŸ§¨ Stable Diffusion in JAX / Flax ! [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_jax_how_to.ipynb)\n# \\*\\*Stable Diffusion in JAX / Flax\\*\\* ðŸš€\nðŸ¤— Hugging Face [Diffusers](https://github.com/huggingface/diffusers) supports Flax since version `0.5.1`! This allows for super fast inference on Google TPUs, such as those available in Colab, Kaggle or Google Cloud Platform. This post shows how to run inference using JAX / Flax. If you want more details about how Stable Diffusion works or want to run it in GPU, please refer to [this Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). If you want to follow along, click the button above to open this post as a Colab notebook. First, make sure you are using a TPU backend. If you are running this notebook in Colab, select `Runtime` in the menu above, then select the option \""Change runtime type\"" and then select `TPU` under the `Hardware accelerator` setting. Note that JAX is not exclusive to TPUs, but it shines on that hardware because each TPU server has 8 TPU accelerators working in parallel. ## Setup\n``` python\nimport jax\nnum\\_devices = jax.device\\_count()\ndevice\\_type = jax.devices()[0].device\\_kind\nprint(f\""Found {num\\_devices} JAX devices of type {device\\_type}.\"")\nassert \""TPU\"" in device\\_type, \""Available device is not a TPU, please select TPU from Edit > Notebook settings > Hardware accelerator\""\n```\n\\*Output\\*:\n```bash\nFound 8 JAX devices of type TPU v2."", ""--\ntitle: Stable Diffusion with ðŸ§¨ Diffusers\nthumbnail: /blog/assets/98\\_stable\\_diffusion/thumbnail.png\nauthors:\n- user: valhalla\n- user: pcuenq\n- user: natolambert\n- user: patrickvonplaten\n---\n# Stable Diffusion with ðŸ§¨ Diffusers\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb)\n# \\*\\*Stable Diffusion\\*\\* ðŸŽ¨\n\\*...using ðŸ§¨ Diffusers\\*\nStable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). It is trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database. \\*LAION-5B\\* is the largest, freely accessible multi-modal dataset that currently exists. In this post, we want to show how to use Stable Diffusion with the [ðŸ§¨ Diffusers library](https://github.com/huggingface/diffusers), explain how the model works and finally dive a bit deeper into how `diffusers` allows\none to customize the image generation pipeline. \\*\\*Note\\*\\*: It is highly recommended to have a basic understanding of how diffusion models work. If diffusion\nmodels are completely new to you, we recommend reading one of the following blog posts:\n- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n- [Getting started with ðŸ§¨ Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers\\_intro.ipynb)\nNow, let's get started by generating some images ðŸŽ¨. ## Running Stable Diffusion\n### License\nBefore using the model, you need to accept the model [license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) in order to download and use the weights."", ""Free solutions include many [spaces](https://huggingface.co/spaces) here on ðŸ¤— Hugging Face, such as the [Stable Diffusion 2.1 Demo](https://huggingface.co/spaces/stabilityai/stable-diffusion) or the [camemduru webui](https://huggingface.co/spaces/camenduru/webui). You can find a list of additional online services [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services). You can even use ðŸ¤— [Diffusers](https://huggingface.co/docs/diffusers/index) to write your own free solution! You can find a simple code example to get started [here](https://colab.research.google.com/drive/1HebngGyjKj7nLdXfj6Qi0N1nh7WvD74z?usp=sharing). \\*Note:\\* Parts of this series will use advanced features such as image2image, which may not be available on all online services. ### Generating Concept Art \nLet's generate some concept art. The steps are simple:\n1. Type what you want. 2. Click generate. ![Stable Diffusion Demo Space](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/sd-demo.png)\nBut, how do you get the results you actually want?""]",The full code for the Stable Diffusion demo can be found in the [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).,https://hf.co/spaces/stabilityai/stable-diffusion/tree/main
"What transformation does the FNet model use to replace the self-attention layer in a BERT model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# FNet\n## Overview\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:\n\\*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \""mix\"" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \""efficient\"" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.\\*\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f\\_net). ## Usage tips\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\nsequence length for fine-tuning and inference. ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## FNetConfig\n[[autodoc]] FNetConfig\n## FNetTokenizer\n[[autodoc]] FNetTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## FNetTokenizerFast\n[[autodoc]] FNetTokenizerFast\n## FNetModel\n[[autodoc]] FNetModel\n- forward\n## FNetForPreTraining\n[[autodoc]] FNetForPreTraining\n- forward\n## FNetForMaskedLM\n[[autodoc]] FNetForMaskedLM\n- forward\n## FNetForNextSentencePrediction\n[[autodoc]] FNetForNextSentencePrediction\n- forward\n## FNetForSequenceClassification\n[[autodoc]] FNetForSequenceClassification\n- forward\n## FNetForMultipleChoice\n[[autodoc]] FNetForMultipleChoice\n- forward\n## FNetForTokenClassification\n[[autodoc]] FNetForTokenClassification\n- forward\n## FNetForQuestionAnswering\n[[autodoc]] FNetForQuestionAnswering\n- forward"", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# DeBERTa\n## Overview\nThe DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's\nBERT model released in 2018 and Facebook's RoBERTa model released in 2019. It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in\nRoBERTa. The abstract from the paper is the following:\n\\*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\nlanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\ndisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\ndisentangled attention mechanism, where each word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed using disentangled matrices on their\ncontents and relative positions."", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# BERT\n\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\n\n## Overview\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It's a\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\n\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\narchitecture modifications.\\*\n\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\*\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf).""]","The FNet model replaces the self-attention layer in a BERT model with a Fourier transform, which returns only the real parts of the transform.",Fourier transform
"What type of test should typically accompany a bug fix in Gradio's testing strategy?
","[""Test Strategy\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to. This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Objectives\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n1. Ensure that the Gradio library functions as we expect it to. 2. Enable the maintenance team to quickly identify both the presence and source of defects. 3. Prevent regressions, i.e. if we fix something it should stay fixed. 4. Improve the quality of the codebase in order to ease maintenance efforts. 5. Reduce the amount of manual testing required. ## Scope\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas. We should not focus on code coverage but on test coverage following the below criteria:\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n## Types of testing\nOur tests will broadly fall into one of three categories:\n- Static Quality checks\n- Dynamic 'Code' tests\n- Dynamic Functional tests\n### Static Quality checks\nStatic quality checks are generally very fast to run and do not require building the code base."", ""Tests in this category could be browser-based end-to-end tests, accessibility tests, or performance tests. They are sometimes called acceptance tests. ## Testing tools\nWe currently use the following tools:\n### Static quality checks\n- Python type-checking (python)\n- Black linting (python)\n- ruff formatting (python)\n- prettier formatting (javascript/svelte)\n- TypeScript type-checking (javascript/svelte)\n- eslint linting (javascript/svelte) [in progress]\n### Dynamic code tests\n- pytest (python unit and integration tests)\n- vitest (node-based unit and integration tests)\n- playwright (browser-based unit and integration tests)\n### Functional/acceptance tests\n- playwright (full end to end testing)\n- chromatic (visual testing) [in progress]\n- Accessibility testing [to do]\n## Supported environments and versions\nAll operating systems refer to the current runner variants supported by GitHub actions. All unspecified version segments (`x`) refer to latest. | Software | Version(s) | Operating System(s) |\n| -------- | --------------------- | --------------------------------- |\n| Python | `3.8.x` | `ubuntu-latest`, `windows-latest` |\n| Node | `18.x.x` | `ubuntu-latest` |\n| Browser | `playwright-chrome-x` | `ubuntu-latest` |\n## Test execution\nTests need to be executed in a number of environments and at different stages of the development cycle in order to be useful. The requirements for tests are as follows:\n- \\*\\*Locally\\*\\*: it is important that developers can easily run most tests locally to ensure a passing suite before making a PR. There are some exceptions to this, certain tests may require access to secret values which we cannot make available to all possible contributors for practical security reasons. It is reasonable that it isn't possible to run these tests but they should be disabled by default when running locally. - \\*\\*CI\\*\\* - It is \\_critical\\_ that all tests run successfully in CI with no exceptions. Not every test is required to pass to satisfy CI checks for practical reasons but it is required that all tests should run in CI and notify us if something unexpected happens in order for the development team to take appropriate action. For instructions on how to write and run tests see the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Managing defects\nAs we formalise our testing strategy and bring / keep our test up to standard, it is important that we have some principles on managing defects as they occur/ are reported. For now we can have one very simple rule:\n- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should \\_typically\\_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule."", ""- [#5682](https://github.com/gradio-app/gradio/pull/5682) [`c57f1b75e`](https://github.com/gradio-app/gradio/commit/c57f1b75e272c76b0af4d6bd0c7f44743ff34f26) - Fix functional tests. Thanks [@abidlabs](https://github.com/abidlabs)! - [#5681](https://github.com/gradio-app/gradio/pull/5681) [`40de3d217`](https://github.com/gradio-app/gradio/commit/40de3d2178b61ebe424b6f6228f94c0c6f679bea) - add query parameters to the `gr.Request` object through the `query\\_params` attribute. Thanks [@DarhkVoyd](https://github.com/DarhkVoyd)! - [#5653](https://github.com/gradio-app/gradio/pull/5653) [`ea0e00b20`](https://github.com/gradio-app/gradio/commit/ea0e00b207b4b90a10e9d054c4202d4e705a29ba) - Prevent Clients from accessing API endpoints that set `api\\_name=False`. Thanks [@abidlabs](https://github.com/abidlabs)! - [#5639](https://github.com/gradio-app/gradio/pull/5639) [`e1874aff8`](https://github.com/gradio-app/gradio/commit/e1874aff814d13b23f3e59ef239cc13e18ad3fa7) - Add `gr.on` listener method. Thanks [@aliabid94](https://github.com/aliabid94)! - [#5652](https://github.com/gradio-app/gradio/pull/5652) [`2e25d4305`](https://github.com/gradio-app/gradio/commit/2e25d430582264945ae3316acd04c4453a25ce38) - Pause autoscrolling if a user scrolls up in a `gr.Textbox` and resume autoscrolling if they go all the way down.""]",Every bug fix in Gradio's testing strategy should typically be accompanied by a dynamic code test that failed before the fix and passes afterwards.,Dynamic code test
"How can you force mixed precision training when initializing the Accelerator in ðŸ¤— Accelerate?
","[""Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of ðŸ¤— Accelerate is:\n- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible. - to have the same functions work for any distributed setup, so only have to learn one API. ### How does it work? To see how the library works in practice, let's have a look at each line of code we need to add to a training loop. ```python\naccelerator = Accelerator()\n```\nOn top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script. ```python\nmodel, optim, data = accelerator.prepare(model, optim, data)\n```\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torch.nn.Module`), optimizers (`torch.optim.Optimizer`) and dataloaders (`torch.data.dataloader.DataLoader`). #### Model\nModel preparation include wrapping it in the proper container (for instance `DistributedDataParallel`) and putting it on the proper device. Like with a regular distributed training, you will need to unwrap your model for saving, or to access its specific methods, which can be done with `accelerator.unwrap\\_model(model)`. #### Optimizer\nThe optimizer is also wrapped in a special container that will perform the necessary operations in the step to make mixed precision work."", ""Alternatively, use ðŸ¤— Accelerate to gain full control over the training loop. Find the ðŸ¤— Accelerate example\n[further down in this guide](#using-accelerate). While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can\nresult in a more pronounced training slowdown. Consider the following example. Let's say, the `per\\_device\\_train\\_batch\\_size=4`\nwithout gradient accumulation hits the GPU's limit. If you would like to train with batches of size 64, do not set the\n`per\\_device\\_train\\_batch\\_size` to 1 and `gradient\\_accumulation\\_steps` to 64. Instead, keep `per\\_device\\_train\\_batch\\_size=4`\nand set `gradient\\_accumulation\\_steps=16`. This results in the same effective batch size while making better use of\nthe available GPU resources. For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\nand [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957). ## Gradient Checkpointing\nSome large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used."", ""# Initialize accelerator and tensorboard logging\n... accelerator = Accelerator(\n... mixed\\_precision=config.mixed\\_precision,\n... gradient\\_accumulation\\_steps=config.gradient\\_accumulation\\_steps,\n... log\\_with=\""tensorboard\"",\n... project\\_dir=os.path.join(config.output\\_dir, \""logs\""),\n... )\n... if accelerator.is\\_main\\_process:\n... if config.output\\_dir is not None:\n... os.makedirs(config.output\\_dir, exist\\_ok=True)\n... if config.push\\_to\\_hub:\n... repo\\_id = create\\_repo(\n... repo\\_id=config.hub\\_model\\_id or Path(config.output\\_dir).name, exist\\_ok=True\n... ).repo\\_id\n... accelerator.init\\_trackers(\""train\\_example\"")\n... # Prepare everything\n... # There is no specific order to remember, you just need to unpack the\n... # objects in the same order you gave them to the prepare method. ... model, optimizer, train\\_dataloader, lr\\_scheduler = accelerator.prepare(\n... model, optimizer, train\\_dataloader, lr\\_scheduler\n... )\n... global\\_step = 0\n... # Now you train the model\n... for epoch in range(config.num\\_epochs):\n... progress\\_bar = tqdm(total=len(train\\_dataloader), disable=not accelerator.is\\_local\\_main\\_process)\n... progress\\_bar.set\\_description(f\""Epoch {epoch}\"")\n...""]","You can force mixed precision training when initializing the Accelerator in ðŸ¤— Accelerate by passing the argument `fp16=True` to the `Accelerator()` initialization. Here is an example:

```python
accelerator = Accelerator(fp16=True)
```",By passing `fp16=True` to the Accelerator init.
"What is the purpose of tokenizers in the NLP pipeline?
","[""p align=\""center\"">\n\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\n\n![Build](https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg)\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\n[![Doc](https://docs.rs/tokenizers/badge.svg)](https://docs.rs/tokenizers/)\n\nThe core of `tokenizers`, written in Rust. Provides an implementation of today's most used tokenizers, with a focus on performance and\nversatility. ## What is a Tokenizer\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`. The various steps of the pipeline are:\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\nthe [unicode normalization standards](https://unicode.org/reports/tr15/#Norm\\_Forms), such as `NFD` or `NFKC`. More details about how to use the `Normalizers` are available on the\n[Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\nsplitting text is simply on whitespace. 3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\n`BPE` or `WordPiece`."", ""FrameworkSwitchCourse {fw} />\n# Tokenizers[[tokenizers]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. In NLP tasks, the data that is generally processed is raw text. Here's an example of such text:\n```\nJim Henson was a puppeteer\n```\nHowever, models can only process numbers, so we need to find a way to convert the raw text to numbers. That's what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation â€” that is, the one that makes the most sense to the model â€” and, if possible, the smallest representation. Let's take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization. ## Word-based[[word-based]]\nThe first type of tokenizer that comes to mind is \\_word-based\\_. It's generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:\n\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg)\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg)\n\nThere are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python's `split()` function:\n```py\ntokenized\\_text = \""Jim Henson was a puppeteer\"".split()\nprint(tokenized\\_text)\n```\n```python out\n['Jim', 'Henson', 'was', 'a', 'puppeteer']\n```\nThere are also variations of word tokenizers that have extra rules for punctuation."", ""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a PyTorch tensor.""]","Tokenizers serve the purpose of translating text into data that can be processed by models in the NLP pipeline. Since models can only process numbers, tokenizers convert raw text inputs into numerical data. This process involves several steps, including normalizing the text, creating initial word splits, and performing the actual tokenization to find meaningful numerical representations for the text.",To translate text into data that can be processed by the model.
"What is the purpose of the Safety Checker in the Diffusers library?
","[""### Swap components in a pipeline\nYou can customize the default components of any pipeline with another compatible component. Customization is important because:\n- Changing the scheduler is important for exploring the trade-off between generation speed and quality. - Different components of a model are typically trained independently and you can swap out a component with a better-performing one. - During finetuning, usually only some components - like the UNet or text encoder - are trained. To find out which schedulers are compatible for customization, you can use the `compatibles` method:\n```py\nfrom diffusers import DiffusionPipeline\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\nstable\\_diffusion = DiffusionPipeline.from\\_pretrained(repo\\_id, use\\_safetensors=True)\nstable\\_diffusion.scheduler.compatibles\n```\nLet's use the [`SchedulerMixin.from\\_pretrained`] method to replace the default [`PNDMScheduler`] with a more performant scheduler, [`EulerDiscreteScheduler`]. The `subfolder=\""scheduler\""` argument is required to load the scheduler configuration from the correct [subfolder](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main/scheduler) of the pipeline repository. Then you can pass the new [`EulerDiscreteScheduler`] instance to the `scheduler` argument in [`DiffusionPipeline`]:\n```python\nfrom diffusers import DiffusionPipeline, EulerDiscreteScheduler\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\nscheduler = EulerDiscreteScheduler.from\\_pretrained(repo\\_id, subfolder=\""scheduler\"")\nstable\\_diffusion = DiffusionPipeline.from\\_pretrained(repo\\_id, scheduler=scheduler, use\\_safetensors=True)\n```\n### Safety checker\nDiffusion models like Stable Diffusion can generate harmful content, which is why ðŸ§¨ Diffusers has a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\_diffusion/safety\\_checker.py) to check generated outputs against known hardcoded NSFW content. If you'd like to disable the safety checker for whatever reason, pass `None` to the `safety\\_checker` argument:\n```python\nfrom diffusers import DiffusionPipeline\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\nstable\\_diffusion = DiffusionPipeline.from\\_pretrained(repo\\_id, safety\\_checker=None, use\\_safetensors=True)\n\""\""\""\nYou have disabled the safety checker for  by passing `safety\\_checker=None`."", ""Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us. - [\\*\\*Community tab\\*\\*](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project. - \\*\\*Bias exploration and evaluation\\*\\*: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations. - \\*\\*Encouraging safety in deployment\\*\\*\n- [\\*\\*Safe Stable Diffusion\\*\\*](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable\\_diffusion/stable\\_diffusion\\_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105). - [\\*\\*Safety Checker\\*\\*](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\_diffusion/safety\\_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated.""]","The Safety Checker in the Diffusers library is designed to check generated outputs against known hardcoded NSFW content to prevent the generation of harmful content. It helps ensure that the outputs from diffusion models like Stable Diffusion are safe and appropriate. If needed, users can disable the safety checker by passing `None` to the `safety_checker` argument when initializing the pipeline.",The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.
"What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?
","[""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Interact with Discussions and Pull Requests\nThe `huggingface\\_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood. ## Retrieve Discussions and Pull Requests from the Hub\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(repo\\_id=\""bigscience/bloom\""):\n... print(f\""{discussion.num} - {discussion.title}, pr: {discussion.is\\_pull\\_request}\"")\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer\\_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]\n```\n`HfApi.get\\_repo\\_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(\n..."", ""Pull requests and Discussions\nHub Pull requests and Discussions allow users to do community contributions to repositories. Pull requests and discussions work the same for all the repo types. At a high level, the aim is to build a simpler version of other git hosts' (like GitHub's) PRs and Issues:\n- no forks are involved: contributors push to a special `ref` branch directly on the source repo. - there's no hard distinction between discussions and PRs: they are essentially the same so they are displayed in the same lists. - they are streamlined for ML (i.e. models/datasets/spaces repos), not arbitrary repos. \\_Note, Pull Requests and discussions can be enabled or disabled from the [repository settings](./repositories-settings#disabling-discussions-pull-requests)\\_\n## List\nBy going to the community tab in any repository, you can see all Discussions and Pull requests. You can also filter to only see the ones that are open. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list-dark.png)\n\n## View\nThe Discussion page allows you to see the comments from different users. If it's a Pull Request, you can see all the changes by going to the Files changed tab. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-view.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-view-dark.png)\n\n## Editing a Discussion / Pull request title\nIf you opened a PR or discussion, are the author of the repository, or have write access to it, you can edit the discussion title by clicking on the pencil button. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-edit-title.PNG)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-edit-title-dark.PNG)\n\n## Pin a Discussion / Pull Request\nIf you have write access to a repository, you can pin discussions and Pull Requests. Pinned discussions appear at the top of all the discussions. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pin.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pin-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pinned.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pinned-dark.png)\n\n## Lock a Discussion / Pull Request\nIf you have write access to a repository, you can lock discussions or Pull Requests. Once a discussion is locked, previous comments are still visible and users won't be able to add new comments. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-lock.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-lock-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-locked.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-locked-dark.png)\n\n## Comment edition and moderation\nIf you wrote a comment or have write access to the repository, you can edit the content of the comment from the contextual menu in the top-right corner of the comment box. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-edit.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-edit-dark.png)\n\nOnce the comment has been edited, a new link will appear above the comment. This link shows the edit history. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-link.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-link-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-history.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-history-dark.png)\n\nYou can also hide a comment. Hiding a comment is irreversible, and nobody will be able to see its content nor edit it anymore. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-hidden.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-hidden-dark.png)\n\nRead also [moderation](./moderation) to see how to report an abusive comment. ## Can I use Markdown and LaTeX in my comments and discussions? Yes! You can use Markdown to add formatting to your comments. Additionally, you can utilize LaTeX for mathematical typesetting, your formulas will be rendered with [KaTeX](https://katex.org/) before being parsed in Markdown."", ""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Interacting with Discussions and Pull Requests\nCheck the [`HfApi`] documentation page for the reference of methods enabling\ninteraction with Pull Requests and Discussions on the Hub. - [`get\\_repo\\_discussions`]\n- [`get\\_discussion\\_details`]\n- [`create\\_discussion`]\n- [`create\\_pull\\_request`]\n- [`rename\\_discussion`]\n- [`comment\\_discussion`]\n- [`edit\\_discussion\\_comment`]\n- [`change\\_discussion\\_status`]\n- [`merge\\_pull\\_request`]\n## Data structures\n[[autodoc]] Discussion\n[[autodoc]] DiscussionWithDetails\n[[autodoc]] DiscussionEvent\n[[autodoc]] DiscussionComment\n[[autodoc]] DiscussionStatusChange\n[[autodoc]] DiscussionCommit\n[[autodoc]] DiscussionTitleChange""]",The `HfApi` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.,HfApi
"What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?
","[""--\ntitle: \""Introducing Skops\""\nthumbnail: /blog/assets/94\\_skops/introducing\\_skops.png\nauthors:\n- user: merve\n- user: adrin\n- user: BenjaminB\n---\n# Introducing Skops\n## Introducing Skops\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others. Let's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production. ```python\n# let's import the libraries first\nimport sklearn\nfrom sklearn.datasets import load\\_breast\\_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model\\_selection import train\\_test\\_split\n# Load the data and split\nX, y = load\\_breast\\_cancer(as\\_frame=True, return\\_X\\_y=True)\nX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(\nX, y, test\\_size=0.3, random\\_state=42\n)\n# Train the model\nmodel = DecisionTreeClassifier().fit(X\\_train, y\\_train)\n```\nYou can use any model filename and serialization method, like `pickle` or `joblib`.""]",The new library introduced by Hugging Face for hosting scikit-learn models is called Skops.,Skops
"What is the purpose of Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""I.e. a few pictures of a person can be used to generate images of that person in different styles. ## Textual Inversion\n[Paper](https://arxiv.org/abs/2208.01618)\n[Textual Inversion](../training/text\\_inversion) fine-tunes a model to teach it about a new concept. I.e. a few pictures of a style of artwork can be used to generate images in that style. ## ControlNet\n[Paper](https://arxiv.org/abs/2302.05543)\n[ControlNet](../api/pipelines/controlnet) is an auxiliary network which adds an extra condition. There are 8 canonical pre-trained ControlNets trained on different conditionings such as edge detection, scribbles,\ndepth maps, and semantic segmentations. ## Prompt Weighting\n[Prompt weighting](../using-diffusers/weighted\\_prompts) is a simple technique that puts more attention weight on certain parts of the text\ninput. ## Custom Diffusion\n[Paper](https://arxiv.org/abs/2212.04488)\n[Custom Diffusion](../training/custom\\_diffusion) only fine-tunes the cross-attention maps of a pre-trained\ntext-to-image diffusion model. It also allows for additionally performing Textual Inversion. It supports\nmulti-concept training by design. Like DreamBooth and Textual Inversion, Custom Diffusion is also used to\nteach a pre-trained text-to-image diffusion model about new concepts to generate outputs involving the\nconcept(s) of interest. ## Model Editing\n[Paper](https://arxiv.org/abs/2303.08084)\nThe [text-to-image model editing pipeline](../api/pipelines/model\\_editing) helps you mitigate some of the incorrect implicit assumptions a pre-trained text-to-image\ndiffusion model might make about the subjects present in the input prompt."", ""# Textual Inversion\n[Textual Inversion](https://hf.co/papers/2208.01618) is a training technique for personalizing image generation models with just a few example images of what you want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide. If you're training on a GPU with limited vRAM, you should try enabling the `gradient\\_checkpointing` and `mixed\\_precision` parameters in the training command. You can also reduce your memory footprint by using memory-efficient attention with [xFormers](../optimization/xformers). JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn't support gradient checkpointing or xFormers. With the same configuration and setup as PyTorch, the Flax training script should be at least ~70% faster! This guide will explore the [textual\\_inversion.py](https://github.com/huggingface/diffusers/blob/main/examples/textual\\_inversion/textual\\_inversion.py) script to help you become more familiar with it, and how you can adapt it for your own use-case. Before running the script, make sure you install the library from source:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install . ```\nNavigate to the example folder with the training script and install the required dependencies for the script you're using:\n\n```bash\ncd examples/textual\\_inversion\npip install -r requirements.txt\n```\n\n```bash\ncd examples/textual\\_inversion\npip install -r requirements\\_flax.txt\n```\n\nðŸ¤— Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the ðŸ¤— Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more. Initialize an ðŸ¤— Accelerate environment:\n```bash\naccelerate config\n```\nTo setup a default ðŸ¤— Accelerate environment without choosing any configurations:\n```bash\naccelerate config default\n```\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n```bash\nfrom accelerate.utils import write\\_basic\\_config\nwrite\\_basic\\_config()\n```\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create\\_dataset) guide to learn how to create a dataset that works with the training script.""]","Textual Inversion is a training technique for personalizing image generation models with just a few example images of what you want the model to learn. It works by learning and updating the text embeddings, which are tied to a special word used in the prompt, to match the example images provided. This allows the model to generate images that reflect the new concepts or styles based on the limited examples given.",Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.
"What is the recommended multiple of batch size for fp16 data type on an A100 GPU?
","[""For instance, for fp16 data type a multiple of 8 is recommended, unless\nit's an A100 GPU, in which case use multiples of 64. For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup. ## Gradient Accumulation\nThe \\*\\*gradient accumulation\\*\\* method aims to calculate gradients in smaller increments instead of computing them for the\nentire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward\nand backward passes through the model and accumulating the gradients during the process. Once a sufficient number of\ngradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\nbecomes possible to increase the \\*\\*effective batch size\\*\\* beyond the limitations imposed by the GPU's memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can\nslow down the training process. You can enable gradient accumulation by adding the `gradient\\_accumulation\\_steps` argument to [`TrainingArguments`]:\n```py\ntraining\\_args = TrainingArguments(per\\_device\\_train\\_batch\\_size=1, gradient\\_accumulation\\_steps=4, \\*\\*default\\_args)\n```\nIn the above example, your effective batch size becomes 4."", ""It was run on a single NVIDIA A100-SXM4-80GB GPU with a prompt length of 512. The model we used was `meta-llama/Llama-2-13b-hf`. with batch size = 1:\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None |None|None |None |26.0 |36.958 |27.058 |29152.98 |\n|gptq |False |4 |128 |exllama|36.2 |33.711 |29.663 |10484.34 |\n|bitsandbytes|None |4|None |None |37.64 |52.00 |19.23 |11018.36 |\nwith batch size = 16:\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None |None|None |None |26.0 |69.94 |228.76 |53986.51 |\n|gptq |False |4 |128 |exllama|36.2 |95.41 |167.68 |34777.04 |\n|bitsandbytes|None |4|None |None |37.64 |113.98 |140.38 |35532.37 |\nFrom the benchmark, we can see that bitsandbyes and GPTQ are equivalent, with GPTQ being slightly faster for large batch size."", ""The impact of `fp16` on latency is less marked with `batch\\_size = 1`, but here it is of enormous interest as it can reduce latency by almost half, and almost double throughput! # Concluding remarks\nThis blog post showcased a few simple optimization tricks bundled in the ðŸ¤— ecosystem. Using anyone of these techniques, or a combination of all three, can greatly improve Bark inference speed and memory footprint. \\* You can use the large version of Bark without any performance degradation and a footprint of just 2GB instead of 5GB, 15% faster, \\*\\*using ðŸ¤— Better Transformer and CPU offload\\*\\*. \\* Do you prefer high throughput? \\*\\*Batch by 8 with ðŸ¤— Better Transformer and half-precision\\*\\*. \\* You can get the best of both worlds by using \\*\\*fp16, ðŸ¤— Better Transformer and CPU offload\\*\\*!""]","For fp16 data type on an A100 GPU, it is recommended to use multiples of 64.",64
"How do you run a Gradio Blocks app in reload mode using a Python IDE?
","[""Developing Faster with Auto-Reloading\n\\*\\*Prerequisite\\*\\*: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control). This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks. ## Why Auto-Reloading? When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes. To make it faster and more convenient to write your code, we've made it easier to \""reload\"" your Gradio apps instantly when you are developing in a \\*\\*Python IDE\\*\\* (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \""magic command\"" that allows you to re-run cells faster if you use \\*\\*Jupyter Notebooks\\*\\* (or any similar environment like Colab). This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster. ## Python IDE Reload ðŸ”¥\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Markdown(\""# Greetings from Gradio!\"")\ninp = gr.Textbox(placeholder=\""What is your name?\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""Welcome, {x}!\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`."", ""By default, the Gradio use UTF-8 encoding for scripts. \\*\\*For reload mode\\*\\*, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n1. Configure encoding declaration of python script, for example: `# -\\*- coding: cp1252 -\\*-`\n2. Confirm that your code editor has identified that encoding format. 3. Run like this: `gradio run.py --encoding cp1252`\nðŸ”¥ If your application accepts command line arguments, you can pass them in as well. Here's an example:\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# Greetings {args.name}!\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nWhich you could run like this: `gradio run.py --name Gretel`\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code."", ""WARNING: The --reload flag should not be used in production on Windows. ```\nè¿™é‡Œæœ€é‡è¦çš„ä¸€è¡Œæ˜¯ `æ­£åœ¨è§‚å¯Ÿ ...`ã€‚è¿™é‡Œå‘ç”Ÿçš„æƒ…å†µæ˜¯ Gradio å°†è§‚å¯Ÿ `run.py` æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ï¼Œå¦‚æžœæ–‡ä»¶å‘ç”Ÿæ›´æ”¹ï¼Œå®ƒå°†è‡ªåŠ¨ä¸ºæ‚¨é‡æ–°è¿è¡Œæ–‡ä»¶ã€‚å› æ­¤ï¼Œæ‚¨åªéœ€ä¸“æ³¨äºŽç¼–å†™ä»£ç ï¼ŒGradio æ¼”ç¤ºå°†è‡ªåŠ¨åˆ·æ–° ðŸ¥³\nâš ï¸ è­¦å‘Šï¼š`gradio` å‘½ä»¤ä¸ä¼šæ£€æµ‹ä¼ é€’ç»™ `launch()` æ–¹æ³•çš„å‚æ•°ï¼Œå› ä¸ºåœ¨é‡æ–°åŠ è½½æ¨¡å¼ä¸‹ä»Žæœªè°ƒç”¨ `launch()` æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œè®¾ç½® `launch()` ä¸­çš„ `auth` æˆ– `show\\_error` ä¸ä¼šåœ¨åº”ç”¨ç¨‹åºä¸­åæ˜ å‡ºæ¥ã€‚\nå½“æ‚¨ä½¿ç”¨é‡æ–°åŠ è½½æ¨¡å¼æ—¶ï¼Œè¯·è®°ä½ä¸€ä»¶é‡è¦çš„äº‹æƒ…ï¼šGradio ä¸“é—¨æŸ¥æ‰¾åä¸º `demo` çš„ Gradio Blocks/Interface æ¼”ç¤ºã€‚å¦‚æžœæ‚¨å°†æ¼”ç¤ºå‘½åä¸ºå…¶ä»–åç§°ï¼Œæ‚¨éœ€è¦åœ¨ä»£ç ä¸­çš„ç¬¬äºŒä¸ªå‚æ•°ä¸­ä¼ å…¥æ¼”ç¤ºçš„ FastAPI åº”ç”¨ç¨‹åºçš„åç§°ã€‚å¯¹äºŽ Gradio æ¼”ç¤ºï¼Œå¯ä»¥ä½¿ç”¨ `.app` å±žæ€§è®¿é—® FastAPI åº”ç”¨ç¨‹åºã€‚å› æ­¤ï¼Œå¦‚æžœæ‚¨çš„ `run.py` æ–‡ä»¶å¦‚ä¸‹æ‰€ç¤ºï¼š\n```python\nimport gradio as gr\nwith gr.Blocks() as my\\_demo:\ngr.Markdown(\""# æ¥è‡ªGradioçš„é—®å€™ï¼\"")\ninp = gr.Textbox(placeholder=\""æ‚¨å«ä»€ä¹ˆåå­—ï¼Ÿ\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""æ¬¢è¿Žï¼Œ{x}ï¼\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\nmy\\_demo.launch()\n```\né‚£ä¹ˆæ‚¨å¯ä»¥è¿™æ ·å¯åŠ¨å®ƒï¼š`gradio run.py my\\_demo.app`ã€‚\nGradioé»˜è®¤ä½¿ç”¨UTF-8ç¼–ç æ ¼å¼ã€‚å¯¹äºŽ\\*\\*é‡æ–°åŠ è½½æ¨¡å¼\\*\\*ï¼Œå¦‚æžœä½ çš„è„šæœ¬ä½¿ç”¨çš„æ˜¯é™¤UTF-8ä»¥å¤–çš„ç¼–ç ï¼ˆå¦‚GBKï¼‰ï¼š\n1. åœ¨Pythonè„šæœ¬çš„ç¼–ç å£°æ˜Žå¤„æŒ‡å®šä½ æƒ³è¦çš„ç¼–ç æ ¼å¼ï¼Œå¦‚ï¼š`# -\\*- coding: gbk -\\*-`\n2. ç¡®ä¿ä½ çš„ä»£ç ç¼–è¾‘å™¨è¯†åˆ«åˆ°è¯¥æ ¼å¼ã€‚\n3. æ‰§è¡Œï¼š`gradio run.py --encoding gbk`\nðŸ”¥ å¦‚æžœæ‚¨çš„åº”ç”¨ç¨‹åºæŽ¥å—å‘½ä»¤è¡Œå‚æ•°ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä¼ é€’å®ƒä»¬ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# æ¬¢è¿Ž {args.name}ï¼\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\næ‚¨å¯ä»¥åƒè¿™æ ·è¿è¡Œå®ƒï¼š`gradio run.py --name Gretel`\nä½œä¸ºä¸€ä¸ªå°æç¤ºï¼Œåªè¦æ›´æ”¹äº† `run.py` æºä»£ç æˆ– Gradio æºä»£ç ï¼Œè‡ªåŠ¨é‡æ–°åŠ è½½å°±ä¼šå‘ç”Ÿã€‚è¿™æ„å‘³ç€å¦‚æžœæ‚¨å†³å®š[ä¸º Gradio åšè´¡çŒ®](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md)ï¼Œè¿™å°†éžå¸¸æœ‰ç”¨ âœ…\n## Jupyter Notebook é­”æ³•å‘½ä»¤ðŸ”®\nå¦‚æžœæ‚¨ä½¿ç”¨ Jupyter Notebooksï¼ˆæˆ– Colab Notebooks ç­‰ï¼‰è¿›è¡Œå¼€å‘ï¼Œæˆ‘ä»¬ä¹Ÿä¸ºæ‚¨æä¾›äº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼\næˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ª \\*\\*magic command é­”æ³•å‘½ä»¤\\*\\*ï¼Œå¯ä»¥ä¸ºæ‚¨åˆ›å»ºå’Œè¿è¡Œä¸€ä¸ª Blocks æ¼”ç¤ºã€‚è¦ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œåœ¨ç¬”è®°æœ¬é¡¶éƒ¨åŠ è½½ gradio æ‰©å±•ï¼š\n`%load\\_ext gradio`\nç„¶åŽï¼Œåœ¨æ‚¨æ­£åœ¨å¼€å‘ Gradio æ¼”ç¤ºçš„å•å…ƒæ ¼ä¸­ï¼Œåªéœ€åœ¨é¡¶éƒ¨å†™å…¥é­”æ³•å‘½ä»¤\\*\\*`%%blocks`\\*\\*ï¼Œç„¶åŽåƒå¹³å¸¸ä¸€æ ·ç¼–å†™å¸ƒå±€å’Œç»„ä»¶ï¼š\n```py\n%%blocks\nimport gradio as gr\ngr.Markdown(\""# æ¥è‡ªGradioçš„é—®å€™ï¼\"")\ninp = gr.Textbox(placeholder=\""æ‚¨å«ä»€ä¹ˆåå­—ï¼Ÿ\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""æ¬¢è¿Žï¼Œ{x}ï¼\"",\ninputs=inp,\noutputs=out)\n```\nè¯·æ³¨æ„ï¼š\n- æ‚¨ä¸éœ€è¦æ”¾ç½®æ ·æ¿ä»£ç  `with gr.Blocks() as demo:` å’Œ `demo.launch()` â€” Gradio ä¼šè‡ªåŠ¨ä¸ºæ‚¨å®Œæˆï¼\n- æ¯æ¬¡é‡æ–°è¿è¡Œå•å…ƒæ ¼æ—¶ï¼ŒGradio éƒ½å°†åœ¨ç›¸åŒçš„ç«¯å£ä¸Šé‡æ–°å¯åŠ¨æ‚¨çš„åº”ç”¨ç¨‹åºï¼Œå¹¶ä½¿ç”¨ç›¸åŒçš„åº•å±‚ç½‘ç»œæœåŠ¡å™¨ã€‚è¿™æ„å‘³ç€æ‚¨å°†æ¯”æ­£å¸¸é‡æ–°è¿è¡Œå•å…ƒæ ¼æ›´å¿«åœ°çœ‹åˆ°å˜åŒ–ã€‚\nä¸‹é¢æ˜¯åœ¨ Jupyter Notebook ä¸­çš„ç¤ºä¾‹ï¼š\n![](https://i.ibb.co/nrszFws/Blocks.gif)\nðŸª„è¿™åœ¨ colab ç¬”è®°æœ¬ä¸­ä¹Ÿé€‚ç”¨ï¼[è¿™æ˜¯ä¸€ä¸ª colab ç¬”è®°æœ¬](https://colab.research.google.com/drive/1jUlX1w7JqckRHVE-nbDyMPyZ7fYD8488?authuser=1#scrollTo=zxHYjbCTTz\\_5)ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­çœ‹åˆ° Blocks é­”æ³•æ•ˆæžœã€‚å°è¯•è¿›è¡Œä¸€äº›æ›´æ”¹å¹¶é‡æ–°è¿è¡Œå¸¦æœ‰ Gradio ä»£ç çš„å•å…ƒæ ¼ï¼\nNotebook Magic çŽ°åœ¨æ˜¯ä½œè€…æž„å»º Gradio æ¼”ç¤ºçš„é¦–é€‰æ–¹å¼ã€‚æ— è®ºæ‚¨å¦‚ä½•ç¼–å†™ Python ä»£ç ï¼Œæˆ‘ä»¬éƒ½å¸Œæœ›è¿™ä¸¤ç§æ–¹æ³•éƒ½èƒ½ä¸ºæ‚¨æä¾›æ›´å¥½çš„ Gradio å¼€å‘ä½“éªŒã€‚\n---\n## ä¸‹ä¸€æ­¥\næ—¢ç„¶æ‚¨å·²ç»äº†è§£äº†å¦‚ä½•ä½¿ç”¨ Gradio å¿«é€Ÿå¼€å‘ï¼Œè¯·å¼€å§‹æž„å»ºè‡ªå·±çš„åº”ç”¨ç¨‹åºå§ï¼\nå¦‚æžœä½ æ­£åœ¨å¯»æ‰¾çµæ„Ÿï¼Œè¯·å°è¯•æµè§ˆå…¶ä»–äººç”¨ Gradio æž„å»ºçš„æ¼”ç¤ºï¼Œ[æµè§ˆ Hugging Face Spaces](http://hf.space/) ðŸ¤—""]","To run a Gradio Blocks app in reload mode using a Python IDE, follow these steps:

1. Write your Gradio code in a Python file (e.g., `run.py`). Hereâ€™s an example structure:

   ```python
   import gradio as gr
   with gr.Blocks() as demo:
       gr.Markdown(""# Greetings from Gradio!"")
       inp = gr.Textbox(placeholder=""What is your name?"")
       out = gr.Textbox()
       inp.change(fn=lambda x: f""Welcome, {x}!"", inputs=inp, outputs=out)
   if __name__ == ""__main__"":
       demo.launch()
   ```

2. By default, Gradio uses UTF-8 encoding for scripts. If you are using a different encoding (like cp1252), make sure to:
   - Configure the encoding declaration in your Python script, for example: `# -*- coding: cp1252 -*-`
   - Confirm that your code editor recognizes that encoding format.
   - Run your script with the command: `gradio run.py --encoding cp1252`

3. To enable auto-reloading, simply run your script using the command: `gradio run.py`. Gradio will watch the directory of your `run.py` file, and if any changes are made, it will automatically reload the app.

4. If your application accepts command line arguments, you can pass them as well. For example:

   ```python
   import gradio as gr
   import argparse
   parser = argparse.ArgumentParser()
   parser.add_argument(""--name"", type=str, default=""User"")
   args, unknown = parser.parse_known_args()
   with gr.Blocks() as demo:
       gr.Markdown(f""# Greetings {args.name}!"")
       inp = gr.Textbox()
       out = gr.Textbox()
       inp.change(fn=lambda x: x, inputs=inp, outputs=out)
   if __name__ == ""__main__"":
       demo.launch()
   ```

   You can run it like this: `gradio run.py --name Gretel`.

Remember that the `--reload` flag should not be used in production on Windows.",Run `gradio run.py` in the terminal.
"How can you install the Hugging Face Unity API in your Unity project?
","[""--\ntitle: \""How to Install and Use the Hugging Face Unity API\""\nthumbnail: /blog/assets/124\\_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n# How to Install and Use the Hugging Face Unity API\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API. ## Installation\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif)\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens). 7. Test the API key by clicking `Test API key` in the API Wizard. 8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API\\_URL` field."", ""To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard. ## Usage Tips\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks. 2. Address slow response times or performance issues by changing model endpoints to lower resource models. ## Conclusion\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!"", ""9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png)\nNow that the API is set up, you can make calls from your scripts to the API. Let's look at an example of performing a Sentence Similarity task:\n```\nusing HuggingFace.API;\n/\\* other code \\*/\n// Make a call to the API\nvoid Query() {\nstring inputText = \""I'm on my way to the forest.\"";\nstring[] candidates = {\n\""The player is going to the city\"",\n\""The player is going to the wilderness\"",\n\""The player is wandering aimlessly\""\n};\nHuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\n}\n// If successful, handle the result\nvoid OnSuccess(float[] result) {\nforeach(float value in result) {\nDebug.Log(value);\n}\n}\n// Otherwise, handle the error\nvoid OnError(string error) {\nDebug.LogError(error);\n}\n/\\* other code \\*/\n```\n## Supported Tasks and Custom Models\nThe Hugging Face Unity API also currently supports the following tasks:\n- [Conversation](https://huggingface.co/tasks/conversational)\n- [Text Generation](https://huggingface.co/tasks/text-generation)\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\n- [Text Classification](https://huggingface.co/tasks/text-classification)\n- [Question Answering](https://huggingface.co/tasks/question-answering)\n- [Translation](https://huggingface.co/tasks/translation)\n- [Summarization](https://huggingface.co/tasks/summarization)\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.""]","To install the Hugging Face Unity API in your Unity project, follow these steps:

1. Open your Unity project.
2. Go to `Window` -> `Package Manager`.
3. Click `+` and select `Add Package from git URL`.
4. Enter `https://github.com/huggingface/unity-api.git`.
5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.
6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).
7. Test the API key by clicking `Test API key` in the API Wizard.
8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.
9. Configure advanced settings if desired.
10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard. 

Now the API is set up and ready for use in your Unity project.","To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`."
"What is the pretraining objective of the Wav2Vec2 context network?
","[""Wav2Vec2 Contrastive Loss PreTraining examples\nThe following example showcases how to pretrain a wav2vec2 model using the JAX/Flax backend. Pretraining Wav2Vec2 is rather complex, so it is highly recommended to read the\n[official paper](https://arxiv.org/abs/2006.11477). JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU. Models written in JAX/Flax are \\*\\*immutable\\*\\* and updated in a purely functional\nway which enables simple and efficient model parallelism. `run\\_wav2vec2\\_pretrain\\_flax.py` is a lightweight example of how to download and preprocess a dataset from the ðŸ¤— Datasets library or use your own files (jsonlines or csv), then pretrain the wav2vec2 architectures above on it. For custom datasets in `jsonlines` format please see: [the Datasets documentation](https://huggingface.co/docs/datasets/loading\\_datasets#json-files) and you also will find examples of these below. Let's start by creating a model repository to save the trained model and logs. Here we call the model `\""wav2vec2-base-robust\""`, but you can change the model name as you like. You can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\nyou are logged in) or via the command line:\n```\nhuggingface-cli repo create wav2vec2-base-robust\n```\nNext we clone the model repository to add the tokenizer and model files. ```\ngit clone https://huggingface.co//wav2vec2-base-robust\n```\nTo ensure that all tensorboard traces will be uploaded correctly, we need to\ntrack them. You can run the following command inside your model repo to do so. ```\ncd wav2vec2-base-robust\ngit lfs track \""\\*tfevents\\*\""\n```\nGreat, we have set up our model repository. During training, we will automatically\npush the training logs and model weights to the repo. Next, let's add a symbolic link to the `run\\_wav2vec2\\_pretrain\\_flax`. ```bash\nexport MODEL\\_DIR=\""./wav2vec2-base-robust\""\nln -s ~/transformers/examples/research\\_projects/jax-projects/wav2vec2/run\\_wav2vec2\\_pretrain\\_flax.py ./\n```\n### Create the model configuration\nLet's first create the model configuration and store it in the model repository."", ""!---\nCopyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Speech Recognition Pre-Training\n## Wav2Vec2 Speech Pre-Training\nThe script [`run\\_speech\\_wav2vec2\\_pretraining\\_no\\_trainer.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run\\_wav2vec2\\_pretraining\\_no\\_trainer.py) can be used to pre-train a [Wav2Vec2](https://huggingface.co/transformers/model\\_doc/wav2vec2.html?highlight=wav2vec2) model from scratch. In the script [`run\\_speech\\_wav2vec2\\_pretraining\\_no\\_trainer`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run\\_wav2vec2\\_pretraining\\_no\\_trainer.py), a Wav2Vec2 model is pre-trained on audio data alone using [Wav2Vec2's contrastive loss objective](https://arxiv.org/abs/2006.11477). The following examples show how to fine-tune a `\""base\""`-sized Wav2Vec2 model as well as a `\""large\""`-sized Wav2Vec2 model using [`accelerate`](https://github.com/huggingface/accelerate). ---\n\\*\\*NOTE 1\\*\\*\nWav2Vec2's pre-training is known to be quite unstable. It is advised to do a couple of test runs with a smaller dataset,\n\\*i.e.\\* `--dataset\\_config\\_names clean clean`, `--dataset\\_split\\_names validation test`\nto find good hyper-parameters for `learning\\_rate`, `batch\\_size`, `num\\_warmup\\_steps`,\nand the optimizer. A good metric to observe during training is the gradient norm which should ideally be between 0.5 and 2. ---\n---\n\\*\\*NOTE 2\\*\\*\nWhen training a model on large datasets it is recommended to run the data preprocessing\nin a first run in a \\*\\*non-distributed\\*\\* mode via `--preprocessing\\_only` so that\nwhen running the model in \\*\\*distributed\\*\\* mode in a second step the preprocessed data\ncan easily be loaded on each distributed device. ---\n### Demo\nIn this demo run we pre-train a `\""base-sized\""` Wav2Vec2 model simply only on the validation\nand test data of [librispeech\\_asr](https://huggingface.co/datasets/librispeech\\_asr). The demo is run on two Titan RTX (24 GB RAM each). In case you have less RAM available\nper device, consider reducing `--batch\\_size` and/or the `--max\\_duration\\_in\\_seconds`. ```bash\naccelerate launch run\\_wav2vec2\\_pretraining\\_no\\_trainer.py \\\n--dataset\\_name=\""librispeech\\_asr\"" \\\n--dataset\\_config\\_names clean clean \\\n--dataset\\_split\\_names validation test \\\n--model\\_name\\_or\\_path=\""patrickvonplaten/wav2vec2-base-v2\"" \\\n--output\\_dir=\""./wav2vec2-pretrained-demo\"" \\\n--max\\_train\\_steps=\""20000\"" \\\n--num\\_warmup\\_steps=\""32000\"" \\\n--gradient\\_accumulation\\_steps=\""8\"" \\\n--learning\\_rate=\""0.005\"" \\\n--weight\\_decay=\""0.01\"" \\\n--max\\_duration\\_in\\_seconds=\""20.0\"" \\\n--min\\_duration\\_in\\_seconds=\""2.0\"" \\\n--logging\\_steps=\""1\"" \\\n--saving\\_steps=\""10000\"" \\\n--per\\_device\\_train\\_batch\\_size=\""8\"" \\\n--per\\_device\\_eval\\_batch\\_size=\""8\"" \\\n--adam\\_beta1=\""0.9\"" \\\n--adam\\_beta2=\""0.98\"" \\\n--adam\\_epsilon=\""1e-06\"" \\\n--gradient\\_checkpointing \\\n--mask\\_time\\_prob=\""0.65\"" \\\n--mask\\_time\\_length=\""10\""\n```\nThe results of this run can be seen [here](https://wandb.ai/patrickvonplaten/wav2vec2-pretrained-demo/reports/Wav2Vec2-PreTraining-Demo-Run--VmlldzoxMDk3MjAw?accessToken=oa05s1y57lizo2ocxy3k01g6db1u4pt8m6ur2n8nl4cb0ug02ms2cw313kb8ruch). ### Base\nTo pre-train `\""base-sized\""` Wav2Vec2 model, \\*e.g.\\* [facebook/wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base)\non [librispeech\\_asr](https://huggingface.co/datasets/librispeech\\_asr), the following command can be run:\n```bash\naccelerate launch run\\_wav2vec2\\_pretraining\\_no\\_trainer.py \\\n--dataset\\_name=librispeech\\_asr \\\n--dataset\\_config\\_names clean clean other \\\n--dataset\\_split\\_names train.100 train.360 train.500 \\\n--model\\_name\\_or\\_path=\""patrickvonplaten/wav2vec2-base-v2\"" \\\n--output\\_dir=\""./wav2vec2-pretrained-demo\"" \\\n--max\\_train\\_steps=\""200000\"" \\\n--num\\_warmup\\_steps=\""32000\"" \\\n--gradient\\_accumulation\\_steps=\""4\"" \\\n--learning\\_rate=\""0.001\"" \\\n--weight\\_decay=\""0.01\"" \\\n--max\\_duration\\_in\\_seconds=\""20.0\"" \\\n--min\\_duration\\_in\\_seconds=\""2.0\"" \\\n--logging\\_steps=\""1\"" \\\n--saving\\_steps=\""10000\"" \\\n--per\\_device\\_train\\_batch\\_size=\""8\"" \\\n--per\\_device\\_eval\\_batch\\_size=\""8\"" \\\n--adam\\_beta1=\""0.9\"" \\\n--adam\\_beta2=\""0.98\"" \\\n--adam\\_epsilon=\""1e-06\"" \\\n--gradient\\_checkpointing \\\n--mask\\_time\\_prob=\""0.65\"" \\\n--mask\\_time\\_length=\""10\""\n```\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 4 days. In case you have more than 8 GPUs available for a higher effective `batch\\_size`,\nit is recommended to increase the `learning\\_rate` to `0.005` for faster convergence. The results of this run can be seen [here](https://wandb.ai/patrickvonplaten/test/reports/Wav2Vec2-Base--VmlldzoxMTUyODQ0?accessToken=rg6e8u9yizx964k8q47zctq1m4afpvtn1i3qi9exgdmzip6xwkfzvagfajpzj55n) and the checkpoint pretrained for 85,000 steps can be accessed [here](https://huggingface.co/patrickvonplaten/wav2vec2-base-repro-960h-libri-85k-steps)\n### Large\nTo pre-train `\""large-sized\""` Wav2Vec2 model, \\*e.g.\\* [facebook/wav2vec2-large-lv60](https://huggingface.co/facebook/wav2vec2-large-lv60),\non [librispeech\\_asr](https://huggingface.co/datasets/librispeech\\_asr), the following command can be run:\n```bash\naccelerate launch run\\_wav2vec2\\_pretraining\\_no\\_trainer.py \\\n--dataset\\_name=librispeech\\_asr \\\n--dataset\\_config\\_names clean clean other \\\n--dataset\\_split\\_names train.100 train.360 train.500 \\\n--output\\_dir=./test \\\n--max\\_train\\_steps=200000 \\\n--num\\_warmup\\_steps=32000 \\\n--gradient\\_accumulation\\_steps=8 \\\n--learning\\_rate=0.001 \\\n--weight\\_decay=0.01 \\\n--max\\_duration\\_in\\_seconds=20.0 \\\n--min\\_duration\\_in\\_seconds=2.0 \\\n--model\\_name\\_or\\_path=./\n--logging\\_steps=1 \\\n--saving\\_steps=10000 \\\n--per\\_device\\_train\\_batch\\_size=2 \\\n--per\\_device\\_eval\\_batch\\_size=4 \\\n--adam\\_beta1=0.9 \\\n--adam\\_beta2=0.98 \\\n--adam\\_epsilon=1e-06 \\\n--gradient\\_checkpointing \\\n--mask\\_time\\_prob=0.65 \\\n--mask\\_time\\_length=10\n```\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 7 days. In case you have more than 8 GPUs available for a higher effective `batch\\_size`,\nit is recommended to increase the `learning\\_rate` to `0.005` for faster convergence."", ""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Wav2Vec2\n## Overview\nThe Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. The abstract from the paper is the following:\n\\*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\nrecognition with limited amounts of labeled data.\\*\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). ## Usage tips\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\nusing [`Wav2Vec2CTCTokenizer`]. ## Resources\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with Wav2Vec2.""]","The pretraining objective of the Wav2Vec2 context network is to solve a contrastive task defined over a quantization of the latent representations, which are jointly learned. This approach allows the model to learn powerful representations from speech audio alone, followed by fine-tuning on transcribed speech.",The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.
"What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?
","[""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a TensorFlow tensor."", ""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a PyTorch tensor."", ""Transformers, what can they do?[[transformers-what-can-they-do]]\nIn this section, we will look at what Transformer models can do and use our first tool from the ðŸ¤— Transformers library: the `pipeline()` function. ðŸ‘€ See that *Open in Colab* button on the top right? Click on it to open a Google Colab notebook with all the code samples of this section. This button will be present in any section containing code examples. If you want to run the examples locally, we recommend taking a look at the [setup](/course/chapter0). ## Transformers are everywhere![[transformers-are-everywhere]]\nTransformer models are used to solve all kinds of NLP tasks, like the ones mentioned in the previous section. Here are some of the companies and organizations using Hugging Face and Transformer models, who also contribute back to the community by sharing their models:\n![Companies using Hugging Face](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG)\nThe [ðŸ¤— Transformers library](https://github.com/huggingface/transformers) provides the functionality to create and use those shared models. The [Model Hub](https://huggingface.co/models) contains thousands of pretrained models that anyone can download and use. You can also upload your own models to the Hub! âš ï¸ The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want! [Create a huggingface.co](https://huggingface.co/join) account to benefit from all available features! Before diving into how Transformer models work under the hood, let's look at a few examples of how they can be used to solve some interesting NLP problems. ## Working with pipelines[[working-with-pipelines]]\nThe most basic object in the ðŸ¤— Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\""sentiment-analysis\"")\nclassifier(\""I've been waiting for a HuggingFace course my whole life.\"")\n```\n```python out\n[{'label': 'POSITIVE', 'score': 0.9598047137260437}]\n```\nWe can even pass several sentences! ```python\nclassifier(\n[\""I've been waiting for a HuggingFace course my whole life.\"", \""I hate this so much!\""]\n)\n```\n```python out\n[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n{'label': 'NEGATIVE', 'score': 0.9994558095932007}]\n```\nBy default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English.""]","The default checkpoint used by the sentiment analysis pipeline in the Transformers library is ""distilbert base uncased finetuned sst2 english.""",distilbert base uncased finetuned sst2 english
"What is the purpose of the notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi""?
","[""Although these devices have quite a lot of memory, the model is so large that a single device is not enough to contain a single instance of BLOOMZ. To solve this issue, we are going to use [DeepSpeed](https://www.deepspeed.ai/), which is a deep learning optimization library that enables many memory and speed improvements to accelerate the model and make it fit the device. In particular, we rely here on [DeepSpeed-inference](https://arxiv.org/abs/2207.00032): it introduces several features such as [model (or pipeline) parallelism](https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism) to make the most of the available devices. For Gaudi2, we use [Habana's DeepSpeed fork](https://github.com/HabanaAI/deepspeed) that adds support for HPUs. ### Latency\nWe measured latencies (batch of one sample) for two different sizes of BLOOMZ, both with multi-billion parameters:\n- [176 billion](https://huggingface.co/bigscience/bloomz) parameters\n- [7 billion](https://huggingface.co/bigscience/bloomz-7b1) parameters\nRuns were performed with DeepSpeed-inference in 16-bit precision with 8 devices and using a [key-value cache](https://huggingface.co/docs/transformers/v4.27.1/en/model\\_doc/bloom#transformers.BloomForCausalLM.forward.use\\_cache)."", ""--\ntitle: \""Accelerate Large Model Training using DeepSpeed\""\nthumbnail: /blog/assets/83\\_accelerate\\_deepspeed/deepspeed-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n# Accelerate Large Model Training using DeepSpeed\nIn this post we will look at how we can leverage the \\*\\*[Accelerate](https://github.com/huggingface/accelerate)\\*\\* library for training large models which enables users to leverage the ZeRO features of \\*\\*[DeeSpeed](https://www.deepspeed.ai)\\*\\*. # Motivation ðŸ¤—\n\\*\\*Tired of Out of Memory (OOM) errors while trying to train large models? We've got you covered. Large models are very performant [1] but difficult to train with the available hardware. To get the most of the available hardware for training large models one can leverage Data Parallelism using ZeRO - Zero Redundancy Optimizer [2]\\*\\*. Below is a short description of Data Parallelism using ZeRO with diagram from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)\n![ZeRO Data Parallelism](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero.png)\n(Source: [link](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/))\na."", ""--\ntitle: \""Faster Training and Inference: Habana GaudiÂ®2 vs Nvidia A100 80GB\""\nthumbnail: /blog/assets/habana-gaudi-2-benchmark/thumbnail.png\nauthors:\n- user: regisss\n---\n# Faster Training and Inference: Habana GaudiÂ®-2 vs Nvidia A100 80GB\nIn this article, you will learn how to use [HabanaÂ® GaudiÂ®2](https://habana.ai/training/gaudi2/) to accelerate model training and inference, and train bigger models with ðŸ¤— [Optimum Habana](https://huggingface.co/docs/optimum/habana/index). Then, we present several benchmarks including BERT pre-training, Stable Diffusion inference and T5-3B fine-tuning, to assess the performance differences between first generation Gaudi, Gaudi2 and Nvidia A100 80GB. Spoiler alert - Gaudi2 is about twice faster than Nvidia A100 80GB for both training and inference! [Gaudi2](https://habana.ai/training/gaudi2/) is the second generation AI hardware accelerator designed by Habana Labs. A single server contains 8 accelerator devices with 96GB of memory each (versus 32GB on first generation Gaudi and 80GB on A100 80GB). The Habana SDK, [SynapseAI](https://developer.habana.ai/), is common to both first-gen Gaudi and Gaudi2. That means that ðŸ¤— Optimum Habana, which offers a very user-friendly interface between the ðŸ¤— Transformers and ðŸ¤— Diffusers libraries and SynapseAI, \\*\\*works the exact same way on Gaudi2 as on first-gen Gaudi!\\*\\*\nSo if you already have ready-to-use training or inference workflows for first-gen Gaudi, we encourage you to try them on Gaudi2, as they will work without any single change. ## How to Get Access to Gaudi2? One of the easy, cost-efficient ways that Intel and Habana have made Gaudi2 available is on the Intel Developer Cloud. To start using Gaudi2 there, you should follow the following steps:\n1.""]","The notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi"" is designed to demonstrate how to leverage DeepSpeed for training large models, specifically on the Habana Gaudi hardware. It provides guidance on utilizing the capabilities of DeepSpeed, such as memory optimization and speed improvements, to effectively train models with billions of parameters. This is particularly useful for users facing challenges related to memory limitations when working with large-scale models.",To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.
"What command line module does PyTorch provide to run a script on multiple GPUs?
","[""## PyTorch Distributed\nPyTorch supports [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) which enables data parallelism. To start, create a Python file and import `torch.distributed` and `torch.multiprocessing` to set up the distributed process group and to spawn the processes for inference on each GPU. You should also initialize a [`DiffusionPipeline`]:\n```py\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom diffusers import DiffusionPipeline\nsd = DiffusionPipeline.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n)\n```\nYou'll want to create a function to run inference; [`init\\_process\\_group`](https://pytorch.org/docs/stable/distributed.html?highlight=init\\_process\\_group#torch.distributed.init\\_process\\_group) handles creating a distributed environment with the type of backend to use, the `rank` of the current process, and the `world\\_size` or the number of processes participating. If you're running inference in parallel over 2 GPUs, then the `world\\_size` is 2. Move the [`DiffusionPipeline`] to `rank` and use `get\\_rank` to assign a GPU to each process, where each process handles a different prompt:\n```py\ndef run\\_inference(rank, world\\_size):\ndist.init\\_process\\_group(\""nccl\"", rank=rank, world\\_size=world\\_size)\nsd.to(rank)\nif torch.distributed.get\\_rank() == 0:\nprompt = \""a dog\""\nelif torch.distributed.get\\_rank() == 1:\nprompt = \""a cat\""\nimage = sd(prompt).images[0]\nimage.save(f\""./{'\\_'.join(prompt)}.png\"")\n```\nTo run the distributed inference, call [`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn) to run the `run\\_inference` function on the number of GPUs defined in `world\\_size`:\n```py\ndef main():\nworld\\_size = 2\nmp.spawn(run\\_inference, args=(world\\_size,), nprocs=world\\_size, join=True)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\nmain()\n```\nOnce you've completed the inference script, use the `--nproc\\_per\\_node` argument to specify the number of GPUs to use and call `torchrun` to run the script:\n```bash\ntorchrun run\\_distributed.py --nproc\\_per\\_node=2\n```"", ""Again, remember to ensure to adjust `TORCH\\_CUDA\\_ARCH\\_LIST` to the target architectures. You can find the complete list of NVIDIA GPUs and their corresponding \\*\\*Compute Capabilities\\*\\* (same as arch in this\ncontext) [here](https://developer.nvidia.com/cuda-gpus). You can check the archs pytorch was built with using:\n```bash\npython -c \""import torch; print(torch.cuda.get\\_arch\\_list())\""\n```\nHere is how to find out the arch for one of the installed GPUs. For example, for GPU 0:\n```bash\nCUDA\\_VISIBLE\\_DEVICES=0 python -c \""import torch; \\\nprint(torch.cuda.get\\_device\\_properties(torch.device('cuda')))\""\n```\nIf the output is:\n```bash\n\\_CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, total\\_memory=24268MB, multi\\_processor\\_count=82)\n```\nthen you know that this card's arch is `8.6`. You can also leave `TORCH\\_CUDA\\_ARCH\\_LIST` out completely and then the build program will automatically query the\narchitecture of the GPUs the build is made on. This may or may not match the GPUs on the target machines, that's why\nit's best to specify the desired archs explicitly. If after trying everything suggested you still encounter build issues, please, proceed with the GitHub Issue of\n[Deepspeed](https://github.com/microsoft/DeepSpeed/issues),\n### Deployment with multiple GPUs\nTo deploy the DeepSpeed integration adjust the [`Trainer`] command line arguments to include a new argument `--deepspeed ds\\_config.json`, where `ds\\_config.json` is the DeepSpeed configuration file as\ndocumented [here](https://www.deepspeed.ai/docs/config-json/)."", ""Now we can fit very large models into a single GPU, but the training might still be very slow. The simplest strategy in this scenario is data parallelism: we replicate the same training setup into separate GPUs and pass different batches to each GPU. With this, you can parallelize the forward/backward passes of the model and scale with the number of GPUs. ![chapter10\\_ddp.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/chapter10\\_ddp.png)\nWe use either the `transformers.Trainer` or `accelerate`, which both support data parallelism without any code changes, by simply passing arguments when calling the scripts with `torchrun` or `accelerate launch`. The following runs a training script with 8 GPUs on a single machine with `accelerate` and `torchrun`, respectively. ```bash\naccelerate launch --multi\\_gpu --num\\_machines 1 --num\\_processes 8 my\\_accelerate\\_script.py\ntorchrun --nnodes 1 --nproc\\_per\\_node 8 my\\_torch\\_script.py\n```\n## Supervised fine-tuning\nBefore we start training reward models and tuning our model with RL, it helps if the model is already good in the domain we are interested in.""]","PyTorch provides the `torchrun` command line module to run a script on multiple GPUs. You can use it with the `--nproc_per_node` argument to specify the number of GPUs to use. For example:

```bash
torchrun run_distributed.py --nproc_per_node=2
```",torchrun
"What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?
","[""Vision Transformers å›¾åƒåˆ†ç±»\nç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/abidlabs/vision-transformer\næ ‡ç­¾ï¼šVISION, TRANSFORMERS, HUB\n## ç®€ä»‹\nå›¾åƒåˆ†ç±»æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„é‡è¦ä»»åŠ¡ã€‚æž„å»ºæ›´å¥½çš„åˆ†ç±»å™¨ä»¥ç¡®å®šå›¾åƒä¸­å­˜åœ¨çš„å¯¹è±¡æ˜¯å½“å‰ç ”ç©¶çš„çƒ­ç‚¹é¢†åŸŸï¼Œå› ä¸ºå®ƒåœ¨ä»Žäººè„¸è¯†åˆ«åˆ°åˆ¶é€ è´¨é‡æŽ§åˆ¶ç­‰æ–¹é¢éƒ½æœ‰åº”ç”¨ã€‚\næœ€å…ˆè¿›çš„å›¾åƒåˆ†ç±»å™¨åŸºäºŽ \\_transformers\\_ æž¶æž„ï¼Œè¯¥æž¶æž„æœ€åˆåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å¾ˆå—æ¬¢è¿Žã€‚è¿™ç§æž¶æž„é€šå¸¸è¢«ç§°ä¸º vision transformers (ViT)ã€‚è¿™äº›æ¨¡åž‹éžå¸¸é€‚åˆä¸Ž Gradio çš„\\*å›¾åƒ\\*è¾“å…¥ç»„ä»¶ä¸€èµ·ä½¿ç”¨ï¼Œå› æ­¤åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æž„å»ºä¸€ä¸ªä½¿ç”¨ Gradio è¿›è¡Œå›¾åƒåˆ†ç±»çš„ Web æ¼”ç¤ºã€‚æˆ‘ä»¬åªéœ€ç”¨\\*\\*ä¸€è¡Œ Python ä»£ç \\*\\*å³å¯æž„å»ºæ•´ä¸ª Web åº”ç”¨ç¨‹åºï¼Œå…¶æ•ˆæžœå¦‚ä¸‹ï¼ˆè¯•ç”¨ä¸€ä¸‹ç¤ºä¾‹ä¹‹ä¸€ï¼ï¼‰ï¼š\nè®©æˆ‘ä»¬å¼€å§‹å§ï¼\n### å…ˆå†³æ¡ä»¶\nç¡®ä¿æ‚¨å·²ç»[å®‰è£…](/getting\\_started)äº† `gradio` Python åŒ…ã€‚\n## æ­¥éª¤ 1 - é€‰æ‹© Vision å›¾åƒåˆ†ç±»æ¨¡åž‹\né¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå›¾åƒåˆ†ç±»æ¨¡åž‹ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[Hugging Face Model Hub](https://huggingface.co/models?pipeline\\_tag=image-classification)ä¸Šçš„ä¸€ä¸ªæ¨¡åž‹ã€‚è¯¥ Hub åŒ…å«æ•°åƒä¸ªæ¨¡åž‹ï¼Œæ¶µç›–äº†å¤šç§ä¸åŒçš„æœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚\nåœ¨å·¦ä¾§è¾¹æ ä¸­å±•å¼€ Tasks ç±»åˆ«ï¼Œå¹¶é€‰æ‹©æˆ‘ä»¬æ„Ÿå…´è¶£çš„â€œImage Classificationâ€ä½œä¸ºæˆ‘ä»¬çš„ä»»åŠ¡ã€‚ç„¶åŽï¼Œæ‚¨å°†çœ‹åˆ° Hub ä¸Šä¸ºå›¾åƒåˆ†ç±»è®¾è®¡çš„æ‰€æœ‰æ¨¡åž‹ã€‚\nåœ¨æ’°å†™æ—¶ï¼Œæœ€å—æ¬¢è¿Žçš„æ¨¡åž‹æ˜¯ `google/vit-base-patch16-224`ï¼Œè¯¥æ¨¡åž‹åœ¨åˆ†è¾¨çŽ‡ä¸º 224x224 åƒç´ çš„ ImageNet å›¾åƒä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚æˆ‘ä»¬å°†åœ¨æ¼”ç¤ºä¸­ä½¿ç”¨æ­¤æ¨¡åž‹ã€‚\n## æ­¥éª¤ 2 - ä½¿ç”¨ Gradio åŠ è½½ Vision Transformer æ¨¡åž‹\nå½“ä½¿ç”¨ Hugging Face Hub ä¸Šçš„æ¨¡åž‹æ—¶ï¼Œæˆ‘ä»¬æ— éœ€ä¸ºæ¼”ç¤ºå®šä¹‰è¾“å…¥æˆ–è¾“å‡ºç»„ä»¶ã€‚åŒæ ·ï¼Œæˆ‘ä»¬ä¸éœ€è¦å…³å¿ƒé¢„å¤„ç†æˆ–åŽå¤„ç†çš„ç»†èŠ‚ã€‚æ‰€æœ‰è¿™äº›éƒ½å¯ä»¥ä»Žæ¨¡åž‹æ ‡ç­¾ä¸­è‡ªåŠ¨æŽ¨æ–­å‡ºæ¥ã€‚\né™¤äº†å¯¼å…¥è¯­å¥å¤–ï¼Œæˆ‘ä»¬åªéœ€è¦ä¸€è¡Œä»£ç å³å¯åŠ è½½å¹¶å¯åŠ¨æ¼”ç¤ºã€‚\næˆ‘ä»¬ä½¿ç”¨ `gr.Interface.load()` æ–¹æ³•ï¼Œå¹¶ä¼ å…¥åŒ…å« `huggingface/` çš„æ¨¡åž‹è·¯å¾„ï¼Œä»¥æŒ‡å®šå®ƒæ¥è‡ª Hugging Face Hubã€‚\n```python\nimport gradio as gr\ngr.Interface.load(\n\""huggingface/google/vit-base-patch16-224\"",\nexamples=[\""alligator.jpg\"", \""laptop.jpg\""]).launch()\n```\nè¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ª `examples` å‚æ•°ï¼Œå…è®¸æˆ‘ä»¬ä½¿ç”¨ä¸€äº›é¢„å®šä¹‰çš„ç¤ºä¾‹é¢„å¡«å……æˆ‘ä»¬çš„ç•Œé¢ã€‚\nè¿™å°†ç”Ÿæˆä»¥ä¸‹æŽ¥å£ï¼Œæ‚¨å¯ä»¥ç›´æŽ¥åœ¨æµè§ˆå™¨ä¸­å°è¯•ã€‚å½“æ‚¨è¾“å…¥å›¾åƒæ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨è¿›è¡Œé¢„å¤„ç†å¹¶å‘é€åˆ° Hugging Face Hub APIï¼Œé€šè¿‡æ¨¡åž‹å¤„ç†ï¼Œå¹¶ä»¥äººç±»å¯è§£é‡Šçš„é¢„æµ‹ç»“æžœè¿”å›žã€‚å°è¯•ä¸Šä¼ æ‚¨è‡ªå·±çš„å›¾åƒï¼\n---\nå®Œæˆï¼åªéœ€ä¸€è¡Œä»£ç ï¼Œæ‚¨å°±å»ºç«‹äº†ä¸€ä¸ªå›¾åƒåˆ†ç±»å™¨çš„ Web æ¼”ç¤ºã€‚å¦‚æžœæ‚¨æƒ³ä¸Žä»–äººåˆ†äº«ï¼Œè¯·åœ¨ `launch()` æŽ¥å£æ—¶è®¾ç½® `share=True`ã€‚"", ""For Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage. AutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate ðŸ¤— Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details. ## The technical philosophy\nIn this section, we wanted to share our philosophy behind adding support for Computer Vision in ðŸ¤— Transformers so that the community is aware of the design choices specific to this area. Even though Transformers started with NLP, we support multiple modalities today, for example â€“ vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:\n- Easy model download with a single line of code with `from\\_pretrained()`\n- Easy model upload with `push\\_to\\_hub()`\n- Support for loading huge checkpoints with efficient checkpoint sharding techniques\n- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))\n- Initialization from model configurations\n- Support for both PyTorch and TensorFlow (non-exhaustive)\n- and many more\nUnlike tokenizers, we have preprocessors (such as [this](https://huggingface.co/docs/transformers/model\\_doc/vit#transformers.ViTImageProcessor)) that take care of preparing data for the vision models. We have worked hard to ensure the user experience of using a vision model still feels easy and similar:\n```py\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nimport torch\nfrom datasets import load\\_dataset\ndataset = load\\_dataset(\""huggingface/cats-image\"")\nimage = dataset[\""test\""][\""image\""][0]\nimage\\_processor  = ViTImageProcessor.from\\_pretrained(\""google/vit-base-patch16-224\"")\nmodel = ViTForImageClassification.from\\_pretrained(\""google/vit-base-patch16-224\"")\ninputs = image\\_processor(image, return\\_tensors=\""pt\"")\nwith torch.no\\_grad():\nlogits = model(\\*\\*inputs).logits\n# model predicts one of the 1000 ImageNet classes\npredicted\\_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted\\_label])\n# Egyptian cat\n```\nEven for a difficult task like object detection, the user experience doesnâ€™t change very much:\n```py\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\nfrom PIL import Image\nimport requests\nurl = \""http://images.cocodataset.org/val2017/000000039769.jpg\""\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\\_processor = AutoImageProcessor.from\\_pretrained(\""microsoft/conditional-detr-resnet-50\"")\nmodel = AutoModelForObjectDetection.from\\_pretrained(\""microsoft/conditional-detr-resnet-50\"")\ninputs = image\\_processor(images=image, return\\_tensors=\""pt\"")\noutputs = model(\\*\\*inputs)\n# convert outputs (bounding boxes and class logits) to COCO API\ntarget\\_sizes = torch.tensor([image.size[::-1]])\nresults = image\\_processor.post\\_process\\_object\\_detection(\noutputs, threshold=0.5, target\\_sizes=target\\_sizes\n)[0]\nfor score, label, box in zip(results[\""scores\""], results[\""labels\""], results[\""boxes\""]):\nbox = [round(i, 2) for i in box.tolist()]\nprint(\nf\""Detected {model.config.id2label[label.item()]} with confidence \""\nf\""{round(score.item(), 3)} at location {box}\""\n)\n```\nLeads to:\n```bash\nDetected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\nDetected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\nDetected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\nDetected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\nDetected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\n```\n## Zero-shot models for vision\nThereâ€™s been a surge of models that reformulate core vision tasks like segmentation and detection in interesting ways and introduce even more flexibility."", ""--\ntitle: \""Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\""\nthumbnail: /blog/assets/97\\_vision\\_transformers/thumbnail.png\nauthors:\n- user: juliensimon\n---\n# Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\nThis blog post will show how easy it is to fine-tune pre-trained Transformer models for your dataset using the Hugging Face Optimum library on Graphcore Intelligence Processing Units (IPUs). As an example, we will show a step-by-step guide and provide a notebook that takes a large, widely-used chest X-ray dataset and trains a vision transformer (ViT) model. ## Introducing vision transformer (ViT) models\n\nIn 2017 a group of Google AI researchers published a paper introducing the transformer model architecture. Characterised by a novel self-attention mechanism, transformers were proposed as a new and efficient group of models for language applications. Indeed, in the last five years, transformers have seen explosive popularity and are now accepted as the de facto standard for natural language processing (NLP). Transformers for language are perhaps most notably represented by the rapidly evolving GPT and BERT model families. Both can run easily and efficiently on Graphcore IPUs as part of the growing [Hugging Face Optimum Graphcore library](/posts/getting-started-with-hugging-face-transformers-for-ipus-with-optimum)). ![transformers_chrono](https://www.graphcore.ai/hs-fs/hubfs/transformers_chrono.png?width=1024&name=transformers_chrono.png)\n\nA timeline showing releases of prominent transformer language models (credit: Hugging Face)\n\nAn in-depth explainer about the transformer model architecture (with a focus on NLP) can be found [on the Hugging Face website](https://huggingface.co/course/chapter1/4?fw=pt).""]",åœ¨æ’°å†™æ—¶ï¼Œæœ€å—æ¬¢è¿Žçš„è§†è§‰å˜æ¢å™¨æ¨¡åž‹æ˜¯ `google/vit-base-patch16-224`ï¼Œè¯¥æ¨¡åž‹åœ¨åˆ†è¾¨çŽ‡ä¸º 224x224 åƒç´ çš„ ImageNet å›¾åƒä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚,google/vit-base-patch16-224
"What is the command to upload an ESPnet model to a Hugging Face repository?
","[""For example:\n\\* `Speech2Text` for Automatic Speech Recognition. \\* `Text2Speech` for Text to Speech. \\* `SeparateSpeech` for Audio Source Separation. Here is an inference example:\n```py\nimport soundfile\nfrom espnet2.bin.tts\\_inference import Text2Speech\ntext2speech = Text2Speech.from\\_pretrained(\""model\\_name\"")\nspeech = text2speech(\""foobar\"")[\""wav\""]\nsoundfile.write(\""out.wav\"", speech.numpy(), text2speech.fs, \""PCM\\_16\"")\n```\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png)\n\n## Sharing your models\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\_model\\_zoo#register-your-model)). The `run.sh` script allows to upload a given model to a Hugging Face repository. ```bash\n./run.sh --stage 15 --skip\\_upload\\_hf false --hf\\_repo username/model\\_repo\n```\n## Additional resources\n\\* ESPnet [docs](https://espnet.github.io/espnet/index.html). \\* ESPnet model zoo [repository](https://github.com/espnet/espnet\\_model\\_zoo)."", ""Using ESPnet at Hugging Face\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks. ## Exploring ESPnet in the Hub\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). All models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more. 2. Metadata tags that help for discoverability and contain information such as license, language and datasets. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png)\n\n## Using existing models\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\_model\\_zoo)). If you're interested in doing inference, different classes for different tasks have a `from\\_pretrained` method that allows loading models from the Hub."", ""n this video, we're going to understand how to manage a model repository on the HuggingFace model hub. In order to handle a repository, you should first have a Hugging Face account. A link to create a new account is available in the description. Once you are logged in, you can create a new repository by clicking on the \""New model\"" option. You should be facing a similar modal to the following. In the \""Owner\"" input, you can put either your own namespace or any of your organisations namespaces. The model name is the model identifier that will then be used to identify your model on your chosen namespace. The final choice is between public and private. Public models are accessible by anyone. This is the recommended, free option, as this makes your model easily accessible and shareable. The owners of your namespace are the only ones who can update and change your model. A more advanced option is the private option. In this case, only the owners of your namespace will have visibility over your model. Other users won't know it exists and will not be able to use it. Let's create a dummy model to play with. Once your model is created, comes the management of that model! Three tabs are available to you. You're facing the first one, which is the model card page; this is the page used to showcase your model to the world.""]","The command to upload an ESPnet model to a Hugging Face repository is:

```bash
./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo
```",./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo
"What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?
","[""Add custom Dependencies\nInference Endpointsâ€™ base image includes all required libraries to run inference on ðŸ¤— Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\\* [customize your inference pipeline](/docs/inference-endpoints/guides/custom\\_handler) and need additional Python dependencies\n\\* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)). To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within. ```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\\* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n\\* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom\\_container) for inference."", ""Inference Endpoints Version\nHugging Face Inference Endpoints comes with a default serving container which is used for all [supported Transformers and Sentence-Transformers tasks](/docs/inference-endpoints/supported\\_tasks) and for [custom inference handler](/docs/inference-endpoints/guides/custom\\_handler) and implement batching. Below you will find information about the installed packages and versions used. You can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository. Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom\\_dependencies). ## Installed packages & version\nThe installed packages are split into `general`, `CPU` & `GPU` packages. The `general` packages are installed in all containers, the `CPU` and `GPU` packages are only installed in the corresponding containers. The Hugging Face Inference Runtime has separate versions for `PyTorch` and `TensorFlow` for `CPU` and `GPU`, which are used based on the selected `framework` when creating an Inference Endpoint. The `TensorFlow` and `PyTorch` flavors are grouped together in the list below. ### General\n- `Python`: `3.9.13`\n### CPU\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\n- `diffusers`: `0.14.0`\n- `accelerate`: `0.17.1`\n- `sentence\\_transformers`: `latest`\n- `pandas`: `latest`\n- `pytorch`: `1.13.1`\n- `torchvision`: `0.14.1`\n- `tensorflow`: `2.9.1`\n### GPU\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\n- `diffusers`: `0.14.0`\n- `accelerate`: `0.17.1`\n- `sentence\\_transformers`: `latest`\n- `pandas`: `latest`\n- `pytorch`: `1.13.1=py3.9\\_cuda11.8\\*`\n- `torchvision`: `0.14.1`\n- `tensorflow`: `2.9.1=\\*cuda112\\*py39\\*`"", ""Create custom Inference Handler\nHugging Face Endpoints supports all of the Transformers and Sentence-Transformers tasks and can support custom tasks, including custom pre- & post-processing. The customization can be done through a [handler.py](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) file in your model repository on the Hugging Face Hub. The [handler.py](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) needs to implement the [EndpointHandler](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) class with a `\\_\\_init\\_\\_` and a `\\_\\_call\\_\\_` method. If you want to use custom dependencies, e.g. [optimum](https://raw.githubusercontent.com/huggingface/optimum), the dependencies must be listed in a `requirements.txt` as described above in â€œadd custom dependencies.â€\n## Custom Handler Examples\nThere are already several public examples on the [Hugging Face Hub](https://huggingface.co/models?other=endpoints-template) where you can take insipiration or directly use them. The repositories are tagged with `endpoints-template` and can be found under this [link](https://huggingface.co/models?other=endpoints-template). Included examples are for:\n\\* [Optimum and ONNX Runtime](https://huggingface.co/philschmid/distilbert-onnx-banking77)\n\\* [Image Embeddings with BLIP](https://huggingface.co/florentgbelidji/blip\\_image\\_embeddings)\n\\* [TrOCR for OCR Detection](https://huggingface.co/philschmid/trocr-base-printed)\n\\* [Optimized Sentence Transformers with Optimum](https://huggingface.co/philschmid/all-MiniLM-L6-v2-optimum-embeddings)\n\\* [Pyannote Speaker diarization](https://huggingface.co/philschmid/pyannote-speaker-diarization-endpoint)\n\\* [LayoutLM](https://huggingface.co/philschmid/layoutlm-funsd)\n\\* [Flair NER](https://huggingface.co/philschmid/flair-ner-english-ontonotes-large)\n\\* [GPT-J 6B Single GPU](https://huggingface.co/philschmid/gpt-j-6B-fp16-sharded)\n\\* [Donut Document understanding](https://huggingface.co/philschmid/donut-base-finetuned-cord-v2)\n\\* [SetFit classifier](https://huggingface.co/philschmid/setfit-ag-news-endpoint)\n## Tutorial\nBefore creating a Custom Handler, you need a Hugging Face Model repository with your model weights and an Access Token with \\_WRITE\\_ access to the repository.""]","To install custom Python dependencies for Inference Endpoints, you should add a `requirements.txt` file to your model repository on the Hugging Face Hub. This file should list the Python dependencies you want to install.",requirements.txt
"How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Training with Intel Extension for PyTorch\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \""textual\\_inversion.py\"". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual\\_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script. The example supports both single node and multi-node distributed training:\n### Single node training\n```bash\nexport MODEL\\_NAME=\""CompVis/stable-diffusion-v1-4\""\nexport DATA\\_DIR=\""path-to-dir-containing-dicoo-images\""\npython textual\\_inversion.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--train\\_data\\_dir=$DATA\\_DIR \\\n--learnable\\_property=\""object\"" \\\n--placeholder\\_token=\""\"" --initializer\\_token=\""toy\"" \\\n--seed=7 \\\n--resolution=512 \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=1 \\\n--max\\_train\\_steps=3000 \\\n--learning\\_rate=2.5e-03 --scale\\_lr \\\n--output\\_dir=\""textual\\_inversion\\_dicoo\""\n```\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. You may not get performance speedup without Bfloat16 support. ### Multi-node distributed training\nBefore running the scripts, make sure to install the library's training dependencies successfully:\n```bash\npython -m pip install oneccl\\_bind\\_pt==1.13 -f https://developer.intel.com/ipex-whl-stable-cpu\n```\n```bash\nexport MODEL\\_NAME=\""CompVis/stable-diffusion-v1-4\""\nexport DATA\\_DIR=\""path-to-dir-containing-dicoo-images\""\noneccl\\_bindings\\_for\\_pytorch\\_path=$(python -c \""from oneccl\\_bindings\\_for\\_pytorch import cwd; print(cwd)\"")\nsource $oneccl\\_bindings\\_for\\_pytorch\\_path/env/setvars.sh\npython -m intel\\_extension\\_for\\_pytorch.cpu.launch --distributed \\\n--hostfile hostfile --nnodes 2 --nproc\\_per\\_node 2 textual\\_inversion.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--train\\_data\\_dir=$DATA\\_DIR \\\n--learnable\\_property=\""object\"" \\\n--placeholder\\_token=\""\"" --initializer\\_token=\""toy\"" \\\n--seed=7 \\\n--resolution=512 \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=1 \\\n--max\\_train\\_steps=750 \\\n--learning\\_rate=2.5e-03 --scale\\_lr \\\n--output\\_dir=\""textual\\_inversion\\_dicoo\""\n```\nThe above is a simple distributed training usage on 2 nodes with 2 processes on each node."", ""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install . ```\nThen cd in the example folder and run:\n```bash\npip install -r requirements.txt\n```\nAnd initialize an [ðŸ¤— Accelerate](https://github.com/huggingface/accelerate/) environment with:\n```bash\naccelerate config\n```\n### Cat toy example\nFirst, let's login so that we can upload the checkpoint to the Hub during training:\n```bash\nhuggingface-cli login\n```\nNow let's get our dataset. For this example we will use some cat images: https://huggingface.co/datasets/diffusers/cat\\_toy\\_example . Let's first download it locally:\n```py\nfrom huggingface\\_hub import snapshot\\_download\nlocal\\_dir = \""./cat\""\nsnapshot\\_download(\""diffusers/cat\\_toy\\_example\"", local\\_dir=local\\_dir, repo\\_type=\""dataset\"", ignore\\_patterns=\"".gitattributes\"")\n```\nThis will be our training data. Now we can launch the training using:\n\\*\\*\\_\\_\\_Note: Change the `resolution` to 768 if you are using the [stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2) 768x768 model.\\_\\_\\_\\*\\*\n```bash\nexport MODEL\\_NAME=\""runwayml/stable-diffusion-v1-5\""\nexport DATA\\_DIR=\""./cat\""\naccelerate launch textual\\_inversion.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--train\\_data\\_dir=$DATA\\_DIR \\\n--learnable\\_property=\""object\"" \\\n--placeholder\\_token=\""\"" \\\n--initializer\\_token=\""toy\"" \\\n--resolution=512 \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=4 \\\n--max\\_train\\_steps=3000 \\\n--learning\\_rate=5.0e-04 \\\n--scale\\_lr \\\n--lr\\_scheduler=\""constant\"" \\\n--lr\\_warmup\\_steps=0 \\\n--push\\_to\\_hub \\\n--output\\_dir=\""textual\\_inversion\\_cat\""\n```\nA full training run takes ~1 hour on one V100 GPU.""]","To teach new concepts to Stable Diffusion using Textual Inversion, you need just 3-5 examples.",3-5 images
"What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?
","[""In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future. ## Sharded checkpoints\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save\\_pretrained(save\\_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in. You can control the maximum size before sharding with the `max\\_shard\\_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model. ```py\nfrom transformers import AutoModel\nmodel = AutoModel.from\\_pretrained(\""bert-base-cased\"")\n```\nIf you save it using [`~PreTrainedModel.save\\_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n```py\n>>> import os\n>>> import tempfile\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir)\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model.bin']\n```\nNow let's use a maximum shard size of 200MB:\n```py\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model-00001-of-00003.bin', 'pytorch\\_model-00002-of-00003.bin', 'pytorch\\_model-00003-of-00003.bin', 'pytorch\\_model.bin.index.json']\n```\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index.""]","In Transformers version 4.18.0, the maximum size of a model checkpoint before it is automatically sharded is 10GB.",10GB
"What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?
","[""As in the examples above, some common steps that may help decide whether and how to apply ML in a way that minimizes bias-related risk include:\n\\* Investigate:\n\\* Reports of bias in the field pre-ML\n\\* At-risk demographic categories for your specific use case\n\\* Examine:\n\\* The impact of your optimization objective on reinforcing biases\n\\* Alternative objectives that favor diversity and positive long-term impacts\n### I am curating/picking a dataset for my ML system, how can I address bias? While training datasets are [not the sole source of bias](https://www.cell.com/patterns/fulltext/S2666-3899(21)00061-1) in the ML development cycle, they do play a significant role. Does your [dataset disproportionately associate](https://aclanthology.org/2020.emnlp-main.23/) biographies of women with life events but those of men with achievements? Those \\*\\*stereotypes\\*\\* are probably going to show up in your full ML system! Does your voice recognition dataset only feature specific accents? Not a good sign for [the inclusivity of technology](https://www.scientificamerican.com/article/speech-recognition-tech-is-yet-another-example-of-bias/) you build with it in terms of \\*\\*disparate performance\\*\\*! Whether youâ€™re curating a dataset for ML applications or selecting a dataset to train an ML model, finding out, mitigating, and [communicating](https://dl.acm.org/doi/10.1145/3479582) to what extent the data exhibits these phenomena are all necessary steps to reducing bias-related risks. You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it comes from, who are the people represented on the data, and what the curation process was. Several frameworks for this reflection and documentation have been proposed such as [Data Statements for NLP](https://direct.mit.edu/tacl/article/doi/10.1162/tacl\\_a\\_00041/43452/Data-Statements-for-Natural-Language-Processing) or [Datasheets for Datasets](https://dl.acm.org/doi/10.1145/3458723). The Hugging Face Hub includes a Dataset Card [template](https://github.com/huggingface/datasets/blob/main/templates/README.md) and [guide](https://github.com/huggingface/datasets/blob/main/templates/README\\_guide.md#dataset-card-creation-guide) inspired by these works; the section on [considerations for using the data](https://github.com/huggingface/datasets/blob/main/templates/README\\_guide.md#considerations-for-using-the-data) is usually a good place to look for information about notable biases if youâ€™re browsing datasets, or to write a paragraph sharing your insights on the topic if youâ€™re sharing a new one. And if youâ€™re looking for more inspiration on what to put there, check out these sections written by Hub users in the [BigLAM organization](https://huggingface.co/biglam) for historical datasets of [legal proceedings](https://huggingface.co/datasets/biglam/old\\_bailey\\_proceedings#social-impact-of-dataset), [image classification](https://huggingface.co/datasets/biglam/brill\\_iconclass#social-impact-of-dataset), and [newspapers](https://huggingface.co/datasets/biglam/bnl\\_newspapers1841-1879#social-impact-of-dataset). ![HF Dataset Card guide for the Social Impact and Bias Sections](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img3.png)\n*[HF Dataset Card guide](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md#social-impact-of-dataset) for the Social Impact and Bias Sections*\n\nWhile describing the origin and context of a dataset is always a good starting point to understand the biases at play, [quantitatively measuring phenomena](https://arxiv.org/abs/2212.05129) that encode those biases can be just as helpful."", ""Whether and to what extent bias in the system concretely affects people ultimately depends on what the system is used for. As such, the first place developers can work to mitigate bias is when deciding how ML fits in their system, e.g., by deciding what optimization objective it will use. For example, letâ€™s go back to one of the first highly-publicized cases of a Machine Learning system used in production for algorithmic content recommendation. From 2006 to 2009, Netflix ran the [Netflix Prize](https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/The-Netflix-Prize-Bennett.pdf), a competition with a 1M$ cash prize challenging teams around the world to develop ML systems to accurately predict a userâ€™s rating for a new movie based on their past ratings. The [winning submission](https://www.asc.ohio-state.edu/statistics/dmsl/GrandPrize2009\\_BPC\\_BigChaos.pdf) improved the RMSE (Root-mean-square-error) of predictions on unseen user-movie pairs by over 10% over Netflixâ€™s own CineMatch algorithm, meaning it got much better at predicting how users would rate a new movie based on their history."", ""When the technology works at its best, it can help smooth interactions between people and technical systems, remove the need for highly repetitive work, or unlock new ways of processing information to support research. These same systems are also likely to reproduce discriminatory and abusive behaviors represented in their training data, especially when the data encodes human behaviors. The technology then has the potential to make these issues significantly worse. Automation and deployment at scale can indeed:\n1. \\*\\*lock in\\*\\* behaviors in time and hinder social progress [from being reflected in technology](https://dl.acm.org/doi/10.1145/3442188.3445922),\n2. \\*\\*spread\\*\\* harmful behaviors [beyond the context](https://arxiv.org/abs/2203.07785) of the original training data,\n3. \\*\\*amplify\\*\\* inequities by [overfocusing on stereotypical associations](https://arxiv.org/abs/2010.03058) when making predictions,\n4. \\*\\*remove possibilities for recourse\\*\\* by hiding biases [inside â€œblack-boxâ€ systems](https://pubmed.ncbi.nlm.nih.gov/33737318/). In order to better understand and address these risks, ML researchers and developers have started studying \\_machine bias\\_ or \\_algorithmic bias\\_, mechanisms that might lead systems to, for example, encode \\*\\*negative stereotypes or associations\\*\\* or to have \\*\\*disparate performance\\*\\* for different population groups in their deployment context. \\*\\*These issues are deeply personal\\*\\* for many of us ML researchers and developers at Hugging Face and in the broader ML community. Hugging Face is [an international company](https://twitter.com/osanseviero/status/1587444072901492737), with many of us existing between countries and cultures. It is hard to fully express our sense of urgency when we see the technology we work on developed [without sufficient concern](https://dl.acm.org/doi/10.1145/3461702.3462624) for protecting people like us; especially when these systems lead to discriminatory [wrongful arrests](https://incidentdatabase.ai/cite/72/) or undue [financial distress](https://racismandtechnology.center/2021/10/29/amnestys-grim-warning-against-another-toeslagenaffaire/) and are being [increasingly sold](https://www.oecd.org/migration/mig/EMN-OECD-INFORM-FEB-2022-The-use-of-Digitalisation-and-AI-in-Migration-Management.pdf) to immigration and law enforcement services around the world. Similarly, seeing our identities routinely [suppressed in training datasets](https://aclanthology.org/2021.emnlp-main.98/) or [underrepresented in the outputs](https://huggingface.co/spaces/sasha/StableDiffusionBiasExplorer) of â€œgenerative AIâ€ [systems ](https://twitter.com/willie\\_agnew/status/1592829238889283585)connects these concerns to our daily lived experiences in ways that are [simultaneously enlightening and taxing](https://www.technologyreview.com/2022/10/28/1062332/responsible-ai-has-a-burnout-problem/). While our own experiences do not come close to covering the myriad ways in which ML-mediated discrimination can disproportionately harm people whose experiences differ from ours, they provide an entry point into considerations of the trade-offs inherent in the technology. We work on these systems because we \\*\\*strongly believe in MLâ€™s potential â€” we think it can shine as a valuable tool as long as it is developed with care and input from people in its deployment context\\*\\*, rather than as a one-size-fits-all panacea. In particular, enabling this care requires developing a better understanding of the mechanisms of machine bias across the ML development process, and developing tools that support people [with all levels of technical knowledge of these systems in participating in the necessary conversations](https://www.vice.com/en/article/bvm35w/this-tool-lets-anyone-see-the-bias-in-ai-image-generators) about how their benefits and harms are distributed. The present blog post from the Hugging Face [Ethics and Society regulars](https://huggingface.co/blog/ethics-soc-1) provides an overview of how we have worked, are working, or recommend users of the HF ecosystem of libraries may work to address bias at the various stages of the ML development process, and the tools we develop to support this process.""]","I'm unable to provide information about Weights and Biases (W&B) as it is not related to HuggingFace or gradio-app. If you have questions specifically about HuggingFace or gradio-app, feel free to ask!","To track their machine learning experiments at every stage, from training to production."
"What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\nðŸ¤— Optimum provides an API called BetterTransformer, a fast path of standard PyTorch Transformer APIs to benefit from interesting speedups on CPU & GPU through sparsity and fused kernels as Flash Attention. For now, BetterTransformer supports the fastpath from the native [`nn.TransformerEncoderLayer`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) as well as Flash Attention and Memory-Efficient Attention from [`torch.nn.functional.scaled\\_dot\\_product\\_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled\\_dot\\_product\\_attention.html)."", ""--\ntitle: \""Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\""\nthumbnail: /blog/assets/60\\_habana/habana.png\nauthors:\n- user: susanlansing\nguest: true\n---\n# Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\n\\*Santa Clara and San Francisco, CA, April 12th, 2022\\*\nPowered by deep learning, transformer models deliver state-of-the-art performance on a wide range of machine learning tasks, such as natural language processing, computer vision, speech, and more. However, training them at scale often requires a large amount of computing power, making the whole process unnecessarily long, complex, and costly. Today, [HabanaÂ® Labs](https://habana.ai/), a pioneer in high-efficiency, purpose-built deep learning processors, and Hugging Face, the home of [Transformer](https://github.com/huggingface/transformers) models, are happy to announce that theyâ€™re joining forces to make it easier and quicker to train high-quality transformer models. Thanks to the integration of Habanaâ€™s [SynapseAI software suite](https://habana.ai/training-software/) with the Hugging Face [Optimum open-source library](https://github.com/huggingface/optimum), data scientists and machine learning engineers can now accelerate their Transformer training jobs on Habana processors with just a few lines of code and enjoy greater productivity as well as lower training cost. [Habana Gaudi](https://habana.ai/training/) training solutions, which power Amazonâ€™s EC2 DL1 instances and Supermicroâ€™s X12 Gaudi AI Training Server, deliver price/performance up to 40% lower than comparable training solutions and enable customers to train more while spending less. The integration of ten 100 Gigabit Ethernet ports onto every Gaudi processor enables system scaling from 1 to thousands of Gaudis with ease and cost-efficiency. Habanaâ€™s SynapseAIÂ® is optimizedâ€”at inceptionâ€”to enable Gaudi performance and usability, supports TensorFlow and PyTorch frameworks, with a focus on computer vision and natural language processing applications. With 60,000+ stars on Github, 30,000+ models, and millions of monthly visits, Hugging Face is one of the fastest-growing projects in open source software history, and the go-to place for the machine learning community. With its [Hardware Partner Program](https://huggingface.co/hardware), Hugging Face provides Gaudiâ€™s advanced deep learning hardware with the ultimate Transformer toolset. This partnership will enable rapid expansion of the Habana Gaudi training transformer model library, bringing Gaudi efficiency and ease of use to a wide array of customer use cases like natural language processing, computer vision, speech, and more. â€œ\\*Weâ€™re excited to partner with Hugging Face and its many open-source developers to address the growing demand for transformer models that benefit from the efficiency, usability, and scalability of the Gaudi training platform\\*â€, said Sree Ganesan, head of software product management, Habana Labs. â€œHabana Gaudi brings a new level of efficiency to deep learning model training, and weâ€™re super excited to make this performance easily accessible to Transformer users with minimal code changes through Optimumâ€, said Jeff Boudier, product director at Hugging Face. To learn how to get started training with Habana Gaudi, please visit [https://developer.habana.ai](https://developer.habana.ai)."", ""--\ntitle: \""Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\""\nthumbnail: /blog/assets/80\\_intel/01.png\nauthors:\n- user: juliensimon\n---\n# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\n![image](assets/80\\_intel/01.png)\nThe mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption. Today, we're excited to announce that Intel has officially joined our [Hardware Partner Program](https://huggingface.co/hardware). Thanks to the [Optimum](https://github.com/huggingface/optimum-intel) open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers. Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage. Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms. â€œ\\*Weâ€™re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.\\*â€, says Wei Li, Intel Vice President & General Manager, AI and Analytics. In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs. The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Letâ€™s take a deeper look at it. ## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices.""]",The name of the open-source library created by Hugging Face to simplify Transformer acceleration is Optimum.,Optimum
"What parameter is used to ensure that elements in a row have the same height in Gradio?
","[""Controlling Layout\nBy default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS\\_Flexible\\_Box\\_Layout/Basic\\_Concepts\\_of\\_Flexbox). ## Rows\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\n```python\nwith gr.Blocks() as demo:\nwith gr.Row():\nbtn1 = gr.Button(\""Button 1\"")\nbtn2 = gr.Button(\""Button 2\"")\n```\nTo make every element in a Row have the same height, use the `equal\\_height` argument of the `style` method. ```python\nwith gr.Blocks() as demo:\nwith gr.Row(equal\\_height=True):\ntextbox = gr.Textbox()\nbtn2 = gr.Button(\""Button 2\"")\n```\nThe widths of elements in a Row can be controlled via a combination of `scale` and `min\\_width` arguments that are present in every Component. - `scale` is an integer that defines how an element will take up space in a Row. If scale is set to `0`, and element will not expand to take up space. If scale is set to `1` or greater, the element well expand. Multiple elements in a row will expand proportional to their scale. Below, `btn1` will expand twice as much as `btn2`, while `btn0` will not expand at all:\n```python\nwith gr.Blocks() as demo:\nwith gr.Row():\nbtn0 = gr.Button(\""Button 0\"", scale=0)\nbtn1 = gr.Button(\""Button 1\"", scale=1)\nbtn2 = gr.Button(\""Button 2\"", scale=2)\n```\n- `min\\_width` will set the minimum width the element will take."", ""The Row will wrap if there isn't sufficient space to satisfy all `min\\_width` values. Learn more about Rows in the [docs](https://gradio.app/docs/#row). ## Columns and Nesting\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n$code\\_rows\\_and\\_columns\n$demo\\_rows\\_and\\_columns\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. Learn more about Columns in the [docs](https://gradio.app/docs/#column). # Dimensions\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases. Below is an example illustrating the use of viewport width (vw):\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\nim = gr.ImageEditor(\nwidth=\""50vw\"",\n)\ndemo.launch()\n```\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n```python\nimport gradio as gr\ncss = \""\""\""\n.container {\nheight: 100vh;\n}\n\""\""\""\nwith gr.Blocks(css=css) as demo:\nwith gr.Column(elem\\_classes=[\""container\""]):\nname = gr.Chatbot(value=[[\""1\"", \""2\""]], height=\""70%\"")\ndemo.launch()\n```\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height."", ""Gradio Demo: blocks\\_group\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndef greet(name):\nreturn \""Hello \"" + name + \""!\""\nwith gr.Blocks() as demo:\ngr.Markdown(\""### This is a couple of elements without any gr.Group. Form elements naturally group together anyway.\"")\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\ngr.Image()\ngr.Slider()\ngr.Markdown(\""### This is the same set put in a gr.Group.\"")\nwith gr.Group():\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\ngr.Image()\ngr.Slider()\ngr.Markdown(\""### Now in a Row, no group.\"")\nwith gr.Row():\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\ngr.Image()\ngr.Slider()\ngr.Markdown(\""### Now in a Row in a group.\"")\nwith gr.Group():\nwith gr.Row():\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\ngr.Image()\ngr.Slider()\ngr.Markdown(\""### Several rows grouped together.\"")\nwith gr.Group():\nwith gr.Row():\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\nwith gr.Row():\ngr.Image()\ngr.Audio()\ngr.Markdown(\""### Several columns grouped together. If columns are uneven, there is a gray group background.\"")\nwith gr.Group():\nwith gr.Row():\nwith gr.Column():\nname = gr.Textbox(label=\""Name\"")\nbtn = gr.Button(\""Hello\"")\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True)\ngr.Number()\ngr.Textbox()\nwith gr.Column():\ngr.Image()\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True)\nwith gr.Row():\ngr.Number(scale=2)\ngr.Textbox()\ngr.Markdown(\""### container=False removes label, padding, and block border, placing elements 'directly' on background.\"")\ngr.Radio([1,2,3], container=False)\ngr.Textbox(container=False)\ngr.Image(\""https://picsum.photos/id/237/200/300\"", container=False, height=200)\ngr.Markdown(\""### Textbox, Dropdown, and Number input boxes takes up full space when within a group without a container.\"")\nwith gr.Group():\nname = gr.Textbox(label=\""Name\"")\noutput = gr.Textbox(show\\_label=False, container=False)\ngreet\\_btn = gr.Button(\""Greet\"")\nwith gr.Row():\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True, container=False)\ngr.Textbox(container=False)\ngr.Number(container=False)\ngr.Image(height=100)\ngreet\\_btn.click(fn=greet, inputs=name, outputs=output, api\\_name=\""greet\"")\ngr.Markdown(\""### More examples\"")\nwith gr.Group():\ngr.Chatbot()\nwith gr.Row():\nname = gr.Textbox(label=\""Prompot\"", container=False)\ngo = gr.Button(\""go\"", scale=0)\nwith gr.Column():\ngr.Radio([1,2,3], container=False)\ngr.Slider(0, 20, container=False)\nwith gr.Group():\nwith gr.Row():\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True, container=False, elem\\_id=\""here2\"")\ngr.Number(container=False)\ngr.Textbox(container=False)\nwith gr.Row():\nwith gr.Column():\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True, container=False, elem\\_id=\""here2\"")\nwith gr.Column():\ngr.Number(container=False)\nwith gr.Column():\ngr.Textbox(container=False)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```""]",The parameter used to ensure that elements in a row have the same height in Gradio is the `equal_height` argument of the `style` method.,equal_height
"What is the command to install the latest version of Optimum with OpenVINO support?
","[""![ONNX Runtime](https://github.com/huggingface/optimum/actions/workflows/test\\_onnxruntime.yml/badge.svg)](https://github.com/huggingface/optimum/actions/workflows/test\\_onnxruntime.yml)\n# Hugging Face Optimum\nðŸ¤— Optimum is an extension of ðŸ¤— Transformers and Diffusers, providing a set of optimization tools enabling maximum efficiency to train and run models on targeted hardware, while keeping things easy to use. ## Installation\nðŸ¤— Optimum can be installed using `pip` as follows:\n```bash\npython -m pip install optimum\n```\nIf you'd like to use the accelerator-specific features of ðŸ¤— Optimum, you can install the required dependencies according to the table below:\n| Accelerator | Installation |\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\n| [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/overview) | `pip install --upgrade-strategy eager optimum[onnxruntime]` |\n| [Intel Neural Compressor](https://huggingface.co/docs/optimum/intel/index) | `pip install --upgrade-strategy eager optimum[neural-compressor]`|\n| [OpenVINO](https://huggingface.co/docs/optimum/intel/index) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |\n| [AMD Instinct GPUs and Ryzen AI NPU](https://huggingface.co/docs/optimum/amd/index) | `pip install --upgrade-strategy eager optimum[amd]` |\n| [Habana Gaudi Processor (HPU)](https://huggingface.co/docs/optimum/habana/index) | `pip install --upgrade-strategy eager optimum[habana]` |\n| [FuriosaAI](https://huggingface.co/docs/optimum/furiosa/index) | `pip install --upgrade-strategy eager optimum[furiosa]` |\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version. To install from source:\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\nFor the accelerator-specific features, append `optimum[accelerator\\_type]` to the above command:\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```\n## Accelerated Inference\nðŸ¤— Optimum provides multiple tools to export and run optimized models on various ecosystems:\n- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage\\_guides/export\\_a\\_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage\\_guides/models)\n- TensorFlow Lite\n- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)\n- Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage\\_guides/accelerate\\_inference)\nThe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line. ### Features summary\n| Features | [ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview)| [Neural Compressor](https://huggingface.co/docs/optimum/main/en/intel/optimization\\_inc)| [OpenVINO](https://huggingface.co/docs/optimum/main/en/intel/inference)| [TensorFlow Lite](https://huggingface.co/docs/optimum/main/en/exporters/tflite/overview)|\n|:----------------------------------:|:------------------:|:------------------:|:------------------:|:------------------:|\n| Graph optimization | :heavy\\_check\\_mark: | N/A | :heavy\\_check\\_mark: | N/A |\n| Post-training dynamic quantization | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | N/A | :heavy\\_check\\_mark: |\n| Post-training static quantization | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: |\n| Quantization Aware Training (QAT) | N/A | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | N/A |\n| FP16 (half precision) | :heavy\\_check\\_mark: | N/A | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: |\n| Pruning | N/A | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | N/A |\n| Knowledge Distillation | N/A | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | N/A |\n### OpenVINO\nBefore you begin, make sure you have all the necessary libraries installed :\n```bash\npip install --upgrade-strategy eager optimum[openvino,nncf]\n```\nIt is possible to export ðŸ¤— Transformers and Diffusers models to the OpenVINO format easily:\n```bash\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert\\_sst2\\_ov\n```\nIf you add `--int8`, the weights will be quantized to INT8."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# OpenVINO\nðŸ¤— [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion pipelines compatible with OpenVINO to perform inference on a variety of Intel processors (see the [full list](https://docs.openvino.ai/latest/openvino\\_docs\\_OV\\_UG\\_supported\\_plugins\\_Supported\\_Devices.html) of supported devices). You'll need to install ðŸ¤— Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\n```bash\npip install --upgrade-strategy eager optimum[\""openvino\""]\n```\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO. ## Stable Diffusion\nTo load and run inference, use the [`~optimum.intel.OVStableDiffusionPipeline`]. If you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, set `export=True`:\n```python\nfrom optimum.intel import OVStableDiffusionPipeline\nmodel\\_id = \""runwayml/stable-diffusion-v1-5\""\npipeline = OVStableDiffusionPipeline.from\\_pretrained(model\\_id, export=True)\nprompt = \""sailing ship in storm by Rembrandt\""\nimage = pipeline(prompt).images[0]\n# Don't forget to save the exported model\npipeline.save\\_pretrained(\""openvino-sd-v1-5\"")\n```\nTo further speed-up inference, statically reshape the model. If you change any parameters such as the outputs height or width, youâ€™ll need to statically reshape your model again. ```python\n# Define the shapes related to the inputs and desired outputs\nbatch\\_size, num\\_images, height, width = 1, 1, 512, 512\n# Statically reshape the model\npipeline.reshape(batch\\_size, height, width, num\\_images)\n# Compile the model before inference\npipeline.compile()\nimage = pipeline(\nprompt,\nheight=height,\nwidth=width,\nnum\\_images\\_per\\_prompt=num\\_images,\n).images[0]\n```\n\n![](https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/stable_diffusion_v1_5_sail_boat_rembrandt.png)\n\nYou can find more examples in the ðŸ¤— Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion), and Stable Diffusion is supported for text-to-image, image-to-image, and inpainting. ## Stable Diffusion XL\nTo load and run inference with SDXL, use the [`~optimum.intel.OVStableDiffusionXLPipeline`]:\n```python\nfrom optimum.intel import OVStableDiffusionXLPipeline\nmodel\\_id = \""stabilityai/stable-diffusion-xl-base-1.0\""\npipeline = OVStableDiffusionXLPipeline.from\\_pretrained(model\\_id)\nprompt = \""sailing ship in storm by Rembrandt\""\nimage = pipeline(prompt).images[0]\n```\nTo further speed-up inference, [statically reshape](#stable-diffusion) the model as shown in the Stable Diffusion section."", ""As demonstrated by this [Intel Space](https://huggingface.co/spaces/Intel/Stable-Diffusion-Side-by-Side), the same code runs on a previous generation Intel Xeon (code name Ice Lake) in about 45 seconds. Out of the box, we can see that Sapphire Rapids CPUs are quite faster without any code change! Now, let's accelerate! ## Optimum Intel and OpenVINO\n[Optimum Intel](https://huggingface.co/docs/optimum/intel/index) accelerates end-to-end pipelines on Intel architectures. Its API is extremely similar to the vanilla [Diffusers](https://huggingface.co/docs/diffusers/index) API, making it trivial to adapt existing code. Optimum Intel supports [OpenVINO](https://docs.openvino.ai/latest/index.html), an Intel open-source toolkit for high-performance inference. Optimum Intel and OpenVINO can be installed as follows:\n```\npip install optimum[openvino]\n```\nStarting from the code above, we only need to replace `StableDiffusionPipeline` with `OVStableDiffusionPipeline`. To load a PyTorch model and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model. ```python\nfrom optimum.intel.openvino import OVStableDiffusionPipeline\n... ov\\_pipe = OVStableDiffusionPipeline.from\\_pretrained(model\\_id, export=True)\nlatency = elapsed\\_time(ov\\_pipe, prompt)\nprint(latency)\n# Don't forget to save the exported model\nov\\_pipe.save\\_pretrained(\""./openvino\"")\n```\nOpenVINO automatically optimizes the model for the `bfloat16` format. Thanks to this, the average latency is now \\*\\*16.7 seconds\\*\\*, a sweet 2x speedup. The pipeline above support dynamic input shapes, with no restriction on the number of images or their resolution. With Stable Diffusion, your application is usually restricted to one (or a few) different output resolutions, such as 512x512, or 256x256.""]","The command to install the latest version of Optimum with OpenVINO support is:

```bash
pip install --upgrade-strategy eager optimum[openvino]
```","pip install --upgrade-strategy eager optimum[""openvino""]"

user_input,retrieved_contexts,response,reference
"What architecture is the `tokenizers-linux-x64-musl` binary designed for?
","[""`tokenizers-linux-x64-musl`\nThis is the \\*\\*x86\\_64-unknown-linux-musl\\*\\* binary for `tokenizers`"", ""This is more robust than preparing the pattern yourself: it avoids pitfalls, such as token glueing, that are very hard to debug. To see how much CPU and GPU memory you need for this model or others, try [this calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage) which can help determine that value. The LLaMA tokenizer is a BPE model based on [sentencepiece](https://github.com/google/sentencepiece). One quirk of sentencepiece is that when decoding a sequence, if the first token is the start of the word (e.g. \""Banana\""), the tokenizer does not prepend the prefix space to the string. Code Llama has the same architecture as the `Llama2` models, refer to [Llama2's documentation page](llama2) for the API reference. Find Code Llama tokenizer reference below. ## CodeLlamaTokenizer\n[[autodoc]] CodeLlamaTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## CodeLlamaTokenizerFast\n[[autodoc]] CodeLlamaTokenizerFast\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- update\\_post\\_processor\n- save\\_vocabulary"", ""p align=\""center\"">\n\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\n\n[![Build](https://badge.fury.io/js/tokenizers.svg)](https://badge.fury.io/js/tokenizers)\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\n\nNodeJS implementation of today's most used tokenizers, with a focus on performance and\nversatility. Bindings over the [Rust](https://github.com/huggingface/tokenizers/tree/master/tokenizers) implementation. If you are interested in the High-level design, you can go check it there. ## Main features\n- Train new vocabularies and tokenize using 4 pre-made tokenizers (Bert WordPiece and the 3\nmost common BPE versions). - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes\nless than 20 seconds to tokenize a GB of text on a server's CPU. - Easy to use, but also extremely versatile. - Designed for research and production. - Normalization comes with alignments tracking. It's always possible to get the part of the\noriginal sentence that corresponds to a given token. - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs. ## Installation\n```bash\nnpm install tokenizers@latest\n```\n## Basic example\n```ts\nimport { Tokenizer } from \""tokenizers\"";\nconst tokenizer = await Tokenizer.fromFile(\""tokenizer.json\"");\nconst wpEncoded = await tokenizer.encode(\""Who is John?\"");\nconsole.log(wpEncoded.getLength());\nconsole.log(wpEncoded.getTokens());\nconsole.log(wpEncoded.getIds());\nconsole.log(wpEncoded.getAttentionMask());\nconsole.log(wpEncoded.getOffsets());\nconsole.log(wpEncoded.getOverflowing());\nconsole.log(wpEncoded.getSpecialTokensMask());\nconsole.log(wpEncoded.getTypeIds());\nconsole.log(wpEncoded.getWordIds());\n```\n## License\n[Apache License 2.0](../../LICENSE)""]",The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.,x86_64-unknown-linux-musl
"What is the purpose of the BLIP-Diffusion model?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# BLIP-Diffusion\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. The abstract from the paper is:\n\\*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).\\*\nThe original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization. `BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## BlipDiffusionPipeline\n[[autodoc]] BlipDiffusionPipeline\n- all\n- \\_\\_call\\_\\_\n## BlipDiffusionControlNetPipeline\n[[autodoc]] BlipDiffusionControlNetPipeline\n- all\n- \\_\\_call\\_\\_"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# BLIP\n## Overview\nThe BLIP model was proposed in [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. BLIP is a model that is able to perform various multi-modal tasks including:\n- Visual Question Answering\n- Image-Text retrieval (Image-text matching)\n- Image Captioning\nThe abstract from the paper is the following:\n\\*Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# MultiDiffusion\n[MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation](https://huggingface.co/papers/2302.08113) is by Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. The abstract from the paper is:\n\\*Recent advances in text-to-image generation with diffusion models present transformative capabilities in image quality. However, user controllability of the generated image, and fast adaptation to new tasks still remains an open challenge, currently mostly addressed by costly and long re-training and fine-tuning or ad-hoc adaptations to specific image generation tasks. In this work, we present MultiDiffusion, a unified framework that enables versatile and controllable image generation, using a pre-trained text-to-image diffusion model, without any further training or finetuning. At the center of our approach is a new generation process, based on an optimization task that binds together multiple diffusion generation processes with a shared set of parameters or constraints. We show that MultiDiffusion can be readily applied to generate high quality and diverse images that adhere to user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals, ranging from tight segmentation masks to bounding boxes.\\*\nYou can find additional information about MultiDiffusion on the [project page](https://multidiffusion.github.io/), [original codebase](https://github.com/omerbt/MultiDiffusion), and try it out in a [demo](https://huggingface.co/spaces/weizmannscience/MultiDiffusion).""]","The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation in text-to-image generation tasks. It creates novel renditions of an input subject based on text prompts while overcoming limitations such as lengthy fine-tuning and difficulties in preserving subject fidelity. BLIP-Diffusion introduces a new multimodal encoder that provides subject representation, allowing for efficient generation of new subject renditions and flexible combinations with existing techniques for novel subject-driven generation and editing applications.",The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.
"How can a user claim authorship of a paper on the Hugging Face Hub?
","[""Paper Pages\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png)\n\n## Linking a Paper to a model, dataset or Space\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository's tags. Clicking on the arxiv tag will let you:\n\\* Visit the Paper page. \\* Filter for other models or datasets on the Hub that cite the same paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\n\n## Claiming authorship to a Paper\nThe Hub will attempt to automatically match paper to users based on their email. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png)\n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \""claim authorship\""."", ""![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\n\nRead more about paper pages [here](./paper-pages)."", ""| Partially handled by `huggingface\\_hub`. |""]","To claim authorship of a paper on the Hugging Face Hub, follow these steps:

1. Go to the corresponding Paper page.
2. Click on your name listed on the Paper page.
3. Select the option to ""claim authorship.""

The Hub will attempt to automatically match the paper to users based on their email. If your paper is not linked to your account, you can use the steps above to claim it. 

For more detailed information, you can read about paper pages [here](./paper-pages).","By clicking their name on the corresponding Paper page and clicking ""claim authorship"", then confirming the request in paper settings for admin team validation."
"What is the purpose of the /healthcheck endpoint in the Datasets server API?
","[""Datasets server API - rows endpoint\n> /rows endpoint\n## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### API service\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split"", ""Datasets server admin machine\n> Admin endpoints\n## Configuration\nThe worker can be configured using environment variables. They are grouped by scope. ### Admin service\nSet environment variables to configure the application (`ADMIN\\_` prefix):\n- `ADMIN\\_HF\\_ORGANIZATION`: the huggingface organization from which the authenticated user must be part of in order to access the protected routes, eg. \""huggingface\"". If empty, the authentication is disabled. Defaults to None. - `ADMIN\\_CACHE\\_REPORTS\\_NUM\\_RESULTS`: the number of results in /cache-reports/... endpoints. Defaults to `100`. - `ADMIN\\_CACHE\\_REPORTS\\_WITH\\_CONTENT\\_NUM\\_RESULTS`: the number of results in /cache-reports-with-content/... endpoints. Defaults to `100`. - `ADMIN\\_HF\\_TIMEOUT\\_SECONDS`: the timeout in seconds for the requests to the Hugging Face Hub. Defaults to `0.2` (200 ms). - `ADMIN\\_HF\\_WHOAMI\\_PATH`: the path of the external whoami service, on the hub (see `HF\\_ENDPOINT`), eg. \""/api/whoami-v2\"". Defaults to `/api/whoami-v2`. - `ADMIN\\_MAX\\_AGE`: number of seconds to set in the `max-age` header on technical endpoints. Defaults to `10` (10 seconds). ### Uvicorn\nThe following environment variables are used to configure the Uvicorn server (`ADMIN\\_UVICORN\\_` prefix):\n- `ADMIN\\_UVICORN\\_HOSTNAME`: the hostname. Defaults to `\""localhost\""`. - `ADMIN\\_UVICORN\\_NUM\\_WORKERS`: the number of uvicorn workers. Defaults to `2`. - `ADMIN\\_UVICORN\\_PORT`: the port. Defaults to `8000`. ### Prometheus\n- `PROMETHEUS\\_MULTIPROC\\_DIR`: the directory where the uvicorn workers share their prometheus metrics. See https://github.com/prometheus/client\\_python#multiprocess-mode-eg-gunicorn. Defaults to empty, in which case every worker manages its own metrics, and the /metrics endpoint returns the metrics of a random worker. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nThe admin service provides endpoints:\n- `/healthcheck`\n- `/metrics`: give info about the cache and the queue\n- `/cache-reports{processing\\_step}`: give detailed reports on the content of the cache for a processing step\n- `/cache-reports-with-content{processing\\_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\n- `/force-refresh{processing\\_step}`: force refresh cache entries for the processing step. It's a POST endpoint. Pass the requested parameters, depending on the processing step's input type:\n- `dataset`: `?dataset={dataset}`\n- `config`: `?dataset={dataset}&config={config}`\n- `split`: `?dataset={dataset}&config={config}&split={split}`\n- `/recreate-dataset`: deletes all the cache entries related to a specific dataset, then run all the steps in order. It's a POST endpoint."", ""Datasets server SSE API\n> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache. ## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub\\_cache` field is null for deleted entries, or when the response is an error. The `num\\_rows` value is `0` if it could not be determined. ```typescript\n{\ndataset: string;\nhub\\_cache: null | {\npreview: boolean;\nviewer: boolean;\npartial: boolean;\nnum\\_rows: int;\n};\n}\n```""]",The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the application is running. It serves as a simple way to check the health status of the server.,Ensure the app is running
"What is the default context window size for Local Attention in the LongT5 model?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# LongT5\n## Overview\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention. The abstract from the paper is the following:\n\\*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.\\*\nThis model was contributed by [stancld](https://huggingface.co/stancld). The original code can be found [here](https://github.com/google-research/longt5). ## Usage tips\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder \\*self-attention\\* layer with efficient either \\*local\\* attention or \\*transient-global\\* (\\*tglobal\\*) attention. - Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`]. - LongT5 model is designed to work efficiently and very well on long-range \\*sequence-to-sequence\\* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens. - For \\*Local Attention\\*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). \\*Local Attention\\* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l\\*r)`. - \\*Transient Global Attention\\* is an extension of the \\*Local Attention\\*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\nalso every global token like in the case of standard global attention (\\*transient\\* represents the fact the global tokens\nare constructed dynamically within each attention operation). As a consequence, \\*TGlobal\\* attention introduces\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.""]","The default context window size for Local Attention in the LongT5 model is set to `r=127`, which means a given token can attend to 127 tokens to the left and 127 tokens to the right of it.",127 tokens
"What method is used to load a checkpoint for a task using `AutoPipeline`?
","[""The `AutoPipeline` class is designed to simplify the variety of pipelines in ðŸ¤— Diffusers. It is a generic, \\*task-first\\* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name. Take a look at the [AutoPipeline](../api/pipelines/auto\\_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting. This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights. ## Choose an AutoPipeline for your task\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nprompt = \""peasant and dragon combat, wood cutting style, viking era, bevel with rune\""\nimage = pipeline(prompt, num\\_inference\\_steps=25).images[0]\nimage\n```\n\n![generated image of peasant fighting dragon in wood cutting style](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png)\n\nUnder the hood, [`AutoPipelineForText2Image`]:\n1. automatically detects a `\""stable-diffusion\""` class from the [`model\\_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model\\_index.json) file\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\""stable-diffusion\""` class name\nLikewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\""stable-diffusion\""` checkpoint from the `model\\_index.json` file and it'll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image:\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\npipeline = AutoPipelineForImage2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"",\ntorch\\_dtype=torch.float16,\nuse\\_safetensors=True,\n).to(\""cuda\"")\nprompt = \""a portrait of a dog wearing a pearl earring\""\nurl = \""https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665\\_Girl\\_with\\_a\\_Pearl\\_Earring.jpg/800px-1665\\_Girl\\_with\\_a\\_Pearl\\_Earring.jpg\""\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\""RGB\"")\nimage.thumbnail((768, 768))\nimage = pipeline(prompt, image, num\\_inference\\_steps=200, strength=0.75, guidance\\_scale=10.5).images[0]\nimage\n```\n\n![generated image of a vermeer portrait of a dog wearing a pearl earring](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png)\n\nAnd if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way:\n```py\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image\nimport torch\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""stabilityai/stable-diffusion-xl-base-1.0\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nimg\\_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\_examples/overture-creations-5sI6fQgYIuo.png\""\nmask\\_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\_examples/overture-creations-5sI6fQgYIuo\\_mask.png\""\ninit\\_image = load\\_image(img\\_url).convert(\""RGB\"")\nmask\\_image = load\\_image(mask\\_url).convert(\""RGB\"")\nprompt = \""A majestic tiger sitting on a bench\""\nimage = pipeline(prompt, image=init\\_image, mask\\_image=mask\\_image, num\\_inference\\_steps=50, strength=0.80).images[0]\nimage\n```\n\n![generated image of a tiger sitting on a bench](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png)\n\nIf you try to load an unsupported checkpoint, it'll throw an error:\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\npipeline = AutoPipelineForImage2Image.from\\_pretrained(\n\""openai/shap-e-img2img\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n)\n\""ValueError: AutoPipeline can't find a pipeline linked to ShapEImg2ImgPipeline for None\""\n```\n## Use multiple pipelines\nFor some workflows or if you're loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\nðŸ§¨ Diffusers offers many pipelines, models, and schedulers for generative tasks. To make loading these components as simple as possible, we provide a single and unified method - `from\\_pretrained()` - that loads any of these components from either the Hugging Face [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) or your local machine. Whenever you load a pipeline or model, the latest files are automatically downloaded and cached so you can quickly reuse them next time without redownloading the files. This section will show you everything you need to know about loading pipelines, how to load different components in a pipeline, how to load checkpoint variants, and how to load community pipelines. You'll also learn how to load schedulers and compare the speed and quality trade-offs of using different schedulers. Finally, you'll see how to convert and load KerasCV checkpoints so you can use them in PyTorch with ðŸ§¨ Diffusers."", ""When using\n`DiffusionPipeline.from\\_pretrained(...)`, checkpoints are loaded into\ntheir default pipeline. The default pipeline for the IF is the\ntext-to-image [`IFPipeline`](https://huggingface.co/docs/diffusers/v0.16.0/en/api/pipelines/if#diffusers.IFPipeline). When loading checkpoints\nwith a non-default pipeline, the pipeline must be explicitly specified. ``` python\nfrom diffusers import IFImg2ImgPipeline\npipe = IFImg2ImgPipeline.from\\_pretrained(\n\""DeepFloyd/IF-I-XL-v1.0\"",\ntext\\_encoder=text\\_encoder,\nunet=None,\ndevice\\_map=\""auto\""\n)\n```\nLet\\'s turn our salesman into an anime character. ``` python\nprompt = \""anime style\""\n```\nAs before, we create the text embeddings with T5\n``` python\nprompt\\_embeds, negative\\_embeds = pipe.encode\\_prompt(prompt)\n```\nand free GPU and CPU memory. First, remove the Python pointers\n``` python\ndel text\\_encoder\ndel pipe\n```\nand then free the memory\n``` python\nflush()\n```\n### 2.2 Stage 1: The main diffusion process\nNext, we only load the stage 1 UNet weights into the pipeline object,\njust like we did in the previous section. ``` python\npipe = IFImg2ImgPipeline.from\\_pretrained(\n\""DeepFloyd/IF-I-XL-v1.0\"",\ntext\\_encoder=None,\nvariant=\""fp16\"",\ntorch\\_dtype=torch.float16,\ndevice\\_map=\""auto\""\n)\n```\nThe image variation pipeline requires both the original image and the\nprompt embeddings.""]","The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically detects the correct pipeline class to use based on the checkpoint provided, allowing you to focus on the task without needing to know the specific pipeline class name.",from_pretrained()
"What is the purpose of Diffusers library?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Philosophy\nðŸ§¨ Diffusers provides \\*\\*state-of-the-art\\*\\* pretrained diffusion models across multiple modalities. Its purpose is to serve as a \\*\\*modular toolbox\\*\\* for both inference and training. We aim at building a library that stands the test of time and therefore take API design very seriously. In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch's Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let's go over the most important ones:\n## Usability over Performance\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library. - Diffusers aims to be a \\*\\*light-weight\\*\\* package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages. - Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired. ## Simple over easy\nAs PyTorch states, \\*\\*explicit is better than implicit\\*\\* and \\*\\*simple is better than complex\\*\\*."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n## Preamble\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training. Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library. The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups."", ""--\ntitle: Swift ðŸ§¨Diffusers - Fast Stable Diffusion for Mac\nthumbnail: /blog/assets/fast-mac-diffusers/thumbnail.png\nauthors:\n- user: pcuenq\n- user: reach-vb\n---\n# Swift ðŸ§¨Diffusers: Fast Stable Diffusion for Mac\nTransform your text into stunning images with ease using Diffusers for Mac, a native app powered by state-of-the-art diffusion models. It leverages a bouquet of SoTA Text-to-Image models contributed by the community to the Hugging Face Hub, and converted to Core ML for blazingly fast performance. Our latest version, 1.1, is now available on the [Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) with significant performance upgrades and user-friendly interface tweaks. It's a solid foundation for future feature updates. Plus, the app is fully open source with a permissive [license](https://github.com/huggingface/swift-coreml-diffusers/blob/main/LICENSE), so you can build on it too! Check out our GitHub repository at https://github.com/huggingface/swift-coreml-diffusers for more information. ![Screenshot showing Diffusers for Mac UI](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-mac-diffusers/UI.png)\n## What exactly is ðŸ§¨Diffusers for Mac anyway? The Diffusers app ([App Store](https://apps.apple.com/app/diffusers/id1666309574), [source code](https://github.com/huggingface/swift-coreml-diffusers)) is the Mac counterpart to our [ðŸ§¨`diffusers` library](https://github.com/huggingface/diffusers). This library is written in Python with PyTorch, and uses a modular design to train and run diffusion models.""]","The purpose of the Diffusers library is to provide state-of-the-art pretrained diffusion models across multiple modalities and to serve as a modular toolbox for both inference and training. It aims to be a lightweight package that is easy to use, ensuring usability across different platforms and accelerators. The library is designed to be a natural extension of PyTorch, focusing on usability over performance and simplicity over complexity.",To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.
"What method does the EulerAncestralDiscreteScheduler use for sampling?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# EulerAncestralDiscreteScheduler\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling\\_euler\\_ancestral\\_discrete.EulerAncestralDiscreteSchedulerOutput"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Schedulers\nðŸ¤— Diffusers provides many scheduler functions for the diffusion process. A scheduler takes a model's output (the sample which the diffusion process is iterating on) and a timestep to return a denoised sample. The timestep is important because it dictates where in the diffusion process the step is; data is generated by iterating forward \\*n\\* timesteps and inference occurs by propagating backward through the timesteps. Based on the timestep, a scheduler may be \\*discrete\\* in which case the timestep is an `int` or \\*continuous\\* in which case the timestep is a `float`. Depending on the context, a scheduler defines how to iteratively add noise to an image or how to update a sample based on a model's output:\n- during \\*training\\*, a scheduler adds noise (there are different algorithms for how to add noise) to a sample to train a diffusion model\n- during \\*inference\\*, a scheduler defines how to update a sample based on a pretrained model's output\nMany schedulers are implemented from the [k-diffusion](https://github.com/crowsonkb/k-diffusion) library by [Katherine Crowson](https://github.com/crowsonkb/), and they're also widely used in A1111. To help you map the schedulers from k-diffusion and A1111 to the schedulers in ðŸ¤— Diffusers, take a look at the table below:\n| A1111/k-diffusion | ðŸ¤— Diffusers | Usage |\n|---------------------|-------------------------------------|---------------------------------------------------------------------------------------------------------------|\n| DPM++ 2M | [`DPMSolverMultistepScheduler`] | |\n| DPM++ 2M Karras | [`DPMSolverMultistepScheduler`] | init with `use\\_karras\\_sigmas=True` |\n| DPM++ 2M SDE | [`DPMSolverMultistepScheduler`] | init with `algorithm\\_type=\""sde-dpmsolver++\""` |\n| DPM++ 2M SDE Karras | [`DPMSolverMultistepScheduler`] | init with `use\\_karras\\_sigmas=True` and `algorithm\\_type=\""sde-dpmsolver++\""` |\n| DPM++ 2S a | N/A | very similar to `DPMSolverSinglestepScheduler` |\n| DPM++ 2S a Karras | N/A | very similar to `DPMSolverSinglestepScheduler(use\\_karras\\_sigmas=True, ...)` |\n| DPM++ SDE | [`DPMSolverSinglestepScheduler`] | |\n| DPM++ SDE Karras | [`DPMSolverSinglestepScheduler`] | init with `use\\_karras\\_sigmas=True` |\n| DPM2 | [`KDPM2DiscreteScheduler`] | |\n| DPM2 Karras | [`KDPM2DiscreteScheduler`] | init with `use\\_karras\\_sigmas=True` |\n| DPM2 a | [`KDPM2AncestralDiscreteScheduler`] | |\n| DPM2 a Karras | [`KDPM2AncestralDiscreteScheduler`] | init with `use\\_karras\\_sigmas=True` |\n| DPM adaptive | N/A | |\n| DPM fast | N/A | |\n| Euler | [`EulerDiscreteScheduler`] | |\n| Euler a | [`EulerAncestralDiscreteScheduler`] | |\n| Heun | [`HeunDiscreteScheduler`] | |\n| LMS | [`LMSDiscreteScheduler`] | |\n| LMS Karras | [`LMSDiscreteScheduler`] | init with `use\\_karras\\_sigmas=True` |\n| N/A | [`DEISMultistepScheduler`] | |\n| N/A | [`UniPCMultistepScheduler`] | |\nAll schedulers are built from the base [`SchedulerMixin`] class which implements low level utilities shared by all schedulers. ## SchedulerMixin\n[[autodoc]] SchedulerMixin\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling\\_utils.SchedulerOutput\n## KarrasDiffusionSchedulers\n[`KarrasDiffusionSchedulers`] are a broad generalization of schedulers in ðŸ¤— Diffusers."", ""Empirically, DPMSolver sampling with only 20 steps can generate high-quality\nsamples, and it can generate quite good samples even in 10 steps. The original implementation can be found at [LuChengTHU/dpm-solver](https://github.com/LuChengTHU/dpm-solver). ## Tips\nIt is recommended to set `solver\\_order` to 2 for guide sampling, and `solver\\_order=3` for unconditional sampling. Dynamic thresholding from [Imagen](https://huggingface.co/papers/2205.11487) is supported, and for pixel-space\ndiffusion models, you can set both `algorithm\\_type=\""dpmsolver++\""` and `thresholding=True` to use dynamic\nthresholding. This thresholding method is unsuitable for latent-space diffusion models such as\nStable Diffusion. ## DPMSolverSinglestepScheduler\n[[autodoc]] DPMSolverSinglestepScheduler\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling\\_utils.SchedulerOutput""]","The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for the diffusion process. This approach allows for fast generation of outputs, typically in 20-30 steps.",Ancestral sampling with Euler method steps.
"What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Image tasks with IDEFICS\n[[open-in-colab]]\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach\nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. For instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can\nsolve image-text tasks with a large multimodal model called IDEFICS. [IDEFICS](../model\\_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198),\na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image\nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content,\ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b)\nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the ðŸ¤— Hub."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# FLAN-UL2\n## Overview\nFlan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same configuration as the [UL2](ul2) model released earlier last year. It was fine tuned using the \""Flan\"" prompt tuning and dataset collection. Similar to `Flan-T5`, one can directly use FLAN-UL2 weights without finetuning the model:\nAccording to the original blog here are the notable improvements:\n- The original UL2 model was only trained with receptive field of 512, which made it non-ideal for N-shot prompting where N is large. - The Flan-UL2 checkpoint uses a receptive field of 2048 which makes it more usable for few-shot in-context learning."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# FLAVA\n## Overview\nThe FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022. The paper aims at creating a single unified foundation model which can work across vision, language\nas well as vision-and-language multimodal tasks. The abstract from the paper is the following:\n\\*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety\nof downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal\n(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising\ndirection would be to use a single holistic universal model, as a \""foundation\"", that targets all modalities\nat once -- a true vision and language foundation model should be good at vision tasks, language tasks, and\ncross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate\nimpressive performance on a wide range of 35 tasks spanning these target modalities.\\*\nThis model was contributed by [aps](https://huggingface.co/aps). The original code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava). ## FlavaConfig\n[[autodoc]] FlavaConfig\n## FlavaTextConfig\n[[autodoc]] FlavaTextConfig\n## FlavaImageConfig\n[[autodoc]] FlavaImageConfig\n## FlavaMultimodalConfig\n[[autodoc]] FlavaMultimodalConfig\n## FlavaImageCodebookConfig\n[[autodoc]] FlavaImageCodebookConfig\n## FlavaProcessor\n[[autodoc]] FlavaProcessor\n## FlavaFeatureExtractor\n[[autodoc]] FlavaFeatureExtractor\n## FlavaImageProcessor\n[[autodoc]] FlavaImageProcessor\n- preprocess\n## FlavaForPreTraining\n[[autodoc]] FlavaForPreTraining\n- forward\n## FlavaModel\n[[autodoc]] FlavaModel\n- forward\n- get\\_text\\_features\n- get\\_image\\_features\n## FlavaImageCodebook\n[[autodoc]] FlavaImageCodebook\n- forward\n- get\\_codebook\\_indices\n- get\\_codebook\\_probs\n## FlavaTextModel\n[[autodoc]] FlavaTextModel\n- forward\n## FlavaImageModel\n[[autodoc]] FlavaImageModel\n- forward\n## FlavaMultimodalModel\n[[autodoc]] FlavaMultimodalModel\n- forward""]",The large multimodal model that can solve image-text tasks and is based on Flamingo is called IDEFICS.,IDEFICS
"What is the purpose of the `gradio.Blocks` API?
","[""Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our \\*\\*newly developed\\*\\* low-level API called `gradio.Blocks`. Now, what's the difference between `Interface` and `Blocks`? - âš¡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs. - ðŸ§± `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \""building blocks\""). ### Why Blocks ðŸ§±?[[why-blocks-]]\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\n- Group together related demos as multiple tabs in one web application\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a dropdown) or its visibility based on user input\nWe will explore all of these concepts below. ### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook. ```py\nimport gradio as gr\ndef flip\\_text(x):\nreturn x[::-1]\ndemo = gr.Blocks()\nwith demo:\ngr.Markdown(\n\""\""\""\n# Flip Text! Start typing below to see the output. \""\""\""\n)\ninput = gr.Textbox(placeholder=\""Flip this text\"")\noutput = gr.Textbox()\ninput.change(fn=flip\\_text, inputs=input, outputs=output)\ndemo.launch()\n```\nThis simple example above introduces 4 concepts that underlie Blocks:\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context."", ""### An Overview of Gradio\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do? #### Chatbots with `gr.ChatInterface`\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast). #### Custom Demos with `gr.Blocks`\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction â€” still all in Python. You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners). #### The Gradio Python & JavaScript Ecosystem\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\\* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio\\_client`): query any Gradio app programmatically in Python. \\* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript. \\* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \\* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications â€” for free!"", ""All components are now keyboard navigable and include better colors to be usable by a wider audience. ![](https://media0.giphy.com/media/Kv1bAN7MX3ya5krkEU/giphy.gif)\n\\*\\*3. Server Side Events\\*\\*:\nGradio's built-in queuing system is now the default for every Gradio app. We now use Server Side Events instead of Websockets for the queue. SSE means everything is served over HTTP and has better device support and better scaling than websockets. ![](https://i.imgur.com/ewUIuUc.png)\n\\*\\*4. Custom Share Servers\\*\\*:\nGradio share links can now run on custom domains. You can now set up your own server to serve Gradio share links. To get started, [read our guide here](https://github.com/huggingface/frp/). ![](https://i.imgur.com/VFWVsqn.png)\n5. We now support adding arbitrary JS to your apps using the `js` parameter in Blocks, and arbitrary modifications to the  of your app using the `head` parameter in Blocks\n6. We no longer expose a user's working directory by default when you release a Gradio app.""]","The `gradio.Blocks` API is a low-level API that provides full control over the data flows and layout of your application. It allows you to build complex, multi-step applications by offering more flexibility compared to the high-level `Interface` class. With `Blocks`, you can:

- Group related demos as multiple tabs in one web application.
- Change the layout of your demo, specifying where inputs and outputs are located.
- Create multi-step interfaces where the output of one model becomes the input to the next model.
- Modify a component's properties or visibility based on user input.

Overall, `gradio.Blocks` enables you to design custom and intricate web applications using Gradio.","The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications."
"What is the purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents""?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# unCLIP\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in ðŸ¤— Diffusers comes from kakaobrain's [karlo](https://github.com/kakaobrain/karlo). The abstract from the paper is following:\n\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\*\nYou can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Prior Transformer\nThe Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process. The abstract from the paper is:\n\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\*\n## PriorTransformer\n[[autodoc]] PriorTransformer\n## PriorTransformerOutput\n[[autodoc]] models.prior\\_transformer.PriorTransformerOutput"", ""While CLIPSeg is trained on simple object descriptions (e.g., â€œa carâ€), its CLIP backbone is pre-trained on engineered text templates (e.g., â€œan image of a carâ€, â€œa photo of a carâ€) and kept frozen during training. Once the inputs are preprocessed, we can perform inference to get a binary segmentation map of shape (height, width) for each text query. ```py\nimport torch\nwith torch.no\\_grad():\noutputs = model(\\*\\*inputs)\nlogits = outputs.logits\nprint(logits.shape)\n>>> torch.Size([3, 352, 352])\n```\nLetâ€™s visualize the results to see how well CLIPSeg performed (code is adapted from [this post](https://huggingface.co/blog/clipseg-zero-shot)). ```py\nimport matplotlib.pyplot as plt\nlogits = logits.unsqueeze(1)\n\\_, ax = plt.subplots(1, len(texts) + 1, figsize=(3\\*(len(texts) + 1), 12))\n[a.axis('off') for a in ax.flatten()]\nax[0].imshow(image)\n[ax[i+1].imshow(torch.sigmoid(logits[i][0])) for i in range(len(texts))];\n[ax[i+1].text(0, -15, prompt) for i, prompt in enumerate(texts)]\n```\n\n![CLIPSeg results](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/clipseg_result.png)\n\nAmazing, isnâ€™t it?""]","The two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents"" serves the purpose of leveraging robust representations of images learned by contrastive models like CLIP for image generation. The model consists of a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the generated image embedding. This approach aims to improve image diversity while maintaining minimal loss in photorealism and caption similarity. Additionally, it allows for the production of variations of an image that preserve its semantics and style, while varying non-essential details. The joint embedding space of CLIP also enables language-guided image manipulations in a zero-shot fashion.",The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.
"What command is used to install the requirements for a research project using ðŸ¤— Transformers?
","[""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Research projects\nThis folder contains various research projects using ðŸ¤— Transformers. They are not maintained and require a specific\nversion of ðŸ¤— Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice. If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder."", ""[SEP] [UNK] Transformers : State of the Art NLP\n[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\nquestion answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting-edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\nthen share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\ncan be modified to enable quick research experiments. Why should I use transformers? 1. Easy-to-use state-of-the-art models:\n- High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user-facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint:\n2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages."", ""4. Set up a development environment by running the following command in a virtual environment:\n```bash\npip install -e \"".[dev]\""\n```\nIf ðŸ¤— Transformers was already installed in the virtual environment, remove\nit with `pip uninstall transformers` before reinstalling it in editable\nmode with the `-e` flag. Depending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\nfailure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n(PyTorch, TensorFlow and/or Flax) then do:\n```bash\npip install -e \"".[quality]\""\n```\nwhich should be enough for most use cases. 5. Develop the features in your branch. As you work on your code, you should make sure the test suite\npasses. Run the tests impacted by your changes like this:\n```bash\npytest tests/.py\n```\nFor more information about tests, check out the\n[Testing](https://huggingface.co/docs/transformers/testing) guide. ðŸ¤— Transformers relies on `black` and `ruff` to format its source code\nconsistently. After you make changes, apply automatic style corrections and code verifications\nthat can't be automated in one go with:\n```bash\nmake fixup\n```\nThis target is also optimized to only work with files modified by the PR you're working on. If you prefer to run the checks one after the other, the following command applies the\nstyle corrections:\n```bash\nmake style\n```\nðŸ¤— Transformers also uses `ruff` and a few custom scripts to check for coding mistakes. Quality\ncontrols are run by the CI, but you can run the same checks with:\n```bash\nmake quality\n```\nFinally, we have a lot of scripts to make sure we don't forget to update\nsome files when adding a new model. You can run these scripts with:\n```bash\nmake repo-consistency\n```\nTo learn more about those checks and how to fix any issues with them, check out the\n[Checks on a Pull Request](https://huggingface.co/docs/transformers/pr\\_checks) guide. If you're modifying documents under the `docs/source` directory, make sure the documentation can still be built. This check will also run in the CI when you open a pull request. To run a local check\nmake sure you install the documentation builder:\n```bash\npip install \"".[docs]\""\n```\nRun the following command from the root of the repository:\n```bash\ndoc-builder build transformers docs/source/en --build\\_dir ~/tmp/test-build\n```\nThis will build the documentation in the `~/tmp/test-build` folder where you can inspect the generated\nMarkdown files with your favorite editor.""]","The command used to install the requirements for a research project using ðŸ¤— Transformers is:

```bash
pip install -r requirements.txt
```

You should run this command inside the folder of your choice for the specific research project.",pip install -r requirements.txt
"What task does the `roberta-large-mnli` checkpoint perform?
","[""!-- DISABLE-FRONTMATTER-SECTIONS -->\n# End-of-chapter quiz[[end-of-chapter-quiz]]\nThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood. First, though, let's test what you learned in this chapter! ### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform? roberta-large-mnli page.\""\n},\n{\ntext: \""Text classification\"",\nexplain: \""More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) â€” a task also called *natural language inference*.\"",\ncorrect: true\n},\n{\ntext: \""Text generation\"",\nexplain: \""Look again on the [roberta-large-mnli page](\\\""https://huggingface.co/roberta-large-mnli\\\"").\""\n}\n]}\n/>\n### 2. What will the following code return? ```py\nfrom transformers import pipeline\nner = pipeline(\""ner\"", grouped\\_entities=True)\nner(\""My name is Sylvain and I work at Hugging Face in Brooklyn.\"")\n```\nsentiment-analysis pipeline.\""\n},\n{\ntext: \""It will return a generated text completing this sentence.\"",\nexplain: \""This is incorrect â€” it would be a `text-generation` pipeline.\"",\n},\n{\ntext: \""It will return the words representing persons, organizations or locations.\"",\nexplain: \""Furthermore, with `grouped_entities=True`, it will group together the words belonging to the same entity, like \\\""Hugging Face\\\"".\"",\ncorrect: true\n}\n]}\n/>\n### 3."", ""- [Question answering task guide](../tasks/question\\_answering)\n\\*\\*Multiple choice\\*\\*\n- [`RobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice.ipynb). - [`TFRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice-tf.ipynb). - [Multiple choice task guide](../tasks/multiple\\_choice)\n## RobertaConfig\n[[autodoc]] RobertaConfig\n## RobertaTokenizer\n[[autodoc]] RobertaTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## RobertaTokenizerFast\n[[autodoc]] RobertaTokenizerFast\n- build\\_inputs\\_with\\_special\\_tokens\n\n## RobertaModel\n[[autodoc]] RobertaModel\n- forward\n## RobertaForCausalLM\n[[autodoc]] RobertaForCausalLM\n- forward\n## RobertaForMaskedLM\n[[autodoc]] RobertaForMaskedLM\n- forward\n## RobertaForSequenceClassification\n[[autodoc]] RobertaForSequenceClassification\n- forward\n## RobertaForMultipleChoice\n[[autodoc]] RobertaForMultipleChoice\n- forward\n## RobertaForTokenClassification\n[[autodoc]] RobertaForTokenClassification\n- forward\n## RobertaForQuestionAnswering\n[[autodoc]] RobertaForQuestionAnswering\n- forward\n\n## TFRobertaModel\n[[autodoc]] TFRobertaModel\n- call\n## TFRobertaForCausalLM\n[[autodoc]] TFRobertaForCausalLM\n- call\n## TFRobertaForMaskedLM\n[[autodoc]] TFRobertaForMaskedLM\n- call\n## TFRobertaForSequenceClassification\n[[autodoc]] TFRobertaForSequenceClassification\n- call\n## TFRobertaForMultipleChoice\n[[autodoc]] TFRobertaForMultipleChoice\n- call\n## TFRobertaForTokenClassification\n[[autodoc]] TFRobertaForTokenClassification\n- call\n## TFRobertaForQuestionAnswering\n[[autodoc]] TFRobertaForQuestionAnswering\n- call\n\n## FlaxRobertaModel\n[[autodoc]] FlaxRobertaModel\n- \\_\\_call\\_\\_\n## FlaxRobertaForCausalLM\n[[autodoc]] FlaxRobertaForCausalLM\n- \\_\\_call\\_\\_\n## FlaxRobertaForMaskedLM\n[[autodoc]] FlaxRobertaForMaskedLM\n- \\_\\_call\\_\\_\n## FlaxRobertaForSequenceClassification\n[[autodoc]] FlaxRobertaForSequenceClassification\n- \\_\\_call\\_\\_\n## FlaxRobertaForMultipleChoice\n[[autodoc]] FlaxRobertaForMultipleChoice\n- \\_\\_call\\_\\_\n## FlaxRobertaForTokenClassification\n[[autodoc]] FlaxRobertaForTokenClassification\n- \\_\\_call\\_\\_\n## FlaxRobertaForQuestionAnswering\n[[autodoc]] FlaxRobertaForQuestionAnswering\n- \\_\\_call\\_\\_"", ""- [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the ðŸ¤— Hugging Face Course. - [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- A blog on [Accelerated Inference with Optimum and Transformers Pipelines](https://huggingface.co/blog/optimum-inference) with RoBERTa for question answering. - [`RobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question\\_answering.ipynb). - [`TFRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question\\_answering-tf.ipynb). - [`FlaxRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering). - [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the ðŸ¤— Hugging Face Course.""]","The `roberta-large-mnli` checkpoint performs text classification, specifically for the task of natural language inference (NLI). It classifies whether two sentences are logically linked across three labels: contradiction, neutral, and entailment.",Text classification
"What service is replacing the Paid tier of the Inference API at Hugging Face?
","[""--\ntitle: Introducing our new pricing\nthumbnail: /blog/assets/114\\_pricing-update/thumbnail.png\nauthors:\n- user: sbrandeis\n- user: pierric\n---\n# Introducing our new pricing\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently. First of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints). Along with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations). Your billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records. \\*\\*TL;DR\\*\\*: \\*\\*At HF we monetize by providing simple access to compute for AI\\*\\*, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub. [Read more](https://huggingface.co/docs/hub/billing) about our pricing and billing system. If you have any questions, feel free to reach out. We welcome your feedback ðŸ”¥"", ""Access ðŸ¤— Inference Endpoints\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account. You can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method. There are two pricing plans:\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. - There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested! After you've added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying!"", ""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Inference\nInference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive,\nrunning on a dedicated server can be an interesting option. The `huggingface\\_hub` library provides an easy way to call a\nservice that runs inference for hosted models. There are several services you can connect to:\n- [Inference API](https://huggingface.co/docs/api-inference/index): a service that allows you to run accelerated inference\non Hugging Face's infrastructure for free. This service is a fast way to get started, test different models, and\nprototype AI products. - [Inference Endpoints](https://huggingface.co/inference-endpoints): a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice.""]","The Paid tier of the Inference API service is being replaced by the new solution called **Inference Endpoints**. The Inference API will still be available for everyone to use for free, but Inference Endpoints provide a fast, enterprise-grade inference as a service.",Inference Endpoints
"What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# SqueezeBERT\n## Overview\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers."", ""But, is it even necessary to compute the full intermediate matrix \\\\( \\mathbf{Y}\\_\\text{int} \\\\) ? Not really, because relevant is only the output matrix \\\\( \\mathbf{Y}\\_\\text{out} \\\\). To trade memory for speed, one can thus chunk the linear layers computation to only process one chunk at the time. Defining `config.chunk\\_size\\_feed\\_forward` as \\\\( c\\_{f} \\\\), chunked linear layers are defined as \\\\( \\mathbf{Y}\\_{\\text{out}} = \\left[\\mathbf{Y}\\_{\\text{out}, 1: c\\_{f}}, \\ldots, \\mathbf{Y}\\_{\\text{out}, (n - c\\_{f}): n}\\right] \\\\) with \\\\( \\mathbf{Y}\\_{\\text{out}, (c\\_{f} \\* i): (i \\* c\\_{f} + i)} = \\text{Linear}\\_{\\text{out}}(\\text{Linear}\\_{\\text{int}}(\\mathbf{\\overline{Z}}\\_{(c\\_{f} \\* i): (i \\* c\\_{f} + i)})) \\\\). In practice, it just means that the output is incrementally computed and concatenated to avoid having to store the whole intermediate tensor \\\\( \\mathbf{Y}\\_{\\text{int}} \\\\) in memory. Assuming \\\\( c\\_{f}=1 \\\\) for our example we can illustrate the incremental computation of the output for position \\\\( i=9 \\\\) as follows. ![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific\\_images/master/reformer\\_benchmark/chunked\\_feed\\_forward.png)\nBy processing the inputs in chunks of size 1, the only tensors that have to be stored in memory at the same time are \\\\( \\mathbf{Y}\\_\\text{out} \\\\) of a maximum size of \\\\( 16 \\times d\\_{h} \\\\), \\\\( \\mathbf{y}\\_{\\text{int}, i} \\\\) of size \\\\( d\\_{f} \\\\) and the input \\\\( \\mathbf{\\overline{Z}} \\\\) of size \\\\( 16 \\times d\\_{h} \\\\), with \\\\( d\\_{h} \\\\) being `config.hidden\\_size` \\\\(^{3}\\\\). Finally, it is important to remember that \\*chunked linear layers\\* yield a mathematically equivalent output to conventional linear layers and can therefore be applied to all transformer linear layers. Making use of `config.chunk\\_size\\_feed\\_forward` therefore allows a better trade-off between memory and speed in certain use cases. ---\n\\\\( {}^1 \\\\) For a simpler explanation, the layer norm layer which is normally applied to \\\\( \\mathbf{\\overline{Z}} \\\\) before being processed by the feed forward layers is omitted for now. \\\\( {}^2 \\\\) In `bert-base-uncased`, \\*e.g.\\* the intermediate dimension \\\\( d\\_{f} \\\\) is with 3072 four times larger than the output dimension \\\\( d\\_{h} \\\\). \\\\( {}^3 \\\\) As a reminder, the output `config.num\\_attention\\_heads` is assumed to be 1 for the sake of clarity and illustration in this notebook, so that the output of the self-attention layers can be assumed to be of size `config.hidden\\_size`."", ""The pretraining objective is to predict the masked token based on the context. This allows BERT to fully use the left and right contexts to help it learn a deeper and richer representation of the inputs. However, there was still room for improvement in BERT's pretraining strategy. [RoBERTa](model\\_doc/roberta) improved upon this by introducing a new pretraining recipe that includes training for longer and on larger batches, randomly masking tokens at each epoch instead of just once during preprocessing, and removing the next-sentence prediction objective. The dominant strategy to improve performance is to increase the model size. But training large models is computationally expensive. One way to reduce computational costs is using a smaller model like [DistilBERT](model\\_doc/distilbert). DistilBERT uses [knowledge distillation](https://arxiv.org/abs/1503.02531) - a compression technique - to create a smaller version of BERT while keeping nearly all of its language understanding capabilities.""]","SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers.",Grouped convolutions
"What type of license is the HuggingFace Team's software distributed under?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction"", ""The ability to use Hugging Faceâ€™s comprehensive hub of foundation models means that users can pick the models that best align with their business needs without having to invest in the resources required to train them. This integration is a significant step forward in making foundation models more accessible to enterprises around the world. \\_If youâ€™re interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!\\_"", ""Security\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)""]","The HuggingFace Team's software is distributed under the Apache License, Version 2.0. You can find more details about the license [here](http://www.apache.org/licenses/LICENSE-2.0).","Apache License, Version 2.0"
"What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?
","[""It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\n\\*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\*\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A."", ""However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model\\_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model\\_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model\\_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix. ### Decoder[[nlp-decoder]]\n[GPT-2](model\\_doc/gpt2) is a decoder-only Transformer that predicts the next word in the sequence."", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ALBERT\n\n[![Models](https://img.shields.io/badge/All_model_pages-albert-blueviolet)](https://huggingface.co/models?filter=albert)\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/albert-base-v2)\n\n## Overview\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\nRadu Soricut.""]","The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:

1. Splitting the embedding matrix into two smaller matrices.
2. Using repeating layers that share parameters among groups. 

These techniques help in scaling the model more efficiently compared to the original BERT.",Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.
"What are the three main steps for fine-tuning a model with the ðŸ¤— Datasets library?
","[""Introduction[[introduction]]\nIn [Chapter 3](/course/chapter3) you got your first taste of the ðŸ¤— Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics. But this is just scratching the surface of what ðŸ¤— Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\\* What do you do when your dataset is not on the Hub? \\* How can you slice and dice a dataset? (And what if you \\_really\\_ need to use Pandas?)\n\\* What do you do when your dataset is huge and will melt your laptop's RAM? \\* What the heck are \""memory mapping\"" and Apache Arrow? \\* How can you create your own dataset and push it to the Hub? The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!"", ""To verify that all libraries are correctly installed, you can run the following command in a Python shell. It verifies that both `transformers` and `datasets` have been correclty installed. ```python\nfrom transformers import AutoModelForCTC, AutoProcessor\nfrom datasets import load\\_dataset\ndummy\\_dataset = load\\_dataset(\""common\\_voice\"", \""ab\"", split=\""test\"")\nmodel = AutoModelForCTC.from\\_pretrained(\""hf-internal-testing/tiny-random-wav2vec2\"")\nmodel.to(\""cuda\"")\nprocessor = AutoProcessor.from\\_pretrained(\""hf-internal-testing/tiny-random-wav2vec2\"")\ninput\\_values = processor(dummy\\_dataset[0][\""audio\""][\""array\""], return\\_tensors=\""pt\"", sampling\\_rate=16\\_000).input\\_values\ninput\\_values = input\\_values.to(\""cuda\"")\nlogits = model(input\\_values).logits\nassert logits.shape[-1] == 32\n```\n## How to finetune an acoustic model\nIn this section, we show you how to fine-tune a pre-trained [XLS-R Model](https://huggingface.co/docs/transformers/model\\_doc/xls\\_r) on the [Common Voice 7 dataset](https://huggingface.co/datasets/mozilla-foundation/common\\_voice\\_7\\_0)."", ""Considerations for model evaluation\nDeveloping an ML model is rarely a one-shot deal: it often involves multiple stages of defining the model architecture and tuning hyper-parameters before converging on a final set. Responsible model evaluation is a key part of this process, and ðŸ¤— Evaluate is here to help! Here are some things to keep in mind when evaluating your model using the ðŸ¤— Evaluate library:\n## Properly splitting your data\nGood evaluation generally requires three splits of your dataset:\n- \\*\\*train\\*\\*: this is used for training your model. - \\*\\*validation\\*\\*: this is used for validating the model hyperparameters. - \\*\\*test\\*\\*: this is used for evaluating your model. Many of the datasets on the ðŸ¤— Hub are separated into 2 splits: `train` and `validation`; others are split into 3 splits (`train`, `validation` and `test`) -- make sure to use the right split for the right purpose! Some datasets on the ðŸ¤— Hub are already separated into these three splits. However, there are also many that only have a train/validation or only train split. If the dataset you're using doesn't have a predefined train-test split, it is up to you to define which part of the dataset you want to use for training your model and which you want to use for hyperparameter tuning or final evaluation. Training and evaluating on the same split can misrepresent your results! If you overfit on your training data the evaluation results on that split will look great but the model will perform poorly on new data. Depending on the size of the dataset, you can keep anywhere from 10-30% for evaluation and the rest for training, while aiming to set up the test set to reflect the production data as close as possible. Check out [this thread](https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090) for a more in-depth discussion of dataset splitting! ## The impact of class imbalance\nWhile many academic datasets, such as the [IMDb dataset](https://huggingface.co/datasets/imdb) of movie reviews, are perfectly balanced, most real-world datasets are not. In machine learning a \\*balanced dataset\\* corresponds to a datasets where all labels are represented equally. In the case of the IMDb dataset this means that there are as many positive as negative reviews in the dataset. In an imbalanced dataset this is not the case: in fraud detection for example there are usually many more non-fraud cases than fraud cases in the dataset. Having an imbalanced dataset can skew the results of your metrics. Imagine a dataset with 99 \""non-fraud\"" cases and 1 \""fraud\"" case. A simple model that always predicts \""non-fraud\"" cases would give yield a 99% accuracy which might sound good at first until you realize that you will never catch a fraud case.""]","The three main steps for fine-tuning a model with the ðŸ¤— Datasets library are:

1. Load a dataset from the Hugging Face Hub.
2. Preprocess the data with `Dataset.map()`.
3. Load and compute metrics.",1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.
"What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?
","[""It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark. Figure 4. Latency (Batch=1, Physical Cores=2)\n\n---\n## Conclusion\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput. Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens. The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial)\n## Resources\n\\* [Hugging Face Infinity](https://huggingface.co/infinity)\n\\* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\n\\* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i)\n\\* [DistilBERT](https://huggingface.co/docs/transformers/model\\_doc/distilbert)\n\\* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\\* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\n\\* [ðŸ¤— Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)"", ""--\ntitle: \""Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\""\nthumbnail: /blog/assets/80\\_intel/01.png\nauthors:\n- user: juliensimon\n---\n# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\n![image](assets/80\\_intel/01.png)\nThe mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption. Today, we're excited to announce that Intel has officially joined our [Hardware Partner Program](https://huggingface.co/hardware). Thanks to the [Optimum](https://github.com/huggingface/optimum-intel) open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers. Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage. Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms. â€œ\\*Weâ€™re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.\\*â€, says Wei Li, Intel Vice President & General Manager, AI and Analytics. In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs. The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Letâ€™s take a deeper look at it. ## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices."", ""This trend has introduced daunting challenges for Machine Learning Engineers when deploying the latest models into production. While 100x speedup is a high bar to reach, thatâ€™s what it takes to serve predictions with acceptable latency in real-time consumer applications. To reach that bar, as Machine Learning Engineers at Hugging Face we certainly have an unfair advantage sitting in the same (virtual) offices as the ðŸ¤— Transformers and ðŸ¤— Tokenizers maintainers ðŸ˜¬. We are also extremely lucky for the rich partnerships we have developed through open source collaborations with hardware and cloud vendors like Intel, NVIDIA, Qualcomm, Amazon and Microsoft that enable us to tune our models x infrastructure with the latest hardware optimizations techniques. If you want to feel the speed on our infrastructure, start a [free trial](https://huggingface.co/pricing) and weâ€™ll get in touch. If you want to benefit from our experience optimizing inference on your own infrastructure participate in our [ðŸ¤— Expert Acceleration Program](https://huggingface.co/support).""]",Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.,+800%
"What is the command to upload a spaCy pipeline to the Hugging Face Hub?
","[""Defaults to `hub` in the current working directory. |\n| `--verbose`, `-V` | bool | Output additional info for debugging, e.g. the full generated hub metadata. |\nYou can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`. ```bash\nhuggingface-cli login\npython -m spacy package ./en\\_ner\\_fashion ./output --build wheel\ncd ./output/en\\_ner\\_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en\\_ner\\_fashion-0.0.0-py3-none-any.whl\n```\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card. The command will output two things:\n\\* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en\\_core\\_web\\_sm\n\\* And how to install the pipeline directly from the Hub! ### From a Python script\nYou can use the `push` function from Python. It returns a dictionary containing the `\""url\""` and \""`whl\\_url`\"" of the published model and the wheel file, which you can later install with `pip install`. ```py\nfrom spacy\\_huggingface\\_hub import push\nresult = push(\""./en\\_ner\\_fashion-0.0.0-py3-none-any.whl\"")\nprint(result[\""url\""])\n```\n## Additional resources\n\\* spacy-huggingface-hub [library](https://github.com/explosion/spacy-huggingface-hub)."", ""--\ntitle: \""Welcome spaCy to the Hugging Face Hub\""\nthumbnail: /blog/assets/23\\_spacy/thumbnail.png\nauthors:\n- user: osanseviero\n- user: ines\n---\n# Welcome spaCy to the Hugging Face Hub\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text. Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production! ### Finding models\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy. ### Widgets\nThis integration includes support for NER widgets, so all models with a NER component will have this out of the box! Coming soon there will be support for text classification and POS. [`spacy/en_core_web_sm`](/spacy/en_core_web_sm)\n\nHosted inference API\n\nToken Classification\n\nCompute\n\nThis model is currently loaded and running on the Inference API. JSON Output \nMaximize\n\n### Using existing models\nAll models from the Hub can be directly installed using `pip install`. ```bash\npip install https://huggingface.co/spacy/en\\_core\\_web\\_sm/resolve/main/en\\_core\\_web\\_sm-any-py3-none-any.whl\n```\n```python\n# Using spacy.load(). import spacy\nnlp = spacy.load(\""en\\_core\\_web\\_sm\"")\n# Importing as module. import en\\_core\\_web\\_sm\nnlp = en\\_core\\_web\\_sm.load()\n```\nWhen you open a repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model!"", ""\\* Launch [blog post](https://huggingface.co/blog/spacy)\n\\* spaCy v 3.1 [Announcement](https://explosion.ai/blog/spacy-v3-1#huggingface-hub)\n\\* spaCy [documentation](https://spacy.io/universe/project/spacy-huggingface-hub/)""]","The command to upload a spaCy pipeline to the Hugging Face Hub is as follows:

```bash
huggingface-cli login
python -m spacy package ./en_ner_fashion ./output --build wheel
cd ./output/en_ner_fashion-0.0.0/dist
python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
```

This sequence of commands logs you in, packages your spaCy model, and then pushes it to the Hugging Face Hub.",python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
"What is the time and memory complexity of the NystrÃ¶mformer's approximation of self-attention?
","[""--\ntitle: \""NystrÃ¶mformer: Approximating self-attention in linear time and memory via the NystrÃ¶m method\""\nthumbnail: /blog/assets/86\\_nystromformer/thumbnail.png\nauthors:\n- user: asi\nguest: true\n---\n# NystrÃ¶mformer: Approximating self-attention in linear time and memory via the NystrÃ¶m method\n## Introduction\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. The [NystrÃ¶mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. NystrÃ¶mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the NystrÃ¶m method and how it can be adapted to approximate self-attention. ## NystrÃ¶m method for matrix approximation\nAt the heart of NystrÃ¶mformer is the NystrÃ¶m method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the NystrÃ¶m method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:\n\nRepresenting P as a block matrix\n\nWe now have four submatrices: \\\\(A\\_P, B\\_P, F\\_P,\\\\) and \\\\(C\\_P\\\\), with sizes \\\\(m \\times m, m \\times (n - m), (n - m) \\times m\\\\) and\n\\\\((n - m) \\times (n - m)\\\\) respectively. The \\\\(m\\\\) sampled columns are contained in \\\\(A\\_P\\\\) and \\\\(F\\_P\\\\), whereas the \\\\(m\\\\) sampled rows are contained in \\\\(A\\_P\\\\) and \\\\(B\\_P\\\\). So, the entries of \\\\(A\\_P, B\\_P,\\\\) and \\\\(F\\_P\\\\) are known to us, and we will estimate \\\\(C\\_P\\\\). According to the NystrÃ¶m method, \\\\(C\\_P\\\\) is given by:\n$$C\\_P = F\\_P A\\_P^+ B\\_P$$\nHere, \\\\(+\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). Thus, the NystrÃ¶m approximation of \\\\(P, \\hat{P}\\\\) can be written as:\n\nNystrÃ¶m approximation of P\n\nAs shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices."", ""The abstract from the paper is the following:\n\\*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\ncommunity. To address this limitation, we propose NystrÃ¶mformer -- a model that exhibits favorable scalability as a\nfunction of sequence length. Our idea is based on adapting the NystrÃ¶m method to approximate standard self-attention\nwith O(n) complexity. The scalability of NystrÃ¶mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\nsequence length, and find that our NystrÃ¶mformer performs comparably, or in a few cases, even slightly better, than\nstandard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, NystrÃ¶mformer performs\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.\\*\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/Nystromformer). ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## NystromformerConfig\n[[autodoc]] NystromformerConfig\n## NystromformerModel\n[[autodoc]] NystromformerModel\n- forward\n## NystromformerForMaskedLM\n[[autodoc]] NystromformerForMaskedLM\n- forward\n## NystromformerForSequenceClassification\n[[autodoc]] NystromformerForSequenceClassification\n- forward\n## NystromformerForMultipleChoice\n[[autodoc]] NystromformerForMultipleChoice\n- forward\n## NystromformerForTokenClassification\n[[autodoc]] NystromformerForTokenClassification\n- forward\n## NystromformerForQuestionAnswering\n[[autodoc]] NystromformerForQuestionAnswering\n- forward"", ""The reason for doing so will become clear later. ## Can we approximate self-attention with the NystrÃ¶m method? Our goal is to ultimately approximate the softmax matrix in standard self attention: S = softmax \\\\( \\frac{QK^T}{\\sqrt{d}} \\\\)\nHere, \\\\(Q\\\\) and \\\\(K\\\\) denote the queries and keys respectively. Following the procedure discussed above, we would sample \\\\(m\\\\) rows and columns from \\\\(S\\\\), form four submatrices, and obtain \\\\(\\hat{S}\\\\):\n\nNystrÃ¶m approximation of S\n\nBut, what does it mean to sample a column from \\\\(S\\\\)? It means we select one element from each row. Recall how S is calculated: the final operation is a row-wise softmax. To find a single entry in a row, we must access all other entries (for the denominator in softmax). So, sampling one column requires us to know all other columns in the matrix. Therefore, we cannot directly apply the NystrÃ¶m method to approximate the softmax matrix. ## How can we adapt the NystrÃ¶m method to approximate self-attention? Instead of sampling from \\\\(S\\\\), the authors propose to sample landmarks (or NystrÃ¶m points) from queries and keys. We denote the query landmarks and key landmarks as \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) respectively. \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) can be used to construct three matrices corresponding to those in the NystrÃ¶m approximation of \\\\(S\\\\). We define the following matrices:\n$$\\tilde{F} = softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) \\hspace{40pt} \\tilde{A} = softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+ \\hspace{40pt} \\tilde{B} = softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}})$$\nThe sizes of \\\\(\\tilde{F}\\\\), \\\\(\\tilde{A}\\\\), and \\\\(\\tilde{B}) are \\\\(n \\times m, m \\times m,\\\\) and \\\\(m \\times n\\\\) respectively. We replace the three matrices in the NystrÃ¶m approximation of \\\\(S\\\\) with the new matrices we have defined to obtain an alternative NystrÃ¶m approximation:\n$$\\begin{aligned}\\hat{S} &= \\tilde{F} \\tilde{A} \\tilde{B} \\\\ &= softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+ softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}}) \\end{aligned}$$\nThis is the NystrÃ¶m approximation of the softmax matrix in the self-attention mechanism.""]","The NystrÃ¶mformer approximates self-attention with a time and memory complexity of \(O(n)\), where \(n\) is the length of the input sequence. This is a significant improvement over the standard self-attention mechanism, which has a complexity of \(O(n^2)\).",O(n)
"What is the goal of the Named Entity Recognition task in token classification?
","[""p align=\""center\""> ![FSNER LOGO](http://sayef.tech:8082/uploads/FSNER-LOGO-2.png) \n\nImplemented by  [sayef](https://huggingface.co/sayef) . ## Overview\nThe FSNER model was proposed in [Example-Based Named Entity Recognition](https://arxiv.org/abs/2008.10570) by Morteza Ziyadi, Yuting Sun, Abhishek Goswami, Jade Huang, Weizhu Chen. To identify entity spans in a new domain, it uses a train-free few-shot learning approach inspired by question-answering. ## Abstract\n----\n> We present a novel approach to named entity recognition (NER) in the presence of scarce data that we call example-based NER. Our train-free few-shot learning approach takes inspiration from question-answering to identify entity spans in a new and unseen domain. In comparison with the current state-of-the-art, the proposed method performs significantly better, especially when using a low number of support examples. ## Model Training Details\n-----\n| identifier | epochs | datasets |\n| ---------- |:----------:| :-----:|\n| [sayef/fsner-bert-base-uncased](https://huggingface.co/sayef/fsner-bert-base-uncased) | 10 | ontonotes5, conll2003, wnut2017, and fin (Alvarado et al.). |\n## Installation and Example Usage\n------\nYou can use the FSNER model in 3 ways:\n1. Install directly from PyPI: `pip install fsner` and import the model as shown in the code example below\nor\n2. Install from source: `python setup.py install` and import the model as shown in the code example below\nor\n3. Clone repo and change directory to `src` and import the model as shown in the code example below\n```python\nfrom fsner import FSNERModel, FSNERTokenizerUtils\nmodel = FSNERModel(\""sayef/fsner-bert-base-uncased\"")\ntokenizer = FSNERTokenizerUtils(\""sayef/fsner-bert-base-uncased\"")\n# size of query and supports must be the same."", ""ote: the following transcripts are associated with Merve Noyan's videos in the Hugging Face Tasks playlist: https://www.youtube.com/playlist?list=PLo2EIpI\\_JMQtyEr-sLJSy5\\_SnLCb4vtQf\nToken Classification video\nWelcome to the Hugging Face tasks series! In this video weâ€™ll take a look at the token classification task. Token classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging. Letâ€™s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity. Another token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech. Token classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score."", ""Named-Entity Recognition\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert\\_ner\\_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial\\_Analyst\\_AI\nTags: NER, TEXT, HIGHLIGHT\n## Introduction\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \""token\"") into different categories, such as names of people or names of locations, or different parts of speech. For example, given the sentence:\n> Does Chicago have any Pakistani restaurants? A named-entity recognition algorithm may identify:\n- \""Chicago\"" as a \\*\\*location\\*\\*\n- \""Pakistani\"" as an \\*\\*ethnicity\\*\\*\nand so on. Using `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team. Here is an example of a demo that you'll be able to build:\n$demo\\_ner\\_pipeline\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!""]","The goal of the Named Entity Recognition (NER) task in token classification is to find and classify entities in a piece of text into predefined categories, such as names of people, locations, organizations, or other specific types of information. This task involves labeling each token in a sentence with a class that corresponds to an entity or indicating that the token does not belong to any entity. For example, in the sentence ""Does Chicago have any Pakistani restaurants?"", ""Chicago"" would be identified as a **location** and ""Pakistani"" as an **ethnicity**.","The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization."
"What is the resolution of images used by the CLIPSeg model?
","[""For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm\\_source=hf&utm\\_medium=blog&utm\\_campaign=clipseg). Before we describe how to do that, letâ€™s first take a look at how CLIPSeg works. ## CLIP: the magic model behind CLIPSeg\n[CLIP](https://huggingface.co/docs/transformers/main/en/model\\_doc/clip), which stands for \\*\\*C\\*\\*ontrastive \\*\\*L\\*\\*anguageâ€“\\*\\*I\\*\\*mage \\*\\*P\\*\\*re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract \\*representation\\* of your input. This abstract representation, also called an \\*embedding\\*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together)."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# CLIPSeg\n## Overview\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo LÃ¼ddecke\nand Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero- and one-shot image segmentation."", ""It allows a computer to not only know what is in an image (classification), where objects are in the image (detection), but also what the outlines of those objects are. Knowing the outlines of objects is essential in fields such as robotics and autonomous driving. For example, a robot has to know the shape of an object to grab it correctly. Segmentation can also be combined with [image inpainting](https://t.co/5q8YHSOfx7) to allow users to describe which part of the image they want to replace. One limitation of most image segmentation models is that they only work with a fixed list of categories. For example, you cannot simply use a segmentation model trained on oranges to segment apples. To teach the segmentation model an additional category, you have to label data of the new category and train a new model, which can be costly and time-consuming. But what if there was a model that can already segment almost any kind of object, without any further training? Thatâ€™s exactly what [CLIPSeg](https://arxiv.org/abs/2112.10003), a zero-shot segmentation model, achieves. Currently, CLIPSeg still has its limitations.""]","The CLIPSeg model uses images of 352 x 352 pixels, which results in relatively low-resolution outputs.",352 x 352 pixels
"What can you use Gradio for?
","[""Key Features\nLet's go through some of the key features of Gradio. This guide is intended to be a high-level overview of various things that you should be aware of as you build your demo. Where appropriate, we link to more detailed guides on specific topics. 1. [Components](#components)\n2. [Queuing](#queuing)\n3. [Streaming outputs](#streaming-outputs)\n4. [Streaming inputs](#streaming-inputs)\n5. [Alert modals](#alert-modals)\n6. [Styling](#styling)\n7. [Progress bars](#progress-bars)\n8. [Batch functions](#batch-functions)\n## Components\nGradio includes more than 30 pre-built components (as well as many user-built \\_custom components\\_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on. Each component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[\""webcam\"", \""upload\""]`). \\*\\*Static and Interactive Components\\*\\*\nEvery component has a \\_static\\_ version that is designed to \\*display\\* data, and most components also have an \\_interactive\\_ version designed to let users input or modify the data. Typically, you don't need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports. \\*\\*Preprocessing and Postprocessing\\*\\*\nWhen a component is used as an input, Gradio automatically handles the \\_preprocessing\\_ needed to convert the data from a type sent by the user's browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array). Similarly, when a component is used as an output, Gradio automatically handles the \\_postprocessing\\_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (a gallery of images). Consider an example demo with three input components (`gr.Textbox`, `gr.Number`, and `gr.Image`) and two outputs (`gr.Number` and `gr.Gallery`) that serve as a UI for your image-to-image generation model. Below is a diagram of what our preprocessing will send to the model and what our postprocessing will require from it. ![](https://github.com/gradio-app/gradio/blob/main/guides/assets/dataflow.svg?raw=true)\nIn this image, the following preprocessing steps happen to send the data from the browser to your function:\n\\* The text in the textbox is converted to a Python `str` (essentially no preprocessing)\n\\* The number in the number input in converted to a Python `float` (essentially no preprocessing)\n\\* Most importantly, ihe image supplied by the user is converted to a `numpy.array` representation of the RGB values in the image\nImages are converted to NumPy arrays because they are a common format for machine learning workflows. You can control the \\_preprocessing\\_ using the component's parameters when constructing the component. For example, if you instantiate the `Image` component with the following parameters, it will preprocess the image to the `PIL` format instead:\n```py\nimg = gr.Image(type=\""pil\"")\n```\nPostprocessing is even simpler! Gradio automatically recognizes the format of the returned data (e.g. does the user's function return a `numpy` array or a `str` filepath for the `gr.Image` component?) and postprocesses it appropriately into a format that can be displayed by the browser. So in the image above, the following postprocessing steps happen to send the data returned from a user's function to the browser:\n\\* The `float` is displayed as a number and displayed directly to the user\n\\* The list of string filepaths (`list[str]`) is interpreted as a list of image filepaths and displayed as a gallery in the browser\nTake a look at the [Docs](https://gradio.app/docs) to see all the parameters for each Gradio component."", ""### An Overview of Gradio\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do? #### Chatbots with `gr.ChatInterface`\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast). #### Custom Demos with `gr.Blocks`\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction â€” still all in Python. You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners). #### The Gradio Python & JavaScript Ecosystem\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\\* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio\\_client`): query any Gradio app programmatically in Python. \\* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript. \\* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \\* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications â€” for free!"", ""Gradio, check![[gradio-check]]\nThis wraps up the chapter on building cool ML demos with Gradio - we hope you enjoyed it! To recap, in this chapter we learned:\n- How to create Gradio demos with the high-level `Interface` API, and how to configure different input and output modalities. - Different ways to share Gradio demos, through temporary links and hosting on [Hugging Face Spaces](https://huggingface.co/spaces). - How to integrate Gradio demos with models and Spaces on the Hugging Face Hub. - Advanced features like storing state in a demo or providing authentication. - How to have full control of the data flow and layout of your demo with Gradio Blocks. If you'd like to test your understanding of the concepts covered in this chapter, check out the quiz in the next section! ## Where to next?[[where-to-next]]\nIf you want to learn more about Gradio you can\n- Take a look at [Demos](https://github.com/gradio-app/gradio/tree/main/demo) in the repo, there are quite a lot of examples there. - See the [Guides](https://gradio.app/guides/) page, where you can find guides about cool and advanced features.""]","Gradio can be used for a variety of purposes, primarily focused on building interactive machine learning demos and applications. Here are some key uses:

1. **Creating Demos**: You can quickly create demos for machine learning models using the `Interface` class, allowing users to interact with your models through a web interface.

2. **Chatbots**: With the `gr.ChatInterface`, you can build chatbot UIs that allow users to interact with conversational models easily.

3. **Custom Applications**: Using the `gr.Blocks` class, you can design more complex web applications with flexible layouts and data flows, enabling you to create custom and intricate applications.

4. **Integration with Models**: Gradio allows you to integrate your demos with models hosted on the Hugging Face Hub, making it easy to showcase your work.

5. **Sharing and Hosting**: You can share your Gradio demos through temporary links or host them on Hugging Face Spaces for free.

6. **Client Libraries**: Gradio provides Python and JavaScript client libraries that allow you to query Gradio apps programmatically.

Overall, Gradio is a powerful tool for anyone looking to create interactive interfaces for machine learning applications, making it accessible for users to test and explore models.","Create a demo for your machine learning model, share your machine learning model with others, and debug your model."
"What TensorFlow API function is used to load a saved tensor file?
","[""Tensorflow API\n[[autodoc]] safetensors.tensorflow.load\\_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save\\_file\n[[autodoc]] safetensors.tensorflow.save"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Load safetensors\n[[open-in-colab]]\n[safetensors](https://github.com/huggingface/safetensors) is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or \\*pickled\\* into a `.bin` file with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights. This guide will show you how you load `.safetensor` files, and how to convert Stable Diffusion model weights stored in other formats to `.safetensor`. Before you start, make sure you have safetensors installed:\n```py\n# uncomment to install the necessary libraries in Colab\n#!pip install safetensors\n```\nIf you look at the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main) repository, you'll see weights inside the `text\\_encoder`, `unet` and `vae` subfolders are stored in the `.safetensors` format. By default, ðŸ¤— Diffusers automatically loads these `.safetensors` files from their subfolders if they're available in the model repository. For more explicit control, you can optionally set `use\\_safetensors=True` (if `safetensors` is not installed, you'll get an error message asking you to install it):\n```py\nfrom diffusers import DiffusionPipeline\npipeline = DiffusionPipeline.from\\_pretrained(\""runwayml/stable-diffusion-v1-5\"", use\\_safetensors=True)\n```\nHowever, model weights are not necessarily stored in separate subfolders like in the example above. Sometimes, all the weights are stored in a single `.safetensors` file. In this case, if the weights are Stable Diffusion weights, you can load the file directly with the [`~diffusers.loaders.FromSingleFileMixin.from\\_single\\_file`] method:\n```py\nfrom diffusers import StableDiffusionPipeline\npipeline = StableDiffusionPipeline.from\\_single\\_file(\n\""https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\""\n)\n```\n## Convert to safetensors\nNot all weights on the Hub are available in the `.safetensors` format, and you may encounter weights stored as `.bin`."", ""!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-logo-light.svg)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-logo-dark.svg)\n\n# Safetensors\nSafetensors is a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy). Safetensors is really [fast ðŸš€](./speed). ## Installation\nwith pip:\n```\npip install safetensors\n```\nwith conda:\n```\nconda install -c huggingface safetensors\n```\n## Usage\n### Load tensors\n```python\nfrom safetensors import safe\\_open\ntensors = {}\nwith safe\\_open(\""model.safetensors\"", framework=\""pt\"", device=0) as f:\nfor k in f.keys():\ntensors[k] = f.get\\_tensor(k)\n```\nLoading only part of the tensors (interesting when running on multiple GPU)\n```python\nfrom safetensors import safe\\_open\ntensors = {}\nwith safe\\_open(\""model.safetensors\"", framework=\""pt\"", device=0) as f:\ntensor\\_slice = f.get\\_slice(\""embedding\"")\nvocab\\_size, hidden\\_dim = tensor\\_slice.get\\_shape()\ntensor = tensor\\_slice[:, :hidden\\_dim]\n```\n### Save tensors\n```python\nimport torch\nfrom safetensors.torch import save\\_file\ntensors = {\n\""embedding\"": torch.zeros((2, 2)),\n\""attention\"": torch.zeros((2, 3))\n}\nsave\\_file(tensors, \""model.safetensors\"")\n```\n## Format\nLet's say you have safetensors file named `model.safetensors`, then `model.safetensors` will have the following internal format:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-format.svg)\n\n## Featured Projects\nSafetensors is being used widely at leading AI enterprises, such as [Hugging Face](https://huggingface.co/), [EleutherAI](https://www.eleuther.ai/), and [StabilityAI](https://stability.ai/). Here is a non-exhaustive list of projects that are using safetensors:\n\\* [huggingface/transformers](https://github.com/huggingface/transformers)\n\\* [AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)\n\\* [Llama-cpp](https://github.com/ggerganov/llama.cpp/blob/e6a46b0ed1884c77267dc70693183e3b7164e0e0/convert.py#L537)\n\\* [microsoft/TaskMatrix](https://github.com/microsoft/TaskMatrix)\n\\* [hpcaitech/ColossalAI](https://github.com/hpcaitech/ColossalAI)\n\\* [huggingface/pytorch-image-models](https://github.com/huggingface/pytorch-image-models)\n\\* [CivitAI](https://civitai.com/)\n\\* [huggingface/diffusers](https://github.com/huggingface/diffusers)\n\\* [coreylowman/dfdx](https://github.com/coreylowman/dfdx)\n\\* [invoke-ai/InvokeAI](https://github.com/invoke-ai/InvokeAI)\n\\* [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)\n\\* [Sanster/lama-cleaner](https://github.com/Sanster/lama-cleaner)\n\\* [PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n\\* [AIGC-Audio/AudioGPT](https://github.com/AIGC-Audio/AudioGPT)\n\\* [brycedrennan/imaginAIry](https://github.com/brycedrennan/imaginAIry)\n\\* [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)\n\\* [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE)\n\\* [alvarobartt/safejax](https://github.com/alvarobartt/safejax)\n\\* [MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)\n\\* [LaurentMazare/tch-rs](https://github.com/LaurentMazare/tch-rs)\n\\* [chainyo/tensorshare](https://github.com/chainyo/tensorshare)""]",The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`.,safetensors.tensorflow.load_file
"Where can you access the logs of your Endpoints in Hugging Face Endpoints?
","[""Access and read Logs\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the â€œLogsâ€ tab of your Endpoint. You will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference. ![select logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png)\nThe Container Logs are only available when your Endpoint is in the â€œRunningâ€ state. \\_Note: If your Endpoint creation is in the â€œFailedâ€ state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc.\\_\n\\*\\*Build Logs:\\*\\*\n![build logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png)\n\\*\\*Container Logs:\\*\\*\n![container logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png)"", ""Access and view Metrics\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the â€œAnalyticsâ€ tab of your Endpoints. ![metric dashboard](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png)\n## Access Metrics via API\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api\\_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints."", ""Once I'm done testing, I delete the endpoints that I've created to avoid unwanted charges. I also delete the VPC Endpoint in the AWS console. Hugging Face customers are already using Inference Endpoints. For example, [Phamily](https://phamily.com/), the #1 in-house chronic care management & proactive care platform, [told us](https://www.youtube.com/watch?v=20C9X5OYO2Q) that Inference Endpoints is helping them simplify and accelerate HIPAA-compliant Transformer deployments. ### Now it's your turn! Thanks to Inference Endpoints, you can deploy production-grade, scalable, secure endpoints in minutes, in just a few clicks. Why don't you [give it a try](https://ui.endpoints.huggingface.co/new)? We have plenty of ideas to make the service even better, and we'd love to hear your feedback in the [Hugging Face forum](https://discuss.huggingface.co/). Thank you for reading and have fun with Inference Endpoints!""]","You can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the â€œLogsâ€ tab of your Endpoint. You will have access to the build logs of your Image artifacts as well as the Container Logs during inference. The Container Logs are only available when your Endpoint is in the â€œRunningâ€ state. If your Endpoint creation is in the â€œFailedâ€ state, you can check the Build Logs to see the reason for the failure.","In the ""Logs"" tab of your Endpoint through the UI."
"What is the latest task added to Hugging Face AutoTrain for Computer Vision?
","[""--\ntitle: Image Classification with AutoTrain\nthumbnail: /blog/assets/105\\_autotrain-image-classification/thumbnail.png\nauthors:\n- user: nimaboscarino\n---\n# Image Classification with AutoTrain\nSo youâ€™ve heard all about the cool things that are happening in the machine learning world, and you want to join in. Thereâ€™s just one problem â€“ you donâ€™t know how to code! ðŸ˜± Or maybe youâ€™re a seasoned software engineer who wants to add some ML to your side-project, but you donâ€™t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. Thatâ€™s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature weâ€™ve just added, weâ€™re making â€œno-codeâ€ machine learning better than ever. Best of all, you can create your first project for âœ¨ free! âœ¨\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with \\*\\*zero\\*\\* configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer ðŸ¤¯ Weâ€™ve been expanding the number of tasks that we support, and weâ€™re proud to announce that \\*\\*you can now use AutoTrain for Computer Vision\\*\\*!"", ""For Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage. AutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate ðŸ¤— Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details. ## The technical philosophy\nIn this section, we wanted to share our philosophy behind adding support for Computer Vision in ðŸ¤— Transformers so that the community is aware of the design choices specific to this area. Even though Transformers started with NLP, we support multiple modalities today, for example â€“ vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:\n- Easy model download with a single line of code with `from\\_pretrained()`\n- Easy model upload with `push\\_to\\_hub()`\n- Support for loading huge checkpoints with efficient checkpoint sharding techniques\n- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))\n- Initialization from model configurations\n- Support for both PyTorch and TensorFlow (non-exhaustive)\n- and many more\nUnlike tokenizers, we have preprocessors (such as [this](https://huggingface.co/docs/transformers/model\\_doc/vit#transformers.ViTImageProcessor)) that take care of preparing data for the vision models. We have worked hard to ensure the user experience of using a vision model still feels easy and similar:\n```py\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nimport torch\nfrom datasets import load\\_dataset\ndataset = load\\_dataset(\""huggingface/cats-image\"")\nimage = dataset[\""test\""][\""image\""][0]\nimage\\_processor  = ViTImageProcessor.from\\_pretrained(\""google/vit-base-patch16-224\"")\nmodel = ViTForImageClassification.from\\_pretrained(\""google/vit-base-patch16-224\"")\ninputs = image\\_processor(image, return\\_tensors=\""pt\"")\nwith torch.no\\_grad():\nlogits = model(\\*\\*inputs).logits\n# model predicts one of the 1000 ImageNet classes\npredicted\\_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted\\_label])\n# Egyptian cat\n```\nEven for a difficult task like object detection, the user experience doesnâ€™t change very much:\n```py\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\nfrom PIL import Image\nimport requests\nurl = \""http://images.cocodataset.org/val2017/000000039769.jpg\""\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\\_processor = AutoImageProcessor.from\\_pretrained(\""microsoft/conditional-detr-resnet-50\"")\nmodel = AutoModelForObjectDetection.from\\_pretrained(\""microsoft/conditional-detr-resnet-50\"")\ninputs = image\\_processor(images=image, return\\_tensors=\""pt\"")\noutputs = model(\\*\\*inputs)\n# convert outputs (bounding boxes and class logits) to COCO API\ntarget\\_sizes = torch.tensor([image.size[::-1]])\nresults = image\\_processor.post\\_process\\_object\\_detection(\noutputs, threshold=0.5, target\\_sizes=target\\_sizes\n)[0]\nfor score, label, box in zip(results[\""scores\""], results[\""labels\""], results[\""boxes\""]):\nbox = [round(i, 2) for i in box.tolist()]\nprint(\nf\""Detected {model.config.id2label[label.item()]} with confidence \""\nf\""{round(score.item(), 3)} at location {box}\""\n)\n```\nLeads to:\n```bash\nDetected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\nDetected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\nDetected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\nDetected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\nDetected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\n```\n## Zero-shot models for vision\nThereâ€™s been a surge of models that reformulate core vision tasks like segmentation and detection in interesting ways and introduce even more flexibility."", ""Tasks\n## What's a task? Tasks, or pipeline types, describe the \""shape\"" of each model's API (inputs and outputs) and are used to determine which Inference API and widget we want to display for any given model. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/tasks.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/tasks-dark.png)\n\nThis classification is relatively coarse-grained (you can always add more fine-grained task names in your model tags), so \\*\\*you should rarely have to create a new task\\*\\*. If you want to add support for a new task, this document explains the required steps. ## Overview\nHaving a new task integrated into the Hub means that:\n\\* Users can search for all models â€“ and datasets â€“ of a given task. \\* The Inference API supports the task. \\* Users can try out models directly with the widget. ðŸ†\nNote that you don't need to implement all the steps by yourself. Adding a new task is a community effort, and multiple people can contribute. ðŸ§‘â€ðŸ¤â€ðŸ§‘\nTo begin the process, open a new issue in the [huggingface\\_hub](https://github.com/huggingface/huggingface\\_hub/issues) repository.""]",The latest task added to Hugging Face AutoTrain for Computer Vision is **image classification**.,Image Classification
"What is the default repository type created by the `create_repo` function on Hugging Face Hub?
","[""It's a model repo by default. Parameters:\n- `type`: Type of repo (dataset or space; model by default). - `name`: Name of repo. - `organization`: Name of organization (optional). Payload:\n```js\npayload = {\n\""type\"": \""model\"",\n\""name\"": \""name\"",\n\""organization\"": \""organization\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.delete\\_repo()`. ### PUT /api/repos/{repo\\_type}/{repo\\_id}/settings\nUpdate repo visibility. Payload:\n```js\npayload = {\n\""private\"": \""private\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.update\\_repo\\_visibility()`. ### POST /api/repos/move\nMove a repository (rename within the same namespace or transfer from user to organization). Parameters:\n- `fromRepo`: repo to rename. - `toRepo`: new name of the repo. - `type`: Type of repo (dataset or space; model by default). Payload:\n```js\npayload = {\n\""fromRepo\"" : \""namespace/repo\\_name\"",\n\""toRepo\"" : \""namespace2/repo\\_name2\"",\n\""type\"": \""model\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.move\\_repo()`. ## User API\nThe following endpoint gets information about a user. ### GET /api/whoami-v2\nGet username and organizations the user belongs to. Payload:\n```js\nheaders = { \""authorization\"" : \""Bearer $token\"" }\n```\nThis is equivalent to `huggingface\\_hub.whoami()`."", ""Firstly, there are a few methods to manage repository creation, deletion, and others:\n```python no-format\nfrom huggingface\\_hub import (\n# User management\nlogin,\nlogout,\nwhoami,\n# Repository creation and management\ncreate\\_repo,\ndelete\\_repo,\nupdate\\_repo\\_visibility,\n# And some methods to retrieve/change information about the content\nlist\\_models,\nlist\\_datasets,\nlist\\_metrics,\nlist\\_repo\\_files,\nupload\\_file,\ndelete\\_file,\n)\n```\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them. The `create\\_repo` method can be used to create a new repository on the hub:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"")\n```\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"", organization=\""huggingface\"")\n```\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization."", ""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# åˆ›å»ºå’Œç®¡ç†å­˜å‚¨åº“\nHugging Face Hubæ˜¯ä¸€ç»„ Git å­˜å‚¨åº“ã€‚[Git](https://git-scm.com/)æ˜¯è½¯ä»¶å¼€å‘ä¸­å¹¿æ³›ä½¿ç”¨çš„å·¥å…·ï¼Œå¯ä»¥åœ¨åä½œå·¥ä½œæ—¶è½»æ¾å¯¹é¡¹ç›®è¿›è¡Œç‰ˆæœ¬æŽ§åˆ¶ã€‚æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä¸Ž Hub ä¸Šçš„å­˜å‚¨åº“è¿›è¡Œäº¤äº’ï¼Œç‰¹åˆ«å…³æ³¨ä»¥ä¸‹å†…å®¹ï¼š\n- åˆ›å»ºå’Œåˆ é™¤å­˜å‚¨åº“\n- ç®¡ç†åˆ†æ”¯å’Œæ ‡ç­¾\n- é‡å‘½åæ‚¨çš„å­˜å‚¨åº“\n- æ›´æ–°æ‚¨çš„å­˜å‚¨åº“å¯è§æ€§\n- ç®¡ç†å­˜å‚¨åº“çš„æœ¬åœ°å‰¯æœ¬\nå¦‚æžœæ‚¨ä¹ æƒ¯äºŽä½¿ç”¨ç±»ä¼¼äºŽGitLab/GitHub/Bitbucketç­‰å¹³å°ï¼Œæ‚¨å¯èƒ½é¦–å…ˆæƒ³åˆ°ä½¿ç”¨ `git`å‘½ä»¤è¡Œå·¥å…·æ¥å…‹éš†å­˜å‚¨åº“ï¼ˆ`git clone`ï¼‰ã€æäº¤æ›´æ”¹ï¼ˆ`git add` , ` git commit`ï¼‰å¹¶æŽ¨é€å®ƒä»¬ï¼ˆ`git push`ï¼‰ã€‚åœ¨ä½¿ç”¨ Hugging Face Hub æ—¶ï¼Œè¿™æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œè½¯ä»¶å·¥ç¨‹å’Œæœºå™¨å­¦ä¹ å¹¶ä¸å…·æœ‰ç›¸åŒçš„è¦æ±‚å’Œå·¥ä½œæµç¨‹ã€‚æ¨¡åž‹å­˜å‚¨åº“å¯èƒ½ä¼šç»´æŠ¤å¤§é‡æ¨¡åž‹æƒé‡æ–‡ä»¶ä»¥é€‚åº”ä¸åŒçš„æ¡†æž¶å’Œå·¥å…·ï¼Œå› æ­¤å…‹éš†å­˜å‚¨åº“ä¼šå¯¼è‡´æ‚¨ç»´æŠ¤å¤§é‡å ç”¨ç©ºé—´çš„æœ¬åœ°æ–‡ä»¶å¤¹ã€‚å› æ­¤ï¼Œä½¿ç”¨æˆ‘ä»¬çš„è‡ªå®šä¹‰HTTPæ–¹æ³•å¯èƒ½æ›´æœ‰æ•ˆã€‚æ‚¨å¯ä»¥é˜…è¯»æˆ‘ä»¬çš„[gitä¸ŽHTTPç›¸æ¯”è¾ƒ](../concepts/git\\_vs\\_http)è§£é‡Šé¡µé¢ä»¥èŽ·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯\nå¦‚æžœä½ æƒ³åœ¨Hubä¸Šåˆ›å»ºå’Œç®¡ç†ä¸€ä¸ªä»“åº“ï¼Œä½ çš„è®¡ç®—æœºå¿…é¡»å¤„äºŽç™»å½•çŠ¶æ€ã€‚å¦‚æžœå°šæœªç™»å½•ï¼Œè¯·å‚è€ƒ[æ­¤éƒ¨åˆ†](../quick-start#login)ã€‚åœ¨æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å‡è®¾ä½ çš„è®¡ç®—æœºå·²ç™»å½•\n## ä»“åº“åˆ›å»ºå’Œåˆ é™¤\nç¬¬ä¸€æ­¥æ˜¯äº†è§£å¦‚ä½•åˆ›å»ºå’Œåˆ é™¤ä»“åº“ã€‚ä½ åªèƒ½ç®¡ç†ä½ æ‹¥æœ‰çš„ä»“åº“ï¼ˆåœ¨ä½ çš„ç”¨æˆ·åå‘½åç©ºé—´ä¸‹ï¼‰æˆ–è€…ä½ å…·æœ‰å†™å…¥æƒé™çš„ç»„ç»‡ä¸­çš„ä»“åº“\n### åˆ›å»ºä¸€ä¸ªä»“åº“\nä½¿ç”¨ [`create\\_repo`] åˆ›å»ºä¸€ä¸ªç©ºä»“åº“ï¼Œå¹¶é€šè¿‡ `repo\\_id`å‚æ•°ä¸ºå…¶å‘½å `repo\\_id`æ˜¯ä½ çš„å‘½åç©ºé—´ï¼ŒåŽé¢è·Ÿç€ä»“åº“åç§°ï¼š`username\\_or\\_org/repo\\_name`\nè¿è¡Œä»¥ä¸‹ä»£ç ï¼Œä»¥åˆ›å»ºä»“åº“ï¼š\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-model\"")\n'https://huggingface.co/lysandre/test-model'\n```\né»˜è®¤æƒ…å†µä¸‹ï¼Œ[`create\\_repo`] ä¼šåˆ›å»ºä¸€ä¸ªæ¨¡åž‹ä»“åº“ã€‚ä½†æ˜¯ä½ å¯ä»¥ä½¿ç”¨ `repo\\_type`å‚æ•°æ¥æŒ‡å®šå…¶ä»–ä»“åº“ç±»åž‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœä½ æƒ³åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ä»“åº“\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-dataset\"", repo\\_type=\""dataset\"")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```\nåˆ›å»ºä»“åº“æ—¶ï¼Œä½ å¯ä»¥ä½¿ç”¨ `private`å‚æ•°è®¾ç½®ä»“åº“çš„å¯è§æ€§\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç \n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-private\"", private=True)\n```\nå¦‚æžœä½ æƒ³åœ¨ä»¥åŽæ›´æ”¹ä»“åº“çš„å¯è§æ€§ï¼Œä½ å¯ä»¥ä½¿ç”¨[`update\\_repo\\_visibility`] å‡½æ•°\n### åˆ é™¤ä¸€ä¸ªä»“åº“\nä½¿ç”¨ [`delete\\_repo`] åˆ é™¤ä¸€ä¸ªä»“åº“ã€‚ç¡®ä¿ä½ ç¡®å®žæƒ³è¦åˆ é™¤ä»“åº“ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªä¸å¯é€†è½¬çš„è¿‡ç¨‹ï¼åšå®Œä¸Šè¿°è¿‡ç¨‹åŽï¼ŒæŒ‡å®šä½ æƒ³è¦åˆ é™¤çš„ä»“åº“çš„ `repo\\_id`\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> delete\\_repo(repo\\_id=\""lysandre/my-corrupted-dataset\"", repo\\_type=\""dataset\"")\n```\n### å…‹éš†ä¸€ä¸ªä»“åº“ï¼ˆä»…é€‚ç”¨äºŽ Spacesï¼‰\nåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½æƒ³è¦å¤åˆ¶åˆ«äººçš„ä»“åº“å¹¶æ ¹æ®è‡ªå·±çš„ç”¨ä¾‹è¿›è¡Œè°ƒæ•´ã€‚å¯¹äºŽ Spacesï¼Œä½ å¯ä»¥ä½¿ç”¨ [`duplicate\\_space`] æ–¹æ³•æ¥å®žçŽ°ã€‚å®ƒå°†å¤åˆ¶æ•´ä¸ªä»“åº“ã€‚\nä½ ä»ç„¶éœ€è¦é…ç½®è‡ªå·±çš„è®¾ç½®ï¼ˆç¡¬ä»¶å’Œå¯†é’¥ï¼‰ã€‚æŸ¥çœ‹æˆ‘ä»¬çš„[ç®¡ç†ä½ çš„SpaceæŒ‡å—](./manage-spaces)ä»¥èŽ·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import duplicate\\_space\n>>> duplicate\\_space(\""multimodalart/dreambooth-training\"", private=False)\nRepoUrl('https://huggingface.co/spaces/nateraw/dreambooth-training',...)\n```\n## ä¸Šä¼ å’Œä¸‹è½½æ–‡ä»¶\næ—¢ç„¶æ‚¨å·²ç»åˆ›å»ºäº†æ‚¨çš„å­˜å‚¨åº“ï¼Œæ‚¨çŽ°åœ¨ä¹Ÿå¯ä»¥æŽ¨é€æ›´æ”¹è‡³å…¶ä¸­å¹¶ä»Žä¸­ä¸‹è½½æ–‡ä»¶\nè¿™ä¸¤ä¸ªä¸»é¢˜æœ‰å®ƒä»¬è‡ªå·±çš„æŒ‡å—ã€‚è¯·[ä¸Šä¼ æŒ‡å—](./upload) å’Œ[ä¸‹è½½æŒ‡å—](./download)æ¥å­¦ä¹ å¦‚ä½•ä½¿ç”¨æ‚¨çš„å­˜å‚¨åº“ã€‚\n## åˆ†æ”¯å’Œæ ‡ç­¾\nGitå­˜å‚¨åº“é€šå¸¸ä½¿ç”¨åˆ†æ”¯æ¥å­˜å‚¨åŒä¸€å­˜å‚¨åº“çš„ä¸åŒç‰ˆæœ¬ã€‚æ ‡ç­¾ä¹Ÿå¯ä»¥ç”¨äºŽæ ‡è®°å­˜å‚¨åº“çš„ç‰¹å®šçŠ¶æ€ï¼Œä¾‹å¦‚ï¼Œåœ¨å‘å¸ƒç‰ˆæœ¬è¿™ä¸ªæƒ…å†µä¸‹ã€‚æ›´ä¸€èˆ¬åœ°è¯´ï¼Œåˆ†æ”¯å’Œæ ‡ç­¾è¢«ç§°ä¸º[gitå¼•ç”¨](https://git-scm.com/book/en/v2/Git-Internals-Git-References). ### åˆ›å»ºåˆ†æ”¯å’Œæ ‡ç­¾\nä½ å¯ä»¥ä½¿ç”¨[`create\\_branch`]å’Œ[`create\\_tag`]æ¥åˆ›å»ºæ–°çš„åˆ†æ”¯å’Œæ ‡ç­¾:\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import create\\_branch, create\\_tag\n# Create a branch on a Space repo from `main` branch\n>>> create\\_branch(\""Matthijs/speecht5-tts-demo\"", repo\\_type=\""space\"", branch=\""handle-dog-speaker\"")\n# Create a tag on a Dataset repo from `v0.1-release` branch\n>>> create\\_branch(\""bigcode/the-stack\"", repo\\_type=\""dataset\"", revision=\""v0.1-release\"", tag=\""v0.1.1\"", tag\\_message=\""Bump release version.\"")\n```\nåŒæ—¶,ä½ å¯ä»¥ä»¥ç›¸åŒçš„æ–¹å¼ä½¿ç”¨ [`delete\\_branch`] å’Œ [`delete\\_tag`] å‡½æ•°æ¥åˆ é™¤åˆ†æ”¯æˆ–æ ‡ç­¾\n### åˆ—å‡ºæ‰€æœ‰çš„åˆ†æ”¯å’Œæ ‡ç­¾\nä½ è¿˜å¯ä»¥ä½¿ç”¨ [`list\\_repo\\_refs`] åˆ—å‡ºå­˜å‚¨åº“ä¸­çš„çŽ°æœ‰ Git å¼•ç”¨\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import list\\_repo\\_refs\n>>> api.list\\_repo\\_refs(\""bigcode/the-stack\"", repo\\_type=\""dataset\"")\nGitRefs(\nbranches=[\nGitRefInfo(name='main', ref='refs/heads/main', target\\_commit='18edc1591d9ce72aa82f56c4431b3c969b210ae3'),\nGitRefInfo(name='v1.1.a1', ref='refs/heads/v1.1.a1', target\\_commit='f9826b862d1567f3822d3d25649b0d6d22ace714')\n],\nconverts=[],\ntags=[\nGitRefInfo(name='v1.0', ref='refs/tags/v1.0', target\\_commit='c37a8cd1e382064d8aced5e05543c5f7753834da')\n]\n)\n```\n## ä¿®æ”¹å­˜å‚¨åº“è®¾ç½®\nå­˜å‚¨åº“å…·æœ‰ä¸€äº›å¯é…ç½®çš„è®¾ç½®ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ‚¨é€šå¸¸ä¼šåœ¨æµè§ˆå™¨ä¸­çš„å­˜å‚¨åº“è®¾ç½®é¡µé¢ä¸Šæ‰‹åŠ¨é…ç½®è¿™äº›è®¾ç½®ã€‚è¦é…ç½®å­˜å‚¨åº“ï¼Œæ‚¨å¿…é¡»å…·æœ‰å¯¹å…¶çš„å†™è®¿é—®æƒé™ï¼ˆæ‹¥æœ‰å®ƒæˆ–å±žäºŽç»„ç»‡ï¼‰ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ `huggingface\\_hub` åœ¨ç¼–ç¨‹æ–¹å¼ä¸Šé…ç½®çš„è®¾ç½®ã€‚\nä¸€äº›è®¾ç½®æ˜¯ç‰¹å®šäºŽ Spacesï¼ˆç¡¬ä»¶ã€çŽ¯å¢ƒå˜é‡ç­‰ï¼‰çš„ã€‚è¦é…ç½®è¿™äº›è®¾ç½®ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„[ç®¡ç†Spaces](../guides/manage-spaces)æŒ‡å—ã€‚\n### æ›´æ–°å¯è§æ€§\nä¸€ä¸ªå­˜å‚¨åº“å¯ä»¥æ˜¯å…¬å…±çš„æˆ–ç§æœ‰çš„ã€‚ç§æœ‰å­˜å‚¨åº“ä»…å¯¹æ‚¨æˆ–å­˜å‚¨åº“æ‰€åœ¨ç»„ç»‡çš„æˆå‘˜å¯è§ã€‚\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç å°†å­˜å‚¨åº“æ›´æ”¹ä¸ºç§æœ‰ï¼š\n```py\n>>> from huggingface\\_hub import update\\_repo\\_visibility\n>>> update\\_repo\\_visibility(repo\\_id=repo\\_id, private=True)\n```\n### é‡å‘½åæ‚¨çš„å­˜å‚¨åº“\næ‚¨å¯ä»¥ä½¿ç”¨ [`move\\_repo`] åœ¨ Hub ä¸Šé‡å‘½åæ‚¨çš„å­˜å‚¨åº“ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæ‚¨è¿˜å¯ä»¥å°†å­˜å‚¨åº“ä»Žä¸€ä¸ªç”¨æˆ·ç§»åŠ¨åˆ°ä¸€ä¸ªç»„ç»‡ã€‚åœ¨è¿™æ ·åšæ—¶ï¼Œæœ‰ä¸€äº›[é™åˆ¶](https://hf.co/docs/hub/repositories-settings#renaming-or-transferring-a-repo)éœ€è¦æ³¨æ„ã€‚ä¾‹å¦‚ï¼Œæ‚¨ä¸èƒ½å°†å­˜å‚¨åº“è½¬ç§»åˆ°å¦ä¸€ä¸ªç”¨æˆ·ã€‚\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import move\\_repo\n>>> move\\_repo(from\\_id=\""Wauplin/cool-model\"", to\\_id=\""huggingface/cool-model\"")\n```\n## ç®¡ç†å­˜å‚¨åº“çš„æœ¬åœ°å‰¯æœ¬\nä¸Šè¿°æ‰€æœ‰æ“ä½œéƒ½å¯ä»¥é€šè¿‡HTTPè¯·æ±‚å®Œæˆã€‚ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½å¸Œæœ›åœ¨æœ¬åœ°æ‹¥æœ‰å­˜å‚¨åº“çš„å‰¯æœ¬ï¼Œå¹¶ä½¿ç”¨æ‚¨ç†Ÿæ‚‰çš„Gitå‘½ä»¤ä¸Žä¹‹äº¤äº’ã€‚\n[`Repository`] ç±»å…è®¸æ‚¨ä½¿ç”¨ç±»ä¼¼äºŽGitå‘½ä»¤çš„å‡½æ•°ä¸ŽHubä¸Šçš„æ–‡ä»¶å’Œå­˜å‚¨åº“è¿›è¡Œäº¤äº’ã€‚å®ƒæ˜¯å¯¹Gitå’ŒGit-LFSæ–¹æ³•çš„åŒ…è£…ï¼Œä»¥ä½¿ç”¨æ‚¨å·²ç»äº†è§£å’Œå–œçˆ±çš„Gitå‘½ä»¤ã€‚åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…Git-LFSï¼ˆè¯·å‚é˜…[æ­¤å¤„](https://git-lfs.github.com/)èŽ·å–å®‰è£…è¯´æ˜Žï¼‰ã€‚\n### ä½¿ç”¨æœ¬åœ°å­˜å‚¨åº“\nä½¿ç”¨æœ¬åœ°å­˜å‚¨åº“è·¯å¾„å®žä¾‹åŒ–ä¸€ä¸ª [`Repository`] å¯¹è±¡ï¼š\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(local\\_dir=\""//\"")\n```\n### å…‹éš†\n`clone\\_from`å‚æ•°å°†ä¸€ä¸ªå­˜å‚¨åº“ä»ŽHugging Faceå­˜å‚¨åº“IDå…‹éš†åˆ°ç”± `local\\_dir`å‚æ•°æŒ‡å®šçš„æœ¬åœ°ç›®å½•ï¼š\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(local\\_dir=\""w2v2\"", clone\\_from=\""facebook/wav2vec2-large-960h-lv60\"")\n```\n`clone\\_from`è¿˜å¯ä»¥ä½¿ç”¨URLå…‹éš†å­˜å‚¨åº“ï¼š\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> repo = Repository(local\\_dir=\""huggingface-hub\"", clone\\_from=\""https://huggingface.co/facebook/wav2vec2-large-960h-lv60\"")\n```\nä½ å¯ä»¥å°†`clone\\_from`å‚æ•°ä¸Ž[`create\\_repo`]ç»“åˆä½¿ç”¨ï¼Œä»¥åˆ›å»ºå¹¶å…‹éš†ä¸€ä¸ªå­˜å‚¨åº“ï¼š\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> repo\\_url = create\\_repo(repo\\_id=\""repo\\_name\"")\n>>> repo = Repository(local\\_dir=\""repo\\_local\\_path\"", clone\\_from=repo\\_url)\n```\nå½“ä½ å…‹éš†ä¸€ä¸ªå­˜å‚¨åº“æ—¶ï¼Œé€šè¿‡åœ¨å…‹éš†æ—¶æŒ‡å®š`git\\_user`å’Œ`git\\_email`å‚æ•°ï¼Œä½ è¿˜å¯ä»¥ä¸ºå…‹éš†çš„å­˜å‚¨åº“é…ç½®Gitç”¨æˆ·åå’Œç”µå­é‚®ä»¶ã€‚å½“ç”¨æˆ·æäº¤åˆ°è¯¥å­˜å‚¨åº“æ—¶ï¼ŒGitå°†çŸ¥é“æäº¤çš„ä½œè€…æ˜¯è°ã€‚\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> repo = Repository(\n...""]",The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository.,model
"How many splits does the ""duorc"" dataset have?
","[""List splits and configurations\nDatasets typically have splits and may also have configurations. A \\_split\\_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A \\_configuration\\_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you're interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load\\_hub)! This guide shows you how to use Datasets Server's `/splits` endpoint to retrieve a dataset's splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\nThe `/splits` endpoint accepts the dataset name as its query parameter:\n\n```python\nimport requests\nheaders = {\""Authorization\"": f\""Bearer {API\\_TOKEN}\""}\nAPI\\_URL = \""https://datasets-server.huggingface.co/splits?dataset=duorc\""\ndef query():\nresponse = requests.get(API\\_URL, headers=headers)\nreturn response.json()\ndata = query()\n```\n\n```js\nimport fetch from \""node-fetch\"";\nasync function query(data) {\nconst response = await fetch(\n\""https://datasets-server.huggingface.co/splits?dataset=duorc\"",\n{\nheaders: { Authorization: `Bearer ${API\\_TOKEN}` },\nmethod: \""GET\""\n}\n);\nconst result = await response.json();\nreturn result;\n}\nquery().then((response) => {\nconsole.log(JSON.stringify(response));\n});\n```\n\n```curl\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\n-X GET \\\n-H \""Authorization: Bearer ${API\\_TOKEN}\""\n```\nThe endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n```json\n{\n\""splits\"": [\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""test\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""test\"" }\n],\n\""pending\"": [],\n\""failed\"": []\n}\n```"", ""Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package\\_reference/main\\_classes#datasets.DatasetInfo) object of the `datasets` library. ```json\n{\n\""dataset\\_info\"": {\n\""description\"": \""DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\n\"",\n\""citation\"": \""@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = {2018}\\n}\\n\"",\n\""homepage\"": \""https://duorc.github.io/\"",\n\""license\"": \""https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\"",\n\""features\"": {\n\""plot\\_id\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""plot\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""title\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""question\\_id\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""question\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""answers\"": {\n\""feature\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""\\_type\"": \""Sequence\""\n},\n\""no\\_answer\"": {\n\""dtype\"": \""bool\"",\n\""\\_type\"": \""Value\""\n}\n},\n\""builder\\_name\"": \""duorc\"",\n\""config\\_name\"": \""SelfRC\"",\n\""version\"": {\n\""version\\_str\"": \""1.0.0\"",\n\""major\"": 1,\n\""minor\"": 0,\n\""patch\"": 0\n},\n\""splits\"": {\n\""train\"": {\n\""name\"": \""train\"",\n\""num\\_bytes\"": 239852729,\n\""num\\_examples\"": 60721,\n\""dataset\\_name\"": \""duorc\""\n},\n\""validation\"": {\n\""name\"": \""validation\"",\n\""num\\_bytes\"": 51662519,\n\""num\\_examples\"": 12961,\n\""dataset\\_name\"": \""duorc\""\n},\n\""test\"": {\n\""name\"": \""test\"",\n\""num\\_bytes\"": 49142710,\n\""num\\_examples\"": 12559,\n\""dataset\\_name\"": \""duorc\""\n}\n},\n\""download\\_checksums\"": {\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_train.json\"": {\n\""num\\_bytes\"": 24388192,\n\""checksum\"": null\n},\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_dev.json\"": {\n\""num\\_bytes\"": 5051240,\n\""checksum\"": null\n},\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_test.json\"": {\n\""num\\_bytes\"": 5023228,\n\""checksum\"": null\n}\n},\n\""download\\_size\"": 34462660,\n\""dataset\\_size\"": 340657958,\n\""size\\_in\\_bytes\"": 375120618\n}\n}\n```"", ""It provides the number of rows, the number of colums (where applicable) and the size in bytes for the different forms of the data: original files, size in memory (RAM) and auto-converted parquet files. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has 187.213 rows along all its configurations and splits, for a total of 97MB. ```json\n{\n\""size\"": {\n\""dataset\"": {\n\""dataset\"": \""duorc\"",\n\""num\\_bytes\\_original\\_files\"": 97383710,\n\""num\\_bytes\\_parquet\\_files\"": 58710973,\n\""num\\_bytes\\_memory\"": 1059067116,\n\""num\\_rows\"": 187213\n},\n\""configs\"": [\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""num\\_bytes\\_original\\_files\"": 62921050,\n\""num\\_bytes\\_parquet\\_files\"": 37709127,\n\""num\\_bytes\\_memory\"": 718409158,\n\""num\\_rows\"": 100972,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""num\\_bytes\\_original\\_files\"": 34462660,\n\""num\\_bytes\\_parquet\\_files\"": 21001846,\n\""num\\_bytes\\_memory\"": 340657958,\n\""num\\_rows\"": 86241,\n\""num\\_columns\"": 7\n}\n],\n\""splits\"": [\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""train\"",\n\""num\\_bytes\\_parquet\\_files\"": 26005668,\n\""num\\_bytes\\_memory\"": 496682909,\n\""num\\_rows\"": 69524,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""validation\"",\n\""num\\_bytes\\_parquet\\_files\"": 5566868,\n\""num\\_bytes\\_memory\"": 106510489,\n\""num\\_rows\"": 15591,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""test\"",\n\""num\\_bytes\\_parquet\\_files\"": 6136591,\n\""num\\_bytes\\_memory\"": 115215760,\n\""num\\_rows\"": 15857,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""train\"",\n\""num\\_bytes\\_parquet\\_files\"": 14851720,\n\""num\\_bytes\\_memory\"": 239852729,\n\""num\\_rows\"": 60721,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""validation\"",\n\""num\\_bytes\\_parquet\\_files\"": 3114390,\n\""num\\_bytes\\_memory\"": 51662519,\n\""num\\_rows\"": 12961,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""test\"",\n\""num\\_bytes\\_parquet\\_files\"": 3035736,\n\""num\\_bytes\\_memory\"": 49142710,\n\""num\\_rows\"": 12559,\n\""num\\_columns\"": 7\n}\n]\n},\n\""pending\"": [],\n\""failed\"": [],\n\""partial\"": false\n}\n```\nIf the size has `partial: true` it means that the actual size of the dataset couldn't been determined because it's too big. In that case the number of rows and bytes can be inferior to the actual numbers.""]","The ""duorc"" dataset has six splits.",Six
"What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Fully Sharded Data Parallel\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or \\*rank\\*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class. Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer. ```bash\npip install accelerate\n```\n## FSDP configuration\nTo start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package\\_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`. ```bash\naccelerate config\n```\nWhen you run `accelerate config`, you'll be prompted with a series of options to configure your training environment."", ""This section covers some of the most important FSDP options. To learn more about the other available FSDP options, take a look at the [fsdp\\_config](https://huggingface.co/docs/transformers/main\\_classes/trainer#transformers.TrainingArguments.fsdp\\_config) parameters. ### Sharding strategy\nFSDP offers a number of sharding strategies to select from:\n\\* `FULL\\_SHARD` - shards model parameters, gradients and optimizer states across workers; select `1` for this option\n\\* `SHARD\\_GRAD\\_OP`- shard gradients and optimizer states across workers; select `2` for this option\n\\* `NO\\_SHARD` - don't shard anything (this is equivalent to DDP); select `3` for this option\n\\* `HYBRID\\_SHARD` - shard model parameters, gradients and optimizer states within each worker where each worker also has a full copy; select `4` for this option\n\\* `HYBRID\\_SHARD\\_ZERO2` - shard gradients and optimizer states within each worker where each worker also has a full copy; select `5` for this option\nThis is enabled by the `fsdp\\_sharding\\_strategy` flag."", ""By sharding the model parameters, optimizer and gradient states, and even offloading them to the CPU when they're inactive, FSDP can reduce the high cost of large-scale training. If you're interested in learning more, the following may be helpful:\n\\* Follow along with the more in-depth Accelerate guide for [FSDP](https://huggingface.co/docs/accelerate/usage\\_guides/fsdp). \\* Read the [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) blog post. \\* Read the [Scaling PyTorch models on Cloud TPUs with FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/) blog post.""]","Fully Sharded Data Parallel (FSDP) is a data parallel method designed to optimize memory usage during distributed training by sharding a model's parameters, gradients, and optimizer states across the available GPUs (workers). The main purposes of FSDP are:

1. **Memory Efficiency**: By sharding the model components, FSDP reduces the memory footprint on each GPU, allowing for the training of larger models on fewer GPUs compared to traditional methods like DistributedDataParallel (DDP), which replicates the entire model on each GPU.

2. **Scalability**: FSDP enables the training of very large models that would otherwise not fit into the memory of a single GPU, making it suitable for scaling up deep learning tasks.

3. **Integration with Accelerate**: FSDP is integrated with the Accelerate library, which simplifies the management of training in distributed environments, making it easier for users to set up and configure their training processes.

Overall, FSDP enhances the efficiency and scalability of distributed training, making it a valuable tool for deep learning practitioners.","FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes."
"What file format is used to save and store PyTorch model weights more securely than `.bin` files?
","[""We are also taking steps to make sure the existing PyTorch files are not malicious, but the best we can do is flag suspicious-looking files. Of course, there are other file formats out there, but\nnone seemed to meet the full set of [ideal requirements](https://github.com/huggingface/safetensors#yet-another-format-) our team identified. In addition to being safe, `safetensors` allows lazy loading and generally faster loads (around 100x faster on CPU). Lazy loading means loading only part of a tensor in an efficient manner. This particular feature enables arbitrary sharding with efficient inference libraries, such as [text-generation-inference](https://github.com/huggingface/text-generation-inference), to load LLMs (such as LLaMA, StarCoder, etc.) on various types of hardware\nwith maximum efficiency. Because it loads so fast and is framework agnostic, we can even use the format\nto load models from the same file in PyTorch or TensorFlow. ## The security audit\nSince `safetensors` main asset is providing safety guarantees, we wanted to make sure\nit actually delivered. That's why Hugging Face, EleutherAI, and Stability AI teamed up to get an external\nsecurity audit to confirm it. Important findings:\n- No critical security flaw leading to arbitrary code execution was found. - Some imprecisions in the spec format were detected and fixed. - Some missing validation allowed [polyglot files](https://en.wikipedia.org/wiki/Polyglot\\_(computing)), which was fixed. - Lots of improvements to the test suite were proposed and implemented. In the name of openness and transparency, all companies agreed to make the report\nfully public. [Full report](https://huggingface.co/datasets/safetensors/trail\\_of\\_bits\\_audit\\_repot/resolve/main/SOW-TrailofBits-EleutherAI\\_HuggingFace-v1.2.pdf)\nOne import thing to note is that the library is written in Rust."", ""Convert weights to safetensors\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`. The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository. For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights. Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space."", ""On CPU, if the file is already in cache, then it can\ntruly be zero-copy, whereas on GPU there is not such disk cache, so a copy is always required\nbut you can bypass allocating all the tensors on CPU at any given point. SafeTensors is not zero-copy for the header. The choice of JSON is pretty arbitrary, but since deserialization is <<< of the time required to load the actual tensor data and is readable I went that way, (also space is <<< to the tensor data). - Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the\nmoment. - Order: 'C' or row-major. This seems to have won. We can add that information later if needed. - Stride: No striding, all tensors need to be packed before being serialized. I have yet to see a case where it seems useful to have a strided tensor stored in serialized format. ### Benefits\nSince we can invent a new format we can propose additional benefits:\n- Prevent DOS attacks: We can craft the format in such a way that it's almost\nimpossible to use malicious files to DOS attack a user. Currently, there's a limit\non the size of the header of 100MB to prevent parsing extremely large JSON. Also when reading the file, there's a guarantee that addresses in the file\ndo not overlap in any way, meaning when you're loading a file you should never\nexceed the size of the file in memory\n- Faster load: PyTorch seems to be the fastest file to load out in the major\nML formats. However, it does seem to have an extra copy on CPU, which we\ncan bypass in this lib by using `torch.UntypedStorage.from\\_file`. Currently, CPU loading times are extremely fast with this lib compared to pickle. GPU loading times are as fast or faster than PyTorch equivalent. Loading first on CPU with memmapping with torch, and then moving all tensors to GPU seems\nto be faster too somehow (similar behavior in torch pickle)\n- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to\nload only part of the tensors on the various models. For\n[BLOOM](https://huggingface.co/bigscience/bloom) using this format enabled\nto load the model on 8 GPUs from 10mn with regular PyTorch weights down to 45s.""]",The file format used to save and store PyTorch model weights more securely than `.bin` files is the `safetensors` format.,`.safetensors`
"What type of security certification does Hugging Face have?
","[""Security\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)"", ""| Version | Supported |\n|---------|--------------------|\n| 1.x.x | :white\\_check\\_mark: |\n| 2.x.x | :white\\_check\\_mark: |\n## Reporting a Vulnerability\nTo report a security vulnerability, please contact: security@huggingface.co"", ""Single Sign-On (SSO)\nThe Hugging Face Hub gives you the ability to implement mandatory Single Sign-On (SSO) for your organization. We support both SAML 2.0 and OpenID Connect (OIDC) protocols. This feature is part of the [Enterprise Hub](https://huggingface.co/enterprise). ## How does it work? When Single Sign-On is enabled, the members of your organization must authenticate through your Identity Provider (IdP) to access any content under the organization's namespace. Public content will still be available to users who are not members of the organization. \\*\\*We use email addresses to identify SSO users. Make sure that your organizational email address (e.g. your company email) has been added to [your user account](https://huggingface.co/settings/account).\\*\\*\nWhen users log in, they will be prompted to complete the Single Sign-On authentication flow with a banner similar to the following:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-sso-prompt.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-sso-prompt-dark.png)\n\nSingle Sign-On only applies to your organization. Members may belong to other organizations on Hugging Face. We support [role mapping](#role-mapping): you can automatically assign [roles](./organizations-security#access-control-in-organizations) to organization members based on attributes provided by your Identity Provider. ### Supported Identity Providers\nYou can easily integrate Hugging Face Hub with a variety of Identity Providers, such as Okta, OneLogin or Azure Active Directory (Azure AD). Hugging Face Hub can work with any OIDC-compliant or SAML Identity Provider. ## How to configure OIDC/SAML provider in the Hub\nWe have some guides available to help with configuring based on your chosen SSO provider, or to take inspiration from:\n- [How to configure OIDC with Okta in the Hub](./security-sso-okta-oidc)\n- [How to configure SAML with Okta in the Hub](./security-sso-okta-saml)\n- [How to configure SAML with Azure in the Hub](./security-sso-azure-saml)\n### Users Management\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-settings-users.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-settings-users-dark.png)\n\n#### Session Timeout\nThis value sets the duration of the session for members of your organization.""]","Hugging Face is SOC2 Type 2 certified, which means we provide security certification to our customers and actively monitor and patch any security weaknesses.",SOC2 Type 2 certified
"What do RAG models combine to generate outputs?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# RAG\n\n[![Models](https://img.shields.io/badge/All_model_pages-rag-blueviolet)](https://huggingface.co/models?filter=rag)\n\n## Overview\nRetrieval-augmented generation (\""RAG\"") models combine the powers of pretrained dense retrieval (DPR) and\nsequence-to-sequence models."", ""This information retrieval step allows [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) to make use of multiple sources of knowledge -- those that are baked into the model parameters and the information that is contained in the contextual passages, allowing it to outperform other state-of-the-art models in tasks like question answering. You can try it for yourself using this [demo provided by Huggingface](https://huggingface.co/rag/)! ### Scaling up fine-tuning\nThis retrieval of contextual documents is crucial for RAG's state-of-the-art results but introduces an extra layer of complexity. When scaling up the training process via a data-parallel training routine, a naive implementation of the document lookup can become a bottleneck for training. Further, the \\*\\*document index\\*\\* used in the retrieval component is often quite large, making it infeasible for each training worker to load its own replicated copy of the index. The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) communication package for the document retrieval portion."", ""If this path already exists, the script will use saved predictions to calculate metrics. Add `--recalculate` parameter to force the script to perform inference from scratch. An example e2e evaluation run could look as follows:\n```bash\npython examples/research\\_projects/rag/eval\\_rag.py \\\n--model\\_name\\_or\\_path facebook/rag-sequence-nq \\\n--model\\_type rag\\_sequence \\\n--evaluation\\_set path/to/test.source \\\n--gold\\_data\\_path path/to/gold\\_data \\\n--predictions\\_path path/to/e2e\\_preds.txt \\\n--eval\\_mode e2e \\\n--gold\\_data\\_mode qa \\\n--n\\_docs 5 \\ # You can experiment with retrieving different number of documents at evaluation time\n--print\\_predictions \\\n--recalculate \\ # adding this parameter will force recalculating predictions even if predictions\\_path already exists\n```\n# Use your own knowledge source\nBy default, RAG uses the English Wikipedia as a knowledge source, known as the 'wiki\\_dpr' dataset. With `use\\_custom\\_knowledge\\_dataset.py` you can build your own knowledge source, \\*e.g.\\* for RAG. For instance, if documents are serialized as tab-separated csv files with the columns \""title\"" and \""text\"", one can use `use\\_own\\_knowledge\\_dataset.py` as follows:\n```bash\npython examples/research\\_projects/rag/use\\_own\\_knowledge\\_dataset.py \\\n--csv\\_path path/to/my\\_csv \\\n--output\\_dir path/to/my\\_knowledge\\_dataset \\\n```\nThe created outputs in `path/to/my\\_knowledge\\_dataset` can then be used to finetune RAG as follows:\n```bash\npython examples/research\\_projects/rag/finetune\\_rag.py \\\n--data\\_dir $DATA\\_DIR \\\n--output\\_dir $OUTPUT\\_DIR \\\n--model\\_name\\_or\\_path $MODEL\\_NAME\\_OR\\_PATH \\\n--model\\_type rag\\_sequence \\\n--fp16 \\\n--gpus 8\n--index\\_name custom\n--passages\\_path path/to/data/my\\_knowledge\\_dataset\n--index\\_path path/to/my\\_knowledge\\_dataset\\_hnsw\\_index.faiss\n```""]","RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs. This combination allows RAG to utilize multiple sources of knowledge, including information baked into the model parameters and contextual passages, enabling it to outperform other state-of-the-art models in tasks like question answering.",Pretrained dense retrieval (DPR) and sequence-to-sequence models.
"What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# MarkupLM\n## Overview\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm). The model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838\\_From\\_one\\_tree\\_to\\_a\\_forest\\_a\\_unified\\_solution\\_for\\_structured\\_web\\_data\\_extraction), a dataset\nfor information extraction from web pages (basically named-entity recogntion on web pages)\nThe abstract from the paper is the following:\n\\*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\ntasks. The pre-trained model and code will be publicly available.\\*\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm). ## Usage tips\n- In addition to `input\\_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath\\_tags\\_seq` and `xpath\\_subs\\_seq`. These are the XPATH tags and subscripts respectively for each token in the input sequence. - One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info. ![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg)\n MarkupLM architecture. Taken from the [original paper.](https://arxiv.org/abs/2110.08518) \n## Usage: MarkupLMProcessor\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input\\_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks. ```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\nfeature\\_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from\\_pretrained(\""microsoft/markuplm-base\"")\nprocessor = MarkupLMProcessor(feature\\_extractor, tokenizer)\n```\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input\\_ids`, `attention\\_mask`, `token\\_type\\_ids`, `xpath\\_subs\\_seq`, `xpath\\_tags\\_seq`. Optionally, one can provide node labels to the processor, which are turned into token-level `labels`. [`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Utilities for `FeatureExtractors`\nThis page lists all the utility functions that can be used by the audio [`FeatureExtractor`] in order to compute special features from a raw audio using common algorithms such as \\*Short Time Fourier Transform\\* or \\*log mel spectrogram\\*. Most of those are only useful if you are studying the code of the audio processors in the library. ## Audio Transformations\n[[autodoc]] audio\\_utils.hertz\\_to\\_mel\n[[autodoc]] audio\\_utils.mel\\_to\\_hertz\n[[autodoc]] audio\\_utils.mel\\_filter\\_bank\n[[autodoc]] audio\\_utils.optimal\\_fft\\_length\n[[autodoc]] audio\\_utils.window\\_function\n[[autodoc]] audio\\_utils.spectrogram\n[[autodoc]] audio\\_utils.power\\_to\\_db\n[[autodoc]] audio\\_utils.amplitude\\_to\\_db"", ""In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs). \\*\\*Use case 1: web page classification (training, inference) + token classification (inference), parse\\_html = True\\*\\*\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML. ```python\n>>> from transformers import MarkupLMProcessor\n>>> processor = MarkupLMProcessor.from\\_pretrained(\""microsoft/markuplm-base\"")\n>>> html\\_string = \""\""\""\n... ... ... ... Hello world\n... ... ... # Welcome\n\n... Here is my website. ... ... \""\""\""\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\n>>> encoding = processor(html\\_string, return\\_tensors=\""pt\"")\n>>> print(encoding.keys())\ndict\\_keys(['input\\_ids', 'token\\_type\\_ids', 'attention\\_mask', 'xpath\\_tags\\_seq', 'xpath\\_subs\\_seq'])\n```\n\\*\\*Use case 2: web page classification (training, inference) + token classification (inference), parse\\_html=False\\*\\*\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse\\_html` to `False`. ```python\n>>> from transformers import MarkupLMProcessor\n>>> processor = MarkupLMProcessor.from\\_pretrained(\""microsoft/markuplm-base\"")\n>>> processor.parse\\_html = False\n>>> nodes = [\""hello\"", \""world\"", \""how\"", \""are\""]\n>>> xpaths = [\""/html/body/div/li[1]/div/span\"", \""/html/body/div/li[1]/div/span\"", \""html/body\"", \""html/body/div\""]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return\\_tensors=\""pt\"")\n>>> print(encoding.keys())\ndict\\_keys(['input\\_ids', 'token\\_type\\_ids', 'attention\\_mask', 'xpath\\_tags\\_seq', 'xpath\\_subs\\_seq'])\n```\n\\*\\*Use case 3: token classification (training), parse\\_html=False\\*\\*\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\ncorresponding node labels in order to train a model.""]","MarkupLMFeatureExtractor uses Beautiful Soup, a Python library for pulling data out of HTML and XML files, to extract data from HTML and XML documents.",Beautiful Soup
"What is the file size limit for syncing to HF Spaces without using Git-LFS?
","[""Managing Spaces with CircleCI Workflows\nYou can keep your app in sync with your GitHub repository with a \\*\\*CircleCI workflow\\*\\*. [CircleCI](https://circleci.com) is a continuous integration and continuous delivery (CI/CD) platform that helps automate the software development process. A [CircleCI workflow](https://circleci.com/docs/workflows/) is a set of automated tasks defined in a configuration file, orchestrated by CircleCI, to streamline the process of building, testing, and deploying software applications. \\*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\\*\nFirst, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository. ```bash\ngit remote add space https://huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME\n```\nThen force push to sync everything for the first time:\n```bash\ngit push --force space main\n```\nNext, set up a [CircleCI workflow](https://circleci.com/docs/workflows/) to push your `main` git branch to Spaces. In the example below:\n\\* Replace `HF\\_USERNAME` with your username and `SPACE\\_NAME` with your Space name. \\* [Create a context in CircleCI](https://circleci.com/docs/contexts/) and add an env variable into it called \\*HF\\_PERSONAL\\_TOKEN\\* (you can give it any name, use the key you create in place of HF\\_PERSONAL\\_TOKEN) and the value as your Hugging Face API token. You can find your Hugging Face API token under \\*\\*API Tokens\\*\\* on [your Hugging Face profile](https://huggingface.co/settings/tokens). ```yaml\nversion: 2.1\nworkflows:\nmain:\njobs:\n- sync-to-huggingface:\ncontext:\n- HuggingFace\nfilters:\nbranches:\nonly:\n- main\njobs:\nsync-to-huggingface:\ndocker:\n- image: alpine\nresource\\_class: small\nsteps:\n- run:\nname: install git\ncommand: apk update && apk add openssh-client git\n- checkout\n- run:\nname: push to Huggingface hub\ncommand: |\ngit config user.email \""\""\ngit config user.name \""\""\ngit push -f https://HF\\_USERNAME:${HF\\_PERSONAL\\_TOKEN}@huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME main\n```"", ""There are a few reasons for this:\n- Uploading and downloading smaller files is much easier both for you and the other users. Connection issues can always\nhappen when streaming data and smaller files avoid resuming from the beginning in case of errors. - Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\nleading to a slower download speed. In all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size. - \\*\\*Number of commits\\*\\*: There is no hard limit for the total number of commits on your repo history. However, from\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a\nfresh start using `huggingface\\_hub`'s [`super\\_squash\\_history`](https://huggingface.co/docs/huggingface\\_hub/main/en/package\\_reference/hf\\_api#huggingface\\_hub.HfApi.super\\_squash\\_history)."", ""Be aware that this is a non-revertible operation. - \\*\\*Number of operations per commit\\*\\*: Once again, there is no hard limit here. When a commit is uploaded on the Hub, each\ngit operation (addition or delete) is checked by the server. When a hundred LFS files are committed at once,\neach file is checked individually to ensure it's been correctly uploaded. When pushing data through HTTP,\na timeout of 60s is set on the request, meaning that if the process takes more time, an error is raised. However, it can\nhappen (in rare cases) that even if the timeout is raised client-side, the process is still\ncompleted server-side. This can be checked manually by browsing the repo on the Hub. To prevent this timeout, we recommend\nadding around 50-100 files per commit.""]","The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. If your files are larger than this, you will need to use Git-LFS or review your files and check your history.",10MB
"What is the title of the paper introducing the ByT5 model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ByT5\n## Overview\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, Colin Raffel."", ""The abstract from the paper is the following:\n\\*Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from\nthe model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they\ncan process text in any language out of the box, they are more robust to noise, and they minimize technical debt by\nremoving complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token\nsequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of\noperating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with\nminimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count,\ntraining FLOPs, and inference speed, and show that byte-level models are competitive with their token-level\ncounterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on\ntasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of\npre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our\nexperiments.\\*\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/byt5). ByT5's architecture is based on the T5v1.1 model, refer to [T5v1.1's documentation page](t5v1.1) for the API reference. They\nonly differ in how inputs should be prepared for the model, see the code examples below. Since ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix. ## Usage example\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\n```python\n>>> from transformers import T5ForConditionalGeneration\n>>> import torch\n>>> model = T5ForConditionalGeneration.from\\_pretrained(\""google/byt5-small\"")\n>>> num\\_special\\_tokens = 3\n>>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5. >>> # => Need to shift utf-8 character encodings by 3 before passing ids to model. >>> input\\_ids = torch.tensor([list(\""Life is like a box of chocolates.\"".encode(\""utf-8\""))]) + num\\_special\\_tokens\n>>> labels = torch.tensor([list(\""La vie est comme une boÃ®te de chocolat.\"".encode(\""utf-8\""))]) + num\\_special\\_tokens\n>>> loss = model(input\\_ids, labels=labels).loss\n>>> loss.item()\n2.66\n```\nFor batched inference and training it is however recommended to make use of the tokenizer:\n```python\n>>> from transformers import T5ForConditionalGeneration, AutoTokenizer\n>>> model = T5ForConditionalGeneration.from\\_pretrained(\""google/byt5-small\"")\n>>> tokenizer = AutoTokenizer.from\\_pretrained(\""google/byt5-small\"")\n>>> model\\_inputs = tokenizer(\n... [\""Life is like a box of chocolates.\"", \""Today is Monday.\""], padding=\""longest\"", return\\_tensors=\""pt\""\n...""]","The title of the paper introducing the ByT5 model is ""ByT5: Towards a token-free future with pre-trained byte-to-byte models.""",ByT5: Towards a token-free future with pre-trained byte-to-byte models
"What is the dimension of the feature vector for the base BERT model?
","[""n this video, we'll study the encoder architecture. An example of a popular encoder-only architecture is BERT, which is the most popular model of its kind. Let's first start by understanding how it works. We'll use a small example, using three words. We use these as inputs, and pass them through the encoder. We retrieve a numerical representation of each word. Here, for example, the encoder converts the three words â€œWelcome to NYCâ€ in these three sequences of numbers. The encoder outputs exactly one sequence of numbers per input word. This numerical representation can also be called a \""Feature vector\"", or \""Feature tensor\"". Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \""to\"", isn't the representation of only the \""to\"" word. It also takes into account the words around it, which we call the â€œcontextâ€.As in, it looks to the left context, the word on the left of the one we're studying (here the word \""Welcome\"") and the context on the right (here the word \""NYC\"") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \""meaning\"" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence."", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# BERT\n\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\n\n## Overview\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It's a\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\n\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\narchitecture modifications.\\*\n\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\*\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf)."", ""---\n## 2. How does BERT Work? BERT works by leveraging the following:\n### 2.1 Large amounts of training data\nA massive dataset of 3.3 Billion words has contributed to BERTâ€™s continued success. BERT was specifically trained on Wikipedia (\\~2.5B words) and Googleâ€™s BooksCorpus (\\~800M words). These large informational datasets contributed to BERTâ€™s deep knowledge not only of the English language but also of our world! ðŸš€\nTraining on a dataset this large takes a long time. BERTâ€™s training was made possible thanks to the novel Transformer architecture and sped up by using TPUs (Tensor Processing Units - Googleâ€™s custom circuit built specifically for large ML models). â€”64 TPUs trained BERT over the course of 4 days. \\*\\*Note:\\*\\* Demand for smaller BERT models is increasing in order to use BERT within smaller computational environments (like cell phones and personal computers). [23 smaller BERT models were released in March 2020](https://github.com/google-research/bert). [DistilBERT](https://huggingface.co/docs/transformers/model\\_doc/distilbert) offers a lighter version of BERT; runs 60% faster while maintaining over 95% of BERTâ€™s performance. ### 2.2 What is a Masked Language Model? MLM enables/enforces bidirectional learning from text by masking (hiding) a word in a sentence and forcing BERT to bidirectionally use the words on either side of the covered word to predict the masked word.""]",The dimension of the feature vector for the base BERT model is 768.,768
"What special identifier does the WordPiece Model use for continuing subwords?
","[""WordPiece tokenization[[wordpiece-tokenization]]\nWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It's very similar to BPE in terms of the training, but the actual tokenization is done differently. ðŸ’¡ This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm. ## Training algorithm[[training-algorithm]]\nâš ï¸ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate. Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\""word\""` gets split like this:\n```\nw ##o ##r ##d\n```\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix. Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\n$$\\mathrm{score} = (\\mathrm{freq\\\\_of\\\\_pair}) / (\\mathrm{freq\\\\_of\\\\_first\\\\_element} \\times \\mathrm{freq\\\\_of\\\\_second\\\\_element})$$\nBy dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary."", ""For instance,\nthe word `\""bug\""` would be tokenized to `[\""b\"", \""ug\""]` but `\""mug\""` would be tokenized as `[\""\"", \""ug\""]` since\nthe symbol `\""m\""` is not in the base vocabulary. In general, single letters such as `\""m\""` are not replaced by the\n`\""\""` symbol because the training data usually includes at least one occurrence of each letter, but it is likely\nto happen for very special characters like emojis. As mentioned earlier, the vocabulary size, \\*i.e.\\* the base vocabulary size + the number of merges, is a hyperparameter\nto choose. For instance [GPT](model\\_doc/gpt) has a vocabulary size of 40,478 since they have 478 base characters\nand chose to stop training after 40,000 merges. #### Byte-level BPE\nA base vocabulary that includes all possible base characters can be quite large if \\*e.g.\\* all unicode characters are\nconsidered as base characters. To have a better base vocabulary, [GPT-2](https://cdn.openai.com/better-language-models/language\\_models\\_are\\_unsupervised\\_multitask\\_learners.pdf) uses bytes\nas the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that\nevery base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2's\ntokenizer can tokenize every text without the need for the  symbol. [GPT-2](model\\_doc/gpt) has a vocabulary\nsize of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned\nwith 50,000 merges. ### WordPiece\nWordPiece is the subword tokenization algorithm used for [BERT](model\\_doc/bert), [DistilBERT](model\\_doc/distilbert), and [Electra](model\\_doc/electra). The algorithm was outlined in [Japanese and Korean\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary. So what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is\nequivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by\nits second symbol is the greatest among all symbol pairs. \\*E.g.\\* `\""u\""`, followed by `\""g\""` would have only been\nmerged if the probability of `\""ug\""` divided by `\""u\""`, `\""g\""` would have been greater than for any other symbol\npair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it \\_loses\\_ by merging two symbols\nto ensure it's \\_worth it\\_. ### Unigram\nUnigram is a subword tokenization algorithm introduced in [Subword Regularization: Improving Neural Network Translation\nModels with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). In contrast to BPE or\nWordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each\nsymbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and\nthe most common substrings. Unigram is not used directly for any of the models in the transformers, but it's used in\nconjunction with [SentencePiece](#sentencepiece). At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training\ndata given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm\ncomputes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then\nremoves p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, \\*i.e.\\* those\nsymbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has\nreached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized. Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of\ntokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:\n```\n[\""b\"", \""g\"", \""h\"", \""n\"", \""p\"", \""s\"", \""u\"", \""ug\"", \""un\"", \""hug\""],\n```\n`\""hugs\""` could be tokenized both as `[\""hug\"", \""s\""]`, `[\""h\"", \""ug\"", \""s\""]` or `[\""h\"", \""u\"", \""g\"", \""s\""]`. So which one\nto choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that\nthe probability of each possible tokenization can be computed after training."", ""We can see that the words `[\""i\"", \""have\"", \""a\"", \""new\""]` are present in the tokenizer's vocabulary, but the word `\""gpu\""` is not. Consequently, the\ntokenizer splits `\""gpu\""` into known subwords: `[\""gp\"" and \""##u\""]`. `\""##\""` means that the rest of the token should\nbe attached to the previous one, without space (for decoding or reversal of the tokenization). As another example, [`~transformers.XLNetTokenizer`] tokenizes our previously exemplary text as follows:\n```py\n>>> from transformers import XLNetTokenizer\n>>> tokenizer = XLNetTokenizer.from\\_pretrained(\""xlnet-base-cased\"")\n>>> tokenizer.tokenize(\""Don't you love ðŸ¤— Transformers? We sure do.\"")\n[\""â–Don\"", \""'\"", \""t\"", \""â–you\"", \""â–love\"", \""â–\"", \""ðŸ¤—\"", \""â–\"", \""Transform\"", \""ers\"", \""?\"", \""â–We\"", \""â–sure\"", \""â–do\"", \"".\""]\n```\nWe'll get back to the meaning of those `\""â–\""` when we look at [SentencePiece](#sentencepiece). As one can see,\nthe rare word `\""Transformers\""` has been split into the more frequent subwords `\""Transform\""` and `\""ers\""`. Let's now look at how the different subword tokenization algorithms work. Note that all of those tokenization\nalgorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained\non. ### Byte-Pair Encoding (BPE)\nByte-Pair Encoding (BPE) was introduced in [Neural Machine Translation of Rare Words with Subword Units (Sennrich et\nal., 2015)](https://arxiv.org/abs/1508.07909).""]","The WordPiece model uses the prefix `##` to indicate continuing subwords. For example, if a word is split into subwords, the subwords will be prefixed with `##` to show that they are part of the same word.",##
"What is the purpose of the ðŸ§¨ Diffusers tutorials?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\nWelcome to ðŸ§¨ Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how ðŸ§¨ Diffusers is meant to be used. You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want. After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications. Feel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers! Let's start diffusing!"", ""!---\nCopyright 2022 - The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![](https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg)\n\n[![GitHub](https://img.shields.io/github/license/huggingface/datasets.svg?color=blue)](https://github.com/huggingface/diffusers/blob/main/LICENSE)\n[![GitHub release](https://img.shields.io/github/release/huggingface/diffusers.svg)](https://github.com/huggingface/diffusers/releases)\n[![GitHub release](https://static.pepy.tech/badge/diffusers/month)](https://pepy.tech/project/diffusers)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CODE_OF_CONDUCT.md)\n[![X account](https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&label=Follow%20%40diffuserslib)](https://twitter.com/diffuserslib)\n\nðŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, ðŸ¤— Diffusers is a modular toolbox that supports both."", ""Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). ðŸ¤— Diffusers offers three core components:\n- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code. - Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality. - Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. ## Installation\nWe recommend installing ðŸ¤— Diffusers in a virtual environment from PyPI or Conda. For more details about installing [PyTorch](https://pytorch.org/get-started/locally/) and [Flax](https://flax.readthedocs.io/en/latest/#installation), please refer to their official documentation. ### PyTorch\nWith `pip` (official package):\n```bash\npip install --upgrade diffusers[torch]\n```\nWith `conda` (maintained by the community):\n```sh\nconda install -c conda-forge diffusers\n```\n### Flax\nWith `pip` (official package):\n```bash\npip install --upgrade diffusers[flax]\n```\n### Apple Silicon (M1/M2) support\nPlease refer to the [How to use Stable Diffusion in Apple Silicon](https://huggingface.co/docs/diffusers/optimization/mps) guide. ## Quickstart\nGenerating outputs is super easy with ðŸ¤— Diffusers. To generate an image from text, use the `from\\_pretrained` method to load any pretrained diffusion model (browse the [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) for 16000+ checkpoints):\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\npipeline = DiffusionPipeline.from\\_pretrained(\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16)\npipeline.to(\""cuda\"")\npipeline(\""An image of a squirrel in Picasso style\"").images[0]\n```\nYou can also dig into the models and schedulers toolbox to build your own diffusion system:\n```python\nfrom diffusers import DDPMScheduler, UNet2DModel\nfrom PIL import Image\nimport torch\nscheduler = DDPMScheduler.from\\_pretrained(\""google/ddpm-cat-256\"")\nmodel = UNet2DModel.from\\_pretrained(\""google/ddpm-cat-256\"").to(\""cuda\"")\nscheduler.set\\_timesteps(50)\nsample\\_size = model.config.sample\\_size\nnoise = torch.randn((1, 3, sample\\_size, sample\\_size), device=\""cuda\"")\ninput = noise\nfor t in scheduler.timesteps:\nwith torch.no\\_grad():\nnoisy\\_residual = model(input, t).sample\nprev\\_noisy\\_sample = scheduler.step(noisy\\_residual, t, input).prev\\_sample\ninput = prev\\_noisy\\_sample\nimage = (input / 2 + 0.5).clamp(0, 1)\nimage = image.cpu().permute(0, 2, 3, 1).numpy()[0]\nimage = Image.fromarray((image \\* 255).round().astype(\""uint8\""))\nimage\n```\nCheck out the [Quickstart](https://huggingface.co/docs/diffusers/quicktour) to launch your diffusion journey today! ## How to navigate the documentation\n| \\*\\*Documentation\\*\\* | \\*\\*What can I learn?\\*\\* |\n|---------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Tutorial](https://huggingface.co/docs/diffusers/tutorials/tutorial\\_overview) | A basic crash course for learning how to use the library's most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model. |\n| [Loading](https://huggingface.co/docs/diffusers/using-diffusers/loading\\_overview) | Guides for how to load and configure all the components (pipelines, models, and schedulers) of the library, as well as how to use different schedulers. |\n| [Pipelines for inference](https://huggingface.co/docs/diffusers/using-diffusers/pipeline\\_overview) | Guides for how to use pipelines for different inference tasks, batched generation, controlling generated outputs and randomness, and how to contribute a pipeline to the library.""]","The purpose of the ðŸ§¨ Diffusers tutorials is to provide a gentle introduction to diffusion models and help users understand the fundamentals of the library. These beginner-friendly tutorials are designed to teach users how to use a pipeline for inference to rapidly generate outputs, as well as how to deconstruct that pipeline to understand the library as a modular toolbox for building their own diffusion systems. After completing the tutorials, users will gain the necessary skills to explore the library on their own and apply it to their own projects and applications.",To provide a gentle introduction to diffusion models and help understand the library fundamentals.
"What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?
","[""Using Flagging\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\nTags: FLAGGING, DATA\n## Introduction\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \""hard\"" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust. Gradio simplifies the collection of this data by including a \\*\\*Flag\\*\\* button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`. ## The \\*\\*Flag\\*\\* button in `gradio.Interface`\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked \\*\\*Flag\\*\\*. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file. There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail. - `allow\\_flagging`: this parameter can be set to either `\""manual\""` (default), `\""auto\""`, or `\""never\""`. - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked. - `auto`: users will not see a button to flag, but every sample will be flagged automatically. - `never`: users will not see a button to flag, and no sample will be flagged. - `flagging\\_options`: this parameter can be either `None` (default) or a list of strings. - If `None`, then the user simply clicks on the \\*\\*Flag\\*\\* button and no additional options are shown. - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\""Incorrect\"", \""Ambiguous\""]`, then buttons labeled \\*\\*Flag as Incorrect\\*\\* and \\*\\*Flag as Ambiguous\\*\\* appear. This only applies if `allow\\_flagging` is `\""manual\""`. - The chosen option is then logged along with the input and output. - `flagging\\_dir`: this parameter takes a string. - It represents what to name the directory where flagged data is stored. - `flagging\\_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n- Using this parameter allows you to write custom code that gets run when the flag button is clicked\n- By default, this is set to an instance of `gr.CSVLogger`\n- One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset."", ""- `description`: which accepts text, markdown or HTML and places it right under the title. - `article`: which also accepts text, markdown or HTML and places it below the interface. ![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)\nIf you're using the `Blocks` API instead, you can insert text, markdown, or HTML anywhere using the `gr.Markdown(...)` or `gr.HTML(...)` components, with descriptive content inside the `Component` constructor. Another useful keyword argument is `label=`, which is present in every `Component`. This modifies the label text at the top of each `Component`. You can also add the `info=` keyword argument to form elements like `Textbox` or `Radio` to provide further information on their usage. ```python\ngr.Number(label='Age', info='In years, must be greater than 0')\n```\n## Flagging\nBy default, an `Interface` will have \""Flag\"" button. When a user testing your `Interface` sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the `flagging\\_dir=` argument to the `Interface` constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:\n```directory\n+-- calculator.py\n+-- flagged/\n| +-- logs.csv\n```\n\\_flagged/logs.csv\\_\n```csv\nnum1,operation,num2,Output\n5,add,7,12\n6,subtract,1.5,4.5\n```\nWith the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:\n```directory\n+-- sepia.py\n+-- flagged/\n| +-- logs.csv\n| +-- im/\n| | +-- 0.png\n| | +-- 1.png\n| +-- Output/\n| | +-- 0.png\n| | +-- 1.png\n```\n\\_flagged/logs.csv\\_\n```csv\nim,Output\nim/0.png,Output/0.png\nim/1.png,Output/1.png\n```\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging\\_options` argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV."", ""(See more below.)\n## What happens to flagged data? Within the directory provided by the `flagging\\_dir` argument, a CSV file will log the flagged data. Here's an example: The code below creates the calculator interface embedded below it:\n```python\nimport gradio as gr\ndef calculator(num1, operation, num2):\nif operation == \""add\"":\nreturn num1 + num2\nelif operation == \""subtract\"":\nreturn num1 - num2\nelif operation == \""multiply\"":\nreturn num1 \\* num2\nelif operation == \""divide\"":\nreturn num1 / num2\niface = gr.Interface(\ncalculator,\n[\""number\"", gr.Radio([\""add\"", \""subtract\"", \""multiply\"", \""divide\""]), \""number\""],\n\""number\"",\nallow\\_flagging=\""manual\""\n)\niface.launch()\n```\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged. ```directory\n+-- flagged/\n| +-- logs.csv\n```\n\\_flagged/logs.csv\\_\n```csv\nnum1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n```\nIf the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure. ```directory\n+-- flagged/\n| +-- logs.csv\n| +-- image/\n| | +-- 0.png\n| | +-- 1.png\n| +-- Output/\n| | +-- 0.png\n| | +-- 1.png\n```\n\\_flagged/logs.csv\\_\n```csv\nim,Output timestamp\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\n```\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging\\_options` argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV. If we go back to the calculator example, the following code will create the interface embedded below it. ```python\niface = gr.Interface(\ncalculator,\n[\""number\"", gr.Radio([\""add\"", \""subtract\"", \""multiply\"", \""divide\""]), \""number\""],\n\""number\"",\nallow\\_flagging=\""manual\"",\nflagging\\_options=[\""wrong sign\"", \""off by one\"", \""other\""]\n)\niface.launch()\n```\nWhen users click the flag button, the csv file will now include a column indicating the selected option. \\_flagged/logs.csv\\_\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```\n## The HuggingFaceDatasetSaver Callback\nSometimes, saving the data to a local CSV file doesn't make sense.""]","The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `""manual""`. This means that users will see a button to flag, and samples are only flagged when the button is clicked.","""manual"""
"Where can the full code for the Stable Diffusion demo be found?
","[""--\ntitle: ðŸ§¨ Stable Diffusion in JAX / Flax ! thumbnail: /blog/assets/108\\_stable\\_diffusion\\_jax/thumbnail.png\nauthors:\n- user: pcuenq\n- user: patrickvonplaten\n---\n# ðŸ§¨ Stable Diffusion in JAX / Flax ! [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_jax_how_to.ipynb)\n# \\*\\*Stable Diffusion in JAX / Flax\\*\\* ðŸš€\nðŸ¤— Hugging Face [Diffusers](https://github.com/huggingface/diffusers) supports Flax since version `0.5.1`! This allows for super fast inference on Google TPUs, such as those available in Colab, Kaggle or Google Cloud Platform. This post shows how to run inference using JAX / Flax. If you want more details about how Stable Diffusion works or want to run it in GPU, please refer to [this Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). If you want to follow along, click the button above to open this post as a Colab notebook. First, make sure you are using a TPU backend. If you are running this notebook in Colab, select `Runtime` in the menu above, then select the option \""Change runtime type\"" and then select `TPU` under the `Hardware accelerator` setting. Note that JAX is not exclusive to TPUs, but it shines on that hardware because each TPU server has 8 TPU accelerators working in parallel. ## Setup\n``` python\nimport jax\nnum\\_devices = jax.device\\_count()\ndevice\\_type = jax.devices()[0].device\\_kind\nprint(f\""Found {num\\_devices} JAX devices of type {device\\_type}.\"")\nassert \""TPU\"" in device\\_type, \""Available device is not a TPU, please select TPU from Edit > Notebook settings > Hardware accelerator\""\n```\n\\*Output\\*:\n```bash\nFound 8 JAX devices of type TPU v2."", ""Free solutions include many [spaces](https://huggingface.co/spaces) here on ðŸ¤— Hugging Face, such as the [Stable Diffusion 2.1 Demo](https://huggingface.co/spaces/stabilityai/stable-diffusion) or the [camemduru webui](https://huggingface.co/spaces/camenduru/webui). You can find a list of additional online services [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services). You can even use ðŸ¤— [Diffusers](https://huggingface.co/docs/diffusers/index) to write your own free solution! You can find a simple code example to get started [here](https://colab.research.google.com/drive/1HebngGyjKj7nLdXfj6Qi0N1nh7WvD74z?usp=sharing). \\*Note:\\* Parts of this series will use advanced features such as image2image, which may not be available on all online services. ### Generating Concept Art \nLet's generate some concept art. The steps are simple:\n1. Type what you want. 2. Click generate. ![Stable Diffusion Demo Space](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/sd-demo.png)\nBut, how do you get the results you actually want?"", ""--\ntitle: Using Stable Diffusion with Core ML on Apple Silicon\nthumbnail: /blog/assets/diffusers\\_coreml/thumbnail.png\nauthors:\n- user: pcuenq\n---\n# Using Stable Diffusion with Core ML on Apple Silicon\nThanks to Apple engineers, you can now run Stable Diffusion on Apple Silicon using Core ML! [This Apple repo](https://github.com/apple/ml-stable-diffusion) provides conversion scripts and inference code based on [ðŸ§¨ Diffusers](https://github.com/huggingface/diffusers), and we love it! To make it as easy as possible for you, we converted the weights ourselves and put the Core ML versions of the models in [the Hugging Face Hub](https://hf.co/apple). \\*\\*Update\\*\\*: some weeks after this post was written we created a native Swift app that you can use to run Stable Diffusion effortlessly on your own hardware. We released [an app in the Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) as well as [the source code to allow other projects to use it](https://github.com/huggingface/swift-coreml-diffusers). The rest of this post guides you on how to use the converted weights in your own code or convert additional weights yourself. ## Available Checkpoints\nThe official Stable Diffusion checkpoints are already converted and ready for use:\n- Stable Diffusion v1.4: [converted](https://hf.co/apple/coreml-stable-diffusion-v1-4) [original](https://hf.co/CompVis/stable-diffusion-v1-4)\n- Stable Diffusion v1.5: [converted](https://hf.co/apple/coreml-stable-diffusion-v1-5) [original](https://hf.co/runwayml/stable-diffusion-v1-5)\n- Stable Diffusion v2 base: [converted](https://hf.co/apple/coreml-stable-diffusion-2-base) [original](https://huggingface.co/stabilityai/stable-diffusion-2-base)\n- Stable Diffusion v2.1 base: [converted](https://hf.co/apple/coreml-stable-diffusion-2-1-base) [original](https://huggingface.co/stabilityai/stable-diffusion-2-1-base)\nCore ML supports all the compute units available in your device: CPU, GPU and Apple's Neural Engine (NE).""]",The full code for the Stable Diffusion demo can be found in the [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_jax_how_to.ipynb). You can open it directly by clicking the link.,https://hf.co/spaces/stabilityai/stable-diffusion/tree/main
"What transformation does the FNet model use to replace the self-attention layer in a BERT model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# FNet\n## Overview\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:\n\\*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \""mix\"" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \""efficient\"" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.\\*\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f\\_net). ## Usage tips\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\nsequence length for fine-tuning and inference. ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## FNetConfig\n[[autodoc]] FNetConfig\n## FNetTokenizer\n[[autodoc]] FNetTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## FNetTokenizerFast\n[[autodoc]] FNetTokenizerFast\n## FNetModel\n[[autodoc]] FNetModel\n- forward\n## FNetForPreTraining\n[[autodoc]] FNetForPreTraining\n- forward\n## FNetForMaskedLM\n[[autodoc]] FNetForMaskedLM\n- forward\n## FNetForNextSentencePrediction\n[[autodoc]] FNetForNextSentencePrediction\n- forward\n## FNetForSequenceClassification\n[[autodoc]] FNetForSequenceClassification\n- forward\n## FNetForMultipleChoice\n[[autodoc]] FNetForMultipleChoice\n- forward\n## FNetForTokenClassification\n[[autodoc]] FNetForTokenClassification\n- forward\n## FNetForQuestionAnswering\n[[autodoc]] FNetForQuestionAnswering\n- forward"", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# DeBERTa\n## Overview\nThe DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's\nBERT model released in 2018 and Facebook's RoBERTa model released in 2019. It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in\nRoBERTa. The abstract from the paper is the following:\n\\*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\nlanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\ndisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\ndisentangled attention mechanism, where each word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed using disentangled matrices on their\ncontents and relative positions."", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# BERT\n\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\n\n## Overview\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It's a\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\n\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\narchitecture modifications.\\*\n\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\*\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf).""]","The FNet model replaces the self-attention layer in a BERT model with a Fourier transform, which returns only the real parts of the transform. This approach allows the model to be significantly faster and more memory efficient compared to the BERT model.",Fourier transform
"What type of test should typically accompany a bug fix in Gradio's testing strategy?
","[""Test Strategy\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to. This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Objectives\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n1. Ensure that the Gradio library functions as we expect it to. 2. Enable the maintenance team to quickly identify both the presence and source of defects. 3. Prevent regressions, i.e. if we fix something it should stay fixed. 4. Improve the quality of the codebase in order to ease maintenance efforts. 5. Reduce the amount of manual testing required. ## Scope\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas. We should not focus on code coverage but on test coverage following the below criteria:\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n## Types of testing\nOur tests will broadly fall into one of three categories:\n- Static Quality checks\n- Dynamic 'Code' tests\n- Dynamic Functional tests\n### Static Quality checks\nStatic quality checks are generally very fast to run and do not require building the code base."", ""Tests in this category could be browser-based end-to-end tests, accessibility tests, or performance tests. They are sometimes called acceptance tests. ## Testing tools\nWe currently use the following tools:\n### Static quality checks\n- Python type-checking (python)\n- Black linting (python)\n- ruff formatting (python)\n- prettier formatting (javascript/svelte)\n- TypeScript type-checking (javascript/svelte)\n- eslint linting (javascript/svelte) [in progress]\n### Dynamic code tests\n- pytest (python unit and integration tests)\n- vitest (node-based unit and integration tests)\n- playwright (browser-based unit and integration tests)\n### Functional/acceptance tests\n- playwright (full end to end testing)\n- chromatic (visual testing) [in progress]\n- Accessibility testing [to do]\n## Supported environments and versions\nAll operating systems refer to the current runner variants supported by GitHub actions. All unspecified version segments (`x`) refer to latest. | Software | Version(s) | Operating System(s) |\n| -------- | --------------------- | --------------------------------- |\n| Python | `3.8.x` | `ubuntu-latest`, `windows-latest` |\n| Node | `18.x.x` | `ubuntu-latest` |\n| Browser | `playwright-chrome-x` | `ubuntu-latest` |\n## Test execution\nTests need to be executed in a number of environments and at different stages of the development cycle in order to be useful. The requirements for tests are as follows:\n- \\*\\*Locally\\*\\*: it is important that developers can easily run most tests locally to ensure a passing suite before making a PR. There are some exceptions to this, certain tests may require access to secret values which we cannot make available to all possible contributors for practical security reasons. It is reasonable that it isn't possible to run these tests but they should be disabled by default when running locally. - \\*\\*CI\\*\\* - It is \\_critical\\_ that all tests run successfully in CI with no exceptions. Not every test is required to pass to satisfy CI checks for practical reasons but it is required that all tests should run in CI and notify us if something unexpected happens in order for the development team to take appropriate action. For instructions on how to write and run tests see the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Managing defects\nAs we formalise our testing strategy and bring / keep our test up to standard, it is important that we have some principles on managing defects as they occur/ are reported. For now we can have one very simple rule:\n- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should \\_typically\\_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule."", ""Thanks [@abidlabs](https://github.com/abidlabs)! - [#6079](https://github.com/gradio-app/gradio/pull/6079) [`3b2d9eaa3`](https://github.com/gradio-app/gradio/commit/3b2d9eaa3e84de3e4a0799e4585a94510d665f26) - fix cc build. Thanks [@pngwn](https://github.com/pngwn)! ### Fixes\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Pending events behavior. Thanks [@pngwn](https://github.com/pngwn)! - [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Reinstate types that were removed in error in #5832. Thanks [@pngwn](https://github.com/pngwn)! - [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Fixes: slider bar are too thin on FireFox. Thanks [@pngwn](https://github.com/pngwn)! - [#6146](https://github.com/gradio-app/gradio/pull/6146) [`40a171ea6`](https://github.com/gradio-app/gradio/commit/40a171ea60c74afa9519d6cb159def16ce68e1ca) - Fix image double change bug. Thanks [@pngwn](https://github.com/pngwn)! - [#6148](https://github.com/gradio-app/gradio/pull/6148) [`0000a1916`](https://github.com/gradio-app/gradio/commit/0000a191688c5480c977c80acdd0c9023865d57e) - fix dropdown arrow size. Thanks [@pngwn](https://github.com/pngwn)! - [#6067](https://github.com/gradio-app/gradio/pull/6067) [`bf38e5f06`](https://github.com/gradio-app/gradio/commit/bf38e5f06a7039be913614901c308794fea83ae0) - remove dupe component. Thanks [@pngwn](https://github.com/pngwn)! - [#6065](https://github.com/gradio-app/gradio/pull/6065) [`7d07001e8`](https://github.com/gradio-app/gradio/commit/7d07001e8e7ca9cbd2251632667b3a043de49f49) - fix storybook. Thanks [@pngwn](https://github.com/pngwn)! - [#5826](https://github.com/gradio-app/gradio/pull/5826) [`ce036c5d4`](https://github.com/gradio-app/gradio/commit/ce036c5d47e741e29812654bcc641ea6be876504) - Pending events behavior. Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)! - [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)! - [#6042](https://github.com/gradio-app/gradio/pull/6042) [`e27997fe6`](https://github.com/gradio-app/gradio/commit/e27997fe6c2bcfebc7015fc26100cee9625eb13a) - Fix `root` when user is unauthenticated so that login page appears correctly. Thanks [@abidlabs](https://github.com/abidlabs)! - [#6076](https://github.com/gradio-app/gradio/pull/6076) [`f3f98f923`](https://github.com/gradio-app/gradio/commit/f3f98f923c9db506284b8440e18a3ac7ddd8398b) - Lite error handler. Thanks [@whitphx](https://github.com/whitphx)! - [#5984](https://github.com/gradio-app/gradio/pull/5984) [`66549d8d2`](https://github.com/gradio-app/gradio/commit/66549d8d256b1845c8c5efa0384695b36cb46eab) - Fixes: slider bar are too thin on FireFox. Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)! ## 3.45.0-beta.13\n### Features\n- [#5964](https://github.com/gradio-app/gradio/pull/5964) [`5fbda0bd2`](https://github.com/gradio-app/gradio/commit/5fbda0bd2b2bbb2282249b8875d54acf87cd7e84) - Wasm release. Thanks [@pngwn](https://github.com/pngwn)! ## 3.45.0-beta.12\n### Features\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-app/gradio/commit/85ba6de136a45b3e92c74e410bb27e3cbe7138d7) - V4: Some misc fixes.""]","In Gradio's testing strategy, every bug fix should typically be accompanied by a dynamic code test that failed before the fix and passes afterwards. This ensures that the issue is properly addressed and helps prevent regressions in the future.",Dynamic code test
"How can you force mixed precision training when initializing the Accelerator in ðŸ¤— Accelerate?
","[""While gradient checkpointing may improve memory efficiency, it slows training by approximately 20%. ## Mixed precision training\n\\*\\*Mixed precision training\\*\\* is a technique that aims to optimize the computational efficiency of training models by\nutilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point\nprecision (fp32 or float32) to represent and process variables. However, not all variables require this high precision\nlevel to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like 16-bit\nfloating point (fp16 or float16), we can speed up the computations. Because in this approach some computations are performed\nin half-precision, while some are still in full precision, the approach is called mixed precision training. Most commonly mixed precision training is achieved by using fp16 (float16) data types, however, some GPU architectures\n(such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. Check\nout the [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/) to learn more about\nthe differences between these data types. ### fp16\nThe main advantage of mixed precision training comes from saving the activations in half precision (fp16). Although the gradients are also computed in half precision they are converted back to full precision for the optimization\nstep so no memory is saved here. While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes. This is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU). To enable mixed precision training, set the `fp16` flag to `True`:\n```py\ntraining\\_args = TrainingArguments(per\\_device\\_train\\_batch\\_size=4, fp16=True, \\*\\*default\\_args)\n```\nIf you prefer to use ðŸ¤— Accelerate, find the ðŸ¤— Accelerate example [further in this guide](#using-accelerate)."", ""### BF16\nIf you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While\nbf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have\nis `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which\nis about the same as fp32 - because both have 8-bits used for the numerical range. You can enable BF16 in the ðŸ¤— Trainer with:\n```python\ntraining\\_args = TrainingArguments(bf16=True, \\*\\*default\\_args)\n```\n### TF32\nThe Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead\nof 23 bits precision it has only 10 bits (same as fp16) and uses only 19 bits in total. It's \""magical\"" in the sense that\nyou can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput\nimprovement. All you need to do is to add the following to your code:\n```\nimport torch\ntorch.backends.cuda.matmul.allow\\_tf32 = True\ntorch.backends.cudnn.allow\\_tf32 = True\n```\nCUDA will automatically switch to using tf32 instead of fp32 where possible, assuming that the used GPU is from the Ampere series. According to [NVIDIA research](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/), the\nmajority of machine learning training workloads show the same perplexity and convergence with tf32 training as with fp32. If you're already using fp16 or bf16 mixed precision it may help with the throughput as well. You can enable this mode in the ðŸ¤— Trainer:\n```python\nTrainingArguments(tf32=True, \\*\\*default\\_args)\n```\ntf32 can't be accessed directly via `tensor.to(dtype=torch.tf32)` because it is an internal CUDA data type. You need `torch>=1.7` to use tf32 data types. For additional information on tf32 vs other precisions, please refer to the following benchmarks:\n[RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803) and\n[A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189). ## Flash Attention 2\nYou can speedup the training throughput by using Flash Attention 2 integration in transformers. Check out the appropriate section in the [single GPU section](./perf\\_infer\\_gpu\\_one#Flash-Attention-2) to learn more about how to load a model with Flash Attention 2 modules. ## Optimizer choice\nThe most common optimizer used to train transformer models is Adam or AdamW (Adam with weight decay)."", ""## Setup\nGet started by installing ðŸ¤— Accelerate:\n```bash\npip install accelerate\n```\nThen import and create an [`~accelerate.Accelerator`] object. The [`~accelerate.Accelerator`] will automatically detect your type of distributed setup and initialize all the necessary components for training. You don't need to explicitly place your model on a device. ```py\n>>> from accelerate import Accelerator\n>>> accelerator = Accelerator()\n```\n## Prepare to accelerate\nThe next step is to pass all the relevant training objects to the [`~accelerate.Accelerator.prepare`] method. This includes your training and evaluation DataLoaders, a model and an optimizer:\n```py\n>>> train\\_dataloader, eval\\_dataloader, model, optimizer = accelerator.prepare(\n... train\\_dataloader, eval\\_dataloader, model, optimizer\n... )\n```\n## Backward\nThe last addition is to replace the typical `loss.backward()` in your training loop with ðŸ¤— Accelerate's [`~accelerate.Accelerator.backward`]method:\n```py\n>>> for epoch in range(num\\_epochs):\n...""]","To force mixed precision training when initializing the `Accelerator` in ðŸ¤— Accelerate, you can set the `mixed_precision` argument to `""fp16""` or `""bf16""` when creating the `Accelerator` object. Hereâ€™s how you can do it:

```python
from accelerate import Accelerator

# For fp16 mixed precision
accelerator = Accelerator(mixed_precision=""fp16"")

# For bf16 mixed precision (if you have compatible hardware)
# accelerator = Accelerator(mixed_precision=""bf16"")
```

This will enable mixed precision training using the specified precision format.",By passing `fp16=True` to the Accelerator init.
"What is the purpose of tokenizers in the NLP pipeline?
","[""p align=\""center\"">\n\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\n\n![Build](https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg)\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\n[![Doc](https://docs.rs/tokenizers/badge.svg)](https://docs.rs/tokenizers/)\n\nThe core of `tokenizers`, written in Rust. Provides an implementation of today's most used tokenizers, with a focus on performance and\nversatility. ## What is a Tokenizer\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`. The various steps of the pipeline are:\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\nthe [unicode normalization standards](https://unicode.org/reports/tr15/#Norm\\_Forms), such as `NFD` or `NFKC`. More details about how to use the `Normalizers` are available on the\n[Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\nsplitting text is simply on whitespace. 3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\n`BPE` or `WordPiece`."", ""FrameworkSwitchCourse {fw} />\n# Tokenizers[[tokenizers]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. In NLP tasks, the data that is generally processed is raw text. Here's an example of such text:\n```\nJim Henson was a puppeteer\n```\nHowever, models can only process numbers, so we need to find a way to convert the raw text to numbers. That's what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation â€” that is, the one that makes the most sense to the model â€” and, if possible, the smallest representation. Let's take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization. ## Word-based[[word-based]]\nThe first type of tokenizer that comes to mind is \\_word-based\\_. It's generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:\n\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg)\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg)\n\nThere are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python's `split()` function:\n```py\ntokenized\\_text = \""Jim Henson was a puppeteer\"".split()\nprint(tokenized\\_text)\n```\n```python out\n['Jim', 'Henson', 'was', 'a', 'puppeteer']\n```\nThere are also variations of word tokenizers that have extra rules for punctuation."", ""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a PyTorch tensor.""]","The purpose of tokenizers in the NLP pipeline is to translate raw text into numerical data that can be processed by models. Since models can only handle numbers, tokenizers convert text inputs into a format that makes sense for the model. This involves several steps, including normalizing the text, splitting it into tokens (which can be words, parts of words, or punctuation), and mapping those tokens to unique IDs in the model's vocabulary. Essentially, tokenizers serve as a bridge between human-readable text and machine-readable data.",To translate text into data that can be processed by the model.
"What is the purpose of the Safety Checker in the Diffusers library?
","[""We will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback. ## Scope\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. ## Ethical guidelines\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - \\*\\*Transparency\\*\\*: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions. - \\*\\*Consistency\\*\\*: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - \\*\\*Simplicity\\*\\*: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent. - \\*\\*Accessibility\\*\\*: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community. - \\*\\*Reproducibility\\*\\*: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library. - \\*\\*Responsibility\\*\\*: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers. ## Examples of implementations: Safety features and Mechanisms\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n## Preamble\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training. Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library. The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups."", ""This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\nis very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline. ## Tweakable, contributor-friendly over abstraction\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t\\_repeat\\_yourself). In short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers. Functions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable. \\*\\*However\\*\\*, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions. - Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions. - Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel. At Hugging Face, we call this design the \\*\\*single-file policy\\*\\* which means that almost all of the code of a certain class should be written in a single, self-contained file.""]","The Safety Checker in the Diffusers library is designed to help mitigate potential ethical and social risks associated with the use of diffusion models. Its primary purpose is to filter out harmful or inappropriate content generated by the models, such as explicit or violent imagery. By implementing this safety feature, the Diffusers library aims to promote responsible usage and ensure that the technology is used in a manner that aligns with ethical guidelines and community standards. This contributes to a safer environment for users and helps prevent the misuse of generated content.",The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.
"What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?
","[""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Interact with Discussions and Pull Requests\nThe `huggingface\\_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood. ## Retrieve Discussions and Pull Requests from the Hub\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(repo\\_id=\""bigscience/bloom\""):\n... print(f\""{discussion.num} - {discussion.title}, pr: {discussion.is\\_pull\\_request}\"")\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer\\_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]\n```\n`HfApi.get\\_repo\\_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(\n..."", ""-->\n# Git ä¸Ž HTTP èŒƒå¼\n`huggingface\\_hub`åº“æ˜¯ç”¨äºŽä¸ŽHugging Face Hubè¿›è¡Œäº¤äº’çš„åº“ï¼ŒHugging Face Hubæ˜¯ä¸€ç»„åŸºäºŽGitçš„å­˜å‚¨åº“ï¼ˆæ¨¡åž‹ã€æ•°æ®é›†æˆ–Spacesï¼‰ã€‚ä½¿ç”¨ `huggingface\\_hub`æœ‰ä¸¤ç§ä¸»è¦æ–¹å¼æ¥è®¿é—®Hubã€‚\nç¬¬ä¸€ç§æ–¹æ³•ï¼Œå³æ‰€è°“çš„â€œåŸºäºŽgitâ€çš„æ–¹æ³•ï¼Œç”±[`Repository`]ç±»é©±åŠ¨ã€‚è¿™ç§æ–¹æ³•ä½¿ç”¨äº†ä¸€ä¸ªåŒ…è£…å™¨ï¼Œå®ƒåœ¨ `git`å‘½ä»¤çš„åŸºç¡€ä¸Šå¢žåŠ äº†ä¸“é—¨ä¸ŽHubäº¤äº’çš„é¢å¤–å‡½æ•°ã€‚ç¬¬äºŒç§é€‰æ‹©ï¼Œç§°ä¸ºâ€œåŸºäºŽHTTPâ€çš„æ–¹æ³•ï¼Œæ¶‰åŠä½¿ç”¨[`HfApi`]å®¢æˆ·ç«¯è¿›è¡ŒHTTPè¯·æ±‚ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€çœ‹æ¯ç§æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚\n## å­˜å‚¨åº“ï¼šåŸºäºŽåŽ†å²çš„ Git æ–¹æ³•\næœ€åˆï¼Œ`huggingface\\_hub`ä¸»è¦å›´ç»• [`Repository`] ç±»æž„å»ºã€‚å®ƒä¸ºå¸¸è§çš„ `git` å‘½ä»¤ï¼ˆå¦‚ `\""git add\""`ã€`\""git commit\""`ã€`\""git push\""`ã€`\""git tag\""`ã€`\""git checkout\""` ç­‰ï¼‰æä¾›äº† Python åŒ…è£…å™¨\nè¯¥åº“è¿˜å¯ä»¥å¸®åŠ©è®¾ç½®å‡­æ®å’Œè·Ÿè¸ªå¤§åž‹æ–‡ä»¶ï¼Œè¿™äº›æ–‡ä»¶é€šå¸¸åœ¨æœºå™¨å­¦ä¹ å­˜å‚¨åº“ä¸­ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥åº“å…è®¸æ‚¨åœ¨åŽå°æ‰§è¡Œå…¶æ–¹æ³•ï¼Œä½¿å…¶åœ¨è®­ç»ƒæœŸé—´ä¸Šä¼ æ•°æ®å¾ˆæœ‰ç”¨ã€‚\nä½¿ç”¨ [`Repository`] çš„æœ€å¤§ä¼˜ç‚¹æ˜¯å®ƒå…è®¸ä½ åœ¨æœ¬åœ°æœºå™¨ä¸Šç»´æŠ¤æ•´ä¸ªå­˜å‚¨åº“çš„æœ¬åœ°å‰¯æœ¬ã€‚è¿™ä¹Ÿå¯èƒ½æ˜¯ä¸€ä¸ªç¼ºç‚¹ï¼Œå› ä¸ºå®ƒéœ€è¦ä½ ä¸æ–­æ›´æ–°å’Œç»´æŠ¤è¿™ä¸ªæœ¬åœ°å‰¯æœ¬ã€‚è¿™ç±»ä¼¼äºŽä¼ ç»Ÿè½¯ä»¶å¼€å‘ä¸­ï¼Œæ¯ä¸ªå¼€å‘äººå‘˜éƒ½ç»´æŠ¤è‡ªå·±çš„æœ¬åœ°å‰¯æœ¬ï¼Œå¹¶åœ¨å¼€å‘åŠŸèƒ½æ—¶æŽ¨é€æ›´æ”¹ã€‚ä½†æ˜¯ï¼Œåœ¨æœºå™¨å­¦ä¹ çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œè¿™å¯èƒ½å¹¶ä¸æ€»æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºç”¨æˆ·å¯èƒ½åªéœ€è¦ä¸‹è½½æŽ¨ç†æ‰€éœ€çš„æƒé‡ï¼Œæˆ–å°†æƒé‡ä»Žä¸€ç§æ ¼å¼è½¬æ¢ä¸ºå¦ä¸€ç§æ ¼å¼ï¼Œè€Œæ— éœ€å…‹éš†æ•´ä¸ªå­˜å‚¨åº“ã€‚\n## HfApi: ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ä¸”æ–¹ä¾¿çš„HTTPå®¢æˆ·ç«¯\n`HfApi` è¢«å¼€å‘ä¸ºæœ¬åœ° git å­˜å‚¨åº“çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå› ä¸ºæœ¬åœ° git å­˜å‚¨åº“åœ¨å¤„ç†å¤§åž‹æ¨¡åž‹æˆ–æ•°æ®é›†æ—¶å¯èƒ½ä¼šå¾ˆéº»çƒ¦ã€‚`HfApi` æä¾›ä¸ŽåŸºäºŽ git çš„æ–¹æ³•ç›¸åŒçš„åŠŸèƒ½ï¼Œä¾‹å¦‚ä¸‹è½½å’ŒæŽ¨é€æ–‡ä»¶ä»¥åŠåˆ›å»ºåˆ†æ”¯å’Œæ ‡ç­¾ï¼Œä½†æ— éœ€æœ¬åœ°æ–‡ä»¶å¤¹æ¥ä¿æŒåŒæ­¥ã€‚\n`HfApi`é™¤äº†æä¾› `git` å·²ç»æä¾›çš„åŠŸèƒ½å¤–ï¼Œè¿˜æä¾›å…¶ä»–åŠŸèƒ½ï¼Œä¾‹å¦‚ï¼š\n\\* ç®¡ç†å­˜å‚¨åº“\n\\* ä½¿ç”¨ç¼“å­˜ä¸‹è½½æ–‡ä»¶ä»¥è¿›è¡Œæœ‰æ•ˆçš„é‡å¤ä½¿ç”¨\n\\* åœ¨ Hub ä¸­æœç´¢å­˜å‚¨åº“å’Œå…ƒæ•°æ®\n\\* è®¿é—®ç¤¾åŒºåŠŸèƒ½ï¼Œå¦‚è®¨è®ºã€PRå’Œè¯„è®º\n\\* é…ç½®Spaces\n## æˆ‘åº”è¯¥ä½¿ç”¨ä»€ä¹ˆï¼Ÿä»¥åŠä½•æ—¶ä½¿ç”¨ï¼Ÿ\næ€»çš„æ¥è¯´ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œ`HTTP æ–¹æ³•`æ˜¯ä½¿ç”¨ huggingface\\_hub çš„æŽ¨èæ–¹æ³•ã€‚ä½†æ˜¯ï¼Œåœ¨ä»¥ä¸‹å‡ ç§æƒ…å†µä¸‹ï¼Œç»´æŠ¤æœ¬åœ° git å…‹éš†ï¼ˆä½¿ç”¨ `Repository`ï¼‰å¯èƒ½æ›´æœ‰ç›Šï¼š\nå¦‚æžœæ‚¨åœ¨æœ¬åœ°æœºå™¨ä¸Šè®­ç»ƒæ¨¡åž‹ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„ git å·¥ä½œæµç¨‹å¹¶å®šæœŸæŽ¨é€æ›´æ–°å¯èƒ½æ›´æœ‰æ•ˆã€‚`Repository` è¢«ä¼˜åŒ–ä¸ºæ­¤ç±»æƒ…å†µï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿåœ¨åŽå°è¿è¡Œã€‚\nå¦‚æžœæ‚¨éœ€è¦æ‰‹åŠ¨ç¼–è¾‘å¤§åž‹æ–‡ä»¶ï¼Œ`git `æ˜¯æœ€ä½³é€‰æ‹©ï¼Œå› ä¸ºå®ƒåªä¼šå°†æ–‡ä»¶çš„å·®å¼‚å‘é€åˆ°æœåŠ¡å™¨ã€‚ä½¿ç”¨ `HfAPI` å®¢æˆ·ç«¯ï¼Œæ¯æ¬¡ç¼–è¾‘éƒ½ä¼šä¸Šä¼ æ•´ä¸ªæ–‡ä»¶ã€‚è¯·è®°ä½ï¼Œå¤§å¤šæ•°å¤§åž‹æ–‡ä»¶æ˜¯äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå› æ­¤æ— æ³•ä»Ž git å·®å¼‚ä¸­å—ç›Šã€‚\nå¹¶éžæ‰€æœ‰ git å‘½ä»¤éƒ½é€šè¿‡ [`HfApi`] æä¾›ã€‚æœ‰äº›å¯èƒ½æ°¸è¿œä¸ä¼šè¢«å®žçŽ°ï¼Œä½†æˆ‘ä»¬ä¸€ç›´åœ¨åŠªåŠ›æ”¹è¿›å¹¶ç¼©å°å·®è·ã€‚å¦‚æžœæ‚¨æ²¡æœ‰çœ‹åˆ°æ‚¨çš„ç”¨ä¾‹è¢«è¦†ç›–ã€‚\nè¯·åœ¨[Github](https://github.com/huggingface/huggingface\\_hub)æ‰“å¼€ä¸€ä¸ª issueï¼æˆ‘ä»¬æ¬¢è¿Žåé¦ˆï¼Œä»¥å¸®åŠ©æˆ‘ä»¬ä¸Žæˆ‘ä»¬çš„ç”¨æˆ·ä¸€èµ·æž„å»º ðŸ¤— ç”Ÿæ€ç³»ç»Ÿã€‚"", ""Pull requests and Discussions\nHub Pull requests and Discussions allow users to do community contributions to repositories. Pull requests and discussions work the same for all the repo types. At a high level, the aim is to build a simpler version of other git hosts' (like GitHub's) PRs and Issues:\n- no forks are involved: contributors push to a special `ref` branch directly on the source repo. - there's no hard distinction between discussions and PRs: they are essentially the same so they are displayed in the same lists. - they are streamlined for ML (i.e. models/datasets/spaces repos), not arbitrary repos. \\_Note, Pull Requests and discussions can be enabled or disabled from the [repository settings](./repositories-settings#disabling-discussions-pull-requests)\\_\n## List\nBy going to the community tab in any repository, you can see all Discussions and Pull requests. You can also filter to only see the ones that are open. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list-dark.png)\n\n## View\nThe Discussion page allows you to see the comments from different users. If it's a Pull Request, you can see all the changes by going to the Files changed tab. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-view.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-view-dark.png)\n\n## Editing a Discussion / Pull request title\nIf you opened a PR or discussion, are the author of the repository, or have write access to it, you can edit the discussion title by clicking on the pencil button. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-edit-title.PNG)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-edit-title-dark.PNG)\n\n## Pin a Discussion / Pull Request\nIf you have write access to a repository, you can pin discussions and Pull Requests. Pinned discussions appear at the top of all the discussions. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pin.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pin-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pinned.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pinned-dark.png)\n\n## Lock a Discussion / Pull Request\nIf you have write access to a repository, you can lock discussions or Pull Requests. Once a discussion is locked, previous comments are still visible and users won't be able to add new comments. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-lock.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-lock-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-locked.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-locked-dark.png)\n\n## Comment edition and moderation\nIf you wrote a comment or have write access to the repository, you can edit the content of the comment from the contextual menu in the top-right corner of the comment box. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-edit.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-edit-dark.png)\n\nOnce the comment has been edited, a new link will appear above the comment. This link shows the edit history. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-link.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-link-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-history.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-history-dark.png)\n\nYou can also hide a comment. Hiding a comment is irreversible, and nobody will be able to see its content nor edit it anymore. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-hidden.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-hidden-dark.png)\n\nRead also [moderation](./moderation) to see how to report an abusive comment. ## Can I use Markdown and LaTeX in my comments and discussions? Yes! You can use Markdown to add formatting to your comments. Additionally, you can utilize LaTeX for mathematical typesetting, your formulas will be rendered with [KaTeX](https://katex.org/) before being parsed in Markdown.""]",The `HfApi` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.,HfApi
"What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?
","[""--\ntitle: \""Introducing Skops\""\nthumbnail: /blog/assets/94\\_skops/introducing\\_skops.png\nauthors:\n- user: merve\n- user: adrin\n- user: BenjaminB\n---\n# Introducing Skops\n## Introducing Skops\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others. Let's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production. ```python\n# let's import the libraries first\nimport sklearn\nfrom sklearn.datasets import load\\_breast\\_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model\\_selection import train\\_test\\_split\n# Load the data and split\nX, y = load\\_breast\\_cancer(as\\_frame=True, return\\_X\\_y=True)\nX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(\nX, y, test\\_size=0.3, random\\_state=42\n)\n# Train the model\nmodel = DecisionTreeClassifier().fit(X\\_train, y\\_train)\n```\nYou can use any model filename and serialization method, like `pickle` or `joblib`.""]",The new library introduced by Hugging Face for hosting scikit-learn models is called Skops.,Skops
"What is the purpose of Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Textual Inversion\nTextual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder. [`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings. To learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading\\_adapters#textual-inversion) loading guide. ## TextualInversionLoaderMixin\n[[autodoc]] loaders.textual\\_inversion.TextualInversionLoaderMixin"", ""# Textual Inversion\n[Textual Inversion](https://hf.co/papers/2208.01618) is a training technique for personalizing image generation models with just a few example images of what you want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide. If you're training on a GPU with limited vRAM, you should try enabling the `gradient\\_checkpointing` and `mixed\\_precision` parameters in the training command. You can also reduce your memory footprint by using memory-efficient attention with [xFormers](../optimization/xformers). JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn't support gradient checkpointing or xFormers. With the same configuration and setup as PyTorch, the Flax training script should be at least ~70% faster! This guide will explore the [textual\\_inversion.py](https://github.com/huggingface/diffusers/blob/main/examples/textual\\_inversion/textual\\_inversion.py) script to help you become more familiar with it, and how you can adapt it for your own use-case. Before running the script, make sure you install the library from source:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install . ```\nNavigate to the example folder with the training script and install the required dependencies for the script you're using:\n\n```bash\ncd examples/textual\\_inversion\npip install -r requirements.txt\n```\n\n```bash\ncd examples/textual\\_inversion\npip install -r requirements\\_flax.txt\n```\n\nðŸ¤— Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the ðŸ¤— Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more. Initialize an ðŸ¤— Accelerate environment:\n```bash\naccelerate config\n```\nTo setup a default ðŸ¤— Accelerate environment without choosing any configurations:\n```bash\naccelerate config default\n```\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n```bash\nfrom accelerate.utils import write\\_basic\\_config\nwrite\\_basic\\_config()\n```\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create\\_dataset) guide to learn how to create a dataset that works with the training script.""]","Textual Inversion is a training technique designed to personalize image generation models, such as text2image models, by learning new text embeddings from just a few example images. The purpose of Textual Inversion is to allow users to adapt models to generate images that closely match specific concepts or styles represented by the provided examples. This is achieved by updating the text embeddings associated with a special token that must be used in prompts, enabling the model to generate images that reflect the characteristics of the examples given.",Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.
"What is the recommended multiple of batch size for fp16 data type on an A100 GPU?
","[""For instance, for fp16 data type a multiple of 8 is recommended, unless\nit's an A100 GPU, in which case use multiples of 64. For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup. ## Gradient Accumulation\nThe \\*\\*gradient accumulation\\*\\* method aims to calculate gradients in smaller increments instead of computing them for the\nentire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward\nand backward passes through the model and accumulating the gradients during the process. Once a sufficient number of\ngradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\nbecomes possible to increase the \\*\\*effective batch size\\*\\* beyond the limitations imposed by the GPU's memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can\nslow down the training process. You can enable gradient accumulation by adding the `gradient\\_accumulation\\_steps` argument to [`TrainingArguments`]:\n```py\ntraining\\_args = TrainingArguments(per\\_device\\_train\\_batch\\_size=1, gradient\\_accumulation\\_steps=4, \\*\\*default\\_args)\n```\nIn the above example, your effective batch size becomes 4."", ""It was run on a single NVIDIA A100-SXM4-80GB GPU with a prompt length of 512. The model we used was `meta-llama/Llama-2-13b-hf`. with batch size = 1:\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None |None|None |None |26.0 |36.958 |27.058 |29152.98 |\n|gptq |False |4 |128 |exllama|36.2 |33.711 |29.663 |10484.34 |\n|bitsandbytes|None |4|None |None |37.64 |52.00 |19.23 |11018.36 |\nwith batch size = 16:\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None |None|None |None |26.0 |69.94 |228.76 |53986.51 |\n|gptq |False |4 |128 |exllama|36.2 |95.41 |167.68 |34777.04 |\n|bitsandbytes|None |4|None |None |37.64 |113.98 |140.38 |35532.37 |\nFrom the benchmark, we can see that bitsandbyes and GPTQ are equivalent, with GPTQ being slightly faster for large batch size."", ""The impact of `fp16` on latency is less marked with `batch\\_size = 1`, but here it is of enormous interest as it can reduce latency by almost half, and almost double throughput! # Concluding remarks\nThis blog post showcased a few simple optimization tricks bundled in the ðŸ¤— ecosystem. Using anyone of these techniques, or a combination of all three, can greatly improve Bark inference speed and memory footprint. \\* You can use the large version of Bark without any performance degradation and a footprint of just 2GB instead of 5GB, 15% faster, \\*\\*using ðŸ¤— Better Transformer and CPU offload\\*\\*. \\* Do you prefer high throughput? \\*\\*Batch by 8 with ðŸ¤— Better Transformer and half-precision\\*\\*. \\* You can get the best of both worlds by using \\*\\*fp16, ðŸ¤— Better Transformer and CPU offload\\*\\*!""]","For fp16 data type on an A100 GPU, it is recommended to use multiples of 64 for the batch size.",64
"How do you run a Gradio Blocks app in reload mode using a Python IDE?
","[""Developing Faster with Auto-Reloading\n\\*\\*Prerequisite\\*\\*: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control). This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks. ## Why Auto-Reloading? When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes. To make it faster and more convenient to write your code, we've made it easier to \""reload\"" your Gradio apps instantly when you are developing in a \\*\\*Python IDE\\*\\* (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \""magic command\"" that allows you to re-run cells faster if you use \\*\\*Jupyter Notebooks\\*\\* (or any similar environment like Colab). This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster. ## Python IDE Reload ðŸ”¥\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Markdown(\""# Greetings from Gradio!\"")\ninp = gr.Textbox(placeholder=\""What is your name?\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""Welcome, {x}!\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`."", ""By default, the Gradio use UTF-8 encoding for scripts. \\*\\*For reload mode\\*\\*, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n1. Configure encoding declaration of python script, for example: `# -\\*- coding: cp1252 -\\*-`\n2. Confirm that your code editor has identified that encoding format. 3. Run like this: `gradio run.py --encoding cp1252`\nðŸ”¥ If your application accepts command line arguments, you can pass them in as well. Here's an example:\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# Greetings {args.name}!\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nWhich you could run like this: `gradio run.py --name Gretel`\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code."", ""WARNING: The --reload flag should not be used in production on Windows. ```\nè¿™é‡Œæœ€é‡è¦çš„ä¸€è¡Œæ˜¯ `æ­£åœ¨è§‚å¯Ÿ ...`ã€‚è¿™é‡Œå‘ç”Ÿçš„æƒ…å†µæ˜¯ Gradio å°†è§‚å¯Ÿ `run.py` æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ï¼Œå¦‚æžœæ–‡ä»¶å‘ç”Ÿæ›´æ”¹ï¼Œå®ƒå°†è‡ªåŠ¨ä¸ºæ‚¨é‡æ–°è¿è¡Œæ–‡ä»¶ã€‚å› æ­¤ï¼Œæ‚¨åªéœ€ä¸“æ³¨äºŽç¼–å†™ä»£ç ï¼ŒGradio æ¼”ç¤ºå°†è‡ªåŠ¨åˆ·æ–° ðŸ¥³\nâš ï¸ è­¦å‘Šï¼š`gradio` å‘½ä»¤ä¸ä¼šæ£€æµ‹ä¼ é€’ç»™ `launch()` æ–¹æ³•çš„å‚æ•°ï¼Œå› ä¸ºåœ¨é‡æ–°åŠ è½½æ¨¡å¼ä¸‹ä»Žæœªè°ƒç”¨ `launch()` æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œè®¾ç½® `launch()` ä¸­çš„ `auth` æˆ– `show\\_error` ä¸ä¼šåœ¨åº”ç”¨ç¨‹åºä¸­åæ˜ å‡ºæ¥ã€‚\nå½“æ‚¨ä½¿ç”¨é‡æ–°åŠ è½½æ¨¡å¼æ—¶ï¼Œè¯·è®°ä½ä¸€ä»¶é‡è¦çš„äº‹æƒ…ï¼šGradio ä¸“é—¨æŸ¥æ‰¾åä¸º `demo` çš„ Gradio Blocks/Interface æ¼”ç¤ºã€‚å¦‚æžœæ‚¨å°†æ¼”ç¤ºå‘½åä¸ºå…¶ä»–åç§°ï¼Œæ‚¨éœ€è¦åœ¨ä»£ç ä¸­çš„ç¬¬äºŒä¸ªå‚æ•°ä¸­ä¼ å…¥æ¼”ç¤ºçš„ FastAPI åº”ç”¨ç¨‹åºçš„åç§°ã€‚å¯¹äºŽ Gradio æ¼”ç¤ºï¼Œå¯ä»¥ä½¿ç”¨ `.app` å±žæ€§è®¿é—® FastAPI åº”ç”¨ç¨‹åºã€‚å› æ­¤ï¼Œå¦‚æžœæ‚¨çš„ `run.py` æ–‡ä»¶å¦‚ä¸‹æ‰€ç¤ºï¼š\n```python\nimport gradio as gr\nwith gr.Blocks() as my\\_demo:\ngr.Markdown(\""# æ¥è‡ªGradioçš„é—®å€™ï¼\"")\ninp = gr.Textbox(placeholder=\""æ‚¨å«ä»€ä¹ˆåå­—ï¼Ÿ\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""æ¬¢è¿Žï¼Œ{x}ï¼\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\nmy\\_demo.launch()\n```\né‚£ä¹ˆæ‚¨å¯ä»¥è¿™æ ·å¯åŠ¨å®ƒï¼š`gradio run.py my\\_demo.app`ã€‚\nGradioé»˜è®¤ä½¿ç”¨UTF-8ç¼–ç æ ¼å¼ã€‚å¯¹äºŽ\\*\\*é‡æ–°åŠ è½½æ¨¡å¼\\*\\*ï¼Œå¦‚æžœä½ çš„è„šæœ¬ä½¿ç”¨çš„æ˜¯é™¤UTF-8ä»¥å¤–çš„ç¼–ç ï¼ˆå¦‚GBKï¼‰ï¼š\n1. åœ¨Pythonè„šæœ¬çš„ç¼–ç å£°æ˜Žå¤„æŒ‡å®šä½ æƒ³è¦çš„ç¼–ç æ ¼å¼ï¼Œå¦‚ï¼š`# -\\*- coding: gbk -\\*-`\n2. ç¡®ä¿ä½ çš„ä»£ç ç¼–è¾‘å™¨è¯†åˆ«åˆ°è¯¥æ ¼å¼ã€‚\n3. æ‰§è¡Œï¼š`gradio run.py --encoding gbk`\nðŸ”¥ å¦‚æžœæ‚¨çš„åº”ç”¨ç¨‹åºæŽ¥å—å‘½ä»¤è¡Œå‚æ•°ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä¼ é€’å®ƒä»¬ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# æ¬¢è¿Ž {args.name}ï¼\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\næ‚¨å¯ä»¥åƒè¿™æ ·è¿è¡Œå®ƒï¼š`gradio run.py --name Gretel`\nä½œä¸ºä¸€ä¸ªå°æç¤ºï¼Œåªè¦æ›´æ”¹äº† `run.py` æºä»£ç æˆ– Gradio æºä»£ç ï¼Œè‡ªåŠ¨é‡æ–°åŠ è½½å°±ä¼šå‘ç”Ÿã€‚è¿™æ„å‘³ç€å¦‚æžœæ‚¨å†³å®š[ä¸º Gradio åšè´¡çŒ®](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md)ï¼Œè¿™å°†éžå¸¸æœ‰ç”¨ âœ…\n## Jupyter Notebook é­”æ³•å‘½ä»¤ðŸ”®\nå¦‚æžœæ‚¨ä½¿ç”¨ Jupyter Notebooksï¼ˆæˆ– Colab Notebooks ç­‰ï¼‰è¿›è¡Œå¼€å‘ï¼Œæˆ‘ä»¬ä¹Ÿä¸ºæ‚¨æä¾›äº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼\næˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ª \\*\\*magic command é­”æ³•å‘½ä»¤\\*\\*ï¼Œå¯ä»¥ä¸ºæ‚¨åˆ›å»ºå’Œè¿è¡Œä¸€ä¸ª Blocks æ¼”ç¤ºã€‚è¦ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œåœ¨ç¬”è®°æœ¬é¡¶éƒ¨åŠ è½½ gradio æ‰©å±•ï¼š\n`%load\\_ext gradio`\nç„¶åŽï¼Œåœ¨æ‚¨æ­£åœ¨å¼€å‘ Gradio æ¼”ç¤ºçš„å•å…ƒæ ¼ä¸­ï¼Œåªéœ€åœ¨é¡¶éƒ¨å†™å…¥é­”æ³•å‘½ä»¤\\*\\*`%%blocks`\\*\\*ï¼Œç„¶åŽåƒå¹³å¸¸ä¸€æ ·ç¼–å†™å¸ƒå±€å’Œç»„ä»¶ï¼š\n```py\n%%blocks\nimport gradio as gr\ngr.Markdown(\""# æ¥è‡ªGradioçš„é—®å€™ï¼\"")\ninp = gr.Textbox(placeholder=\""æ‚¨å«ä»€ä¹ˆåå­—ï¼Ÿ\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""æ¬¢è¿Žï¼Œ{x}ï¼\"",\ninputs=inp,\noutputs=out)\n```\nè¯·æ³¨æ„ï¼š\n- æ‚¨ä¸éœ€è¦æ”¾ç½®æ ·æ¿ä»£ç  `with gr.Blocks() as demo:` å’Œ `demo.launch()` â€” Gradio ä¼šè‡ªåŠ¨ä¸ºæ‚¨å®Œæˆï¼\n- æ¯æ¬¡é‡æ–°è¿è¡Œå•å…ƒæ ¼æ—¶ï¼ŒGradio éƒ½å°†åœ¨ç›¸åŒçš„ç«¯å£ä¸Šé‡æ–°å¯åŠ¨æ‚¨çš„åº”ç”¨ç¨‹åºï¼Œå¹¶ä½¿ç”¨ç›¸åŒçš„åº•å±‚ç½‘ç»œæœåŠ¡å™¨ã€‚è¿™æ„å‘³ç€æ‚¨å°†æ¯”æ­£å¸¸é‡æ–°è¿è¡Œå•å…ƒæ ¼æ›´å¿«åœ°çœ‹åˆ°å˜åŒ–ã€‚\nä¸‹é¢æ˜¯åœ¨ Jupyter Notebook ä¸­çš„ç¤ºä¾‹ï¼š\n![](https://i.ibb.co/nrszFws/Blocks.gif)\nðŸª„è¿™åœ¨ colab ç¬”è®°æœ¬ä¸­ä¹Ÿé€‚ç”¨ï¼[è¿™æ˜¯ä¸€ä¸ª colab ç¬”è®°æœ¬](https://colab.research.google.com/drive/1jUlX1w7JqckRHVE-nbDyMPyZ7fYD8488?authuser=1#scrollTo=zxHYjbCTTz\\_5)ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­çœ‹åˆ° Blocks é­”æ³•æ•ˆæžœã€‚å°è¯•è¿›è¡Œä¸€äº›æ›´æ”¹å¹¶é‡æ–°è¿è¡Œå¸¦æœ‰ Gradio ä»£ç çš„å•å…ƒæ ¼ï¼\nNotebook Magic çŽ°åœ¨æ˜¯ä½œè€…æž„å»º Gradio æ¼”ç¤ºçš„é¦–é€‰æ–¹å¼ã€‚æ— è®ºæ‚¨å¦‚ä½•ç¼–å†™ Python ä»£ç ï¼Œæˆ‘ä»¬éƒ½å¸Œæœ›è¿™ä¸¤ç§æ–¹æ³•éƒ½èƒ½ä¸ºæ‚¨æä¾›æ›´å¥½çš„ Gradio å¼€å‘ä½“éªŒã€‚\n---\n## ä¸‹ä¸€æ­¥\næ—¢ç„¶æ‚¨å·²ç»äº†è§£äº†å¦‚ä½•ä½¿ç”¨ Gradio å¿«é€Ÿå¼€å‘ï¼Œè¯·å¼€å§‹æž„å»ºè‡ªå·±çš„åº”ç”¨ç¨‹åºå§ï¼\nå¦‚æžœä½ æ­£åœ¨å¯»æ‰¾çµæ„Ÿï¼Œè¯·å°è¯•æµè§ˆå…¶ä»–äººç”¨ Gradio æž„å»ºçš„æ¼”ç¤ºï¼Œ[æµè§ˆ Hugging Face Spaces](http://hf.space/) ðŸ¤—""]","To run a Gradio Blocks app in reload mode using a Python IDE, follow these steps:

1. Create your Gradio app in a Python file (e.g., `run.py`). Hereâ€™s a simple example:

   ```python
   import gradio as gr
   with gr.Blocks() as demo:
       gr.Markdown(""# Greetings from Gradio!"")
       inp = gr.Textbox(placeholder=""What is your name?"")
       out = gr.Textbox()
       inp.change(fn=lambda x: f""Welcome, {x}!"", inputs=inp, outputs=out)
   if __name__ == ""__main__"":
       demo.launch()
   ```

2. By default, Gradio uses UTF-8 encoding for scripts. If you are using a different encoding (like cp1252), make sure to:
   - Configure the encoding declaration in your Python script, for example: `# -*- coding: cp1252 -*-`
   - Confirm that your code editor recognizes that encoding format.
   - Run your script with the command: `gradio run.py --encoding cp1252`

3. To enable auto-reloading, simply run your script using the command: `gradio run.py`. Gradio will watch for changes in your `run.py` file and automatically reload the app when changes are detected.

4. If your application accepts command line arguments, you can pass them as well. For example:

   ```python
   import gradio as gr
   import argparse
   parser = argparse.ArgumentParser()
   parser.add_argument(""--name"", type=str, default=""User"")
   args, unknown = parser.parse_known_args()
   with gr.Blocks() as demo:
       gr.Markdown(f""# Greetings {args.name}!"")
       inp = gr.Textbox()
       out = gr.Textbox()
       inp.change(fn=lambda x: x, inputs=inp, outputs=out)
   if __name__ == ""__main__"":
       demo.launch()
   ```

   You can run it like this: `gradio run.py --name Gretel`.

Remember, the `--reload` flag should not be used in production on Windows, and Gradio specifically looks for a Gradio Blocks/Interface demo named `demo`. If your demo is named differently, you need to pass the FastAPI app name in the command.",Run `gradio run.py` in the terminal.
"How can you install the Hugging Face Unity API in your Unity project?
","[""--\ntitle: \""How to Install and Use the Hugging Face Unity API\""\nthumbnail: /blog/assets/124\\_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n# How to Install and Use the Hugging Face Unity API\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API. ## Installation\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif)\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens). 7. Test the API key by clicking `Test API key` in the API Wizard. 8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API\\_URL` field."", ""9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png)\nNow that the API is set up, you can make calls from your scripts to the API. Let's look at an example of performing a Sentence Similarity task:\n```\nusing HuggingFace.API;\n/\\* other code \\*/\n// Make a call to the API\nvoid Query() {\nstring inputText = \""I'm on my way to the forest.\"";\nstring[] candidates = {\n\""The player is going to the city\"",\n\""The player is going to the wilderness\"",\n\""The player is wandering aimlessly\""\n};\nHuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\n}\n// If successful, handle the result\nvoid OnSuccess(float[] result) {\nforeach(float value in result) {\nDebug.Log(value);\n}\n}\n// Otherwise, handle the error\nvoid OnError(string error) {\nDebug.LogError(error);\n}\n/\\* other code \\*/\n```\n## Supported Tasks and Custom Models\nThe Hugging Face Unity API also currently supports the following tasks:\n- [Conversation](https://huggingface.co/tasks/conversational)\n- [Text Generation](https://huggingface.co/tasks/text-generation)\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\n- [Text Classification](https://huggingface.co/tasks/text-classification)\n- [Question Answering](https://huggingface.co/tasks/question-answering)\n- [Translation](https://huggingface.co/tasks/translation)\n- [Summarization](https://huggingface.co/tasks/summarization)\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks."", ""### 4. Speech Recognition\nNext, we'll want to use the Hugging Face Unity API to run speech recognition on our encoded audio. To do so, we'll create a `SendRecording()` method:\n```\nusing HuggingFace.API;\nprivate void SendRecording() {\nHuggingFaceAPI.AutomaticSpeechRecognition(bytes, response => {\ntext.color = Color.white;\ntext.text = response;\n}, error => {\ntext.color = Color.red;\ntext.text = error;\n});\n}\n```\nThis will send the encoded audio to the API, displaying the response in white if successful, otherwise the error message in red. Don't forget to call `SendRecording()` at the end of the `StopRecording()` method:\n```\nprivate void StopRecording() {\n/\\* other code \\*/\nSendRecording();\n}\n```\n### 5. Final Touches\nFinally, let's improve the UX of this demo a bit using button interactability and status messages. The Start and Stop buttons should only be interactable when appropriate, i.e. when a recording is ready to be started/stopped. Then, set the response text to a simple status message while recording or waiting for the API. The finished script should look something like this:\n```\nusing System.IO;\nusing HuggingFace.API;\nusing TMPro;\nusing UnityEngine;\nusing UnityEngine.UI;\npublic class SpeechRecognitionTest : MonoBehaviour {\n[SerializeField] private Button startButton;\n[SerializeField] private Button stopButton;\n[SerializeField] private TextMeshProUGUI text;\nprivate AudioClip clip;\nprivate byte[] bytes;\nprivate bool recording;\nprivate void Start() {\nstartButton.onClick.AddListener(StartRecording);\nstopButton.onClick.AddListener(StopRecording);\nstopButton.interactable = false;\n}\nprivate void Update() {\nif (recording && Microphone.GetPosition(null) >= clip.samples) {\nStopRecording();\n}\n}\nprivate void StartRecording() {\ntext.color = Color.white;\ntext.text = \""Recording...\"";\nstartButton.interactable = false;\nstopButton.interactable = true;\nclip = Microphone.Start(null, false, 10, 44100);\nrecording = true;\n}\nprivate void StopRecording() {\nvar position = Microphone.GetPosition(null);\nMicrophone.End(null);\nvar samples = new float[position \\* clip.channels];\nclip.GetData(samples, 0);\nbytes = EncodeAsWAV(samples, clip.frequency, clip.channels);\nrecording = false;\nSendRecording();\n}\nprivate void SendRecording() {\ntext.color = Color.yellow;\ntext.text = \""Sending...\"";\nstopButton.interactable = false;\nHuggingFaceAPI.AutomaticSpeechRecognition(bytes, response => {\ntext.color = Color.white;\ntext.text = response;\nstartButton.interactable = true;\n}, error => {\ntext.color = Color.red;\ntext.text = error;\nstartButton.interactable = true;\n});\n}\nprivate byte[] EncodeAsWAV(float[] samples, int frequency, int channels) {\nusing (var memoryStream = new MemoryStream(44 + samples.Length \\* 2)) {\nusing (var writer = new BinaryWriter(memoryStream)) {\nwriter.Write(\""RIFF\"".ToCharArray());\nwriter.Write(36 + samples.Length \\* 2);\nwriter.Write(\""WAVE\"".ToCharArray());\nwriter.Write(\""fmt \"".ToCharArray());\nwriter.Write(16);\nwriter.Write((ushort)1);\nwriter.Write((ushort)channels);\nwriter.Write(frequency);\nwriter.Write(frequency \\* channels \\* 2);\nwriter.Write((ushort)(channels \\* 2));\nwriter.Write((ushort)16);\nwriter.Write(\""data\"".ToCharArray());\nwriter.Write(samples.Length \\* 2);\nforeach (var sample in samples) {\nwriter.Write((short)(sample \\* short.MaxValue));\n}\n}\nreturn memoryStream.ToArray();\n}\n}\n}\n```\nCongratulations, you can now use state-of-the-art Speech Recognition in Unity! If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!""]","To install the Hugging Face Unity API in your Unity project, follow these steps:

1. Open your Unity project.
2. Go to `Window` -> `Package Manager`.
3. Click `+` and select `Add Package from git URL`.
4. Enter `https://github.com/huggingface/unity-api.git`.
5. Once installed, the Unity API wizard should pop up. If it doesn't, go to `Window` -> `Hugging Face API Wizard`.
6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).
7. Test the API key by clicking `Test API key` in the API Wizard.
8. Optionally, change the model endpoints to select which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the URL from the `API_URL` field.
9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`.
10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard.

After completing these steps, the Hugging Face Unity API will be set up in your project.","To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`."
"What is the pretraining objective of the Wav2Vec2 context network?
","[""Wav2Vec2 Contrastive Loss PreTraining examples\nThe following example showcases how to pretrain a wav2vec2 model using the JAX/Flax backend. Pretraining Wav2Vec2 is rather complex, so it is highly recommended to read the\n[official paper](https://arxiv.org/abs/2006.11477). JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU. Models written in JAX/Flax are \\*\\*immutable\\*\\* and updated in a purely functional\nway which enables simple and efficient model parallelism. `run\\_wav2vec2\\_pretrain\\_flax.py` is a lightweight example of how to download and preprocess a dataset from the ðŸ¤— Datasets library or use your own files (jsonlines or csv), then pretrain the wav2vec2 architectures above on it. For custom datasets in `jsonlines` format please see: [the Datasets documentation](https://huggingface.co/docs/datasets/loading\\_datasets#json-files) and you also will find examples of these below. Let's start by creating a model repository to save the trained model and logs. Here we call the model `\""wav2vec2-base-robust\""`, but you can change the model name as you like. You can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\nyou are logged in) or via the command line:\n```\nhuggingface-cli repo create wav2vec2-base-robust\n```\nNext we clone the model repository to add the tokenizer and model files. ```\ngit clone https://huggingface.co//wav2vec2-base-robust\n```\nTo ensure that all tensorboard traces will be uploaded correctly, we need to\ntrack them. You can run the following command inside your model repo to do so. ```\ncd wav2vec2-base-robust\ngit lfs track \""\\*tfevents\\*\""\n```\nGreat, we have set up our model repository. During training, we will automatically\npush the training logs and model weights to the repo. Next, let's add a symbolic link to the `run\\_wav2vec2\\_pretrain\\_flax`. ```bash\nexport MODEL\\_DIR=\""./wav2vec2-base-robust\""\nln -s ~/transformers/examples/research\\_projects/jax-projects/wav2vec2/run\\_wav2vec2\\_pretrain\\_flax.py ./\n```\n### Create the model configuration\nLet's first create the model configuration and store it in the model repository."", ""!---\nCopyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Speech Recognition Pre-Training\n## Wav2Vec2 Speech Pre-Training\nThe script [`run\\_speech\\_wav2vec2\\_pretraining\\_no\\_trainer.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run\\_wav2vec2\\_pretraining\\_no\\_trainer.py) can be used to pre-train a [Wav2Vec2](https://huggingface.co/transformers/model\\_doc/wav2vec2.html?highlight=wav2vec2) model from scratch. In the script [`run\\_speech\\_wav2vec2\\_pretraining\\_no\\_trainer`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run\\_wav2vec2\\_pretraining\\_no\\_trainer.py), a Wav2Vec2 model is pre-trained on audio data alone using [Wav2Vec2's contrastive loss objective](https://arxiv.org/abs/2006.11477). The following examples show how to fine-tune a `\""base\""`-sized Wav2Vec2 model as well as a `\""large\""`-sized Wav2Vec2 model using [`accelerate`](https://github.com/huggingface/accelerate). ---\n\\*\\*NOTE 1\\*\\*\nWav2Vec2's pre-training is known to be quite unstable. It is advised to do a couple of test runs with a smaller dataset,\n\\*i.e.\\* `--dataset\\_config\\_names clean clean`, `--dataset\\_split\\_names validation test`\nto find good hyper-parameters for `learning\\_rate`, `batch\\_size`, `num\\_warmup\\_steps`,\nand the optimizer. A good metric to observe during training is the gradient norm which should ideally be between 0.5 and 2. ---\n---\n\\*\\*NOTE 2\\*\\*\nWhen training a model on large datasets it is recommended to run the data preprocessing\nin a first run in a \\*\\*non-distributed\\*\\* mode via `--preprocessing\\_only` so that\nwhen running the model in \\*\\*distributed\\*\\* mode in a second step the preprocessed data\ncan easily be loaded on each distributed device. ---\n### Demo\nIn this demo run we pre-train a `\""base-sized\""` Wav2Vec2 model simply only on the validation\nand test data of [librispeech\\_asr](https://huggingface.co/datasets/librispeech\\_asr). The demo is run on two Titan RTX (24 GB RAM each). In case you have less RAM available\nper device, consider reducing `--batch\\_size` and/or the `--max\\_duration\\_in\\_seconds`. ```bash\naccelerate launch run\\_wav2vec2\\_pretraining\\_no\\_trainer.py \\\n--dataset\\_name=\""librispeech\\_asr\"" \\\n--dataset\\_config\\_names clean clean \\\n--dataset\\_split\\_names validation test \\\n--model\\_name\\_or\\_path=\""patrickvonplaten/wav2vec2-base-v2\"" \\\n--output\\_dir=\""./wav2vec2-pretrained-demo\"" \\\n--max\\_train\\_steps=\""20000\"" \\\n--num\\_warmup\\_steps=\""32000\"" \\\n--gradient\\_accumulation\\_steps=\""8\"" \\\n--learning\\_rate=\""0.005\"" \\\n--weight\\_decay=\""0.01\"" \\\n--max\\_duration\\_in\\_seconds=\""20.0\"" \\\n--min\\_duration\\_in\\_seconds=\""2.0\"" \\\n--logging\\_steps=\""1\"" \\\n--saving\\_steps=\""10000\"" \\\n--per\\_device\\_train\\_batch\\_size=\""8\"" \\\n--per\\_device\\_eval\\_batch\\_size=\""8\"" \\\n--adam\\_beta1=\""0.9\"" \\\n--adam\\_beta2=\""0.98\"" \\\n--adam\\_epsilon=\""1e-06\"" \\\n--gradient\\_checkpointing \\\n--mask\\_time\\_prob=\""0.65\"" \\\n--mask\\_time\\_length=\""10\""\n```\nThe results of this run can be seen [here](https://wandb.ai/patrickvonplaten/wav2vec2-pretrained-demo/reports/Wav2Vec2-PreTraining-Demo-Run--VmlldzoxMDk3MjAw?accessToken=oa05s1y57lizo2ocxy3k01g6db1u4pt8m6ur2n8nl4cb0ug02ms2cw313kb8ruch). ### Base\nTo pre-train `\""base-sized\""` Wav2Vec2 model, \\*e.g.\\* [facebook/wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base)\non [librispeech\\_asr](https://huggingface.co/datasets/librispeech\\_asr), the following command can be run:\n```bash\naccelerate launch run\\_wav2vec2\\_pretraining\\_no\\_trainer.py \\\n--dataset\\_name=librispeech\\_asr \\\n--dataset\\_config\\_names clean clean other \\\n--dataset\\_split\\_names train.100 train.360 train.500 \\\n--model\\_name\\_or\\_path=\""patrickvonplaten/wav2vec2-base-v2\"" \\\n--output\\_dir=\""./wav2vec2-pretrained-demo\"" \\\n--max\\_train\\_steps=\""200000\"" \\\n--num\\_warmup\\_steps=\""32000\"" \\\n--gradient\\_accumulation\\_steps=\""4\"" \\\n--learning\\_rate=\""0.001\"" \\\n--weight\\_decay=\""0.01\"" \\\n--max\\_duration\\_in\\_seconds=\""20.0\"" \\\n--min\\_duration\\_in\\_seconds=\""2.0\"" \\\n--logging\\_steps=\""1\"" \\\n--saving\\_steps=\""10000\"" \\\n--per\\_device\\_train\\_batch\\_size=\""8\"" \\\n--per\\_device\\_eval\\_batch\\_size=\""8\"" \\\n--adam\\_beta1=\""0.9\"" \\\n--adam\\_beta2=\""0.98\"" \\\n--adam\\_epsilon=\""1e-06\"" \\\n--gradient\\_checkpointing \\\n--mask\\_time\\_prob=\""0.65\"" \\\n--mask\\_time\\_length=\""10\""\n```\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 4 days. In case you have more than 8 GPUs available for a higher effective `batch\\_size`,\nit is recommended to increase the `learning\\_rate` to `0.005` for faster convergence. The results of this run can be seen [here](https://wandb.ai/patrickvonplaten/test/reports/Wav2Vec2-Base--VmlldzoxMTUyODQ0?accessToken=rg6e8u9yizx964k8q47zctq1m4afpvtn1i3qi9exgdmzip6xwkfzvagfajpzj55n) and the checkpoint pretrained for 85,000 steps can be accessed [here](https://huggingface.co/patrickvonplaten/wav2vec2-base-repro-960h-libri-85k-steps)\n### Large\nTo pre-train `\""large-sized\""` Wav2Vec2 model, \\*e.g.\\* [facebook/wav2vec2-large-lv60](https://huggingface.co/facebook/wav2vec2-large-lv60),\non [librispeech\\_asr](https://huggingface.co/datasets/librispeech\\_asr), the following command can be run:\n```bash\naccelerate launch run\\_wav2vec2\\_pretraining\\_no\\_trainer.py \\\n--dataset\\_name=librispeech\\_asr \\\n--dataset\\_config\\_names clean clean other \\\n--dataset\\_split\\_names train.100 train.360 train.500 \\\n--output\\_dir=./test \\\n--max\\_train\\_steps=200000 \\\n--num\\_warmup\\_steps=32000 \\\n--gradient\\_accumulation\\_steps=8 \\\n--learning\\_rate=0.001 \\\n--weight\\_decay=0.01 \\\n--max\\_duration\\_in\\_seconds=20.0 \\\n--min\\_duration\\_in\\_seconds=2.0 \\\n--model\\_name\\_or\\_path=./\n--logging\\_steps=1 \\\n--saving\\_steps=10000 \\\n--per\\_device\\_train\\_batch\\_size=2 \\\n--per\\_device\\_eval\\_batch\\_size=4 \\\n--adam\\_beta1=0.9 \\\n--adam\\_beta2=0.98 \\\n--adam\\_epsilon=1e-06 \\\n--gradient\\_checkpointing \\\n--mask\\_time\\_prob=0.65 \\\n--mask\\_time\\_length=10\n```\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 7 days. In case you have more than 8 GPUs available for a higher effective `batch\\_size`,\nit is recommended to increase the `learning\\_rate` to `0.005` for faster convergence."", ""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Wav2Vec2\n## Overview\nThe Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. The abstract from the paper is the following:\n\\*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\nrecognition with limited amounts of labeled data.\\*\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). ## Usage tips\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\nusing [`Wav2Vec2CTCTokenizer`]. ## Resources\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with Wav2Vec2.""]","The pretraining objective of the Wav2Vec2 context network is to solve a contrastive task defined over a quantization of the latent representations. This involves masking parts of the speech input in the latent space and training the model to predict the masked portions based on the unmasked context. This approach allows the model to learn powerful representations from speech audio alone, which can then be fine-tuned on transcribed speech for improved performance in speech recognition tasks.",The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.
"What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?
","[""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a TensorFlow tensor."", ""Transformers, what can they do?[[transformers-what-can-they-do]]\nIn this section, we will look at what Transformer models can do and use our first tool from the ðŸ¤— Transformers library: the `pipeline()` function. ðŸ‘€ See that *Open in Colab* button on the top right? Click on it to open a Google Colab notebook with all the code samples of this section. This button will be present in any section containing code examples. If you want to run the examples locally, we recommend taking a look at the [setup](/course/chapter0). ## Transformers are everywhere![[transformers-are-everywhere]]\nTransformer models are used to solve all kinds of NLP tasks, like the ones mentioned in the previous section. Here are some of the companies and organizations using Hugging Face and Transformer models, who also contribute back to the community by sharing their models:\n![Companies using Hugging Face](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG)\nThe [ðŸ¤— Transformers library](https://github.com/huggingface/transformers) provides the functionality to create and use those shared models. The [Model Hub](https://huggingface.co/models) contains thousands of pretrained models that anyone can download and use. You can also upload your own models to the Hub! âš ï¸ The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want! [Create a huggingface.co](https://huggingface.co/join) account to benefit from all available features! Before diving into how Transformer models work under the hood, let's look at a few examples of how they can be used to solve some interesting NLP problems. ## Working with pipelines[[working-with-pipelines]]\nThe most basic object in the ðŸ¤— Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\""sentiment-analysis\"")\nclassifier(\""I've been waiting for a HuggingFace course my whole life.\"")\n```\n```python out\n[{'label': 'POSITIVE', 'score': 0.9598047137260437}]\n```\nWe can even pass several sentences! ```python\nclassifier(\n[\""I've been waiting for a HuggingFace course my whole life.\"", \""I hate this so much!\""]\n)\n```\n```python out\n[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n{'label': 'NEGATIVE', 'score': 0.9994558095932007}]\n```\nBy default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English."", ""he pipeline function. The pipeline function is the most high-level API of the Transformers library. It regroups together all the steps to go from raw texts to usable predictions. The model used is at the core of a pipeline, but the pipeline also include all the necessary pre-processing (since the model does not expect texts, but numbers) as well as some post-processing to make the output of the model human-readable. Let's look at a first example with the sentiment analysis pipeline. This pipeline performs text classification on a given input, and determines if it's positive or negative. Here, it attributed the positive label on the given text, with a confidence of 95%. You can pass multiple texts to the same pipeline, which will be processed and passed through the model together, as a batch. The output is a list of individual results, in the same order as the input texts. Here we find the same label and score for the first text, and the second text is judged positive with a confidence of 99.99%. The zero-shot classification pipeline is a more general text-classification pipeline: it allows you to provide the labels you want.""]",The default checkpoint used by the sentiment analysis pipeline in the Transformers library is `distilbert-base-uncased-finetuned-sst2-english`.,distilbert base uncased finetuned sst2 english
"What is the purpose of the notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi""?
","[""Although these devices have quite a lot of memory, the model is so large that a single device is not enough to contain a single instance of BLOOMZ. To solve this issue, we are going to use [DeepSpeed](https://www.deepspeed.ai/), which is a deep learning optimization library that enables many memory and speed improvements to accelerate the model and make it fit the device. In particular, we rely here on [DeepSpeed-inference](https://arxiv.org/abs/2207.00032): it introduces several features such as [model (or pipeline) parallelism](https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism) to make the most of the available devices. For Gaudi2, we use [Habana's DeepSpeed fork](https://github.com/HabanaAI/deepspeed) that adds support for HPUs. ### Latency\nWe measured latencies (batch of one sample) for two different sizes of BLOOMZ, both with multi-billion parameters:\n- [176 billion](https://huggingface.co/bigscience/bloomz) parameters\n- [7 billion](https://huggingface.co/bigscience/bloomz-7b1) parameters\nRuns were performed with DeepSpeed-inference in 16-bit precision with 8 devices and using a [key-value cache](https://huggingface.co/docs/transformers/v4.27.1/en/model\\_doc/bloom#transformers.BloomForCausalLM.forward.use\\_cache)."", ""Note that while [CUDA graphs](https://developer.nvidia.com/blog/cuda-graphs/) are not currently compatible with model parallelism in DeepSpeed (DeepSpeed v0.8.2, see [here](https://github.com/microsoft/DeepSpeed/blob/v0.8.2/deepspeed/inference/engine.py#L158)), HPU graphs are supported in Habana's DeepSpeed fork. All benchmarks are doing [greedy generation](https://huggingface.co/blog/how-to-generate#greedy-search) of 100 token outputs. The input prompt is:\n> \""DeepSpeed is a machine learning framework\""\nwhich consists of 7 tokens with BLOOM's tokenizer. The results for inference latency are displayed in the table below (the unit is \\*seconds\\*). | Model | Number of devices | Gaudi2 latency (seconds) | A100-80GB latency (seconds) | First-gen Gaudi latency (seconds) |\n|:-----------:|:-----------------:|:-------------------------:|:-----------------:|:----------------------------------:|\n| BLOOMZ | 8 | 3.103 | 4.402 | / |\n| BLOOMZ-7B | 8 | 0.734 | 2.417 | 3.321 |\n| BLOOMZ-7B | 1 | 0.772 | 2.119 | 2.387 |\n\\*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, leading to a\\* x\\*1.42 speedup on BLOOMZ with Gaudi2 compared to A100.\\*\nThe Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. \\*\\*For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB\\*\\*. Smaller checkpoints present interesting results too. \\*\\*Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!\\*\\* It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device. We also ran these models on first-gen Gaudi. While it is slower than Gaudi2, it is interesting from a price perspective as a DL1 instance on AWS costs approximately 13\\$ per hour. Latency for BLOOMZ-7B on first-gen Gaudi is 2.387 seconds. Thus, \\*\\*first-gen Gaudi offers for the 7-billion checkpoint a better price-performance ratio than A100\\*\\* which costs more than 30\\$ per hour! We expect the Habana team will optimize the performance of these models in the upcoming SynapseAI releases. For example, in our last benchmark, we saw that [Gaudi2 performs Stable Diffusion inference 2.2x faster than A100](https://huggingface.co/blog/habana-gaudi-2-benchmark#generating-images-from-text-with-stable-diffusion) and this has since been improved further to 2.37x with the latest optimizations provided by Habana. We will update these numbers as new versions of SynapseAI are released and integrated within Optimum Habana."", ""--\ntitle: \""Accelerate Large Model Training using DeepSpeed\""\nthumbnail: /blog/assets/83\\_accelerate\\_deepspeed/deepspeed-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n# Accelerate Large Model Training using DeepSpeed\nIn this post we will look at how we can leverage the \\*\\*[Accelerate](https://github.com/huggingface/accelerate)\\*\\* library for training large models which enables users to leverage the ZeRO features of \\*\\*[DeeSpeed](https://www.deepspeed.ai)\\*\\*. # Motivation ðŸ¤—\n\\*\\*Tired of Out of Memory (OOM) errors while trying to train large models? We've got you covered. Large models are very performant [1] but difficult to train with the available hardware. To get the most of the available hardware for training large models one can leverage Data Parallelism using ZeRO - Zero Redundancy Optimizer [2]\\*\\*. Below is a short description of Data Parallelism using ZeRO with diagram from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)\n![ZeRO Data Parallelism](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero.png)\n(Source: [link](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/))\na.""]","The notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi"" is designed to provide users with a practical guide on leveraging the DeepSpeed library for training large models, specifically on Habana Gaudi hardware. It aims to demonstrate how to effectively utilize the features of DeepSpeed, such as model parallelism and memory optimization, to handle the challenges associated with training models that have billions of parameters. The notebook includes code examples and explanations to help users understand the process and optimize their training workflows on the Gaudi architecture.",To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.
"What command line module does PyTorch provide to run a script on multiple GPUs?
","[""## PyTorch Distributed\nPyTorch supports [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) which enables data parallelism. To start, create a Python file and import `torch.distributed` and `torch.multiprocessing` to set up the distributed process group and to spawn the processes for inference on each GPU. You should also initialize a [`DiffusionPipeline`]:\n```py\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom diffusers import DiffusionPipeline\nsd = DiffusionPipeline.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n)\n```\nYou'll want to create a function to run inference; [`init\\_process\\_group`](https://pytorch.org/docs/stable/distributed.html?highlight=init\\_process\\_group#torch.distributed.init\\_process\\_group) handles creating a distributed environment with the type of backend to use, the `rank` of the current process, and the `world\\_size` or the number of processes participating. If you're running inference in parallel over 2 GPUs, then the `world\\_size` is 2. Move the [`DiffusionPipeline`] to `rank` and use `get\\_rank` to assign a GPU to each process, where each process handles a different prompt:\n```py\ndef run\\_inference(rank, world\\_size):\ndist.init\\_process\\_group(\""nccl\"", rank=rank, world\\_size=world\\_size)\nsd.to(rank)\nif torch.distributed.get\\_rank() == 0:\nprompt = \""a dog\""\nelif torch.distributed.get\\_rank() == 1:\nprompt = \""a cat\""\nimage = sd(prompt).images[0]\nimage.save(f\""./{'\\_'.join(prompt)}.png\"")\n```\nTo run the distributed inference, call [`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn) to run the `run\\_inference` function on the number of GPUs defined in `world\\_size`:\n```py\ndef main():\nworld\\_size = 2\nmp.spawn(run\\_inference, args=(world\\_size,), nprocs=world\\_size, join=True)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\nmain()\n```\nOnce you've completed the inference script, use the `--nproc\\_per\\_node` argument to specify the number of GPUs to use and call `torchrun` to run the script:\n```bash\ntorchrun run\\_distributed.py --nproc\\_per\\_node=2\n```"", ""Again, remember to ensure to adjust `TORCH\\_CUDA\\_ARCH\\_LIST` to the target architectures. You can find the complete list of NVIDIA GPUs and their corresponding \\*\\*Compute Capabilities\\*\\* (same as arch in this\ncontext) [here](https://developer.nvidia.com/cuda-gpus). You can check the archs pytorch was built with using:\n```bash\npython -c \""import torch; print(torch.cuda.get\\_arch\\_list())\""\n```\nHere is how to find out the arch for one of the installed GPUs. For example, for GPU 0:\n```bash\nCUDA\\_VISIBLE\\_DEVICES=0 python -c \""import torch; \\\nprint(torch.cuda.get\\_device\\_properties(torch.device('cuda')))\""\n```\nIf the output is:\n```bash\n\\_CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, total\\_memory=24268MB, multi\\_processor\\_count=82)\n```\nthen you know that this card's arch is `8.6`. You can also leave `TORCH\\_CUDA\\_ARCH\\_LIST` out completely and then the build program will automatically query the\narchitecture of the GPUs the build is made on. This may or may not match the GPUs on the target machines, that's why\nit's best to specify the desired archs explicitly. If after trying everything suggested you still encounter build issues, please, proceed with the GitHub Issue of\n[Deepspeed](https://github.com/microsoft/DeepSpeed/issues),\n### Deployment with multiple GPUs\nTo deploy the DeepSpeed integration adjust the [`Trainer`] command line arguments to include a new argument `--deepspeed ds\\_config.json`, where `ds\\_config.json` is the DeepSpeed configuration file as\ndocumented [here](https://www.deepspeed.ai/docs/config-json/)."", ""The file naming is up to you. It's recommended to use DeepSpeed's `add\\_config\\_arguments` utility to add the necessary command line arguments to your code. For more information please see [DeepSpeed's Argument Parsing](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) doc. You can use a launcher of your choice here. You can continue using the pytorch launcher:\n```bash\ntorch.distributed.run --nproc\\_per\\_node=2 your\\_program.py  --deepspeed ds\\_config.json\n```\nor use the launcher provided by `deepspeed`:\n```bash\ndeepspeed --num\\_gpus=2 your\\_program.py  --deepspeed ds\\_config.json\n```\nAs you can see the arguments aren't the same, but for most needs either of them works. The\nfull details on how to configure various nodes and GPUs can be found [here](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node). When you use the `deepspeed` launcher and you want to use all available gpus you can just omit the `--num\\_gpus` flag. Here is an example of running `run\\_translation.py` under DeepSpeed deploying all available GPUs:\n```bash\ndeepspeed examples/pytorch/translation/run\\_translation.py \\\n--deepspeed tests/deepspeed/ds\\_config\\_zero3.json \\\n--model\\_name\\_or\\_path t5-small --per\\_device\\_train\\_batch\\_size 1 \\\n--output\\_dir output\\_dir --overwrite\\_output\\_dir --fp16 \\\n--do\\_train --max\\_train\\_samples 500 --num\\_train\\_epochs 1 \\\n--dataset\\_name wmt16 --dataset\\_config \""ro-en\"" \\\n--source\\_lang en --target\\_lang ro\n```\nNote that in the DeepSpeed documentation you are likely to see `--deepspeed --deepspeed\\_config ds\\_config.json` - i.e. two DeepSpeed-related arguments, but for the sake of simplicity, and since there are already so many arguments to deal\nwith, we combined the two into a single argument. For some practical usage examples, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400). ### Deployment with one GPU\nTo deploy DeepSpeed with one GPU adjust the [`Trainer`] command line arguments as follows:\n```bash\ndeepspeed --num\\_gpus=1 examples/pytorch/translation/run\\_translation.py \\\n--deepspeed tests/deepspeed/ds\\_config\\_zero2.json \\\n--model\\_name\\_or\\_path t5-small --per\\_device\\_train\\_batch\\_size 1 \\\n--output\\_dir output\\_dir --overwrite\\_output\\_dir --fp16 \\\n--do\\_train --max\\_train\\_samples 500 --num\\_train\\_epochs 1 \\\n--dataset\\_name wmt16 --dataset\\_config \""ro-en\"" \\\n--source\\_lang en --target\\_lang ro\n```\nThis is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly to use just one GPU via\n`--num\\_gpus=1`. By default, DeepSpeed deploys all GPUs it can see on the given node. If you have only 1 GPU to start\nwith, then you don't need this argument. The following [documentation](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) discusses the launcher options.""]","PyTorch provides the `torch.distributed.run` command line module to run a script on multiple GPUs. You can use it with the `--nproc_per_node` argument to specify the number of GPUs to use. For example:

```bash
torch.distributed.run --nproc_per_node=2 your_program.py
``` 

This allows you to execute your script in a distributed manner across the specified number of GPUs.",torchrun
"What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Vision Transformer (ViT)\n## Overview\nThe Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\nat Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, Neil Houlsby."", ""It's the first paper that successfully trains a Transformer encoder on ImageNet, attaining\nvery good results compared to familiar convolutional architectures. The abstract from the paper is the following:\n\\*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its\napplications to computer vision remain limited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional networks while keeping their overall\nstructure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),\nVision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring\nsubstantially fewer computational resources to train.\\*\n![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg)\n ViT architecture. Taken from the [original paper.](https://arxiv.org/abs/2010.11929) \nFollowing the original Vision Transformer, some follow-up works have been made:\n- [DeiT](deit) (Data-efficient Image Transformers) by Facebook AI. DeiT models are distilled vision transformers. The authors of DeiT also released more efficiently trained ViT models, which you can directly plug into [`ViTModel`] or\n[`ViTForImageClassification`]. There are 4 variants available (in 3 different sizes): \\*facebook/deit-tiny-patch16-224\\*,\n\\*facebook/deit-small-patch16-224\\*, \\*facebook/deit-base-patch16-224\\* and \\*facebook/deit-base-patch16-384\\*."", ""With so many Transformer variants available, it can be easy to miss the bigger picture. What all these models have in common is they're based on the original Transformer architecture. Some models only use the encoder or decoder, while others use both. This provides a useful taxonomy to categorize and examine the high-level differences within models in the Transformer family, and it'll help you understand Transformers you haven't encountered before. If you aren't familiar with the original Transformer model or need a refresher, check out the [How do Transformers work](https://huggingface.co/course/chapter1/4?fw=pt) chapter from the Hugging Face course.## Computer vision\n### Convolutional network\nFor a long time, convolutional networks (CNNs) were the dominant paradigm for computer vision tasks until the [Vision Transformer](https://arxiv.org/abs/2010.11929) demonstrated its scalability and efficiency. Even then, some of a CNN's best qualities, like translation invariance, are so powerful (especially for certain tasks) that some Transformers incorporate convolutions in their architecture. [ConvNeXt](model\\_doc/convnext) flipped this exchange around and incorporated design choices from Transformers to modernize a CNN. For example, ConvNeXt uses non-overlapping sliding windows to patchify an image and a larger kernel to increase its global receptive field. ConvNeXt also makes several layer design choices to be more memory-efficient and improve performance, so it competes favorably with Transformers! ### Encoder[[cv-encoder]]\nThe [Vision Transformer (ViT)](model\\_doc/vit) opened the door to computer vision tasks without convolutions. ViT uses a standard Transformer encoder, but its main breakthrough was how it treated an image. It splits an image into fixed-size patches and uses them to create an embedding, just like how a sentence is split into tokens. ViT capitalized on the Transformers' efficient architecture to demonstrate competitive results with the CNNs at the time while requiring fewer resources to train. ViT was soon followed by other vision models that could also handle dense vision tasks like segmentation as well as detection. One of these models is the [Swin](model\\_doc/swin) Transformer. It builds hierarchical feature maps (like a CNN ðŸ‘€ and unlike ViT) from smaller-sized patches and merges them with neighboring patches in deeper layers. Attention is only computed within a local window, and the window is shifted between attention layers to create connections to help the model learn better. Since the Swin Transformer can produce hierarchical feature maps, it is a good candidate for dense prediction tasks like segmentation and detection. The [SegFormer](model\\_doc/segformer) also uses a Transformer encoder to build hierarchical feature maps, but it adds a simple multilayer perceptron (MLP) decoder on top to combine all the feature maps and make a prediction. Other vision models, like BeIT and ViTMAE, drew inspiration from BERT's pretraining objective. [BeIT](model\\_doc/beit) is pretrained by \\*masked image modeling (MIM)\\*; the image patches are randomly masked, and the image is also tokenized into visual tokens. BeIT is trained to predict the visual tokens corresponding to the masked patches. [ViTMAE](model\\_doc/vitmae) has a similar pretraining objective, except it must predict the pixels instead of visual tokens.""]","The most popular Vision Transformer model for image classification on the Hugging Face Model Hub is typically the `ViT` (Vision Transformer) model itself, along with its variants like `DeiT` (Data-efficient Image Transformers). These models have been widely used and have shown excellent performance on various image classification tasks. You can find them on the Hugging Face Model Hub by searching for ""ViT"" or ""DeiT"".",google/vit-base-patch16-224
"What is the command to upload an ESPnet model to a Hugging Face repository?
","[""Using ESPnet at Hugging Face\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks. ## Exploring ESPnet in the Hub\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). All models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more. 2. Metadata tags that help for discoverability and contain information such as license, language and datasets. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png)\n\n## Using existing models\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\_model\\_zoo)). If you're interested in doing inference, different classes for different tasks have a `from\\_pretrained` method that allows loading models from the Hub."", ""n this video, we're going to understand how to manage a model repository on the HuggingFace model hub. In order to handle a repository, you should first have a Hugging Face account. A link to create a new account is available in the description. Once you are logged in, you can create a new repository by clicking on the \""New model\"" option. You should be facing a similar modal to the following. In the \""Owner\"" input, you can put either your own namespace or any of your organisations namespaces. The model name is the model identifier that will then be used to identify your model on your chosen namespace. The final choice is between public and private. Public models are accessible by anyone. This is the recommended, free option, as this makes your model easily accessible and shareable. The owners of your namespace are the only ones who can update and change your model. A more advanced option is the private option. In this case, only the owners of your namespace will have visibility over your model. Other users won't know it exists and will not be able to use it. Let's create a dummy model to play with. Once your model is created, comes the management of that model! Three tabs are available to you. You're facing the first one, which is the model card page; this is the page used to showcase your model to the world."", ""FrameworkSwitchCourse {fw} />\n# Sharing pretrained models[[sharing-pretrained-models]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nIn the steps below, we'll take a look at the easiest ways to share pretrained models to the ðŸ¤— Hub. There are tools and utilities available that make it simple to share and update models directly on the Hub, which we will explore below. We encourage all users that train models to contribute by sharing them with the community â€” sharing models, even when trained on very specific datasets, will help others, saving them time and compute resources and providing access to useful trained artifacts. In turn, you can benefit from the work that others have done! There are three ways to go about creating new model repositories:\n- Using the `push\\_to\\_hub` API\n- Using the `huggingface\\_hub` Python library\n- Using the web interface\nOnce you've created a repository, you can upload files to it via git and git-lfs. We'll walk you through creating model repositories and uploading files to them in the following sections. ## Using the `push\\_to\\_hub` API[[using-the-pushtohub-api]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nThe simplest way to upload files to the Hub is by leveraging the `push\\_to\\_hub` API.""]","To upload an ESPnet model to a Hugging Face repository, you can use the `push_to_hub` API. This command allows you to easily upload your model files to the Hub. For detailed instructions on how to use this API, you can refer to the official documentation on sharing pretrained models.",./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo
"What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?
","[""Add custom Dependencies\nInference Endpointsâ€™ base image includes all required libraries to run inference on ðŸ¤— Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\\* [customize your inference pipeline](/docs/inference-endpoints/guides/custom\\_handler) and need additional Python dependencies\n\\* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)). To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within. ```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\\* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n\\* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom\\_container) for inference."", ""Inference Endpoints Version\nHugging Face Inference Endpoints comes with a default serving container which is used for all [supported Transformers and Sentence-Transformers tasks](/docs/inference-endpoints/supported\\_tasks) and for [custom inference handler](/docs/inference-endpoints/guides/custom\\_handler) and implement batching. Below you will find information about the installed packages and versions used. You can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository. Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom\\_dependencies). ## Installed packages & version\nThe installed packages are split into `general`, `CPU` & `GPU` packages. The `general` packages are installed in all containers, the `CPU` and `GPU` packages are only installed in the corresponding containers. The Hugging Face Inference Runtime has separate versions for `PyTorch` and `TensorFlow` for `CPU` and `GPU`, which are used based on the selected `framework` when creating an Inference Endpoint. The `TensorFlow` and `PyTorch` flavors are grouped together in the list below. ### General\n- `Python`: `3.9.13`\n### CPU\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\n- `diffusers`: `0.14.0`\n- `accelerate`: `0.17.1`\n- `sentence\\_transformers`: `latest`\n- `pandas`: `latest`\n- `pytorch`: `1.13.1`\n- `torchvision`: `0.14.1`\n- `tensorflow`: `2.9.1`\n### GPU\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\n- `diffusers`: `0.14.0`\n- `accelerate`: `0.17.1`\n- `sentence\\_transformers`: `latest`\n- `pandas`: `latest`\n- `pytorch`: `1.13.1=py3.9\\_cuda11.8\\*`\n- `torchvision`: `0.14.1`\n- `tensorflow`: `2.9.1=\\*cuda112\\*py39\\*`"", ""It contains information about a deployed `InferenceEndpoint`, including its configuration and current state. Once deployed, you can run inference on the Endpoint using the [`InferenceEndpoint.client`] and [`InferenceEndpoint.async\\_client`] properties that respectively return an [`InferenceClient`] and an [`AsyncInferenceClient`] object. [[autodoc]] InferenceEndpoint\n- from\\_raw\n- client\n- async\\_client\n- all\n## InferenceEndpointStatus\n[[autodoc]] InferenceEndpointStatus\n## InferenceEndpointType\n[[autodoc]] InferenceEndpointType\n## InferenceEndpointError\n[[autodoc]] InferenceEndpointError""]","To install custom Python dependencies for Inference Endpoints, you should add a `requirements.txt` file to your model repository on the Hugging Face Hub. This file should list the Python dependencies you want to install.",requirements.txt
"How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""Distillation for quantization on Textual Inversion models to personalize text2image\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images.\\_By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images\\_\nThe `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. We have enabled distillation for quantization in `textual\\_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method. ## Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n```bash\npip install -r requirements.txt\n```\n## Prepare Datasets\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n[![](https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg)](https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg)\n## Get a FP32 Textual Inversion model\nUse the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model. ```bash\nexport MODEL\\_NAME=\""CompVis/stable-diffusion-v1-4\""\nexport DATA\\_DIR=\""./dicoo\""\naccelerate launch textual\\_inversion.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--train\\_data\\_dir=$DATA\\_DIR \\\n--learnable\\_property=\""object\"" \\\n--placeholder\\_token=\""\"" --initializer\\_token=\""toy\"" \\\n--resolution=512 \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=4 \\\n--max\\_train\\_steps=3000 \\\n--learning\\_rate=5.0e-04 --scale\\_lr \\\n--lr\\_scheduler=\""constant\"" \\\n--lr\\_warmup\\_steps=0 \\\n--output\\_dir=\""dicoo\\_model\""\n```\n## Do distillation for quantization\nDistillation for quantization is a method that combines [intermediate layer knowledge distillation](https://github.com/intel/neural-compressor/blob/master/docs/source/distillation.md#intermediate-layer-knowledge-distillation) and [quantization aware training](https://github.com/intel/neural-compressor/blob/master/docs/source/quantization.md#quantization-aware-training) in the same training process to improve the performance of the quantized model."", ""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Training with Intel Extension for PyTorch\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \""textual\\_inversion.py\"". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual\\_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script. The example supports both single node and multi-node distributed training:\n### Single node training\n```bash\nexport MODEL\\_NAME=\""CompVis/stable-diffusion-v1-4\""\nexport DATA\\_DIR=\""path-to-dir-containing-dicoo-images\""\npython textual\\_inversion.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--train\\_data\\_dir=$DATA\\_DIR \\\n--learnable\\_property=\""object\"" \\\n--placeholder\\_token=\""\"" --initializer\\_token=\""toy\"" \\\n--seed=7 \\\n--resolution=512 \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=1 \\\n--max\\_train\\_steps=3000 \\\n--learning\\_rate=2.5e-03 --scale\\_lr \\\n--output\\_dir=\""textual\\_inversion\\_dicoo\""\n```\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. You may not get performance speedup without Bfloat16 support. ### Multi-node distributed training\nBefore running the scripts, make sure to install the library's training dependencies successfully:\n```bash\npython -m pip install oneccl\\_bind\\_pt==1.13 -f https://developer.intel.com/ipex-whl-stable-cpu\n```\n```bash\nexport MODEL\\_NAME=\""CompVis/stable-diffusion-v1-4\""\nexport DATA\\_DIR=\""path-to-dir-containing-dicoo-images\""\noneccl\\_bindings\\_for\\_pytorch\\_path=$(python -c \""from oneccl\\_bindings\\_for\\_pytorch import cwd; print(cwd)\"")\nsource $oneccl\\_bindings\\_for\\_pytorch\\_path/env/setvars.sh\npython -m intel\\_extension\\_for\\_pytorch.cpu.launch --distributed \\\n--hostfile hostfile --nnodes 2 --nproc\\_per\\_node 2 textual\\_inversion.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--train\\_data\\_dir=$DATA\\_DIR \\\n--learnable\\_property=\""object\"" \\\n--placeholder\\_token=\""\"" --initializer\\_token=\""toy\"" \\\n--seed=7 \\\n--resolution=512 \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=1 \\\n--max\\_train\\_steps=750 \\\n--learning\\_rate=2.5e-03 --scale\\_lr \\\n--output\\_dir=\""textual\\_inversion\\_dicoo\""\n```\nThe above is a simple distributed training usage on 2 nodes with 2 processes on each node.""]","To teach new concepts to Stable Diffusion using Textual Inversion, you need just 3-5 images.",3-5 images
"What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?
","[""In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future. ## Sharded checkpoints\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save\\_pretrained(save\\_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in. You can control the maximum size before sharding with the `max\\_shard\\_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model. ```py\nfrom transformers import AutoModel\nmodel = AutoModel.from\\_pretrained(\""bert-base-cased\"")\n```\nIf you save it using [`~PreTrainedModel.save\\_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n```py\n>>> import os\n>>> import tempfile\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir)\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model.bin']\n```\nNow let's use a maximum shard size of 200MB:\n```py\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model-00001-of-00003.bin', 'pytorch\\_model-00002-of-00003.bin', 'pytorch\\_model-00003-of-00003.bin', 'pytorch\\_model.bin.index.json']\n```\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index.""]","In Transformers version 4.18.0, the maximum size of a model checkpoint before it is automatically sharded is 10GB.",10GB
"What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?
","[""As in the examples above, some common steps that may help decide whether and how to apply ML in a way that minimizes bias-related risk include:\n\\* Investigate:\n\\* Reports of bias in the field pre-ML\n\\* At-risk demographic categories for your specific use case\n\\* Examine:\n\\* The impact of your optimization objective on reinforcing biases\n\\* Alternative objectives that favor diversity and positive long-term impacts\n### I am curating/picking a dataset for my ML system, how can I address bias? While training datasets are [not the sole source of bias](https://www.cell.com/patterns/fulltext/S2666-3899(21)00061-1) in the ML development cycle, they do play a significant role. Does your [dataset disproportionately associate](https://aclanthology.org/2020.emnlp-main.23/) biographies of women with life events but those of men with achievements? Those \\*\\*stereotypes\\*\\* are probably going to show up in your full ML system! Does your voice recognition dataset only feature specific accents? Not a good sign for [the inclusivity of technology](https://www.scientificamerican.com/article/speech-recognition-tech-is-yet-another-example-of-bias/) you build with it in terms of \\*\\*disparate performance\\*\\*! Whether youâ€™re curating a dataset for ML applications or selecting a dataset to train an ML model, finding out, mitigating, and [communicating](https://dl.acm.org/doi/10.1145/3479582) to what extent the data exhibits these phenomena are all necessary steps to reducing bias-related risks. You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it comes from, who are the people represented on the data, and what the curation process was. Several frameworks for this reflection and documentation have been proposed such as [Data Statements for NLP](https://direct.mit.edu/tacl/article/doi/10.1162/tacl\\_a\\_00041/43452/Data-Statements-for-Natural-Language-Processing) or [Datasheets for Datasets](https://dl.acm.org/doi/10.1145/3458723). The Hugging Face Hub includes a Dataset Card [template](https://github.com/huggingface/datasets/blob/main/templates/README.md) and [guide](https://github.com/huggingface/datasets/blob/main/templates/README\\_guide.md#dataset-card-creation-guide) inspired by these works; the section on [considerations for using the data](https://github.com/huggingface/datasets/blob/main/templates/README\\_guide.md#considerations-for-using-the-data) is usually a good place to look for information about notable biases if youâ€™re browsing datasets, or to write a paragraph sharing your insights on the topic if youâ€™re sharing a new one. And if youâ€™re looking for more inspiration on what to put there, check out these sections written by Hub users in the [BigLAM organization](https://huggingface.co/biglam) for historical datasets of [legal proceedings](https://huggingface.co/datasets/biglam/old\\_bailey\\_proceedings#social-impact-of-dataset), [image classification](https://huggingface.co/datasets/biglam/brill\\_iconclass#social-impact-of-dataset), and [newspapers](https://huggingface.co/datasets/biglam/bnl\\_newspapers1841-1879#social-impact-of-dataset). ![HF Dataset Card guide for the Social Impact and Bias Sections](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img3.png)\n*[HF Dataset Card guide](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md#social-impact-of-dataset) for the Social Impact and Bias Sections*\n\nWhile describing the origin and context of a dataset is always a good starting point to understand the biases at play, [quantitatively measuring phenomena](https://arxiv.org/abs/2212.05129) that encode those biases can be just as helpful."", ""Whether and to what extent bias in the system concretely affects people ultimately depends on what the system is used for. As such, the first place developers can work to mitigate bias is when deciding how ML fits in their system, e.g., by deciding what optimization objective it will use. For example, letâ€™s go back to one of the first highly-publicized cases of a Machine Learning system used in production for algorithmic content recommendation. From 2006 to 2009, Netflix ran the [Netflix Prize](https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/The-Netflix-Prize-Bennett.pdf), a competition with a 1M$ cash prize challenging teams around the world to develop ML systems to accurately predict a userâ€™s rating for a new movie based on their past ratings. The [winning submission](https://www.asc.ohio-state.edu/statistics/dmsl/GrandPrize2009\\_BPC\\_BigChaos.pdf) improved the RMSE (Root-mean-square-error) of predictions on unseen user-movie pairs by over 10% over Netflixâ€™s own CineMatch algorithm, meaning it got much better at predicting how users would rate a new movie based on their history."", ""Conceptualizing bias as a risk factor then allows us to better understand the \\*\\*shared responsibility\\*\\* for machine biases between developers at all stages. Bias can never be fully removed, not least because the definitions of social biases and the power dynamics that tie them to discrimination vary vastly across social contexts. However:\n1. Each stage of the development process, from task specification, dataset curation, and model training, to model integration and system deployment, can take steps to minimize the aspects of machine bias\\*\\* that most directly depend on its choices\\*\\* and technical decisions, and\n2. Clear communication and \\*\\*information flow between the various ML development stages\\*\\* can make the difference between making choices that build on top of each other to attenuate the negative potential of bias (multipronged approach to bias mitigation, as in deployment scenario 1 above) \\_versus\\_ making choices that compound this negative potential to exacerbate the risk of harm (as in deployment scenario 3).""]","I'm sorry, but I can only provide information related to HuggingFace and gradio-app. If you have any questions about those topics, feel free to ask!","To track their machine learning experiments at every stage, from training to production."
"What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\nðŸ¤— Optimum provides an API called BetterTransformer, a fast path of standard PyTorch Transformer APIs to benefit from interesting speedups on CPU & GPU through sparsity and fused kernels as Flash Attention. For now, BetterTransformer supports the fastpath from the native [`nn.TransformerEncoderLayer`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) as well as Flash Attention and Memory-Efficient Attention from [`torch.nn.functional.scaled\\_dot\\_product\\_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled\\_dot\\_product\\_attention.html)."", ""--\ntitle: \""Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\""\nthumbnail: /blog/assets/60\\_habana/habana.png\nauthors:\n- user: susanlansing\nguest: true\n---\n# Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\n\\*Santa Clara and San Francisco, CA, April 12th, 2022\\*\nPowered by deep learning, transformer models deliver state-of-the-art performance on a wide range of machine learning tasks, such as natural language processing, computer vision, speech, and more. However, training them at scale often requires a large amount of computing power, making the whole process unnecessarily long, complex, and costly. Today, [HabanaÂ® Labs](https://habana.ai/), a pioneer in high-efficiency, purpose-built deep learning processors, and Hugging Face, the home of [Transformer](https://github.com/huggingface/transformers) models, are happy to announce that theyâ€™re joining forces to make it easier and quicker to train high-quality transformer models. Thanks to the integration of Habanaâ€™s [SynapseAI software suite](https://habana.ai/training-software/) with the Hugging Face [Optimum open-source library](https://github.com/huggingface/optimum), data scientists and machine learning engineers can now accelerate their Transformer training jobs on Habana processors with just a few lines of code and enjoy greater productivity as well as lower training cost. [Habana Gaudi](https://habana.ai/training/) training solutions, which power Amazonâ€™s EC2 DL1 instances and Supermicroâ€™s X12 Gaudi AI Training Server, deliver price/performance up to 40% lower than comparable training solutions and enable customers to train more while spending less. The integration of ten 100 Gigabit Ethernet ports onto every Gaudi processor enables system scaling from 1 to thousands of Gaudis with ease and cost-efficiency. Habanaâ€™s SynapseAIÂ® is optimizedâ€”at inceptionâ€”to enable Gaudi performance and usability, supports TensorFlow and PyTorch frameworks, with a focus on computer vision and natural language processing applications. With 60,000+ stars on Github, 30,000+ models, and millions of monthly visits, Hugging Face is one of the fastest-growing projects in open source software history, and the go-to place for the machine learning community. With its [Hardware Partner Program](https://huggingface.co/hardware), Hugging Face provides Gaudiâ€™s advanced deep learning hardware with the ultimate Transformer toolset. This partnership will enable rapid expansion of the Habana Gaudi training transformer model library, bringing Gaudi efficiency and ease of use to a wide array of customer use cases like natural language processing, computer vision, speech, and more. â€œ\\*Weâ€™re excited to partner with Hugging Face and its many open-source developers to address the growing demand for transformer models that benefit from the efficiency, usability, and scalability of the Gaudi training platform\\*â€, said Sree Ganesan, head of software product management, Habana Labs. â€œHabana Gaudi brings a new level of efficiency to deep learning model training, and weâ€™re super excited to make this performance easily accessible to Transformer users with minimal code changes through Optimumâ€, said Jeff Boudier, product director at Hugging Face. To learn how to get started training with Habana Gaudi, please visit [https://developer.habana.ai](https://developer.habana.ai)."", ""--\ntitle: \""Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\""\nthumbnail: /blog/assets/80\\_intel/01.png\nauthors:\n- user: juliensimon\n---\n# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\n![image](assets/80\\_intel/01.png)\nThe mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption. Today, we're excited to announce that Intel has officially joined our [Hardware Partner Program](https://huggingface.co/hardware). Thanks to the [Optimum](https://github.com/huggingface/optimum-intel) open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers. Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage. Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms. â€œ\\*Weâ€™re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.\\*â€, says Wei Li, Intel Vice President & General Manager, AI and Analytics. In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs. The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Letâ€™s take a deeper look at it. ## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices.""]",The name of the open-source library created by Hugging Face to simplify Transformer acceleration is **Optimum**.,Optimum
"What parameter is used to ensure that elements in a row have the same height in Gradio?
","[""Controlling Layout\nBy default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS\\_Flexible\\_Box\\_Layout/Basic\\_Concepts\\_of\\_Flexbox). ## Rows\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\n```python\nwith gr.Blocks() as demo:\nwith gr.Row():\nbtn1 = gr.Button(\""Button 1\"")\nbtn2 = gr.Button(\""Button 2\"")\n```\nTo make every element in a Row have the same height, use the `equal\\_height` argument of the `style` method. ```python\nwith gr.Blocks() as demo:\nwith gr.Row(equal\\_height=True):\ntextbox = gr.Textbox()\nbtn2 = gr.Button(\""Button 2\"")\n```\nThe widths of elements in a Row can be controlled via a combination of `scale` and `min\\_width` arguments that are present in every Component. - `scale` is an integer that defines how an element will take up space in a Row. If scale is set to `0`, and element will not expand to take up space. If scale is set to `1` or greater, the element well expand. Multiple elements in a row will expand proportional to their scale. Below, `btn1` will expand twice as much as `btn2`, while `btn0` will not expand at all:\n```python\nwith gr.Blocks() as demo:\nwith gr.Row():\nbtn0 = gr.Button(\""Button 0\"", scale=0)\nbtn1 = gr.Button(\""Button 1\"", scale=1)\nbtn2 = gr.Button(\""Button 2\"", scale=2)\n```\n- `min\\_width` will set the minimum width the element will take."", ""The Row will wrap if there isn't sufficient space to satisfy all `min\\_width` values. Learn more about Rows in the [docs](https://gradio.app/docs/#row). ## Columns and Nesting\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n$code\\_rows\\_and\\_columns\n$demo\\_rows\\_and\\_columns\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. Learn more about Columns in the [docs](https://gradio.app/docs/#column). # Dimensions\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases. Below is an example illustrating the use of viewport width (vw):\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\nim = gr.ImageEditor(\nwidth=\""50vw\"",\n)\ndemo.launch()\n```\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n```python\nimport gradio as gr\ncss = \""\""\""\n.container {\nheight: 100vh;\n}\n\""\""\""\nwith gr.Blocks(css=css) as demo:\nwith gr.Column(elem\\_classes=[\""container\""]):\nname = gr.Chatbot(value=[[\""1\"", \""2\""]], height=\""70%\"")\ndemo.launch()\n```\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height."", ""Gradio Demo: blocks\\_group\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndef greet(name):\nreturn \""Hello \"" + name + \""!\""\nwith gr.Blocks() as demo:\ngr.Markdown(\""### This is a couple of elements without any gr.Group. Form elements naturally group together anyway.\"")\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\ngr.Image()\ngr.Slider()\ngr.Markdown(\""### This is the same set put in a gr.Group.\"")\nwith gr.Group():\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\ngr.Image()\ngr.Slider()\ngr.Markdown(\""### Now in a Row, no group.\"")\nwith gr.Row():\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\ngr.Image()\ngr.Slider()\ngr.Markdown(\""### Now in a Row in a group.\"")\nwith gr.Group():\nwith gr.Row():\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\ngr.Image()\ngr.Slider()\ngr.Markdown(\""### Several rows grouped together.\"")\nwith gr.Group():\nwith gr.Row():\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\nwith gr.Row():\ngr.Image()\ngr.Audio()\ngr.Markdown(\""### Several columns grouped together. If columns are uneven, there is a gray group background.\"")\nwith gr.Group():\nwith gr.Row():\nwith gr.Column():\nname = gr.Textbox(label=\""Name\"")\nbtn = gr.Button(\""Hello\"")\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True)\ngr.Number()\ngr.Textbox()\nwith gr.Column():\ngr.Image()\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True)\nwith gr.Row():\ngr.Number(scale=2)\ngr.Textbox()\ngr.Markdown(\""### container=False removes label, padding, and block border, placing elements 'directly' on background.\"")\ngr.Radio([1,2,3], container=False)\ngr.Textbox(container=False)\ngr.Image(\""https://picsum.photos/id/237/200/300\"", container=False, height=200)\ngr.Markdown(\""### Textbox, Dropdown, and Number input boxes takes up full space when within a group without a container.\"")\nwith gr.Group():\nname = gr.Textbox(label=\""Name\"")\noutput = gr.Textbox(show\\_label=False, container=False)\ngreet\\_btn = gr.Button(\""Greet\"")\nwith gr.Row():\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True, container=False)\ngr.Textbox(container=False)\ngr.Number(container=False)\ngr.Image(height=100)\ngreet\\_btn.click(fn=greet, inputs=name, outputs=output, api\\_name=\""greet\"")\ngr.Markdown(\""### More examples\"")\nwith gr.Group():\ngr.Chatbot()\nwith gr.Row():\nname = gr.Textbox(label=\""Prompot\"", container=False)\ngo = gr.Button(\""go\"", scale=0)\nwith gr.Column():\ngr.Radio([1,2,3], container=False)\ngr.Slider(0, 20, container=False)\nwith gr.Group():\nwith gr.Row():\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True, container=False, elem\\_id=\""here2\"")\ngr.Number(container=False)\ngr.Textbox(container=False)\nwith gr.Row():\nwith gr.Column():\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True, container=False, elem\\_id=\""here2\"")\nwith gr.Column():\ngr.Number(container=False)\nwith gr.Column():\ngr.Textbox(container=False)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```""]","In Gradio, the parameter used to ensure that elements in a row have the same height is the `equal_height` argument of the `style` method. You can set it to `True` when creating a row to achieve this effect. For example:

```python
with gr.Row(equal_height=True):
    textbox = gr.Textbox()
    btn2 = gr.Button(""Button 2"")
```",equal_height
"What is the command to install the latest version of Optimum with OpenVINO support?
","[""![ONNX Runtime](https://github.com/huggingface/optimum/actions/workflows/test\\_onnxruntime.yml/badge.svg)](https://github.com/huggingface/optimum/actions/workflows/test\\_onnxruntime.yml)\n# Hugging Face Optimum\nðŸ¤— Optimum is an extension of ðŸ¤— Transformers and Diffusers, providing a set of optimization tools enabling maximum efficiency to train and run models on targeted hardware, while keeping things easy to use. ## Installation\nðŸ¤— Optimum can be installed using `pip` as follows:\n```bash\npython -m pip install optimum\n```\nIf you'd like to use the accelerator-specific features of ðŸ¤— Optimum, you can install the required dependencies according to the table below:\n| Accelerator | Installation |\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\n| [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/overview) | `pip install --upgrade-strategy eager optimum[onnxruntime]` |\n| [Intel Neural Compressor](https://huggingface.co/docs/optimum/intel/index) | `pip install --upgrade-strategy eager optimum[neural-compressor]`|\n| [OpenVINO](https://huggingface.co/docs/optimum/intel/index) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |\n| [AMD Instinct GPUs and Ryzen AI NPU](https://huggingface.co/docs/optimum/amd/index) | `pip install --upgrade-strategy eager optimum[amd]` |\n| [Habana Gaudi Processor (HPU)](https://huggingface.co/docs/optimum/habana/index) | `pip install --upgrade-strategy eager optimum[habana]` |\n| [FuriosaAI](https://huggingface.co/docs/optimum/furiosa/index) | `pip install --upgrade-strategy eager optimum[furiosa]` |\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version. To install from source:\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\nFor the accelerator-specific features, append `optimum[accelerator\\_type]` to the above command:\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```\n## Accelerated Inference\nðŸ¤— Optimum provides multiple tools to export and run optimized models on various ecosystems:\n- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage\\_guides/export\\_a\\_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage\\_guides/models)\n- TensorFlow Lite\n- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)\n- Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage\\_guides/accelerate\\_inference)\nThe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line. ### Features summary\n| Features | [ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview)| [Neural Compressor](https://huggingface.co/docs/optimum/main/en/intel/optimization\\_inc)| [OpenVINO](https://huggingface.co/docs/optimum/main/en/intel/inference)| [TensorFlow Lite](https://huggingface.co/docs/optimum/main/en/exporters/tflite/overview)|\n|:----------------------------------:|:------------------:|:------------------:|:------------------:|:------------------:|\n| Graph optimization | :heavy\\_check\\_mark: | N/A | :heavy\\_check\\_mark: | N/A |\n| Post-training dynamic quantization | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | N/A | :heavy\\_check\\_mark: |\n| Post-training static quantization | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: |\n| Quantization Aware Training (QAT) | N/A | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | N/A |\n| FP16 (half precision) | :heavy\\_check\\_mark: | N/A | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: |\n| Pruning | N/A | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | N/A |\n| Knowledge Distillation | N/A | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | N/A |\n### OpenVINO\nBefore you begin, make sure you have all the necessary libraries installed :\n```bash\npip install --upgrade-strategy eager optimum[openvino,nncf]\n```\nIt is possible to export ðŸ¤— Transformers and Diffusers models to the OpenVINO format easily:\n```bash\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert\\_sst2\\_ov\n```\nIf you add `--int8`, the weights will be quantized to INT8."", ""As demonstrated by this [Intel Space](https://huggingface.co/spaces/Intel/Stable-Diffusion-Side-by-Side), the same code runs on a previous generation Intel Xeon (code name Ice Lake) in about 45 seconds. Out of the box, we can see that Sapphire Rapids CPUs are quite faster without any code change! Now, let's accelerate! ## Optimum Intel and OpenVINO\n[Optimum Intel](https://huggingface.co/docs/optimum/intel/index) accelerates end-to-end pipelines on Intel architectures. Its API is extremely similar to the vanilla [Diffusers](https://huggingface.co/docs/diffusers/index) API, making it trivial to adapt existing code. Optimum Intel supports [OpenVINO](https://docs.openvino.ai/latest/index.html), an Intel open-source toolkit for high-performance inference. Optimum Intel and OpenVINO can be installed as follows:\n```\npip install optimum[openvino]\n```\nStarting from the code above, we only need to replace `StableDiffusionPipeline` with `OVStableDiffusionPipeline`. To load a PyTorch model and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model. ```python\nfrom optimum.intel.openvino import OVStableDiffusionPipeline\n... ov\\_pipe = OVStableDiffusionPipeline.from\\_pretrained(model\\_id, export=True)\nlatency = elapsed\\_time(ov\\_pipe, prompt)\nprint(latency)\n# Don't forget to save the exported model\nov\\_pipe.save\\_pretrained(\""./openvino\"")\n```\nOpenVINO automatically optimizes the model for the `bfloat16` format. Thanks to this, the average latency is now \\*\\*16.7 seconds\\*\\*, a sweet 2x speedup. The pipeline above support dynamic input shapes, with no restriction on the number of images or their resolution. With Stable Diffusion, your application is usually restricted to one (or a few) different output resolutions, such as 512x512, or 256x256."", ""--\ntitle: \""Accelerate your models with ðŸ¤— Optimum Intel and OpenVINO\""\nthumbnail: /blog/assets/113\\_openvino/thumbnail.png\nauthors:\n- user: echarlaix\n- user: juliensimon\n---\n# Accelerate your models with ðŸ¤— Optimum Intel and OpenVINO\n![image](assets/113\\_openvino/thumbnail.png)\nLast July, we [announced](https://huggingface.co/blog/intel) that Intel and Hugging Face would collaborate on building state-of-the-art yet simple hardware acceleration tools for Transformer models. â€‹\nToday, we are very happy to announce that we added Intel [OpenVINO](https://docs.openvino.ai/latest/index.html) to [Optimum Intel](https://github.com/huggingface/optimum-intel). You can now easily perform inference with OpenVINO Runtime on a variety of Intel processors ([see](https://docs.openvino.ai/latest/openvino\\_docs\\_OV\\_UG\\_supported\\_plugins\\_Supported\\_Devices.html) the full list of supported devices) using Transformers models which can be hosted either on the Hugging Face hub or locally. You can also quantize your model with the OpenVINO Neural Network Compression Framework ([NNCF](https://github.com/openvinotoolkit/nncf)), and reduce its size and prediction latency in near minutes. â€‹\nThis first release is based on OpenVINO 2022.2 and enables inference for a large quantity of PyTorch models using our [`OVModels`](https://huggingface.co/docs/optimum/intel/inference). Post-training static quantization and quantization aware training can be applied on many encoder models (BERT, DistilBERT, etc.). More encoder models will be supported in the upcoming OpenVINO release. Currently the quantization of Encoder Decoder models is not enabled, however this restriction should be lifted with our integration of the next OpenVINO release. â€‹Let us show you how to get started in minutes!â€‹\n## Quantizing a Vision Transformer with Optimum Intel and OpenVINO\nâ€‹\nIn this example, we will run post-training static quantization on a Vision Transformer (ViT) [model](https://huggingface.co/juliensimon/autotrain-food101-1471154050) fine-tuned for image classification on the [food101](https://huggingface.co/datasets/food101) dataset.""]","The command to install the latest version of Optimum with OpenVINO support is:

```bash
pip install --upgrade-strategy eager optimum[openvino]
```","pip install --upgrade-strategy eager optimum[""openvino""]"

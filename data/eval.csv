context,question,answer,source_doc,groundedness_score,groundedness_eval,relevance_score,relevance_eval,standalone_score,standalone_eval
"!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Hybrid Vision Transformer (ViT Hybrid)

## Overview

The hybrid Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition
at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk
Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob
Uszkoreit, Neil Houlsby. It's the first paper that successfully trains a Transformer encoder on ImageNet, attaining
very good results compared to familiar convolutional architectures. ViT hybrid is a slight variant of the [plain Vision Transformer](vit),
by leveraging a convolutional backbone (specifically, [BiT](bit)) whose features are used as initial ""tokens"" for the Transformer.

The abstract from the paper is the following:",Who proposed the hybrid Vision Transformer model?,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.",huggingface/transformers/blob/main/docs/source/en/model_doc/vit_hybrid.md,5,"The context provides very detailed information about the hybrid Vision Transformer model, including the exact authors who proposed it. The list of authors is comprehensive and directly answers the question. The context is clear and unambiguous, leaving no room for misinterpretation regarding who proposed the model. It mentions the specific paper in which the model was proposed, along with all the contributing authors, making it very easy to answer the question accurately. Therefore, the context is perfectly aligned to answer the question.",2,"The question 'Who proposed the hybrid Vision Transformer model?' is relevant to those interested in the development and history of machine learning models, particularly in the field of computer vision. However, it is not directly related to NLP applications or the Hugging Face ecosystem, which primarily focuses on natural language processing tasks. Therefore, while the question might be useful for those studying machine learning models more generally, it is not specifically useful for NLP developers using Hugging Face tools. This limits its usefulness to the target audience specified.",5,"The question 'Who proposed the hybrid Vision Transformer model?' is context-independent and can be understood on its own. It asks for the name of an individual or group who proposed a specific model in the field of machine learning. Although it refers to a specific model, it does not require additional context to comprehend the question itself. Anyone familiar with Vision Transformers and their development can attempt to answer it with the right resources."
"## Summaries

### [Longformer - The Long-Document Transformer](https://arxiv.org/abs/2004.05150)

Iz Beltagy, Matthew E. Peters, Arman Cohan

Longformer addresses the memory bottleneck of transformers by replacing conventional self-attention with a combination of windowed/local/sparse (cf. [Sparse Transformers (2019)](https://arxiv.org/abs/1904.10509)) attention and global attention that scales linearly with the sequence length. As opposed to previous long-range transformer models (e.g. [Transformer-XL (2019)](https://arxiv.org/abs/1901.02860), [Reformer (2020)](https://arxiv.org/abs/2001.04451), [Adaptive Attention Span (2019)](https://arxiv.org/abs/1905.07799)), Longformer’s self-attention layer is designed as a drop-in replacement for the standard self-attention, thus making it possible to leverage pre-trained checkpoints for further pre-training and/or fine-tuning on long sequence tasks.

The standard self-attention matrix (Figure a) scales quadratically with the input length:

<figure>
  <img src=""/blog/assets/14_long_range_transformers/Longformer.png"" alt=""Longformer attention""/>
  <figcaption>Figure taken from Longformer</figcaption>
</figure>",Who are the authors of the Longformer paper?,"Iz Beltagy, Matthew E. Peters, Arman Cohan",huggingface/blog/blob/main/long-range-transformers.md,5,"The context provides the exact information needed to answer the question. It clearly lists the authors of the 'Longformer' paper as Iz Beltagy, Matthew E. Peters, and Arman Cohan. The question is straightforward, and the context gives a direct answer without any ambiguity or need for inference. Therefore, the question is completely answerable with the given context.",2,"This question is not directly useful for machine learning developers who are building NLP applications using the Hugging Face ecosystem. Knowing the authors of the Longformer paper does not provide any technical or practical insights into how to implement, use, or optimize the Longformer model within a project. Developers would benefit more from questions about model capabilities, integration specifics, performance benchmarks, or application examples. While understanding the authorship can be a part of academic interest, it is not directly relevant to the core development tasks related to NLP applications.",4,"This question is specific and asks for the authors of a well-known paper, ""the Longformer paper."" However, it assumes that the reader knows about the Longformer model and its related paper. The question is quite clear in its intention, and anyone familiar with academic papers or with access to academic databases could answer it. It does not intrinsically depend on additional context to be understood, aside from the general knowledge of what a paper is and that it has authors. Therefore, it leans towards being context-independent."
"- `distilbert-base-uncased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-uncased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 66M parameters.
- `distilbert-base-uncased-distilled-squad`: A finetuned version of `distilbert-base-uncased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 86.9 on the dev set (for comparison, Bert `bert-base-uncased` version reaches a 88.5 F1 score).
- `distilbert-base-cased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-cased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 65M parameters.
- `distilbert-base-cased-distilled-squad`: A finetuned version of `distilbert-base-cased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 87.1 on the dev set (for comparison, Bert `bert-base-cased` version reaches a 88.7 F1 score).
- `distilbert-base-german-cased`: DistilBERT German language model pretrained on 1/2 of the data used to pretrain Bert using distillation with the supervision of the `bert-base-german-dbmdz-cased` version of German DBMDZ Bert. For NER tasks the model reaches a F1 score of 83.49 on the CoNLL-2003 test set (for comparison, `bert-base-german-dbmdz-cased` reaches a 84.52 F1 score), and a F1 score of 85.23 on the GermEval 2014 test set (`bert-base-german-dbmdz-cased` reaches a 86.89 F1 score).",How many layers does the distilbert-base-uncased model have?,6,huggingface/transformers/blob/main/examples/research_projects/distillation/README.md,5,"The context provides specific details about the `distilbert-base-uncased` model, including the number of layers it has. According to the context, the model has 6 layers. Therefore, the question about the number of layers in the `distilbert-base-uncased` model is directly and clearly answerable from the provided context without any ambiguity. The information is explicit and straightforward, ensuring that the question can be answered accurately.",5,"The question is quite specific and pertains to a particular model within the Hugging Face ecosystem, namely the 'distilbert-base-uncased' model. Knowing the number of layers in a model is important for understanding its architecture, which can impact how a developer might choose to use or modify the model for their specific NLP tasks. This information is directly relevant and useful for machine learning developers working with transformer models, as it aids in tasks like model selection, understanding computational costs, and customizing models for specific applications. This question is highly relevant for practitioners looking to gain detailed knowledge of the model architecture, which is a common task when using Hugging Face's resources for building NLP applications.",5,"This question is context-independent because it seeks specific information about a well-known model, DistilBERT, which is a variant of the BERT model. The question does not depend on any external context or document to be understood. The DistilBERT model is a common term in machine learning and natural language processing literature, and the question is straightforward in asking for the number of layers in a specific model."
"New users are often very confused by the range of TPUs, and the different ways to access them. The first key distinction to understand is the difference between **TPU Nodes** and **TPU VMs.**

When you use a **TPU Node**, you are effectively indirectly accessing a remote TPU. You will need a separate VM, which will initialize your network and data pipeline and then forward them to the remote node. When you use a TPU on Google Colab, you are accessing it in the **TPU Node** style.

Using TPU Nodes can have some quite unexpected behaviour for people who aren’t used to them! In particular, because the TPU is located on a physically different system to the machine you’re running your Python code on, your data cannot be local to your machine - any data pipeline that loads from your machine’s internal storage will totally fail! Instead, data must be stored in Google Cloud Storage where your data pipeline can still access it, even when the pipeline is running on the remote TPU node.

<Tip>

If you can fit all your data in memory as `np.ndarray` or `tf.Tensor`, then you can `fit()` on that data even when using Colab or a TPU Node, without needing to upload it to Google Cloud Storage.

</Tip>

<Tip>

**🤗Specific Hugging Face Tip🤗:** The methods `Dataset.to_tf_dataset()` and its higher-level wrapper `model.prepare_tf_dataset()` , which you will see throughout our TF code examples, will both fail on a TPU Node. The reason for this is that even though they create a `tf.data.Dataset` it is not a “pure” `tf.data` pipeline and uses `tf.numpy_function` or `Dataset.from_generator()` to stream data from the underlying HuggingFace `Dataset`. This HuggingFace `Dataset` is backed by data that is on a local disc and which the remote TPU Node will not be able to read.

</Tip>",What is the style of TPU access when using Google Colab?,TPU Node style.,huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md,5,"The context provides an explanation of the TPU access style when using Google Colab. It clearly states that when using a TPU on Google Colab, one accesses it in the 'TPU Node' style, where the TPU is remotely accessed and requires data to be stored in Google Cloud Storage rather than locally. This information directly answers the question about the style of TPU access in Google Colab. The context adequately clarifies the differences between TPU Nodes and TPU VMs and provides specific details about data handling when using TPUs in this manner. Therefore, the question is answerable clearly and unambiguously with the given context.",3,"This question is somewhat relevant for machine learning developers, particularly those who are interested in utilizing TPUs for their NLP applications using Google Colab. TPUs (Tensor Processing Units) can significantly accelerate the training of machine learning models, and understanding the access style (e.g., how to enable and utilize TPU resources) is crucial for optimizing resource usage and performance. However, the question is quite specific to Google Colab and doesn't directly address the Hugging Face ecosystem or NLP applications. Thus, while it is useful, it is not directly aligned with the Hugging Face ecosystem focus, which slightly limits its broader applicability in that context.",5,"The question is clear and can be understood without additional context. It asks specifically about the style of TPU (Tensor Processing Unit) access when using Google Colab, a cloud service. An operator with knowledge of Google Colab or access to documentation would understand that the question is about the configuration or manner in which TPUs are accessed via this service. No specific context or prior information is necessary to comprehend the question, as it is both specific and self-contained."
"<iframe src=""https://course-demos-hello-world.hf.space"" frameBorder=""0"" height=""250"" title=""Gradio app"" class=""container p-0 flex-grow space-iframe"" allow=""accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"" sandbox=""allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads""></iframe>

Try using this GUI right now with your own name or some other input!

You'll notice that in this GUI, Gradio automatically inferred the name of the input parameter (`name`)
and applied it as a label on top of the textbox. What if you'd like to change that?
Or if you'd like to customize the textbox in some other way? In that case, you can
instantiate a class object representing the input component.

Take a look at the example below:

```py
import gradio as gr


def greet(name):
    return ""Hello "" + name


# We instantiate the Textbox class
textbox = gr.Textbox(label=""Type your name here:"", placeholder=""John Doe"", lines=2)

gr.Interface(fn=greet, inputs=textbox, outputs=""text"").launch()
```

<iframe src=""https://course-demos-hello-world-custom.hf.space"" frameBorder=""0"" height=""300"" title=""Gradio app"" class=""container p-0 flex-grow space-iframe"" allow=""accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"" sandbox=""allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads""></iframe>",What is the default label inferred by Gradio for the input parameter in the GUI?,name,huggingface/course/blob/main/chapters/en/chapter9/2.mdx,5,"The question asks about the default label inferred by Gradio for the input parameter in the GUI. The context provides information about Gradio's behavior regarding input labels, specifically mentioning that Gradio automatically infers the name of the input parameter and applies it as a label on the textbox. This context directly answers the question by stating that the default label is the name of the input parameter, which in this example is 'name'. However, for someone unfamiliar with the setup, it might not be immediately clear without some understanding of how Gradio infers labels. Nonetheless, the context does provide a clear enough explanation for those familiar with programming concepts and Gradio's interface.",3,"The question is specific to the Gradio library, which is used for building graphical user interfaces for machine learning models, including NLP applications. Knowing the default label inferred by Gradio for input parameters can be useful for developers who are working on quickly deploying or testing models using Gradio, as it helps in understanding how the interface will be initially presented to the end user. However, this question is quite narrow in scope and only relevant to those who are specifically using Gradio within the Hugging Face ecosystem or similar environments. It does not directly contribute to understanding or improving machine learning models themselves, which limits its broader applicability.",5,"This question is fairly context-independent because it asks about a specific functionality of Gradio, a known tool for building GUIs. The question is clear in its intention to find out the default behavior of Gradio with respect to labeling input parameters. Anyone familiar with Gradio or who can reference its documentation would understand what is being asked without needing additional context."
"</Tip>

For the quicktour, you'll instantiate the [`DDPMScheduler`] with its [`~diffusers.ConfigMixin.from_config`] method:

```py
>>> from diffusers import DDPMScheduler

>>> scheduler = DDPMScheduler.from_pretrained(repo_id)
>>> scheduler
DDPMScheduler {
  ""_class_name"": ""DDPMScheduler"",
  ""_diffusers_version"": ""0.21.4"",
  ""beta_end"": 0.02,
  ""beta_schedule"": ""linear"",
  ""beta_start"": 0.0001,
  ""clip_sample"": true,
  ""clip_sample_range"": 1.0,
  ""dynamic_thresholding_ratio"": 0.995,
  ""num_train_timesteps"": 1000,
  ""prediction_type"": ""epsilon"",
  ""sample_max_value"": 1.0,
  ""steps_offset"": 0,
  ""thresholding"": false,
  ""timestep_spacing"": ""leading"",
  ""trained_betas"": null,
  ""variance_type"": ""fixed_small""
}
```

<Tip>

💡 Unlike a model, a scheduler does not have trainable weights and is parameter-free!

</Tip>

Some of the most important parameters are:

* `num_train_timesteps`: the length of the denoising process or, in other words, the number of timesteps required to process random Gaussian noise into a data sample.
* `beta_schedule`: the type of noise schedule to use for inference and training.
* `beta_start` and `beta_end`: the start and end noise values for the noise schedule.

To predict a slightly less noisy image, pass the following to the scheduler's [`~diffusers.DDPMScheduler.step`] method: model output, `timestep`, and current `sample`.

```py
>>> less_noisy_sample = scheduler.step(model_output=noisy_residual, timestep=2, sample=noisy_sample).prev_sample
>>> less_noisy_sample.shape
torch.Size([1, 3, 256, 256])
```

The `less_noisy_sample` can be passed to the next `timestep` where it'll get even less noisy! Let's bring it all together now and visualize the entire denoising process.

First, create a function that postprocesses and displays the denoised image as a `PIL.Image`:

```py
>>> import PIL.Image
>>> import numpy as np",What does the 'beta_start' parameter value mean in DDPMScheduler?,0.0001,huggingface/diffusers/blob/main/docs/source/en/quicktour.md,5,"The context provides a detailed explanation of the parameters used in the `DDPMScheduler`, including `beta_start`. It states that `beta_start` is the start noise value for the noise schedule. This information directly answers the question about the meaning of the 'beta_start' parameter in `DDPMScheduler`. Since the context provides a clear and direct definition of the `beta_start` parameter, the question is answerable unambiguously with the provided information.",4,"This question is quite useful for machine learning developers using the Hugging Face ecosystem, particularly those interested in diffusion models. The 'beta_start' parameter is often used in the context of Denoising Diffusion Probabilistic Models (DDPMs), which are gaining popularity for generative tasks. Understanding what 'beta_start' means is crucial when configuring or fine-tuning these models, as it affects the variance of the noise schedule and consequently the quality of the generated samples. Additionally, the question is specific and technical, which indicates that the asker is seeking detailed knowledge about the DDPMScheduler, likely to improve their model's performance or stability. However, it might not be directly relevant to all NLP practitioners, as diffusion models are more common in image and generative model tasks. Therefore, while very useful to a subset of users, it is not universally applicable across all NLP applications within Hugging Face.",5,"The question asks about the meaning of a specific parameter, 'beta_start', in the context of `DDPMScheduler`. While the acronym DDPMScheduler might be technical, it is explicit enough that someone with knowledge in the relevant domain (such as machine learning or software engineering) would understand it's about a parameter in a scheduler. The question does not refer to any specific document or context, making it context-independent. The question is clear and does not require further information to understand what is being asked, assuming the audience has the relevant technical background."
"## Registering a model with custom code to the auto classes

If you are writing a library that extends 🤗 Transformers, you may want to extend the auto classes to include your own
model. This is different from pushing the code to the Hub in the sense that users will need to import your library to
get the custom models (contrarily to automatically downloading the model code from the Hub).

As long as your config has a `model_type` attribute that is different from existing model types, and that your model
classes have the right `config_class` attributes, you can just add them to the auto classes like this:

```py
from transformers import AutoConfig, AutoModel, AutoModelForImageClassification

AutoConfig.register(""resnet"", ResnetConfig)
AutoModel.register(ResnetConfig, ResnetModel)
AutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification)
```

Note that the first argument used when registering your custom config to [`AutoConfig`] needs to match the `model_type`
of your custom config, and the first argument used when registering your custom models to any auto model class needs
to match the `config_class` of those models.

## Sending the code to the Hub

<Tip warning={true}>

This API is experimental and may have some slight breaking changes in the next releases.

</Tip>

First, make sure your model is fully defined in a `.py` file. It can rely on relative imports to some other files as
long as all the files are in the same directory (we don't support submodules for this feature yet). For our example,
we'll define a `modeling_resnet.py` file and a `configuration_resnet.py` file in a folder of the current working
directory named `resnet_model`. The configuration file contains the code for `ResnetConfig` and the modeling file
contains the code of `ResnetModel` and `ResnetModelForImageClassification`.

```
.
└── resnet_model
    ├── __init__.py
    ├── configuration_resnet.py
    └── modeling_resnet.py
```",How do you register a custom Resnet model to the auto classes in Transformers?,"Use AutoConfig.register(""resnet"", ResnetConfig), AutoModel.register(ResnetConfig, ResnetModel), and AutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification) to register the custom Resnet model to the auto classes.",huggingface/transformers/blob/main/docs/source/en/custom_models.md,5,"The context provides a clear explanation of how to register a custom Resnet model to the auto classes in Transformers. It includes concrete code examples and a step-by-step guide on what files need to be created and how they should be structured. The context specifies the need to have a `model_type` attribute in the config and to properly align this with the `config_class` attributes in the model classes. It also describes the registration process using the `AutoConfig`, `AutoModel`, and `AutoModelForImageClassification` classes. Therefore, the question is clearly and unambiguously answerable with the given context.",5,"This question is highly relevant to machine learning developers who are working with the Hugging Face Transformers library and are interested in customizing their models. Registering a custom Resnet model to the auto classes in Transformers is a specific and advanced task that many developers might encounter when they wish to extend the library's functionality with their own custom architectures. The answer to this question would provide valuable guidance on how to integrate custom models into the Hugging Face ecosystem, an essential task for developers looking to leverage the powerful features of Transformers with their own models. This makes the question very useful for experienced developers aiming to optimize their workflow and enhance their models' capabilities.",5,"The question is quite specific and technical, but it does not rely on any additional context to be understood. It asks about a specific process involving a Resnet model and the auto classes in Transformers, which are concepts that can be found in the documentation. Therefore, someone familiar with the Transformers library or with access to its documentation should be able to understand the question without needing more information."
"1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.
1. **[CLAP](https://huggingface.co/docs/transformers/model_doc/clap)** (from LAION-AI) released with the paper [Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation](https://arxiv.org/abs/2211.06687) by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.
1. **[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)** (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.
1. **[CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)** (from University of Göttingen) released with the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke and Alexander Ecker.
1. **[CodeGen](https://huggingface.co/docs/transformers/model_doc/codegen)** (from Salesforce) released with the paper [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.",Who is the author of the paper 'Learning Transferable Visual Models From Natural Language Supervision'?,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.",huggingface/transformers/blob/main/README_te.md,5,"The context provided includes a list of papers and their authors, one of which is 'Learning Transferable Visual Models From Natural Language Supervision' by OpenAI. The context clearly identifies the authors of this particular paper: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Therefore, the question is answerable unambiguously with the given context.",2,"The question asks for the author of a specific academic paper. While this question is relevant to individuals working with NLP applications, it is not directly related to the development or use of machine learning models in the Hugging Face ecosystem. It does not provide insight into concepts, techniques, or tools useful for building NLP applications. Therefore, its utility for developers in this context is limited primarily to those interested in the provenance of certain research papers, rather than practical application development or problem-solving.",5,"The question ""Who is the author of the paper 'Learning Transferable Visual Models From Natural Language Supervision'?"" is largely context-independent because it asks for specific information about a named scientific paper. The question can be understood and answered directly by someone with access to relevant databases or the paper itself. It does not rely on any external context or additional information beyond what is contained in the question itself. Thus, it is clear and specific enough to make sense on its own."
"## Local attention

[Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the
left and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small
window, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a
representation of the whole sentence.

Some preselected input tokens are also given global attention: for those few tokens, the attention matrix can access
all tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in
their local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:

<div class=""flex justify-center"">
    <img scale=""50 %"" align=""center"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png""/>
</div>

Using those attention matrices with less parameters then allows the model to have inputs having a bigger sequence
length.

## Other tricks

### Axial positional encodings

[Reformer](#reformer) uses axial positional encodings: in traditional transformer models, the positional encoding
E is a matrix of size \\(l\\) by \\(d\\), \\(l\\) being the sequence length and \\(d\\) the dimension of the
hidden state. If you have very long texts, this matrix can be huge and take way too much space on the GPU. To alleviate
that, axial positional encodings consist of factorizing that big matrix E in two smaller matrices E1 and E2, with
dimensions \\(l_{1} \times d_{1}\\) and \\(l_{2} \times d_{2}\\), such that \\(l_{1} \times l_{2} = l\\) and
\\(d_{1} + d_{2} = d\\) (with the product for the lengths, this ends up being way smaller). The embedding for time
step \\(j\\) in E is obtained by concatenating the embeddings for timestep \\(j \% l1\\) in E1 and \\(j // l1\\)
in E2.",What is the purpose of local attention in Longformer?,"To take action for a given token using the local context, such as the two tokens to the left and right, and to build a representation of the whole sentence by stacking attention layers with a small window.",huggingface/transformers/blob/main/docs/source/en/attention.md,5,"The context provides a clear explanation of local attention in Longformer. It describes how local attention focuses on the immediate neighboring tokens, allowing for efficient processing by stacking attention layers to capture broader sentence context. The explanation also mentions the incorporation of global attention for specific tokens and how this combination reduces the number of parameters, enabling the model to handle longer sequences effectively. Thus, the purpose of local attention in Longformer, as outlined in the context, is well-defined and unambiguous: it allows the model to efficiently manage attention over longer sequences by focusing on local context and selectively using global attention.",5,"This question is quite useful for developers working with the Hugging Face ecosystem, especially those dealing with transformer models such as Longformer. Understanding local attention in Longformer is crucial because it is one of the key features that allows processing of longer sequences efficiently, making it an important aspect of model optimization and performance. Additionally, knowing the purpose of local attention helps in fine-tuning the model for specific NLP tasks that may require handling extensive contexts. Hence, this question is relevant for developers aiming to leverage Longformer's capabilities effectively in their applications.",5,"The question 'What is the purpose of local attention in Longformer?' is clear and specific. It directly asks about the role of a particular feature (local attention) within a specific model (Longformer), which is a known concept in the field of machine learning and natural language processing. The question stands alone without needing reference to external or prior context to be understood, assuming the reader has some basic knowledge of the Longformer model. Thus, it is context-independent within its field."
"For more information about community pipelines, take a look at the [Community pipelines](custom_pipeline_examples) guide for how to use them and if you're interested in adding a community pipeline check out the [How to contribute a community pipeline](contribute_pipeline) guide!

## Community components

Community components allow users to build pipelines that may have customized components that are not a part of Diffusers. If your pipeline has custom components that Diffusers doesn't already support, you need to provide their implementations as Python modules. These customized components could be a VAE, UNet, and scheduler. In most cases, the text encoder is imported from the Transformers library. The pipeline code itself can also be customized.

This section shows how users should use community components to build a community pipeline.

You'll use the [showlab/show-1-base](https://huggingface.co/showlab/show-1-base) pipeline checkpoint as an example. So, let's start loading the components:

1. Import and load the text encoder from Transformers:

```python
from transformers import T5Tokenizer, T5EncoderModel

pipe_id = ""showlab/show-1-base""
tokenizer = T5Tokenizer.from_pretrained(pipe_id, subfolder=""tokenizer"")
text_encoder = T5EncoderModel.from_pretrained(pipe_id, subfolder=""text_encoder"")
```

2. Load a scheduler:

```python
from diffusers import DPMSolverMultistepScheduler

scheduler = DPMSolverMultistepScheduler.from_pretrained(pipe_id, subfolder=""scheduler"")
```

3. Load an image processor:

```python
from transformers import CLIPFeatureExtractor

feature_extractor = CLIPFeatureExtractor.from_pretrained(pipe_id, subfolder=""feature_extractor"")
```

<Tip warning={true}>",What library is used to import the text encoder?,Transformers,huggingface/diffusers/blob/main/docs/source/en/using-diffusers/custom_pipeline_overview.md,5,"The context provides a detailed explanation of the libraries and components used in building community pipelines. It explicitly mentions that the text encoder is imported from the Transformers library, supported by a code snippet showing how the import is done using `T5EncoderModel`. This directly answers the question about which library is used to import the text encoder. Therefore, the question is clearly and unambiguously answerable with the given context.",2,"This question is quite vague and lacks specific context. In the realm of machine learning and NLP with the Hugging Face ecosystem, there are multiple libraries that could potentially involve a 'text encoder'. Some of these include Transformers, Tokenizers, and possibly Datasets, depending on the task. However, the question does not specify which text encoder or for what purpose it is being used, making it difficult for developers to ascertain the exact library being referred to. For it to be useful, the question should provide more context or specify the task or model involved. Thus, its utility is limited without additional information.",4,"The question ""What library is used to import the text encoder?"" is context-independent to a large extent. It asks about a general concept without referring to any specific document or context. However, the question assumes that the reader has a certain level of knowledge about programming or machine learning libraries, and what a 'text encoder' might be. Despite this, the question is clear and understandable on its own, assuming the reader is familiar with the domain. Therefore, it does not require additional context to make sense, making it quite independent."
"|      |                                                                            |[TheBloke/Chronohermes-Grad-L2-13B-GPTQ](https://huggingface.co/TheBloke/Chronohermes-Grad-L2-13B-GPTQ)|6           |1                        |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/Chronohermes-Grad-L2-13B-GPTQ/blob/main/LICENSE.txt) |                                                                                                                     |                                                                                   |
|      |                                                                            |[TheBloke/Llama2-22B-Daydreamer-v3-GGUF](https://huggingface.co/TheBloke/Llama2-22B-Daydreamer-v3-GGUF)|6           |1                        |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/Llama2-22B-Daydreamer-v3-GGUF/blob/main/LICENSE.txt) |                                                                                                                     |                                                                                   |",What license is used for TheBloke/Chronohermes-Grad-L2-13B-GPTQ?,llama-2-community-license,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md,5,"The context provides a table with information about two models hosted on Hugging Face. Each entry in the table includes a link to the model, a column that seems to represent a license, and a link to the license. For the model 'TheBloke/Chronohermes-Grad-L2-13B-GPTQ', the license is clearly mentioned as 'llama-2-community-license.' This matches the format and structure of the context provided, which directly answers the question about the licensing of the model. The clarity and directness of the context make it easy to unambiguously answer the question regarding the license used for the specified model.",3,"This question is specific to a particular model, TheBloke/Chronohermes-Grad-L2-13B-GPTQ, and its licensing information. Knowing the license type is crucial for developers who are concerned about the legal aspects of using this model in their applications. It helps them understand the permissions, limitations, and obligations associated with the use of the model. This is particularly important for applications that will be distributed or commercialized. However, the question is fairly narrow in scope as it pertains to a single model, rather than a broader aspect of using or developing with the Hugging Face ecosystem. Thus, while it's useful, its utility is limited to those specifically interested in this model.",5,"This question is quite clear and does not rely on any external context. It specifically asks about the license for a particular named entity, which is presumably a model or project. The question is straightforward and anyone with access to the relevant documentation or repository can find the answer without needing additional context. Thus, it is context-independent."
"> ⚠️ **Limitation:** Using outputs from language models directly may have unintended legal, ethical, and commercial ramifications.

Some potential unintended ramifications of using outputs directly are as follows:
- <u>Legal:</u> The legal landscape surrounding Generative AI is currently very unclear, with several ongoing lawsuits.
- <u>Ethical:</u> Language models can produce plagiarized or biased outputs. For more information, check out the [Ethics and Society Newsletter](https://huggingface.co/blog/ethics-soc-2).
- <u>Commercial:</u> [Some](https://www.searchenginejournal.com/google-says-ai-generated-content-is-against-guidelines/444916/) sources have stated that AI-generated content may be deprioritized by search engines. This [may not](https://seo.ai/blog/google-is-not-against-ai-content) be the case for most non-spam content, but is worth considering. Tools such as [AI Content Detector](https://writer.com/ai-content-detector/) can be used to check whether content may be detected as AI-generated. There is ongoing research on language model [watermarking](https://arxiv.org/abs/2301.10226) which may mark text as AI-generated.

Given these limitations, the safest approach may be to use language models like ChatGPT for brainstorming but write the final content by hand.

5. **Scale the content.** I continue to use ChatGPT to flesh out descriptions for the items in the store.

<div align=""center"">
  <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/chatgpt5.png"" alt=""ChatGPT for Story #5"">
</div>

For my simple farming game, this may be an effective approach to producing all the story content for the game. However, this may quickly run into scaling limitations. ChatGPT isn't well-suited to very long cohesive storytelling. Even after generating a few item descriptions for the farming game, the results begin to drift in quality and fall into repetition.",What is a potential legal issue with using outputs from language models?,"The legal landscape surrounding Generative AI is currently very unclear, with several ongoing lawsuits.",huggingface/blog/blob/main/ml-for-games-5.md,5,"The context provides a clear indication of potential legal issues associated with using outputs from language models. Specifically, it mentions the 'unclear legal landscape' and 'ongoing lawsuits' that suggest legal risks involved. Although the context does not delve into specifics about the nature of these lawsuits, the mention of an 'unclear legal landscape' gives enough information to identify legal uncertainty as a key issue. This directly answers the question about potential legal issues, making it unambiguously answerable with the given context.",4,"This question is quite useful for machine learning developers, especially those working in the NLP domain using Hugging Face models. Understanding potential legal issues is crucial for ensuring compliance with copyright laws, data protection regulations, and ethical guidelines when deploying language models in real-world applications. It can guide developers to consider aspects like data privacy, model transparency, and the implications of using AI-generated content, which are important for building responsible and legally compliant AI systems. However, the question could be more specific to Hugging Face or their specific tools and models to make it even more relevant to the ecosystem.",5,"The question 'What is a potential legal issue with using outputs from language models?' is context-independent. It asks generally about legal issues related to the use of outputs from language models, which is a broad topic applicable across many scenarios involving AI models. The question does not refer to a specific context, document, or particular situation, making it understandable without additional context. Anyone with knowledge in the field of AI or legal studies can interpret and attempt to answer this question based on general principles or known issues in the domain."
"#### Do not modify this code

```python
from urllib.error import HTTPError

from huggingface_hub import hf_hub_download


def load_from_hub(repo_id: str, filename: str) -> str:
    """"""
    Download a model from Hugging Face Hub.
    :param repo_id: id of the model repository from the Hugging Face Hub
    :param filename: name of the model zip file from the repository
    """"""
    # Get the model from the Hub, download and cache the model on your local disk
    pickle_model = hf_hub_download(repo_id=repo_id, filename=filename)

    with open(pickle_model, ""rb"") as f:
        downloaded_model_file = pickle.load(f)

    return downloaded_model_file
```

### .

```python
model = load_from_hub(repo_id=""ThomasSimonini/q-Taxi-v3"", filename=""q-learning.pkl"")  # Try to use another model

print(model)
env = gym.make(model[""env_id""])

evaluate_agent(env, model[""max_steps""], model[""n_eval_episodes""], model[""qtable""], model[""eval_seed""])
```

```python
model = load_from_hub(
    repo_id=""ThomasSimonini/q-FrozenLake-v1-no-slippery"", filename=""q-learning.pkl""
)  # Try to use another model

env = gym.make(model[""env_id""], is_slippery=False)

evaluate_agent(env, model[""max_steps""], model[""n_eval_episodes""], model[""qtable""], model[""eval_seed""])
```

## Some additional challenges 🏆

The best way to learn **is to try things on your own**! As you saw, the current agent is not doing great. As a first suggestion, you can train for more steps. With 1,000,000 steps, we saw some great results!

In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?

Here are some ideas to climb up the leaderboard:

* Train more steps
* Try different hyperparameters by looking at what your classmates have done.
* **Push your new trained model** on the Hub 🔥",What is the filename used when downloading the model from the Hugging Face Hub?,q-learning.pkl,huggingface/deep-rl-class/blob/main/units/en/unit2/hands-on.mdx,5,"The context provides Python code snippets that show how to download a model from the Hugging Face Hub using a function called `load_from_hub`. The function uses the `hf_hub_download` utility to download a model file given a `repo_id` and a `filename`. From the code examples provided, the filename used when downloading a model is consistently 'q-learning.pkl', as seen in the two instances of the function call `load_from_hub` with different `repo_id` values but the same filename parameter. This makes the question clearly answerable from the context, as the filename is explicitly stated in the code.",3,"This question is somewhat relevant to machine learning developers using the Hugging Face ecosystem, as it pertains to downloading models from the Hugging Face Hub, a common task in NLP application development. However, the question is quite specific and lacks broader applicability. Knowing the filename is useful in limited scenarios, such as when scripting downloads or organizing files, but it does not delve into more critical aspects of model usage, such as loading, fine-tuning, or deployment. Additionally, the filename can vary based on the model and its configuration, and developers often interact with these models programmatically through libraries like Transformers, which abstract away such details. Thus, while it has some utility, it is not a highly impactful question in the context of developing NLP applications.",4,"The question, 'What is the filename used when downloading the model from the Hugging Face Hub?' is mostly context-independent. It asks for specific information regarding the downloading process of a model from the Hugging Face Hub. While it does assume some familiarity with the Hugging Face Hub and the general concept of downloading models, these are common terms within the field of machine learning and AI. Thus, the question is clear and understandable to anyone familiar with these domains, without needing additional context or information to make sense."
"## ViTHybridConfig

[[autodoc]] ViTHybridConfig

## ViTHybridImageProcessor

[[autodoc]] ViTHybridImageProcessor
    - preprocess

## ViTHybridModel

[[autodoc]] ViTHybridModel
    - forward

## ViTHybridForImageClassification

[[autodoc]] ViTHybridForImageClassification
    - forward",What method is used by ViTHybridImageProcessor?,preprocess,huggingface/transformers/blob/main/docs/source/en/model_doc/vit_hybrid.md,4,"The context provided lists several components of a system related to ViTHybrid, namely ViTHybridConfig, ViTHybridImageProcessor, ViTHybridModel, and ViTHybridForImageClassification. However, the only method mentioned for the ViTHybridImageProcessor is 'preprocess'. While this suggests that 'preprocess' is a method used by ViTHybridImageProcessor, it is the only method mentioned, and there is no additional information that clearly indicates whether other methods exist or any details about what 'preprocess' does. Therefore, while the question can be answered with the context provided, the answer is based on limited information. The context allows us to identify 'preprocess' as a method of ViTHybridImageProcessor, but it lacks detail and confirmation if it is the only method or how it functions, which makes the answer somewhat ambiguous.",2,"The question asks about the method used by 'ViTHybridImageProcessor', which is a specific term likely related to a particular model or approach in the Hugging Face ecosystem. However, the question lacks context and detail. It doesn't specify what aspect of the method is being inquired about (e.g., its purpose, implementation, or effect) or what the user intends to achieve by knowing this method. Additionally, 'ViTHybridImageProcessor' seems to relate more to computer vision than NLP, unless it specifically integrates with NLP models somehow. Without further context, it's challenging to determine the relevance to NLP applications, which makes it less useful for the intended audience.",4,"The question asks about a specific method used by the 'ViTHybridImageProcessor'. While the question is clear in its request—seeking information about a specific method—it assumes that the reader knows what 'ViTHybridImageProcessor' is and has access to related documentation. However, it doesn't rely on any additional context or make reference to specific scenarios or documents. Therefore, it is largely context-independent and can be understood by someone with the right background knowledge about the subject."
"by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3428](https://github.com/gradio-app/gradio/pull/3428)

###### Code component 🦾

New code component allows you to enter, edit and display code with full syntax highlighting by [@pngwn](https://github.com/pngwn) in [PR 3421](https://github.com/gradio-app/gradio/pull/3421)

###### The `Chatbot` component now supports audio, video, and images

The `Chatbot` component now supports audio, video, and images with a simple syntax: simply
pass in a tuple with the URL or filepath (the second optional element of the tuple is alt text), and the image/audio/video will be displayed:

```python
gr.Chatbot([
    ((""driving.mp4"",), ""cool video""),
    ((""cantina.wav"",), ""cool audio""),
    ((""lion.jpg"", ""A lion""), ""cool pic""),
]).style(height=800)
```

<img width=""1054"" alt=""image"" src=""https://user-images.githubusercontent.com/1778297/224116682-5908db47-f0fa-405c-82ab-9c7453e8c4f1.png"">

Note: images were previously supported via Markdown syntax and that is still supported for backwards compatibility. By [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3413](https://github.com/gradio-app/gradio/pull/3413)

- Allow consecutive function triggers with `.then` and `.success` by [@aliabid94](https://github.com/aliabid94) in [PR 3430](https://github.com/gradio-app/gradio/pull/3430)

- New code component allows you to enter, edit and display code with full syntax highlighting by [@pngwn](https://github.com/pngwn) in [PR 3421](https://github.com/gradio-app/gradio/pull/3421)

![](https://user-images.githubusercontent.com/12937446/224116643-5cfb94b3-93ce-43ee-bb7b-c25c3b66e0a1.png)

- Added the `.select()` event listener, which also includes event data that can be passed as an argument to a function with type hint `gr.SelectData`. The following components support the `.select()` event listener: Chatbot, CheckboxGroup, Dataframe, Dropdown, File, Gallery, HighlightedText, Label, Radio, TabItem, Tab, Textbox. Example usage:",Who contributed the code component for syntax highlighting in Gradio?,@pngwn,gradio-app/gradio/blob/main/CHANGELOG.md,5,"The context explicitly mentions that the new code component for syntax highlighting was contributed by [@pngwn](https://github.com/pngwn) in PR 3421. The question asks for the contributor of the code component for syntax highlighting, which is directly answered by the context. There is no ambiguity or missing information regarding the contributor, making the answer clear and straightforward.",2,"The question seeks to identify a specific contributor to the Gradio project, focusing on the syntax highlighting feature. While this question might be relevant to someone interested in the history or development details of Gradio, it is not directly useful for machine learning developers building NLP applications with the Hugging Face ecosystem. Such developers are more likely to be interested in how to use Gradio with NLP models, its integration capabilities, or troubleshooting common issues. Therefore, the question has limited utility for the target audience.",4,"The question is specific and requires knowledge about Gradio's codebase or contributors. It does not refer to any particular document, setting, or context implicitly, making it reasonably clear and specific. However, it requires external information (like a contribution log or a repository history) to answer, which means it is not entirely context-independent. It stands alone as a question but requires context (in the form of additional resources) for a complete answer."
"!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Prompt weighting

[[open-in-colab]]

Prompt weighting provides a way to emphasize or de-emphasize certain parts of a prompt, allowing for more control over the generated image. A prompt can include several concepts, which gets turned into contextualized text embeddings. The embeddings are used by the model to condition its cross-attention layers to generate an image (read the Stable Diffusion [blog post](https://huggingface.co/blog/stable_diffusion) to learn more about how it works).","Where can you find the Apache License, Version 2.0?",http://www.apache.org/licenses/LICENSE-2.0,huggingface/diffusers/blob/main/docs/source/en/using-diffusers/weighted_prompts.md,5,"The context provides a direct answer to the question by stating the URL where the Apache License, Version 2.0 can be found. The context includes the specific link: 'http://www.apache.org/licenses/LICENSE-2.0,' which is unambiguous and directly answers the question without any need for inference or interpretation. This link is clearly highlighted in the context in relation to the Apache License, making it easy for anyone reading to understand where to find the document specified by the question.",2,"This question is not directly relevant to machine learning developers building NLP applications with Hugging Face. The Apache License, Version 2.0 is a type of software license, and while it's important for developers to understand licensing, this question is more about general software licensing knowledge rather than specific to the Hugging Face ecosystem or NLP applications. Therefore, it doesn't provide significant value or insight for those focused on building with these specific tools and technologies.",5,"The question ""Where can you find the Apache License, Version 2.0?"" is quite context-independent. It directly asks for the location or source where this specific license can be found. This is a factual question that can be answered with a specific URL or resource reference. The Apache License, Version 2.0 is a well-known open-source license, and information about it is widely available on the internet. Therefore, the question can be understood and answered without additional context."
"## Natural language processing

NLP tasks are among the most common types of tasks because text is such a natural way for us to communicate. To get text into a format recognized by a model, it needs to be tokenized. This means dividing a sequence of text into separate words or subwords (tokens) and then converting these tokens into numbers. As a result, you can represent a sequence of text as a sequence of numbers, and once you have a sequence of numbers, it can be input into a model to solve all sorts of NLP tasks!

### Text classification

Like classification tasks in any modality, text classification labels a sequence of text (it can be sentence-level, a paragraph, or a document) from a predefined set of classes. There are many practical applications for text classification, some of which include:

* sentiment analysis: label text according to some polarity like `positive` or `negative` which can inform and support decision-making in fields like politics, finance, and marketing
* content classification: label text according to some topic to help organize and filter information in news and social media feeds (`weather`, `sports`, `finance`, etc.)

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task=""sentiment-analysis"")
>>> preds = classifier(""Hugging Face is the best thing since sliced bread!"")
>>> preds = [{""score"": round(pred[""score""], 4), ""label"": pred[""label""]} for pred in preds]
>>> preds
[{'score': 0.9991, 'label': 'POSITIVE'}]
```

### Token classification

In any NLP task, text is preprocessed by separating the sequence of text into individual words or subwords. These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. 

Two common types of token classification are:",What tool can be used for sentiment analysis in NLP?,transformers pipeline,huggingface/transformers/blob/main/docs/source/en/task_summary.md,5,"The context provides a detailed explanation of sentiment analysis as a text classification task in NLP and even includes a Python code snippet using the 'transformers' library from Hugging Face to perform sentiment analysis. The specific function used for sentiment analysis is the 'pipeline' with the task specified as 'sentiment-analysis'. This directly answers the question, providing a tool (the Hugging Face transformers library) and the method to use for sentiment analysis. The context is clear and directly related to the question, making it easily answerable without ambiguity.",4,"The question 'What tool can be used for sentiment analysis in NLP?' is quite useful for machine learning developers, especially those who are new to NLP and the Hugging Face ecosystem. It directly addresses a common task in NLP—sentiment analysis—and seeks information about tools to accomplish this task. The question is general enough to allow for a wide range of potential answers, such as libraries, frameworks, or specific models, making it highly relevant to anyone looking to implement sentiment analysis. However, to make it more useful, the question could be more specific by asking about tools within the Hugging Face ecosystem, which would guide the developers more effectively. Overall, it is a useful question that could lead to valuable insights.",5,The question 'What tool can be used for sentiment analysis in NLP?' is context-independent and clear. It does not require additional information to be understood and can be answered with general knowledge about NLP tools. The question refers to a broad and common task in Natural Language Processing (NLP) and asks for a tool that can perform this task. It is clear and straightforward for anyone with a basic understanding of NLP or access to relevant resources.
"1. **[Graphormer](https://huggingface.co/docs/transformers/model_doc/graphormer)** (from Microsoft) released with the paper [Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234) by Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.
1. **[GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit)** (from UCSD, NVIDIA) released with the paper [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.
1. **[HerBERT](https://huggingface.co/docs/transformers/model_doc/herbert)** (from Allegro.pl, AGH University of Science and Technology) released with the paper [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.
1. **[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)** (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.
1. **[I-BERT](https://huggingface.co/docs/transformers/model_doc/ibert)** (from Berkeley) released with the paper [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.",Who are the authors of the paper associated with Graphormer?,"Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.",huggingface/transformers/blob/main/README_ru.md,5,"The context provides a direct and clear answer to the question of who the authors of the paper associated with Graphormer are. It mentions, ""Do Transformers Really Perform Bad for Graph Representation?"" by Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu. This information is unambiguous and directly responds to the question asked.",3,"This question is somewhat useful to machine learning developers working with the Hugging Face ecosystem, particularly if they are interested in understanding the theoretical foundations or contributions behind certain models, such as Graphormer. Knowing the authors can help developers locate additional resources, papers, or follow the work of specific researchers for further insights. However, it is not directly related to the practical aspects of building or deploying NLP applications using Hugging Face tools. Thus, while it provides some context, it does not directly assist in practical development or deployment tasks.",4,"The question ""Who are the authors of the paper associated with Graphormer?"" is context-independent in terms of clarity and focus. It asks specifically for the authors of a particular paper related to Graphormer. While it assumes knowledge of what Graphormer is, it doesn't rely on any external context or document to be understood. The question is precise and can be answered by finding the relevant paper or looking up information about Graphormer. Thus, it is clear and self-contained for someone familiar with academic papers or the Graphormer model."
"## Computing a device map

Before we start loading the pretrained weights, we will need to know where we want to put them. This way we can free the CPU RAM each time we have put a weight in its right place. This can be done with the empty model on the meta device, since we only need to know the shape of each tensor and its dtype to compute how much space it will take in memory.

Accelerate provides a function to automatically determine a *device map* from an empty model. It will try to maximize the use of all available GPUs, then CPU RAM, and finally flag the weights that don't fit for disk offload. Let's have a look using [OPT-13b](https://huggingface.co/facebook/opt-13b).

```python
from accelerate import infer_auto_device_map, init_empty_weights
from transformers import AutoConfig, AutoModelForCausalLM

config = AutoConfig.from_pretrained(""facebook/opt-13b"")
with init_empty_weights():
    model = AutoModelForCausalLM.from_config(config)

device_map = infer_auto_device_map(model)
```

This will return a dictionary mapping modules or weights to a device. On a machine with one Titan RTX for instance, we get the following:",What function does Accelerate provide to determine a device map?,infer_auto_device_map,huggingface/blog/blob/main/accelerate-large-models.md,5,"The question asks for the specific function provided by Accelerate to determine a device map. The context clearly states that Accelerate provides a function called `infer_auto_device_map` to perform this task. The context explains that this function is used to automatically determine a device map from an empty model, maximizing the use of available GPUs and CPU RAM, and flagging weights that don't fit for disk offload. The provided code snippet also demonstrates the use of this function, reinforcing the clarity of the context in answering the question. Therefore, the context unambiguously provides the answer to the question.",4,"This question is quite useful for machine learning developers working within the Hugging Face ecosystem, particularly those dealing with distributed training or model deployment. The Accelerate library by Hugging Face is designed to simplify these tasks, and knowing the specific function that determines a device map can help developers effectively allocate resources and optimize their models' performance. This kind of question indicates a practical, implementation-focused interest, which is crucial for real-world applications. However, it may not cover broader aspects of using Accelerate, like its integration with other tools or its configuration options. Thus, while the question is specific and relevant, it could be part of a broader set of inquiries needed to fully understand and utilize the library.",5,"The question clearly asks about a specific function provided by Accelerate to determine a device map. This requires knowledge of Accelerate's functionalities and documentation, but it does not depend on any additional context or specific settings. It's a straightforward technical question that can be answered by someone familiar with Accelerate, making it context-independent."
"**Note**: All pipelines have PyTorch's autograd disabled by decorating the `__call__` method with a [`torch.no_grad`](https://pytorch.org/docs/stable/generated/torch.no_grad.html) decorator because pipelines should
not be used for training. If you want to store the gradients during the forward pass, we recommend writing your own pipeline, see also our [community-examples](https://github.com/huggingface/diffusers/tree/main/examples/community)

## Contribution

We are more than happy about any contribution to the officially supported pipelines 🤗. We aspire
all of our pipelines to be  **self-contained**, **easy-to-tweak**, **beginner-friendly** and for **one-purpose-only**.",Why is the __call__ method decorated with torch.no_grad?,Because pipelines should not be used for training.,huggingface/diffusers/blob/main/src/diffusers/pipelines/README.md,5,"The context provides a clear explanation for why the `__call__` method is decorated with `torch.no_grad`. It states that this is done because pipelines are not intended for training, and therefore do not need to store gradients during the forward pass. This directly answers the question by providing the rationale behind the use of the `torch.no_grad` decorator. Furthermore, the context even offers additional resources for users who may want to modify this behavior. Thus, the question is unambiguously answerable with the given context.",4,"This question is quite useful, especially for machine learning developers working with PyTorch and Transformers models in the Hugging Face ecosystem. The `__call__` method is a critical component of model classes and understanding why it might be decorated with `torch.no_grad` is important. This decorator is used to disable gradient tracking, which is essential for reducing memory usage and improving inference speed during evaluation. Therefore, developers need to understand its significance to optimize their models effectively. Overall, the question aids in understanding performance optimization and resource management in model inference, key aspects of deploying efficient NLP applications.",5,"The question is asking about a specific technical method, __call__, and its use in conjunction with a specific PyTorch context manager, torch.no_grad. While it assumes some familiarity with PyTorch (a machine learning library), it is a direct and clear technical question that should be understandable to someone with knowledge in the field. It doesn't rely on any additional context or external documents to be understood, as it asks for the reasoning behind a specific coding practice or pattern within PyTorch. Therefore, the question is context-independent for those familiar with the topic."
"| absolute values               | Latency | Memory  | Throghput |
|-------------------------------|---------|---------|-----------|
| base case (bettertransformer) |   19.26 | 8329.2M |      0.42 |
| + fp16                          |   10.32 | 4198.8M |      0.78 |
| + offload                       |   20.46 | 5172.1M |      0.39 |
| + offload + fp16                |   10.91 | 2619.5M |      0.73 |

| Relative value                | Latency | Memory | Throughput |
|-------------------------------|---------|--------|------------|
| + base case (bettertransformer) |      0% |     0% |         0% |
| + fp16                          |    -46% |   -50% |        87% |
| + offload                       |      6% |   -38% |        -6% |
| + offload + fp16                |    -43% |   -69% |       77% |

### Comment

This is where we can see the potential of combining all three optimization features!

The impact of `fp16` on latency is less marked with `batch_size = 1`, but here it is of enormous interest as it can reduce latency by almost half, and almost double throughput!

# Concluding remarks

This blog post showcased a few simple optimization tricks bundled in the 🤗 ecosystem. Using anyone of these techniques, or a combination of all three, can greatly improve Bark inference speed and memory footprint.

* You can use the large version of Bark without any performance degradation and a footprint of just 2GB instead of 5GB, 15% faster, **using 🤗 Better Transformer and CPU offload**.


* Do you prefer high throughput? **Batch by 8 with 🤗 Better Transformer and half-precision**.


* You can get the best of both worlds by using **fp16, 🤗 Better Transformer and CPU offload**!",What is the latency in milliseconds with fp16 optimization?,10.32,huggingface/blog/blob/main/optimizing-bark.md,5,"The context provides a table with the latency values for different optimization scenarios, including one specifically for fp16 optimization. The table clearly shows that the latency with fp16 optimization is 10.32 milliseconds, as found in the row labeled ""+ fp16"" under the ""Latency"" column. This information is presented in a straightforward manner, allowing the question to be answered directly and unambiguously.",3,"This question targets a specific aspect of performance optimization in machine learning models, particularly with regards to fp16 (half precision floating point) optimization. Understanding and measuring latency is crucial for developers who want to deploy efficient models, especially in real-time applications where response times are critical. However, the question lacks context, such as the model being used, the hardware specifications, and the specific conditions under which latency is measured. Adding these details would make the question more actionable and informative for developers using the Hugging Face ecosystem.",4,"The question 'What is the latency in milliseconds with fp16 optimization?' is fairly context-independent. It is clear that it asks for a specific metric (latency) in relation to a specific optimization technique (fp16, or 16-bit floating point optimization). The question is technical and assumes the reader has an understanding of what fp16 optimization is, but it does not depend on any specific external document or context to be understood. Therefore, it could be understood by someone familiar with computing or machine learning contexts where such optimization is relevant, and they could look up or calculate the answer independently."
"分享您的应用

如何分享您的 Gradio 应用：

1. [使用 share 参数分享演示](#sharing-demos)
2. [在 HF Spaces 上托管](#hosting-on-hf-spaces)
3. [嵌入托管的空间](#embedding-hosted-spaces)
4. [使用 Web 组件嵌入](#embedding-with-web-components)
5. [使用 API 页面](#api-page)
6. [在页面上添加身份验证](#authentication)
7. [访问网络请求](#accessing-the-network-request-directly)
8. [在 FastAPI 中挂载](#mounting-within-another-fastapi-app)
9. [安全性](#security-and-file-access)

## 分享演示

通过在 `launch()` 方法中设置 `share=True`，可以轻松公开分享 Gradio 演示。就像这样：

```python
demo.launch(share=True)
```

这将生成一个公开的可分享链接，您可以将其发送给任何人！当您发送此链接时，对方用户可以在其浏览器中尝试模型。因为处理过程发生在您的设备上（只要您的设备保持开启！），您不必担心任何打包依赖项的问题。一个分享链接通常看起来像这样：**XXXXX.gradio.app**。尽管链接是通过 Gradio URL 提供的，但我们只是您本地服务器的代理，并不会存储通过您的应用发送的任何数据。

但请记住，这些链接可以被公开访问，这意味着任何人都可以使用您的模型进行预测！因此，请确保不要通过您编写的函数公开任何敏感信息，也不要允许在您的设备上进行任何关键更改。如果您设置 `share=False`（默认值，在 colab 笔记本中除外），则只创建一个本地链接，可以通过[端口转发](https://www.ssh.com/ssh/tunneling/example)与特定用户共享。

<img style=""width: 40%"" src=""/assets/guides/sharing.svg"">

分享链接在 72 小时后过期。

## 在 HF Spaces 上托管

如果您想在互联网上获得您的 Gradio 演示的永久链接，请使用 Hugging Face Spaces。 [Hugging Face Spaces](http://huggingface.co/spaces/) 提供了免费托管您的机器学习模型的基础设施！

在您创建了一个免费的 Hugging Face 账户后，有三种方法可以将您的 Gradio 应用部署到 Hugging Face Spaces：

1. 从终端：在应用目录中运行 `gradio deploy`。CLI 将收集一些基本元数据，然后启动您的应用。要更新您的空间，可以重新运行此命令或启用 Github Actions 选项，在 `git push` 时自动更新 Spaces。
2. 从浏览器：将包含 Gradio 模型和所有相关文件的文件夹拖放到 [此处](https://huggingface.co/new-space)。
3. 将 Spaces 与您的 Git 存储库连接，Spaces 将从那里拉取 Gradio 应用。有关更多信息，请参阅 [此指南如何在 Hugging Face Spaces 上托管](https://huggingface.co/blog/gradio-spaces)。

<video autoplay muted loop>
  <source src=""/assets/guides/hf_demo.mp4"" type=""video/mp4"" />
</video>

## 嵌入托管的空间

一旦您将应用托管在 Hugging Face Spaces（或您自己的服务器上），您可能希望将演示嵌入到不同的网站上，例如您的博客或个人作品集。嵌入交互式演示使人们可以在他们的浏览器中尝试您构建的机器学习模型，而无需下载或安装任何内容！最好的部分是，您甚至可以将交互式演示嵌入到静态网站中，例如 GitHub 页面。

有两种方法可以嵌入您的 Gradio 演示。您可以在 Hugging Face Space 页面的“嵌入此空间”下拉选项中直接找到这两个选项的快速链接：

![嵌入此空间下拉选项](/assets/guides/embed_this_space.png)

### 使用 Web 组件嵌入",How can you share a Gradio demo publicly?,Set `share=True` in the `launch()` method of the Gradio demo.,gradio-app/gradio/blob/main/guides/cn/01_getting-started/03_sharing-your-app.md,5,"The question 'How can you share a Gradio demo publicly?' is clearly answerable with the given context. The context provides detailed steps on how to share a Gradio demo publicly by using the 'share' parameter in the 'launch()' method of Gradio to generate a public link, as well as alternative methods like hosting on HF Spaces. The instructions are specific and include code examples and descriptions of the process. Therefore, the context contains all the necessary information to unambiguously answer the question.",4,"This question is practical and directly related to using Gradio, a tool often employed in the Hugging Face ecosystem for creating and sharing machine learning demos. Knowing how to share a Gradio demo publicly is important for developers who want to showcase their models and applications, receive feedback, and collaborate with others. The question is specific and clear, making it relevant for developers looking to expand the reach of their NLP applications through public demos. However, it could be more comprehensive by addressing potential issues or considerations when sharing demos publicly.",5,"This question is clear and understandable without requiring any specific additional context. It directly asks about the method to share a Gradio demo publicly. The term 'Gradio demo' is specific, but it is a known concept in the context of Gradio's functionalities, and the question clearly focuses on the 'how' aspect of sharing it. Thus, the question is self-contained and context-independent."
"<!--
Type: model-index
Collections:
- Name: SWSL ResNet
  Paper:
    Title: Billion-scale semi-supervised learning for image classification
    URL: https://paperswithcode.com/paper/billion-scale-semi-supervised-learning-for
Models:
- Name: swsl_resnet18
  In Collection: SWSL ResNet
  Metadata:
    FLOPs: 2337073152
    Parameters: 11690000
    File Size: 46811375
    Architecture:
    - 1x1 Convolution
    - Batch Normalization
    - Bottleneck Residual Block
    - Convolution
    - Global Average Pooling
    - Max Pooling
    - ReLU
    - Residual Block
    - Residual Connection
    - Softmax
    Tasks:
    - Image Classification
    Training Techniques:
    - SGD with Momentum
    - Weight Decay
    Training Data:
    - IG-1B-Targeted
    - ImageNet
    Training Resources: 64x GPUs
    ID: swsl_resnet18
    LR: 0.0015
    Epochs: 30
    Layers: 18
    Crop Pct: '0.875'
    Batch Size: 1536
    Image Size: '224'
    Weight Decay: 0.0001
    Interpolation: bilinear
  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/resnet.py#L954
  Weights: https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnet18-118f1556.pth
  Results:
  - Task: Image Classification
    Dataset: ImageNet
    Metrics:
      Top 1 Accuracy: 73.28%
      Top 5 Accuracy: 91.76%
- Name: swsl_resnet50
  In Collection: SWSL ResNet
  Metadata:
    FLOPs: 5282531328
    Parameters: 25560000
    File Size: 102480594
    Architecture:
    - 1x1 Convolution
    - Batch Normalization
    - Bottleneck Residual Block
    - Convolution
    - Global Average Pooling
    - Max Pooling
    - ReLU
    - Residual Block
    - Residual Connection
    - Softmax
    Tasks:
    - Image Classification
    Training Techniques:
    - SGD with Momentum
    - Weight Decay
    Training Data:
    - IG-1B-Targeted
    - ImageNet
    Training Resources: 64x GPUs
    ID: swsl_resnet50
    LR: 0.0015
    Epochs: 30
    Layers: 50",What is the Top 1 Accuracy of swsl_resnet18 on ImageNet?,73.28%,huggingface/pytorch-image-models/blob/main/hfdocs/source/models/swsl-resnet.mdx,5,"The context contains detailed information about the `swsl_resnet18` model, including its architecture, training techniques, data, resources, and specific results on the ImageNet dataset. The Top 1 Accuracy of the model on ImageNet is clearly stated as 73.28% under the 'Results' section. The context provides a direct answer to the question, making it clearly and unambiguously answerable. There is no ambiguity or missing information regarding the Top 1 Accuracy of the `swsl_resnet18` model on ImageNet.",2,"The question asks for the Top 1 Accuracy of a specific model, 'swsl_resnet18', on the ImageNet dataset. While this question is very specific, it is not directly useful for machine learning developers focusing on NLP applications, particularly those using the Hugging Face ecosystem, which is more centered around natural language processing rather than computer vision tasks. However, it might be of some indirect interest to developers who are also working on multi-modal tasks or exploring model performance benchmarks across different types of data. Overall, the question's relevance to the primary audience of Hugging Face's NLP community is limited.",5,"The question asks for a specific metric (Top 1 Accuracy) of a specified model (swsl_resnet18) on a well-known dataset (ImageNet). The question is clear and can be understood on its own without requiring additional context or information. An operator with access to the relevant documentation or resources can find the answer. Therefore, it is context-independent."
"SWSL ResNet

**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack [residual blocks](https://paperswithcode.com/method/residual-block) ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. 

The models in this collection utilise semi-weakly supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. 

Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only.

## How do I use this model on an image?

To load a pretrained model:

```py
>>> import timm
>>> model = timm.create_model('swsl_resnet18', pretrained=True)
>>> model.eval()
```

To load and preprocess the image:

```py 
>>> import urllib
>>> from PIL import Image
>>> from timm.data import resolve_data_config
>>> from timm.data.transforms_factory import create_transform

>>> config = resolve_data_config({}, model=model)
>>> transform = create_transform(**config)

>>> url, filename = (""https://github.com/pytorch/hub/raw/master/images/dog.jpg"", ""dog.jpg"")
>>> urllib.request.urlretrieve(url, filename)
>>> img = Image.open(filename).convert('RGB')
>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension
```

To get the model predictions:

```py
>>> import torch
>>> with torch.no_grad():
...     out = model(tensor)
>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)
>>> print(probabilities.shape)
>>> # prints: torch.Size([1000])
```

To get the top-5 predictions class names:",What is the license for the weights of the SWSL ResNet models?,CC-BY-NC 4.0 license,huggingface/pytorch-image-models/blob/main/hfdocs/source/models/swsl-resnet.mdx,5,"The context provides a specific mention of the license for the weights of the SWSL ResNet models, stating ""Please note the CC-BY-NC 4.0 license on these weights, non-commercial use only."" This directly answers the question about the license type, indicating that the weights are under a CC-BY-NC 4.0 license, which restricts usage to non-commercial purposes. The information is clear and unambiguous, making it easy to answer the question with certainty. Therefore, the context fully supports answering the question with precision.",4,"The question is specific and directly relevant to developers who are interested in using or contributing to models in the Hugging Face ecosystem. Knowing the license of the weights for a model like SWSL ResNet is crucial for understanding the legal and ethical implications of using the model, such as whether it can be used for commercial purposes or modified. This information is particularly useful for developers who need to ensure compliance with licensing terms when integrating models into their applications. Thus, this question is valuable for developers concerned with legal and compliance aspects of using models in the Hugging Face ecosystem.",4,"The question is quite specific and does not directly depend on any particular context being provided to make sense. It clearly asks about the licensing of a particular model's weights, which is a well-defined query. However, the question assumes the reader knows what SWSL ResNet models are, though this does not make it context-dependent. It stands as an independent question regarding licensing, which can likely be answered by referring to documentation or licensing information specific to those models."
"!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Consistency Models

Consistency Models were proposed in [Consistency Models](https://huggingface.co/papers/2303.01469) by Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.

The abstract from the paper is:",Who proposed Consistency Models?,"Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever",huggingface/diffusers/blob/main/docs/source/en/api/pipelines/consistency_models.md,5,"The context provided clearly answers the question of who proposed Consistency Models. It lists the names of the proposers: Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. The reference to the paper provides additional credibility and context to the answer. Therefore, this context is perfectly suitable for answering the question unambiguously.",2,"The question 'Who proposed Consistency Models?' is quite specific and factual. It is potentially useful for machine learning developers, especially those interested in the historical context or the development of specific models in the NLP field. However, the question is not directly related to the practical aspects of utilizing the Hugging Face ecosystem for building NLP applications. Developers would generally be more interested in questions about model implementation, fine-tuning, integration, or performance rather than the history or origin of a particular model concept. Unless the identity of the person who proposed Consistency Models is crucial for understanding or evaluating those models, this question is of limited practical utility for most developers. Thus, while informative, it does not significantly aid in the practical development process within the Hugging Face ecosystem.",5,"The question 'Who proposed Consistency Models?' is quite clear and context-independent. It asks for the name of the individual or group that introduced the concept of Consistency Models. There is no specific reference to a document, setting, or context that is needed to understand the question. An expert or someone familiar with the topic could provide an answer without needing additional context. Therefore, the question stands alone effectively and is easy to interpret on its own."
"Weights: https://download.pytorch.org/models/resnet101-5d3b4d8f.pth
  Results:
  - Task: Image Classification
    Dataset: ImageNet
    Metrics:
      Top 1 Accuracy: 77.37%
      Top 5 Accuracy: 93.56%
- Name: tv_resnet152
  In Collection: ResNet
  Metadata:
    FLOPs: 14857660416
    Parameters: 60190000
    File Size: 241530880
    Architecture:
    - 1x1 Convolution
    - Batch Normalization
    - Bottleneck Residual Block
    - Convolution
    - Global Average Pooling
    - Max Pooling
    - ReLU
    - Residual Block
    - Residual Connection
    - Softmax
    Tasks:
    - Image Classification
    Training Techniques:
    - SGD with Momentum
    - Weight Decay
    Training Data:
    - ImageNet
    ID: tv_resnet152
    LR: 0.1
    Epochs: 90
    Crop Pct: '0.875'
    LR Gamma: 0.1
    Momentum: 0.9
    Batch Size: 32
    Image Size: '224'
    LR Step Size: 30
    Weight Decay: 0.0001
    Interpolation: bilinear
  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/resnet.py#L769
  Weights: https://download.pytorch.org/models/resnet152-b121ed2d.pth
  Results:
  - Task: Image Classification
    Dataset: ImageNet
    Metrics:
      Top 1 Accuracy: 78.32%
      Top 5 Accuracy: 94.05%
- Name: tv_resnet34
  In Collection: ResNet
  Metadata:
    FLOPs: 4718469120
    Parameters: 21800000
    File Size: 87306240
    Architecture:
    - 1x1 Convolution
    - Batch Normalization
    - Bottleneck Residual Block
    - Convolution
    - Global Average Pooling
    - Max Pooling
    - ReLU
    - Residual Block
    - Residual Connection
    - Softmax
    Tasks:
    - Image Classification
    Training Techniques:
    - SGD with Momentum
    - Weight Decay
    Training Data:
    - ImageNet
    ID: tv_resnet34
    LR: 0.1
    Epochs: 90
    Crop Pct: '0.875'
    LR Gamma: 0.1
    Momentum: 0.9
    Batch Size: 32
    Image Size: '224'
    LR Step Size: 30
    Weight Decay: 0.0001
    Interpolation: bilinear",What is the Top 1 Accuracy of tv_resnet152 on ImageNet?,78.32%,huggingface/pytorch-image-models/blob/main/docs/models/resnet.md,5,"The context provides detailed information on the performance metrics for different versions of the ResNet models, specifically including the Top 1 Accuracy for the tv_resnet152 model. The question asks for the Top 1 Accuracy of tv_resnet152 on ImageNet, and the context clearly states it as 78.32% under the 'Results' section for the tv_resnet152 model. This information is unambiguous and directly answers the question.

Additionally, the context differentiates between various ResNet models by providing details such as FLOPs, parameters, and specific accuracy metrics, which helps in avoiding any confusion regarding different model versions. Therefore, the context perfectly supports the question, allowing it to be answered without any ambiguity.",2,"The question asks for the Top 1 Accuracy of a specific model (tv_resnet152) on a specific dataset (ImageNet), which is relevant for those interested in using pre-trained models in the Hugging Face ecosystem. However, this question is more centered around computer vision rather than NLP applications, which is the primary focus of machine learning developers using the Hugging Face ecosystem. While understanding model accuracy is important, this specific question is not directly applicable to NLP tasks or Hugging Face's typical use cases, which limits its usefulness to NLP developers.",5,This question is context-independent because it clearly asks about a specific metric (Top 1 Accuracy) of a specific model (tv_resnet152) on a well-known dataset (ImageNet). Anyone familiar with these terms or with access to relevant documentation will be able to understand what is being asked without needing any additional context or information.
"!---
Copyright 2021 The HuggingFace Team. All rights reserved.
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Examples

This folder contains actively maintained examples of the use of 🤗 Transformers organized into different ML tasks. All examples in this folder are **TensorFlow** examples and are written using native Keras rather than classes like `TFTrainer`, which we now consider deprecated. If you've previously only used 🤗 Transformers via `TFTrainer`, we highly recommend taking a look at the new style - we think it's a big improvement!

In addition, all scripts here now support the [🤗 Datasets](https://github.com/huggingface/datasets) library - you can grab entire datasets just by changing one command-line argument!

## A note on code folding

Most of these examples have been formatted with #region blocks. In IDEs such as PyCharm and VSCode, these blocks mark
named regions of code that can be folded for easier viewing. If you find any of these scripts overwhelming or difficult
to follow, we highly recommend beginning with all regions folded and then examining regions one at a time!

## The Big Table of Tasks

Here is the list of all our examples:",Under which license is the HuggingFace Transformers library released?,"Apache License, Version 2.0",huggingface/transformers/blob/main/examples/tensorflow/README.md,5,"The context clearly states that the HuggingFace Transformers library is licensed under the Apache License, Version 2.0. This information is directly extracted from the context, which explicitly mentions the license type. Therefore, the question is fully answerable with the provided context, as it gives a precise and unambiguous answer regarding the licensing of the library.",3,"This question is quite relevant for machine learning developers using the Hugging Face ecosystem, as understanding the license under which a library is released is crucial for compliance and legal considerations. The Hugging Face Transformers library is widely used in NLP applications, and knowing its license helps developers know about usage rights, distribution, and modification permissions. However, the question is quite specific and factual, so once answered, it might not provide ongoing utility or learning for developers. It doesn't directly address technical challenges or implementation details that developers might face, so while important, it's not highly impactful for development itself.",5,"The question is clear and specific, asking for the type of license under which the HuggingFace Transformers library is released. It does not rely on any external context or prior information to be understood. Even though it refers to a specific library, the question is direct and complete on its own."
"## GPTSAN Features

GPTSAN has some unique features. It has a model structure of Prefix-LM. It works as a shifted Masked Language Model for Prefix Input tokens. Un-prefixed inputs behave like normal generative models.
The Spout vector is a GPTSAN specific input. Spout is pre-trained with random inputs, but you can specify a class of text or an arbitrary vector during fine-tuning. This allows you to indicate the tendency of the generated text.
GPTSAN has a sparse Feed Forward based on Switch-Transformer. You can also add other layers and train them partially. See the original GPTSAN repository for details.

### Prefix-LM Model

GPTSAN has the structure of the model named Prefix-LM in the `T5` paper. (The original GPTSAN repository calls it `hybrid`)
In GPTSAN, the `Prefix` part of Prefix-LM, that is, the input position that can be referenced by both tokens, can be specified with any length.
Arbitrary lengths can also be specified differently for each batch.
This length applies to the text entered in `prefix_text` for the tokenizer.
The tokenizer returns the mask of the `Prefix` part of Prefix-LM as `token_type_ids`.
The model treats the part where `token_type_ids` is 1 as a `Prefix` part, that is, the input can refer to both tokens before and after.

## Usage tips

Specifying the Prefix part is done with a mask passed to self-attention.
When token_type_ids=None or all zero, it is equivalent to regular causal mask

for example:

>>> x_token = tokenizer(""ｱｲｳｴ"")
input_ids:      | SOT | SEG | ｱ | ｲ | ｳ | ｴ |
token_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |
prefix_lm_mask:
SOT | 1 0 0 0 0 0 |
SEG | 1 1 0 0 0 0 |
ｱ   | 1 1 1 0 0 0 |
ｲ   | 1 1 1 1 0 0 |
ｳ   | 1 1 1 1 1 0 |
ｴ   | 1 1 1 1 1 1 |

>>> x_token = tokenizer("""", prefix_text=""ｱｲｳｴ"")
input_ids:      | SOT | ｱ | ｲ | ｳ | ｴ | SEG |
token_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |
prefix_lm_mask:
SOT | 1 1 1 1 1 0 |
ｱ   | 1 1 1 1 1 0 |
ｲ   | 1 1 1 1 1 0 |
ｳ   | 1 1 1 1 1 0 |
ｴ   | 1 1 1 1 1 0 |
SEG | 1 1 1 1 1 1 |",What is the model structure of GPTSAN?,Prefix-LM,huggingface/transformers/blob/main/docs/source/en/model_doc/gptsan-japanese.md,5,"The question asks about the model structure of GPTSAN, and the provided context gives a detailed description of this aspect. It clearly states that GPTSAN has a model structure of Prefix-LM, which is central to the question. The context elaborates on how the Prefix-LM model works, including details on how the input is processed and the role of the tokenizer and masks. Additionally, it makes references to the T5 paper for further understanding. Given these elements, the context provides a comprehensive and unambiguous answer to the question about the model structure of GPTSAN.",3,"The question asks about the model structure of GPTSAN, which appears to be a specific NLP model. Understanding the architecture of such models is crucial for developers working with or building upon existing models in the Hugging Face ecosystem. This question is potentially useful if GPTSAN is a recognized model within the community or if it's available in the Hugging Face Model Hub. However, if GPTSAN is not widely recognized or is a niche/academic model not available in the Hub, the utility of this question decreases significantly for most developers. Additionally, without context or additional information about GPTSAN, it may be difficult for developers to directly apply this knowledge in practice. Therefore, the utility of this question is limited without further context or availability of GPTSAN within the Hugging Face ecosystem.",5,"The question 'What is the model structure of GPTSAN?' is fairly context-independent. It specifies a particular model, GPTSAN, and asks for details about its structure. While it may require specific knowledge about GPTSAN, the question itself is clear and does not depend on any additional context to be understood. Anyone familiar with model structures or with access to relevant documentation can address this question without needing further information or context. Therefore, it makes sense by itself."
"## LCM LoRAs and Models Released Today

- [Latent Consistency Models LoRAs Collection](https://huggingface.co/collections/latent-consistency/latent-consistency-models-loras-654cdd24e111e16f0865fba6)
    - [`latent-consistency/lcm-lora-sdxl`](https://huggingface.co/latent-consistency/lcm-lora-sdxl). LCM LoRA for [SDXL 1.0 base](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0), as seen in the examples above.
    - [`latent-consistency/lcm-lora-sdv1-5`](https://huggingface.co/latent-consistency/lcm-lora-sdv1-5). LCM LoRA for [Stable Diffusion 1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5).
    - [`latent-consistency/lcm-lora-ssd-1b`](https://huggingface.co/latent-consistency/lcm-lora-ssd-1b). LCM LoRA for [`segmind/SSD-1B`](https://huggingface.co/segmind/SSD-1B), a distilled SDXL model that's 50% smaller and 60% faster than the original SDXL.

- [`latent-consistency/lcm-sdxl`](https://huggingface.co/latent-consistency/lcm-sdxl). Full fine-tuned consistency model derived from [SDXL 1.0 base](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0).
- [`latent-consistency/lcm-ssd-1b`](https://huggingface.co/latent-consistency/lcm-ssd-1b). Full fine-tuned consistency model derived from [`segmind/SSD-1B`](https://huggingface.co/segmind/SSD-1B).

## Bonus: Combine LCM LoRAs with regular SDXL LoRAs

Using the [diffusers + PEFT integration](https://huggingface.co/docs/diffusers/main/en/tutorials/using_peft_for_inference), you can combine LCM LoRAs with regular SDXL LoRAs, giving them the superpower to run LCM inference in only 4 steps.

Here we are going to combine `CiroN2022/toy_face` LoRA with the LCM LoRA:

```py
from diffusers import DiffusionPipeline, LCMScheduler
import torch

model_id = ""stabilityai/stable-diffusion-xl-base-1.0""
lcm_lora_id = ""latent-consistency/lcm-lora-sdxl""
pipe = DiffusionPipeline.from_pretrained(model_id, variant=""fp16"")
pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)",Which LCM LoRA model is derived from the SDXL 1.0 base?,latent-consistency/lcm-lora-sdxl,huggingface/blog/blob/main/lcm_lora.md,5,"The context clearly lists the LCM LoRA models and their derivations. Specifically, it mentions that the `latent-consistency/lcm-lora-sdxl` is derived from the SDXL 1.0 base. This information directly answers the question without ambiguity. The context provides a direct reference to the model and its base, making it easy to conclusively identify the correct model in relation to the question asked.",3,"This question is quite specific and technical, targeting users who are familiar with both LCM LoRA models and the SDXL 1.0 base. It is useful for machine learning developers who are interested in the specifics of model derivations, particularly those working with LoRA models or the SDXL framework. However, the utility of this question is somewhat niche, as it pertains to a specific use case within the broader Hugging Face ecosystem. Developers who are working directly with these models will find it more relevant than those who are not. Therefore, the question is useful but not broadly applicable to all developers within the Hugging Face ecosystem.",5,"The question asks about a specific LCM LoRA model derived from the SDXL 1.0 base. While it involves technical terms like 'LCM LoRA model' and 'SDXL 1.0 base', it is clear in its request for the name of a model derived from a specific version of a base. There's no reference to a particular document or context needed to understand what is being asked, assuming the reader has knowledge of these technical terms. Therefore, the question is fairly self-contained and specific, making it understandable to those familiar with the subject matter."
"Momentum: 0.9
    Batch Size: 2048
    Image Size: '456'
    Weight Decay: 1.0e-05
    Interpolation: bicubic
    RMSProp Decay: 0.9
    Label Smoothing: 0.1
    BatchNorm Momentum: 0.99
  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1291
  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ra-9a3e5369.pth
  Results:
  - Task: Image Classification
    Dataset: ImageNet
    Metrics:
      Top 1 Accuracy: 83.81%
      Top 5 Accuracy: 96.75%
- Name: tf_efficientnet_b6
  In Collection: TF EfficientNet
  Metadata:
    FLOPs: 24180518488
    Parameters: 43040000
    File Size: 173232007
    Architecture:
    - 1x1 Convolution
    - Average Pooling
    - Batch Normalization
    - Convolution
    - Dense Connections
    - Dropout
    - Inverted Residual Block
    - Squeeze-and-Excitation Block
    - Swish
    Tasks:
    - Image Classification
    Training Techniques:
    - AutoAugment
    - Label Smoothing
    - RMSProp
    - Stochastic Depth
    - Weight Decay
    Training Data:
    - ImageNet
    ID: tf_efficientnet_b6
    LR: 0.256
    Epochs: 350
    Crop Pct: '0.942'
    Momentum: 0.9
    Batch Size: 2048
    Image Size: '528'
    Weight Decay: 1.0e-05
    Interpolation: bicubic
    RMSProp Decay: 0.9
    Label Smoothing: 0.1
    BatchNorm Momentum: 0.99
  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1301
  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_aa-80ba17e4.pth
  Results:
  - Task: Image Classification
    Dataset: ImageNet
    Metrics:
      Top 1 Accuracy: 84.11%
      Top 5 Accuracy: 96.89%
- Name: tf_efficientnet_b7
  In Collection: TF EfficientNet
  Metadata:
    FLOPs: 48205304880
    Parameters: 66349999
    File Size: 266850607
    Architecture:",What is the Top 1 Accuracy for tf_efficientnet_b6 on ImageNet?,84.11%,huggingface/pytorch-image-models/blob/main/hfdocs/source/models/tf-efficientnet.mdx,5,"The context provided contains detailed information about the model `tf_efficientnet_b6`, including its Top 1 Accuracy on the ImageNet dataset, which is 84.11%. This directly answers the question about Top 1 Accuracy for this specific model. Therefore, the context is highly relevant and directly answers the question unambiguously.",2,"The question asks for the Top 1 Accuracy of a specific model, 'tf_efficientnet_b6', on the ImageNet dataset. This is a straightforward request for a performance metric, which is often useful for developers when comparing models and making decisions about which pre-trained models to use as a starting point for transfer learning or other applications. However, this question is more relevant to computer vision tasks rather than natural language processing (NLP) applications. Since the focus is on Hugging Face, which is more known for its NLP capabilities (though it also supports vision models), the direct relevance to NLP is limited. Therefore, while it provides valuable information about model performance, it doesn't directly assist developers working within the Hugging Face ecosystem on NLP tasks.",5,The question asks for a specific performance metric (Top 1 Accuracy) of a specific model (tf_efficientnet_b6) on a well-known dataset (ImageNet). It is clear and context-independent because anyone with knowledge of machine learning models and datasets would understand what is being asked. The question does not depend on any document or particular context to be understood.
"As a result its now trivialized to perform distributed training with Accelerate and keeping as much of the barebones PyTorch code the same as possible.

Earlier it was mentioned that Accelerate also makes the DataLoaders more efficient. This is through custom Samplers that can send parts of the batches automatically to different devices during training allowing for a single copy of the data to be known at one time, rather than four at once into memory depending on the configuration. Along with this, there is only a single full copy of the original dataset in memory total. Subsets of this dataset are split between all of the nodes that are utilized for training, allowing for much larger datasets to be trained on a single instance without an explosion in memory utilized.

### Using the `notebook_launcher`

Earlier it was mentioned you can start distributed code directly out of your Jupyter Notebook. This comes from Accelerate's [`notebook_launcher`](https://huggingface.co/docs/accelerate/v0.12.0/en/basic_tutorials/notebook) utility, which allows for starting multi-gpu training based on code inside of a Jupyter Notebook.

To use it is as trivial as importing the launcher:

```python
from accelerate import notebook_launcher
```

And passing the training function we declared earlier, any arguments to be passed, and the number of processes to use (such as 8 on a TPU, or 2 for two GPUs). Both of the above training functions can be ran, but do note that after you start a single launch, the instance needs to be restarted before spawning another

```python
notebook_launcher(train_ddp, args=(), num_processes=2)
```

Or:

```python
notebook_launcher(train_ddp_accelerate, args=(), num_processes=2)
```

## Using 🤗 Trainer

Finally, we arrive at the highest level of API -- the Hugging Face [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer).",What utility allows starting multi-gpu training in a Jupyter Notebook?,notebook_launcher,huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md,5,"The question ""What utility allows starting multi-gpu training in a Jupyter Notebook?"" is directly answerable from the provided context. The context explicitly mentions the utility called `notebook_launcher` from the `Accelerate` library, which is used to initiate multi-GPU training within a Jupyter Notebook. The context provides sufficient detail about how the `notebook_launcher` is utilized, including the relevant Python code snippet that demonstrates its usage. Therefore, the question is clearly and unambiguously answerable based on the information given in the context.",4,"This question is highly relevant for machine learning developers who are using Jupyter Notebooks for experimentation and development. Multi-GPU training is a common requirement for training large models efficiently, and knowing the utility that facilitates this within the Hugging Face ecosystem is crucial. The question targets a specific technical need, indicating that the asker is likely working on sophisticated machine learning tasks. However, the question could be improved by specifying whether it refers to a specific library or framework within the Hugging Face ecosystem. Overall, it is highly useful for developers looking to optimize their training processes.",5,"The question seeks information about a utility for starting multi-GPU training specifically within a Jupyter Notebook. It is clear and specific in what it is asking, without any need for additional context or information. It doesn't refer to any particular document or setting which requires further clarification. The question is straightforward and can be answered by someone familiar with multi-GPU training tools and Jupyter Notebooks. Therefore, the question is context-independent and can be understood on its own."
"@article{su2022contrastiveiswhatyouneed,
  title={Contrastive Search Is What You Need For Neural Text Generation},
  author={Su, Yixuan and Collier, Nigel},
  journal={arXiv preprint arXiv:2210.14140},
  year={2022}
}
```



****

<span id='references'/>

## Reference:
> [1] Su et al., 2022 [""A Contrastive Framework for Neural Text Generation""](https://arxiv.org/abs/2202.06417), NeurIPS 2022

> [2] Su and Collier, 2022 [""Contrastive Search Is What You Need For Neural Text Generation""](https://arxiv.org/abs/2210.14140), Arxiv 2022

> [3] Fan et al., 2018 [""Hierarchical Neural Story Generation""](https://arxiv.org/abs/1805.04833), ACL 2018

> [4] Holtzman et al., 2020 [""The Curious Case of Neural Text Degeneration""](https://arxiv.org/abs/1904.09751), ICLR 2020

> [5] Zhang et al., 2022 [""OPT: Open Pre-trained Transformer Language Models""](https://arxiv.org/abs/2205.01068), Arxiv 2022

> [6] He et al., 2016 [""Deep Residual Learning for Image Recognition""](https://arxiv.org/abs/1512.03385), CVPR 2016

****

*- Written by Yixuan Su and Tian Lan*

****



<span id='acknowledgements'/>


## Acknowledgements:

We would like to thank Joao Gante ([@joaogante](https://huggingface.co/joaogante)), Patrick von Platen ([@patrickvonplaten](https://huggingface.co/patrickvonplaten)), and Sylvain Gugger ([@sgugger](https://github.com/sgugger)) for their help and guidance in adding contrastive search mentioned in this blog post into the `transformers` library.",Who are the authors of 'Contrastive Search Is What You Need For Neural Text Generation'?,Yixuan Su and Nigel Collier,huggingface/blog/blob/main/introducing-csearch.md,5,"The context provides a clear answer to the question regarding the authors of the paper titled 'Contrastive Search Is What You Need For Neural Text Generation'. The article reference explicitly lists 'Su, Yixuan and Collier, Nigel' as the authors. Furthermore, the reference [2] in the context corroborates this information by listing the same authors for the same paper. The context is sufficient and specific enough to answer the question unambiguously, as the authors are clearly identified in multiple parts of the provided information.",2,"The question asks for the authors of a specific paper, which may not directly aid machine learning developers in building NLP applications with the Hugging Face ecosystem. However, the identification of authors can still be useful for developers who are interested in learning more about the research background or expertise of the authors, potentially leading them to explore related work or further research by these authors. This could indirectly aid in understanding the state-of-the-art techniques or methodologies.

However, the question itself is not directly about implementation, model usage, or specific techniques within the Hugging Face ecosystem, which limits its immediate practical utility for developers. Therefore, while the question has some relevance, it is not highly useful for developers focused on practical application development.",5,"The question asks for the authors of a specific paper titled 'Contrastive Search Is What You Need For Neural Text Generation'. It is context-independent as it clearly refers to a specific, identifiable paper. Anyone with access to a database of papers or a search engine could find the answer without needing additional context. Therefore, the question is quite straightforward and self-explanatory."
"[TODO FILL: Here the mentor should add very specific information on what exactly has to be changed for this model]
[...]
[...]

**6. Write a conversion script**

Next, you should write a conversion script that lets you convert the
checkpoint you used to debug *[camelcase name of model]* in the original
repository to a checkpoint compatible with your just created 🤗
Transformers implementation of *[camelcase name of model]*. It is not advised to
write the conversion script from scratch, but rather to look through
already existing conversion scripts in 🤗 Transformers for one that has
been used to convert a similar model that was written in the same
framework as *[camelcase name of model]*. Usually, it is enough to copy an
already existing conversion script and slightly adapt it for your use
case. Don't hesitate to ask [name of mentor] to point you to a
similar already existing conversion script for your model.

-   If you are porting a model from TensorFlow to PyTorch, a good
    starting point might be BERT's conversion script
    [here](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91)
-   If you are porting a model from PyTorch to PyTorch, a good starting
    point might be BART's conversion script
    [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py)

In the following, we'll quickly explain how PyTorch models store layer
weights and define layer names. In PyTorch, the name of a layer is
defined by the name of the class attribute you give the layer. Let's
define a dummy model in PyTorch, called `SimpleModel` as follows:

```python
from torch import nn

class SimpleModel(nn.Module):
    def __init__(self):
            super().__init__()
            self.dense = nn.Linear(10, 10)
            self.intermediate = nn.Linear(10, 10)
            self.layer_norm = nn.LayerNorm(10)
```",Where can you find BERT's conversion script for porting from TensorFlow to PyTorch?,https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91,huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md,5,"The context provided is quite detailed regarding the process of writing a conversion script and gives explicit instructions on where to find BERT's conversion script. It mentions that for porting a model from TensorFlow to PyTorch, BERT's conversion script can be found at a specific URL: [here](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91). This directly answers the question about where to find the script. Additionally, the context provides further guidance on adapting existing scripts, which supports the idea of reusing code efficiently. Thus, the question is answerable clearly and unambiguously with the given context.",4,"The question is quite useful for machine learning developers who are working with BERT, as it addresses a specific technical need: converting models from TensorFlow to PyTorch. This is a common requirement, as developers often need to work with models in different frameworks, and Hugging Face supports both. Knowing where to find such conversion scripts is crucial for developers who want to leverage pre-trained models across these frameworks. Hence, this question provides value by directing developers towards practical tools that can streamline their workflow within the Hugging Face ecosystem.",5,"The question is clear and specific, asking for the location of a particular script related to BERT model conversion from TensorFlow to PyTorch. It does not depend on any additional context to be understood, except for basic knowledge about BERT and the concept of converting models between frameworks, which is expected. This makes it a context-independent question."
"This is analogous to _downloading_ a TV show versus _streaming_ it. When we download a TV show, we download the entire video 
offline and save it to our disk. We have to wait for the entire video to download before we can watch it and require as 
much disk space as size of the video file. Compare this to streaming a TV show. Here, we don’t download any part of the 
video to disk, but rather iterate over the remote video file and load each part in real-time as required. We don't have 
to wait for the full video to buffer before we can start watching, we can start as soon as the first portion of the video 
is ready! This is the same _streaming_ principle that we apply to loading datasets.

<figure>
<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming.gif"" alt=""Trulli"" style=""width:100%"">
<figcaption align = ""center""><b>Figure 1:</b> Streaming mode. The dataset is loaded progressively as we iterate over the dataset.</figcaption>
</figure>

Streaming mode has three primary advantages over downloading the entire dataset at once:
1. **Disk space:** samples are loaded to memory one-by-one as we iterate over the dataset. Since the data is not downloaded locally, there are no disk space requirements, so you can use datasets of arbitrary size.
2. **Download and processing time:** audio datasets are large and need a significant amount of time to download and process. With streaming, loading and processing is done on the fly, meaning you can start using the dataset as soon as the first sample is ready.
3. **Easy experimentation:** you can experiment on a handful samples to check that your script works without having to download the entire dataset.",What are the advantages of streaming mode over downloading a dataset?,1. Disk space: No local disk space is required as data is loaded to memory one-by-one. 2. Download and processing time: Allows immediate use of the dataset without waiting for full download. 3. Easy experimentation: Enables testing on a few samples without downloading the entire dataset.,huggingface/blog/blob/main/audio-datasets.md,5,"The context provided clearly outlines the advantages of streaming mode over downloading a dataset. It directly compares the two methods in terms of disk space requirements, download and processing time, and flexibility for experimentation. The context uses an analogy of streaming versus downloading a TV show to explain the streaming process in a relatable way. Additionally, it lists and explains the three primary benefits of streaming mode in the context of datasets. This makes it straightforward to answer the question unambiguously based on the given context.",4,"This question is quite useful for machine learning developers, particularly those working with large datasets in NLP applications using the Hugging Face ecosystem. Understanding the advantages of streaming mode over downloading a dataset is crucial for optimizing workflow efficiency, managing storage limitations, and ensuring faster processing times.

Streaming mode can offer several benefits such as reduced local storage requirements, the ability to work with large datasets without the need for complete downloads, and potentially faster data access speeds. These benefits can significantly impact the design and implementation of machine learning models, thus making this question relevant and practical for developers aiming to optimize data handling and processing.

However, while the question is useful, it could be more specific by asking about the context in which streaming mode is used, such as when dealing with certain types of datasets or when performing specific tasks like training or inference. Despite this, the question remains valuable for general understanding and application.",5,"The question is clear and can be understood without needing additional context. It asks about the advantages of a specific method (streaming mode) in comparison to another (downloading a dataset). While the terms 'streaming mode' and 'downloading a dataset' may require some technical understanding, they are common enough concepts that someone with a relevant background could answer the question without needing more information. Thus, the question is context-independent."
"| FrugalScore                                        | Student     | Teacher        | Method     |
|----------------------------------------------------|-------------|----------------|------------|
| [moussaKam/frugalscore_tiny_bert-base_bert-score](https://huggingface.co/moussaKam/frugalscore_tiny_bert-base_bert-score)    | BERT-tiny   | BERT-Base      | BERTScore  |
| [moussaKam/frugalscore_small_bert-base_bert-score](https://huggingface.co/moussaKam/frugalscore_small_bert-base_bert-score)   | BERT-small  | BERT-Base      | BERTScore  |
| [moussaKam/frugalscore_medium_bert-base_bert-score](https://huggingface.co/moussaKam/frugalscore_medium_bert-base_bert-score) | BERT-medium | BERT-Base      | BERTScore  |
| [moussaKam/frugalscore_tiny_roberta_bert-score](https://huggingface.co/moussaKam/frugalscore_tiny_roberta_bert-score)     | BERT-tiny   | RoBERTa-Large  | BERTScore  |
| [moussaKam/frugalscore_small_roberta_bert-score](https://huggingface.co/moussaKam/frugalscore_small_roberta_bert-score)     | BERT-small  | RoBERTa-Large  | BERTScore  |
| [moussaKam/frugalscore_medium_roberta_bert-score](https://huggingface.co/moussaKam/frugalscore_medium_roberta_bert-score)    | BERT-medium | RoBERTa-Large  | BERTScore  |
| [moussaKam/frugalscore_tiny_deberta_bert-score](https://huggingface.co/moussaKam/frugalscore_tiny_deberta_bert-score)      | BERT-tiny   | DeBERTa-XLarge | BERTScore  |
| [moussaKam/frugalscore_small_deberta_bert-score](https://huggingface.co/moussaKam/frugalscore_small_deberta_bert-score)     | BERT-small  | DeBERTa-XLarge | BERTScore  |
| [moussaKam/frugalscore_medium_deberta_bert-score](https://huggingface.co/moussaKam/frugalscore_medium_deberta_bert-score)    | BERT-medium | DeBERTa-XLarge | BERTScore  |
| [moussaKam/frugalscore_tiny_bert-base_mover-score](https://huggingface.co/moussaKam/frugalscore_tiny_bert-base_mover-score)   | BERT-tiny   | BERT-Base      | MoverScore |",What teacher model is used with frugalscore_tiny_bert-base_mover-score?,BERT-Base,huggingface/evaluate/blob/main/metrics/frugalscore/README.md,5,"The context provides a table listing various models with their respective student models, teacher models, and methods. Among these, the specific model 'frugalscore_tiny_bert-base_mover-score' is clearly listed. The table indicates that for this model, the teacher model used is 'BERT-Base'. The context directly addresses the question by including this specific entry, providing an unambiguous answer to what teacher model is used with 'frugalscore_tiny_bert-base_mover-score'. Therefore, the question is clearly and unambiguously answerable with the given context.",4,"The question asks about the teacher model used with a specific model configuration, ""frugalscore_tiny_bert-base_mover-score."" This is relevant to developers using Hugging Face's model hub or those interested in model distillation techniques. Knowing the teacher model can help understand the design choices behind the model's efficiency and performance and can be crucial for tasks such as transfer learning, model optimization, and improving inference speed. The question is specific and technical, which makes it highly relevant to practitioners in the field. However, it assumes familiarity with the models and terminology, which might limit its usefulness to a broader audience. Overall, it is a valuable question for NLP developers focusing on model efficiency and distillation.",5,"This question is specific and asks about the teacher model used with a particular variant of a model (frugalscore_tiny_bert-base_mover-score). It is clear in what it is asking, namely the name or type of teacher model associated with the specified model variant. Even though it mentions a technical term, it does not require additional context to understand what is being asked; an operator with access to the relevant documentation or knowledge of machine learning models would understand this question. Thus, it is context-independent and makes sense by itself."
"<div style=""font-size: 14px; line-height: 1.3;"">
<script src=""https://gist.github.com/nickmaxfield/1924be9dc0aeb17e301936c5566b4de2.js""></script>
</div>
<p>To train the model, we define a trainer using the <code>IPUTrainer</code> class which takes care of compiling the model to run on IPUs, and of performing training and evaluation. The <code>IPUTrainer</code> class works just like the Hugging Face Trainer class, but takes the additional <code>ipu_config</code> argument.</p>
<div style=""font-size: 14px; line-height: 1.3;"">
<script src=""https://gist.github.com/nickmaxfield/0b273df36666ceb85763e3210c39d5f6.js""></script>
</div>
<div style=""font-size: 14px; line-height: 1.3;"">
<script src=""https://gist.github.com/nickmaxfield/c94c59a6aed6165b0519af24e168139b.js""></script>
</div>
<h2>Running the training</h2>
<p>To accelerate training we will load the last checkpoint if it exists.</p>
<div style=""font-size: 14px; line-height: 1.3;"">
<script src=""https://gist.github.com/nickmaxfield/6033ce6f471af9f2136cf45002db97ab.js""></script>
</div>
<p>Now we are ready to train.</p>
<div style=""font-size: 14px; line-height: 1.3;"">
<script src=""https://gist.github.com/nickmaxfield/e203649cd06809ecf52821efbbdac7f6.js""></script>
</div>
<div style=""font-size: 14px; line-height: 1.3;"">
<script src=""https://gist.github.com/nickmaxfield/cc5e9367cfd1f8c295d016c35b552620.js""></script>
</div>
<h2>Plotting convergence</h2>
<p>Now that we have completed the training, we can format and plot the trainer output to evaluate the training behaviour.</p>
<div style=""font-size: 14px; line-height: 1.3;"">
<script src=""https://gist.github.com/nickmaxfield/05fbef22532f22c64572e9a62d9f219b.js""></script>
</div>
<p>We plot the training loss and the learning rate.</p>
<div style=""font-size: 14px; line-height: 1.3;"">
<script src=""https://gist.github.com/nickmaxfield/3f124ca1d9362c51c6ebd7573019133d.js""></script>
</div>",What class is used to compile the model to run on IPUs?,IPUTrainer,huggingface/blog/blob/main/vision-transformers.md,5,"The context provided clearly mentions the use of the `IPUTrainer` class to compile the model to run on IPUs. It explicitly states that this class is responsible for compiling the model, as well as handling training and evaluation. The context directly answers the question about which class is used for this purpose, with no ambiguity or need for additional information. Therefore, the question is fully answerable using the given context.",4,"The question is specific and relevant to machine learning developers who are interested in optimizing their NLP models for different hardware, such as IPUs (Intellectual Property Units). Knowing which class to use for compiling models for specific hardware is crucial for performance optimization and deployment. This question would be particularly useful for developers working with Hugging Face's ecosystem that supports hardware-specific optimizations. However, the question could be improved by providing more context, such as mentioning the specific library or framework being referred to, as different libraries may have different methods or classes for this purpose.",5,"The question ""What class is used to compile the model to run on IPUs?"" is well-formed and clear on its own. It is asking specifically for a class related to compiling a model for IPUs, which are specialized hardware for machine learning tasks. This is a technical question that assumes some prior knowledge about models and IPUs, but it does not reference any specific document or context that would be needed to understand the question itself. Therefore, it is context-independent and can be understood by anyone familiar with model compilation and IPUs."
"Get a closer look at [DistilBERT](model_doc/distilbert) by accessing [`DistilBertConfig`] to inspect it's attributes:

```py
>>> from transformers import DistilBertConfig

>>> config = DistilBertConfig()
>>> print(config)
DistilBertConfig {
  ""activation"": ""gelu"",
  ""attention_dropout"": 0.1,
  ""dim"": 768,
  ""dropout"": 0.1,
  ""hidden_dim"": 3072,
  ""initializer_range"": 0.02,
  ""max_position_embeddings"": 512,
  ""model_type"": ""distilbert"",
  ""n_heads"": 12,
  ""n_layers"": 6,
  ""pad_token_id"": 0,
  ""qa_dropout"": 0.1,
  ""seq_classif_dropout"": 0.2,
  ""sinusoidal_pos_embds"": false,
  ""transformers_version"": ""4.16.2"",
  ""vocab_size"": 30522
}
```

[`DistilBertConfig`] displays all the default attributes used to build a base [`DistilBertModel`]. All attributes are customizable, creating space for experimentation. For example, you can customize a default model to:

- Try a different activation function with the `activation` parameter.
- Use a higher dropout ratio for the attention probabilities with the `attention_dropout` parameter.

```py
>>> my_config = DistilBertConfig(activation=""relu"", attention_dropout=0.4)
>>> print(my_config)
DistilBertConfig {
  ""activation"": ""relu"",
  ""attention_dropout"": 0.4,
  ""dim"": 768,
  ""dropout"": 0.1,
  ""hidden_dim"": 3072,
  ""initializer_range"": 0.02,
  ""max_position_embeddings"": 512,
  ""model_type"": ""distilbert"",
  ""n_heads"": 12,
  ""n_layers"": 6,
  ""pad_token_id"": 0,
  ""qa_dropout"": 0.1,
  ""seq_classif_dropout"": 0.2,
  ""sinusoidal_pos_embds"": false,
  ""transformers_version"": ""4.16.2"",
  ""vocab_size"": 30522
}
```

Pretrained model attributes can be modified in the [`~PretrainedConfig.from_pretrained`] function:

```py
>>> my_config = DistilBertConfig.from_pretrained(""distilbert-base-uncased"", activation=""relu"", attention_dropout=0.4)
```

Once you are satisfied with your model configuration, you can save it with [`~PretrainedConfig.save_pretrained`]. Your configuration file is stored as a JSON file in the specified save directory:",What is the default activation function used in DistilBertConfig?,gelu,huggingface/transformers/blob/main/docs/source/en/create_a_model.md,5,"The context provided includes detailed information about the attributes of `DistilBertConfig`, specifically mentioning the default settings for various parameters, including the activation function. It explicitly states that the default activation function for `DistilBertConfig` is `gelu`. This information is clear and unambiguous, directly addressing the question about the default activation function. The context also provides a code snippet that confirms this by showing the default configuration output, which lists ""activation"": ""gelu"" as one of the attributes. Therefore, the question can be answered with complete clarity from the given context.",4,"This question is specific and directly relates to the Hugging Face ecosystem, particularly concerning the DistilBERT model, which is a popular NLP model available in the Hugging Face model hub. Understanding the default configurations of models like DistilBERT is crucial for developers who need to customize or fine-tune these models for various NLP tasks. Knowing the default activation function can significantly impact how a model is interpreted and adjusted, which is valuable information for anyone working with or extending the DistilBERT model. Thus, this question is quite useful for machine learning developers working within this ecosystem.",5,"The question is clear and specific enough for someone familiar with the DistilBertConfig or with access to its documentation. It refers to a particular configuration parameter, the activation function, which is a common concept in neural network architectures. Thus, it is not dependent on additional context beyond what is generally understood in the field of machine learning. The question does not mention any 'in the context' or 'in the document' phrasing, which would otherwise decrease its context-independence rating."
"<!--
Type: model-index
Collections:
- Name: MobileNet V3
  Paper:
    Title: Searching for MobileNetV3
    URL: https://paperswithcode.com/paper/searching-for-mobilenetv3
Models:
- Name: mobilenetv3_large_100
  In Collection: MobileNet V3
  Metadata:
    FLOPs: 287193752
    Parameters: 5480000
    File Size: 22076443
    Architecture:
    - 1x1 Convolution
    - Batch Normalization
    - Convolution
    - Dense Connections
    - Depthwise Separable Convolution
    - Dropout
    - Global Average Pooling
    - Hard Swish
    - Inverted Residual Block
    - ReLU
    - Residual Connection
    - Softmax
    - Squeeze-and-Excitation Block
    Tasks:
    - Image Classification
    Training Techniques:
    - RMSProp
    - Weight Decay
    Training Data:
    - ImageNet
    Training Resources: 4x4 TPU Pod
    ID: mobilenetv3_large_100
    LR: 0.1
    Dropout: 0.8
    Crop Pct: '0.875'
    Momentum: 0.9
    Batch Size: 4096
    Image Size: '224'
    Weight Decay: 1.0e-05
    Interpolation: bicubic
  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/mobilenetv3.py#L363
  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_large_100_ra-f55367f5.pth
  Results:
  - Task: Image Classification
    Dataset: ImageNet
    Metrics:
      Top 1 Accuracy: 75.77%
      Top 5 Accuracy: 92.54%
- Name: mobilenetv3_rw
  In Collection: MobileNet V3
  Metadata:
    FLOPs: 287190638
    Parameters: 5480000
    File Size: 22064048
    Architecture:
    - 1x1 Convolution
    - Batch Normalization
    - Convolution
    - Dense Connections
    - Depthwise Separable Convolution
    - Dropout
    - Global Average Pooling
    - Hard Swish
    - Inverted Residual Block
    - ReLU
    - Residual Connection
    - Softmax
    - Squeeze-and-Excitation Block
    Tasks:
    - Image Classification
    Training Techniques:
    - RMSProp
    - Weight Decay
    Training Data:
    - ImageNet",What is the top 1 accuracy of mobilenetv3_large_100 on ImageNet?,75.77%,huggingface/pytorch-image-models/blob/main/hfdocs/source/models/mobilenet-v3.mdx,5,"The context provides specific details about the mobilenetv3_large_100 model, including its task, dataset, metrics, and results. The Top 1 Accuracy for the mobilenetv3_large_100 model on the ImageNet dataset is explicitly mentioned as 75.77%. This directly answers the question posed, with no ambiguity or need for inference. The context is comprehensive and clearly specifies the required information.",2,"The question is asking for specific performance metrics of a particular model (mobilenetv3_large_100) on the ImageNet dataset. While this is relevant to those working with computer vision models, it is not directly related to NLP applications or the Hugging Face ecosystem, which primarily focuses on natural language processing. Therefore, the question is not particularly useful for developers building NLP applications using Hugging Face tools. If the question were instead about a model or metric related to NLP, it would be more relevant.",5,"The question is clear and specific about what is being asked: the top 1 accuracy of a specific model (mobilenetv3_large_100) on a particular dataset (ImageNet). It does not refer to any external context or require additional information to be understood. Anyone familiar with machine learning models and datasets should be able to comprehend the question fully. Therefore, it is context-independent and can be rated highly for clarity and specificity."
"## Do you want to add documentation?

We're always looking for improvements to the documentation that make it more clear and accurate. Please let us know how the documentation can be improved such as typos and any content that is missing, unclear or inaccurate. We'll be happy to make the changes or help you make a contribution if you're interested!

For more details about how to generate, build, and write the documentation, take a look at the documentation [README](https://github.com/huggingface/transformers/tree/main/docs).

## Create a Pull Request

Before writing any code, we strongly advise you to search through the existing PRs or
issues to make sure nobody is already working on the same thing. If you are
unsure, it is always a good idea to open an issue to get some feedback.

You will need basic `git` proficiency to contribute to
🤗 Transformers. While `git` is not the easiest tool to use, it has the greatest
manual. Type `git --help` in a shell and enjoy! If you prefer books, [Pro
Git](https://git-scm.com/book/en/v2) is a very good reference.

You'll need **[Python 3.8]((https://github.com/huggingface/transformers/blob/main/setup.py#L426))** or above to contribute to 🤗 Transformers. Follow the steps below to start contributing:

1. Fork the [repository](https://github.com/huggingface/transformers) by
   clicking on the **[Fork](https://github.com/huggingface/transformers/fork)** button on the repository's page. This creates a copy of the code
   under your GitHub user account.

2. Clone your fork to your local disk, and add the base repository as a remote:

   ```bash
   git clone git@github.com:<your Github handle>/transformers.git
   cd transformers
   git remote add upstream https://github.com/huggingface/transformers.git
   ```

3. Create a new branch to hold your development changes:

   ```bash
   git checkout -b a-descriptive-name-for-my-changes
   ```

   🚨 **Do not** work on the `main` branch!",What version of Python is required to contribute to 🤗 Transformers?,Python 3.8 or above,huggingface/transformers/blob/main/CONTRIBUTING.md,5,"The context explicitly states that Python 3.8 or above is required to contribute to 🤗 Transformers. This information is clearly presented in the section about creating a pull request and preparing the development environment. There is no room for ambiguity as the specific version requirement is directly mentioned with a reference to its location in the setup file. Therefore, the question is fully answerable with the given context.",3,"This question is moderately useful to machine learning developers working with the Hugging Face ecosystem, particularly those who are interested in contributing to the 🤗 Transformers library. Understanding the correct Python version is essential for ensuring compatibility and avoiding issues related to syntax or dependencies when contributing code. However, the question is narrowly focused on contributors rather than users of the library, and it doesn't cover broader aspects of using the library or applying it to NLP tasks. It is specific to the development environment setup rather than the implementation or usage of NLP models.",5,"The question ""What version of Python is required to contribute to 🤗 Transformers?"" is context-independent and clear. It specifies the subject (🤗 Transformers) and what is being asked (the required version of Python), making it understandable to anyone familiar with software development or who knows how to look up information about the 🤗 Transformers project. There is no implicit reference to any specific document or context outside of the general knowledge about software version requirements. Thus, it stands alone and is self-explanatory without needing additional context."
"!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# Environment variables

`huggingface_hub` can be configured using environment variables.

If you are unfamiliar with environment variable, here are generic articles about them
[on macOS and Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)
and on [Windows](https://phoenixnap.com/kb/windows-set-environment-variable).

This page will guide you through all environment variables specific to `huggingface_hub`
and their meaning.

## Generic

### HF_INFERENCE_ENDPOINT

To configure the inference api base url. You might want to set this variable if your organization
is pointing at an API Gateway rather than directly at the inference api.

Defaults to `""https://api-inference.huggingface.com""`.

### HF_HOME

To configure where `huggingface_hub` will locally store data. In particular, your token
and the cache will be stored in this folder.

Defaults to `""~/.cache/huggingface""` unless [XDG_CACHE_HOME](#xdgcachehome) is set.

### HF_HUB_CACHE

To configure where repositories from the Hub will be cached locally (models, datasets and
spaces).

Defaults to `""$HF_HOME/hub""` (e.g. `""~/.cache/huggingface/hub""` by default).

### HF_ASSETS_CACHE

To configure where [assets](../guides/manage-cache#caching-assets) created by downstream libraries
will be cached locally. Those assets can be preprocessed data, files downloaded from GitHub,
logs,...

Defaults to `""$HF_HOME/assets""` (e.g. `""~/.cache/huggingface/assets""` by default).

### HF_TOKEN

To configure the User Access Token to authenticate to the Hub. If set, this value will
overwrite the token stored on the machine (in `""$HF_HOME/token""`).

For more details about authentication, check out [this section](../quick-start#authentication).

### HF_HUB_VERBOSITY",What is the default value of HF_INFERENCE_ENDPOINT?,"""https://api-inference.huggingface.com""",huggingface/huggingface_hub/blob/main/docs/source/en/package_reference/environment_variables.md,5,"The context provided includes clear information about the ""HF_INFERENCE_ENDPOINT"" environment variable, specifically mentioning that it defaults to ""https://api-inference.huggingface.com"". This directly answers the question about the default value of ""HF_INFERENCE_ENDPOINT"" without any ambiguity. The context is straightforward and provides an unambiguous answer to the question asked.",3,"The question is quite specific and technical, which may not be broadly applicable to all machine learning developers using the Hugging Face ecosystem. However, it can be useful for developers who are specifically working with Hugging Face's inference services and need to understand environment configurations. Knowing default values is important for debugging and configuring environments correctly. That said, this question is only relevant to a niche group within the broader Hugging Face community, and it assumes some prior knowledge about Hugging Face's infrastructure and environment variables. Thus, while it is useful, it is not universally applicable.",4,"The question 'What is the default value of HF_INFERENCE_ENDPOINT?' is mostly self-contained and does not rely on any external context to be understood. It clearly asks for a specific piece of information, namely the default value of a particular variable or setting. The abbreviation 'HF' might be interpreted as 'Hugging Face,' common in certain technical communities, and 'INFERENCE_ENDPOINT' suggests a technical parameter or configuration related to endpoint settings in a software environment. However, if the audience is not familiar with what 'HF_INFERENCE_ENDPOINT' refers to, they might need additional context about what this variable is used for. Nonetheless, the question is clear in what it asks and assumes the user has knowledge of the specific system or documentation where this variable is defined."
"(Legacy) SE-ResNet

**SE ResNet** is a variant of a [ResNet](https://www.paperswithcode.com/method/resnet) that employs [squeeze-and-excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block) to enable the network to perform dynamic channel-wise feature recalibration.

## How do I use this model on an image?

To load a pretrained model:

```py
>>> import timm
>>> model = timm.create_model('legacy_seresnet101', pretrained=True)
>>> model.eval()
```

To load and preprocess the image:

```py 
>>> import urllib
>>> from PIL import Image
>>> from timm.data import resolve_data_config
>>> from timm.data.transforms_factory import create_transform

>>> config = resolve_data_config({}, model=model)
>>> transform = create_transform(**config)

>>> url, filename = (""https://github.com/pytorch/hub/raw/master/images/dog.jpg"", ""dog.jpg"")
>>> urllib.request.urlretrieve(url, filename)
>>> img = Image.open(filename).convert('RGB')
>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension
```

To get the model predictions:

```py
>>> import torch
>>> with torch.no_grad():
...     out = model(tensor)
>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)
>>> print(probabilities.shape)
>>> # prints: torch.Size([1000])
```

To get the top-5 predictions class names:

```py
>>> # Get imagenet class mappings
>>> url, filename = (""https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"", ""imagenet_classes.txt"")
>>> urllib.request.urlretrieve(url, filename) 
>>> with open(""imagenet_classes.txt"", ""r"") as f:
...     categories = [s.strip() for s in f.readlines()]",What is the name of the ResNet variant that uses squeeze-and-excitation blocks?,SE ResNet,huggingface/pytorch-image-models/blob/main/hfdocs/source/models/legacy-se-resnet.mdx,5,"The context clearly states that SE-ResNet is a variant of ResNet that employs squeeze-and-excitation blocks. The question asks for the name of the ResNet variant that uses these blocks, and the context provides the exact term 'SE-ResNet'. This makes the question unambiguously answerable with the given context, as the specific variant name is directly mentioned and linked to the use of squeeze-and-excitation blocks.",4,"This question is highly relevant for machine learning developers, especially those working on tasks involving computer vision models. ResNet is a widely used architecture in deep learning, and understanding its variants, like the one with squeeze-and-excitation blocks, can significantly enhance model performance. This knowledge is applicable when building or refining models for image processing tasks, which are often part of NLP applications that involve visual data, such as image captioning or visual question answering. Thus, knowing about ResNet variants is useful for developers using the Hugging Face ecosystem who might work on multi-modal models. However, it is not directly related to NLP tasks that do not involve vision components, which slightly limits its usefulness for purely NLP-focused developers.",5,"The question 'What is the name of the ResNet variant that uses squeeze-and-excitation blocks?' is context-independent, as it directly asks for the name of a specific architecture variant within the ResNet family. It is clear and specific, and someone with knowledge of neural network architectures or access to relevant resources can understand and answer it without needing further context. The use of technical terms like 'ResNet' and 'squeeze-and-excitation blocks' is appropriate for the domain, and the question is precise in what it asks."
"Gradio Demo: dropdown_component


```
!pip install -q gradio 
```


```
import gradio as gr 

with gr.Blocks() as demo:
    gr.Dropdown(choices=[""First Choice"", ""Second Choice"", ""Third Choice""])

demo.launch()
```",What Python package needs to be installed for the Gradio demo?,gradio,gradio-app/gradio/blob/main/demo/dropdown_component/run.ipynb,5,"The context provides a clear snippet of Python code aimed at setting up a Gradio demo. The code explicitly includes a command to install the Gradio package using the pip installer: `!pip install -q gradio`. This directly answers the question, as it specifies that the 'gradio' package needs to be installed for running the demo. There is no ambiguity in the context, as the installation command is straightforward and directly related to the Gradio demo setup. Therefore, the question is fully answerable with the provided context.",4,"This question is quite specific and directly relevant to machine learning developers working with Gradio, a popular tool for building web-based demos for machine learning models. Knowing the correct Python package to install is a basic but essential step to successfully using Gradio in the Hugging Face ecosystem. Therefore, it provides value to those who are setting up their development environment and want to ensure they have the appropriate packages installed.

However, the question is a bit narrow as it only addresses one aspect of using Gradio, and it doesn't offer insight into more complex aspects of integrating Gradio with machine learning models or the Hugging Face API. 

Overall, the question is useful but limited in scope.",5,"The question 'What Python package needs to be installed for the Gradio demo?' is quite context-independent. It is specific in asking about a Python package related to 'Gradio demo'. Someone familiar with Gradio or with access to its documentation can understand and answer the question without needing additional context or information. The question does not reference any specific document or implicit context, making it clear and direct."
"## Data Parallelism

Most users with just a few GPUs are likely to be familiar with `DistributedDataParallel` (DDP)  [PyTorch documentation](https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel). In this method the model is fully replicated to each GPU and then after each iteration all the models synchronize their states with each other. This approach allows training speed up but throwing more resources at the problem, but it only works if the model can fit onto a single GPU.


### ZeRO Data Parallelism

ZeRO-powered data parallelism (ZeRO-DP) is described on the following diagram from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)
![DeepSpeed-Image-1](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero.png)

It can be difficult to wrap one's head around it, but in reality, the concept is quite simple. This is just the usual DDP, except, instead of replicating the full model params, gradients and optimizer states, each GPU stores only a slice of it.  And then at run-time when the full layer params are needed just for the given layer, all GPUs synchronize to give each other parts that they miss - this is it.

This component is implemented by DeepSpeed.

## Tensor Parallelism

In Tensor Parallelism (TP) each GPU processes only a slice of a tensor and only aggregates the full tensor for operations that require the whole thing.

In this section we use concepts and diagrams from the [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) paper: [Efficient Large-Scale Language Model Training on GPU Clusters](https://arxiv.org/abs/2104.04473).

The main building block of any transformer is a fully connected `nn.Linear` followed by a nonlinear activation `GeLU`.",What does ZeRO-powered data parallelism store on each GPU?,"A slice of the model parameters, gradients, and optimizer states.",huggingface/blog/blob/main/bloom-megatron-deepspeed.md,5,"The context provided includes a detailed explanation of ZeRO-powered data parallelism, specifically highlighting that instead of replicating the full model parameters, gradients, and optimizer states on each GPU, ZeRO-DP stores only a slice of these elements on each GPU. This information directly answers the question about what is stored on each GPU in ZeRO-powered data parallelism. The explanation is clear and provides a direct reference to the components involved in ZeRO-DP, making it straightforward to understand what is stored on each GPU. Therefore, the question is answerable clearly and unambiguously with the given context.",4,"This question is quite relevant for machine learning developers working with the Hugging Face ecosystem, particularly those interested in optimizing distributed training of large models. Zero Redundancy Optimizer (ZeRO) is a technique used to reduce memory consumption and increase the scalability of model training by efficiently distributing the data across multiple GPUs. Understanding what ZeRO stores on each GPU can help developers in implementing more efficient training regimes, which is crucial for the performance of NLP applications. 

However, while the question is pertinent, it might be a bit specific and technical for some audiences who are not familiar with distributed training concepts or the ZeRO approach. Thus, while it is quite useful, it may not be universally applicable to all developers using the Hugging Face suite, as it assumes a certain level of pre-existing knowledge about distributed training strategies like ZeRO.",4,"This question is quite specific and addresses a particular concept related to data parallelism and GPU usage in computing, especially within frameworks that use ZeRO (Zero Redundancy Optimizer). While the question uses technical terms, it is clear and specific enough for someone familiar with the context of distributed computing and GPU operations. It doesn't rely on external context to be understood, as the question directly asks about what is stored on each GPU when using ZeRO-powered data parallelism. Therefore, the question is largely context-independent, assuming knowledge of the specific technical terms."
"|      |      |[Neko-Institute-of-Science/LLaMA-13B-HF](https://huggingface.co/Neko-Institute-of-Science/LLaMA-13B-HF)                                            |7           |13      |llama-license            |https://huggingface.co/Neko-Institute-of-Science/LLaMA-13B-HF/blob/main/LICENSE    |[LICENSE](https://huggingface.co/Neko-Institute-of-Science/LLaMA-13B-HF/blob/main/LICENSE)                              |                                                                                                    |             |
|      |      |[TheBloke/Llama2-22B-Daydreamer-v3-GGUF](https://huggingface.co/TheBloke/Llama2-22B-Daydreamer-v3-GGUF)                                            |7           |1       | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/Llama2-22B-Daydreamer-v3-GGUF/blob/main/LICENSE.txt)                      |                                                                                                    |             |
|      |      |[valurank/t5-paraphraser](https://huggingface.co/valurank/t5-paraphraser)                                                                          |7           |2       |                         |                                                                                   |[LICENSE](https://huggingface.co/valurank/t5-paraphraser/blob/main/LICENSE)                                             |                                                                                                    |             |",What is the license URL for TheBloke/Llama2-22B-Daydreamer-v3-GGUF?,https://huggingface.co/TheBloke/Llama2-22B-Daydreamer-v3-GGUF/blob/main/LICENSE.txt,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md,5,"The context provides clear information about the model ""TheBloke/Llama2-22B-Daydreamer-v3-GGUF"" including a link to its license. The license URL is specified as [LICENSE.txt](https://huggingface.co/TheBloke/Llama2-22B-Daydreamer-v3-GGUF/blob/main/LICENSE.txt). This directly answers the question about the license URL, offering a specific and unambiguous link. Therefore, the context is sufficient to answer the question accurately without ambiguity.",3,"The question asks for the license URL of a specific model, TheBloke/Llama2-22B-Daydreamer-v3-GGUF, which is available on Hugging Face. For machine learning developers, understanding the licensing of a model is crucial as it affects how the model can be used in applications. However, this question is very specific and pertains to only one model, which might limit its usefulness to developers in general. It doesn't provide broader insight into model licensing practices, nor does it address common issues developers face when dealing with models in the Hugging Face ecosystem. Therefore, while it is useful for a developer interested in this specific model, its general utility is limited.",5,"The question is specific to a particular model called 'TheBloke/Llama2-22B-Daydreamer-v3-GGUF', asking for its license URL. This question is self-contained because it clearly asks for a specific piece of information that can be found with standard documentation or repository information about this specific model. There is no requirement for additional context to understand what is being asked."
"<div class=""flex flex-row gap-4"">
  <div class=""flex-1"">
    <img class=""rounded-xl"" src=""https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/watercolor_painting.jpeg""/>
    <figcaption class=""mt-2 text-center text-sm text-gray-500"">input image</figcaption>
  </div>
  <div class=""flex-1"">
    <img class=""rounded-xl"" src=""https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/sdxl_t2i.png""/>
    <figcaption class=""mt-2 text-center text-sm text-gray-500"">adapted image</figcaption>
  </div>
</div>

You can use the IP-Adapter face model to apply specific faces to your images.  It is an effective way to maintain consistent characters in your image generations.
Weights are loaded with the same method used for the other IP-Adapters.  

```python
# Load ip-adapter-full-face_sd15.bin
pipeline.load_ip_adapter(""h94/IP-Adapter"", subfolder=""models"", weight_name=""ip-adapter-full-face_sd15.bin"")
```

<Tip>

It is recommended to use `DDIMScheduler` and `EulerDiscreteScheduler` for face model. 


</Tip>

```python
import torch
from diffusers import StableDiffusionPipeline, DDIMScheduler
from diffusers.utils import load_image

noise_scheduler = DDIMScheduler(
    num_train_timesteps=1000,
    beta_start=0.00085,
    beta_end=0.012,
    beta_schedule=""scaled_linear"",
    clip_sample=False,
    set_alpha_to_one=False,
    steps_offset=1
)

pipeline = StableDiffusionPipeline.from_pretrained(
    ""runwayml/stable-diffusion-v1-5"",
    torch_dtype=torch.float16,
    scheduler=noise_scheduler,
).to(""cuda"")

pipeline.load_ip_adapter(""h94/IP-Adapter"", subfolder=""models"", weight_name=""ip-adapter-full-face_sd15.bin"")

pipeline.set_ip_adapter_scale(0.7)

image = load_image(""https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/ai_face2.png"")

generator = torch.Generator(device=""cpu"").manual_seed(33)",What is the recommended scheduler for the IP-Adapter face model?,DDIMScheduler and EulerDiscreteScheduler,huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md,5,"The context provides a clear answer to the question. It specifically mentions that it is recommended to use `DDIMScheduler` and `EulerDiscreteScheduler` for the IP-Adapter face model. This information is presented in a separate tip section, making it easy to locate. The context, therefore, directly addresses the question and provides the recommended schedulers unambiguously.",5,"This question is highly relevant for machine learning developers using the Hugging Face ecosystem, especially those working with the IP-Adapter face model. Understanding the recommended scheduler is crucial for optimizing model performance and ensuring efficient training and inference. It addresses a specific aspect of model configuration that can have significant implications on model outcomes. Therefore, this question is practical and directly applicable to developers' needs, making it very useful.",5,"The question ""What is the recommended scheduler for the IP-Adapter face model?"" is clear and specific in its inquiry about the recommended scheduler for a particular model. It does not refer to a broader context or require additional information to understand the question itself. The technical terms like 'IP-Adapter face model' might not be universally understood, but they are specific enough that someone familiar with the subject or with access to relevant documentation can grasp the meaning without needing additional context. Thus, the question is largely context-independent."
"### 2. What is the advantage of using a generator of lists of texts compared to a list of lists of texts when using `train_new_from_iterator()`?

<Question
	choices={[
		{
			text: ""That's the only type the method <code>train_new_from_iterator()</code> accepts."",
			explain: ""A list of lists of texts is a particular kind of generator of lists of texts, so the method will accept this too. Try again!""
		},
		{
			text: ""You will avoid loading the whole dataset into memory at once."",
			explain: ""Right! Each batch of texts will be released from memory when you iterate, and the gain will be especially visible if you use 🤗 Datasets to store your texts."",
			correct: true
		},
		{
			text: ""This will allow the 🤗 Tokenizers library to use multiprocessing."",
			explain: ""No, it will use multiprocessing either way.""
		},
        {
			text: ""The tokenizer you train will generate better texts."",
			explain: ""The tokenizer does not generate text -- are you confusing it with a language model?""
		}
	]}
/>

### 3. What are the advantages of using a ""fast"" tokenizer?

<Question
	choices={[
		{
			text: ""It can process inputs faster than a slow tokenizer when you batch lots of inputs together."",
			explain: ""Correct! Thanks to parallelism implemented in Rust, it will be faster on batches of inputs. What other benefit can you think of?"",
			correct: true
		},
		{
			text: ""Fast tokenizers always tokenize faster than their slow counterparts."",
			explain: ""A fast tokenizer can actually be slower when you only give it one or very few texts, since it can't use parallelism.""
		},
		{
			text: ""It can apply padding and truncation."",
			explain: ""True, but slow tokenizers also do that.""
		},
        {
			text: ""It has some additional features allowing you to map tokens to the span of text that created them."",
			explain: ""Indeed -- those are called offset mappings. That's not the only advantage, though."",
			correct: true
		}
	]}
/>",What is one advantage of using a generator of lists of texts in `train_new_from_iterator()`?,You will avoid loading the whole dataset into memory at once.,huggingface/course/blob/main/chapters/en/chapter6/10.mdx,5,"The context provided directly addresses the question by discussing the advantages of using a generator of lists of texts in the `train_new_from_iterator()` method. It specifies that one key advantage is avoiding the need to load the entire dataset into memory at once, which is particularly beneficial when using datasets stored with the 🤗 Datasets library. This explanation is clear and directly answers the question posed. Therefore, the context sufficiently provides an unambiguous answer to the question.",4,"This question is useful for machine learning developers using the Hugging Face ecosystem, particularly those involved in training language models. The question prompts consideration of memory efficiency and flexibility in handling training data, which are important aspects when dealing with large text datasets typical in NLP tasks. It encourages developers to think about the practical benefits of using generators, such as reducing memory overhead by not loading the entire dataset into memory at once. This is crucial for training models on limited hardware resources. However, the question could be improved by providing more context or background for beginners who may not be familiar with the `train_new_from_iterator()` method or the concept of generators in Python.",5,"The question is context-independent because it is asking about a specific advantage of using a generator of lists of texts in a particular function, `train_new_from_iterator()`. The function name suggests a specific programming or machine learning context, and anyone familiar with the function or with access to its documentation should be able to understand the question. It does not rely on any additional context or information beyond what is specified in the question itself, making it clear and self-contained."
". Of course, a start index greater than an end index corresponds to an impossible answer. Here is the code to find the best score for a possible answer. Once we have the start and end positions of the tokens, we use the offset mappings provided by our tokenizer to find the span of characters in the initial context, and get our answer! Now, when the context is long, it might get truncated by the tokenizer. This might result in part of the answer, or worse, the whole answer, being truncated. So we don't discard the truncated tokens but build new features with them. Each of those features contains the question, then a chunk of text in the context. If we take disjoint chunks of texts, we might end up with the answer being split between two features. So instead, we take overlapping chunks of texts, to make sure at least one of the chunks will fully contain the answer to the question. The tokenizers do all of this for us automatically with the return overflowing tokens option. The stride argument controls the number of overlapping tokens. Here is how our very long context gets truncated in two features with some overlap. By applying the same post-processing we saw before for each feature, we get the answer with a score for each of them, and we take the answer with the best score as a final solution.",What does the stride argument control in the tokenizer?,The stride argument controls the number of overlapping tokens.,huggingface/course/blob/main/subtitles/en/raw/chapter6/04_question-answering-pipeline-tf.md,5,"The context provides a direct and clear explanation of what the stride argument controls in the tokenizer. It states that the stride argument is used to control the number of overlapping tokens when chunking text that is too long, ensuring that at least one chunk fully contains the answer to a question. The context includes relevant details about why overlapping tokens are necessary and how they help avoid truncation issues. This allows the question to be answered unambiguously and accurately.",5,"The question ""What does the stride argument control in the tokenizer?"" is specific and directly relevant to developers working with tokenization in the Hugging Face ecosystem. Understanding the 'stride' parameter is crucial for tasks involving tokenization, especially when dealing with long texts that need to be split into smaller chunks for processing. This knowledge helps in fine-tuning tokenization processes to ensure efficient handling of text data, which is a critical component of many NLP applications. Therefore, this question is highly useful for developers who need to optimize text preprocessing in their NLP models using Hugging Face libraries.",5,"This question is context-independent and can be understood on its own. It specifies a particular argument ('stride') within a tokenizer and asks what it controls. Even if a reader is unfamiliar with the specific tokenizer being referred to, the structure of the question is clear and doesn't rely on additional context to be comprehensible. The question is specific and technical but does not require external information to understand the inquiry being made."
"**Networked hospitals and connected care:**

With predictive care, command centers are all set to analyze clinical and location data to monitor supply and demand across healthcare networks in real-time. With ML, healthcare professionals will be able to spot high-risk patients more quickly and efficiently, thus removing bottlenecks in the system. You can check the spread of contractible diseases faster, take better measures to manage epidemics, identify at-risk patients more accurately, especially for genetic diseases, and more.

**Better staff and patient experiences:**

Predictive healthcare networks are expected to reduce wait times, improve staff workflows, and take on the ever-growing administrative burden. By learning from every patient, diagnosis, and procedure, ML is expected to create experiences that adapt to hospital staff as well as the patient. This improves health outcomes and reduces clinician shortages and burnout while enabling the system to be financially sustainable.

---

🤗   Thank you for joining us in this second installment of ML Director Insights. Stay tuned for more insights from ML Directors in Finance, Healthcare and e-Commerce. 
 
Big thanks to Omar Rahman, Cao (Danica) Xiao, Raphael Cohen, and Martin Ostrovsky for their brilliant insights and participation in this piece. We look forward to watching each of your continued successes and will be cheering you on each step of the way. 🎉 
 
If you or your team are interested in accelerating your ML roadmap with Hugging Face Experts please visit [hf.co/support](https://huggingface.co/support?utm_source=article&utm_medium=blog&utm_campaign=ml_director_insights_2) to learn more.",What is the role of machine learning in predictive healthcare networks?,"Machine learning helps spot high-risk patients more quickly and efficiently, checks the spread of contractible diseases faster, manages epidemics better, identifies at-risk patients more accurately, and creates experiences that adapt to both hospital staff and patients.",huggingface/blog/blob/main/ml-director-insights-2.md,5,"The context provides a clear explanation of the role of machine learning in predictive healthcare networks. It specifies that machine learning helps in identifying high-risk patients, managing epidemics, and improving both patient and staff experiences. By detailing these aspects, the context directly addresses the role of machine learning as a tool for creating efficient and adaptive healthcare systems. The information is straightforward and closely aligned with the question, making the question unambiguously answerable using the provided context.",3,"This question is moderately useful for machine learning developers working with the Hugging Face ecosystem, especially those focusing on NLP applications in the healthcare sector. It encourages exploration of how machine learning models, potentially including NLP models, can impact predictive healthcare networks. However, it does not directly relate to the technical aspects of using Hugging Face tools or models. The question is broad and might not provide specific guidance or insights into leveraging Hugging Face's capabilities. Therefore, while it is relevant to understanding the broader context of machine learning in healthcare, it might not be directly useful for developers seeking specific technical advice on using Hugging Face for NLP in healthcare.",5,"The question is clear and context-independent, as it asks about the role of machine learning in a specific application domain—predictive healthcare networks. While it refers to a specific technological application (predictive healthcare networks), it doesn't rely on any specific document or context to be understood. Anyone with a basic understanding of machine learning and healthcare can comprehend the question."
"## Access Parquet files

Datasets Server converts every public dataset on the Hub to the [Parquet](https://parquet.apache.org/) format. The `/parquet` endpoint returns a JSON list of the Parquet URLs for a dataset:

<inferencesnippet>
<python>
```python
import requests
API_URL = ""https://datasets-server.huggingface.co/parquet?dataset=rotten_tomatoes""
def query():
    response = requests.get(API_URL)
    return response.json()
data = query()
```
</python>
<js>
```js
import fetch from ""node-fetch"";
async function query(data) {
    const response = await fetch(
        ""https://datasets-server.huggingface.co/parquet?dataset=rotten_tomatoes"",
        {
            method: ""GET""
        }
    );
    const result = await response.json();
    return result;
}
query().then((response) => {
    console.log(JSON.stringify(response));
});
```
</js>
<curl>
```curl
curl https://datasets-server.huggingface.co/parquet?dataset=rotten_tomatoes \
        -X GET
```
</curl>
</inferencesnippet>

This returns a URL to the Parquet file for each split:

```json
{
  ""parquet_files"": [
    {
      ""dataset"": ""rotten_tomatoes"",
      ""config"": ""default"",
      ""split"": ""test"",
      ""url"": ""https://huggingface.co/datasets/rotten_tomatoes/resolve/refs%2Fconvert%2Fparquet/default/test/0000.parquet"",
      ""filename"": ""0000.parquet"",
      ""size"": 92206
    },
    {
      ""dataset"": ""rotten_tomatoes"",
      ""config"": ""default"",
      ""split"": ""train"",
      ""url"": ""https://huggingface.co/datasets/rotten_tomatoes/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet"",
      ""filename"": ""0000.parquet"",
      ""size"": 698845
    },
    {
      ""dataset"": ""rotten_tomatoes"",
      ""config"": ""default"",
      ""split"": ""validation"",
      ""url"": ""https://huggingface.co/datasets/rotten_tomatoes/resolve/refs%2Fconvert%2Fparquet/default/validation/0000.parquet"",
      ""filename"": ""0000.parquet"",
      ""size"": 90001
    }
  ],
  ""pending"": [],
  ""failed"": [],
  ""partial"": false
}
```",What is the size of the Parquet file for the train split of the rotten_tomatoes dataset?,698845,huggingface/datasets-server/blob/main/docs/source/quick_start.mdx,5,"The context provides a detailed JSON response that includes the size of the Parquet file for each split of the rotten_tomatoes dataset. This includes the train split, with the key 'size' directly indicating its size as 698845. The question is specifically about the size of the Parquet file for the train split, and this information is explicitly provided in the context. Therefore, the question is clearly and unambiguously answerable with the given context, as it offers all the necessary details to answer the question accurately.",4,"The question is specific and directly related to a practical aspect of using datasets within the Hugging Face ecosystem, particularly for NLP applications. Knowing the size of the Parquet file for a dataset split is useful for developers who need to manage storage resources and optimize data loading times. This is especially relevant for those working with large datasets or in environments with limited computational resources. However, the question is very specific and may not be broadly applicable to all developers using the Hugging Face ecosystem, which slightly limits its general usefulness.",5,"The question is clear and specific about what information is being sought: the size of the Parquet file for the train split of a specific dataset ('rotten_tomatoes'). It does not require any additional context to understand what is being asked. Although it refers to a specific dataset, the question itself is about a quantifiable attribute (file size), which is straightforward and context-independent for someone familiar with datasets and file formats."
"Contributing a Guide

Want to help teach Gradio? Consider contributing a Guide! 🤗

Broadly speaking, there are two types of guides:

- **Use cases**: guides that cover step-by-step how to build a particular type of machine learning demo or app using Gradio. Here's an example: [_Creating a Chatbot_](https://github.com/gradio-app/gradio/blob/master/guides/creating_a_chatbot.md)
- **Feature explanation**: guides that describe in detail a particular feature of Gradio. Here's an example: [_Using Flagging_](https://github.com/gradio-app/gradio/blob/master/guides/using_flagging.md)

We encourage you to submit either type of Guide! (Looking for ideas? We may also have open [issues](https://github.com/gradio-app/gradio/issues?q=is%3Aopen+is%3Aissue+label%3Aguides) where users have asked for guides on particular topics)

## Guide Structure

As you can see with the previous examples, Guides are standard markdown documents. They usually:

- start with an Introduction section describing the topic
- include subheadings to make articles easy to navigate
- include real code snippets that make it easy to follow along and implement the Guide
- include embedded Gradio demos to make them more interactive and provide immediate demonstrations of the topic being discussed. These Gradio demos are hosted on [Hugging Face Spaces](https://huggingface.co/spaces) and are embedded using the standard \<iframe\> tag.

## How to Contribute a Guide

1. Clone or fork this `gradio` repo
2. Add a new markdown document with a descriptive title to the `/guides` folder
3. Write your Guide in standard markdown! Embed Gradio demos wherever helpful
4. Add a list of `related_spaces` at the top of the markdown document (see the previously linked Guides for how to do this)
5. Add 3 `tags` at the top of the markdown document to help users find your guide (again, see the previously linked Guides for how to do this)
6. Open a PR to have your guide reviewed",What are the two types of guides for contributing to Gradio?,Use cases and Feature explanation.,gradio-app/gradio/blob/main/guides/CONTRIBUTING.md,5,"The question 'What are the two types of guides for contributing to Gradio?' is directly answerable with the context provided. The context clearly outlines the two types of guides as 'Use cases' and 'Feature explanation'. Each type is also briefly described, providing a comprehensive understanding of what they entail. Therefore, the question is unambiguously answerable with the given context.",3,"This question is quite specific and directly related to contributing to Gradio, which is a part of the Hugging Face ecosystem. Understanding the types of guides available for contributing to Gradio can be useful for developers looking to contribute to its development or extend its functionalities. However, the question is somewhat narrow in focus, as it pertains only to Gradio and not to broader aspects of NLP application development or other parts of the Hugging Face ecosystem. It is useful for developers specifically interested in contributing to Gradio, but may not be widely applicable to all machine learning developers or those interested in broader NLP applications. Thus, while it is relevant, its applicability is limited to a specific audience.",5,"The question is specific and well-defined, asking about the types of guides related to contributing to Gradio. It does not rely on any external context to be understood. Anyone familiar with Gradio or willing to research it can comprehend what is being asked about the 'two types of guides'. The question is clear and precise, making it context-independent."
"We aren't including the `trainer.train()` line yet, to take the time to check that everything looks good. If we request a batch and pass it to our model, it now works without error!

```py
for batch in trainer.get_train_dataloader():
    break

outputs = trainer.model.cpu()(**batch)
```

The next step is then to move back to the GPU and check that everything still works:

```py
import torch

device = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")
batch = {k: v.to(device) for k, v in batch.items()}

outputs = trainer.model.to(device)(**batch)
```

If you still get an error, make sure you restart your notebook and only execute the last version of the script.

### Performing one optimization step[[performing-one-optimization-step]]

Now that we know that we can build batches that actually go through the model, we are ready for the next step of the training pipeline: computing the gradients and performing an optimization step.

The first part is just a matter of calling the `backward()` method on the loss:

```py
loss = outputs.loss
loss.backward()
```

It's pretty rare to get an error at this stage, but if you do get one, make sure to go back to the CPU to get a helpful error message.

To perform the optimization step, we just need to create the `optimizer` and call its `step()` method:

```py
trainer.create_optimizer()
trainer.optimizer.step()
```

Again, if you're using the default optimizer in the `Trainer`, you shouldn't get an error at this stage, but if you have a custom optimizer, there might be some problems to debug here. Don't forget to go back to the CPU if you get a weird CUDA error at this stage. Speaking of CUDA errors, earlier we mentioned a special case. Let's have a look at that now.

### Dealing with CUDA out-of-memory errors[[dealing-with-cuda-out-of-memory-errors]]",What method is called to compute gradients in the training pipeline?,backward(),huggingface/course/blob/main/chapters/en/chapter8/4.mdx,5,"The context provides a detailed description of a training pipeline, including the steps for computing gradients and performing an optimization step. It mentions the use of the `backward()` method on the loss to compute the gradients. This directly answers the question about the method called to compute gradients. The context is clear and unambiguous in explaining the process, making the question fully answerable with the provided information.",3,"The question ""What method is called to compute gradients in the training pipeline?"" is quite specific and is oriented towards understanding the technical aspects of training models, which is an important part of developing machine learning applications. However, it is not directly related to NLP or the Hugging Face ecosystem specifically. It applies to general machine learning practices, particularly in the context of frameworks like PyTorch or TensorFlow, where methods like backward() are used to compute gradients. Therefore, while it is useful, it is not specifically helpful for those who are using Hugging Face tools unless they are also working with these underlying frameworks. Thus, the rating reflects its general usefulness in machine learning rather than specific utility for Hugging Face users.",4,"The question ""What method is called to compute gradients in the training pipeline?"" is quite context-independent. It refers to a general concept common in machine learning and deep learning contexts, namely the computation of gradients during the training of models. The question does not specify any particular framework or library, so it could apply broadly to any context where such a concept is relevant. However, it might assume some level of familiarity with concepts in machine learning. Regardless, the question itself is clear and does not depend on additional context to be understood. Therefore, it scores high on the context-independence scale."
"--
title: ""The Falcon has landed in the Hugging Face ecosystem"" 
thumbnail: /blog/assets/147_falcon/falcon_thumbnail.jpg
authors:
- user: lvwerra
- user: ybelkada
- user: smangrul
- user: lewtun
- user: olivierdehaene
- user: pcuenq
- user: philschmid
- user: osanseviero
---

# The Falcon has landed in the Hugging Face ecosystem


## Introduction

Falcon is a new family of state-of-the-art language models created by the [Technology Innovation Institute](https://www.tii.ae/) in Abu Dhabi, and released under the Apache 2.0 license. **Notably, [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) is the first “truly open” model with capabilities rivaling many current closed-source models**. This is fantastic news for practitioners, enthusiasts, and industry, as it opens the door for many exciting use cases.

<div style=""background-color: #e6f9e6; padding: 16px 32px; outline: 2px solid; border-radius: 5px;"">
  September 2023 Update: <a href=""https://huggingface.co/blog/falcon-180b"">Falcon 180B</a> has just been released! It's currently the largest openly available model, and rivals proprietary models like PaLM-2. 
</div>


In this blog, we will be taking a deep dive into the Falcon models: first discussing what makes them unique and then **showcasing how easy it is to build on top of them (inference, quantization, finetuning, and more) with tools from the Hugging Face ecosystem**. 

## Table of Contents

- [The Falcon models](#the-falcon-models)
- [Demo](#demo)
- [Inference](#inference)
- [Evaluation](#evaluation)
- [Fine-tuning with PEFT](#fine-tuning-with-peft)
- [Conclusion](#conclusion)

## The Falcon models",Who created the Falcon language models?,Technology Innovation Institute,huggingface/blog/blob/main/falcon.md,5,"The question 'Who created the Falcon language models?' is directly answerable using the provided context. The context clearly states that the Falcon language models were created by the Technology Innovation Institute in Abu Dhabi. This information is presented prominently in the introduction of the context, making it easy to locate and understand. Therefore, the question is unambiguously answerable with the given context, as it provides a straightforward answer without requiring additional information or inference.",4,"The question 'Who created the Falcon language models?' is quite relevant to machine learning developers working with NLP applications, especially those utilizing or considering using models from the Hugging Face ecosystem. Knowing the creators of a model can provide insights into the model's design philosophy, potential biases, and the support or resources available. This information can be crucial for developers when deciding on which models to integrate into their applications. However, the question is relatively straightforward and factual, thus might not contribute much to deeper learning or problem-solving in NLP development. Therefore, it is useful but not deeply insightful.",5,"The question 'Who created the Falcon language models?' is specific and clear. It does not rely on any prior context or additional information to be understood. It asks for the creator or originator of a specific entity, which is the Falcon language models in this case. Anyone familiar with AI or language models can understand what is being asked even if they don't know the answer, as the question is self-contained."

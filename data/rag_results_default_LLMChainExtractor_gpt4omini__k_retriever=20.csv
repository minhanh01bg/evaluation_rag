user_input,retrieved_contexts,response,reference
"What architecture is the `tokenizers-linux-x64-musl` binary designed for?
","[""`tokenizers-linux-x64-musl`\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`""]",The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.,x86_64-unknown-linux-musl
"What is the purpose of the BLIP-Diffusion model?
","[""BLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. The abstract from the paper is:\n*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications.*"", ""The aim of this project is to provide retrieval augmented diffusion models to diffusers!"", ""**a neural network learns to gradually denoise data** starting from pure noise. **a learned reverse denoising diffusion process \\\\(p\\_\\theta\\\\), where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image.**"", ""In [Training Diffusion Models with Reinforcement Learning, Black](https://arxiv.org/abs/2305.13301) et al. show how to augment diffusion models to leverage RL to fine-tune them with respect to an objective function via a method named Denoising Diffusion Policy Optimization (DDPO). In this blog post, we discuss how DDPO came to be, a brief description of how it works, and how DDPO can be incorporated into an RLHF workflow to achieve model outputs more aligned with the human aesthetics."", ""# BLIP\n## Overview\nThe BLIP model was proposed in [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. BLIP is a model that is able to perform various multi-modal tasks including:\n- Visual Question Answering\n- Image-Text retrieval (Image-text matching)\n- Image Captioning"", ""| [BLIP Diffusion](blip\\_diffusion) | text2image |"", ""ü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, ü§ó Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). The library has three main components:\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in ü§ó Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve. - Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality. - Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems."", ""*We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.*"", ""BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner."", ""*By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.*"", ""*Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples 10√ó to 50√ó faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.*""]","The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation in text-to-image generation tasks. It aims to create novel renditions of an input subject based on text prompts while overcoming limitations of existing models, such as lengthy fine-tuning and difficulties in preserving subject fidelity. BLIP-Diffusion introduces a new multimodal encoder pre-trained to provide subject representation, allowing for efficient fine-tuning and flexible combinations with existing techniques for novel subject-driven generation and editing applications.",The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.
"How can a user claim authorship of a paper on the Hugging Face Hub?
","[""## Claiming authorship to a Paper\nThe Hub will attempt to automatically match paper to users based on their email. \n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \""claim authorship\""."", ""| Partially handled by `huggingface\\_hub`. |"", ""4. In case this is related to a paper, please attach a link;"", ""```\n- authors:\n- user: your_hf_user\n- user: your_coauthor\n```"", ""## Contribute\nAll contributions to the `huggingface\\_hub` are welcomed and equally valued! ü§ó Besides\nadding or fixing existing issues in the code, you can also help improve the\ndocumentation by making sure it is accurate and up-to-date, help answer questions on\nissues, and request new features you think will improve the library. Take a look at the\n[contribution\nguide](https://github.com/huggingface/huggingface\\_hub/blob/main/CONTRIBUTING.md) to\nlearn more about how to submit a new issue or feature request, how to submit a pull\nrequest, and how to test your contributions to make sure everything works as expected. Contributors should also be respectful of our [code of\nconduct](https://github.com/huggingface/huggingface\\_hub/blob/main/CODE\\_OF\\_CONDUCT.md) to\ncreate an inclusive and welcoming collaborative space for everyone.""]","A user can claim authorship of a paper on the Hugging Face Hub by clicking on their name in the corresponding Paper page and then selecting ""claim authorship."" The Hub will attempt to automatically match the paper to users based on their email. If the paper is not linked to the user's account, they can follow this process to claim it.","By clicking their name on the corresponding Paper page and clicking ""claim authorship"", then confirming the request in paper settings for admin team validation."
"What is the purpose of the /healthcheck endpoint in the Datasets server API?
","[""- /healthcheck: ensure the app is running"", ""- `/healthcheck`\n- The admin service provides endpoints:"", ""- /healthcheck: Ensure the app is running"", ""- /healthcheck: ensure the app is running"", ""- /healthcheck: Ensure the app is running""]",The purpose of the `/healthcheck` endpoint in the Datasets server API is to ensure that the app is running. It serves as a simple way to verify the operational status of the server.,Ensure the app is running
"What is the default context window size for Local Attention in the LongT5 model?
","[""- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r` tokens to the left and right of it (with `r=127` by default)."", ""Most tokens only attend \""locally\"" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and \\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in `config.attention\\_window`. Note that `config.attention\\_window` can be of type `List` to define a different \\\\(w\\\\) for each layer.""]",The default context window size for Local Attention in the LongT5 model is 127 tokens.,127 tokens
"What method is used to load a checkpoint for a task using `AutoPipeline`?
","[""The `AutoPipeline` class is designed to simplify the variety of pipelines in ü§ó Diffusers. It is a generic, *task-first* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name. Take a look at the [AutoPipeline](../api/pipelines/auto_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting. This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights. ## Choose an AutoPipeline for your task Start by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]: ```py from diffusers import AutoPipelineForText2Image import torch pipeline = AutoPipelineForText2Image.from_pretrained( \""runwayml/stable-diffusion-v1-5\"", torch_dtype=torch.float16, use_safetensors=True ).to(\""cuda\"") prompt = \""peasant and dragon combat, wood cutting style, viking era, bevel with rune\"" image = pipeline(prompt, num_inference_steps=25).images[0] image ``` Under the hood, [`AutoPipelineForText2Image`]: 1. automatically detects a `\""stable-diffusion\""` class from the [`model_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json) file 2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\""stable-diffusion\""` class name Likewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\""stable-diffusion\""` checkpoint from the `model_index.json` file and it'll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image: ```py from diffusers import AutoPipelineForImage2Image import torch import requests from PIL import Image from io import BytesIO pipeline = AutoPipelineForImage2Image.from_pretrained( \""runwayml/stable-diffusion-v1-5\"", torch_dtype=torch.float16, use_safetensors=True, ).to(\""cuda\"") prompt = \""a portrait of a dog wearing a pearl earring\"" url = \""https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665_Girl_with_a_Pearl_Earring.jpg/800px-1665_Girl_with_a_Pearl_Earring.jpg\"" response = requests.get(url) image = Image.open(BytesIO(response.content)).convert(\""RGB\"") image.thumbnail((768, 768)) image = pipeline(prompt, image, num_inference_steps=200, strength=0.75, guidance_scale=10.5).images[0] image ``` And if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way: ```py from diffusers import AutoPipelineForInpainting from diffusers.utils import load_image import torch pipeline = AutoPipelineForInpainting.from_pretrained( \""stabilityai/stable-diffusion-xl-base-1.0\"", torch_dtype=torch.float16, use_safetensors=True ).to(\""cuda\"") img_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"" mask_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"" init_image = load_image(img_url).convert(\""RGB\"") mask_image = load_image(mask_url).convert(\""RGB\"") prompt = \""A majestic tiger sitting on a bench\"" image = pipeline(prompt, image=init_image, mask_image=mask_image, num_inference_steps=50, strength=0.80).images[0] image ``` If you try to load an unsupported checkpoint, it'll throw an error: ```py from diffusers import AutoPipelineForImage2Image import torch pipeline = AutoPipelineForImage2Image.from_pretrained( \""openai/shap-e-img2img\"", torch_dtype=torch.float16, use_safetensors=True ) \""ValueError: AutoPipeline can't find a pipeline linked to ShapEImg2ImgPipeline for None\"" ```"", ""```py\n# resume from latest checkpoint\ntrainer.train(resume\\_from\\_checkpoint=True)\n# resume from specific checkpoint saved in output directory\ntrainer.train(resume\\_from\\_checkpoint=\""your-model/checkpoint-1000\"")\n```"", ""```py\n>>> new\\_model = AutoModel.from\\_pretrained(tmp\\_dir)\n```\n```py\n>>> load\\_sharded\\_checkpoint(model, tmp\\_dir)\n```"", ""```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from_pretrained(\""stabilityai/stable-diffusion-xl-base-1.0\"", torch_dtype=torch.float16).to(\""cuda\"")\npipeline.load_lora_weights(\""TheLastBen/William_Eggleston_Style_SDXL\"", weight_name=\""wegg.safetensors\"")\n# use by william eggleston in the prompt to trigger the LoRA\nprompt = \""a house by william eggleston, sunrays, beautiful, sunlight, sunrays, beautiful\""\nimage = pipeline(prompt=prompt).images[0]\nimage\n```"", ""If you want to use one of the official checkpoints for a task, explore the [gligen](https://huggingface.co/gligen) Hub organizations! [`StableDiffusionGLIGENPipeline`] was contributed by [Nikhil Gajendrakumar](https://github.com/nikhil-masterful) and [`StableDiffusionGLIGENTextImagePipeline`] was contributed by [Nguy·ªÖn C√¥ng T√∫ Anh](https://github.com/tuanh123789)."", ""```python\nfrom diffusers import DiffusionPipeline\n#Return a CheckpointMergerPipeline class that allows you to merge checkpoints. #The checkpoint passed here is ignored. But still pass one of the checkpoints you plan to\n#merge for convenience\npipe = DiffusionPipeline.from_pretrained(\""CompVis/stable-diffusion-v1-4\"", custom_pipeline=\""checkpoint_merger\"")\n#There are multiple possible scenarios:\n#The pipeline with the merged checkpoints is returned in all the scenarios\n#Compatible checkpoints a.k.a matched model_index.json files. Ignores the meta attributes in model_index.json during comparison.( attrs with _ as prefix )\nmerged_pipe = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""CompVis/stable-diffusion-v1-2\""], interp = \""sigmoid\"", alpha = 0.4)\n#Incompatible checkpoints in model_index.json but merge might be possible. Use force = True to ignore model_index.json compatibility\nmerged_pipe_1 = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""hakurei/waifu-diffusion\""], force = True, interp = \""sigmoid\"", alpha = 0.4)\n#Three checkpoint merging. Only \""add_difference\"" method actually works on all three checkpoints. Using any other options will ignore the 3rd checkpoint. merged_pipe_2 = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""hakurei/waifu-diffusion\"",\""prompthero/openjourney\""], force = True, interp = \""add_difference\"", alpha = 0.4)\n```"", ""All pipeline types inherit from the base [`DiffusionPipeline`] class; pass it any checkpoint, and it'll automatically detect the pipeline type and load the necessary components."", ""```py\npipeline = AutoPipelineForInpainting.from_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch_dtype=torch.float16, variant=\""fp16\""\n).to(\""cuda\"")\n```\n\n```py\npipeline = AutoPipelineForInpainting.from_pretrained(\n\""runwayml/stable-diffusion-inpainting\"", torch_dtype=torch.float16, variant=\""fp16\""\n).to(\""cuda\"")\n```\n\n```py\nimage = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, generator=generator).images[0]\n```\n\n```py\nimage = pipeline(prompt=\""road\"", image=init_image, mask_image=mask_image).images[0]\n```"", ""The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it's going to be used for. For example, here we are loading the `camembert-base` checkpoint in the `fill-mask` pipeline, which is completely fine. But if we were to load this checkpoint in the `text-classification` pipeline, the results would not make any sense because the head of `camembert-base` is not suitable for this task! We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:\n\nYou can also instantiate the checkpoint using the model architecture directly:\n```py\nfrom transformers import CamembertTokenizer, CamembertForMaskedLM\ntokenizer = CamembertTokenizer.from_pretrained(\""camembert-base\"")\nmodel = CamembertForMaskedLM.from_pretrained(\""camembert-base\"")\n```\nHowever, we recommend using the [`Auto*` classes](https://huggingface.co/transformers/model_doc/auto.html?highlight=auto#auto-classes) instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the `Auto*` classes makes switching checkpoints simple:\n```py\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from_pretrained(\""camembert-base\"")\nmodel = AutoModelForMaskedLM.from_pretrained(\""camembert-base\"")\n```"", ""``` python\npipe = IFImg2ImgPipeline.from\\_pretrained(\n\""DeepFloyd/IF-I-XL-v1.0\"",\ntext\\_encoder=text\\_encoder,\nunet=None,\ndevice\\_map=\""auto\""\n)\n```\n``` python\npipe = IFImg2ImgPipeline.from\\_pretrained(\n\""DeepFloyd/IF-I-XL-v1.0\"",\ntext\\_encoder=None,\nvariant=\""fp16\"",\ntorch\\_dtype=torch.float16,\ndevice\\_map=\""auto\""\n)\n```"", ""\""To make loading these components as simple as possible, we provide a single and unified method - `from\\_pretrained()` - that loads any of these components from either the Hugging Face [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) or your local machine.\"""", ""For example, if you're using a checkpoint for text-to-image and you want to use it again for image-to-image, use the [`~AutoPipelineForImage2Image.from\\_pipe`] method. This method creates a new pipeline from the components of a previously loaded pipeline at no additional memory cost. The [`~AutoPipelineForImage2Image.from\\_pipe`] method detects the original pipeline class and maps it to the new pipeline class corresponding to the task you want to do. For example, if you load a `\""stable-diffusion\""` class pipeline for text-to-image:\n```py\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch\npipeline\\_text2img = AutoPipelineForText2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n)\nprint(type(pipeline\\_text2img))\n\""\""\n```\nThen [`~AutoPipelineForImage2Image.from\\_pipe`] maps the original `\""stable-diffusion\""` pipeline class to [`StableDiffusionImg2ImgPipeline`]:\n```py\npipeline\\_img2img = AutoPipelineForImage2Image.from\\_pipe(pipeline\\_text2img)\nprint(type(pipeline\\_img2img))\n\""\""\n```"", ""ü§ó Diffusers is able to complete many different tasks, and you can often reuse the same pretrained weights for multiple tasks such as text-to-image, image-to-image, and inpainting. If you're new to the library and diffusion models though, it may be difficult to know which pipeline to use for a task. For example, if you're using the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint for text-to-image, you might not know that you could also use it for image-to-image and inpainting by loading the checkpoint with the [`StableDiffusionImg2ImgPipeline`] and [`StableDiffusionInpaintPipeline`] classes respectively."", ""If you're interested in using one of the official checkpoints for a task, explore the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations!"", ""`AutoPipeline` is designed to:\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method. To seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one. ```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch_dtype=torch.float16, use_safetensors=True\n).to(\""cuda\"")\n```"", ""```python\nfrom diffusers import DiffusionPipeline\nrepo_id = \""runwayml/stable-diffusion-v1-5\""\npipe = DiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)\n```\n```python\nfrom diffusers import StableDiffusionPipeline\nrepo_id = \""runwayml/stable-diffusion-v1-5\""\npipe = StableDiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)\n```\n```python\nfrom diffusers import StableDiffusionImg2ImgPipeline\nrepo_id = \""runwayml/stable-diffusion-v1-5\""\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(repo_id)\n```\n```bash\ngit-lfs install\ngit clone https://huggingface.co/runwayml/stable-diffusion-v1-5\n```\n```python\nfrom diffusers import DiffusionPipeline\nrepo_id = \""./stable-diffusion-v1-5\""\nstable_diffusion = DiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)\n```""]",The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights.,from_pretrained()
"What is the purpose of Diffusers library?
","[""üß® Diffusers Experimental\nWe are adding experimental code to support novel applications and usages of the Diffusers library."", ""This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training is very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline."", ""The aim of this project is to provide retrieval augmented diffusion models to diffusers!"", ""The Diffusers app ([App Store](https://apps.apple.com/app/diffusers/id1666309574), [source code](https://github.com/huggingface/swift-coreml-diffusers)) is the Mac counterpart to our [üß®`diffusers` library](https://github.com/huggingface/diffusers). This library is written in Python with PyTorch, and uses a modular design to train and run diffusion models."", ""Diffusers provides pre-trained diffusion models and serves as a modular toolbox for inference and training."", ""Diffusers is in the process of expanding to modalities other than images."", ""Diffusers essentially consists of three major classes: [pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines), [models](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models), and [schedulers](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers)."", ""Diffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library for a variety of use cases involving training or fine-tuning."", ""Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities. Its purpose is to serve as a **modular toolbox** for both inference and training."", ""Most of them are used internally for development, but they may also be useful to you if you're interested in building a diffusion model with some custom parts or if you're interested in some of our helper utilities for working with ü§ó Diffusers."", ""ü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, ü§ó Diffusers is a modular toolbox that supports both."", ""Welcome to üß® Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how üß® Diffusers is meant to be used. You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want. After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications."", ""ü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, ü§ó Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). The library has three main components:\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in ü§ó Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve. - Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality. - Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems."", ""Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities. Its purpose is to serve as a **modular toolbox** for both inference and training."", ""This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training is very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline.""]","The purpose of the Diffusers library is to serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities, including images, audio, and even 3D structures of molecules. It provides pre-trained diffusion models and allows users to easily customize and debug their models by separating different components of the diffusion pipeline. This modular design supports various applications, including training and fine-tuning diffusion models.",To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.
"What method does the EulerAncestralDiscreteScheduler use for sampling?
","[""# EulerAncestralDiscreteScheduler\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/)."", ""| A1111/k-diffusion | ü§ó Diffusers | Usage |\n|---------------------|-------------------------------------|---------------------------------------------------------------------------------------------------------------|\n| Euler a | [`EulerAncestralDiscreteScheduler`] | |"", ""# EulerDiscreteScheduler\nThe Euler scheduler (Algorithm 2) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\_diffusion/sampling.py#L51) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerDiscreteScheduler""]",The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling.,Ancestral sampling with Euler method steps.
"What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?
","[""Also, thanks to Niels Rogge, Younes Belkada, and Suraj Patil, among many others at Hugging Face, who laid out the foundations for increasing the use of multi-modal models from Transformers."", ""*To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a \""red bounding box\"" or \""pointed arrow\"". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark.*"", ""IDEFICS is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198), a state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image and text inputs and generates coherent text as output. It can answer questions about images, describe visual content, create stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b) and [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the ü§ó Hub."", ""Models such as [SimVLM](https://arxiv.org/abs/2108.10904) and [VirTex](https://arxiv.org/abs/2006.06666v3) use this pre-training objective and feature a unified multi-modal architecture consisting of a transformer encoder and transformer decoder, similar to that of an autoregressive language model. Flamingo sets a new state-of-the-art in few-shot learning on a wide range of open-ended vision and language tasks by adding Perceiver Resampler modules on top of the pre-trained frozen vision model and inserting new cross-attention layers between existing pre-trained and frozen LM layers to condition the LM on visual data."", ""Models such as [VisualGPT](https://arxiv.org/abs/2102.10407), [VC-GPT](https://arxiv.org/abs/2201.12723), and [Flamingo](https://arxiv.org/abs/2204.14198) use this pre-training strategy and are trained on image captioning and visual question-answering tasks. The main goal of such models is to balance the mixture of text generation capacity and visual information efficiently, which is highly important in the absence of large multi-modal datasets.""]",The large multimodal model that can solve image-text tasks and is based on Flamingo is called IDEFICS.,IDEFICS
"What is the purpose of the `gradio.Blocks` API?
","[""In this section we will introduce our **newly developed** low-level API called `gradio.Blocks`. Now, what's the difference between `Interface` and `Blocks`? - ‚ö° `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs. - üß± `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \""building blocks\""). \n\nThe `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\n- Group together related demos as multiple tabs in one web application\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a dropdown) or its visibility based on user input"", ""What does Gradio `Blocks` allow you to do?"", ""## ‰ªãÁªç\n‰Ω†Áü•ÈÅìÂêóÔºåÈô§‰∫Ü‰Ωú‰∏∫‰∏Ä‰∏™ÂÖ®Ê†àÊú∫Âô®Â≠¶‰π†ÊºîÁ§∫ÔºåGradio ÂùóÂ∫îÁî®ÂÖ∂ÂÆû‰πüÊòØ‰∏Ä‰∏™ÊôÆÈÄöÁöÑ Python ÂáΩÊï∞ÔºÅÔºü\nËøôÊÑèÂë≥ÁùÄÂ¶ÇÊûú‰Ω†Êúâ‰∏Ä‰∏™Âêç‰∏∫ `demo` ÁöÑ Gradio ÂùóÔºàÊàñÁïåÈù¢ÔºâÂ∫îÁî®Ôºå‰Ω†ÂèØ‰ª•ÂÉè‰ΩøÁî®‰ªª‰Ωï Python ÂáΩÊï∞‰∏ÄÊ†∑‰ΩøÁî® `demo`„ÄÇ\nÊâÄ‰ª•ÔºåÂÉè `output = demo(\""Hello\"", \""friend\"")` ËøôÊ†∑ÁöÑÊìç‰Ωú‰ºöÂú®ËæìÂÖ•‰∏∫ \""Hello\"" Âíå \""friend\"" ÁöÑÊÉÖÂÜµ‰∏ãËøêË°å `demo` ‰∏≠ÂÆö‰πâÁöÑÁ¨¨‰∏Ä‰∏™‰∫ã‰ª∂ÔºåÂπ∂Â∞ÜÂÖ∂Â≠òÂÇ®Âú®ÂèòÈáè `output` ‰∏≠„ÄÇ\n\n## ÁªìËØ≠\nÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂ∞ÜÂùóÂ∫îÁî®ËßÜ‰∏∫ÊôÆÈÄö Python ÂáΩÊï∞ÁöÑÊñπÊ≥ïÔºåËøôÊúâÂä©‰∫éÂú®‰∏çÂêåÁöÑÂ∫îÁî®Á®ãÂ∫è‰πãÈó¥ÁªÑÂêàÂäüËÉΩ„ÄÇ\n‰ªª‰ΩïÂùóÂ∫îÁî®Á®ãÂ∫èÈÉΩÂèØ‰ª•Ë¢´ËßÜ‰∏∫‰∏Ä‰∏™ÂáΩÊï∞Ôºå‰ΩÜ‰∏Ä‰∏™Âº∫Â§ßÁöÑÊ®°ÂºèÊòØÂú®Â∞ÜÂÖ∂ËßÜ‰∏∫ÂáΩÊï∞‰πãÂâçÔºå\nÂú®[Ëá™Â∑±ÁöÑÂ∫îÁî®Á®ãÂ∫è‰∏≠Âä†ËΩΩ](https://huggingface.co/spaces)ÊâòÁÆ°Âú®[Hugging Face Spaces](https://huggingface.co/spaces)‰∏äÁöÑÂ∫îÁî®Á®ãÂ∫è„ÄÇ"", ""\""We'll be using the `gradio.Blocks` class to build our Chatbot demo.\"""", ""Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. You can build very custom and complex applications using `gr.Blocks()`."", ""We briefly descirbed the Blocks class in the [Quickstart](/main/guides/quickstart#custom-demos-with-gr-blocks) as a way to build custom demos. Let's dive deeper. ## Blocks Structure Take a look at the demo below. $code\\_hello\\_blocks $demo\\_hello\\_blocks - First, note the `with gr.Blocks() as demo:` clause. The Blocks app code will be contained within this clause. - Next come the Components. These are the same Components used in `Interface`. However, instead of being passed to some constructor, Components are automatically added to the Blocks as they are created within the `with` clause. - Finally, the `click()` event listener. Event listeners define the data flow within the app. In the example above, the listener ties the two Textboxes together. The Textbox `name` acts as the input and Textbox `output` acts as the output to the `greet` method. This dataflow is triggered when the Button `greet\\_btn` is clicked. Like an Interface, an event listener can take multiple inputs or outputs. You can also attach event listeners using decorators - skip the `fn` argument and assign `inputs` and `outputs` directly: $code\\_hello\\_blocks\\_decorator"", ""Did you know that apart from being a full-stack machine learning demo, a Gradio Blocks app is also a regular-old python function!? This means that if you have a gradio Blocks (or Interface) app called `demo`, you can use `demo` like you would any python function. So doing something like `output = demo(\""Hello\"", \""friend\"")` will run the first event defined in `demo` on the inputs \""Hello\"" and \""friend\"" and store it in the variable `output`. By using apps like functions, you can seamlessly compose Gradio apps. \n\nOption 2 lets you borrow the functionality you want without tightly coupling our apps. All you have to do is call the `Blocks.load` class method in your source file. After that, you can use my translation app like a regular python function! \n\nIf the app you are loading defines more than one function, you can specify which function to use with the `fn_index` and `api_name` parameters. The `api_name` gives this function a unique name in our app. You can use this name to tell gradio which function in the upstream space you want to use. Functions in gradio spaces are zero-indexed."", ""\""We've created a new low-level language called **Gradio Blocks** that lets you build complex custom web apps, right in Python: ... The low-level Blocks API allows you to do all of this, right in Python.\"""", ""Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. You can build very custom and complex applications using `gr.Blocks()`.""]","The purpose of the `gradio.Blocks` API is to provide a low-level approach for designing web applications with more flexible layouts and data flows in Python. It allows developers to have full control over the data flows and layout of their applications, enabling the creation of complex, multi-step applications. With `gradio.Blocks`, you can:

- Control where components appear on the page.
- Handle complex data flows, where outputs can serve as inputs to other functions.
- Update properties or visibility of components based on user interaction.

This flexibility allows for the building of very custom and complex applications.","The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications."
"What is the purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents""?
","[""[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The abstract from the paper is following:\n\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\*"", ""*State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.*"", ""*In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts.*"", ""The Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process. The abstract from the paper is:\n\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\*"", ""*Here we propose a system that can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text or an image. This approach enables us to create a unified model (trained once) for three common segmentation tasks, which come with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation. We build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense prediction. After training on an extended version of the PhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on an additional image expressing the query. This novel hybrid input allows for dynamic adaptation not only to the three segmentation tasks mentioned above, but to any binary segmentation task where a text or image query can be formulated.*"", ""Stable unCLIP checkpoints are finetuned from [Stable Diffusion 2.1](./stable\\_diffusion/stable\\_diffusion\\_2) checkpoints to condition on CLIP image embeddings. Stable unCLIP still conditions on text embeddings. Given the two separate conditionings, stable unCLIP can be used for text guided image variation. When combined with an unCLIP prior, it can also be used for full text to image generation. The abstract from the paper is: *Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*"", ""AltCLIP (Altering the Language Encoder in CLIP) is a neural network trained on a variety of image-text and text-text pairs. By switching CLIP's text encoder with a pretrained multilingual text encoder XLM-R, we could obtain very close performances with CLIP on almost all tasks, and extended original CLIP's capabilities such as multilingual understanding. The abstract from the paper is the following: *In this work, we present a conceptually simple and effective method to train a strong bilingual multimodal representation model. Starting from the pretrained multimodal representation model CLIP released by OpenAI, we switched its text encoder with a pretrained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning.""]","The purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents"" is to leverage robust representations of images learned by contrastive models like CLIP for image generation. The model consists of a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. This approach aims to improve image diversity with minimal loss in photorealism and caption similarity, allowing for variations of an image that preserve its semantics and style while varying non-essential details. Additionally, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion.",The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.
"What command is used to install the requirements for a research project using ü§ó Transformers?
","[""# Research projects\nThis folder contains various research projects using ü§ó Transformers. They are not maintained and require a specific version of ü§ó Transformers that is indicated in the requirements file of each folder. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice."", ""You should install ü§ó Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies. Start by creating a virtual environment in your project directory:\n```bash\npython -m venv .env\n```\nActivate the virtual environment. On Linux and MacOs:\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n```bash\n.env/Scripts/activate\n```\nNow you're ready to install ü§ó Transformers with the following command:\n```bash\npip install transformers\n```\nFor CPU-support only, you can conveniently install ü§ó Transformers and a deep learning library in one line. For example, install ü§ó Transformers and PyTorch with:\n```bash\npip install 'transformers[torch]'\n```\nü§ó Transformers and TensorFlow 2.0:\n```bash\npip install 'transformers[tf-cpu]'\n```\nü§ó Transformers and Flax:\n```bash\npip install 'transformers[flax]'\n```\nFinally, check if ü§ó Transformers has been properly installed by running the following command. It will download a pretrained model:\n```bash\npython -c \""from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\""\n```\nThen print out the label and score:\n```bash\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```\n## Install from source\nInstall ü§ó Transformers from source with the following command:\n```bash\npip install git+https://github.com/huggingface/transformers\n```"", ""```bash\npip install -e \"".[dev]\""\n```\n```bash\npip install -e .\n```"", ""4. Set up a flax environment by running the following command in a virtual environment:\n```bash\n$ pip install -e \"".[flax]\""\n```\n(If transformers was already installed in the virtual environment, remove\nit with `pip uninstall transformers` before reinstalling it in editable\nmode with the `-e` flag.)\nRunning this command will automatically install `flax`, `jax` and `optax`. Next, you should also install the ü§ó Datasets library. Simply run the following steps:\n```\n$ cd ~/\n$ git clone https://github.com/huggingface/datasets.git\n$ cd datasets\n$ pip install -e \"".[streaming]\""\n```"", ""Setup transformers following instructions in README.md, (I would fork first). ```bash\ngit clone git@github.com:huggingface/transformers.git\ncd transformers\npip install -e . pip install pandas GitPython wget\n```"", ""**Important**\nTo make sure you can successfully run the latest versions of the example scripts, you have to **install the library from source** and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/transformers\ncd transformers\npip install . ```\nThen cd in the example folder of your choice and run\n```bash\npip install -r requirements.txt\n```"", ""```bash\npip install -e \"".[dev]\""\n```\nIf ü§ó Transformers was already installed in the virtual environment, remove\nit with `pip uninstall transformers` before reinstalling it in editable\nmode with the `-e` flag. Depending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\nfailure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n(PyTorch, TensorFlow and/or Flax) then do:\n```bash\npip install -e \"".[quality]\""\n```\n```bash\npip install \"".[docs]\""\n```"", ""# Installation\nInstall ü§ó Transformers for whichever deep learning library you're working with, setup your cache, and optionally configure ü§ó Transformers to run offline. ü§ó Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:\n\\* [PyTorch](https://pytorch.org/get-started/locally/) installation instructions."", ""### Installation\n### With pip\nThis repository is tested on Python 3.8+, Flax 0.4.1+, PyTorch 1.10+, and TensorFlow 2.6+. You should install ü§ó Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). First, create a virtual environment with the version of Python you're going to use and activate it. Then, you will need to install at least one of Flax, PyTorch, or TensorFlow. Please refer to [TensorFlow installation page](https://www.tensorflow.org/install/), [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation) installation pages regarding the specific installation command for your platform. When one of those backends has been installed, ü§ó Transformers can be installed using pip as follows:\n```bash\npip install transformers\n```\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must [install the library from source](https://huggingface.co/docs/transformers/installation#installing-from-source). ### With conda\nSince Transformers version v4.0.0, we now have a conda channel: `huggingface`. ü§ó Transformers can be installed using conda as follows:\n```shell script\nconda install -c huggingface transformers\n```\nFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda."", ""To successfully run the latest version of the example scripts, you have to **install ü§ó Transformers from source** in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/transformers\ncd transformers\npip install . ```\nThen switch your current clone of ü§ó Transformers to a specific version, like v3.5.1 for example:\n```bash\ngit checkout tags/v3.5.1\n```\nAfter you've setup the correct library version, navigate to the example folder of your choice and install the example specific requirements:\n```bash\npip install -r requirements.txt\n```"", ""4. Set up a flax environment by running the following command in a virtual environment:\n```bash\n$ pip install -e \"".[flax]\""\n```\n(If transformers was already installed in the virtual environment, remove\nit with `pip uninstall transformers` before reinstalling it in editable\nmode with the `-e` flag.)\nIf you have already cloned that repo, you might need to `git pull` to get the most recent changes in the `datasets`\nlibrary. Running this command will automatically install `flax`, `jax` and `optax`. Next, you should also install the ü§ó Datasets library. We strongly recommend installing the\nlibrary from source to profit from the most current additions during the community week. Simply run the following steps:\n```\n$ cd ~/\n$ git clone https://github.com/huggingface/datasets.git\n$ cd datasets\n$ pip install -e \"".[streaming]\""\n```"", ""```bash\npip install -e \"".[dev]\""\n```\nIf ü§ó Transformers was already installed in the virtual environment, remove\nit with `pip uninstall transformers` before reinstalling it in editable\nmode with the `-e` flag. Depending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\nfailure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n(PyTorch, TensorFlow and/or Flax) then do:\n```bash\npip install -e \"".[quality]\""\n```\nwhich should be enough for most use cases. \n```bash\npip install \"".[docs]\""\n```""]","To install the requirements for a research project using ü§ó Transformers, you should run the following command inside the folder of your choice:

```
pip install -r requirements.txt
```",pip install -r requirements.txt
"What task does the `roberta-large-mnli` checkpoint perform?
","[""```python\nroberta_checkpoint = \""roberta-large\""\n```"", ""\""Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform? roberta-large-mnli page.\""\n\""Text classification\""\n\""More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) ‚Äî a task also called *natural language inference*.\"""", ""```python\nfrom transformers import AutoModelForSequenceClassification\nroberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_checkpoint, num_labels=2)\n```""]","The `roberta-large-mnli` checkpoint performs text classification. More precisely, it classifies if two sentences are logically linked across three labels: contradiction, neutral, and entailment ‚Äî a task also called *natural language inference*.",Text classification
"What service is replacing the Paid tier of the Inference API at Hugging Face?
","[""- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. - There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested!"", ""Benefit from higher rate limits, custom accelerated endpoints for the latest models, and early access to features. If you've built some exciting projects with the Inference API or are looking for a model not available in Inference for PROs, please [use this discussion](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/13). [Enterprise users](https://huggingface.co/enterprise) also benefit from PRO Inference API on top of other features, such as SSO."", ""First of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints)."", ""- [Inference API](https://huggingface.co/docs/api-inference/index): a service that allows you to run accelerated inference on Hugging Face's infrastructure for free. This service is a fast way to get started, test different models, and prototype AI products. - [Inference Endpoints](https://huggingface.co/inference-endpoints): a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice."", ""- Inference Endpoints are more expensive that what we were doing before, there‚Äôs an increased cost of between 24% and 50%. At the scale we‚Äôre currently operating, this additional cost, a difference of ~$60 a month for a large CPU instance is nothing compared to the time and cognitive load we are saving by not having to worry about APIs, and containers.\n- You can find pricing for Inference Endpoints [here](https://huggingface.co/pricing#endpoints), but a different number is displayed when you deploy a new endpoint from the [GUI](https://ui.endpoints.huggingface.co/new). I‚Äôve used the latter, which is higher.\n- We find Hugging Face Inference Endpoints to be a very simple and convenient way to deploy transformer (and [sklearn](https://huggingface.co/scikit-learn)) models into an endpoint so they can be consumed by an application. Whilst they cost a little more than the ECS approach we were using before, it‚Äôs well worth it because it saves us time on thinking about deployment."", ""## Production with Inference Endpoints\nOnce you're happy with the performance of your ML model, it's time to deploy it for production. Unfortunately, when leaving the sandbox, everything becomes a concern: security, scaling, monitoring, etc. This is where a lot of ML stumble and sometimes fall. We built [Inference Endpoints](https://huggingface.co/inference-endpoints) to solve this problem. In just a few clicks, Inference Endpoints let you deploy any hub model on secure and scalable infrastructure, hosted in your AWS or Azure region of choice. Additional settings include CPU and GPU hosting, built-in auto-scaling, and more. This makes finding the appropriate cost/performance ratio easy, with [pricing](https://huggingface.co/pricing#endpoints) starting as low as $0.06 per hour. Inference Endpoints support three security levels:\n\\* Public: the endpoint runs in a public Hugging Face subnet, and anyone on the Internet can access it without any authentication. \\* Protected: the endpoint runs in a public Hugging Face subnet, and anyone on the Internet with the appropriate Hugging Face token can access it. \\* Private: the endpoint runs in a private Hugging Face subnet and is not accessible on the Internet. It's only available through a private connection in your AWS or Azure account."", ""- [Inference API](https://huggingface.co/docs/api-inference/index): a service that allows you to run accelerated inference on Hugging Face's infrastructure for free. This service is a fast way to get started, test different models, and prototype AI products. - [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index): a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. These services can be called with the [`InferenceClient`] object. It acts as a replacement for the legacy [`InferenceApi`] client, adding specific support for tasks and handling inference on both [Inference API](https://huggingface.co/docs/api-inference/index) and [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index).""]",The service replacing the Paid tier of the Inference API at Hugging Face is the Inference Endpoints.,Inference Endpoints
"What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?
","[""The key difference between the BERT architecture and the SqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial) instead of fully-connected layers for the Q, K, V and FFN layers."", ""1. \\*\\*[SqueezeBERT](https://huggingface.co/docs/transformers/model\\_doc/squeezebert)\\*\\* (‡§¨‡§∞‡•ç‡§ï‡§≤‡•á ‡§∏‡•á) ‡§ï‡§æ‡§ó‡§ú ‡§ï‡•á ‡§∏‡§æ‡§• [SqueezeBERT: ‡§ï‡•Å‡§∂‡§≤ ‡§§‡§Ç‡§§‡•ç‡§∞‡§ø‡§ï‡§æ ‡§®‡•á‡§ü‡§µ‡§∞‡•ç‡§ï ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç NLP ‡§ï‡•ã ‡§ï‡§Ç‡§™‡•ç‡§Ø‡•Ç‡§ü‡§∞ ‡§µ‡§ø‡§ú‡§º‡§® ‡§ï‡•ç‡§Ø‡§æ ‡§∏‡§ø‡§ñ‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à?](https: //arxiv.org/abs/2006.11316) ‡§´‡•â‡§∞‡•á‡§∏‡•ç‡§ü ‡§è‡§®."", ""We demonstrate how to replace several operations in self-attention layers with grouped convolutions, and we use this technique in a novel network architecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test set.""]","SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers.",Grouped convolutions
"What type of license is the HuggingFace Team's software distributed under?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. -->"", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. -->"", ""[Licenses](./repositories-licenses)"", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. -->""]","The HuggingFace Team's software is distributed under the Apache License, Version 2.0.","Apache License, Version 2.0"
"What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?
","[""It presents two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT:\n- Splitting the embedding matrix into two smaller matrices. \n- Using repeating layers split among groups. \n\nTo address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. \n\nALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same number of (repeating) layers. \n\nLayers are split in groups that share parameters (to save memory)."", ""# ü§ó PEFT\n\n### State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods\n\nParameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. Fine-tuning large-scale PLMs is often prohibitively costly. In this regard, PEFT methods only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs. Recent State-of-the-Art PEFT techniques achieve performance comparable to that of full fine-tuning."", ""[AdaLoRA](https://hf.co/papers/2303.10512) is a method for optimizing the number of trainable parameters to assign to weight matrices and layers, unlike LoRA, which distributes parameters evenly across all modules. More parameters are budgeted for important weight matrices and layers while less important ones receive fewer parameters. The abstract from the paper is:\n\\*Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings."", ""Movement Pruning: Adaptive Sparsity by Fine-Tuning  \nAuthor: @VictorSanh  \n\\*Magnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of \\*movement pruning\\*, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3% of the model parameters:\\*  \nThis page contains information on how to fine-prune pre-trained models such as `BERT` to obtain extremely sparse models with movement pruning."", ""1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut."", ""**Efficient 8-bit matrix multiplication is a method that has been first introduced in the paper [LLM.int8()](https://arxiv.org/abs/2208.07339) and aims to solve the performance degradation issue when quantizing large-scale models. The proposed method breaks down the matrix multiplications that are applied under the hood in Linear layers in two stages: the outlier hidden states part that is going to be performed in float16 & the ‚Äúnon-outlier‚Äù part that is performed in int8.** \n\n**In a nutshell, you can reduce the size of a full-precision model by 4 (thus, by 2 for half-precision models) if you use 8-bit matrix multiplication.** \n\n**In 2021, a paper called LoRA: Low-Rank Adaption of Large Language Models demonstrated that fine tuning of large language models can be performed by freezing the pretrained weights and creating low rank versions of the query and value layers attention matrices. These low rank matrices have far fewer parameters than the original model, enabling fine-tuning with far less GPU memory. The authors demonstrate that fine-tuning of low-rank adapters achieved comparable results to fine-tuning the full pretrained model.** \n\n**This technique allows the fine tuning of LLMs using a fraction of the memory requirements.**"", ""ALBERT reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters."", ""**Model merging** is a way to fuse the weights of different models together in a single model to (ideally) combine the respective strengths of each model in a unified single model. A few techniques exist to do so that have been extended and often published mostly in community forums, a striking case of fully decentralized research happening all over the world between a community of practitioners, researchers, and hobbyists. One of the simplest published methods consists in averaging the parameters of a set of models sharing a common architecture ([example 1](https://huggingface.co/papers/2204.03044), [example 2](https://huggingface.co/papers/2109.01903)) but more complex parameter combinations exist, such as determining which parameters are the most influential in each model for a given task ([weighted averaging](https://huggingface.co/papers/2111.09832)), or considering parameters interference between models before selecting which parameters to keep when merging ([ties merging](https://huggingface.co/papers/2306.01708)). These techniques allow anybody to easily generate combinations of models and are made especially easy by the fact that most models are nowadays variations on the same architecture. That's the reason some models submitted to the [open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) have names such as `llama2-zephyr-orca-ultra`. This particular example is likely a merge of `llama2` and `zephyr` models, fine-tuned on orca and ultra datasets. Usually, more details are to be found in the respective model card on the Hugging Face hub. Sometimes, you may want more controlled personalization, without enough memory to load a whole model in memory to fine tune it. Did you know that you don't need to use an entire model when fine-tuning? You might want to use what is called **parameter efficient fine-tuning** (PEFT). This technique first freezes up the parameters of your pretrained model of interest, then adds a number of new parameters on top of it, called the adapters."", ""- LoRA, Low-Rank Adaptation, is a PEFT method that shares similarities with Adapter layers."", ""Another option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the [`peft`](https://github.com/huggingface/peft) library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit. Loading the model in 8bit reduces the memory footprint drastically since you only need one byte per parameter for the weights."", ""Some parameters of the LM are frozen because fine-tuning an entire 10B or 100B+ parameter model is prohibitively expensive (for more, see Low-Rank Adaptation ([LoRA](https://arxiv.org/abs/2106.09685)) for LMs or the [Sparrow](https://arxiv.org/abs/2209.14375) LM from DeepMind) -- depending on the scale of the model and infrastructure being used. The exact dynamics of how many parameters to freeze, or not, is considered an open research problem.""]","The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:

1. Splitting the embedding matrix into two smaller matrices.
2. Using repeating layers split among groups that share parameters.",Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.
"What are the three main steps for fine-tuning a model with the ü§ó Datasets library?
","[""In [Chapter 3](/course/chapter3) you got your first taste of the ü§ó Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n1. Load a dataset from the Hugging Face Hub. \n2. Preprocess the data with `Dataset.map()`. \n3. Load and compute metrics."", ""This directory contains a script, `run\\_funsd\\_cord.py`, that can be used to fine-tune (or evaluate) LayoutLMv3 on form understanding datasets, such as [FUNSD](https://guillaumejaume.github.io/FUNSD/) and [CORD](https://github.com/clovaai/cord). The script `run\\_funsd\\_cord.py` leverages the ü§ó Datasets library and the Trainer API. You can easily customize it to your needs. ## Fine-tuning on FUNSD\nFine-tuning LayoutLMv3 for token classification on [FUNSD](https://guillaumejaume.github.io/FUNSD/) can be done as follows:\n```bash\npython run\\_funsd\\_cord.py \\\n--model\\_name\\_or\\_path microsoft/layoutlmv3-base \\\n--dataset\\_name funsd \\\n--output\\_dir layoutlmv3-test \\\n--do\\_train \\\n--do\\_eval \\\n--max\\_steps 1000 \\\n--evaluation\\_strategy steps \\\n--eval\\_steps 100 \\\n--learning\\_rate 1e-5 \\\n--load\\_best\\_model\\_at\\_end \\\n--metric\\_for\\_best\\_model \""eval\\_f1\"" \\\n--push\\_to\\_hub \\\n--push\\_to\\_hub¬∞model\\_id layoutlmv3-finetuned-funsd\n```"", ""# Fine-tune a pretrained model\nThere are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ü§ó Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:\n\\* Fine-tune a pretrained model with ü§ó Transformers [`Trainer`]. \\* Fine-tune a pretrained model in TensorFlow with Keras. \\* Fine-tune a pretrained model in native PyTorch. ## Prepare a dataset\nBefore you can fine-tune a pretrained model, download a dataset and prepare it for training. The previous tutorial showed you how to process data for training, and now you get an opportunity to put those skills to the test! Begin by loading the [Yelp Reviews](https://huggingface.co/datasets/yelp\\_review\\_full) dataset:"", ""```py\n## Load Food-101 dataset\nStart by loading a smaller subset of the Food-101 dataset from the ü§ó Datasets library. This will give you a chance to\nexperiment and make sure everything works before spending more time training on the full dataset. ```py\n>>> from datasets import load\\_dataset\n>>> food = load\\_dataset(\""food101\"", split=\""train[:5000]\"")\n```\nSplit the dataset's `train` split into a train and test set with the [`~datasets.Dataset.train\\_test\\_split`] method:\n```py\n>>> food = food.train\\_test\\_split(test\\_size=0.2)\n```\n## Preprocess\nThe next step is to load a ViT image processor to process the image into a tensor:\n```py\n>>> from transformers import AutoImageProcessor\n>>> checkpoint = \""google/vit-base-patch16-224-in21k\""\n>>> image\\_processor = AutoImageProcessor.from\\_pretrained(checkpoint)\n```\n```py\n>>> from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n>>> normalize = Normalize(mean=image\\_processor.image\\_mean, std=image\\_processor.image\\_std)\n>>> size = (\n... image\\_processor.size[\""shortest\\_edge\""]\n...\n```""]","The three main steps for fine-tuning a model with the ü§ó Datasets library are:

1. Load a dataset from the Hugging Face Hub.
2. Preprocess the data with `Dataset.map()`.
3. Load and compute metrics.",1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.
"What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?
","[""Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers."", ""\""We achieve a reliable 10x speed up compared to an out-of-box deployment for a given model/hardware pair.\"""", ""Hugging Face‚Äôs [Optimum library](https://huggingface.co/docs/optimum/index), through its integration with ONNX Runtime for training, provides an open solution to __improve training times by 35% or more__ for many popular Hugging Face models. The chart below shows impressive acceleration __from 39% to 130%__ for Hugging Face models with Optimum when __using ONNX Runtime and DeepSpeed ZeRO Stage 1__ for training. The performance measurements were done on selected Hugging Face models with PyTorch as the baseline run, only ONNX Runtime for training as the second run, and ONNX Runtime + DeepSpeed ZeRO Stage 1 as the final run, showing maximum gains."", ""### Throughput\nBelow you can find the throughput comparison for running infinity on 2 physical cores with batch size 1, compared with vanilla transformers. Figure 3. Throughput: Infinity vs Transformers\n\n| Sequence Length | Infinity | Transformers | improvement |\n|-----------------|-------------|--------------|-------------|\n| 8 | 248 req/sec | 49 req/sec | +506% |\n| 16 | 212 req/sec | 50 req/sec | +424% |\n| 32 | 150 req/sec | 40 req/sec | +375% |\n| 64 | 97 req/sec | 28 req/sec | +346% |\n| 128 | 55 req/sec | 18 req/sec | +305% |\n| 256 | 27 req/sec | 9 req/sec | +300% |\n| 384 | 17 req/sec | 5 req/sec | +340% |\n| 512 | 12 req/sec | 4 req/sec | +300% |"", ""This ice-lake optimized Infinity Container can achieve up to 800% better latency & throughput compared to vanilla transformers running on ice-lake.""]",The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is up to 800%.,+800%
"What is the command to upload a spaCy pipeline to the Hugging Face Hub?
","[""* spaCy [documentation](https://spacy.io/universe/project/spacy-huggingface-hub/)"", ""You can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`. ```bash\nhuggingface-cli login\npython -m spacy package ./en\\_ner\\_fashion ./output --build wheel\ncd ./output/en\\_ner\\_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en\\_ner\\_fashion-0.0.0-py3-none-any.whl\n```\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card. The command will output two things:\n\\* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en\\_core\\_web\\_sm\n\\* And how to install the pipeline directly from the Hub! ### From a Python script\nYou can use the `push` function from Python. It returns a dictionary containing the `\""url\""` and \""`whl\\_url`\"" of the published model and the wheel file, which you can later install with `pip install`. ```py\nfrom spacy\\_huggingface\\_hub import push\nresult = push(\""./en\\_ner\\_fashion-0.0.0-py3-none-any.whl\"")\nprint(result[\""url\""])\n```"", ""| Partially handled by `huggingface\\_hub`. |"", ""To learn how to upload models to the Hub, you can refer to the [Repositories Getting Started Guide](./repositories-getting-started)."", ""spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text. Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!"", ""The `push\\_to\\_hub()` method accepts several arguments, making it possible to upload to a specific repository or organization namespace, or to use a different API token. The `push\\_to\\_hub()` method is backed by the [`huggingface\\_hub`](https://github.com/huggingface/huggingface\\_hub) Python package, which offers a direct API to the Hugging Face Hub. It's integrated within ü§ó Transformers and several other machine learning libraries, like [`allenlp`](https://github.com/allenai/allennlp). The `huggingface\\_hub` Python library is a package which offers a set of tools for the model and datasets hubs. It provides simple methods and classes for common tasks like getting information about repositories on the hub and managing them. It provides simple APIs that work on top of git to manage those repositories' content and to integrate the Hub in your projects and libraries. Similarly to using the `push\\_to\\_hub` API, this will require you to have your API token saved in your cache. In order to do this, you will need to use the `login` command from the CLI, as mentioned in the previous section (again, make sure to prepend these commands with the `!` character if running in Google Colab): ```bash huggingface-cli login ``` The `huggingface\\_hub` package offers several methods and classes which are useful for our purpose."", ""```py\nYou can also push an entire pipeline with all it's components to the Hub. For example, initialize the components of a [`StableDiffusionPipeline`] with the parameters you want:\n```py\nfrom diffusers import (\nUNet2DConditionModel,\nAutoencoderKL,\nDDIMScheduler,\nStableDiffusionPipeline,\n)\nfrom transformers import CLIPTextModel, CLIPTextConfig, CLIPTokenizer\nunet = UNet2DConditionModel(\nblock\\_out\\_channels=(32, 64),\nlayers\\_per\\_block=2,\nsample\\_size=32,\nin\\_channels=4,\nout\\_channels=4,\ndown\\_block\\_types=(\""DownBlock2D\"", \""CrossAttnDownBlock2D\""),\nup\\_block\\_types=(\""CrossAttnUpBlock2D\"", \""UpBlock2D\""),\ncross\\_attention\\_dim=32,\n)\nscheduler = DDIMScheduler(\nbeta\\_start=0.00085,\nbeta\\_end=0.012,\nbeta\\_schedule=\""scaled\\_linear\"",\nclip\\_sample=False,\nset\\_alpha\\_to\\_one=False,\n)\nvae = AutoencoderKL(\nblock\\_out\\_channels=[32, 64],\nin\\_channels=3,\nout\\_channels=3,\ndown\\_block\\_types=[\""DownEncoderBlock2D\"", \""DownEncoderBlock2D\""],\nup\\_block\\_types=[\""UpDecoderBlock2D\"", \""UpDecoderBlock2D\""],\nlatent\\_channels=4,\n)\ntext\\_encoder\\_config = CLIPTextConfig(\nbos\\_token\\_id=0,\neos\\_token\\_id=2,\nhidden\\_size=32,\nintermediate\\_size=37,\nlayer\\_norm\\_eps=1e-05,\nnum\\_attention\\_heads=4,\nnum\\_hidden\\_layers=5,\npad\\_token\\_id=1,\nvocab\\_size=1000,\n)\ntext\\_encoder = CLIPTextModel(text\\_encoder\\_config)\ntokenizer = CLIPTokenizer.from\\_pretrained(\""hf-internal-testing/tiny-random-clip\"")\n```\nPass all of the components to the [`StableDiffusionPipeline`] and call [`~diffusers.utils.PushToHubMixin.push\\_to\\_hub`] to push the pipeline to the Hub:\n```py\ncomponents = {\n\""unet\"": unet,\n\""scheduler\"": scheduler,\n\""vae\"": vae,\n\""text\\_encoder\"": text\\_encoder,\n\""tokenizer\"": tokenizer,\n\""safety\\_checker\"": None,\n\""feature\\_extractor\"": None,\n}\npipeline = StableDiffusionPipeline(\\*\\*components)\npipeline.push\\_to\\_hub(\""my-pipeline\"")\n```\nThe [`~diffusers.utils.PushToHubMixin.push\\_to\\_hub`] function saves each component to a subfolder in the repository. Now you can reload the pipeline from your repository on the Hub:\n```py\npipeline = StableDiffusionPipeline.from\\_pretrained(\""your-namespace/my-pipeline\"")\n``` \n```"", ""### Create a repository\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(repo\\_id=\""super-cool-model\"")\n```\n### Upload files\nUpload a single file\n```py\nfrom huggingface\\_hub import upload\\_file\nupload\\_file(\npath\\_or\\_fileobj=\""/home/lysandre/dummy-test/README.md\"",\npath\\_in\\_repo=\""README.md\"",\nrepo\\_id=\""lysandre/test-model\"",\n)\n```\nOr an entire folder\n```py\nfrom huggingface\\_hub import upload\\_folder\nupload\\_folder(\nfolder\\_path=\""/path/to/local/space\"",\nrepo\\_id=\""username/my-cool-space\"",\nrepo\\_type=\""space\"",\n)\n```"", ""### Sharing your models\n### Using the spaCy CLI (recommended)\nThe `spacy-huggingface-hub` library extends `spaCy` native CLI so people can easily push their packaged models to the Hub.""]","The command to upload a spaCy pipeline to the Hugging Face Hub is as follows:

```bash
huggingface-cli login
python -m spacy package ./en_ner_fashion ./output --build wheel
cd ./output/en_ner_fashion-0.0.0/dist
python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
```

This will package your spaCy pipeline and push it to the Hub.",python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
"What is the time and memory complexity of the Nystr√∂mformer's approximation of self-attention?
","[""Transformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. The [Nystr√∂mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nystr√∂mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention."", ""Only at longer sequence lengths of 4096, a slight decrease in memory usage can be seen. Now a clear decrease in peak memory usage can be seen for longer input sequences."", ""Transformer-XL has a memory complexity of \\\\(O(n^2+ n n\\_m)\\\\), which shows that memory cost can increase significantly for very large \\\\(n\\_m\\\\). Hence, Transformer-XL has to eventually discard past activations from the memory when the number of cached activations gets larger than \\\\(n\\_m\\\\). Compressive Transformer addresses this problem by adding an additional compressed memory to efficiently cache past activations that would have otherwise eventually been discarded."", ""Regarding the memory complexity, we now have two terms that compete which each other to be the memory bottleneck: the dot-product: \\\\( \\mathcal{O}(n\\_{h} \\* n\\_{c} \\* l\\_{c}^2) = \\mathcal{O}(n \\* n\\_{h} \\* l\\_{c}) \\\\) and the required memory for LSH bucketing: \\\\( \\mathcal{O}(n \\* n\\_{h} \\* \\frac{n\\_{b}}{2}) \\\\) with \\\\( l\\_{c} \\\\) being the chunk length. Because for large \\\\( n \\\\), the number of buckets \\\\( \\frac{n\\_{b}}{2} \\\\) grows much faster than the chunk length \\\\( l\\_{c} \\\\), the user can again factorize the number of buckets `config.num\\_buckets` as explained [here](https://huggingface.co/transformers/model\\_doc/reformer.html#lsh-self-attention)."", ""Using Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually represents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n\\_s \\times n\\_s)\\\\) to \\\\(\\mathcal{O}(n\\_s \\times w)\\\\), with \\\\(n\\_s\\\\) being the sequence length and \\\\(w\\\\) being the average window size."", ""*While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences -- a topic being actively studied in the community. To address this limitation, we propose Nystr√∂mformer -- a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nystr√∂m method to approximate standard self-attention with O(n) complexity. The scalability of Nystr√∂mformer enables application to longer sequences with thousands of tokens.*"", ""In Reformer, each part of the standard transformer architecture is re-engineered to optimize for minimal memory requirement without a significant drop in performance. The memory improvements can be attributed to **4** features which the Reformer authors introduced to the transformer world:\n1. **Reformer Self-Attention Layer** - *How to efficiently implement self-attention without being restricted to a local context?*\n2. **Chunked Feed Forward Layers** - *How to get a better time-memory trade-off for large feed forward layers?*\n3. **Reversible Residual Layers** - *How to drastically reduce memory consumption in training by a smart residual architecture?*\n4. **Axial Positional Encodings** - *How to make positional encodings usable for extremely large input sequences?*"", ""The memory consumption is thus reduced from \\\\(n\\_s^2+ n\\_s n\\_t +n\\_t^2\\\\) to \\\\(w n\\_s +gn\\_s +n\\_s n\\_t +n\\_t^2\\\\) with \\\\(n\\_s\\\\) and \\\\(n\\_t\\\\) being the source (encoder input) and target (decoder input) lengths respectively. For Longformer Encoder-Decoder to be efficient, it is assumed that \\\\(n\\_s\\\\) is much bigger than \\\\(n\\_t\\\\)."", ""The four papers above offer different ways to deal with the quadratic memory complexity of the self-attention mechanism, usually by reducing it to linear complexity. Linformer and Longformer both rely on the observation that the self-attention matrix does not contain \\\\(n √ó n\\\\) worth of information (the attention matrix is low-rank and sparse). Performer gives a principled method to approximate the softmax-attention kernel (and any kernelizable attention mechanisms beyond softmax). Compressive Transformer offers an orthogonal approach to model long range dependencies based on recurrence. These different inductive biases have implications in terms of computational speed and generalization beyond the training setup. In particular, Linformer and Longformer lead to different trade-offs: Longformer explicitly designs the sparse attention patterns of the self-attention (fixed patterns) while Linformer learns the low-rank matrix factorization of the self-attention matrix. In our experiments, Longformer is less efficient than Linformer, and is currently highly dependent on implementation details. On the other hand, Linformer‚Äôs decomposition only works for fixed context length (fixed at training) and cannot generalize to longer sequences without specific adaptation. Moreover, it cannot cache previous activations which can be extremely useful in the generative setup. Interestingly, Performer is conceptually different: it learns to approximate the softmax attention kernel without relying on any sparsity or low-rank assumption."", ""The peak GPU memory consumption for self-attention layers grows *quadratically* both in compute and memory complexity with number of input tokens (also called *sequence length*) that we denote in the following by \\\\( N \\\\) . While this is not really noticeable for shorter input sequences (of up to 1000 input tokens), it becomes a serious problem for longer input sequences (at around 16000 input tokens). The projections \\\\( \\mathbf{Q} \\\\) and \\\\( \\mathbf{K} \\\\) will each consist of \\\\( N \\\\) vectors resulting in the \\\\( \\mathbf{QK}^T \\\\) being of size \\\\( N^2 \\\\) . Assuming, the LLM has 40 attention heads and runs in bfloat16 precision, we can calculate the memory requirement to store the \\\\( \\mathbf{QK^T} \\\\) matrices to be \\\\( 40 \\* 2 \\* N^2 \\\\) bytes. For \\\\( N=1000 \\\\) only around 50 MB of VRAM are needed, however, for \\\\( N=16000 \\\\) we would need 19 GB of VRAM, and for \\\\( N=100,000 \\\\) we would need almost 1TB just to store the \\\\( \\mathbf{QK}^T \\\\) matrices."", ""## Conclusion\nNystr√∂mformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other linear self-attention schemes."", ""As expected using local and LSH self-attention is much more memory efficient for longer input sequences, so that the model runs out of memory only at 16K tokens for a 11GB RAM GPU in this notebook."", ""The peak GPU memory consumption for self-attention layers grows *quadratically* both in compute and memory complexity with number of input tokens (also called *sequence length*) that we denote in the following by \\( N \\). While this is not really noticeable for shorter input sequences (of up to 1000 input tokens), it becomes a serious problem for longer input sequences (at around 16000 input tokens). The projections \\( \\mathbf{Q} \\) and \\( \\mathbf{K} \\) will each consist of \\( N \\) vectors resulting in the \\( \\mathbf{QK}^T \\) being of size \\( N^2 \\). Assuming, the LLM has 40 attention heads and runs in bfloat16 precision, we can calculate the memory requirement to store the \\( \\mathbf{QK^T} \\) matrices to be \\( 40 * 2 * N^2 \\) bytes. For \\( N=1000 \\) only around 50 MB of VRAM are needed, however, for \\( N=16000 \\) we would need 19 GB of VRAM, and for \\( N=100,000 \\) we would need almost 1TB just to store the \\( \\mathbf{QK}^T \\) matrices.""]","Nystr√∂mformer approximates standard self-attention with a time and memory complexity of \(O(n)\), where \(n\) is the length of the input sequence. This efficient complexity allows Nystr√∂mformer to exhibit competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention.",O(n)
"What is the goal of the Named Entity Recognition task in token classification?
","[""* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names."", ""## Abstract\n----\n> We present a novel approach to named entity recognition (NER) in the presence of scarce data that we call example-based NER. Our train-free few-shot learning approach takes inspiration from question-answering to identify entity spans in a new and unseen domain. In comparison with the current state-of-the-art, the proposed method performs significantly better, especially when using a low number of support examples."", ""The goal of this task is to classify each token in the input text into one of the following categories: person, organization, location, or none of them."", ""Named-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \""token\"") into different categories, such as names of people or names of locations, or different parts of speech. For example, given the sentence:\n> Does Chicago have any Pakistani restaurants? A named-entity recognition algorithm may identify:\n- \""Chicago\"" as a **location**\n- \""Pakistani\"" as an **ethnicity**"", ""The last step is to group together tokens that correspond to the same entity. This is why we had two labels for each type of entity: I-PER and B-PER for instance. It allows us to know if a token is in the same entity as the previous one. Note that there are two ways of labelling used for token classification, one (in pink here) uses the B-PER label at the beginning of each new entity, but the other (in blue) only uses it to separate two adjacent entities of the same type. In both cases, we can flag a new entity each time we see a new label appearing (either with the I or B prefix) then take all the following tokens labelled the same, with an I-flag. This, coupled with the offset mapping to get the start and end characters allows us to get the span of texts for each entity."", ""To use BERT for token classification tasks like named entity recognition (NER), add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and each token to find the most likely label."", ""Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels. You can extract important information from invoices using named entity recognition models, such as date, organization name or address."", ""The letter that prefixes each `ner_tag` indicates the token position of the entity:\n- `B-` indicates the beginning of an entity. - `I-` indicates a token is contained inside the same entity (for example, the `State` token is a part of an entity like `Empire State Building`). - `0` indicates the token doesn't correspond to any entity."", ""Token classification regroups any task that can be framed as labelling each word (or token) in a sentence, like identifying the persons, organizations and locations for instance. Here we are doing named entity recognitions, so ours labels are either O for words that do not belong to any entity, LOC, for location, PER, for person, ORG for organization and MISC for miscellaneous. Each label has two versions: the B- labels indicate a word that begins an entity while the I- labels indicate a word that is inside an entity."", ""- **Named entity recognition (NER)**: Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for \""no entity.\"""", ""The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity."", ""Token classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization."", ""In the pipeline video, we looked at the different applications the Transformers library supports out of the box, one of them being token classification, for instance predicting for each word in a sentence whether they correspond to a person, an organization or a location. We can even group together the tokens corresponding to the same entity, for instance all the tokens that formed the word Sylvain here, or Hugging and Face. The token classification pipeline works the same way as the text classification pipeline we studied in a previous video. There are three steps: the tokenization, the model, and the post processing. The first two steps are identical to the text classification pipeline, except we use an auto token classification model instead of a sequence classification one. We tokenize our text then feed it to the model. Instead of getting one number for each possible label for the whole sentence, we get one number for each of the possible 9 labels for every token in the sentence, here 19. Like all the other models of the Transformers library, our model outputs logits, which we turn into predictions by using a SoftMax. We also get the predicted label for each token by taking the maximum prediction (since the softmax function preserves the order, we could have done it on the logits if we had no need of the predictions)."", ""The label `B-XXX` indicates the token is at the beginning of an entity `XXX` and the label `I-XXX` indicates the token is inside the entity `XXX`. For instance, in the current example we would expect our model to classify the token `S` as `B-PER` (beginning of a person entity) and the tokens `##yl`, `##va` and `##in` as `I-PER` (inside a person entity). You might think the model was wrong in this case as it gave the label `I-PER` to all four of these tokens, but that's not entirely true. There are actually two formats for those `B-` and `I-` labels: *IOB1* and *IOB2*. The IOB2 format (in pink below), is the one we introduced whereas in the IOB1 format (in blue), the labels beginning with `B-` are only ever used to separate two adjacent entities of the same type. The model we are using was fine-tuned on a dataset using that format, which is why it assigns the label `I-PER` to the `S` token."", ""### Emerging and Rare Entities task: WNUT‚Äô17 (English NER) dataset\nDescription of the WNUT‚Äô17 task from the [shared task website](http://noisy-text.github.io/2017/index.html):\n> The WNUT‚Äô17 shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions."", ""The last step is to group together tokens that correspond to the same entity. This is why we had two labels for each type of entity: I-PER and B-PER for instance. It allows us to know if a token is in the same entity as the previous one. Note that there are two ways of labelling used for token classification, one (in pink here) uses the B-PER label at the beginning of each new entity, but the other (in blue) only uses it to separate two adjacent entities of the same type. In both cases, we can flag a new entity each time we see a new label appearing (either with the I or B prefix) then take all the following tokens labelled the same, with an I-flag."", ""In the pipeline video, we looked at the different applications the Transformers library supports out of the box, one of them being token classification, for instance predicting for each word in a sentence whether they correspond to a person, an organization or a location. We can even group together the tokens corresponding to the same entity, for instance all the tokens that formed the word Sylvain here, or Hugging and Face. The token classification pipeline works the same way as the text classification pipeline we studied in a previous video. There are three steps: the tokenization, the model, and the post processing. The first two steps are identical to the text classification pipeline, except we use an auto token classification model instead of a sequence classification one. We tokenize our text then feed it to the model. Instead of getting one number for each possible label for the whole sentence, we get one number for each of the possible 9 labels for every token in the sentence, here 19. Like all the other models of the Transformers library, our model outputs logits, which we turn into predictions by using a SoftMax. We also get the predicted label for each token by taking the maximum prediction (since the softmax function preserves the order, we could have done it on the logits if we had no need of the predictions).""]","The goal of the Named Entity Recognition (NER) task in token classification is to find the entities in a piece of text, such as persons, locations, or organizations. This task is formulated as labeling each token with one class for each entity, and another class for tokens that have no entity. Essentially, NER aims to classify each token in the input text into categories like person, organization, location, or none of them.","The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization."
"What is the resolution of images used by the CLIPSeg model?
","[""For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution."", ""Images are (500, 500) RGB and provides 5 captions for each image."", ""As you can see, the results from CLIPSeg are a little fuzzy and very low-res. If we want to obtain better results, you can fine-tune a state-of-the-art segmentation model, as explained in [our previous blogpost](https://huggingface.co/blog/fine-tune-segformer)."", ""```py\noutputs = model(**inputs)\nlogits = outputs.logits\nprint(logits.shape)\n>>> torch.Size([3, 352, 352])\n```"", ""CLIPSeg is a zero-shot segmentation model that works with both text and image prompts. The model adds a decoder to CLIP and can segment almost anything. However, the output segmentation masks are still very low-res for now, so you‚Äôll probably still want to fine-tune a different segmentation model if accuracy is important."", ""The images are (224, 224) RGB images at various resolutions, and each image has up to 5 captions associated with it.""]",The CLIPSeg model uses images of 352 x 352 pixels.,352 x 352 pixels
"What can you use Gradio for?
","[""Gradio Demo: image\\_segmentation\n### Simple image segmentation using gradio's AnnotatedImage component."", ""Gradio Blocks Party[[gradio-blocks-party]]\nAlong with the release of the Gradio chapter of the course, Hugging Face hosted a community event on building cool machine learning demos using the new Gradio Blocks feature. You can find all the demos that the community created under the [`Gradio-Blocks`](https://huggingface.co/Gradio-Blocks) organisation on the Hub. Here's a few examples from the winners:\n\\*\\*Natural language to SQL\\*\\*"", ""Let's say that your Gradio demo goes \\_viral\\_ on social media -- you have lots of users trying it out simultaneously, and you want to provide your users with the best possible experience or, in other words, minimize the amount of time that each user has to wait in the queue to see their prediction. How can you configure your Gradio demo to handle the most traffic? In this Guide, we dive into some of the parameters of Gradio's `.queue()` method as well as some other related parameters, and discuss how to set these parameters in a way that allows you to serve lots of users simultaneously with minimal latency. This is an advanced guide, so make sure you know the basics of Gradio already, such as [how to create and launch a Gradio Interface](https://gradio.app/guides/quickstart/). Most of the information in this Guide is relevant whether you are hosting your demo on [Hugging Face Spaces](https://hf.space) or on your own server. ## Overview of Gradio's Queueing System By default, every Gradio demo includes a built-in queuing system that scales to thousands of requests."", ""Gradio is a popular Python library for creating interactive machine learning apps. Traditionally, Gradio applications have relied on server-side infrastructure to run, which can be a hurdle for developers who need to host their applications. Enter Gradio-lite (`@gradio/lite`): a library that leverages [Pyodide](https://pyodide.org/en/stable/) to bring Gradio directly to your browser. In this blog post, we'll explore what `@gradio/lite` is, go over example code, and discuss the benefits it offers for running Gradio applications. ## What is `@gradio/lite`? `@gradio/lite` is a JavaScript library that enables you to run Gradio applications directly within your web browser. It achieves this by utilizing Pyodide, a Python runtime for WebAssembly, which allows Python code to be executed in the browser environment. With `@gradio/lite`, you can **write regular Python code for your Gradio applications**, and they will **run seamlessly in the browser** without the need for server-side infrastructure."", ""Gradio includes more than 30 pre-built components (as well as many user-built _custom components_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on. Each component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[\""webcam\"", \""upload\""]`). **Static and Interactive Components** Every component has a _static_ version that is designed to *display* data, and most components also have an _interactive_ version designed to let users input or modify the data. Typically, you don't need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports. **Preprocessing and Postprocessing** When a component is used as an input, Gradio automatically handles the _preprocessing_ needed to convert the data from a type sent by the user's browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array). Similarly, when a component is used as an output, Gradio automatically handles the _postprocessing_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (a gallery of images). Consider an example demo with three input components (`gr.Textbox`, `gr.Number`, and `gr.Image`) and two outputs (`gr.Number` and `gr.Gallery`) that serve as a UI for your image-to-image generation model. Below is a diagram of what our preprocessing will send to the model and what our postprocessing will require from it. In this image, the following preprocessing steps happen to send the data from the browser to your function: * The text in the textbox is converted to a Python `str` (essentially no preprocessing) * The number in the number input in converted to a Python `float` (essentially no preprocessing) * Most importantly, the image supplied by the user is converted to a `numpy.array` representation of the RGB values in the image Images are converted to NumPy arrays because they are a common format for machine learning workflows. You can control the _preprocessing_ using the component's parameters when constructing the component. For example, if you instantiate the `Image` component with the following parameters, it will preprocess the image to the `PIL` format instead: ```py img = gr.Image(type=\""pil\"") ``` Postprocessing is even simpler! Gradio automatically recognizes the format of the returned data (e.g. does the user's function return a `numpy` array or a `str` filepath for the `gr.Image` component?) and postprocesses it appropriately into a format that can be displayed by the browser. So in the image above, the following postprocessing steps happen to send the data returned from a user's function to the browser: * The `float` is displayed as a number and displayed directly to the user * The list of string filepaths (`list[str]`) is interpreted as a list of image filepaths and displayed as a gallery in the browser Take a look at the [Docs](https://gradio.app/docs) to see all the parameters for each Gradio component."", ""- How to create Gradio demos with the high-level `Interface` API, and how to configure different input and output modalities. - Different ways to share Gradio demos, through temporary links and hosting on [Hugging Face Spaces](https://huggingface.co/spaces). - How to integrate Gradio demos with models and Spaces on the Hugging Face Hub. - Advanced features like storing state in a demo or providing authentication. - How to have full control of the data flow and layout of your demo with Gradio Blocks. - Take a look at [Demos](https://github.com/gradio-app/gradio/tree/main/demo) in the repo, there are quite a lot of examples there. - See the [Guides](https://gradio.app/guides/) page, where you can find guides about cool and advanced features."", ""Gradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. You can build very custom and complex applications using `gr.Blocks()`."", ""The Gradio client works with any hosted Gradio app, whether it be an image generator, a text summarizer, a stateful chatbot, a tax calculator, or anything else!""]","Gradio can be used to create interactive machine learning applications. It allows developers to build demos that can handle various input and output modalities, such as images, text, and numbers. Some specific uses of Gradio include:

- Creating user interfaces for machine learning models, enabling users to interact with models directly.
- Building chatbots using the `gr.ChatInterface` class for conversational UIs.
- Designing complex web applications with flexible layouts and data flows using the `gr.Blocks` class.
- Sharing demos through temporary links or hosting them on Hugging Face Spaces.
- Integrating with models and Spaces on the Hugging Face Hub.
- Utilizing pre-built components for common data types in machine learning and data science.

Overall, Gradio simplifies the process of developing and sharing interactive demos for machine learning applications.","Create a demo for your machine learning model, share your machine learning model with others, and debug your model."
"What TensorFlow API function is used to load a saved tensor file?
","[""[[autodoc]] safetensors.tensorflow.load\\_file  \n[[autodoc]] safetensors.tensorflow.load"", ""```py\n>>> tf\\_model = TFDistilBertForSequenceClassification.from\\_pretrained(\""path/to/awesome-name-you-picked\"", from\\_pt=True)\n>>> tf\\_model.save\\_pretrained(\""path/to/awesome-name-you-picked\"")\n```"", ""```py\nimport tensorflow as tf\nmodel\\_inputs = tf.constant(encoded\\_sequences)\n```"", ""```py\npipeline = StableDiffusionPipeline.from_single_file(\n\""https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\""\n)\n```"", ""```py\nmodel.save_pretrained(temp_model_dir, saved_model=True)\n```\n```bash\nsaved_model_cli show --dir {temp_model_dir}/saved_model/1 --tag_set serve --signature_def serving_default\n```"", ""```py\n>>> from transformers import TFAutoModelForSequenceClassification\n>>> model = TFAutoModelForSequenceClassification.from\\_pretrained(\""distilbert-base-uncased\"")\n```\n```py\n>>> from transformers import TFAutoModelForTokenClassification\n>>> model = TFAutoModelForTokenClassification.from\\_pretrained(\""distilbert-base-uncased\"")\n```"", ""```python\nfrom safetensors import safe_open\ntensors = {}\nwith safe_open(\""model.safetensors\"", framework=\""pt\"", device=0) as f:\n    for k in f.keys():\n        tensors[k] = f.get_tensor(k)\n```"", ""```python\ntraining\\_records = tf.io.gfile.glob(os.path.join(args.train\\_dataset, \""\\*.tfrecord\""))\n```""]",The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`.,safetensors.tensorflow.load_file
"Where can you access the logs of your Endpoints in Hugging Face Endpoints?
","[""Access and view Metrics\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the ‚ÄúAnalytics‚Äù tab of your Endpoints. \n\n## Access Metrics via API\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api\\_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints."", ""Hugging Face Endpoints provides access to the logs of your Endpoints through the UI in the ‚ÄúLogs‚Äù tab of your Endpoint. You will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference. The Container Logs are only available when your Endpoint is in the ‚ÄúRunning‚Äù state. \\_Note: If your Endpoint creation is in the ‚ÄúFailed‚Äù state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc.\\_"", ""Hugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days."", ""You can download a report of all access requests for a gated model with the **download user access report** button. Click on it to download a json file with a list of users. For each entry, you have:\n- **user**: the user id. Example: *julien-c*. \n- **fullname**: name of the user on the Hub. Example: *Julien Chaumond*. \n- **status**: status of the request. Either `\""pending\""`, `\""accepted\""` or `\""rejected\""`. \n- **email**: email of the user. \n- **time**: datetime when the user initially made the request.""]","You can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the ‚ÄúLogs‚Äù tab of your Endpoint. You will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference. The Container Logs are only available when your Endpoint is in the ‚ÄúRunning‚Äù state. If your Endpoint creation is in the ‚ÄúFailed‚Äù state, you can check the Build Logs to see the reason for the failure.","In the ""Logs"" tab of your Endpoint through the UI."
"What is the latest task added to Hugging Face AutoTrain for Computer Vision?
","[""**you can now use AutoTrain for Computer Vision**!"", ""Image Classification is the latest task we‚Äôve added, with more on the way.""]",The latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification.,Image Classification
"What is the default repository type created by the `create_repo` function on Hugging Face Hub?
","[""```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"")\n```\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"", organization=\""huggingface\"")\n```"", ""```py\nÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºå[`create_repo`] ‰ºöÂàõÂª∫‰∏Ä‰∏™Ê®°Âûã‰ªìÂ∫ì„ÄÇ‰ΩÜÊòØ‰Ω†ÂèØ‰ª•‰ΩøÁî® `repo_type`ÂèÇÊï∞Êù•ÊåáÂÆöÂÖ∂‰ªñ‰ªìÂ∫ìÁ±ªÂûã„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûú‰Ω†ÊÉ≥ÂàõÂª∫‰∏Ä‰∏™Êï∞ÊçÆÈõÜ‰ªìÂ∫ì\nËØ∑ËøêË°å‰ª•‰∏ã‰ª£Á†ÅÔºö\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\""lysandre/test-dataset\"", repo_type=\""dataset\"")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n``` \n```"", ""- `type`: Type of repo (dataset or space; model by default). \n- Payload:\n```js\npayload = {\n\""type\"": \""model\"",\n\""name\"": \""name\"",\n\""organization\"": \""organization\"",\n}\n```"", ""```py\nrepo = Repository(\n... \""my-dataset\"",\n... clone\\_from=\""/\"",\n... token=True,\n... repo\\_type=\""dataset\"",\n... git\\_user=\""MyName\"",\n... git\\_email=\""me@cool.mail\""\n... )\n```"", ""```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(repo\\_id=\""super-cool-model\"")\n```"", ""```python\n>>> repo = Repository(\""my-dataset\"", clone_from=\""/\"", token=True, repo_type=\""dataset\"")\n```"", ""- `repo\\_type`, if you would like to create a `dataset` or a `space` instead of a model. Accepted values are `\""dataset\""` and `\""space\""`."", ""By default, [`create_repo`] creates a model repository. But you can use the `repo_type` parameter to specify another repository type. For example, if you want to create a dataset repository:\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\""lysandre/test-dataset\"", repo_type=\""dataset\"")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```"", ""There are three kinds of repositories on the Hub, and in this guide you'll be creating a **model repository** for demonstration purposes.""]",The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository.,model
"How many splits does the ""duorc"" dataset have?
","[""```json\n{\n\""splits\"": [\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""test\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""test\"" }\n],\n\""pending\"": [],\n\""failed\"": []\n}\n```"", ""### Prepare dataset files and split the dataset. ```python\nimport json\nimport collections\nimages\\_dir = \""coco\\_dataset/train2014\""\nannotation\\_file = \""coco\\_dataset/annotations/captions\\_train2014.json\""\nwith open(annotation\\_file, \""r\"") as f:\nannotations = json.load(f)[\""annotations\""]\nimage\\_path\\_to\\_caption = collections.defaultdict(list)\nfor element in annotations:\ncaption = f\""{element['caption'].lower().rstrip('.')}\""\nimage\\_path = images\\_dir + \""/COCO\\_train2014\\_\"" + \""%012d.jpg\"" % (element[\""image\\_id\""])\nimage\\_path\\_to\\_caption[image\\_path].append(caption)\nlines = []\nfor image\\_path, captions in image\\_path\\_to\\_caption.items():\nlines.append(json.dumps({\""image\\_path\"": image\\_path, \""captions\"": captions}))\ntrain\\_lines = lines[:-8000]\nvalid\\_line = lines[-8000:]\nwith open(\""coco\\_dataset/train\\_dataset.json\"", \""w\"") as f:\nf.write(\""\\n\"".join(train\\_lines))\nwith open(\""coco\\_dataset/valid\\_dataset.json\"", \""w\"") as f:\nf.write(\""\\n\"".join(valid\\_line))\n```"", ""```python\nimport os\nfrom datasets.distributed import split\\_dataset\\_by\\_node\nds = split\\_dataset\\_by\\_node(ds, rank=int(os.environ[\""RANK\""]), world\\_size=int(os.environ[\""WORLD\\_SIZE\""]))\n```\nThis works for both map-style datasets and iterable datasets. The dataset is split for the node at rank `rank` in a pool of nodes of size `world\\_size`. For map-style datasets:\nEach node is assigned a chunk of data, e.g. rank 0 is given the first chunk of the dataset. For iterable datasets:\nIf the dataset has a number of shards that is a factor of `world\\_size` (i.e. if `dataset.n\\_shards % world\\_size == 0`),\nthen the shards are evenly assigned across the nodes, which is the most optimized. Otherwise, each node keeps 1 example out of `world\\_size`, skipping the other examples. This can also be combined with a `torch.utils.data.DataLoader` if you want each node to use multiple workers to load the data."", ""- The slice of `rows` of a dataset and the content contained in each column of a specific row. For example, here are the `features` and the slice of `rows` of the `duorc`/`SelfRC` train split from 150 to 151:\n```json\n// https://datasets-server.huggingface.co/rows?dataset=duorc&config=SelfRC&split=train&offset=150&length=2\n{\n\""features\"": [\n{\n\""feature\\_idx\"": 0,\n\""name\"": \""plot\\_id\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 1,\n\""name\"": \""plot\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 2,\n\""name\"": \""title\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 3,\n\""name\"": \""question\\_id\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 4,\n\""name\"": \""question\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 5,\n\""name\"": \""answers\"",\n\""type\"": {\n\""feature\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" },\n\""\\_type\"": \""Sequence\""\n}\n},\n{\n\""feature\\_idx\"": 6,\n\""name\"": \""no\\_answer\"",\n\""type\"": { \""dtype\"": \""bool\"", \""\\_type\"": \""Value\"" }\n}\n],\n\""rows\"": [\n{\n\""row\\_idx\"": 150,\n\""row\"": {\n\""plot\\_id\"": \""/m/03wj\\_q\"",\n\""plot\"": \""The film is centered on Mortal Kombat, a fighting tournament between the representatives of the realms of Earth and Outworld conceived by the Elder Gods amid looming invasion of the Earth by Outworld."", ""```\n## Split dataset\nYou can split your dataset one of two ways:\n- [`IterableDataset.take`] returns the first `n` examples in a dataset:\n```py\n>>> dataset = load\\_dataset('oscar', \""unshuffled\\_deduplicated\\_en\"", split='train', streaming=True)\n>>> dataset\\_head = dataset.take(2)\n>>> list(dataset\\_head)\n[{'id': 0, 'text': 'Mtendere Village was...'}, {'id': 1, 'text': 'Lily James cannot fight the music...'}]\n```\n- [`IterableDataset.skip`] omits the first `n` examples in a dataset and returns the remaining examples:\n```py\n>>> train\\_dataset = shuffled\\_dataset.skip(1000)\n```\n`take` and `skip` prevent future calls to `shuffle` because they lock in the order of the shards. You should `shuffle` your dataset before splitting it.\n```"", ""There are several ways to name splits. Validation splits are sometimes called \""dev\"", and test splits may be referred to as \""eval\"". These other split names are also supported, and the following keywords are equivalent:\n- train, training\n- validation, valid, val, dev\n- test, testing, eval, evaluation\n\nIf one of your splits comprises several files, ü§ó Datasets can still infer whether it is the train, validation, and test split from the file name. Make sure all the files of your `train` set have *train* in their names (same for test and validation). Even if you add a prefix or suffix to `train` in the file name (like `my_train_file_00001.csv` for example), ü§ó Datasets can still infer the appropriate split."", ""```json\n\""splits\"": {\n\""train\"": {\n\""name\"": \""train\"",\n\""num_bytes\"": 239852729,\n\""num_examples\"": 60721,\n\""dataset_name\"": \""duorc\""\n},\n\""validation\"": {\n\""name\"": \""validation\"",\n\""num_bytes\"": 51662519,\n\""num_examples\"": 12961,\n\""dataset_name\"": \""duorc\""\n},\n\""test\"": {\n\""name\"": \""test\"",\n\""num_bytes\"": 49142710,\n\""num_examples\"": 12559,\n\""dataset_name\"": \""duorc\""\n}\n}\n```"", ""```py\nYou can also choose only to load specific slices of a split. There are two options for slicing a split: using strings or the [`ReadInstruction`] API. Strings are more compact and readable for simple cases, while [`ReadInstruction`] is easier to use with variable slicing parameters. Concatenate a `train` and `test` split by:\n>>> train_test_ds = datasets.load_dataset(\""bookcorpus\"", split=\""train+test\"")\n===STRINGAPI-READINSTRUCTION-SPLIT===\n>>> ri = datasets.ReadInstruction(\""train\"") + datasets.ReadInstruction(\""test\"")\n>>> train_test_ds = datasets.load_dataset(\""bookcorpus\"", split=ri)\n\nSelect specific rows of the `train` split:\n>>> train_10_20_ds = datasets.load_dataset(\""bookcorpus\"", split=\""train[10:20]\"")\n===STRINGAPI-READINSTRUCTION-SPLIT===\n>>> train_10_20_ds = datasets.load_dataset(\""bookcorpu\"", split=datasets.ReadInstruction(\""train\"", from_=10, to=20, unit=\""abs\""))\n\nOr select a percentage of a split with:\n>>> train_10pct_ds = datasets.load_dataset(\""bookcorpus\"", split=\""train[:10%]\"")\n===STRINGAPI-READINSTRUCTION-SPLIT===\n>>> train_10_20_ds = datasets.load_dataset(\""bookcorpus\"", split=datasets.ReadInstruction(\""train\"", to=10, unit=\""%\""))\n\nSelect a combination of percentages from each split:\n>>> train_10_80pct_ds = datasets.load_dataset(\""bookcorpus\"", split=\""train[:10%]+train[-80%:]\"")\n===STRINGAPI-READINSTRUCTION-SPLIT===\n>>> ri = (datasets.ReadInstruction(\""train\"", to=10, unit=\""%\"") + datasets.ReadInstruction(\""train\"", from_=-80, unit=\""%\""))\n>>> train_10_80pct_ds = datasets.load_dataset(\""bookcorpus\"", split=ri)\n\nFinally, you can even create cross-validated splits. The example below creates 10-fold cross-validated splits. Each validation dataset is a 10% chunk, and the training dataset makes up the remaining complementary 90% chunk:\n>>> val_ds = datasets.load_dataset(\""bookcorpus\"", split=[f\""train[{k}%:{k+10}%]\"" for k in range(0, 100, 10)])\n>>> train_ds = datasets.load_dataset(\""bookcorpus\"", split=[f\""train[:{k}%]+train[{k+10}%:]\"" for k in range(0, 100, 10)])\n===STRINGAPI-READINSTRUCTION-SPLIT===\n>>> val_ds = datasets.load_dataset(\""bookcorpus\"", [datasets.ReadInstruction(\""train\"", from_=k, to=k+10, unit=\""%\"") for k in range(0, 100, 10)])\n>>> train_ds = datasets.load_dataset(\""bookcorpus\"", [(datasets.ReadInstruction(\""train\"", to=k, unit=\""%\"") + datasets.ReadInstruction(\""train\"", from_=k+10, unit=\""%\"")) for k in range(0, 100, 10)])\n```"", ""```json\n{\n\""splits\"": [\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""train\"",\n\""num\\_bytes\\_parquet\\_files\"": 26005668,\n\""num\\_bytes\\_memory\"": 496682909,\n\""num\\_rows\"": 69524,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""validation\"",\n\""num\\_bytes\\_parquet\\_files\"": 5566868,\n\""num\\_bytes\\_memory\"": 106510489,\n\""num\\_rows\"": 15591,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""test\"",\n\""num\\_bytes\\_parquet\\_files\"": 6136591,\n\""num\\_bytes\\_memory\"": 115215760,\n\""num\\_rows\"": 15857,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""train\"",\n\""num\\_bytes\\_parquet\\_files\"": 14851720,\n\""num\\_bytes\\_memory\"": 239852729,\n\""num\\_rows\"": 60721,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""validation\"",\n\""num\\_bytes\\_parquet\\_files\"": 3114390,\n\""num\\_bytes\\_memory\"": 51662519,\n\""num\\_rows\"": 12961,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""test\"",\n\""num\\_bytes\\_parquet\\_files\"": 3035736,\n\""num\\_bytes\\_memory\"": 49142710,\n\""num\\_rows\"": 12559,\n\""num\\_columns\"": 7\n}\n]\n}\n```"", ""### Process multiple splits\nMany datasets have splits that can be processed simultaneously with [`DatasetDict.map`]. For example, tokenize the `sentence1` field in the train and test split by:\n```py\n>>> from datasets import load\\_dataset\n# load all the splits\n>>> dataset = load\\_dataset('glue', 'mrpc')\n>>> encoded\\_dataset = dataset.map(lambda examples: tokenizer(examples[\""sentence1\""]), batched=True)\n>>> encoded\\_dataset[\""train\""][0]\n{'sentence1': 'Amrozi accused his brother , whom he called \"" the witness \"" , of deliberately distorting his evidence .',\n'sentence2': 'Referring to him as only \"" the witness \"" , Amrozi accused his brother of deliberately distorting his evidence .',\n'label': 1,\n'idx': 0,\n'input\\_ids': [ 101, 7277, 2180, 5303, 4806, 1117, 1711, 117, 2292, 1119, 1270, 107, 1103, 7737, 107, 117, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102],\n'token\\_type\\_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n'attention\\_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n}\n```"", ""--train\\_shards\\_path\\_or\\_url=\""pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true\"""", ""- `split`: the split name, for example `train`\n- `API_URL = \""https://datasets-server.huggingface.co/rows?dataset=duorc&config=SelfRC&split=train&offset=150&length=10\""`"", ""## Splits\nEvery processed and cleaned dataset contains \\*splits\\*, specific subsets of data reserved for specific needs. The most common splits are:\n\\* `train`: data used to train a model; this data is exposed to the model\n\\* `validation`: data reserved for evaluation and improving model hyperparameters; this data is hidden from the model\n\\* `test`: data reserved for evaluation only; this data is completely hidden from the model and ourselves\nThe `validation` and `test` sets are especially important to ensure a model is actually learning instead of \\*overfitting\\*, or just memorizing the data."", ""```json\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""train\"",\n\""features\"": [\n{\n\""feature_idx\"": 0,\n\""name\"": \""plot_id\"",\n\""type\"": { \""dtype\"": \""string\"", \""_type\"": \""Value\"" }\n},\n{\n\""feature_idx\"": 1,\n\""name\"": \""plot\"",\n\""type\"": { \""dtype\"": \""string\"", \""_type\"": \""Value\"" }\n},\n{\n\""feature_idx\"": 2,\n\""name\"": \""title\"",\n\""type\"": { \""dtype\"": \""string\"", \""_type\"": \""Value\"" }\n},\n{\n\""feature_idx\"": 3,\n\""name\"": \""question_id\"",\n\""type\"": { \""dtype\"": \""string\"", \""_type\"": \""Value\"" }\n},\n{\n\""feature_idx\"": 4,\n\""name\"": \""question\"",\n\""type\"": { \""dtype\"": \""string\"", \""_type\"": \""Value\"" }\n},\n{\n\""feature_idx\"": 5,\n\""name\"": \""answers\"",\n\""type\"": {\n\""feature\"": { \""dtype\"": \""string\"", \""_type\"": \""Value\"" },\n\""_type\"": \""Sequence\""\n}\n},\n{\n\""feature_idx\"": 6,\n\""name\"": \""no_answer\"",\n\""type\"": { \""dtype\"": \""bool\"", \""_type\"": \""Value\"" }\n}\n],\n\""rows\"": [\n{\n\""row_idx\"": 0,\n\""row\"": {\n\""plot_id\"": \""/m/03vyhn\"",\n\""plot\"": \""200 years in the future, Mars has been colonized by a high-tech company.\\nMelanie Ballard (Natasha Henstridge) arrives by train to a Mars mining camp which has cut all communication links with the company headquarters.\n```""]","The ""duorc"" dataset has a total of 6 splits: 

1. ParaphraseRC - train
2. ParaphraseRC - validation
3. ParaphraseRC - test
4. SelfRC - train
5. SelfRC - validation
6. SelfRC - test",Six
"What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?
","[""# Fully Sharded Data Parallel\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or *rank*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class."", ""However, at the end of training, we want the whole model state dict instead of the sharded state dict which is only compatible with FSDP."", ""In terms of train time, DDP with mixed precision is the fastest followed by FSDP using ZERO Stage 2 and Stage 3, respectively. As the task of causal language modelling always has fixed context sequence length (--block_size), the train time speedup with FSDP wasn‚Äôt that great. For applications with dynamic batching, FSDP which enables larger batch sizes will likely have considerable speed up in terms of train time. FSDP mixed precision support currently has few [issues](https://github.com/pytorch/pytorch/issues/75676) with transformer. Once this is supported, the training time speed up will further improve considerably."", ""1. \""For full fine-tuning using FSDP along with Flash Attention V2 and Gradient Checkpointing, the memory occupied per GPU ranges between **70 GB to 77.6 GB** with a **per_gpu_batch_size of 1**.\""\n2. \""We will look at how to do full fine-tuning of `bigcode/starcoder` (15B params) on 8 A100 80GB GPUs using PyTorch Fully Sharded Data Parallel (FSDP) technique.\"""", ""FSDP offers a number of sharding strategies to select from:\n\\* `FULL\\_SHARD` - shards model parameters, gradients and optimizer states across workers; select `1` for this option\n\\* `SHARD\\_GRAD\\_OP`- shard gradients and optimizer states across workers; select `2` for this option\n\\* `NO\\_SHARD` - don't shard anything (this is equivalent to DDP); select `3` for this option\n\\* `HYBRID\\_SHARD` - shard model parameters, gradients and optimizer states within each worker where each worker also has a full copy; select `4` for this option\n\\* `HYBRID\\_SHARD\\_ZERO2` - shard gradients and optimizer states within each worker where each worker also has a full copy; select `5` for this option\nThis is enabled by the `fsdp\\_sharding\\_strategy` flag."", ""In this post we will look at Data Parallelism using ZeRO and more specifically the latest PyTorch feature **[FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**. PyTorch recently upstreamed the Fairscale FSDP into PyTorch Distributed with additional optimizations."", ""FSDP precisely addresses this by sharding the optimizer states, gradients and model parameters across the data parallel workers. It further facilitates CPU offloading of all those tensors, thereby enabling loading large models which won't fit the available GPU memory. Similar to DDP, each worker gets a different batch of data. During the forward pass, if the CPU offload is enabled, the parameters of the local shard are first copied to the GPU/accelerator. Then, each worker performs all-gather operation for a given FSDP wrapped module/layer(s) to all get the needed parameters, computation is performed followed by releasing/emptying the parameter shards of other workers. This continues for all the FSDP modules. The loss gets computed after the forward pass and during the backward pass, again an all-gather operation is performed to get all the needed parameters for a given FSDP module, computation is performed to get local gradients followed by releasing the shards of other workers."", ""Fully Sharded Data Parallelism (FSDP) is a paradigm in which the optimizer states, gradients and parameters are sharded across devices. During the forward pass, each FSDP unit performs an _all-gather operation_ to get the complete weights, computation is performed followed by discarding the shards from other devices. After the forward pass, the loss is computed followed by the backward pass. In the backward pass, each FSDP unit performs an all-gather operation to get the complete weights, with computation performed to get the local gradients. These local gradients are averaged and sharded across the devices via a _reduce-scatter operation_ so that each device can update the parameters of its shard."", ""By sharding the model parameters, optimizer and gradient states, and even offloading them to the CPU when they're inactive, FSDP can reduce the high cost of large-scale training."", ""**FSDP with CPU offload enables training GPT-2 1.5B model on a single GPU with a batch size of 10**. This enables ML practitioners with minimal compute resources to train such large models, thereby democratizing large model training."", ""FSDP can be a powerful tool for training really large models and you have access to more than one GPU or TPU."", ""`SHARDED_STATE_DICT` saves shard per GPU separately which makes it quick to save or resume training from intermediate checkpoint. When `FULL_STATE_DICT` is used, first process (rank 0) gathers the whole model on CPU and then saving it in a standard format. Here, the sharding strategy is `FULL_SHARD`. We are using `TRANSFORMER_BASED_WRAP` for auto wrap policy and it uses `_no_split_module` to find the Transformer block name for nested FSDP auto wrap. We use `SHARDED_STATE_DICT` to save the intermediate checkpoints and optimizer states in this format recommended by the PyTorch team. Make sure to enable broadcasting module parameters from rank 0 at the start as mentioned in the above paragraph on addressing Challenge 1. We are enabling `bf16` mixed precision training. For final checkpoint being the whole model state dict, below code snippet is used:\n```python\nif trainer.is_fsdp_enabled:\ntrainer.accelerator.state.fsdp_plugin.set_state_dict_type(\""FULL_STATE_DICT\"")\ntrainer.save_model(script_args.output_dir) # alternatively, trainer.push_to_hub() if the whole ckpt is below 50GB as the LFS limit per file is 50GB\n```"", ""Now, the local gradients are averaged and sharded to each relevant workers using reduce-scatter operation. This allows each worker to update the parameters of its local shard. If CPU offload is activated, the gradients are passed to CPU for updating parameters directly on CPU. Please refer [7, 8, 9] for all the in-depth details on the workings of the PyTorch FSDP and the extensive experimentation carried out using this feature."", ""[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes. Currently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training."", ""FSDP enables larger batch sizes for GPT-2 Large model and it enables training the GPT-2 XL model with decent batch size unlike DDP. With respect to DDP, from Table 1 we can observe that FSDP **enables larger batch sizes**, up to **2X-3X** without and with CPU offload setting, respectively.""]","The purpose of Fully Sharded Data Parallel (FSDP) in distributed training is to shard a model's parameters, gradients, and optimizer states across multiple GPUs (or workers). This approach reduces memory usage by not replicating the entire model on each GPU, allowing for improved GPU memory efficiency. FSDP enables the training of much larger models on fewer GPUs and facilitates CPU offloading of tensors, which helps in loading large models that may not fit into the available GPU memory. By sharding these components, FSDP can significantly reduce the high cost of large-scale training and enable larger batch sizes, making it a powerful tool for training large models in distributed environments.","FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes."
"What file format is used to save and store PyTorch model weights more securely than `.bin` files?
","[""Convert weights to safetensors\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`."", ""- Prevent DOS attacks: We can craft the format in such a way that it's almost impossible to use malicious files to DOS attack a user. Currently, there's a limit on the size of the header of 100MB to prevent parsing extremely large JSON. Also when reading the file, there's a guarantee that addresses in the file do not overlap in any way, meaning when you're loading a file you should never exceed the size of the file in memory\n- Faster load: PyTorch seems to be the fastest file to load out in the major ML formats. However, it does seem to have an extra copy on CPU, which we can bypass in this lib by using `torch.UntypedStorage.from_file`. Currently, CPU loading times are extremely fast with this lib compared to pickle. GPU loading times are as fast or faster than PyTorch equivalent. Loading first on CPU with memmapping with torch, and then moving all tensors to GPU seems to be faster too somehow (similar behavior in torch pickle)\n- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to load only part of the tensors on the various models. For [BLOOM](https://huggingface.co/bigscience/bloom) using this format enabled to load the model on 8 GPUs from 10mn with regular PyTorch weights down to 45s."", ""Pickle is a widely used serialization format in ML. Most notably, it is the default format for PyTorch model weights. There are dangerous arbitrary code execution attacks that can be perpetrated when you load a pickle file. We suggest loading models from users and organizations you trust, relying on signed commits, and/or loading models from TF or Jax formats with the `from_tf=True` auto-conversion mechanism. We also alleviate this issue by displaying/\""vetting\"" the list of imports in any pickled file, directly on the Hub. Finally, we are experimenting with a new, simple serialization format for weights called [`safetensors`](https://github.com/huggingface/safetensors)."", ""```python\nfrom safetensors.torch import save\\_file as safe\\_save\\_file\nfrom transformers.models.wav2vec2.modeling\\_wav2vec2 import WAV2VEC2\\_ADAPTER\\_SAFE\\_FILE\nimport os\nadapter\\_file = WAV2VEC2\\_ADAPTER\\_SAFE\\_FILE.format(target\\_lang)\nadapter\\_file = os.path.join(training\\_args.output\\_dir, adapter\\_file)\nsafe\\_save\\_file(model.\\_get\\_adapters(), adapter\\_file, metadata={\""format\"": \""pt\""})\n```"", ""In addition to being safe, `safetensors` allows lazy loading and generally faster loads (around 100x faster on CPU). Since `safetensors` main asset is providing safety guarantees, we wanted to make sure it actually delivered. That's why Hugging Face, EleutherAI, and Stability AI teamed up to get an external security audit to confirm it. Important findings: - No critical security flaw leading to arbitrary code execution was found. - Some imprecisions in the spec format were detected and fixed. - Some missing validation allowed [polyglot files](https://en.wikipedia.org/wiki/Polyglot\\_(computing)), which was fixed. - Lots of improvements to the test suite were proposed and implemented. One import thing to note is that the library is written in Rust."", ""- Safety is the number one reason for using safetensors. As open-source and model distribution grows, it is important to be able to trust the model weights you downloaded don't contain any malicious code. \n- Loading speed between switching models is another reason to use safetensors, which performs zero-copy of the tensors. It is especially fast compared to `pickle` if you're loading the weights to CPU (the default case), and just as fast if not faster when directly loading the weights to GPU. \n- Lazy loading is also supported in safetensors, which is useful in distributed settings to only load some of the tensors."", ""### Use your own serialization format\n- [safetensors](https://github.com/huggingface/safetensors)\nThis last format, `safetensors`, is a simple serialization format that we are working on and experimenting with currently!"", ""safetensors is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or *pickled* into a `.bin` file with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights."", ""When you're using `transformers`, if `safetensors` is installed, then those files will already be used preferentially in order to prevent issues, which means that\n```\npip install safetensors\n```\nis likely to be the only thing needed to run `safetensors` files safely. Going forward and thanks to the validation of the library, `safetensors` will now be installed in `transformers` by default. The next step is saving models in `safetensors` by default. We are thrilled to see that the `safetensors` library is already seeing use in the ML ecosystem, including:\n- [Civitai](https://civitai.com/)\n- [Stable Diffusion Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)\n- [dfdx](https://github.com/coreylowman/dfdx)\n- [LLaMA.cpp](https://github.com/ggerganov/llama.cpp/blob/e6a46b0ed1884c77267dc70693183e3b7164e0e0/convert.py#L537)\n## Why create something new? The creation of this library was driven by the fact that PyTorch uses `pickle` under\nthe hood, which is inherently unsafe. (Sources: [1](https://huggingface.co/docs/hub/security-pickle), [2, video](https://www.youtube.com/watch?v=2ethDz9KnLk), [3](https://github.com/pytorch/pytorch/issues/52596))\nWith pickle, it is possible to write a malicious file posing as a model\nthat gives full control of a user's computer to an attacker without the user's knowledge,\nallowing the attacker to steal all their bitcoins üòì. While this vulnerability in pickle is widely known in the computer security world (and is acknowledged in the PyTorch [docs](https://pytorch.org/docs/stable/generated/torch.load.html)), it‚Äôs not common knowledge in the broader ML community. Since the Hugging Face Hub is a platform where anyone can upload and share models, it is important to make efforts\nto prevent users from getting infected by malware."", ""# Audit shows that safetensors is safe and ready to become the default\n[Hugging Face](https://huggingface.co/), in close collaboration with [EleutherAI](https://www.eleuther.ai/) and [Stability AI](https://stability.ai/), has ordered\nan external security audit of the `safetensors` library, the results of which allow\nall three organizations to move toward making the library the default format\nfor saved models. The full results of the security audit, performed by [Trail of Bits](https://www.trailofbits.com/),\ncan be found here: [Report](https://huggingface.co/datasets/safetensors/trail\\_of\\_bits\\_audit\\_repot/resolve/main/SOW-TrailofBits-EleutherAI\\_HuggingFace-v1.2.pdf). The following blog post explains the origins of the library, why these audit results are important,\nand the next steps. ## What is safetensors? üê∂[Safetensors](https://github.com/huggingface/safetensors) is a library\nfor saving and loading tensors in the most common frameworks (including PyTorch, TensorFlow, JAX, PaddlePaddle, and NumPy). For a more concrete explanation, we'll use PyTorch. ```python\nimport torch\nfrom safetensors.torch import load\\_file, save\\_file\nweights = {\""embeddings\"": torch.zeros((10, 100))}\nsave\\_file(weights, \""model.safetensors\"")\nweights2 = load\\_file(\""model.safetensors\"")\n```\nIt also has a number of [cool features](https://github.com/huggingface/safetensors#yet-another-format-) compared to other formats, most notably that loading files is \\_safe\\_, as we'll see later."", ""- Prevent DOS attacks: We can craft the format in such a way that it's almost impossible to use malicious files to DOS attack a user. Currently, there's a limit on the size of the header of 100MB to prevent parsing extremely large JSON. Also when reading the file, there's a guarantee that addresses in the file do not overlap in any way, meaning when you're loading a file you should never exceed the size of the file in memory\n- Faster load: PyTorch seems to be the fastest file to load out in the major ML formats. However, it does seem to have an extra copy on CPU, which we can bypass in this lib by using `torch.UntypedStorage.from_file`. Currently, CPU loading times are extremely fast with this lib compared to pickle. GPU loading times are as fast or faster than PyTorch equivalent. Loading first on CPU with memmapping with torch, and then moving all tensors to GPU seems to be faster too somehow (similar behavior in torch pickle)\n- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to load only part of the tensors on the various models. For [BLOOM](https://huggingface.co/bigscience/bloom) using this format enabled to load the model on 8 GPUs from 10mn with regular PyTorch weights down to 45s.""]",The file format used to save and store PyTorch model weights more securely than `.bin` files is `safetensors`.,`.safetensors`
"What type of security certification does Hugging Face have?
","[""Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses."", ""The certification process is **completely free**:\n- To get a *certificate of completion*: you need **to pass 80% of the assignments**. - To get a *certificate of excellence*: you need **to pass 100% of the assignments**. There's **no deadlines, the course is self-paced**. \nWhen we say pass, **we mean that your model must be pushed to the Hub and get a result equal or above the minimal requirement**. \nNow that you're ready for the certification process, you need to:\n1. Go here: https://huggingface.co/spaces/huggingface-projects/Deep-RL-Course-Certification/\n2. Type your *hugging face username*, your *first name*, *last name*\n3. Click on \""Generate my certificate\"". - If you passed 80% of the assignments, **congratulations** you've just got the certificate of completion. - If you passed 100% of the assignments, **congratulations** you've just got the excellence certificate. - If you are below 80%, don't be discouraged!"", ""To report a security vulnerability, please contact: security@huggingface.co"", ""We are happy to announce that Hugging Face has been selected by the [CNIL](https://www.cnil.fr/en/home) (French Data Protection Authority) to benefit from its [Enhanced Support program](https://www.cnil.fr/en/enhanced-support-cnil-selects-3-digital-companies-strong-potential)! This new program picked three companies with ‚Äústrong potential for economic development‚Äù out of over 40 candidates, who will receive support in understanding and implementing their duties with respect to data protection - a daunting and necessary endeavor in the context of the rapidly evolving field of Artificial Intelligence."", ""Single Sign-On (SSO)\nThe Hugging Face Hub gives you the ability to implement mandatory Single Sign-On (SSO) for your organization. We support both SAML 2.0 and OpenID Connect (OIDC) protocols. This feature is part of the [Enterprise Hub](https://huggingface.co/enterprise). ## How does it work? When Single Sign-On is enabled, the members of your organization must authenticate through your Identity Provider (IdP) to access any content under the organization's namespace. Public content will still be available to users who are not members of the organization. **We use email addresses to identify SSO users. Make sure that your organizational email address (e.g. your company email) has been added to [your user account](https://huggingface.co/settings/account).**\nWhen users log in, they will be prompted to complete the Single Sign-On authentication flow with a banner similar to the following:\n\nSingle Sign-On only applies to your organization. Members may belong to other organizations on Hugging Face. We support [role mapping](#role-mapping): you can automatically assign [roles](./organizations-security#access-control-in-organizations) to organization members based on attributes provided by your Identity Provider. ### Supported Identity Providers\nYou can easily integrate Hugging Face Hub with a variety of Identity Providers, such as Okta, OneLogin or Azure Active Directory (Azure AD). Hugging Face Hub can work with any OIDC-compliant or SAML Identity Provider. ## How to configure OIDC/SAML provider in the Hub\nWe have some guides available to help with configuring based on your chosen SSO provider, or to take inspiration from:\n- [How to configure OIDC with Okta in the Hub](./security-sso-okta-oidc)\n- [How to configure SAML with Okta in the Hub](./security-sso-okta-saml)\n- [How to configure SAML with Azure in the Hub](./security-sso-azure-saml)"", ""Security Policy\n## Reporting a Vulnerability\nü§ó We have our bug bounty program set up with HackerOne. Please feel free to submit vulnerability reports to our private program at https://hackerone.com/hugging\\_face. Note that you'll need to be invited to our program, so send us a quick email at security@huggingface.co if you've found a vulnerability."", ""## Inference Endpoints and Hub Security\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified.""]","Hugging Face is SOC2 Type 2 certified, meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.",SOC2 Type 2 certified
"What do RAG models combine to generate outputs?
","[""Retrieval-augmented generation (\""RAG\"") models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models."", ""- A mixture of experts:\n- [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1), the model is made of 8 sub-models (transformer decoders), and for each input, a router picks the 2 best sub-models and sums their outputs."", ""This information retrieval step allows [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) to make use of multiple sources of knowledge -- those that are baked into the model parameters and the information that is contained in the contextual passages, allowing it to outperform other state-of-the-art models in tasks like question answering. This retrieval of contextual documents is crucial for RAG's state-of-the-art results but introduces an extra layer of complexity."", ""The model retrieves contextual documents from an external dataset as part of its execution. These contextual documents are used in conjunction with the original input to produce an output. However, [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) has an intermediate component that retrieves contextual documents from an external knowledge base (like a Wikipedia text corpus). These documents are then used in conjunction with the input sequence and passed into the underlying seq2seq [generator](https://huggingface.co/blog/how-to-generate)."", ""The original RAG implementation is able to train the question encoder and generator end-to-end. This extension enables complete end-to-end training of RAG including the context encoder in the retriever component."", ""RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks. Retrieval-augmented generation (\""RAG\"") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs."", ""A RAG model encapsulates two core components: a question encoder and a generator. During a forward pass, we encode the input with the question encoder and pass it to the retriever to extract relevant context documents. The documents are then prepended to the input. Such contextualized inputs are passed to the generator."", ""\""Huggingface Transformers recently added the Retrieval Augmented Generation (RAG) model, a new NLP architecture that leverages external documents (like Wikipedia) to augment its knowledge and achieve state of the art results on knowledge-intensive tasks.\""""]","RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence (Seq2Seq) models to generate outputs. They retrieve documents, pass them to a Seq2Seq model, and then marginalize to produce the final output.",Pretrained dense retrieval (DPR) and sequence-to-sequence models.
"What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?
","[""The feature extractor is used to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the token-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately, if you only want to handle one of the two tasks. \n\n`MarkupLMFeatureExtractor` uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for pulling data out of HTML and XML files, under the hood."", ""In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these use cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs). **Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True** This is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML. ```python\n>>> from transformers import MarkupLMProcessor\n>>> processor = MarkupLMProcessor.from_pretrained(\""microsoft/markuplm-base\"")\n>>> html_string = \""\""\""\n... ... ... ... Hello world\n... ... ... # Welcome\n\n... Here is my website. ... ... \""\""\""\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\n>>> encoding = processor(html_string, return_tensors=\""pt\"")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```\n**Use case 2: web page classification (training, inference) + token classification (inference), parse_html=False** In case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should provide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse_html` to `False`. ```python\n>>> from transformers import MarkupLMProcessor\n>>> processor = MarkupLMProcessor.from_pretrained(\""microsoft/markuplm-base\"")\n>>> processor.parse_html = False\n>>> nodes = [\""hello\"", \""world\"", \""how\"", \""are\""]\n>>> xpaths = [\""/html/body/div/li[1]/div/span\"", \""/html/body/div/li[1]/div/span\"", \""html/body\"", \""html/body/div\""]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return_tensors=\""pt\"")\n>>> print(encoding.keys())\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n```""]","`MarkupLMFeatureExtractor` uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for pulling data out of HTML and XML files, under the hood.",Beautiful Soup
"What is the file size limit for syncing to HF Spaces without using Git-LFS?
","[""- In all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size."", ""*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history.*"", ""| File size | <5GB | split data into chunked files |"", ""Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. ... filesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces"", ""- \\*\\*File size\\*\\*: In the case of uploading large files (e.g. model weights), we strongly recommend splitting them \\*\\*into chunks of around 5GB each\\*\\*.""]","The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. For files larger than 10MB, Git-LFS is required.",10MB
"What is the title of the paper introducing the ByT5 model?
","[""The ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel."", ""The LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916) by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang."", ""The mT5 model was presented in [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel."", ""This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be found [here](https://github.com/google-research/byt5). ByT5's architecture is based on the T5v1.1 model, refer to [T5v1.1's documentation page](t5v1.1) for the API reference."", ""The T5 model was presented in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) by [Colin Raffel](https://huggingface.co/craffel), Noam Shazeer, [Adam Roberts](https://huggingface.co/adarob), Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, [Peter J.""]","The title of the paper introducing the ByT5 model is ""ByT5: Towards a token-free future with pre-trained byte-to-byte models.""",ByT5: Towards a token-free future with pre-trained byte-to-byte models
"What is the dimension of the feature vector for the base BERT model?
","[""The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768."", ""`bert-base-cased` is limited to 512 tokens, which means we would have to cut possibly important information from the article. Because most of the important information is often found at the beginning of articles and because we want to be computationally efficient, we decide to stick to `bert-base-cased` with a `max_length` of 512 in this notebook."", ""| BERTbase | 12 | 768 | 12 | 110M | 4 TPUs | 4 days |"", ""- Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters.""]",The dimension of the feature vector for the base BERT model is 768.,768
"What special identifier does the WordPiece Model use for continuing subwords?
","[""Subword-based tokenizers generally have a way to identify which tokens are start of words, and which tokens complete start of words: token as the start of a word. ##ization as completing a word. Here the ## prefix indicates that ization is part of a word rather than the beginning of it. The ## comes from the BERT tokenizer, based on the WordPiece algorithm."", ""| WordPiece | Reverts the WordPiece Model. This model uses a special identifier `##` for continuing subwords, and so this Decoder helps with decoding these. |"", ""\""each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\""word\""` gets split like this:\n```\nw ##o ##r ##d\n```\"""", ""```python\nword = f\""##{word}\""\n```\n```python\n['Hugg', '##i', '##n', '##g']\n```\n```python\ntokenize(\""This is the Hugging Face course!\"")\n```\n```python\n['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s', '##e', '[UNK]']\n```"", ""| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don‚Äôt exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word). |"", ""WordPiece is the subword tokenization algorithm used for [BERT](model_doc/bert), [DistilBERT](model_doc/distilbert), and [Electra](model_doc/electra). The algorithm was outlined in [Japanese and Korean Voice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to BPE. WordPiece first initializes the vocabulary to include every character present in the training data and progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary. So what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is equivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by its second symbol is the greatest among all symbol pairs. *E.g.* `\""u\""`, followed by `\""g\""` would have only been merged if the probability of `\""ug\""` divided by `\""u\""`, `\""g\""` would have been greater than for any other symbol pair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it _loses_ by merging two symbols to ensure it's _worth it_."", ""`\""##\""` means that the rest of the token should be attached to the previous one, without space (for decoding or reversal of the tokenization)."", ""Tokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not the merge rules learned. Starting from the word to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it. For instance, if we use the vocabulary learned in the example above, for the word `\""hugs\""` the longest subword starting from the beginning that is inside the vocabulary is `\""hug\""`, so we split there and get `[\""hug\"", \""##s\""]`. We then continue with `\""##s\""`, which is in the vocabulary, so the tokenization of `\""hugs\""` is `[\""hug\"", \""##s\""]`. With BPE, we would have applied the merges learned in order and tokenized this as `[\""hu\"", \""##gs\""]`, so the encoding is different. As another example, let's see how the word `\""bugs\""` would be tokenized. `\""b\""` is the longest subword starting at the beginning of the word that is in the vocabulary, so we split there and get `[\""b\"", \""##ugs\""]`. Then `\""##u\""` is the longest subword starting at the beginning of `\""##ugs\""` that is in the vocabulary, so we split there and get `[\""b\"", \""##u, \""##gs\""]`. Finally, `\""##gs\""` is in the vocabulary, so this last list is the tokenization of `\""bugs\""`. When the tokenization gets to a stage where it's not possible to find a subword in the vocabulary, the whole word is tokenized as unknown -- so, for instance, `\""mug\""` would be tokenized as `[\""[UNK]\""]`, as would `\""bum\""` (even if we can begin with `\""b\""` and `\""##u\""`, `\""##m\""` is not the vocabulary, and the resulting tokenization will just be `[\""[UNK]\""]`, not `[\""b\"", \""##u\"", \""[UNK]\""]`). This is another difference from BPE, which would only classify the individual characters not in the vocabulary as unknown."", ""WordPiece (used for example by BERT)"", ""By keeping only one occurrence per elementary unit we now have our initial vocabulary. We will list all the existing pairs in our corpus. Once we have this list, we will calculate a score for each of these pairs. As for the BPE algorithm, we will select the pair with the highest score. Taking for example the first pair composed of H and U. The score of a pair is simply equal to the frequency of appearance of the pair divided by the product of the frequency of appearance of the first token by the frequency of appearance of the second token. Thus at a fixed frequency of appearance of the pair, if the subparts of the pair are very frequent in the corpus then this score will be decreased. In our example, the pair \""hu\"" appears 4 times, the letter \""h\"" 4 times and the letter u 4 times.""]",The WordPiece Model uses the special identifier `##` for continuing subwords.,##
"What is the purpose of the üß® Diffusers tutorials?
","[""The aim of this project is to provide retrieval augmented diffusion models to diffusers!"", ""# üß® Diffusers‚Äô Ethical Guidelines\n## Preamble\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training. Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users‚Äô contributions, and usage of the Diffusers library. The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups."", ""Transform your text into stunning images with ease using Diffusers for Mac, a native app powered by state-of-the-art diffusion models. It leverages a bouquet of SoTA Text-to-Image models contributed by the community to the Hugging Face Hub, and converted to Core ML for blazingly fast performance. The Diffusers app ([App Store](https://apps.apple.com/app/diffusers/id1666309574), [source code](https://github.com/huggingface/swift-coreml-diffusers)) is the Mac counterpart to our [üß®`diffusers` library](https://github.com/huggingface/diffusers). This library is written in Python with PyTorch, and uses a modular design to train and run diffusion models."", ""- The [Getting Started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers\\_intro.ipynb) notebook, that gives a broader overview on Diffusion systems."", ""ü§ó Diffusers provides a collection of training scripts for you to train your own diffusion models. Each training script is:\n- **Self-contained**: the training script does not depend on any local files, and all packages required to run the script are installed from the `requirements.txt` file. \n- **Easy-to-tweak**: the training scripts are an example of how to train a diffusion model for a specific task and won't work out-of-the-box for every training scenario. You'll likely need to adapt the training script for your specific use-case. To help you with that, we've fully exposed the data preprocessing code and the training loop so you can modify it for your own use. \n- **Beginner-friendly**: the training scripts are designed to be beginner-friendly and easy to understand, rather than including the latest state-of-the-art methods to get the best and most competitive results."", ""The APIs in this section are more experimental and prone to breaking changes. Most of them are used internally for development, but they may also be useful to you if you're interested in building a diffusion model with some custom parts or if you're interested in some of our helper utilities for working with ü§ó Diffusers."", ""| **Documentation** | **What can I learn?** |\n|---------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Tutorial](https://huggingface.co/docs/diffusers/tutorials/tutorial_overview) | A basic crash course for learning how to use the library's most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model. |"", ""Welcome to üß® Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how üß® Diffusers is meant to be used. You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want. After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications."", ""ü§ó Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, ü§ó Diffusers is a modular toolbox that supports both."", ""Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities. Its purpose is to serve as a **modular toolbox** for both inference and training."", ""# üß® Diffusers Examples\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library for a variety of use cases involving training or fine-tuning."", ""In this post, we want to show how to use Stable Diffusion with the [üß® Diffusers library](https://github.com/huggingface/diffusers), explain how the model works and finally dive a bit deeper into how `diffusers` allows one to customize the image generation pipeline."", ""Learn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you're using ü§ó Diffusers for the first time!"", ""Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities. Its purpose is to serve as a **modular toolbox** for both inference and training.""]","The purpose of the üß® Diffusers tutorials is to provide a gentle introduction to diffusion models and help users understand the library fundamentals. These beginner-friendly tutorials are designed to teach users how to use a pipeline for inference to rapidly generate outputs, and then deconstruct that pipeline to understand how to use the library as a modular toolbox for building their own diffusion systems. After completing the tutorials, users will gain the necessary skills to start exploring the library on their own and apply it to their own projects and applications.",To provide a gentle introduction to diffusion models and help understand the library fundamentals.
"What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?
","[""- `allow\\_flagging`: this parameter can be set to either `\""manual\""` (default), `\""auto\""`, or `\""never\""`."", ""By default, an `Interface` will have \""Flag\"" button. If you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging_options` argument of Interface."", ""```python\nallow_flagging=\""manual\""\n```\n```python\nallow_flagging=\""manual\"",\n```"", ""- `allow\\_flagging`ÔºöÊ≠§ÂèÇÊï∞ÂèØ‰ª•ËÆæÁΩÆ‰∏∫ `\""manual\""`ÔºàÈªòËÆ§ÂÄºÔºâÔºå`\""auto\""` Êàñ `\""never\""`„ÄÇ\n- `manual`ÔºöÁî®Êà∑Â∞ÜÁúãÂà∞‰∏Ä‰∏™Ê†áËÆ∞ÊåâÈíÆÔºåÂè™ÊúâÂú®ÁÇπÂáªÊåâÈíÆÊó∂Ê†∑Êú¨Êâç‰ºöË¢´Ê†áËÆ∞„ÄÇ\n- `auto`ÔºöÁî®Êà∑Â∞Ü‰∏ç‰ºöÁúãÂà∞‰∏Ä‰∏™Ê†áËÆ∞ÊåâÈíÆÔºå‰ΩÜÊØè‰∏™Ê†∑Êú¨ÈÉΩ‰ºöËá™Âä®Ë¢´Ê†áËÆ∞„ÄÇ\n- `never`ÔºöÁî®Êà∑Â∞Ü‰∏ç‰ºöÁúãÂà∞‰∏Ä‰∏™Ê†áËÆ∞ÊåâÈíÆÔºåÂπ∂‰∏î‰∏ç‰ºöÊ†áËÆ∞‰ªª‰ΩïÊ†∑Êú¨„ÄÇ"", ""\""on Hugging Face Spaces, `allow_flagging` is set to `\""never\""` by default.\""""]","The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `""manual""`.","""manual"""
"Where can the full code for the Stable Diffusion demo be found?
","[""ote: This is a simplified version of the code needed to create the Stable Diffusion demo."", ""# üß® Stable Diffusion in JAX / Flax ! [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_jax_how_to.ipynb)\n# **Stable Diffusion in JAX / Flax** üöÄ\nü§ó Hugging Face [Diffusers](https://github.com/huggingface/diffusers) supports Flax since version `0.5.1`! This allows for super fast inference on Google TPUs, such as those available in Colab, Kaggle or Google Cloud Platform. This post shows how to run inference using JAX / Flax. If you want more details about how Stable Diffusion works or want to run it in GPU, please refer to [this Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb). If you want to follow along, click the button above to open this post as a Colab notebook. First, make sure you are using a TPU backend. If you are running this notebook in Colab, select `Runtime` in the menu above, then select the option \""Change runtime type\"" and then select `TPU` under the `Hardware accelerator` setting. Note that JAX is not exclusive to TPUs, but it shines on that hardware because each TPU server has 8 TPU accelerators working in parallel. ## Setup\n``` python\nimport jax\nnum_devices = jax.device_count()\ndevice_type = jax.devices()[0].device_kind\nprint(f\""Found {num_devices} JAX devices of type {device_type}.\"")\nassert \""TPU\"" in device_type, \""Available device is not a TPU, please select TPU from Edit > Notebook settings > Hardware accelerator\""\n```\n*Output*:\n```bash\nFound 8 JAX devices of type TPU v2.\n```"", ""Free solutions include many [spaces](https://huggingface.co/spaces) here on ü§ó Hugging Face, such as the [Stable Diffusion 2.1 Demo](https://huggingface.co/spaces/stabilityai/stable-diffusion) or the [camemduru webui](https://huggingface.co/spaces/camenduru/webui). You can even use ü§ó [Diffusers](https://huggingface.co/docs/diffusers/index) to write your own free solution! You can find a simple code example to get started [here](https://colab.research.google.com/drive/1HebngGyjKj7nLdXfj6Qi0N1nh7WvD74z?usp=sharing)."", ""```python\nfrom diffusers import DiffusionPipeline\nimport matplotlib.pyplot as plt\npipe = DiffusionPipeline.from\\_pretrained('CompVis/stable-diffusion-v1-4', custom\\_pipeline='suvadityamuk/StableDiffusionComparison')\npipe.enable\\_attention\\_slicing()\npipe = pipe.to('cuda')\nprompt = \""an astronaut riding a horse on mars\""\noutput = pipe(prompt)\nplt.subplots(2,2,1)\nplt.imshow(output.images[0])\nplt.title('Stable Diffusion v1.1')\nplt.axis('off')\nplt.subplots(2,2,2)\nplt.imshow(output.images[1])\nplt.title('Stable Diffusion v1.2')\nplt.axis('off')\nplt.subplots(2,2,3)\nplt.imshow(output.images[2])\nplt.title('Stable Diffusion v1.3')\nplt.axis('off')\nplt.subplots(2,2,4)\nplt.imshow(output.images[3])\nplt.title('Stable Diffusion v1.4')\nplt.axis('off')\nplt.show()\n```"", ""- Our [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). \n- Our [code in GitHub](https://github.com/huggingface/diffusers) where we'd be more than happy if you leave a ‚≠ê if `diffusers` is useful to you!"", ""The original codebase can be found at [Fantasy-Studio/Paint-by-Example](https://github.com/Fantasy-Studio/Paint-by-Example), and you can try it out in a [demo](https://huggingface.co/spaces/Fantasy-Studio/Paint-by-Example)."", ""Check out the Stable Diffusion demo for more: https://hf.co/spaces/stabilityai/stable-diffusion/"", ""```\nThe [Diffusers](https://huggingface.co/docs/diffusers/index) library makes it extremely simple to generate images with Stable Diffusion models. First, let's create a virtual environment with the required libraries: Transformers, Diffusers, Accelerate, and PyTorch. ```\nvirtualenv sd\\_inference\nsource sd\\_inference/bin/activate\npip install pip --upgrade\npip install transformers diffusers accelerate torch==1.13.1\n```\nThen, we write a simple benchmarking function that repeatedly runs inference, and returns the average latency for a single-image generation. ```python\nimport time\ndef elapsed\\_time(pipeline, prompt, nb\\_pass=10, num\\_inference\\_steps=20):\n# warmup\nimages = pipeline(prompt, num\\_inference\\_steps=10).images\nstart = time.time()\nfor \\_ in range(nb\\_pass):\n\\_ = pipeline(prompt, num\\_inference\\_steps=num\\_inference\\_steps, output\\_type=\""np\"")\nend = time.time()\nreturn (end - start) / nb\\_pass\n```\nNow, let's build a `StableDiffusionPipeline` with the default `float32` data type, and measure its inference latency. ```python\nfrom diffusers import StableDiffusionPipeline\nmodel\\_id = \""runwayml/stable-diffusion-v1-5\""\npipe = StableDiffusionPipeline.from\\_pretrained(model\\_id)\nprompt = \""sailing ship in storm by Rembrandt\""\nlatency = elapsed\\_time(pipe, prompt)\nprint(latency)\n```\nThe average latency is **32.3 seconds**.\n```"", ""See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main"", ""Code is available at https://github.com/CompVis/latent-diffusion.*"", ""Gradio Demo: stable-diffusion\n### Note: This is a simplified version of the code needed to create the Stable Diffusion demo."", ""You can find the original codebase for Stable Diffusion v1.0 at [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion) and Stable Diffusion v2.0 at [Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion) as well as their original scripts for various tasks.""]",The full code for the Stable Diffusion demo can be found at this link: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main.,https://hf.co/spaces/stabilityai/stable-diffusion/tree/main
"What transformation does the FNet model use to replace the self-attention layer in a BERT model?
","[""The FNet model replaces the self-attention layer in a BERT model with a fourier transform which returns only the real parts of the transform. The model is significantly faster than the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97% accuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the paper is the following:\n*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \""mix\"" input tokens. These linear mixers, along with standard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text classification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the \""efficient\"" Transformers on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.*"", ""*Pre-trained language models like BERT and its variants have recently achieved impressive performance in various natural language understanding tasks. However, BERT heavily relies on the global self-attention block and thus suffers large memory footprint and computation cost. Although all its attention heads query on the whole input sequence for generating the attention map from a global perspective, we observe some heads only need to learn local dependencies, which means the existence of computation redundancy. We therefore propose a novel span-based dynamic convolution to replace these self-attention heads to directly model local dependencies. The novel convolution heads, together with the rest self-attention heads, form a new mixed attention block that is more efficient at both global and local context learning. We equip BERT with this mixed attention design and build a ConvBERT model. Experiments have shown that ConvBERT significantly outperforms BERT and its variants in various downstream tasks, with lower training cost and fewer model parameters.*"", ""2. Second, BERT\\'s *bi-directional* self-attention layers have to be changed to *uni-directional* self-attention layers to comply with auto-regressive generation. Because both the bi-directional and the uni-directional self-attention layer are based on the same *key*, *query* and *value* projection weights, the decoder\\'s self-attention layer weights can be initialized with BERT\\'s self-attention layer weights. *E.g.* the query, key and value weight parameters of the decoder\\'s uni-directional self-attention layer are initialized with those of BERT\\'s bi-directional self-attention layer \\\\(\\theta\\_{\\text{BERT}}^{\\text{self-attn}, 2} = \\{\\mathbf{W}\\_{\\text{BERT}, k}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{BERT}, v}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{BERT}, q}^{\\text{self-attn}, 2} \\} \\to \\theta\\_{\\text{dec}}^{\\text{self-attn}, 2} = \\{\\mathbf{W}\\_{\\text{dec}, k}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{dec}, v}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{dec}, q}^{\\text{self-attn}, 2} \\}. \\\\) However, in *uni-directional* self-attention each token only attends to all previous tokens, so that the decoder\\'s self-attention layers yield different output vectors than BERT\\'s self-attention layers even though they share the same weights."", ""1. \\*\\*[FNet](https://huggingface.co/docs/transformers/model\\_doc/fnet)\\*\\* (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon."", ""The FocalNet model was proposed in [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. FocalNets completely replace self-attention (used in models like [ViT](vit) and [Swin](swin)) by a focal modulation mechanism for modeling token interactions in vision. The authors claim that FocalNets outperform self-attention based models with similar computational costs on the tasks of image classification, object detection, and segmentation. The abstract from the paper is the following:\n*We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its content, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show FocalNets outperform the state-of-the-art SA counterparts (e.g., Swin and Focal Transformers) with similar computational costs on the tasks of image classification, object detection, and segmentation.*"", ""1. \\*\\*[FNet](https://huggingface.co/docs/transformers/model\\_doc/fnet)\\*\\* (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon."", ""1. \\*\\*[FNet](https://huggingface.co/docs/transformers/model\\_doc/fnet)\\*\\* (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon."", ""1. \\*\\*[FNet](https://huggingface.co/docs/transformers/model\\_doc/fnet)\\*\\* (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon."", ""1. \\*\\*[FNet](https://huggingface.co/docs/transformers/model\\_doc/fnet)\\*\\* (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon."", ""1. \\*\\*[FNet](https://huggingface.co/docs/transformers/model\\_doc/fnet)\\*\\* (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon."", ""1. \\*\\*[FNet](https://huggingface.co/docs/transformers/model\\_doc/fnet)\\*\\* (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.""]","The FNet model replaces the self-attention layer in a BERT model with a Fourier transform, which returns only the real parts of the transform.",Fourier transform
"What type of test should typically accompany a bug fix in Gradio's testing strategy?
","[""- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)"", ""```\nGradio Demo: queue\\_full\\_e2e\\_test\n```\n```\nimport gradio as gr\nimport time\nimport random\nn\\_calls = 0\ndef get\\_random\\_number():\nglobal n\\_calls\nif n\\_calls == 1:\nn\\_calls += 1\nraise gr.Error(\""This is a gradio error\"")\nn\\_calls += 1\ntime.sleep(5)\nreturn random.randrange(1, 10)\nwith gr.Blocks() as demo:\nwith gr.Row():\nwith gr.Column():\nfirst = gr.Button(\""First Call\"")\nsecond = gr.Button(\""Second Call\"")\nthird = gr.Button(\""Third Call\"")\nfourth = gr.Button(\""Fourth Call\"")\nwith gr.Column():\nfirst\\_o = gr.Number(label=\""First Result\"")\nsecond\\_o = gr.Number(label=\""Second Result\"")\nthird\\_o = gr.Number(label=\""Third Result\"")\nfourth\\_o = gr.Number(label=\""Fourth Result\"")\nfirst.click(get\\_random\\_number, None, first\\_o, concurrency\\_id=\""f\"")\nsecond.click(get\\_random\\_number, None, second\\_o, concurrency\\_id=\""f\"")\nthird.click(get\\_random\\_number, None, third\\_o, concurrency\\_id=\""f\"")\nfourth.click(get\\_random\\_number, None, fourth\\_o, concurrency\\_id=\""f\"")\ndemo.queue(max\\_size=2)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""- Fix example inputs for `gr.File(file\\_count='multiple')` output components by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4153](https://github.com/gradio-app/gradio/pull/4153)\n- Only send request to `/info` route if demo version is above `3.28.3` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4109](https://github.com/gradio-app/gradio/pull/4109)\n- Fix bug in test from gradio 3.29.0 refactor by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4138](https://github.com/gradio-app/gradio/pull/4138)\n- Removes extraneous `State` component info from the `Client.view\\_api()` method by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)\n- Separates flaky tests from non-flaky tests by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)"", ""- [#5682](https://github.com/gradio-app/gradio/pull/5682) [`c57f1b75e`](https://github.com/gradio-app/gradio/commit/c57f1b75e272c76b0af4d6bd0c7f44743ff34f26) - Fix functional tests. Thanks [@abidlabs](https://github.com/abidlabs)!"", ""- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should _typically_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule."", ""- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!"", ""- Fix audio streaming, which was previously choppy in [PR 2351](https://github.com/gradio-app/gradio/pull/2351). Big thanks to [@yannickfunk](https://github.com/yannickfunk) for the proposed solution. \n- Fix bug where new typeable slider doesn't respect the minimum and maximum values [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 2380](https://github.com/gradio-app/gradio/pull/2380) \n- Fix bug where errors would cause apps run in reload mode to hang forever by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2394](https://github.com/gradio-app/gradio/pull/2394)"", ""- Fixes bug that the chatbot sample code does not work with certain input value by [@petrov826](https://github.com/petrov826) in [PR 3039](https://github.com/gradio-app/gradio/pull/3039).\n- Change chatbot to show dots on progress, and fix bug where chatbot would not stick to bottom in the case of images by [@aliabid94](https://github.com/aliabid94) in [PR 3067](https://github.com/gradio-app/gradio/pull/3079)"", ""- Fix example inputs for `gr.File(file\\_count='multiple')` output components by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4153](https://github.com/gradio-app/gradio/pull/4153)\n- Only send request to `/info` route if demo version is above `3.28.3` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4109](https://github.com/gradio-app/gradio/pull/4109)\n- Fix bug in test from gradio 3.29.0 refactor by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4138](https://github.com/gradio-app/gradio/pull/4138)\n- Removes extraneous `State` component info from the `Client.view\\_api()` method by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)\n- Separates flaky tests from non-flaky tests by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)"", ""- Fixes bug that the chatbot sample code does not work with certain input value by [@petrov826](https://github.com/petrov826) in [PR 3039](https://github.com/gradio-app/gradio/pull/3039).\n- Change chatbot to show dots on progress, and fix bug where chatbot would not stick to bottom in the case of images by [@aliabid94](https://github.com/aliabid94) in [PR 3067](https://github.com/gradio-app/gradio/pull/3079)"", ""- Fix audio streaming, which was previously choppy in [PR 2351](https://github.com/gradio-app/gradio/pull/2351). Big thanks to [@yannickfunk](https://github.com/yannickfunk) for the proposed solution. \n- Fix bug where new typeable slider doesn't respect the minimum and maximum values [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 2380](https://github.com/gradio-app/gradio/pull/2380)"", ""- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!"", ""- [#5682](https://github.com/gradio-app/gradio/pull/5682) [`c57f1b75e`](https://github.com/gradio-app/gradio/commit/c57f1b75e272c76b0af4d6bd0c7f44743ff34f26) - Fix functional tests. Thanks [@abidlabs](https://github.com/abidlabs)!""]","Every bug fix should typically be accompanied by a dynamic code test that failed before the fix and passes afterwards. However, it could also be a linting rule or a new type if that is appropriate.",Dynamic code test
"How can you force mixed precision training when initializing the Accelerator in ü§ó Accelerate?
","[""You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init."", ""```bash\naccelerate launch train_controlnet.py \\\n--mixed_precision=\""fp16\""\n```"", ""```python\n... accelerator = Accelerator(\n... mixed_precision=config.mixed_precision,\n...\n```"", ""```py\nfrom accelerate import Accelerator\naccelerator = Accelerator()\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\nmodel, optimizer, train_dataloader, eval_dataloader\n)\n```"", ""To enable mixed precision training, set the `fp16` flag to `True`:\n```py\ntraining\\_args = TrainingArguments(per\\_device\\_train\\_batch\\_size=4, fp16=True, \\*\\*default\\_args)\n```"", ""we can specify if we want to use mixed precision training and it will take care of it for us in the [`prepare`] call."", ""```py\naccelerator = Accelerator(fp16=training_args.fp16)\n```"", ""```json\n{\n\""communication\\_data\\_type\"": \""fp32\""\n}\n```\n```json\n\""amp\"": {\n\""enabled\"": \""auto\"",\n\""opt\\_level\"": \""auto\""\n}\n```\n```json\n{\n\""amp\"": {\n\""enabled\"": true,\n\""opt\\_level\"": \""O1\""\n}\n}\n```""]","To force mixed precision training when initializing the Accelerator in ü§ó Accelerate, you can set the `fp16` flag to `True` during the initialization. Here‚Äôs how you can do it:

```python
from accelerate import Accelerator
accelerator = Accelerator(fp16=True)
```

This will enable mixed precision training for your model. Additionally, you can also specify mixed precision in the `TrainingArguments`:

```python
training_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)
```

This ensures that mixed precision training is utilized throughout your training process.",By passing `fp16=True` to the Accelerator init.
"What is the purpose of tokenizers in the NLP pipeline?
","[""A Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`. The various steps of the pipeline are:\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are the [unicode normalization standards](https://unicode.org/reports/tr15/#Norm\\_Forms), such as `NFD` or `NFKC`.\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of splitting text is simply on whitespace.\n3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be `BPE` or `WordPiece`."", ""First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor."", ""In Natural Language Processing, most of the data that we handle consists of raw text. However, machine learning models cannot read and understand text in its raw form they can only work with numbers. The tokenizer's objective will be to translate the text into numbers. There are several possible approaches to this conversion, and the objective is to find the most meaningful representation."", ""he tokenization pipeline involves several steps that convert raw text into numbers. In this video, we will see what happens during the pre-tokenization step. The pre-tokenization operation is the operation performed after the normalization of the text and before the application of the tokenization algorithm. This step consists in applying rules that do not need to be learned to perform a first division of the text. Through these 3 examples, we could observe the two main types of operations brought by the pre-tokenization: some changes on the text and the division of the string into tokens that can be associated to words. Finally, the \""backend_tokenizer\"" of the fast tokenizers also allows to test the pre-tokenization operation very easily thanks to its \""pre_tokenize_str\"" method. We notice that the output of this operation is composed of both tokens and offsets which allow to link the token to its position in the text given in input of the method."", ""The algorithm simply picks the most\nlikely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their\nprobabilities. Those probabilities are defined by the loss the tokenizer is trained on. Assuming that the training data consists of\nthe words \\\\(x\\_{1}, \\dots, x\\_{N}\\\\) and that the set of all possible tokenizations for a word \\\\(x\\_{i}\\\\) is\ndefined as \\\\(S(x\\_{i})\\\\), then the overall loss is defined as\n$$\\mathcal{L} = -\\sum\\_{i=1}^{N} \\log \\left ( \\sum\\_{x \\in S(x\\_{i})} p(x) \\right )$$\n### SentencePiece\nAll tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to\nseparate words. However, not all languages use spaces to separate words. One possible solution is to use language\nspecific pre-tokenizers, \\*e.g.\\* [XLM](model\\_doc/xlm) uses a specific Chinese, Japanese, and Thai pre-tokenizer). To solve this problem more generally, [SentencePiece: A simple and language independent subword tokenizer and\ndetokenizer for Neural Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) treats the input\nas a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram\nalgorithm to construct the appropriate vocabulary. The [`XLNetTokenizer`] uses SentencePiece for example, which is also why in the example earlier the\n`\""‚ñÅ\""` character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be\nconcatenated and `\""‚ñÅ\""` is replaced by a space. All transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models\nusing SentencePiece are [ALBERT](model\\_doc/albert), [XLNet](model\\_doc/xlnet), [Marian](model\\_doc/marian), and [T5](model\\_doc/t5)."", ""Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. In NLP tasks, the data that is generally processed is raw text. However, models can only process numbers, so we need to find a way to convert the raw text to numbers. That's what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation ‚Äî that is, the one that makes the most sense to the model ‚Äî and, if possible, the smallest representation."", ""- Normalization (any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.)\n- Pre-tokenization (splitting the input into words)\n- Running the input through the model (using the pre-tokenized words to produce a sequence of tokens)\n- Post-processing (adding the special tokens of the tokenizer, generating the attention mask and token type IDs)"", ""he fast tokenizers of the Transformers library are fast, but they also implement features that will be super useful for data pre-processing and post-processing. Let's have a look at them! First, let's have a look at the usual output of a tokenizer. We get input IDs that correspond to tokens, but we lose a lot of information in the process. For instance, here the tokenization is the same for the two sentences, even if one has several more spaces than the other. Just having the input IDs is thus not enough if we want to match some tokens with a span of text (something we will need to do when tackling question answering for instance). It's also difficult to know when two tokens belong to the same word or not: it looks easy when you just look at the output of a BERT tokenizer, we just need to look for the ##. But other tokenizers have different ways to tokenize parts of words. For instance RoBERTa adds this special G symbol to mark the tokens at the beginning of a word, and T5 uses this special underscore symbol for the same purpose."", ""The code in the `tokenizers` library takes care of all these details for all the models in the Hub. Specifically, we currently support BPE (Byte-Pair Encoding) tokenizers, one of the three main families in use today. The GPT models, Falcon and Llama, all use this method. However, you don't usually need to tokenize the input text yourself - the [`Generation` code](https://github.com/huggingface/swift-transformers/blob/17d4bfae3598482fc7ecf1a621aa77ab586d379a/Sources/Generation/Generation.swift#L82) will take care of it."", ""With this kind of tokenizer, we can end up with some pretty large \""vocabularies,\"" where a vocabulary is defined by the total number of independent tokens that we have in our corpus. Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word. If we want to completely cover a language with a word-based tokenizer, we'll need to have an identifier for each word in the language, which will generate a huge amount of tokens. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we'd need to keep track of that many IDs. Furthermore, words like \""dog\"" are represented differently from words like \""dogs\"", and the model will initially have no way of knowing that \""dog\"" and \""dogs\"" are similar: it will identify the two words as unrelated. The same applies to other similar words, like \""run\"" and \""running\"", which the model will not see as being similar initially. Finally, we need a custom token to represent words that are not in our vocabulary. This is known as the \""unknown\"" token, often represented as \""[UNK]\"" or \""<unk>\"". It's generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn't able to retrieve a sensible representation of a word and you're losing information along the way. The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token. One way to reduce the amount of unknown tokens is to go one level deeper, using a _character-based_ tokenizer. Character-based tokenizers split the text into characters, rather than words. This has two primary benefits: - The vocabulary is much smaller. - There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters. But here too some questions arise concerning spaces and punctuation: This approach isn't perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, it's less meaningful: each character doesn't mean a lot on its own, whereas that is the case with words. However, this again differs according to the language; in Chinese, for example, each character carries more information than a character in a Latin language. Another thing to consider is that we'll end up with a very large amount of tokens to be processed by our model: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters. To get the best of both worlds, we can use a third technique that combines the two approaches: *subword tokenization*. Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. For instance, \""annoyingly\"" might be considered a rare word and could be decomposed into \""annoying\"" and \""ly\"". These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of \""annoyingly\"" is kept by the composite meaning of \""annoying\"" and \""ly\"". These subwords end up providing a lot of semantic meaning: for instance, in the example above \""tokenization\"" was split into \""token\"" and \""ization\"", two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word)."", ""Pre-tokenization is the act of splitting a text into smaller objects that give an upper bound to what your tokens will be at the end of training. A good way to think of this is that the pre-tokenizer will split your text into \""words\"" and then, your final tokens will be parts of those words. An easy way to pre-tokenize inputs is to split on spaces and punctuations, which is done by the `pre_tokenizers.Whitespace` pre-tokenizer: The output is a list of tuples, with each tuple containing one word and its span in the original sentence (which is used to determine the final `offsets` of our `Encoding`). Note that splitting on punctuation will split contractions like `\""I'm\""` in this example. You can combine together any `PreTokenizer` together. For instance, here is a pre-tokenizer that will split on space, punctuation and digits, separating numbers in their individual digits: As we saw in the `quicktour`, you can customize the pre-tokenizer of a `Tokenizer` by just changing the corresponding attribute. Of course, if you change the way the pre-tokenizer, you should probably retrain your tokenizer from scratch afterward."", ""he tokenizer pipeline. In this video, we'll look at how a tokenizer converts raw text to numbers that a Transformer model can make sense of, like when we execute this code. Here is a quick overview of what happens inside the tokenizer object: first the text is split into tokens, which are words, parts of words, or punctuation symbols. Then the tokenizer adds potential special tokens and converts each token to their unique respective ID as defined by the tokenizer's vocabulary. The first step is to split our input text into tokens with the tokenize method. To do this, the tokenizer may first perform some operations like lowercasing all words, then follow a set of rules to split the result in small chunks of text. Most of the Transformers models use a subword tokenization algorithm, which means that one given word can be split in several tokens, like tokenize here."", ""The `PreTokenizer` takes care of splitting the input according to a set of rules."", ""The tokenization pipeline\nWhen calling `Tokenizer.encode` or\n`Tokenizer.encode_batch`, the input\ntext(s) go through the following pipeline:\n- `normalization`\n- `pre-tokenization`\n- `model`\n- `post-processing`"", ""# Tokenizers\nFast State-of-the-art tokenizers, optimized for both research and production\n[ü§ó Tokenizers](https://github.com/huggingface/tokenizers) provides an implementation of today's most used tokenizers, with a focus on performance and versatility. These tokenizers are also used in [ü§ó Transformers](https://github.com/huggingface/transformers). # Main features:\n- Train new vocabularies and tokenize, using today's most used tokenizers. - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server's CPU. - Easy to use, but also extremely versatile. - Designed for both research and production. - Full alignment tracking. Even with destructive normalization, it's always possible to get the part of the original sentence that corresponds to any token. - Does all the pre-processing: Truncation, Padding, add the special tokens your model needs."", ""- Train new vocabularies and tokenize using 4 pre-made tokenizers (Bert WordPiece and the 3 most common BPE versions). \n- Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server's CPU. \n- Easy to use, but also extremely versatile. \n- Designed for research and production. \n- Normalization comes with alignments tracking. It's always possible to get the part of the original sentence that corresponds to a given token. \n- Does all the pre-processing: Truncate, Pad, add the special tokens your model needs."", ""Provides an implementation of today's most used tokenizers, with a focus on performance and versatility. ## Main features:\n- Train new vocabularies and tokenize, using today's most used tokenizers. - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server's CPU. - Easy to use, but also extremely versatile. - Designed for research and production. - Normalization comes with alignments tracking. It's always possible to get the part of the original sentence that corresponds to a given token. - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs."", ""First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a TensorFlow tensor.""]","Tokenizers are one of the core components of the NLP pipeline. Their primary purpose is to translate text into data that can be processed by machine learning models. Since models can only process numbers, tokenizers convert raw text inputs into numerical data. The goal is to find the most meaningful representation of the text, which allows the model to understand and work with the data effectively. The tokenization process involves several steps, including normalization, pre-tokenization, running the input through the model, and post-processing.",To translate text into data that can be processed by the model.
"What is the purpose of the Safety Checker in the Diffusers library?
","[""### Safety checker\nDiffusion models like Stable Diffusion can generate harmful content, which is why üß® Diffusers has a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\_diffusion/safety\\_checker.py) to check generated outputs against known hardcoded NSFW content. If you'd like to disable the safety checker for whatever reason, pass `None` to the `safety\\_checker` argument:\n```python\nfrom diffusers import DiffusionPipeline\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\nstable\\_diffusion = DiffusionPipeline.from\\_pretrained(repo\\_id, safety\\_checker=None, use\\_safetensors=True)\n\""\""\""\nYou have disabled the safety checker for  by passing `safety\\_checker=None`."", ""- [\\*\\*Safety Checker\\*\\*](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\_diffusion/safety\\_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated."", ""There are 4 configurations (`SafetyConfig.WEAK`, `SafetyConfig.MEDIUM`, `SafetyConfig.STRONG`, and `SafetyConfig.MAX`) that can be applied:\n```python\n>>> from diffusers import StableDiffusionPipelineSafe\n>>> from diffusers.pipelines.stable\\_diffusion\\_safe import SafetyConfig\n>>> pipeline = StableDiffusionPipelineSafe.from\\_pretrained(\""AIML-TUDA/stable-diffusion-safe\"")\n>>> prompt = \""the four horsewomen of the apocalypse, painting by tom of finland, gaston bussiere, craig mullins, j. c. leyendecker\""\n>>> out = pipeline(prompt=prompt, \\*\\*SafetyConfig.MAX)\n```""]",The purpose of the Safety Checker in the Diffusers library is to check generated outputs against known hardcoded NSFW (Not Safe For Work) content. It compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. This helps to prevent the generation of harmful content by ensuring that outputs are screened for safety.,The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.
"What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?
","[""# Interact with Discussions and Pull Requests\nThe `huggingface\\_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub. \n\n## Retrieve Discussions and Pull Requests from the Hub\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(repo\\_id=\""bigscience/bloom\""):\n... print(f\""{discussion.num} - {discussion.title}, pr: {discussion.is\\_pull\\_request}\"")\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer\\_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]\n```\n`HfApi.get\\_repo\\_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions("", ""`HfApi`Èô§‰∫ÜÊèê‰æõ `git` Â∑≤ÁªèÊèê‰æõÁöÑÂäüËÉΩÂ§ñÔºåËøòÊèê‰æõÂÖ∂‰ªñÂäüËÉΩÔºå‰æãÂ¶ÇÔºö\n\\* ÁÆ°ÁêÜÂ≠òÂÇ®Â∫ì\n\\* ‰ΩøÁî®ÁºìÂ≠ò‰∏ãËΩΩÊñá‰ª∂‰ª•ËøõË°åÊúâÊïàÁöÑÈáçÂ§ç‰ΩøÁî®\n\\* Âú® Hub ‰∏≠ÊêúÁ¥¢Â≠òÂÇ®Â∫ìÂíåÂÖÉÊï∞ÊçÆ\n\\* ËÆøÈóÆÁ§æÂå∫ÂäüËÉΩÔºåÂ¶ÇËÆ®ËÆ∫„ÄÅPRÂíåËØÑËÆ∫"", ""We are thrilled to announce the release of our latest collaborative features: pull requests and discussions on the Hugging Face Hub! Pull requests and discussions are available today under the [community tab](https://huggingface.co/gpt2/discussions) for all repository types: models, datasets, and Spaces. Any member of the community can create and participate in discussions and pull requests, facilitating collaborations not only within teams, but also with everyone else in the community! \n\n[Discussions](https://huggingface.co/gpt2/discussions?type=discussion) allow community members ask and answer questions as well as share their ideas and suggestions directly with the repository owners and the community. Anyone can create and participate in discussions in the community tab of a repository. \n\n[Pull requests](https://huggingface.co/gpt2/discussions?type=pull_request) allow community members open, comment, merge, or close pull requests directly from the website. The easiest way to open a pull request is to use the \""Collaborate\"" button in the \""Files and versions\"" tab. It will let you do single file contributions very easily. Under the hood, our Pull requests do not use forks and branches, but instead, custom \""branches\"" called `refs` that are stored directly on the source repo. This approach to avoids the need to create a forks for each new version of the model/dataset."", ""The `huggingface\\_hub` Python library is a package which offers a set of tools for the model and datasets hubs. It provides simple methods and classes for common tasks like getting information about repositories on the hub and managing them. It provides simple APIs that work on top of git to manage those repositories' content and to integrate the Hub in your projects and libraries."", ""Pull requests and Discussions allow users to do community contributions to repositories. Pull requests and discussions work the same for all the repo types. At a high level, the aim is to build a simpler version of other git hosts' (like GitHub's) PRs and Issues:\n- no forks are involved: contributors push to a special `ref` branch directly on the source repo. - there's no hard distinction between discussions and PRs: they are essentially the same so they are displayed in the same lists. - they are streamlined for ML (i.e. models/datasets/spaces repos), not arbitrary repos. \\_Note, Pull Requests and discussions can be enabled or disabled from the [repository settings](./repositories-settings#disabling-discussions-pull-requests)\\_\n\nBy going to the community tab in any repository, you can see all Discussions and Pull requests. You can also filter to only see the ones that are open. \n\nThe Discussion page allows you to see the comments from different users. If it's a Pull Request, you can see all the changes by going to the Files changed tab. \n\nIf you opened a PR or discussion, are the author of the repository, or have write access to it, you can edit the discussion title by clicking on the pencil button. \n\nIf you have write access to a repository, you can pin discussions and Pull Requests. Pinned discussions appear at the top of all the discussions. \n\nIf you have write access to a repository, you can lock discussions or Pull Requests. Once a discussion is locked, previous comments are still visible and users won't be able to add new comments. \n\nIf you wrote a comment or have write access to the repository, you can edit the content of the comment from the contextual menu in the top-right corner of the comment box. \n\nYou can also hide a comment. Hiding a comment is irreversible, and nobody will be able to see its content nor edit it anymore. \n\nYes! You can use Markdown to add formatting to your comments. Additionally, you can utilize LaTeX for mathematical typesetting, your formulas will be rendered with [KaTeX](https://katex.org/) before being parsed in Markdown."", ""- [Pull Requests & Discussions](./repositories-pull-requests-discussions)\n- [Pull Requests advanced usage](./repositories-pull-requests-discussions#pull-requests-advanced-usage)"", ""```\nYou can also use [`HfApi.create_discussion`] (respectively [`HfApi.create_pull_request`]) to create a Discussion (respectively a Pull Request) on a repo. Opening a Pull Request this way can be useful if you need to work on changes locally. Pull Requests opened this way will be in `\""draft\""` mode. ```python\n>>> from huggingface_hub import create_discussion, create_pull_request\n>>> create_discussion(\n... repo_id=\""username/repo-name\"",\n... title=\""Hi from the huggingface_hub library!\"",\n... token=\""\"",\n... )\nDiscussionWithDetails(...)\n>>> create_pull_request(\n... repo_id=\""username/repo-name\"",\n... title=\""Hi from the huggingface_hub library!\"",\n... token=\""\"",\n... )\nDiscussionWithDetails(..., is_pull_request=True)\n```\nManaging Pull Requests and Discussions can be done entirely with the [`HfApi`] class. For example:\n* [`comment_discussion`] to add comments\n* [`edit_discussion_comment`] to edit comments\n* [`rename_discussion`] to rename a Discussion or Pull Request\n* [`change_discussion_status`] to open or close a Discussion / Pull Request\n* [`merge_pull_request`] to merge a Pull Request\nVisit the [`HfApi`] documentation page for an exhaustive reference of all available methods.\n```"", ""Check the [`HfApi`] documentation page for the reference of methods enabling interaction with Pull Requests and Discussions on the Hub. - [`get\\_repo\\_discussions`] - [`get\\_discussion\\_details`] - [`create\\_discussion`] - [`create\\_pull\\_request`] - [`rename\\_discussion`] - [`comment\\_discussion`] - [`edit\\_discussion\\_comment`] - [`change\\_discussion\\_status`] - [`merge\\_pull\\_request`]"", ""```python\n>>> from huggingface_hub import get_repo_discussions\n>>> discussions_list = list(get_repo_discussions(repo_id=\""bert-base-uncased\""))\n```\nThe [`Discussion`] object returned by [`HfApi.get_repo_discussions`] contains high-level overview of the\nDiscussion or Pull Request. You can also get more detailed information using [`HfApi.get_discussion_details`]:\n```python\n>>> from huggingface_hub import get_discussion_details\n>>> get_discussion_details(\n... repo_id=\""bigscience/bloom-1b3\"",\n... discussion_num=2\n... )\nDiscussionWithDetails(\nnum=2,\nauthor='cakiki',\ntitle='Update VRAM memory for the V100s',\nstatus='open',\nis_pull_request=True,\nevents=[\nDiscussionComment(type='comment', author='cakiki', ...),\nDiscussionCommit(type='commit', author='cakiki', summary='Update VRAM memory for the V100s', oid='1256f9d9a33fa8887e1c1bf0e09b4713da96773a', ...),\n],\nconflicting_files=[],\ntarget_branch='refs/heads/main',\nmerge_commit_oid=None,\ndiff='diff --git a/README.md b/README.md\\nindex a6ae3b9294edf8d0eda0d67c7780a10241242a7e..3a1814f212bc3f0d3cc8f74bdbd316de4ae7b9e3 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -132,7 +132,7 [...]',\n)\n```\n```python\n>>> from huggingface_hub import metadata_update\n>>> metadata_update(\n...\n```""]",The `HfApi` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.,HfApi
"What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?
","[""At Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others.""]",The name of the new library introduced by Hugging Face for hosting scikit-learn models is Skops.,Skops
"What is the purpose of Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion."", ""We also ran a final experiment where we combined [Textual Inversion](https://textual-inversion.github.io) with Dreambooth. Both techniques have a similar goal, but their approaches are different. In this experiment we first ran textual inversion for 2000 steps. From that model, we then ran Dreambooth for an additional 500 steps using a learning rate of `1e-6`. These are the results:  \nWe think the results are much better than doing plain Dreambooth but not as good as when we fine-tune the whole text encoder. It seems to copy the style of the training images a bit more, so it could be overfitting to them. We didn't explore this combination further, but it could be an interesting alternative to improve Dreambooth and still fit the process in a 16GB GPU. Feel free to explore and tell us about your results!"", ""# Textual Inversion\nTextual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder. [`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings. To learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading\\_adapters#textual-inversion) loading guide."", ""- Learn how to use [Textual Inversion](textual\\_inversion\\_inference) for inference with Stable Diffusion 1/2 and Stable Diffusion XL."", ""## Textual Inversion\n[Textual Inversion](../training/text\\_inversion) fine-tunes a model to teach it about a new concept. I.e. a few pictures of a style of artwork can be used to generate images in that style. ## Custom Diffusion\n[Custom Diffusion](../training/custom\\_diffusion) only fine-tunes the cross-attention maps of a pre-trained text-to-image diffusion model. It also allows for additionally performing Textual Inversion. It supports multi-concept training by design. Like DreamBooth and Textual Inversion, Custom Diffusion is also used to teach a pre-trained text-to-image diffusion model about new concepts to generate outputs involving the concept(s) of interest."", ""This method works by training and finding new embeddings that represent the images you provide with a special word in the prompt. As a result, the diffusion model weights stay the same and the training process produces a relatively tiny (a few KBs) file. Because textual inversion creates embeddings, it cannot be used on its own like DreamBooth and requires another model. \n\nTextual inversion can also be trained on undesirable things to create *negative embeddings* to discourage a model from generating images with those undesirable things like blurry images or extra fingers on a hand. This can be an easy way to quickly improve your prompt. You'll also load the embeddings with [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`], but this time, you'll need two more parameters:\n- `weight_name`: specifies the weight file to load if the file was saved in the ü§ó Diffusers format with a specific name or if the file is stored in the A1111 format\n- `token`: specifies the special word to use in the prompt to trigger the embeddings"", ""# Textual Inversion\n[Textual Inversion](https://hf.co/papers/2208.01618) is a training technique for personalizing image generation models with just a few example images of what you want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide."", ""# Textual inversion\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer). This guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text\\_inversion) training guide. \n\n## Stable Diffusion 1 and 2\nPick a Stable Diffusion checkpoint and a pre-learned concept from the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer):\n```py\npretrained\\_model\\_name\\_or\\_path = \""runwayml/stable-diffusion-v1-5\""\nrepo\\_id\\_embeds = \""sd-concepts-library/cat-toy\""\n```\nNow you can load a pipeline, and pass the pre-learned concept to it:\n```py\npipeline = StableDiffusionPipeline.from\\_pretrained(\npretrained\\_model\\_name\\_or\\_path, torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\npipeline.load\\_textual\\_inversion(repo\\_id\\_embeds)\n```\nCreate a prompt with the pre-learned concept by using the special placeholder token ``, and choose the number of samples and rows of images you'd like to generate:\n```py\nprompt = \""a grafitti in a favela wall with a  on it\""\nnum\\_samples\\_per\\_row = 2\nnum\\_rows = 2\n```\nThen run the pipeline (feel free to adjust the parameters like `num\\_inference\\_steps` and `guidance\\_scale` to see how they affect image quality), save the generated images and visualize them with the helper function you created at the beginning:\n```py\nall\\_images = []\nfor \\_ in range(num\\_rows):\nimages = pipeline(prompt, num\\_images\\_per\\_prompt=num\\_samples\\_per\\_row, num\\_inference\\_steps=50, guidance\\_scale=7.5).images\nall\\_images.extend(images)\ngrid = make\\_image\\_grid(all\\_images, num\\_rows, num\\_samples\\_per\\_row)\ngrid\n```\n\n## Stable Diffusion XL\nStable Diffusion XL (SDXL) can also use textual inversion vectors for inference. In contrast to Stable Diffusion 1 and 2, SDXL has two text encoders so you'll need two textual inversion embeddings - one for each text encoder model. Let's download the SDXL textual inversion embeddings and have a closer look at it's structure:\n```py\nfrom huggingface\\_hub import hf\\_hub\\_download\nfrom safetensors.torch import load\\_file\nfile = hf\\_hub\\_download(\""dn118/unaestheticXL\"", filename=\""unaestheticXLv31.safetensors\"")\nstate\\_dict = load\\_file(file)\nstate\\_dict\n```\nThere are two tensors, `\""clip\\_g\""` and `\""clip\\_l\""`. `\""clip\\_g\""` corresponds to the bigger text encoder in SDXL and refers to\n`pipe.text\\_encoder\\_2` and `\""clip\\_l\""` refers to `pipe.text\\_encoder`. Now you can load each tensor separately by passing them along with the correct text encoder and tokenizer\nto [`~loaders.TextualInversionLoaderMixin.load\\_textual\\_inversion`]:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipe = AutoPipelineForText2Image.from\\_pretrained(\""stabilityai/stable-diffusion-xl-base-1.0\"", variant=\""fp16\"", torch\\_dtype=torch.float16)\npipe.to(\""cuda\"")\npipe.load\\_textual\\_inversion(state\\_dict[\""clip\\_g\""], token=\""unaestheticXLv31\"", text\\_encoder=pipe.text\\_encoder\\_2, tokenizer=pipe.tokenizer\\_2)\npipe.load\\_textual\\_inversion(state\\_dict[\""clip\\_l\""], token=\""unaestheticXLv31\"", text\\_encoder=pipe.text\\_encoder, tokenizer=pipe.tokenizer)\n# the embedding should be used as a negative embedding, so we pass it as a negative prompt\ngenerator = torch.Generator().manual\\_seed(33)\nimage = pipe(\""a woman standing in front of a mountain\"", negative\\_prompt=\""unaestheticXLv31\"", generator=generator).images[0]\nimage\n```"", ""[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images.\\_By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images\\_ \n\nWe have enabled distillation for quantization in `textual\\_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method. \n\nDistillation for quantization is a method that combines [intermediate layer knowledge distillation](https://github.com/intel/neural-compressor/blob/master/docs/source/distillation.md#intermediate-layer-knowledge-distillation) and [quantization aware training](https://github.com/intel/neural-compressor/blob/master/docs/source/quantization.md#quantization-aware-training) in the same training process to improve the performance of the quantized model."", ""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion."", ""**IMPORTART: This research project is deprecated. Multi Token Textual Inversion is now supported natively in [the official textual inversion example](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion#running-locally-with-pytorch).**  \nTextual inversion is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion."", ""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.""]","Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. Its purpose is to teach a model, like Stable Diffusion, about new concepts using just 3-5 images. This allows users to generate images that reflect specific styles or concepts by using a special word in the prompt that corresponds to the learned embeddings.",Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.
"What is the recommended multiple of batch size for fp16 data type on an A100 GPU?
","[""For instance, for fp16 data type a multiple of 8 is recommended, unless it's an A100 GPU, in which case use multiples of 64."", ""|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None |None|None |None |26.0 |36.958 |27.058 |29152.98 |\n|fp16|None |None|None |None |26.0 |69.94 |228.76 |53986.51 |"", ""*Batch by 8 with ü§ó Better Transformer and half-precision.*"", ""I was able to fit a batch size (BS) of 16 before hitting Out of Memory (OOM) error. Since the key optimization here is that each technique deploys GPU RAM more efficiently, we will try to continually increase the batch size and expect the training and evaluation to complete faster."", ""**15.5B model -> 248GB of GPU memory without even considering huge memory requirements for storing intermediate activations -> minimum 4X A100 80GB GPUs required**  \n**Adding all of the above -> 9.51 GB ~10GB -> 1 A100 40GB GPU required** ü§Ø. The reason for A100 40GB GPU is that the intermediate activations for long sequence lengths of 2048 and batch size of 4 for training lead to higher memory requirements. As we will see below, GPU memory required is 26GB which can be accommodated on A100 40GB GPU. Also, A100 GPUs have better compatibilty with Flash Attention 2. In the above calculations, we didn't consider memory required for intermediate activation checkpointing which is considerably huge. We leverage Flash Attention V2 and Gradient Checkpointing to overcome this issue. 1. For QLoRA along with flash attention V2 and gradient checkpointing, the total memory occupied by the model on a single A100 40GB GPU is **26 GB** with a **batch size of 4**. 2. For full fine-tuning using FSDP along with Flash Attention V2 and Gradient Checkpointing, the memory occupied per GPU ranges between **70 GB to 77.6 GB** with a **per_gpu_batch_size of 1**."", ""If you would like to train with batches of size 64, do not set the `per_device_train_batch_size` to 1 and `gradient_accumulation_steps` to 64. Instead, keep `per_device_train_batch_size=4` and set `gradient_accumulation_steps=16`. This results in the same effective batch size while making better use of the available GPU resources. For additional information, please refer to batch size and gradient accumulation benchmarks for [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957)."", ""| Method | Batch Size Max ($BS) | Approx Train Time (minutes) | Notes |\n| --- | --- | --- | --- |\n| DDP (Distributed Data Parallel) | 7 | 15 | |\n| DDP + FP16 | 7 | 8 | |\n| FSDP with SHARD_GRAD_OP | 11 | 11 | |\n| FSDP with min_num_params = 1M + FULL_SHARD | 15 | 12 | |\n| FSDP with min_num_params = 2K + FULL_SHARD | 15 | 13 | |\n| FSDP with min_num_params = 1M + FULL_SHARD + Offload to CPU | 20 | 23 | |\n| FSDP with min_num_params = 2K + FULL_SHARD + Offload to CPU | 22 | 24 | |\nWith respect to DDP, from Table 1 we can observe that FSDP **enables larger batch sizes**, up to **2X-3X** without and with CPU offload setting, respectively.""]","For fp16 data type on an A100 GPU, it is recommended to use multiples of 64.",64
"How do you run a Gradio Blocks app in reload mode using a Python IDE?
","[""This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks. ## Why Auto-Reloading? When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes. To make it faster and more convenient to write your code, we've made it easier to \""reload\"" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. ## Python IDE Reload üî• If you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this: ```python import gradio as gr with gr.Blocks() as demo: gr.Markdown(\""# Greetings from Gradio!\"") inp = gr.Textbox(placeholder=\""What is your name?\"") out = gr.Textbox() inp.change(fn=lambda x: f\""Welcome, {x}!\"", inputs=inp, outputs=out) if __name__ == \""__main__\"": demo.launch() ``` The problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`."", ""**For reload mode**, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n1. Configure encoding declaration of python script, for example: `# -\\*- coding: cp1252 -\\*-`\n2. Confirm that your code editor has identified that encoding format. 3. Run like this: `gradio run.py --encoding cp1252`\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code."", ""ËøôÈáåÊúÄÈáçË¶ÅÁöÑ‰∏ÄË°åÊòØ `Ê≠£Âú®ËßÇÂØü ...`„ÄÇËøôÈáåÂèëÁîüÁöÑÊÉÖÂÜµÊòØ Gradio Â∞ÜËßÇÂØü `run.py` Êñá‰ª∂ÊâÄÂú®ÁöÑÁõÆÂΩïÔºåÂ¶ÇÊûúÊñá‰ª∂ÂèëÁîüÊõ¥ÊîπÔºåÂÆÉÂ∞ÜËá™Âä®‰∏∫ÊÇ®ÈáçÊñ∞ËøêË°åÊñá‰ª∂„ÄÇÂõ†Ê≠§ÔºåÊÇ®Âè™ÈúÄ‰∏ìÊ≥®‰∫éÁºñÂÜô‰ª£Á†ÅÔºåGradio ÊºîÁ§∫Â∞ÜËá™Âä®Âà∑Êñ∞ ü•≥\n‚ö†Ô∏è Ë≠¶ÂëäÔºö`gradio` ÂëΩ‰ª§‰∏ç‰ºöÊ£ÄÊµã‰º†ÈÄíÁªô `launch()` ÊñπÊ≥ïÁöÑÂèÇÊï∞ÔºåÂõ†‰∏∫Âú®ÈáçÊñ∞Âä†ËΩΩÊ®°Âºè‰∏ã‰ªéÊú™Ë∞ÉÁî® `launch()` ÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåËÆæÁΩÆ `launch()` ‰∏≠ÁöÑ `auth` Êàñ `show\\_error` ‰∏ç‰ºöÂú®Â∫îÁî®Á®ãÂ∫è‰∏≠ÂèçÊò†Âá∫Êù•„ÄÇ\nÂΩìÊÇ®‰ΩøÁî®ÈáçÊñ∞Âä†ËΩΩÊ®°ÂºèÊó∂ÔºåËØ∑ËÆ∞‰Ωè‰∏Ä‰ª∂ÈáçË¶ÅÁöÑ‰∫ãÊÉÖÔºöGradio ‰∏ìÈó®Êü•ÊâæÂêç‰∏∫ `demo` ÁöÑ Gradio Blocks/Interface ÊºîÁ§∫„ÄÇÂ¶ÇÊûúÊÇ®Â∞ÜÊºîÁ§∫ÂëΩÂêç‰∏∫ÂÖ∂‰ªñÂêçÁß∞ÔºåÊÇ®ÈúÄË¶ÅÂú®‰ª£Á†Å‰∏≠ÁöÑÁ¨¨‰∫å‰∏™ÂèÇÊï∞‰∏≠‰º†ÂÖ•ÊºîÁ§∫ÁöÑ FastAPI Â∫îÁî®Á®ãÂ∫èÁöÑÂêçÁß∞„ÄÇÂØπ‰∫é Gradio ÊºîÁ§∫ÔºåÂèØ‰ª•‰ΩøÁî® `.app` Â±ûÊÄßËÆøÈóÆ FastAPI Â∫îÁî®Á®ãÂ∫è„ÄÇÂõ†Ê≠§ÔºåÂ¶ÇÊûúÊÇ®ÁöÑ `run.py` Êñá‰ª∂Â¶Ç‰∏ãÊâÄÁ§∫Ôºö\n```python\nimport gradio as gr\nwith gr.Blocks() as my\\_demo:\ngr.Markdown(\""# Êù•Ëá™GradioÁöÑÈóÆÂÄôÔºÅ\"")\ninp = gr.Textbox(placeholder=\""ÊÇ®Âè´‰ªÄ‰πàÂêçÂ≠óÔºü\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""Ê¨¢ËøéÔºå{x}ÔºÅ\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\nmy\\_demo.launch()\n```\nÈÇ£‰πàÊÇ®ÂèØ‰ª•ËøôÊ†∑ÂêØÂä®ÂÆÉÔºö`gradio run.py my\\_demo.app`„ÄÇ\nüî• Â¶ÇÊûúÊÇ®ÁöÑÂ∫îÁî®Á®ãÂ∫èÊé•ÂèóÂëΩ‰ª§Ë°åÂèÇÊï∞ÔºåÊÇ®‰πüÂèØ‰ª•‰º†ÈÄíÂÆÉ‰ª¨„ÄÇ‰∏ãÈù¢ÊòØ‰∏Ä‰∏™‰æãÂ≠êÔºö\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# Ê¨¢Ëøé {args.name}ÔºÅ\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nÊÇ®ÂèØ‰ª•ÂÉèËøôÊ†∑ËøêË°åÂÆÉÔºö`gradio run.py --name Gretel`"", ""This means you'll see your changes _much, much faster_ than if you were rerunning the cell normally. Here's what it looks like in a jupyter notebook: ![](https://gradio-builds.s3.amazonaws.com/demo-files/jupyter_reload.gif) ü™Ñ This works in colab notebooks too! [Here's a colab notebook](https://colab.research.google.com/drive/1zAuWoiTIb3O2oitbtVb2_ekv1K6ggtC1?usp=sharing) where you can see the Blocks magic in action. Try making some changes and re-running the cell with the Gradio code! The Notebook Magic is now the author's preferred way of building Gradio demos. Regardless of how you write Python code, we hope either of these methods will give you a much better development experience using Gradio."", ""Instead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:\nIn the terminal, run `gradio run.py`. That's it! Now, you'll see that after you'll see something like this:\n```bash\nWatching: '/Users/freddy/sources/gradio/gradio', '/Users/freddy/sources/gradio/demo/'\nRunning on local URL: http://127.0.0.1:7860\n```\nThe important part here is the line that says `Watching...` What's happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically ü•≥\n‚ö†Ô∏è Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show_error` in `launch()` will not be reflected in the app. There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n```python\nimport gradio as gr\nwith gr.Blocks() as my_demo:\ngr.Markdown(\""# Greetings from Gradio!\"")\ninp = gr.Textbox(placeholder=\""What is your name?\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""Welcome, {x}!\"",\ninputs=inp,\noutputs=out)\nif __name__ == \""__main__\"":\nmy_demo.launch()\n```\nThen you would launch it in reload mode like this: `gradio run.py my_demo`."", ""ÈÄöËøáËá™Âä®ÈáçËΩΩÂÆûÁé∞Êõ¥Âø´ÁöÑÂºÄÂèë  \nÊú¨ÊåáÂçó‰ªãÁªç‰∫ÜËá™Âä®ÈáçÊñ∞Âä†ËΩΩ„ÄÅÂú® Python IDE ‰∏≠ÈáçÊñ∞Âä†ËΩΩ‰ª•ÂèäÂú® Jupyter Notebooks ‰∏≠‰ΩøÁî® gradio ÁöÑÊñπÊ≥ï„ÄÇ  \n## Python IDE ÈáçËΩΩ üî•  \nÂ¶ÇÊûúÊÇ®‰ΩøÁî® Python IDE ÊûÑÂª∫ Gradio BlocksÔºåÈÇ£‰πà‰ª£Á†ÅÊñá‰ª∂ÔºàÂÅáËÆæÂëΩÂêç‰∏∫ `run.py`ÔºâÂèØËÉΩÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö  \n```python  \nimport gradio as gr  \nwith gr.Blocks() as demo:  \ngr.Markdown(\""# Êù•Ëá™GradioÁöÑÈóÆÂÄôÔºÅ\"")  \ninp = gr.Textbox(placeholder=\""ÊÇ®Âè´‰ªÄ‰πàÂêçÂ≠óÔºü\"")  \nout = gr.Textbox()  \ninp.change(fn=lambda x: f\""Ê¨¢ËøéÔºå{x}ÔºÅ\"",  \ninputs=inp,  \noutputs=out)  \nif __name__ == \""__main__\"":  \ndemo.launch()  \n```  \nËÄå‰∏çÊòØËøôÊ†∑ÂÅöÔºåÊÇ®ÂèØ‰ª•ÈÄöËøáÊõ¥Êîπ 1 ‰∏™ÂçïËØçÊù•‰ª•**ÈáçÊñ∞Âä†ËΩΩÊ®°Âºè**ËøêË°å‰ª£Á†ÅÔºöÂ∞Ü `python` Êõ¥Êîπ‰∏∫ `gradio`Ôºö  \nÂú®ÁªàÁ´Ø‰∏≠ËøêË°å `gradio run.py`„ÄÇÂ∞±ÊòØËøôÊ†∑ÔºÅ  \nÁé∞Âú®ÔºåÊÇ®Â∞ÜÁúãÂà∞Á±ª‰ºº‰∫éËøôÊ†∑ÁöÑÂÜÖÂÆπÔºö  \n```bash  \nLaunching in *reload mode* on: http://127.0.0.1:7860 (Press CTRL+C to quit)  \nWatching...  \n```"", ""```\ndemo = gr.Blocks()\nwith demo:\ninp = gr.Textbox(placeholder=\""Enter text.\"")\nscroll_btn = gr.Button(\""Scroll\"")\nno_scroll_btn = gr.Button(\""No Scroll\"")\nout = gr.Textbox()\nscroll_btn.click(lambda x: x,\ninputs=inp,\noutputs=out,\nscroll_to_output=True)\nno_scroll_btn.click(lambda x: x,\ninputs=inp,\noutputs=out)\nif __name__ == \""__main__\"":\ndemo.launch()\n```"", ""```\nwith gr.Blocks() as demo:\ngr.Code(\nvalue=\""\""\""def hello\\_world():\nreturn \""Hello, world!\""\nprint(hello\\_world())\""\""\"",\nlanguage=\""python\"",\ninteractive=True,\nshow\\_label=False,\n)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```""]","Ë¶ÅÂú® Python IDE ‰∏≠‰ª•ÈáçÊñ∞Âä†ËΩΩÊ®°ÂºèËøêË°å Gradio Blocks Â∫îÁî®Á®ãÂ∫èÔºåÊÇ®ÂèØ‰ª•ÊåâÁÖß‰ª•‰∏ãÊ≠•È™§Êìç‰ΩúÔºö

1. Á°Æ‰øùÊÇ®ÁöÑ‰ª£Á†ÅÊñá‰ª∂Ôºà‰æãÂ¶Ç `run.py`ÔºâÂåÖÂê´ Gradio Blocks ÁöÑÂÆö‰πâ„ÄÇ‰æãÂ¶ÇÔºö

   ```python
   import gradio as gr
   with gr.Blocks() as demo:
       gr.Markdown(""# Êù•Ëá™GradioÁöÑÈóÆÂÄôÔºÅ"")
       inp = gr.Textbox(placeholder=""ÊÇ®Âè´‰ªÄ‰πàÂêçÂ≠óÔºü"")
       out = gr.Textbox()
       inp.change(fn=lambda x: f""Ê¨¢ËøéÔºå{x}ÔºÅ"", inputs=inp, outputs=out)
   if __name__ == ""__main__"":
       demo.launch()
   ```

2. Âú®ÁªàÁ´Ø‰∏≠ËøêË°å‰ª•‰∏ãÂëΩ‰ª§ÔºåÂ∞Ü `python` Êõ¥Êîπ‰∏∫ `gradio`Ôºö

   ```bash
   gradio run.py
   ```

3. ÊÇ®Â∞ÜÁúãÂà∞Á±ª‰ºº‰∫é‰ª•‰∏ãÂÜÖÂÆπÁöÑËæìÂá∫Ôºö

   ```bash
   Watching: '/path/to/your/directory'
   Running on local URL: http://127.0.0.1:7860
   ```

ËøôÊÑèÂë≥ÁùÄ Gradio Â∞ÜËßÇÂØü `run.py` Êñá‰ª∂ÊâÄÂú®ÁöÑÁõÆÂΩïÔºåÂ¶ÇÊûúÊñá‰ª∂ÂèëÁîüÊõ¥ÊîπÔºåÂÆÉÂ∞ÜËá™Âä®ÈáçÊñ∞ËøêË°åÊñá‰ª∂ÔºåÊÇ®ÂèØ‰ª•‰∏ìÊ≥®‰∫éÁºñÂÜô‰ª£Á†Å„ÄÇ",Run `gradio run.py` in the terminal.
"How can you install the Hugging Face Unity API in your Unity project?
","[""1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens). \n7. Test the API key by clicking `Test API key` in the API Wizard. \n8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field."", ""### 4. Speech Recognition\nNext, we'll want to use the Hugging Face Unity API to run speech recognition on our encoded audio. To do so, we'll create a `SendRecording()` method:\n```\nusing HuggingFace.API;\nprivate void SendRecording() {\nHuggingFaceAPI.AutomaticSpeechRecognition(bytes, response => {\ntext.color = Color.white;\ntext.text = response;\n}, error => {\ntext.color = Color.red;\ntext.text = error;\n});\n}\n```\nThis will send the encoded audio to the API, displaying the response in white if successful, otherwise the error message in red. Don't forget to call `SendRecording()` at the end of the `StopRecording()` method:\n```\nprivate void StopRecording() {\n/\\* other code \\*/\nSendRecording();\n}\n```\n### 5. Final Touches\nFinally, let's improve the UX of this demo a bit using button interactability and status messages. The Start and Stop buttons should only be interactable when appropriate, i.e. when a recording is ready to be started/stopped. Then, set the response text to a simple status message while recording or waiting for the API. The finished script should look something like this:\n```\nusing System.IO;\nusing HuggingFace.API;\nusing TMPro;\nusing UnityEngine;\nusing UnityEngine.UI;\npublic class SpeechRecognitionTest : MonoBehaviour {\n[SerializeField] private Button startButton;\n[SerializeField] private Button stopButton;\n[SerializeField] private TextMeshProUGUI text;\nprivate AudioClip clip;\nprivate byte[] bytes;\nprivate bool recording;\nprivate void Start() {\nstartButton.onClick.AddListener(StartRecording);\nstopButton.onClick.AddListener(StopRecording);\nstopButton.interactable = false;\n}\nprivate void Update() {\nif (recording && Microphone.GetPosition(null) >= clip.samples) {\nStopRecording();\n}\n}\nprivate void StartRecording() {\ntext.color = Color.white;\ntext.text = \""Recording...\"";\nstartButton.interactable = false;\nstopButton.interactable = true;\nclip = Microphone.Start(null, false, 10, 44100);\nrecording = true;\n}\nprivate void StopRecording() {\nvar position = Microphone.GetPosition(null);\nMicrophone.End(null);\nvar samples = new float[position * clip.channels];\nclip.GetData(samples, 0);\nbytes = EncodeAsWAV(samples, clip.frequency, clip.channels);\nrecording = false;\nSendRecording();\n}\nprivate void SendRecording() {\ntext.color = Color.yellow;\ntext.text = \""Sending...\"";\nstopButton.interactable = false;\nHuggingFaceAPI.AutomaticSpeechRecognition(bytes, response => {\ntext.color = Color.white;\ntext.text = response;\nstartButton.interactable = true;\n}, error => {\ntext.color = Color.red;\ntext.text = error;\nstartButton.interactable = true;\n});\n}\nprivate byte[] EncodeAsWAV(float[] samples, int frequency, int channels) {\nusing (var memoryStream = new MemoryStream(44 + samples.Length * 2)) {\nusing (var writer = new BinaryWriter(memoryStream)) {\nwriter.Write(\""RIFF\"".ToCharArray());\nwriter.Write(36 + samples.Length * 2);\nwriter.Write(\""WAVE\"".ToCharArray());\nwriter.Write(\""fmt \"".ToCharArray());\nwriter.Write(16);\nwriter.Write((ushort)1);\nwriter.Write((ushort)channels);\nwriter.Write(frequency);\nwriter.Write(frequency * channels * 2);\nwriter.Write((ushort)(channels * 2));\nwriter.Write((ushort)16);\nwriter.Write(\""data\"".ToCharArray());\nwriter.Write(samples.Length * 2);\nforeach (var sample in samples) {\nwriter.Write((short)(sample * short.MaxValue));\n}\n}\nreturn memoryStream.ToArray();\n}\n}\n}\n```\nCongratulations, you can now use state-of-the-art Speech Recognition in Unity! If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!"", ""## Step 3: Open your Unity Project\nOpen the Unity project you want to host in your Space. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/3.png)\n## Step 4: Switch the Build Target to WebGL\nNavigate to `File > Build Settings` and switch the Build Target to WebGL. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/4.png)\n## Step 5: Open Player Settings\nIn the Build Settings window, click the \""Player Settings\"" button to open the Player Settings panel. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/5.png)\n## Step 6: Optionally, Download the Hugging Face Unity WebGL Template\nYou can enhance your game's appearance in a Space by downloading the Hugging Face Unity WebGL template, available [here](https://github.com/huggingface/Unity-WebGL-template-for-Hugging-Face-Spaces)."", ""The Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects."", ""This tutorial guides you through the process of implementing state-of-the-art Speech Recognition in your Unity game using the Hugging Face Unity API. This tutorial assumes basic knowledge of Unity. It also requires you to have installed the [Hugging Face Unity API](https://github.com/huggingface/unity-api). For instructions on setting up the API, check out our [earlier blog post](https://huggingface.co/blog/unity-api).""]","To install the Hugging Face Unity API in your Unity project, follow these steps:

1. Open your Unity project.
2. Go to `Window` -> `Package Manager`.
3. Click `+` and select `Add Package from git URL`.
4. Enter `https://github.com/huggingface/unity-api.git`.
5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.
6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).
7. Test the API key by clicking `Test API key` in the API Wizard.
8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.","To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`."
"What is the pretraining objective of the Wav2Vec2 context network?
","[""The Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. The abstract from the paper is the following:\n\\*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned.\\*"", ""Now, we can load the pretrained `Wav2Vec2` checkpoint. The tokenizer\\'s `pad\\_token\\_id` must be to define the model\\'s `pad\\_token\\_id` or in the case of `Wav2Vec2ForCTC` also CTC\\'s \\*blank token\\* \\\\({}^2\\\\). To save GPU memory, we enable PyTorch\\'s [gradient checkpointing](https://pytorch.org/docs/stable/checkpoint.html) and also set the loss reduction to \\\""\\*mean\\*\\\"". ```python\nfrom transformers import Wav2Vec2ForCTC\nmodel = Wav2Vec2ForCTC.from\\_pretrained(\n\""facebook/wav2vec2-base\"",\nctc\\_loss\\_reduction=\""mean\"",\npad\\_token\\_id=processor.tokenizer.pad\\_token\\_id,\n)\n```"", ""### Pretraining Wav2Vec2\nThe `run\\_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss objective (see official [paper](https://arxiv.org/abs/2006.11477) for more information). It is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed (please refer to [this guide](https://huggingface.co/transformers/main/main\\_classes/deepspeed.html#deepspeed-trainer-integration) for more information). Here is an example of how you can use DeepSpeed ZeRO-2 to pretrain a small Wav2Vec2 model:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 4 run\\_pretrain.py \\\n--output\\_dir=\""./wav2vec2-base-libri-100h\"" \\\n--num\\_train\\_epochs=\""3\"" \\\n--per\\_device\\_train\\_batch\\_size=\""32\"" \\\n--per\\_device\\_eval\\_batch\\_size=\""32\"" \\\n--gradient\\_accumulation\\_steps=\""2\"" \\\n--save\\_total\\_limit=\""3\"" \\\n--save\\_steps=\""500\"" \\\n--logging\\_steps=\""10\"" \\\n--learning\\_rate=\""5e-4\"" \\\n--weight\\_decay=\""0.01\"" \\\n--warmup\\_steps=\""3000\"" \\\n--model\\_name\\_or\\_path=\""patrickvonplaten/wav2vec2-base-libri-100h\"" \\\n--dataset\\_name=\""librispeech\\_asr\"" \\\n--dataset\\_config\\_name=\""clean\"" \\\n--train\\_split\\_name=\""train.100\"" \\\n--preprocessing\\_num\\_workers=\""4\"" \\\n--max\\_duration\\_in\\_seconds=\""10.0\"" \\\n--group\\_by\\_length \\\n--verbose\\_logging \\\n--fp16 \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero2.json \\\n```"", ""To use the pretrained model for automatic speech recognition, add a language modeling head on top of the base Wav2Vec2 model for [connectionist temporal classification (CTC)](glossary#connectionist-temporal-classification-ctc). The language modeling head is a linear layer that accepts the encoder's hidden states and transforms them into logits. Each logit represents a token class (the number of tokens comes from the task vocabulary). The CTC loss is calculated between the logits and targets to find the most likely sequence of tokens, which are then decoded into a transcription."", ""Wav2Vec2 was pretrained on the audio data of [LibriSpeech](https://huggingface.co/datasets/librispeech_asr) and LibriVox which both were sampling with 16kHz. A Wav2Vec2 feature extractor object requires the following parameters to be instantiated: - `feature_size`: Speech models take a sequence of feature vectors as an input. While the length of this sequence obviously varies, the feature size should not. In the case of Wav2Vec2, the feature size is 1 because the model was trained on the raw speech signal \\\\({}^2\\\\) . - `sampling_rate`: The sampling rate at which the model is trained on. - `padding_value`: For batched inference, shorter inputs need to be padded with a specific value - `do_normalize`: Whether the input should be *zero-mean-unit-variance* normalized or not. Usually, speech models perform better when normalizing the input - `return_attention_mask`: Whether the model should make use of an `attention_mask` for batched inference. In general, models should **always** make use of the `attention_mask` to mask padded tokens. However, due to a very specific design choice of `Wav2Vec2`'s \""base\"" checkpoint, better results are achieved when using no `attention_mask`."", ""A fine-tuned Wav2Vec2 checkpoint needs to map this sequence of context representations to its corresponding transcription so that a linear layer has to be added on top of the transformer block (shown in yellow). This linear layer is used to classifies each context representation to a token class analogous how, \\*e.g.\\*, after pretraining a linear layer is added on top of BERT\\'s embeddings for further classification - \\*cf.\\* with \\*\\\""BERT\\\""\\* section of this [blog post](https://huggingface.co/blog/warm-starting-encoder-decoder). The output size of this layer corresponds to the number of tokens in the vocabulary, which does \\*\\*not\\*\\* depend on Wav2Vec2\\'s pretraining task, but only on the labeled dataset used for fine-tuning."", ""The script [`run\\_speech\\_wav2vec2\\_pretraining\\_no\\_trainer.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run\\_wav2vec2\\_pretraining\\_no\\_trainer.py) can be used to pre-train a [Wav2Vec2](https://huggingface.co/transformers/model\\_doc/wav2vec2.html?highlight=wav2vec2) model from scratch. In the script [`run\\_speech\\_wav2vec2\\_pretraining\\_no\\_trainer`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run\\_wav2vec2\\_pretraining\\_no\\_trainer.py), a Wav2Vec2 model is pre-trained on audio data alone using [Wav2Vec2's contrastive loss objective](https://arxiv.org/abs/2006.11477)."", ""**Wav2Vec2** is a popular pre-trained model for speech recognition. Released in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) by Meta AI Research, the novel architecture catalyzed progress in self-supervised pretraining for speech recognition. Using Connectionist Temporal Classification (CTC), pre-trained Wav2Vec2-like checkpoints are extremely easy to fine-tune on downstream speech recognition tasks. In a nutshell, fine-tuning pre-trained Wav2Vec2 checkpoints works as follows: A single randomly initialized linear layer is stacked on top of the pre-trained checkpoint and trained to classify raw audio input to a sequence of letters."", ""Wav2Vec2's architecture is based on transformer layers, thus giving each processed audio representation context from all other audio representations. In addition, Wav2Vec2 leverages the [CTC algorithm](https://distill.pub/2017/ctc/) for fine-tuning, which solves the problem of alignment between a varying \""input audio length\""-to-\""output text length\"" ratio. Having contextualized audio classifications and no alignment problems, Wav2Vec2 does not require an external language model or dictionary to yield acceptable audio transcriptions."", ""\""Wav2Vec2 is a self-supervised model pretrained on unlabeled speech data and finetuned on labeled data for audio classification and automatic speech recognition. [...] 4. The pretraining objective of the context network is a *contrastive task*. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label).\""""]","The pretraining objective of the Wav2Vec2 context network is a *contrastive task*. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label).",The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.
"What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?
","[""Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english."", ""```python\n>>> from transformers import pipeline\n# –í—ã–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('–ú—ã –æ—á–µ–Ω—å —Ä–∞–¥—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –∫–æ–Ω–≤–µ–π–µ—Ä –≤ transformers.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```"", ""```python\nfrom transformers import pipeline\n# Allocate a pipeline for sentiment-analysis\nclassifier = pipeline('sentiment-analysis')\nclassifier('We are very happy to include pipeline into the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9978193640708923}]\n```"", ""The most basic object in the ü§ó Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\""sentiment-analysis\"")\nclassifier(\""I've been waiting for a HuggingFace course my whole life.\"")\n```\n```python out\n[{'label': 'POSITIVE', 'score': 0.9598047137260437}]\n```\nBy default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English."", ""The model used is at the core of a pipeline, but the pipeline also include all the necessary pre-processing (since the model does not expect texts, but numbers) as well as some post-processing to make the output of the model human-readable. Let's look at a first example with the sentiment analysis pipeline. This pipeline performs text classification on a given input, and determines if it's positive or negative. Here, it attributed the positive label on the given text, with a confidence of 95%. You can pass multiple texts to the same pipeline, which will be processed and passed through the model together, as a batch. The output is a list of individual results, in the same order as the input texts. Here we find the same label and score for the first text, and the second text is judged positive with a confidence of 99.99%."", ""```python\n>>> from transformers import pipeline\n# Allocate a pipeline for sentiment-analysis\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('We are very happy to introduce pipeline to the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```"", ""ü§ó Transformers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç—ã—Å—è—á–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–µ–∫—Å—Ç, –∑—Ä–µ–Ω–∏–µ –∏ –∞—É–¥–∏–æ."", ""Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english.""]",The default checkpoint used by the sentiment analysis pipeline in the Transformers library is `distilbert base uncased finetuned sst2 english`.,distilbert base uncased finetuned sst2 english
"What is the purpose of the notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi""?
","[""The Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. **For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB**. Smaller checkpoints present interesting results too. **Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!** It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device."", ""# Habana Gaudi\nü§ó Diffusers is compatible with Habana Gaudi through ü§ó [Optimum](https://huggingface.co/docs/optimum/habana/usage_guides/stable_diffusion). Follow the [installation](https://docs.habana.ai/en/latest/Installation_Guide/index.html) guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:\n```bash\npython -m pip install --upgrade-strategy eager optimum[habana]\n```\nTo generate images with Stable Diffusion 1 and 2 on Gaudi, you need to instantiate two instances:\n- [`~optimum.habana.diffusers.GaudiStableDiffusionPipeline`], a pipeline for text-to-image generation. - [`~optimum.habana.diffusers.GaudiDDIMScheduler`], a Gaudi-optimized scheduler. When you initialize the pipeline, you have to specify `use_habana=True` to deploy it on HPUs and to get the fastest possible generation, you should enable **HPU graphs** with `use_hpu_graphs=True`. Finally, specify a [`~optimum.habana.GaudiConfig`] which can be downloaded from the [Habana](https://huggingface.co/Habana) organization on the Hub. ```python\nfrom optimum.habana import GaudiConfig\nfrom optimum.habana.diffusers import GaudiDDIMScheduler, GaudiStableDiffusionPipeline\nmodel_name = \""stabilityai/stable-diffusion-2-base\""\nscheduler = GaudiDDIMScheduler.from_pretrained(model_name, subfolder=\""scheduler\"")\npipeline = GaudiStableDiffusionPipeline.from_pretrained(\nmodel_name,\nscheduler=scheduler,\nuse_habana=True,\nuse_hpu_graphs=True,\ngaudi_config=\""Habana/stable-diffusion-2\"",\n)\n```\nNow you can call the pipeline to generate images by batches from one or several prompts:\n```python\noutputs = pipeline(\nprompt=[\n\""High quality photo of an astronaut riding a horse in space\"",\n\""Face of a yellow cat, high resolution, sitting on a park bench\"",\n],\nnum_images_per_prompt=10,\nbatch_size=4,\n)\n```\nFor more information, check out ü§ó Optimum Habana's [documentation](https://huggingface.co/docs/optimum/habana/usage_guides/stable_diffusion) and the [example](https://github.com/huggingface/optimum-habana/tree/main/examples/stable-diffusion) provided in the official GitHub repository."", ""To reproduce this benchmark, you first need to get access to Gaudi2 through the [Intel Developer Cloud](https://www.intel.com/content/www/us/en/secure/developer/devcloud/cloud-launchpad.html) (see [this guide](https://huggingface.co/blog/habana-gaudi-2-benchmark#how-to-get-access-to-gaudi2) for more information). Then, you need to install the latest version of Optimum Habana and run `run_bridgetower.py` which you can find [here](https://github.com/huggingface/optimum-habana/blob/main/examples/contrastive-image-text/run_bridgetower.py). Here is how to do it:\n```bash\npip install optimum[habana]\ngit clone https://github.com/huggingface/optimum-habana.git\ncd optimum-habana/examples/contrastive-image-text\npip install -r requirements.txt\n```\nThe base command line to run the script is:\n```bash\npython ../gaudi_spawn.py --use_mpi --world_size 8 run_bridgetower.py \\\n--output_dir /tmp/bridgetower-test \\\n--model_name_or_path BridgeTower/bridgetower-large-itm-mlm-itc \\\n--dataset_name jmhessel/newyorker_caption_contest --dataset_config_name matching \\\n--dataset_revision 3c6c4f6c0ff7e902833d3afa5f8f3875c2b036e6 \\\n--image_column image --caption_column image_description \\\n--remove_unused_columns=False \\\n--do_train --do_eval --do_predict \\\n--per_device_train_batch_size=\""40\"" --per_device_eval_batch_size=\""16\"" \\\n--num_train_epochs 5 \\\n--learning_rate=\""1e-5\"" \\\n--push_to_hub --report_to tensorboard --hub_model_id bridgetower\\\n--overwrite_output_dir \\\n--use_habana --use_lazy_mode --use_hpu_graphs_for_inference --gaudi_config_name Habana/clip \\\n--throughput_warmup_steps 3 \\\n--logging_steps 10\n```\nwhich corresponds to the case `--dataloader_num_workers 0`. You can then add `--dataloader_num_workers N` and `--mediapipe_dataloader` to test other configurations. To push your model and Tensorboard logs to the Hugging Face Hub, you will have to log in to your account beforehand with:\n```bash\nhuggingface-cli login\n```"", ""And this is super easy to use as you just need to provide a few additional training arguments. To go further, we are looking forward to using HPU graphs for training models even faster and to presenting how to use DeepSpeed ZeRO-3 on Gaudi2 to accelerate the training of your LLMs."", ""On this instance, the last setup step is to pull the Habana container for PyTorch, which is the framework I'll use to fine-tune my model. You can find information on other prebuilt containers and on how to build your own in the Habana [documentation](https://docs.habana.ai/en/latest/Installation_Guide/index.html). ```\ndocker pull \\\nvault.habana.ai/gaudi-docker/1.5.0/ubuntu20.04/habanalabs/pytorch-installer-1.11.0:1.5.0-610\n```\nOnce the image has been pulled to the instance, I run it in interactive mode. ```\ndocker run -it \\\n--runtime=habana \\\n-e HABANA_VISIBLE_DEVICES=all \\\n-e OMPI_MCA_btl_vader_single_copy_mechanism=none \\\n--cap-add=sys_nice \\\n--net=host \\\n--ipc=host vault.habana.ai/gaudi-docker/1.5.0/ubuntu20.04/habanalabs/pytorch-installer-1.11.0:1.5.0-610\n```\nI'm now ready to fine-tune my model. ## Fine-tuning a text classification model on Habana Gaudi\nI first clone the [Optimum Habana](https://github.com/huggingface/optimum-habana) repository inside the container I've just started. ```\ngit clone https://github.com/huggingface/optimum-habana.git\n```\nThen, I install the Optimum Habana package from source. ```\ncd optimum-habana\npip install . ```\nThen, I move to the subdirectory containing the text classification example and install the required Python packages. ```\ncd examples/text-classification\npip install -r requirements.txt\n```\nI can now launch the training job, which downloads the [bert-large-uncased-whole-word-masking](https://huggingface.co/bert-large-uncased-whole-word-masking) model from the Hugging Face hub, and fine-tunes it on the [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) task of the [GLUE](https://gluebenchmark.com/) benchmark."", ""We are looking forward on to do the experiment again once the [Gaudi DeepSpeed integration](https://docs.habana.ai/en/latest/PyTorch/DeepSpeed/DeepSpeed_User_Guide.html#deepspeed-configs) is more widely available."", ""Although these devices have quite a lot of memory, the model is so large that a single device is not enough to contain a single instance of BLOOMZ. To solve this issue, we are going to use [DeepSpeed](https://www.deepspeed.ai/), which is a deep learning optimization library that enables many memory and speed improvements to accelerate the model and make it fit the device. In particular, we rely here on [DeepSpeed-inference](https://arxiv.org/abs/2207.00032): it introduces several features such as [model (or pipeline) parallelism](https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism) to make the most of the available devices. For Gaudi2, we use [Habana's DeepSpeed fork](https://github.com/HabanaAI/deepspeed) that adds support for HPUs."", ""With 96 GB of memory per device, Gaudi2 enables running much bigger models. For instance, we managed to fine-tune T5-3B (containing 3 billion parameters) with gradient checkpointing being the only applied memory optimization. This is not possible on first-gen Gaudi. We are looking forward to expanding this benchmark using newer versions of Habana's SDK and also using [DeepSpeed](https://www.deepspeed.ai/) to see if the same trend holds."", ""## CPU/Disk Offloading to enable training humongous models that won‚Äôt fit the GPU memory\nOn a single 24GB NVIDIA Titan RTX GPU, one cannot train GPT-XL Model (1.5B parameters) even with a batch size of 1. We will look at how we can use DeepSpeed ZeRO Stage-3 with CPU offloading of optimizer states, gradients and parameters to train GPT-XL Model. We will leverage the DeepSpeed Zero Stage-3 CPU offload config [zero3\\_offload\\_config\\_accelerate.json](https://github.com/pacman100/accelerate-deepspeed-test/blob/main/src/modeling/configs/zero3\\_offload\\_config\\_accelerate.json) (given below) for training. The rest of the process of using the config with ü§ó `accelerate` is similar to the above experiment. ```json\n{\n\""fp16\"": {\n\""enabled\"": true,\n\""loss\\_scale\"": 0,\n\""loss\\_scale\\_window\"": 1000,\n\""initial\\_scale\\_power\"": 16,\n\""hysteresis\"": 2,\n\""min\\_loss\\_scale\"": 1\n},\n\""optimizer\"": {\n\""type\"": \""AdamW\"",\n\""params\"": {\n\""lr\"": \""auto\"",\n\""weight\\_decay\"": \""auto\""\n}\n},\n\""scheduler\"": {\n\""type\"": \""WarmupDecayLR\"",\n\""params\"": {\n\""warmup\\_min\\_lr\"": \""auto\"",\n\""warmup\\_max\\_lr\"": \""auto\"",\n\""warmup\\_num\\_steps\"": \""auto\"",\n\""total\\_num\\_steps\"": \""auto\""\n}\n},\n\""zero\\_optimization\"": {\n\""stage\"": 3,\n\""offload\\_optimizer\"": {\n\""device\"": \""cpu\"",\n\""pin\\_memory\"": true\n},\n\""offload\\_param\"": {\n\""device\"": \""cpu\"",\n\""pin\\_memory\"": true\n},\n\""overlap\\_comm\"": true,\n\""contiguous\\_gradients\"": true,\n\""reduce\\_bucket\\_size\"": \""auto\"",\n\""stage3\\_prefetch\\_bucket\\_size\"": \""auto\"",\n\""stage3\\_param\\_persistence\\_threshold\"": \""auto\"",\n\""sub\\_group\\_size\"": 1e9,\n\""stage3\\_max\\_live\\_parameters\"": 1e9,\n\""stage3\\_max\\_reuse\\_distance\"": 1e9,\n\""stage3\\_gather\\_16bit\\_weights\\_on\\_model\\_save\"": true\n},\n\""gradient\\_accumulation\\_steps\"": 1,\n\""gradient\\_clipping\"": \""auto\"",\n\""steps\\_per\\_print\"": 2000,\n\""train\\_batch\\_size\"": \""auto\"",\n\""train\\_micro\\_batch\\_size\\_per\\_gpu\"": \""auto\"",\n\""wall\\_clock\\_breakdown\"": false\n}\n```\n\\*\\*ZeRO Stage-3 CPU Offload DeepSpeed Config File Example\\*\\*\n```bash\ncompute\\_environment: LOCAL\\_MACHINE\ndeepspeed\\_config:\ndeepspeed\\_config\\_file: /path/to/zero3\\_offload\\_config\\_accelerate.json\nzero3\\_init\\_flag: true\ndistributed\\_type: DEEPSPEED\nfsdp\\_config: {}\nmachine\\_rank: 0\nmain\\_process\\_ip: null\nmain\\_process\\_port: null\nmain\\_training\\_function: main\nmixed\\_precision: fp16\nnum\\_machines: 1\nnum\\_processes: 2\nuse\\_cpu: false\n```\nNow, run below command for training:\n```bash\naccelerate launch run\\_clm\\_no\\_trainer.py \\\n--config\\_name \""gpt2-xl\"" \\\n--tokenizer\\_name \""gpt2-xl\"" \\\n--dataset\\_name \""wikitext\"" \\\n--dataset\\_config\\_name \""wikitext-2-raw-v1\"" \\\n--block\\_size 128 \\\n--output\\_dir \""/tmp/clm\\_deepspeed\\_stage3\\_offload\\_\\_accelerate\"" \\\n--learning\\_rate 5e-4 \\\n--per\\_device\\_train\\_batch\\_size 16 \\\n--per\\_device\\_eval\\_batch\\_size 1 \\\n--num\\_train\\_epochs 1 \\\n--with\\_tracking \\\n--report\\_to \""wandb\""\\\n```"", ""Habana's SDK, SynapseAI‚Ñ¢, supports PyTorch and DeepSpeed for accelerating LLM training and inference. The [SynapseAI graph compiler](https://docs.habana.ai/en/latest/Gaudi_Overview/SynapseAI_Software_Suite.html#graph-compiler-and-runtime) will optimize the execution of the operations accumulated in the graph (e.g. operator fusion, data layout management, parallelization, pipelining and memory management, and graph-level optimizations). Moreover, support for [HPU graphs](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inference_Using_HPU_Graphs.html) and [DeepSpeed-inference](https://docs.habana.ai/en/latest/PyTorch/DeepSpeed/Inference_Using_DeepSpeed.html) have just recently been introduced in SynapseAI, and these are well-suited for latency-sensitive applications as shown in our benchmark below. All these features are integrated into the ü§ó [Optimum Habana](https://github.com/huggingface/optimum-habana) library so that deploying your model on Gaudi is very simple."", ""- [Leverage DeepSpeed to Train Faster and Cheaper Large Scale Transformer Models with Hugging Face and Habana Labs Gaudi](https://developer.habana.ai/events/leverage-deepspeed-to-train-faster-and-cheaper-large-scale-transformer-models-with-hugging-face-and-habana-labs-gaudi/)""]","The notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi"" is designed to demonstrate how to leverage DeepSpeed in conjunction with Habana Gaudi to efficiently train large-scale transformer models. It showcases the integration of DeepSpeed's optimization capabilities, such as memory and speed improvements, to enable the training of models that may not fit into the memory of a single device. The notebook provides practical examples and configurations to help users effectively utilize the Habana Gaudi architecture for training large models, making the process more accessible and efficient.",To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.
"What command line module does PyTorch provide to run a script on multiple GPUs?
","[""```\nThis is where `torch.distributed` comes into play\n## PyTorch Distributed Data Parallelism\nAs the name implies, `torch.distributed` is meant to work on *distributed* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both. \n```python\nfrom torch.nn.parallel import DistributedDataParallel as DDP\ndef train(model, rank, world_size):\nsetup(rank, world_size)\nmodel = model.to(rank)\nddp_model = DDP(model, device_ids=[rank])\noptimizer = optim.AdamW(ddp_model.parameters(), lr=1e-3)\n# Train for one epoch\nmodel.train()\nfor batch_idx, (data, target) in enumerate(train_loader):\ndata, target = data.to(device), target.to(device)\noutput = model(data)\nloss = F.nll_loss(output, target)\nloss.backward()\noptimizer.step()\noptimizer.zero_grad()\ncleanup()\n```\nLastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\n```bash\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\n```\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\n```"", ""```\nThe following command shows how to fine-tune [XLSR-Wav2Vec2](https://huggingface.co/transformers/main/model_doc/xlsr_wav2vec2.html) on [Common Voice](https://huggingface.co/datasets/common_voice) using 8 GPUs in half-precision. ```bash\ntorchrun \\\n--nproc_per_node 8 run_speech_recognition_ctc.py \\\n--dataset_name=\""common_voice\"" \\\n--model_name_or_path=\""facebook/wav2vec2-large-xlsr-53\"" \\\n--dataset_config_name=\""tr\"" \\\n--output_dir=\""./wav2vec2-common_voice-tr-demo-dist\"" \\\n--overwrite_output_dir \\\n--num_train_epochs=\""15\"" \\\n--per_device_train_batch_size=\""4\"" \\\n--learning_rate=\""3e-4\"" \\\n--warmup_steps=\""500\"" \\\n--evaluation_strategy=\""steps\"" \\\n--text_column_name=\""sentence\"" \\\n--length_column_name=\""input_length\"" \\\n--save_steps=\""400\"" \\\n--eval_steps=\""100\"" \\\n--logging_steps=\""1\"" \\\n--layerdrop=\""0.0\"" \\\n--save_total_limit=\""3\"" \\\n--freeze_feature_encoder \\\n--gradient_checkpointing \\\n--chars_to_ignore , ?\n```"", ""To train an SE-ResNet34 on ImageNet, locally distributed, 4 GPUs, one process per GPU w/ cosine schedule, random-erasing prob of 50% and per-pixel random value:\n```bash\n./distributed\\_train.sh 4 /data/imagenet --model seresnet34 --sched cosine --epochs 150 --warmup-epochs 5 --lr 0.4 --reprob 0.5 --remode pixel --batch-size 256 --amp -j 4\n```"", ""```bash\ntorch.distributed.run --nproc_per_node=2 your_program.py  --deepspeed ds_config.json\n```\n```bash\ndeepspeed --num_gpus=2 your_program.py  --deepspeed ds_config.json\n```\n```bash\ndeepspeed examples/pytorch/translation/run_translation.py \\\n--deepspeed tests/deepspeed/ds_config_zero3.json \\\n--model_name_or_path t5-small --per_device_train_batch_size 1 \\\n--output_dir output_dir --overwrite_output_dir --fp16 \\\n--do_train --max_train_samples 500 --num_train_epochs 1 \\\n--dataset_name wmt16 --dataset_config \""ro-en\"" \\\n--source_lang en --target_lang ro\n```\n```bash\ndeepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n--deepspeed tests/deepspeed/ds_config_zero2.json \\\n--model_name_or_path t5-small --per_device_train_batch_size 1 \\\n--output_dir output_dir --overwrite_output_dir --fp16 \\\n--do_train --max_train_samples 500 --num_train_epochs 1 \\\n--dataset_name wmt16 --dataset_config \""ro-en\"" \\\n--source_lang en --target_lang ro\n```"", ""## Multi-GPU Network Issues Debug\nWhen training or inferencing with `DistributedDataParallel` and multiple GPU, if you run into issue of inter-communication between processes and/or nodes, you can use the following script to diagnose network issues. ```bash\nwget https://raw.githubusercontent.com/huggingface/transformers/main/scripts/distributed/torch-distributed-gpu-test.py\n```\nFor example to test how 2 GPUs interact do:\n```bash\npython -m torch.distributed.run --nproc\\_per\\_node 2 --nnodes 1 torch-distributed-gpu-test.py\n```"", ""```bash\ntorchrun --nproc_per_node=8 run_distributed_eval.py \\\n--model_name sshleifer/distilbart-large-xsum-12-3 \\\n--save_dir xsum_generations \\\n--data_dir xsum \\\n--fp16 # you can pass generate kwargs like num_beams here, just like run_eval.py\n```"", ""```bash\naccelerate launch run_ner_no_trainer.py \\\n--model_name_or_path bert-base-cased \\\n--dataset_name conll2003 \\\n--task_name $TASK_NAME \\\n--max_length 128 \\\n--per_device_train_batch_size 32 \\\n--learning_rate 2e-5 \\\n--num_train_epochs 3 \\\n--output_dir /tmp/$TASK_NAME/\n```\nThis command is the same and will work for:\n- a CPU-only setup\n- a setup with one GPU\n- a distributed training with several GPUs (single or multi node)\n- a training on TPUs"", ""We use either the `transformers.Trainer` or `accelerate`, which both support data parallelism without any code changes, by simply passing arguments when calling the scripts with `torchrun` or `accelerate launch`. The following runs a training script with 8 GPUs on a single machine with `accelerate` and `torchrun`, respectively. ```bash\naccelerate launch --multi\\_gpu --num\\_machines 1 --num\\_processes 8 my\\_accelerate\\_script.py\ntorchrun --nnodes 1 --nproc\\_per\\_node 8 my\\_torch\\_script.py\n```"", ""When training on multiple GPUs, you can specify the number of GPUs to use and in what order. This can be useful for instance when you have GPUs with different computing power and want to use the faster GPU first. The selection process works for both [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) and [DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html) to use only a subset of the available GPUs, and you don't need Accelerate or the [DeepSpeed integration](./main_classes/deepspeed). ### Number of GPUs For example, if you have 4 GPUs and you only want to use the first 2:\n\nUse the `--nproc_per_node` to select how many GPUs to use. ```bash\ntorchrun --nproc_per_node=2 trainer-program.py ... ```\n\nUse `--num_processes` to select how many GPUs to use. ```bash\naccelerate launch --num_processes 2 trainer-program.py ... ```\n\nUse `--num_gpus` to select how many GPUs to use. ```bash\ndeepspeed --num_gpus 2 trainer-program.py ... ```\n### Order of GPUs Now, to select which GPUs to use and their order, you'll use the `CUDA_VISIBLE_DEVICES` environment variable. It is easiest to set the environment variable in a `~/bashrc` or another startup config file. `CUDA_VISIBLE_DEVICES` is used to map which GPUs are used. For example, if you have 4 GPUs (0, 1, 2, 3) and you only want to run GPUs 0 and 2: ```bash\nCUDA_VISIBLE_DEVICES=0,2 torchrun trainer-program.py ... ``` Only the 2 physical GPUs (0 and 2) are \""visible\"" to PyTorch and these are mapped to `cuda:0` and `cuda:1` respectively. You can also reverse the order of the GPUs to use 2 first. Now, the mapping is `cuda:1` for GPU 0 and `cuda:0` for GPU 2. ```bash\nCUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ... ``` You can also set the `CUDA_VISIBLE_DEVICES` environment variable to an empty value to create an environment without GPUs. ```bash\nCUDA_VISIBLE_DEVICES= python trainer-program.py ... ``` As with any environment variable, they can be exported instead of being added to the command line."", ""```bash\ntorchrun \\\n--nproc\\_per\\_node 8 pytorch/summarization/run\\_summarization.py \\\n--fp16 \\\n--model\\_name\\_or\\_path t5-small \\\n--do\\_train \\\n--do\\_eval \\\n--dataset\\_name cnn\\_dailymail \\\n--dataset\\_config \""3.0.0\"" \\\n--source\\_prefix \""summarize: \"" \\\n--output\\_dir /tmp/tst-summarization \\\n--per\\_device\\_train\\_batch\\_size=4 \\\n--per\\_device\\_eval\\_batch\\_size=4 \\\n--overwrite\\_output\\_dir \\\n--predict\\_with\\_generate\n```"", ""### Multi-GPU and TPU usage\nBy default, the script uses a `MirroredStrategy` and will use multiple GPUs effectively if they are available. TPUs can also be used by passing the name of the TPU resource with the `--tpu` argument."", ""```bash\n**To lanuch fine-tuninig on multiple GPUs:**\n```bash\npython -m torch.distributed.launch \\\n--nproc\\_per\\_node 4 run\\_common\\_voice.py \\\n--model\\_name\\_or\\_path=\""facebook/wav2vec2-large-xlsr-53\"" \\\n--dataset\\_config\\_name=\""tr\"" \\ # use this argument to specify the language code\n--output\\_dir=./wav2vec2-large-xlsr-turkish-demo \\\n--overwrite\\_output\\_dir \\\n--num\\_train\\_epochs=\""5\"" \\\n--per\\_device\\_train\\_batch\\_size=\""16\"" \\\n--learning\\_rate=\""3e-4\"" \\\n--warmup\\_steps=\""500\"" \\\n--evaluation\\_strategy=\""steps\"" \\\n--save\\_steps=\""400\"" \\\n--eval\\_steps=\""400\"" \\\n--logging\\_steps=\""400\"" \\\n--save\\_total\\_limit=\""3\"" \\\n--freeze\\_feature\\_extractor \\\n--feat\\_proj\\_dropout=\""0.0\"" \\\n--layerdrop=\""0.1\"" \\\n--gradient\\_checkpointing \\\n--fp16 \\\n--group\\_by\\_length \\\n--do\\_train --do\\_eval\n```\nThe above command will launch the training on 4 GPUs.\n```"", ""# Efficient Training on Multiple GPUs\nIf training a model on a single GPU is too slow or if the model's weights do not fit in a single GPU's memory, transitioning to a multi-GPU setup may be a viable option. ... Transitioning from a single GPU to multiple GPUs requires the introduction of some form of parallelism, as the workload must be distributed across the resources. Multiple techniques can be employed to achieve parallelism, such as data parallelism, tensor parallelism, and pipeline parallelism. ... For step-by-step tutorials on distributed training, please refer to the [ü§ó Accelerate documentation](https://huggingface.co/docs/accelerate/index). While the main concepts discussed in this guide are likely applicable across frameworks, here we focus on PyTorch-based implementations. ... \\*\\*Parallelization strategy for a single Node / multi-GPU setup\\*\\* When training a model on a single node with multiple GPUs, your choice of parallelization strategy can significantly impact performance. ... \\*\\*Case 1: Your model fits onto a single GPU\\*\\* If your model can comfortably fit onto a single GPU, you have two primary options: 1. DDP - Distributed DataParallel 2. ZeRO - depending on the situation and configuration used, this method may or may not be faster, however, it's worth experimenting with it. \\*\\*Case 2: Your model doesn't fit onto a single GPU:\\*\\* If your model is too large for a single GPU, you have several alternatives to consider: 1."", ""```bash\ntorchrun run\\_distributed.py --nproc\\_per\\_node=2\n```"", ""Run your **raw** PyTorch training scripts on any kind of device. Most high-level libraries above PyTorch provide support for distributed training and mixed precision, but the abstraction they introduce require a user to learn a new API if they want to customize the underlying training loop. ü§ó Accelerate was created for PyTorch users who like to have full control over their training loops but are reluctant to write (and maintain) the boilerplate code needed to use distributed training (for multi-GPU on one or several nodes, TPUs, ...) or mixed precision training. It provides two things: a simple and consistent API that abstracts that boilerplate code and a launcher command to easily run those scripts on various setups. \n\nIn contrast, here are the changes needed to have this code run with distributed training are the followings:\n```diff\n+ import os\nimport torch\nimport torch.nn.functional as F\nfrom datasets import load_dataset\n+ from torch.utils.data import DistributedSampler\n+ from torch.nn.parallel import DistributedDataParallel\n+ local_rank = int(os.environ.get(\""LOCAL_RANK\"", -1))\n- device = 'cpu'\n+ device = device = torch.device(\""cuda\"", local_rank)\nmodel = torch.nn.Transformer().to(device)\n+ model = DistributedDataParallel(model)\noptim = torch.optim.Adam(model.parameters())\ndataset = load_dataset('my_dataset')\n+ sampler = DistributedSampler(dataset)\n- data = torch.utils.data.DataLoader(dataset, shuffle=True)\n+ data = torch.utils.data.DataLoader(dataset, sampler=sampler)\nmodel.train()\nfor epoch in range(10):\n+ sampler.set_epoch(epoch)\nfor source, targets in data:\nsource = source.to(device)\ntargets = targets.to(device)\noptimizer.zero_grad()\noutput = model(source)\nloss = F.cross_entropy(output, targets)\nloss.backward()\noptimizer.step()\n```\nThese changes will make your training script work for multiple GPUs, but your script will then stop working on CPU or one GPU (unless you start adding if statements everywhere)."", ""```bash\naccelerate launch run_glue_no_trainer.py \\\n--model_name_or_path bert-base-cased \\\n--task_name $TASK_NAME \\\n--max_length 128 \\\n--per_device_train_batch_size 32 \\\n--learning_rate 2e-5 \\\n--num_train_epochs 3 \\\n--output_dir /tmp/$TASK_NAME/\n```\nThis command is the same and will work for:\n- a CPU-only setup\n- a setup with one GPU\n- a distributed training with several GPUs (single or multi node)\n- a training on TPUs"", ""```bash\n**torchrun \\\n--nproc_per_node 4 run_speech_recognition_ctc_streaming.py \\\n--dataset_name=\""common_voice\"" \\\n--model_name_or_path=\""facebook/wav2vec2-xls-r-300m\"" \\\n--tokenizer_name_or_path=\""anton-l/wav2vec2-tokenizer-turkish\"" \\\n--dataset_config_name=\""tr\"" \\\n--train_split_name=\""train+validation\"" \\\n--eval_split_name=\""test\"" \\\n--output_dir=\""wav2vec2-xls-r-common_voice-tr-ft\"" \\\n--overwrite_output_dir \\\n--max_steps=\""5000\"" \\\n--per_device_train_batch_size=\""8\"" \\\n--gradient_accumulation_steps=\""2\"" \\\n--learning_rate=\""5e-4\"" \\\n--warmup_steps=\""500\"" \\\n--evaluation_strategy=\""steps\"" \\\n--text_column_name=\""sentence\"" \\\n--save_steps=\""500\"" \\\n--eval_steps=\""500\"" \\\n--logging_steps=\""1\"" \\\n--layerdrop=\""0.0\"" \\\n--eval_metrics wer cer \\\n--save_total_limit=\""1\"" \\\n--mask_time_prob=\""0.3\"" \\\n--mask_time_length=\""10\"" \\\n--mask_feature_prob=\""0.1\"" \\\n--mask_feature_length=\""64\"" \\\n--freeze_feature_encoder \\\n--chars_to_ignore , ?\n```""]","PyTorch provides the `torchrun` command line module to run a script on multiple GPUs. You can use it by specifying the number of processes per node and the script to run, for example:

```bash
torchrun --nproc_per_node=2 example_script.py
```",torchrun
"What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?
","[""Âú®Êí∞ÂÜôÊó∂ÔºåÊúÄÂèóÊ¨¢ËøéÁöÑÊ®°ÂûãÊòØ `google/vit-base-patch16-224`ÔºåËØ•Ê®°ÂûãÂú®ÂàÜËæ®Áéá‰∏∫ 224x224 ÂÉèÁ¥†ÁöÑ ImageNet ÂõæÂÉè‰∏äËøõË°å‰∫ÜËÆ≠ÁªÉ„ÄÇÊàë‰ª¨Â∞ÜÂú®ÊºîÁ§∫‰∏≠‰ΩøÁî®Ê≠§Ê®°Âûã„ÄÇ"", ""1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. \n1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)** (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo."", ""- BEiT models outperform supervised pre-trained vision transformers using a self-supervised method inspired by BERT (masked image modeling) and based on a VQ-VAE. \n- DINO (a method for self-supervised training of Vision Transformers) by Facebook AI. Vision Transformers trained using the DINO method show very interesting properties not seen with convolutional models. \n- [MAE](vit_mae) (Masked Autoencoders) by Facebook AI. By pre-training Vision Transformers to reconstruct pixel values for a high portion (75%) of masked patches (using an asymmetric encoder-decoder architecture), the authors show that this simple method outperforms supervised pre-training after fine-tuning."", ""# Transformer2D\nA Transformer model for image-like data from [CompVis](https://huggingface.co/CompVis) that is based on the [Vision Transformer](https://huggingface.co/papers/2010.11929) introduced by Dosovitskiy et al. The [`Transformer2DModel`] accepts discrete (classes of vector embeddings) or continuous (actual embeddings) inputs."", ""# Pyramid Vision Transformer (PVT)\n## Overview\nThe PVT model was proposed in\n[Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/abs/2102.12122)\nby Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao. The PVT is a type of\nvision transformer that utilizes a pyramid structure to make it an effective backbone for dense prediction tasks."", ""* Although the post uses a ViT-based model ([`nateraw/vit-base-beans`](https://huggingface.co/nateraw/vit-base-beans)) and a particular dataset ([Beans](https://huggingface.co/datasets/beans)), it can be extended to use other models supporting vision modality and other image datasets. Some notable models you could try:\n\\* [Swin Transformer](https://huggingface.co/docs/transformers/model\\_doc/swin)\n\\* [ConvNeXT](https://huggingface.co/docs/transformers/model\\_doc/convnext)\n\\* [RegNet](https://huggingface.co/docs/transformers/model\\_doc/regnet)"", ""```py\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset(\""huggingface/cats-image\"")\nimage = dataset[\""test\""][\""image\""][0]\nimage_processor  = ViTImageProcessor.from_pretrained(\""google/vit-base-patch16-224\"")\nmodel = ViTForImageClassification.from_pretrained(\""google/vit-base-patch16-224\"")\ninputs = image_processor(image, return_tensors=\""pt\"")\nwith torch.no_grad():\nlogits = model(**inputs).logits\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])\n# Egyptian cat\n```"", ""*We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.*"", ""# Vision Transformer (ViT)\n## Overview\nThe Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\nat Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, Neil Houlsby."", ""### Encoder[[cv-encoder]]\nThe [Vision Transformer (ViT)](model\\_doc/vit) opened the door to computer vision tasks without convolutions. ViT uses a standard Transformer encoder, but its main breakthrough was how it treated an image. It splits an image into fixed-size patches and uses them to create an embedding, just like how a sentence is split into tokens. ViT capitalized on the Transformers' efficient architecture to demonstrate competitive results with the CNNs at the time while requiring fewer resources to train. ViT was soon followed by other vision models that could also handle dense vision tasks like segmentation as well as detection. One of these models is the [Swin](model\\_doc/swin) Transformer. It builds hierarchical feature maps (like a CNN üëÄ and unlike ViT) from smaller-sized patches and merges them with neighboring patches in deeper layers. Attention is only computed within a local window, and the window is shifted between attention layers to create connections to help the model learn better. Since the Swin Transformer can produce hierarchical feature maps, it is a good candidate for dense prediction tasks like segmentation and detection. The [SegFormer](model\\_doc/segformer) also uses a Transformer encoder to build hierarchical feature maps, but it adds a simple multilayer perceptron (MLP) decoder on top to combine all the feature maps and make a prediction. Other vision models, like BeIT and ViTMAE, drew inspiration from BERT's pretraining objective. [BeIT](model\\_doc/beit) is pretrained by *masked image modeling (MIM)*; the image patches are randomly masked, and the image is also tokenized into visual tokens. BeIT is trained to predict the visual tokens corresponding to the masked patches. [ViTMAE](model\\_doc/vitmae) has a similar pretraining objective, except it must predict the pixels instead of visual tokens."", ""# Hybrid Vision Transformer (ViT Hybrid)\n## Overview\nThe hybrid Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\nat Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, Neil Houlsby. It's the first paper that successfully trains a Transformer encoder on ImageNet, attaining\nvery good results compared to familiar convolutional architectures. ViT hybrid is a slight variant of the [plain Vision Transformer](vit),\nby leveraging a convolutional backbone (specifically, [BiT](bit)) whose features are used as initial \""tokens\"" for the Transformer."", ""1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. \n1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)** (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo."", ""1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. \n1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)** (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo."", ""1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. \n1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)** (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo."", ""1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. \n1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)** (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo."", ""1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. \n1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)** (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.""]",Âú® Hugging Face Model Hub ‰∏äÔºåÊúÄÂèóÊ¨¢ËøéÁöÑËßÜËßâÂèòÊç¢Âô®Ê®°ÂûãÊòØ `google/vit-base-patch16-224`ÔºåËØ•Ê®°ÂûãÂú®ÂàÜËæ®Áéá‰∏∫ 224x224 ÂÉèÁ¥†ÁöÑ ImageNet ÂõæÂÉè‰∏äËøõË°å‰∫ÜËÆ≠ÁªÉ„ÄÇ,google/vit-base-patch16-224
"What is the command to upload an ESPnet model to a Hugging Face repository?
","[""Using ESPnet at Hugging Face  \n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks. ## Exploring ESPnet in the Hub  \nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). All models on the Hub come up with useful features:  \n1. An automatically generated model card with a description, a training configuration, licenses and more. 2. Metadata tags that help for discoverability and contain information such as license, language and datasets. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests.  \n## Using existing models  \nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo)). If you're interested in doing inference, different classes for different tasks have a `from_pretrained` method that allows loading models from the Hub."", ""In order to handle a repository, you should first have a Hugging Face account. Once you are logged in, you can create a new repository by clicking on the \""New model\"" option. The model name is the model identifier that will then be used to identify your model on your chosen namespace. The final choice is between public and private. Public models are accessible by anyone. This is the recommended, free option, as this makes your model easily accessible and shareable. The owners of your namespace are the only ones who can update and change your model. A more advanced option is the private option. In this case, only the owners of your namespace will have visibility over your model. Other users won't know it exists and will not be able to use it. Once your model is created, comes the management of that model! Three tabs are available to you. You're facing the first one, which is the model card page; this is the page used to showcase your model to the world."", ""```python\nmodel.push\\_to\\_hub(\nf\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\""/\"", \""\\_\""),\ntoken = \""hf\\_...\""\n)\n```\ntoken (`bool` or `str`, \\*optional\\*):\n`token` is to be used for HTTP Bearer authorization when accessing remote files. If `True`, will use the token generated\nwhen running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo\\_url`\nis not specified. Or you can get your token from https://huggingface.co/settings/token"", ""4. As soon as your Space is built, Hugging Face will detect that it is associated with the model. A \""Linked Models\"" button should appear in the top right corner of the Space, as shown here:\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/linked-models.png)\n\\*Note\\*: You can also add linked models manually by explicitly updating them in the [README metadata for the Space, as described here](https://huggingface.co/docs/hub/spaces-config-reference). Your Space should appear in the Demo tab next to the paper on ArXiv in a few minutes ü§ó"", ""```python\nAfter we have our unzipped model and model card located in `my_bart_model` we can use the either `huggingface_hub` SDK to create a repository and upload it to [huggingface.co](https://huggingface.co) ‚Äì or just to https://huggingface.co/new an create a new repository and upload it. ```python\nfrom getpass import getpass\nfrom huggingface_hub import HfApi, Repository\nhf_username = \""philschmid\"" # your username on huggingface.co\nhf_email = \""philipp@huggingface.co\"" # email used for commit\nrepository_name = f\""{hyperparameters['model_name_or_path'].split('/')[1]}-{hyperparameters['dataset_name']}\"" # repository name on huggingface.co\npassword = getpass(\""Enter your password:\"") # creates a prompt for entering password\n# get hf token\ntoken = HfApi().login(username=hf_username, password=password)\n# create repository\nrepo_url = HfApi().create_repo(token=token, name=repository_name, exist_ok=True)\n# create a Repository instance\nmodel_repo = Repository(use_auth_token=token,\nclone_from=repo_url,\nlocal_dir=local_path,\ngit_user=hf_username,\ngit_email=hf_email)\n# push model to the hub\nmodel_repo.push_to_hub()\n``` \n```python\nprint(f\""https://huggingface.co/{hf_username}/{repository_name}\"")\n``` \n```python\n![inference](assets/19_sagemaker_distributed_training_seq2seq/inference-test.png)\n```"", ""In the steps below, we'll take a look at the easiest ways to share pretrained models to the ü§ó Hub. There are tools and utilities available that make it simple to share and update models directly on the Hub, which we will explore below. We encourage all users that train models to contribute by sharing them with the community ‚Äî sharing models, even when trained on very specific datasets, will help others, saving them time and compute resources and providing access to useful trained artifacts. In turn, you can benefit from the work that others have done! There are three ways to go about creating new model repositories:\n- Using the `push\\_to\\_hub` API\n- Using the `huggingface\\_hub` Python library\n- Using the web interface\nOnce you've created a repository, you can upload files to it via git and git-lfs. We'll walk you through creating model repositories and uploading files to them in the following sections. ## Using the `push\\_to\\_hub` API[[using-the-pushtohub-api]]\nThe simplest way to upload files to the Hub is by leveraging the `push\\_to\\_hub` API."", ""## Upload files to the Hub\nYou might also want to provide a method for creating model repositories and uploading files to the Hub directly from your library. The `huggingface\\_hub` library offers two ways to assist you with creating repositories and uploading files:\n- `create\\_repo` creates a repository on the Hub. - `upload\\_file` directly uploads files to a repository on the Hub. ### `create\\_repo`\nThe `create\\_repo` method creates a repository on the Hub. Use the `name` parameter to provide a name for your repository:\n```python\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(repo\\_id=\""test-model\"")\n'https://huggingface.co/lysandre/test-model'\n```\nWhen you check your Hugging Face account, you should now see a `test-model` repository under your namespace."", ""The file name here should be `README.md` since it is what Hugging Face Hub expects. ```python\nmodel\\_card.save(Path(local\\_repo) / \""README.md\"")\n```\nWe can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub\\_utils`. Hugging Face Hub requires tokens for authentication, therefore you need to pass your token in either `notebook\\_login` if you're logging in from a notebook, or `huggingface-cli login` if you're logging in from the CLI. ```python\n# if the repository doesn't exist remotely on the Hugging Face Hub, it will be created when we set create\\_remote to True\nrepo\\_id = \""skops-user/my-awesome-model\""\nhub\\_utils.push(\nrepo\\_id=repo\\_id,\nsource=local\\_repo,\ntoken=token,\ncommit\\_message=\""pushing files to the repo from the example!\"",\ncreate\\_remote=True,\n)\n```\nOnce we push the model to the Hub, anyone can use it unless the repository is private."", ""To share a model with the community, you need an account on [huggingface.co](https://huggingface.co/join). You can also join an existing organization or create a new one. ## Setup Before sharing a model to the Hub, you will need your Hugging Face credentials. If you have access to a terminal, run the following command in the virtual environment where ü§ó Transformers is installed. This will store your access token in your Hugging Face cache folder (`~/.cache/` by default): ```bash huggingface-cli login ``` If you are using a notebook like Jupyter or Colaboratory, make sure you have the [`huggingface\\_hub`](https://huggingface.co/docs/hub/adding-a-library) library installed. This library allows you to programmatically interact with the Hub. ```bash pip install huggingface\\_hub ``` Then use `notebook\\_login` to sign-in to the Hub, and follow the link [here](https://huggingface.co/settings/token) to generate a token to login with: ```py >>> from huggingface\\_hub import notebook\\_login >>> notebook\\_login() ```"", ""Uploading models\nTo upload models to the Hub, you'll need to create an account at [Hugging Face](https://huggingface.co/join). Models on the Hub are [Git-based repositories](./repositories), which give you versioning, branches, discoverability and sharing features, integration with over a dozen libraries, and more! You have control over what you want to upload to your repository, which could include checkpoints, configs, and any other files. There are several ways to upload models to the Hub, described below. We suggest adding a [Model Card](./model-cards) to your repo to document your model. ## Using the web interface\nTo create a brand new model repository, visit [huggingface.co/new](http://huggingface.co/new)."", ""# Share a model\nThe last two tutorials showed how you can fine-tune a model with PyTorch, Keras, and ü§ó Accelerate for distributed setups. The next step is to share your model with the community! At Hugging Face, we believe in openly sharing knowledge and resources to democratize artificial intelligence for everyone. We encourage you to consider sharing your model with the community to help others save time and resources. In this tutorial, you will learn two methods for sharing a trained or fine-tuned model on the [Model Hub](https://huggingface.co/models):\n- Programmatically push your files to the Hub. - Drag-and-drop your files to the Hub with the web interface."", ""`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model)). The `run.sh` script allows to upload a given model to a Hugging Face repository. ```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```"", ""To learn how to upload models to the Hub, you can refer to the [Repositories Getting Started Guide](./repositories-getting-started)."", ""- [Uploading Models](./models-uploading)"", ""The Hugging Face Hub[[the-hugging-face-hub]] The [Hugging Face Hub](https://huggingface.co/) ‚Äì- our main website ‚Äì- is a central platform that enables anyone to discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with more than 10,000 publicly available. Each of these models is hosted as a Git repository, which allows versioning and reproducibility. Sharing a model on the Hub means opening it up to the community and making it accessible to anyone looking to easily use it, in turn eliminating their need to train a model on their own and simplifying sharing and usage. Additionally, sharing a model on the Hub automatically deploys a hosted Inference API for that model. Anyone in the community is free to test it out directly on the model's page, with custom inputs and appropriate widgets. The best part is that sharing and using any public model on the Hub is completely free! Having a huggingface.co account is required to follow along this part, as we'll be creating and managing repositories on the Hugging Face Hub: [create an account](https://huggingface.co/join)""]","To upload an ESPnet model to a Hugging Face repository, you can use the following command in the `run.sh` script:

```bash
./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo
```

Make sure to replace `username/model_repo` with your actual Hugging Face username and the desired model repository name.",./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo
"What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?
","[""Add custom Dependencies\nInference Endpoints‚Äô base image includes all required libraries to run inference on ü§ó Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\\* [customize your inference pipeline](/docs/inference-endpoints/guides/custom\\_handler) and need additional Python dependencies\n\\* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)). To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within. ```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\\* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n\\* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom\\_container) for inference."", ""### 6. Deploy your Custom Handler as an Inference Endpoint\nThe last step is to deploy your Custom Handler as an Inference Endpoint. You can deploy your Custom Handler like you would a regular Inference Endpoint. Add your repository, select your cloud and region, your instance and security setting, and deploy. When creating your Endpoint, the Inference Endpoint Service will check for an available and valid `handler.py`, and will use it for serving requests no matter which ‚ÄúTask‚Äù you select. \\_Note: In your [Inference Endpoints dashboard](https://ui.endpoints.huggingface.co/), the Task for this Endpoint should now be set to Custom\\_"", ""```\nNext, we will create a `requirements.txt` file containing all the dependencies we need to run our inference code:\n```\ntransformers==4.31.0\naccelerate>=0.20.3\n```\nUploading these two files to our repository will suffice to serve the model.\n```"", ""```\nThe custom handler is a Python class (`EndpointHandler`) inside a `handler.py` file in our repository. The `EndpointHandler` needs to implement an `\\_\\_init\\_\\_` and a `\\_\\_call\\_\\_` method. - The `\\_\\_init\\_\\_` method will be called when starting the Endpoint and will receive 1 argument, a string with the path to your model weights. This allows you to load your model correctly. - The `\\_\\_call\\_\\_` method will be called on every request and receive a dictionary with your request body as a python dictionary. It will always contain the `inputs` key. The first step is to create our `handler.py` in the local clone of our repository. ```\n!cd distilbert-base-uncased-emotion && touch handler.py\n```\nIn there, you define your `EndpointHandler` class with the `\\_\\_init\\_\\_` and `\\_\\_call\\_\\_ `method. ```python\nfrom typing import Dict, List, Any\nclass EndpointHandler():\ndef \\_\\_init\\_\\_(self, path=\""\""):\n# Preload all the elements you are going to need at inference. # pseudo:\n# self.model= load\\_model(path)\ndef \\_\\_call\\_\\_(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:\n\""\""\""\ndata args:\ninputs (:obj: `str` | `PIL.Image` | `np.array`)\nkwargs\nReturn:\nA :obj:`list` | `dict`: will be serialized and returned\n\""\""\""\n# pseudo\n# self.model(input)\n```"", ""```\n# Change target model\n>>> endpoint.update(repository=\""gpt2-large\"")\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2-large', status='pending', url=None)\n```"", ""By default the Inference Endpoint is built from a docker image provided by Hugging Face. However, it is possible to specify any docker image using the `custom_image` parameter. A common use case is to run LLMs using the [text-generation-inference](https://github.com/huggingface/text-generation-inference) framework. This can be done like this:\n```python\n# Start an Inference Endpoint running Zephyr-7b-beta on TGI\n>>> from huggingface_hub import create_inference_endpoint\n>>> endpoint = create_inference_endpoint(\n..."", ""The `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:\n\\* `model\\_fn(model\\_dir)` overrides the default method for loading a model. The return value `model` will be used in `predict` for predictions. `predict` receives argument the `model\\_dir`, the path to your unzipped `model.tar.gz`. \\* `transform\\_fn(model, data, content\\_type, accept\\_type)` overrides the default transform function with your custom implementation. You will need to implement your own `preprocess`, `predict` and `postprocess` steps in the `transform\\_fn`. This method can't be combined with `input\\_fn`, `predict\\_fn` or `output\\_fn` mentioned below. \\* `input\\_fn(input\\_data, content\\_type)` overrides the default method for preprocessing. The return value `data` will be used in `predict` for predictions. The inputs are:\n- `input\\_data` is the raw body of your request. - `content\\_type` is the content type from the request header. \\* `predict\\_fn(processed\\_data, model)` overrides the default method for predictions. The return value `predictions` will be used in `postprocess`. The input is `processed\\_data`, the result from `preprocess`. \\* `output\\_fn(prediction, accept)` overrides the default method for postprocessing. The return value `result` will be the response of your request (e.g.`JSON`). The inputs are:\n- `predictions` is the result from `predict`. - `accept` is the return accept type from the HTTP Request, e.g. `application/json`. Here is an example of a custom inference module with `model\\_fn`, `input\\_fn`, `predict\\_fn`, and `output\\_fn`:\n```python\nfrom sagemaker\\_huggingface\\_inference\\_toolkit import decoder\\_encoder\ndef model\\_fn(model\\_dir):\n# implement custom code to load the model\nloaded\\_model = ... return loaded\\_model\ndef input\\_fn(input\\_data, content\\_type):\n# decode the input data (e.g. JSON string -> dict)\ndata = decoder\\_encoder.decode(input\\_data, content\\_type)\nreturn data\ndef predict\\_fn(data, model):\n# call your custom model with the data\noutputs = model(data , ... )\nreturn predictions\ndef output\\_fn(prediction, accept):\n# convert the model output to the desired output format (e.g. dict -> JSON string)\nresponse = decoder\\_encoder.encode(prediction, accept)\nreturn response\n```"", ""- \""You can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository.\""\n- \""Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom\\_dependencies).\"""", ""Inference Endpoints not only allows you to [customize your inference handler](/docs/inference-endpoints/guides/custom\\_handler), but it also allows you to provide a custom container image. The [creation flow](/docs/inference-endpoints/guides/create\\_endpoint) of your Image artifacts from a custom image is the same as the base image. This means Inference Endpoints will create a unique image artifact derived from your provided image, including all Model Artifacts. The Model Artifacts (weights) are stored under `/repository`. For example, if you use` tensorflow/serving` as your custom image, then you have to set `model\\_base\\_path=\""/repository\"": \n```\ntensorflow\\_model\\_server \\\n--rest\\_api\\_port=5000 \\\n--model\\_name=my\\_model \\\n--model\\_base\\_path=\""/repository\""\n```"", ""\""my-endpoint-name\"",\n... repository=\""gpt2\"",\n... framework=\""pytorch\"",\n... task=\""text-generation\"",\n... accelerator=\""cpu\"",\n... vendor=\""aws\"",\n... region=\""us-east-1\"",\n... type=\""protected\"",\n... instance\\_size=\""medium\"",\n... instance\\_type=\""c6i\""\n... )\nIn this example, we created a `protected` Inference Endpoint named `\""my-endpoint-name\""`, to serve [gpt2](https://huggingface.co/gpt2) for `text-generation`. A `protected` Inference Endpoint means your token is required to access the API. We also need to provide additional information to configure the hardware requirements, such as vendor, region, accelerator, instance type, and size. You can check out the list of available resources [here](https://api.endpoints.huggingface.cloud/#/v2%3A%3Aprovider/list\\_vendors). Alternatively, you can create an Inference Endpoint manually using the [Web interface](https://ui.endpoints.huggingface.co/new) for convenience. Refer to this [guide](https://huggingface.co/docs/inference-endpoints/guides/advanced) for details on advanced settings and their usage. The value returned by [`create\\_inference\\_endpoint`] is an [`InferenceEndpoint`] object:\n```py\n>>> endpoint\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='pending', url=None)\n```"", ""The customization can be done through a [handler.py](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) file in your model repository on the Hugging Face Hub. The [handler.py](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) needs to implement the [EndpointHandler](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) class with a `\\_\\_init\\_\\_` and a `\\_\\_call\\_\\_` method. If you want to use custom dependencies, e.g. [optimum](https://raw.githubusercontent.com/huggingface/optimum), the dependencies must be listed in a `requirements.txt` as described above in ‚Äúadd custom dependencies.‚Äù"", ""* [Add custom Dependencies](/docs/inference-endpoints/guides/custom\\_dependencies)""]","To install custom Python dependencies for Inference Endpoints, you should add a `requirements.txt` file to your model repository on the Hugging Face Hub. This file should list the Python dependencies you want to install.",requirements.txt
"How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?
","[""Textual inversion is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples."", ""We used approximately 100 million images with Japanese captions, including the Japanese subset of [LAION-5B](https://laion.ai/blog/laion-5b/)."", ""The [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images."", ""By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images."", ""Textual inversion is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples."", ""```python\nparser.add_argument(\""--num_images\"", type=int, default=1)\n```\n```python\nfor i in range(args.num_images):\n```\n```python\nnum_prompt_samples=1\n```"", ""Textual inversion is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.""]","To teach new concepts to Stable Diffusion using Textual Inversion, you need just 3-5 images.",3-5 images
"What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?
","[""Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces."", ""This is why large models on the Hugging Face Hub are not saved and shared with one big file containing all the weights, but **several** of them. If you go to the [BLOOM model page](https://huggingface.co/bigscience/bloom/tree/main) for instance, you will see there is 72 files named `pytorch_model_xxxxx-of-00072.bin`, which each contain part of the model weights. Using this format, we can load one part of the state dict in memory, put the weights inside the model, move them on the right device, then discard this state dict part before going to the next. Instead of requiring to have enough RAM to accommodate the whole model, we only need enough RAM to get the biggest checkpoint part, which we call a **shard**, so 7.19GB in the case of BLOOM. We call the checkpoints saved in several files like BLOOM *sharded checkpoints*, and we have standardized their format as such:"", ""```py\nmodel.save_pretrained(tmp_dir, max_shard_size=\""200MB\"")\n```\n```py\nindex[\""metadata\""]\n{'total_size': 433245184}\n```""]","In Transformers version 4.18.0, model checkpoints that take more than 10GB of space are automatically sharded into smaller pieces.",10GB
"What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?
","[""If you‚Äôre choosing between two different datasets for a given task or choosing between two ML models trained on different datasets, knowing which one better represents the demographic makeup of your ML system‚Äôs user base can help you make an informed decision to minimize bias-related risks. If you‚Äôre curating a dataset iteratively by filtering data points from a source or selecting new sources of data to add, measuring how these choices affect the diversity and biases present in your overall dataset can make it safer to use in general. We‚Äôve recently released two tools you can leverage to measure your data through a bias-informed lens. The [disaggregatorsü§ó library](https://github.com/huggingface/disaggregators) provides utilities to quantify the composition of your dataset, using either metadata or leveraging models to infer properties of data points. This can be particularly useful to minimize risks of bias-related **[representation harms](https://aclanthology.org/P16-2096/)** or **disparate performances** of trained models. Look at the [demo](https://huggingface.co/spaces/society-ethics/disaggregators) to see it applied to the LAION, MedMCQA, and The Stack datasets! \n\nOnce you have some helpful statistics about the composition of your dataset, you‚Äôll also want to look at associations between features in your data items, particularly at associations that may encode derogatory or otherwise negative stereotypes. The Data Measurements Tool we [originally introduced](https://huggingface.co/blog/data-measurements-tool#comparison-statistics) last year allows you to do this by looking at the [normalized Pointwise Mutual Information (nPMI)](https://dl.acm.org/doi/10.1145/3461702.3462557) between terms in your text-based dataset; particularly associations between gendered pronouns that may denote gendered stereotypes. \n\nThese tools aren‚Äôt full solutions by themselves, rather, they are designed to support critical examination and improvement of datasets through several lenses, including the lens of bias and bias-related risks. In general, we encourage you to keep the following steps in mind when leveraging these and other tools to mitigate bias risks at the dataset curation/selection stage:\n* Identify:\n* Aspects of the dataset creation that may exacerbate specific biases\n* Demographic categories and social variables that are particularly important to the dataset‚Äôs task and domain\n* Measure:\n* The demographic distribution in your dataset\n* Pre-identified negative stereotypes represented\n* Document:\n* Share what you‚Äôve Identified and Measured in your Dataset Card so it can benefit other users, developers, and otherwise affected people\n* Adapt:\n* By choosing the dataset least likely to cause bias-related harms\n* By iteratively improving your dataset in ways that reduce bias risks""]","I'm unable to provide information about Weights and Biases (W&B) as it is not related to HuggingFace or gradio-app. If you have questions specifically about HuggingFace or gradio-app, feel free to ask!","To track their machine learning experiments at every stage, from training to production."
"What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?
","[""### State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n\nü§ó Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio."", ""[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices."", ""ü§ó Optimum provides an API called BetterTransformer, a fast path of standard PyTorch Transformer APIs to benefit from interesting speedups on CPU & GPU through sparsity and fused kernels as Flash Attention."", ""\""So what I've been working on for the last few months on the transformers library is providing the functionality to export these models into a format that lets you run them much more efficiently using tools that we have at Hugging Face, but also just general tools in the open-source ecosystem. In a way, the philosophy of the transformers library is like writing lots of code so that the users don't have to write that code. In this particular example, what we're talking about is something called the ONNX format. It's a special format that is used in industry where you can basically have a model that's written in PyTorch but you can then convert it to TensorFlow or you can run it on some very dedicated hardware. And if you actually look at what's needed to make this conversion happen in the transformers library, it's fairly gnarly. But we make it so that you only really have to run one line of code and the library will take care of you. So the idea is that this particular feature lets machine learning engineers or even data scientists take their model, convert it to this format, and then optimize it to get faster latency and higher throughput.\"""", ""Port of Hugging Face's Transformers library, using the tch-rs crate and pre-processing from rust-tokenizers. Supports multi-threaded tokenization and GPU inference. This repository exposes the model base architecture, task-specific heads and ready-to-use pipelines. A fast and user-friendly runtime for transformer inference (Bert, Albert, GPT2, Decoders, etc) on CPU and GPU."", ""Transformers.js [repository](https://github.com/xenova/transformers.js)"", ""ü§ó Transformers has become the default library for data scientists all around the world to explore state of the art NLP models and build new NLP features. With over 5,000 pre-trained and fine-tuned models available, in over 250 languages, it is a rich playground, easily accessible whichever framework you are working in. This 100x performance gain and built-in scalability is why subscribers of our hosted [Accelerated Inference API](https://huggingface.co/pricing) chose to build their NLP features on top of it. The first leg of the optimization journey is the most accessible, all about using the best combination of techniques offered by the [Hugging Face libraries](https://github.com/huggingface/), independent of the target hardware. We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) to reduce the amount of computation during each forward pass. Tokenization is often a bottleneck for efficiency during inference. We use the most efficient methods from the [ü§ó Tokenizers](https://github.com/huggingface/tokenizers/) library, leveraging the Rust implementation of the model tokenizer in combination with smart caching to get up to 10x speedup for the overall latency."", ""- Infinity comes as a single-container and can be deployed in any production environment. - It can achieve 1ms latency for BERT-like models on GPU and 4-10ms on CPU ü§Øü§Øü§Ø\n- Infinity meets the highest security requirements and can be integrated into your system without the need for internet access. You have control over all incoming and outgoing traffic. ‚ö†Ô∏è Join us for a [live announcement and demo on Sep 28](https://app.livestorm.co/hugging-face/hugging-face-infinity-launch?type=detailed), where we will be showcasing Infinity for the first time in public!"", ""Thanks to the integration of Habana‚Äôs [SynapseAI software suite](https://habana.ai/training-software/) with the Hugging Face [Optimum open-source library](https://github.com/huggingface/optimum), data scientists and machine learning engineers can now accelerate their Transformer training jobs on Habana processors with just a few lines of code and enjoy greater productivity as well as lower training cost."", ""# Philosophy\nü§ó Transformers is an opinionated library built for:\n- machine learning researchers and educators seeking to use, study or extend large-scale Transformers models. - hands-on practitioners who want to fine-tune those models or serve them in production, or both. - engineers who just want to download a pretrained model and use it to solve a given machine learning task. The library was designed with two strong goals in mind:\n1. Be as easy and fast to use as possible:\n- We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions,\njust three standard classes required to use each model: [configuration](main\\_classes/configuration),\n[models](main\\_classes/model), and a preprocessing class ([tokenizer](main\\_classes/tokenizer) for NLP, [image processor](main\\_classes/image\\_processor) for vision, [feature extractor](main\\_classes/feature\\_extractor) for audio, and [processor](main\\_classes/processors) for multimodal inputs). - All of these classes can be initialized in a simple and unified way from pretrained instances by using a common\n`from\\_pretrained()` method which downloads (if needed), caches and\nloads the related class instance and associated data (configurations' hyperparameters, tokenizers' vocabulary,\nand models' weights) from a pretrained checkpoint provided on [Hugging Face Hub](https://huggingface.co/models) or your own saved checkpoint. - On top of those three base classes, the library provides two APIs: [`pipeline`] for quickly\nusing a model for inference on a given task and [`Trainer`] to quickly train or fine-tune a PyTorch model (all TensorFlow models are compatible with `Keras.fit`). - As a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to\nextend or build upon the library, just use regular Python, PyTorch, TensorFlow, Keras modules and inherit from the base\nclasses of the library to reuse functionalities like model loading and saving. If you'd like to learn more about our coding philosophy for models, check out our [Repeat Yourself](https://huggingface.co/blog/transformers-design-philosophy) blog post. 2. Provide state-of-the-art models with performances as close as possible to the original models:\n- We provide at least one example for each architecture which reproduces a result provided by the official authors\nof said architecture. - The code is usually as close to the original code base as possible which means some PyTorch code may be not as\n*pytorchic* as it could be as a result of being converted TensorFlow code and vice versa."", ""* Transformers [library](https://github.com/huggingface/transformers)."", ""# ü§ó Transformers\nState-of-the-art Machine Learning for [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), and [JAX](https://jax.readthedocs.io/en/latest/). ü§ó Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:\nüìù **Natural Language Processing**: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation. üñºÔ∏è **Computer Vision**: image classification, object detection, and segmentation. üó£Ô∏è **Audio**: automatic speech recognition and audio classification. üêô **Multimodal**: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering. ü§ó Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model's life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments. Join the growing community on the [Hub](https://huggingface.co/models), [forum](https://discuss.huggingface.co/), or [Discord](https://discord.com/invite/JfAtkvEtRb) today!"", ""### Aprendizado de m√°quina de √∫ltima gera√ß√£o para JAX, PyTorch e TensorFlow\n\n### \n\nA biblioteca ü§ó Transformers oferece milhares de modelos pr√©-treinados para executar tarefas em diferentes modalidades, como texto, vis√£o e √°udio."", ""ü§ó Transformers –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç—ã—Å—è—á–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–µ–∫—Å—Ç, –∑—Ä–µ–Ω–∏–µ –∏ –∞—É–¥–∏–æ.""]",The name of the open-source library created by Hugging Face to simplify Transformer acceleration is **Optimum**.,Optimum
"What parameter is used to ensure that elements in a row have the same height in Gradio?
","[""```\nwith gr.Row():\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\ngr.Image()\ngr.Slider()\ngr.Markdown(\""### Now in a Row in a group.\"")\nwith gr.Group():\nwith gr.Row():\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\ngr.Image()\ngr.Slider()\n```"", ""To make every element in a Row have the same height, use the `equal_height` argument of the `style` method. ```python\nwith gr.Blocks() as demo:\nwith gr.Row(equal_height=True):\ntextbox = gr.Textbox()\nbtn2 = gr.Button(\""Button 2\"")\n```"", ""```javascript\nexport let height: number | \""auto\"" = \""auto\"";\n```"", ""5. Fix combining adjacent components without gaps by introducing `gr.Row(variant=\""compact\"")` by [@aliabid94](https://github.com/aliabid94) in [PR 2291](https://github.com/gradio-app/gradio/pull/2291) This comes with deprecation of the following arguments for `Component.style`: `round`, `margin`, `border`."", ""```\nwith gr.Row():\ntext1 = gr.Textbox(label=\""t1\"")\nslider2 = gr.Textbox(label=\""s2\"")\ndrop3 = gr.Dropdown([\""a\"", \""b\"", \""c\""], label=\""d3\"")\nwith gr.Row():\nwith gr.Column(scale=1, min_width=600):\ntext1 = gr.Textbox(label=\""prompt 1\"")\ntext2 = gr.Textbox(label=\""prompt 2\"")\ninbtw = gr.Button(\""Between\"")\ntext4 = gr.Textbox(label=\""prompt 1\"")\ntext5 = gr.Textbox(label=\""prompt 2\"")\nwith gr.Column(scale=2, min_width=600):\nimg1 = gr.Image(\""images/cheetah.jpg\"")\nbtn = gr.Button(\""Go\"")\n```"", ""5. Fix combining adjacent components without gaps by introducing `gr.Row(variant=\""compact\"")` by [@aliabid94](https://github.com/aliabid94) in [PR 2291](https://github.com/gradio-app/gradio/pull/2291) This comes with deprecation of the following arguments for `Component.style`: `round`, `margin`, `border`.""]",The parameter used to ensure that elements in a row have the same height in Gradio is the `equal_height` argument of the `style` method. You can set it to `True` to achieve this.,equal_height
"What is the command to install the latest version of Optimum with OpenVINO support?
","[""| [Intel OpenVINO](https://docs.openvino.ai/latest/index.html) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |"", ""You can find more examples in the ü§ó Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion-xl), and running SDXL in OpenVINO is supported for text-to-image and image-to-image."", ""```bash\npip install optimum[onnxruntime-gpu]\n```\nTo avoid conflicts between `onnxruntime` and `onnxruntime-gpu`, make sure the package `onnxruntime` is not installed by running `pip uninstall onnxruntime` prior to installing Optimum."", ""```\n### Install Optimum\nYou can install Optimum via pypi:\n```bash\npip install optimum\n```\nOr install from source:\n```bash\npip install git+https://github.com/huggingface/optimum.git\n```\nThis command installs the current main dev version of Optimum, which could include latest developments(new features, bug fixes). However, the\nmain version might not be very stable. If you run into any problem, please open an [issue](https://github.com/huggingface/optimum/issues) so\nthat we can fix it as soon as possible.\n```"", ""Optimum Intel supports [OpenVINO](https://docs.openvino.ai/latest/index.html), an Intel open-source toolkit for high-performance inference. Optimum Intel and OpenVINO can be installed as follows:\n```\npip install optimum[openvino]\n```"", ""```bash\npip install --upgrade-strategy eager optimum[openvino,nncf]\n```"", ""```\nTo load a model and run inference with OpenVINO Runtime, you can just replace your `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. If you want to load a PyTorch checkpoint, set `export=True` to convert your model to the OpenVINO IR (Intermediate Representation). ```diff\n- from transformers import AutoModelForSequenceClassification\n+ from optimum.intel.openvino import OVModelForSequenceClassification\nfrom transformers import AutoTokenizer, pipeline\n# Download a tokenizer and model from the Hub and convert to OpenVINO format\ntokenizer = AutoTokenizer.from\\_pretrained(model\\_id)\nmodel\\_id = \""distilbert-base-uncased-finetuned-sst-2-english\""\n- model = AutoModelForSequenceClassification.from\\_pretrained(model\\_id)\n+ model = OVModelForSequenceClassification.from\\_pretrained(model\\_id, export=True)\n# Run inference! classifier = pipeline(\""text-classification\"", model=model, tokenizer=tokenizer)\nresults = classifier(\""He's a dreadful magician.\"")\n``` \n```"", ""[Intel\n\nOptimize your model to speedup inference with OpenVINO and Neural Compressor](./intel/index)"", ""```bash\npip install --upgrade-strategy eager optimum[\""openvino\""]\n```"", ""\\*\\*How can I use GPUs?\\*\\*\nTo be able to use GPUs you simply need to install `optimum[onnxruntine-gpu]` which will install the required GPU providers and use them by default.""]","The command to install the latest version of Optimum with OpenVINO support is:

```bash
pip install optimum[openvino]
```","pip install --upgrade-strategy eager optimum[""openvino""]"

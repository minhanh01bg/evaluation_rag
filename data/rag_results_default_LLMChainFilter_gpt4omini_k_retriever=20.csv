user_input,retrieved_contexts,response,reference
"What architecture is the `tokenizers-linux-x64-musl` binary designed for?
","[""`tokenizers-linux-x64-musl`\nThis is the \\*\\*x86\\_64-unknown-linux-musl\\*\\* binary for `tokenizers`""]",The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.,x86_64-unknown-linux-musl
"What is the purpose of the BLIP-Diffusion model?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# BLIP-Diffusion\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. The abstract from the paper is:\n\\*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).\\*\nThe original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization. `BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## BlipDiffusionPipeline\n[[autodoc]] BlipDiffusionPipeline\n- all\n- \\_\\_call\\_\\_\n## BlipDiffusionControlNetPipeline\n[[autodoc]] BlipDiffusionControlNetPipeline\n- all\n- \\_\\_call\\_\\_"", ""For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.\\*\\*\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!"", ""A (denoising) diffusion model isn't that complex if you compare it to other generative models such as Normalizing Flows, GANs or VAEs: they all convert noise from some simple distribution to a data sample. This is also the case here where \\*\\*a neural network learns to gradually denoise data\\*\\* starting from pure noise. In a bit more detail for images, the set-up consists of 2 processes:\n\\* a fixed (or predefined) forward diffusion process \\\\(q\\\\) of our choosing, that gradually adds Gaussian noise to an image, until you end up with pure noise\n\\* a learned reverse denoising diffusion process \\\\(p\\_\\theta\\\\), where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image. ![](assets/78_annotated-diffusion/diffusion_figure.png)\n\nBoth the forward and reverse process indexed by \\\\(t\\\\) happen for some number of finite time steps \\\\(T\\\\) (the DDPM authors use \\\\(T=1000\\\\)). You start with \\\\(t=0\\\\) where you sample a real image \\\\(\\mathbf{x}\\_0\\\\) from your data distribution (let's say an image of a cat from ImageNet), and the forward process samples some noise from a Gaussian distribution at each time step \\\\(t\\\\), which is added to the image of the previous time step. Given a sufficiently large \\\\(T\\\\) and a well behaved schedule for adding noise at each time step, you end up with what is called an [isotropic Gaussian distribution](https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic) at \\\\(t=T\\\\) via a gradual process. ## In more mathematical form\nLet's write this down more formally, as ultimately we need a tractable loss function which our neural network needs to optimize."", ""--\ntitle: \""Finetune Stable Diffusion Models with DDPO via TRL\""\nthumbnail: /blog/assets/166\\_trl\\_ddpo/thumbnail.png\nauthors:\n- user: metric-space\nguest: true\n- user: sayakpaul\n- user: kashif\n- user: lvwerra\n---\n# Finetune Stable Diffusion Models with DDPO via TRL\n## Introduction\nDiffusion models (e.g., DALL-E 2, Stable Diffusion) are a class of generative models that are widely successful at generating images most notably of the photorealistic kind. However, the images generated by these models may not always be on par with human preference or human intention. Thus arises the alignment problem i.e. how does one go about making sure that the outputs of a model are aligned with human preferences like â€œqualityâ€ or that outputs are aligned with intent that is hard to express via prompts? This is where Reinforcement Learning comes into the picture. In the world of Large Language Models (LLMs), Reinforcement learning (RL) has proven to become a very effective tool for aligning said models to human preferences. Itâ€™s one of the main recipes behind the superior performance of systems like ChatGPT. More precisely, RL is the critical ingredient of Reinforcement Learning from Human Feedback (RLHF), which makes ChatGPT chat like human beings. In [Training Diffusion Models with Reinforcement Learning, Black](https://arxiv.org/abs/2305.13301) et al. show how to augment diffusion models to leverage RL to fine-tune them with respect to an objective function via a method named Denoising Diffusion Policy Optimization (DDPO). In this blog post, we discuss how DDPO came to be, a brief description of how it works, and how DDPO can be incorporated into an RLHF workflow to achieve model outputs more aligned with the human aesthetics. We then quickly switch gears to talk about how you can apply DDPO to your models with the newly integrated `DDPOTrainer` from the `trl` library and discuss our findings from running DDPO on Stable Diffusion. ## The Advantages of DDPO\nDDPO is not the only working answer to the question of how to attempt to fine-tune diffusion models with RL."", ""If you're interested in training, please take a look at the [Training](../../training/overview) guides instead! The table below lists all the pipelines currently available in ðŸ¤— Diffusers and the tasks they support. Click on a pipeline to view its abstract and published paper. | Pipeline | Tasks |\n|---|---|\n| [AltDiffusion](alt\\_diffusion) | image2image |\n| [AnimateDiff](animatediff) | text2video |\n| [Attend-and-Excite](attend\\_and\\_excite) | text2image |\n| [Audio Diffusion](audio\\_diffusion) | image2audio |\n| [AudioLDM](audioldm) | text2audio |\n| [AudioLDM2](audioldm2) | text2audio |\n| [BLIP Diffusion](blip\\_diffusion) | text2image |\n| [Consistency Models](consistency\\_models) | unconditional image generation |\n| [ControlNet](controlnet) | text2image, image2image, inpainting |\n| [ControlNet with Stable Diffusion XL](controlnet\\_sdxl) | text2image |\n| [ControlNet-XS](controlnetxs) | text2image |\n| [ControlNet-XS with Stable Diffusion XL](controlnetxs\\_sdxl) | text2image |\n| [Cycle Diffusion](cycle\\_diffusion) | image2image |\n| [Dance Diffusion](dance\\_diffusion) | unconditional audio generation |\n| [DDIM](ddim) | unconditional image generation |\n| [DDPM](ddpm) | unconditional image generation |\n| [DeepFloyd IF](deepfloyd\\_if) | text2image, image2image, inpainting, super-resolution |\n| [DiffEdit](diffedit) | inpainting |\n| [DiT](dit) | text2image |\n| [GLIGEN](stable\\_diffusion/gligen) | text2image |\n| [InstructPix2Pix](pix2pix) | image editing |\n| [Kandinsky 2.1](kandinsky) | text2image, image2image, inpainting, interpolation |\n| [Kandinsky 2.2](kandinsky\\_v22) | text2image, image2image, inpainting |\n| [Kandinsky 3](kandinsky3) | text2image, image2image |\n| [Latent Consistency Models](latent\\_consistency\\_models) | text2image |\n| [Latent Diffusion](latent\\_diffusion) | text2image, super-resolution |\n| [LDM3D](stable\\_diffusion/ldm3d\\_diffusion) | text2image, text-to-3D, text-to-pano, upscaling |\n| [MultiDiffusion](panorama) | text2image |\n| [MusicLDM](musicldm) | text2audio |\n| [Paint by Example](paint\\_by\\_example) | inpainting |\n| [ParaDiGMS](paradigms) | text2image |\n| [Pix2Pix Zero](pix2pix\\_zero) | image editing |\n| [PixArt-Î±](pixart) | text2image |\n| [PNDM](pndm) | unconditional image generation |\n| [RePaint](repaint) | inpainting |\n| [Score SDE VE](score\\_sde\\_ve) | unconditional image generation |\n| [Self-Attention Guidance](self\\_attention\\_guidance) | text2image |\n| [Semantic Guidance](semantic\\_stable\\_diffusion) | text2image |\n| [Shap-E](shap\\_e) | text-to-3D, image-to-3D |\n| [Spectrogram Diffusion](spectrogram\\_diffusion) | |\n| [Stable Diffusion](stable\\_diffusion/overview) | text2image, image2image, depth2image, inpainting, image variation, latent upscaler, super-resolution |\n| [Stable Diffusion Model Editing](model\\_editing) | model editing |\n| [Stable Diffusion XL](stable\\_diffusion/stable\\_diffusion\\_xl) | text2image, image2image, inpainting |\n| [Stable Diffusion XL Turbo](stable\\_diffusion/sdxl\\_turbo) | text2image, image2image, inpainting |\n| [Stable unCLIP](stable\\_unclip) | text2image, image variation |\n| [Stochastic Karras VE](stochastic\\_karras\\_ve) | unconditional image generation |\n| [T2I-Adapter](stable\\_diffusion/adapter) | text2image |\n| [Text2Video](text\\_to\\_video) | text2video, video2video |\n| [Text2Video-Zero](text\\_to\\_video\\_zero) | text2video |\n| [unCLIP](unclip) | text2image, image variation |\n| [Unconditional Latent Diffusion](latent\\_diffusion\\_uncond) | unconditional image generation |\n| [UniDiffuser](unidiffuser) | text2image, image2text, image variation, text variation, unconditional image generation, unconditional audio generation |\n| [Value-guided planning](value\\_guided\\_sampling) | value guided sampling |\n| [Versatile Diffusion](versatile\\_diffusion) | text2image, image variation |\n| [VQ Diffusion](vq\\_diffusion) | text2image |\n| [Wuerstchen](wuerstchen) | text2image |\n## DiffusionPipeline\n[[autodoc]] DiffusionPipeline\n- all\n- \\_\\_call\\_\\_\n- device\n- to\n- components\n## FlaxDiffusionPipeline\n[[autodoc]] pipelines.pipeline\\_flax\\_utils.FlaxDiffusionPipeline\n## PushToHubMixin\n[[autodoc]] utils.PushToHubMixin"", ""--\ntitle: The Annotated Diffusion Model\nthumbnail: /blog/assets/78\\_annotated-diffusion/thumbnail.png\nauthors:\n- user: nielsr\n- user: kashif\n---\n# The Annotated Diffusion Model\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb)\nIn this blog post, we'll take a deeper look into \\*\\*Denoising Diffusion Probabilistic Models\\*\\* (also known as DDPMs, diffusion models, score-based generative models or simply [autoencoders](https://benanne.github.io/2022/01/31/diffusion.html)) as researchers have been able to achieve remarkable results with them for (un)conditional image/audio/video generation. Popular examples (at the time of writing) include [GLIDE](https://arxiv.org/abs/2112.10741) and [DALL-E 2](https://openai.com/dall-e-2/) by OpenAI, [Latent Diffusion](https://github.com/CompVis/latent-diffusion) by the University of Heidelberg and [ImageGen](https://imagen.research.google/) by Google Brain. We'll go over the original DDPM paper by ([Ho et al., 2020](https://arxiv.org/abs/2006.11239)), implementing it step-by-step in PyTorch, based on Phil Wang's [implementation](https://github.com/lucidrains/denoising-diffusion-pytorch) - which itself is based on the [original TensorFlow implementation](https://github.com/hojonathanho/diffusion). Note that the idea of diffusion for generative modeling was actually already introduced in ([Sohl-Dickstein et al., 2015](https://arxiv.org/abs/1503.03585)). However, it took until ([Song et al., 2019](https://arxiv.org/abs/1907.05600)) (at Stanford University), and then ([Ho et al., 2020](https://arxiv.org/abs/2006.11239)) (at Google Brain) who independently improved the approach. Note that there are [several perspectives](https://twitter.com/sedielem/status/1530894256168222722?s=20&t=mfv4afx1GcNQU5fZklpACw) on diffusion models. Here, we employ the discrete-time (latent variable model) perspective, but be sure to check out the other perspectives as well. Alright, let's dive in! ```python\nfrom IPython.display import Image\nImage(filename='assets/78\\_annotated-diffusion/ddpm\\_paper.png')\n```\n\n![](assets/78_annotated-diffusion/ddpm_paper.png)\n\nWe'll install and import the required libraries first (assuming you have [PyTorch](https://pytorch.org/) installed). ```python\n!pip install -q -U einops datasets matplotlib tqdm\nimport math\nfrom inspect import isfunction\nfrom functools import partial\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom einops import rearrange, reduce\nfrom einops.layers.torch import Rearrange\nimport torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\n```\n## What is a diffusion model?"", ""However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.\\*\n![BLIP.gif](https://cdn-uploads.huggingface.co/production/uploads/1670928184033-62441d1d9fdefb55a0b7d12c.gif)\nThis model was contributed by [ybelkada](https://huggingface.co/ybelkada). The original code can be found [here](https://github.com/salesforce/BLIP). ## Resources\n- [Jupyter notebook](https://github.com/huggingface/notebooks/blob/main/examples/image\\_captioning\\_blip.ipynb) on how to fine-tune BLIP for image captioning on a custom dataset\n## BlipConfig\n[[autodoc]] BlipConfig\n- from\\_text\\_vision\\_configs\n## BlipTextConfig\n[[autodoc]] BlipTextConfig\n## BlipVisionConfig\n[[autodoc]] BlipVisionConfig\n## BlipProcessor\n[[autodoc]] BlipProcessor\n## BlipImageProcessor\n[[autodoc]] BlipImageProcessor\n- preprocess\n\n## BlipModel\n[[autodoc]] BlipModel\n- forward\n- get\\_text\\_features\n- get\\_image\\_features\n## BlipTextModel\n[[autodoc]] BlipTextModel\n- forward\n## BlipVisionModel\n[[autodoc]] BlipVisionModel\n- forward\n## BlipForConditionalGeneration\n[[autodoc]] BlipForConditionalGeneration\n- forward\n## BlipForImageTextRetrieval\n[[autodoc]] BlipForImageTextRetrieval\n- forward\n## BlipForQuestionAnswering\n[[autodoc]] BlipForQuestionAnswering\n- forward\n\n## TFBlipModel\n[[autodoc]] TFBlipModel\n- call\n- get\\_text\\_features\n- get\\_image\\_features\n## TFBlipTextModel\n[[autodoc]] TFBlipTextModel\n- call\n## TFBlipVisionModel\n[[autodoc]] TFBlipVisionModel\n- call\n## TFBlipForConditionalGeneration\n[[autodoc]] TFBlipForConditionalGeneration\n- call\n## TFBlipForImageTextRetrieval\n[[autodoc]] TFBlipForImageTextRetrieval\n- call\n## TFBlipForQuestionAnswering\n[[autodoc]] TFBlipForQuestionAnswering\n- call"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Latent Diffusion\nLatent Diffusion was proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://huggingface.co/papers/2112.10752) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer. The abstract from the paper is:\n\\*By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.\\*\nThe original codebase can be found at [CompVis/latent-diffusion](https://github.com/CompVis/latent-diffusion). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.""]","The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation in text-to-image generation tasks. It creates novel renditions of an input subject based on text prompts while overcoming limitations such as lengthy fine-tuning and difficulties in preserving subject fidelity. BLIP-Diffusion introduces a new multimodal encoder pre-trained to provide subject representation, allowing for efficient fine-tuning and flexible combinations with existing techniques for novel subject-driven generation and editing applications.",The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.
"How can a user claim authorship of a paper on the Hugging Face Hub?
","[""Paper Pages\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png)\n\n## Linking a Paper to a model, dataset or Space\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository's tags. Clicking on the arxiv tag will let you:\n\\* Visit the Paper page. \\* Filter for other models or datasets on the Hub that cite the same paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\n\n## Claiming authorship to a Paper\nThe Hub will attempt to automatically match paper to users based on their email. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png)\n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \""claim authorship\""."", ""| Partially handled by `huggingface\\_hub`. |"", ""\\* Is it related to something you would need for a project? We'd love to hear\nabout it! \\* Is it something you worked on and think could benefit the community? Awesome! Tell us what problem it solved for you. 2. Write a \\*full paragraph\\* describing the feature;\n3. Provide a \\*\\*code snippet\\*\\* that demonstrates its future use;\n4. In case this is related to a paper, please attach a link;\n5. Attach any additional information (drawings, screenshots, etc.) you think may help. If your issue is well written we're already 80% of the way there by the time you post it! ## Submitting a pull request (PR)\nBefore writing code, we strongly advise you to search through the existing PRs or\nissues to make sure that nobody is already working on the same thing. If you are\nunsure, it is always a good idea to open an issue to get some feedback. You will need basic `git` proficiency to be able to contribute to\n`huggingface\\_hub`. `git` is not the easiest tool to use but it has the greatest\nmanual. Type `git --help` in a shell and enjoy. If you prefer books, [Pro\nGit](https://git-scm.com/book/en/v2) is a very good reference. Follow these steps to start contributing:\n1. Fork the [repository](https://github.com/huggingface/huggingface\\_hub) by\nclicking on the 'Fork' button on the repository's page. This creates a copy of the code\nunder your GitHub user account. 2. Clone your fork to your local disk, and add the base repository as a remote."", ""The Hugging Face Blog Repository ðŸ¤—\nThis is the official repository of the [Hugging Face Blog](https://hf.co/blog). \\*\\*If you are an external contributor\\*\\*: If your blog post is not a collaboration post with Hugging Face, please consider creating a [community blog](https://huggingface.co/blog-explorers) instead. Community blog posts appear on our blogs main page just like the blogs in this repository. ## How to write an article? ðŸ“\n1ï¸âƒ£ Create a branch `YourName/Title`\n2ï¸âƒ£ Create a md (markdown) file, \\*\\*use a short file name\\*\\*. For instance, if your title is \""Introduction to Deep Reinforcement Learning\"", the md file name could be `intro-rl.md`. This is important because the \\*\\*file name will be the blogpost's URL\\*\\*. 3ï¸âƒ£ Create a new folder in `assets`. Use the same name as the name of the md file. Optionally you may add a numerical prefix to that folder, using the number that hasn't been used yet. But this is no longer required. i.e. the asset folder in this example could be `123\\_intro-rl` or `intro-rl`. This folder will contain \\*\\*your thumbnail only\\*\\*. The folder number is mostly for (rough) ordering purposes, so it's no big deal if two concurrent articles use the same number. For the rest of your files, create a mirrored folder in the HuggingFace Documentation Images [repo](https://huggingface.co/datasets/huggingface/documentation-images/tree/main/blog). This is to reduce bloat in the GitHub base repo when cloning and pulling. ðŸ–¼ï¸: In terms of images, \\*\\*try to have small files\\*\\* to avoid having a slow loading user experience:\n- Use compressed images, you can use this website: https://tinypng.com or https://www.iloveimg.com/compress-image\n4ï¸âƒ£ Copy and paste this to your md file and change the elements\n- title\n- thumbnail\n- authors\n```\n---\ntitle: \""PUT YOUR TITLE HERE\""\nthumbnail: /blog/assets/101\\_decision-transformers-train/thumbnail.gif\nauthors:\n- user: your\\_hf\\_user\n- user: your\\_coauthor\n---\n# Train your first Decision Transformer\nYour content here [...]\n```\nWhen published, the Hub will insert the following UI elements right after the blogpost's main header (i.e."", ""The Hugging Face Hub[[the-hugging-face-hub]]\nThe [Hugging Face Hub](https://huggingface.co/) â€“- our main website â€“- is a central platform that enables anyone to discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with more than 10,000 publicly available. We'll focus on the models in this chapter, and take a look at the datasets in Chapter 5. The models in the Hub are not limited to ðŸ¤— Transformers or even NLP. There are models from [Flair](https://github.com/flairNLP/flair) and [AllenNLP](https://github.com/allenai/allennlp) for NLP, [Asteroid](https://github.com/asteroid-team/asteroid) and [pyannote](https://github.com/pyannote/pyannote-audio) for speech, and [timm](https://github.com/rwightman/pytorch-image-models) for vision, to name a few. Each of these models is hosted as a Git repository, which allows versioning and reproducibility. Sharing a model on the Hub means opening it up to the community and making it accessible to anyone looking to easily use it, in turn eliminating their need to train a model on their own and simplifying sharing and usage. Additionally, sharing a model on the Hub automatically deploys a hosted Inference API for that model. Anyone in the community is free to test it out directly on the model's page, with custom inputs and appropriate widgets. The best part is that sharing and using any public model on the Hub is completely free! [Paid plans](https://huggingface.co/pricing) also exist if you wish to share models privately. The video below shows how to navigate the Hub. Having a huggingface.co account is required to follow along this part, as we'll be creating and managing repositories on the Hugging Face Hub: [create an account](https://huggingface.co/join)"", ""This allows you to create a place to share your organization's artefacts. - \\*\\*Minting DOIs\\*\\*: A [DOI](https://www.doi.org/) (Digital Object Identifier) is a persistent digital identifier for an object. DOIs have become essential for creating persistent identifiers for publications, datasets and software. A persistent identifier is often required by journals, conferences or researcher funders when referencing academic outputs. The Hugging Face Hub supports issuing DOIs for models, datasets, and demos shared on the Hub. - \\*\\*Usage tracking\\*\\*: you can view download stats for datasets and models hosted in the Hub monthly or see the total number of downloads over all time. These stats can be a valuable way for institutions to demonstrate their impact. - \\*\\*Script-based dataset sharing\\*\\*: if you already have dataset hosted somewhere, you can still provide access to them via the Hugging Face hub using a [dataset loading script](https://huggingface.co/docs/datasets/dataset\\_script). - \\*\\*Model and dataset gating\\*\\*: there are circumstances where you want more control over who is accessing models and datasets. The Hugging Face hub supports model and dataset gating, allowing you to add access controls. ## How can I get help using the Hub? The Hub [docs](https://huggingface.co/docs/hub/index) go into more detail about the various features of the Hugging Face Hub. You can also find more information about [sharing datasets on the Hub](https://huggingface.co/docs/datasets/upload\\_dataset) and information about [sharing Transformers models to the Hub](https://huggingface.co/docs/transformers/model\\_sharing)."", ""This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings-dark.png)\n\n## Frequently Asked Questions\n### Can I control which Paper pages show in my profile? Yes! You can visit your Papers in [settings](https://huggingface.co/settings/papers), where you will see a list of verified papers. There, you can click the \""Show on profile\"" checkbox to hide/show it in your profile. ### Do you support ACL anthology? We're starting with Arxiv as it accounts for 95% of the paper URLs Hugging Face users have linked in their repos organically. We'll check how this evolve and potentially extend to other paper hosts in the future. ### Can I have a Paper page even if I have no model/dataset/Space? Yes. You can go to [the main Papers page](https://huggingface.co/papers), click search and write the name of the paper or the full Arxiv id. If the paper does not exist, you will get an option to index it. You can also just visit the page `hf.co/papers/xxxx.yyyyy` replacing with the arxiv id of the paper you wish to index."", ""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ðŸ¤— Hub client library\nThe `huggingface\\_hub` library allows you to interact with the [Hugging Face\nHub](https://hf.co), a machine learning platform for creators and collaborators. Discover pre-trained models and datasets for your projects or play with the hundreds of\nmachine learning apps hosted on the Hub. You can also create and share your own models\nand datasets with the community. The `huggingface\\_hub` library provides a simple way to\ndo all these things with Python. Read the [quick start guide](quick-start) to get up and running with the\n`huggingface\\_hub` library. You will learn how to download files from the Hub, create a\nrepository, and upload files to the Hub. Keep reading to learn more about how to manage\nyour repositories on the ðŸ¤— Hub, how to interact in discussions or even how to access\nthe Inference API. [How-to guides\n\nPractical guides to help you achieve a specific goal. Take a look at these guides to learn how to use huggingface\\_hub to solve real-world problems.](./guides/overview)\n[Reference\n\nExhaustive and technical description of huggingface\\_hub classes and methods.](./package_reference/overview)\n[Conceptual guides\n\nHigh-level explanations for building a better understanding of huggingface\\_hub philosophy.](./concepts/git_vs_http)\n\n## Contribute\nAll contributions to the `huggingface\\_hub` are welcomed and equally valued! ðŸ¤— Besides\nadding or fixing existing issues in the code, you can also help improve the\ndocumentation by making sure it is accurate and up-to-date, help answer questions on\nissues, and request new features you think will improve the library. Take a look at the\n[contribution\nguide](https://github.com/huggingface/huggingface\\_hub/blob/main/CONTRIBUTING.md) to\nlearn more about how to submit a new issue or feature request, how to submit a pull\nrequest, and how to test your contributions to make sure everything works as expected. Contributors should also be respectful of our [code of\nconduct](https://github.com/huggingface/huggingface\\_hub/blob/main/CODE\\_OF\\_CONDUCT.md) to\ncreate an inclusive and welcoming collaborative space for everyone."", ""Hugging Face Hub documentation\nThe Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. The Hub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine Learning. Are you ready to join the path towards open source Machine Learning? ðŸ¤—\n\nRepositories\n\n[Introduction](./repositories)\n[Getting Started](./repositories-getting-started)\n[Repository Settings](./repositories-settings)\n[Pull requests and Discussions](./repositories-pull-requests-discussions)\n[Notifications](./notifications)\n[Collections](./collections)\n[Webhooks](./webhooks)\n[Next Steps](./repositories-next-steps)\n[Licenses](./repositories-licenses)\n\nModels\n\n[Introduction](./models)\n[The Model Hub](./models-the-hub)\n[Model Cards](./model-cards)\n[Gated Models](./models-gated)\n[Uploading Models](./models-uploading)\n[Downloading Models](./models-downloading)\n[Libraries](./models-libraries)\n[Tasks](./models-tasks)\n[Widgets](./models-widgets)\n[Inference API](./models-inference)\n[Download Stats](./models-download-stats)\n\nDatasets\n\n[Introduction](./datasets)\n[Datasets Overview](./datasets-overview)\n[Dataset Cards](./datasets-cards)\n[Gated Datasets](./datasets-gated)\n[Uploading Datasets](./datasets-adding)\n[Downloading Datasets](./datasets-downloading)\n[Libraries](./datasets-libraries)\n[Dataset Viewer](./datasets-viewer)\n[Download Stats](./datasets-download-stats)\n[Data files Configuration](./datasets-data-files-configuration)\n\nSpaces\n\n[Introduction](./spaces)\n[Spaces Overview](./spaces-overview)\n[Gradio Spaces](./spaces-sdks-gradio)\n[Streamlit Spaces](./spaces-sdks-streamlit)\n[Static HTML Spaces](./spaces-sdks-static)\n[Docker Spaces](./spaces-sdks-docker)\n[Embed your Space](./spaces-embed)\n[Run with Docker](./spaces-run-with-docker)\n[Reference](./spaces-config-reference)\n[Changelog](./spaces-changelog)\n[Advanced Topics](./spaces-advanced)\n[Sign in with HF](./spaces-oauth)\n\nOther\n\n[Organizations](./organizations)\n[Enterprise Hub](./enterprise-hub)\n[Billing](./billing)\n[Security](./security)\n[Moderation](./moderation)\n[Paper Pages](./paper-pages)\n[Search](./search)\n[Digital Object Identifier (DOI)](./doi)\n[Hub API Endpoints](./api)\n[Sign in with HF](./oauth)\n[Contributor Code of Conduct](https://huggingface.co/code-of-conduct)\n[Content Guidelines](https://huggingface.co/content-guidelines)\n\n## What's the Hugging Face Hub?""]","A user can claim authorship of a paper on the Hugging Face Hub by visiting the corresponding Paper page. If the paper is not linked to their account, they can click on their name on the Paper page and then click ""claim authorship."" The Hub will attempt to automatically match the paper to users based on their email.","By clicking their name on the corresponding Paper page and clicking ""claim authorship"", then confirming the request in paper settings for admin team validation."
"What is the purpose of the /healthcheck endpoint in the Datasets server API?
","[""Datasets server API - rows endpoint\n> /rows endpoint\n## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### API service\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split"", ""Datasets server admin machine\n> Admin endpoints\n## Configuration\nThe worker can be configured using environment variables. They are grouped by scope. ### Admin service\nSet environment variables to configure the application (`ADMIN\\_` prefix):\n- `ADMIN\\_HF\\_ORGANIZATION`: the huggingface organization from which the authenticated user must be part of in order to access the protected routes, eg. \""huggingface\"". If empty, the authentication is disabled. Defaults to None. - `ADMIN\\_CACHE\\_REPORTS\\_NUM\\_RESULTS`: the number of results in /cache-reports/... endpoints. Defaults to `100`. - `ADMIN\\_CACHE\\_REPORTS\\_WITH\\_CONTENT\\_NUM\\_RESULTS`: the number of results in /cache-reports-with-content/... endpoints. Defaults to `100`. - `ADMIN\\_HF\\_TIMEOUT\\_SECONDS`: the timeout in seconds for the requests to the Hugging Face Hub. Defaults to `0.2` (200 ms). - `ADMIN\\_HF\\_WHOAMI\\_PATH`: the path of the external whoami service, on the hub (see `HF\\_ENDPOINT`), eg. \""/api/whoami-v2\"". Defaults to `/api/whoami-v2`. - `ADMIN\\_MAX\\_AGE`: number of seconds to set in the `max-age` header on technical endpoints. Defaults to `10` (10 seconds). ### Uvicorn\nThe following environment variables are used to configure the Uvicorn server (`ADMIN\\_UVICORN\\_` prefix):\n- `ADMIN\\_UVICORN\\_HOSTNAME`: the hostname. Defaults to `\""localhost\""`. - `ADMIN\\_UVICORN\\_NUM\\_WORKERS`: the number of uvicorn workers. Defaults to `2`. - `ADMIN\\_UVICORN\\_PORT`: the port. Defaults to `8000`. ### Prometheus\n- `PROMETHEUS\\_MULTIPROC\\_DIR`: the directory where the uvicorn workers share their prometheus metrics. See https://github.com/prometheus/client\\_python#multiprocess-mode-eg-gunicorn. Defaults to empty, in which case every worker manages its own metrics, and the /metrics endpoint returns the metrics of a random worker. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nThe admin service provides endpoints:\n- `/healthcheck`\n- `/metrics`: give info about the cache and the queue\n- `/cache-reports{processing\\_step}`: give detailed reports on the content of the cache for a processing step\n- `/cache-reports-with-content{processing\\_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\n- `/force-refresh{processing\\_step}`: force refresh cache entries for the processing step. It's a POST endpoint. Pass the requested parameters, depending on the processing step's input type:\n- `dataset`: `?dataset={dataset}`\n- `config`: `?dataset={dataset}&config={config}`\n- `split`: `?dataset={dataset}&config={config}&split={split}`\n- `/recreate-dataset`: deletes all the cache entries related to a specific dataset, then run all the steps in order. It's a POST endpoint."", ""Datasets server SSE API\n> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache. ## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub\\_cache` field is null for deleted entries, or when the response is an error. The `num\\_rows` value is `0` if it could not be determined. ```typescript\n{\ndataset: string;\nhub\\_cache: null | {\npreview: boolean;\nviewer: boolean;\npartial: boolean;\nnum\\_rows: int;\n};\n}\n```"", ""Datasets server API - search service\n> /search endpoint\n> /filter endpoint\n## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### Duckdb index full text search\n- `DUCKDB\\_INDEX\\_CACHE\\_DIRECTORY`: directory where the temporal duckdb index files are downloaded. Defaults to empty. - `DUCKDB\\_INDEX\\_TARGET\\_REVISION`: the git revision of the dataset where the index file is stored in the dataset repository. ### API service\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /search: get a slice of a search result over a dataset split\n- /filter: filter rows of a dataset split"", ""Quickstart\n[[open-in-colab]]\nIn this quickstart, you'll learn how to use the Datasets Server's REST API to:\n- Check whether a dataset on the Hub is functional. - Return the configuration and splits of a dataset. - Preview the first 100 rows of a dataset. - Download slices of rows of a dataset. - Search a word in a dataset. - Access the dataset as parquet files. ## API endpoints\nEach feature is served through an endpoint summarized in the table below:\n| Endpoint | Method | Description | Query parameters |\n| --------------------------- | ------ | ------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [/is-valid](./valid) | GET | Check whether a specific dataset is valid. | `dataset`: name of the dataset |\n| [/splits](./splits) | GET | Get the list of configurations and splits of a dataset. | `dataset`: name of the dataset |\n| [/first-rows](./first\\_rows) | GET | Get the first rows of a dataset split. | - `dataset`: name of the dataset  \n- `config`: name of the config  \n- `split`: name of the split |\n| [/rows](./rows) | GET | Get a slice of rows of a dataset split. | - `dataset`: name of the dataset  \n- `config`: name of the config  \n- `split`: name of the split  \n- `offset`: offset of the slice  \n- `length`: length of the slice (maximum 100) |\n| [/search](./search) | GET | Search text in a dataset split. | - `dataset`: name of the dataset  \n- `config`: name of the config  \n- `split`: name of the split  \n- `query`: text to search for  \n |\n| [/filter](./filter) | GET | Filter rows in a dataset split. | - `dataset`: name of the dataset  \n- `config`: name of the config  \n- `split`: name of the split  \n- `where`: filter query  \n- `offset`: offset of the slice  \n- `length`: length of the slice (maximum 100) |\n| [/parquet](./parquet) | GET | Get the list of parquet files of a dataset."", ""Datasets server API\n> API on ðŸ¤— datasets\n## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### API service\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /webhook: Add, update or remove a dataset\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first\\_rows) for a dataset split\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset""]",The purpose of the /healthcheck endpoint in the Datasets server API is to ensure that the app is running.,Ensure the app is running
"What is the default context window size for Local Attention in the LongT5 model?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# LongT5\n## Overview\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention. The abstract from the paper is the following:\n\\*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.\\*\nThis model was contributed by [stancld](https://huggingface.co/stancld). The original code can be found [here](https://github.com/google-research/longt5). ## Usage tips\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder \\*self-attention\\* layer with efficient either \\*local\\* attention or \\*transient-global\\* (\\*tglobal\\*) attention. - Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`]. - LongT5 model is designed to work efficiently and very well on long-range \\*sequence-to-sequence\\* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens. - For \\*Local Attention\\*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). \\*Local Attention\\* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l\\*r)`. - \\*Transient Global Attention\\* is an extension of the \\*Local Attention\\*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\nalso every global token like in the case of standard global attention (\\*transient\\* represents the fact the global tokens\nare constructed dynamically within each attention operation). As a consequence, \\*TGlobal\\* attention introduces\na few new parameters -- global relative position biases and a layer normalization for global token's embedding."", ""The Authors' code can be found [here](https://github.com/allenai/longformer). ## Usage tips\n- Since the Longformer is based on RoBERTa, it doesn't have `token\\_type\\_ids`. You don't need to indicate which\ntoken belongs to which segment. Just separate your segments with the separation token `tokenizer.sep\\_token` (or\n``). - A transformer model replacing the attention matrices by sparse matrices to go faster. Often, the local context (e.g., what are the two tokens left and right?) is enough to take action for a given token. Some preselected input tokens are still given global attention, but the attention matrix has way less parameters, resulting in a speed-up. See the local attention section for more information. ## Longformer Self Attention\nLongformer self attention employs self attention on both a \""local\"" context and a \""global\"" context. Most tokens only\nattend \""locally\"" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\n`config.attention\\_window`. Note that `config.attention\\_window` can be of type `List` to define a\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \""globally\"" to all other tokens, as it is\nconventionally done for all tokens in `BertSelfAttention`. Note that \""locally\"" and \""globally\"" attending tokens are projected by different query, key and value matrices. Also note\nthat every \""locally\"" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \""globally\""\nattending tokens so that global attention is \\*symmetric\\*. The user can define which tokens attend \""locally\"" and which tokens attend \""globally\"" by setting the tensor\n`global\\_attention\\_mask` at run-time appropriately. All Longformer models employ the following logic for\n`global\\_attention\\_mask`:\n- 0: the token attends \""locally\"",\n- 1: the token attends \""globally\"". For more information please also refer to [`~LongformerModel.forward`] method. Using Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n\\_s \\times n\\_s)\\\\) to\n\\\\(\\mathcal{O}(n\\_s \\times w)\\\\), with \\\\(n\\_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize.""]",The default context window size for Local Attention in the LongT5 model is `r=127`.,127 tokens
"What method is used to load a checkpoint for a task using `AutoPipeline`?
","[""The `AutoPipeline` class is designed to simplify the variety of pipelines in ðŸ¤— Diffusers. It is a generic, \\*task-first\\* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name. Take a look at the [AutoPipeline](../api/pipelines/auto\\_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting. This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights. ## Choose an AutoPipeline for your task\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nprompt = \""peasant and dragon combat, wood cutting style, viking era, bevel with rune\""\nimage = pipeline(prompt, num\\_inference\\_steps=25).images[0]\nimage\n```\n\n![generated image of peasant fighting dragon in wood cutting style](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png)\n\nUnder the hood, [`AutoPipelineForText2Image`]:\n1. automatically detects a `\""stable-diffusion\""` class from the [`model\\_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model\\_index.json) file\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\""stable-diffusion\""` class name\nLikewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\""stable-diffusion\""` checkpoint from the `model\\_index.json` file and it'll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image:\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\npipeline = AutoPipelineForImage2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"",\ntorch\\_dtype=torch.float16,\nuse\\_safetensors=True,\n).to(\""cuda\"")\nprompt = \""a portrait of a dog wearing a pearl earring\""\nurl = \""https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665\\_Girl\\_with\\_a\\_Pearl\\_Earring.jpg/800px-1665\\_Girl\\_with\\_a\\_Pearl\\_Earring.jpg\""\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\""RGB\"")\nimage.thumbnail((768, 768))\nimage = pipeline(prompt, image, num\\_inference\\_steps=200, strength=0.75, guidance\\_scale=10.5).images[0]\nimage\n```\n\n![generated image of a vermeer portrait of a dog wearing a pearl earring](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png)\n\nAnd if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way:\n```py\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image\nimport torch\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""stabilityai/stable-diffusion-xl-base-1.0\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nimg\\_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\_examples/overture-creations-5sI6fQgYIuo.png\""\nmask\\_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\_examples/overture-creations-5sI6fQgYIuo\\_mask.png\""\ninit\\_image = load\\_image(img\\_url).convert(\""RGB\"")\nmask\\_image = load\\_image(mask\\_url).convert(\""RGB\"")\nprompt = \""A majestic tiger sitting on a bench\""\nimage = pipeline(prompt, image=init\\_image, mask\\_image=mask\\_image, num\\_inference\\_steps=50, strength=0.80).images[0]\nimage\n```\n\n![generated image of a tiger sitting on a bench](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png)\n\nIf you try to load an unsupported checkpoint, it'll throw an error:\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\npipeline = AutoPipelineForImage2Image.from\\_pretrained(\n\""openai/shap-e-img2img\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n)\n\""ValueError: AutoPipeline can't find a pipeline linked to ShapEImg2ImgPipeline for None\""\n```\n## Use multiple pipelines\nFor some workflows or if you're loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory."", ""You'll find the checkpoints saved in a `checkpoint-000` subfolder where the numbers at the end correspond to the training step. Saving checkpoints are useful for resuming training later. ```py\n# resume from latest checkpoint\ntrainer.train(resume\\_from\\_checkpoint=True)\n# resume from specific checkpoint saved in output directory\ntrainer.train(resume\\_from\\_checkpoint=\""your-model/checkpoint-1000\"")\n```\nYou can save your checkpoints (the optimizer state is not saved by default) to the Hub by setting `push\\_to\\_hub=True` in [`TrainingArguments`] to commit and push them. Other options for deciding how your checkpoints are saved are set up in the [`hub\\_strategy`](https://huggingface.co/docs/transformers/main\\_classes/trainer#transformers.TrainingArguments.hub\\_strategy) parameter:\n\\* `hub\\_strategy=\""checkpoint\""` pushes the latest checkpoint to a subfolder named \""last-checkpoint\"" from which you can resume training\n\\* `hug\\_strategy=\""all\\_checkpoints\""` pushes all checkpoints to the directory defined in `output\\_dir` (you'll see one checkpoint per folder in your model repository)\nWhen you resume training from a checkpoint, the [`Trainer`] tries to keep the Python, NumPy, and PyTorch RNG states the same as they were when the checkpoint was saved."", ""A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from\\_pretrained`] method:\n```py\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... new\\_model = AutoModel.from\\_pretrained(tmp\\_dir)\n```\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard. Behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n```py\n>>> import json\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... with open(os.path.join(tmp\\_dir, \""pytorch\\_model.bin.index.json\""), \""r\"") as f:\n... index = json.load(f)\n>>> print(index.keys())\ndict\\_keys(['metadata', 'weight\\_map'])\n```\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n```py\n>>> index[\""metadata\""]\n{'total\\_size': 433245184}\n```\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state\\_dict`) to the file it's stored in:\n```py\n>>> index[\""weight\\_map\""]\n{'embeddings.LayerNorm.bias': 'pytorch\\_model-00001-of-00003.bin',\n'embeddings.LayerNorm.weight': 'pytorch\\_model-00001-of-00003.bin',\n... ```\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from\\_pretrained`] (like you would do `model.load\\_state\\_dict()` for a full checkpoint) you should use [`~modeling\\_utils.load\\_sharded\\_checkpoint`]:\n```py\n>>> from transformers.modeling\\_utils import load\\_sharded\\_checkpoint\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... load\\_sharded\\_checkpoint(model, tmp\\_dir)\n```\n## Low memory loading\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library. Please read the following guide for more information: [Large model loading using Accelerate](./main\\_classes/model#large-model-loading)"", ""We'll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task). {/if}\nIn the code sample above we didn't use `BertConfig`, and instead loaded a pretrained model via the `bert-base-cased` identifier. This is a model checkpoint that was trained by the authors of BERT themselves; you can find more details about it in its [model card](https://huggingface.co/bert-base-cased). This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results. The weights have been downloaded and cached (so future calls to the `from\\_pretrained()` method won't re-download them) in the cache folder, which defaults to \\*~/.cache/huggingface/transformers\\*. You can customize your cache folder by setting the `HF\\_HOME` environment variable."", ""- [LyCORIS checkpoints](https://github.com/KohakuBlueleaf/LyCORIS) aren't fully supported. The [`~loaders.LoraLoaderMixin.load\\_lora\\_weights`] method loads LyCORIS checkpoints with LoRA and LoCon modules, but Hada and LoKR are not supported. Loading a checkpoint from TheLastBen is very similar. For example, to load the [TheLastBen/William\\_Eggleston\\_Style\\_SDXL](https://huggingface.co/TheLastBen/William\\_Eggleston\\_Style\\_SDXL) checkpoint:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\""stabilityai/stable-diffusion-xl-base-1.0\"", torch\\_dtype=torch.float16).to(\""cuda\"")\npipeline.load\\_lora\\_weights(\""TheLastBen/William\\_Eggleston\\_Style\\_SDXL\"", weight\\_name=\""wegg.safetensors\"")\n# use by william eggleston in the prompt to trigger the LoRA\nprompt = \""a house by william eggleston, sunrays, beautiful, sunlight, sunrays, beautiful\""\nimage = pipeline(prompt=prompt).images[0]\nimage\n```\n## IP-Adapter\n[IP-Adapter](https://ip-adapter.github.io/) is an effective and lightweight adapter that adds image prompting capabilities to a diffusion model."", ""If you want to use one of the official checkpoints for a task, explore the [gligen](https://huggingface.co/gligen) Hub organizations! [`StableDiffusionGLIGENPipeline`] was contributed by [Nikhil Gajendrakumar](https://github.com/nikhil-masterful) and [`StableDiffusionGLIGENTextImagePipeline`] was contributed by [Nguyá»…n CÃ´ng TÃº Anh](https://github.com/tuanh123789). ## StableDiffusionGLIGENPipeline\n[[autodoc]] StableDiffusionGLIGENPipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n- enable\\_model\\_cpu\\_offload\n- prepare\\_latents\n- enable\\_fuser\n## StableDiffusionGLIGENTextImagePipeline\n[[autodoc]] StableDiffusionGLIGENTextImagePipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n- enable\\_model\\_cpu\\_offload\n- prepare\\_latents\n- enable\\_fuser\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput"", ""This is a custom pipeline that merges upto 3 pretrained model checkpoints as long as they are in the HuggingFace model\\_index.json format. The checkpoint merging is currently memory intensive as it modifies the weights of a DiffusionPipeline object in place. Expect atleast 13GB RAM Usage on Kaggle GPU kernels and\non colab you might run out of the 12GB memory even while merging two checkpoints. Usage:-\n```python\nfrom diffusers import DiffusionPipeline\n#Return a CheckpointMergerPipeline class that allows you to merge checkpoints. #The checkpoint passed here is ignored. But still pass one of the checkpoints you plan to\n#merge for convenience\npipe = DiffusionPipeline.from\\_pretrained(\""CompVis/stable-diffusion-v1-4\"", custom\\_pipeline=\""checkpoint\\_merger\"")\n#There are multiple possible scenarios:\n#The pipeline with the merged checkpoints is returned in all the scenarios\n#Compatible checkpoints a.k.a matched model\\_index.json files. Ignores the meta attributes in model\\_index.json during comparison.( attrs with \\_ as prefix )\nmerged\\_pipe = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""CompVis/stable-diffusion-v1-2\""], interp = \""sigmoid\"", alpha = 0.4)\n#Incompatible checkpoints in model\\_index.json but merge might be possible. Use force = True to ignore model\\_index.json compatibility\nmerged\\_pipe\\_1 = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""hakurei/waifu-diffusion\""], force = True, interp = \""sigmoid\"", alpha = 0.4)\n#Three checkpoint merging. Only \""add\\_difference\"" method actually works on all three checkpoints. Using any other options will ignore the 3rd checkpoint. merged\\_pipe\\_2 = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""hakurei/waifu-diffusion\"",\""prompthero/openjourney\""], force = True, interp = \""add\\_difference\"", alpha = 0.4)\nprompt = \""An astronaut riding a horse on Mars\""\nimage = merged\\_pipe(prompt).images[0]\n```\nSome examples along with the merge details:\n1. \""CompVis/stable-diffusion-v1-4\"" + \""hakurei/waifu-diffusion\"" ; Sigmoid interpolation; alpha = 0.8\n![Stable plus Waifu Sigmoid 0.8](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/stability\\_v1\\_4\\_waifu\\_sig\\_0.8.png)\n2. \""hakurei/waifu-diffusion\"" + \""prompthero/openjourney\"" ; Inverse Sigmoid interpolation; alpha = 0.8\n![Stable plus Waifu Sigmoid 0.8](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/waifu\\_openjourney\\_inv\\_sig\\_0.8.png)\n3. \""CompVis/stable-diffusion-v1-4\"" + \""hakurei/waifu-diffusion\"" + \""prompthero/openjourney\""; Add Difference interpolation; alpha = 0.5\n![Stable plus Waifu plus openjourney add\\_diff 0.5](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/stable\\_waifu\\_openjourney\\_add\\_diff\\_0.5.png)\n### Stable Diffusion Comparisons\nThis Community Pipeline enables the comparison between the 4 checkpoints that exist for Stable Diffusion."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\nA pipeline is an end-to-end class that provides a quick and easy way to use a diffusion system for inference by bundling independently trained models and schedulers together. Certain combinations of models and schedulers define specific pipeline types, like [`StableDiffusionXLPipeline`] or [`StableDiffusionControlNetPipeline`], with specific capabilities. All pipeline types inherit from the base [`DiffusionPipeline`] class; pass it any checkpoint, and it'll automatically detect the pipeline type and load the necessary components. This section demonstrates how to use specific pipelines such as Stable Diffusion XL, ControlNet, and DiffEdit."", ""But you can also use regular checkpoints like [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5). Let's compare the results of the two checkpoints. The image on the left is generated from a regular checkpoint, and the image on the right is from an inpaint checkpoint. You'll immediately notice the image on the left is not as clean, and you can still see the outline of the area the model is supposed to inpaint. The image on the right is much cleaner and the inpainted area appears more natural. ```py\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image, make\\_image\\_grid\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, variant=\""fp16\""\n).to(\""cuda\"")\npipeline.enable\\_model\\_cpu\\_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable\\_xformers\\_memory\\_efficient\\_attention()\n# load base and mask image\ninit\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\"")\nmask\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint\\_mask.png\"")\ngenerator = torch.Generator(\""cuda\"").manual\\_seed(92)\nprompt = \""concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\""\nimage = pipeline(prompt=prompt, image=init\\_image, mask\\_image=mask\\_image, generator=generator).images[0]\nmake\\_image\\_grid([init\\_image, image], rows=1, cols=2)\n```\n\n```py\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image, make\\_image\\_grid\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""runwayml/stable-diffusion-inpainting\"", torch\\_dtype=torch.float16, variant=\""fp16\""\n).to(\""cuda\"")\npipeline.enable\\_model\\_cpu\\_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable\\_xformers\\_memory\\_efficient\\_attention()\n# load base and mask image\ninit\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\"")\nmask\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint\\_mask.png\"")\ngenerator = torch.Generator(\""cuda\"").manual\\_seed(92)\nprompt = \""concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\""\nimage = pipeline(prompt=prompt, image=init\\_image, mask\\_image=mask\\_image, generator=generator).images[0]\nmake\\_image\\_grid([init\\_image, image], rows=1, cols=2)\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/non-inpaint-specific.png)\n\nrunwayml/stable-diffusion-v1-5\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint-specific.png)\n\nrunwayml/stable-diffusion-inpainting\n\nHowever, for more basic tasks like erasing an object from an image (like the rocks in the road for example), a regular checkpoint yields pretty good results. There isn't as noticeable of difference between the regular and inpaint checkpoint. ```py\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image, make\\_image\\_grid\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, variant=\""fp16\""\n).to(\""cuda\"")\npipeline.enable\\_model\\_cpu\\_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable\\_xformers\\_memory\\_efficient\\_attention()\n# load base and mask image\ninit\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\"")\nmask\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/road-mask.png\"")\nimage = pipeline(prompt=\""road\"", image=init\\_image, mask\\_image=mask\\_image).images[0]\nmake\\_image\\_grid([init\\_image, image], rows=1, cols=2)\n```\n\n```py\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image, make\\_image\\_grid\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""runwayml/stable-diffusion-inpainting\"", torch\\_dtype=torch.float16, variant=\""fp16\""\n).to(\""cuda\"")\npipeline.enable\\_model\\_cpu\\_offload()\n# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\npipeline.enable\\_xformers\\_memory\\_efficient\\_attention()\n# load base and mask image\ninit\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\"")\nmask\\_image = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/road-mask.png\"")\nimage = pipeline(prompt=\""road\"", image=init\\_image, mask\\_image=mask\\_image).images[0]\nmake\\_image\\_grid([init\\_image, image], rows=1, cols=2)\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/regular-inpaint-basic.png)\n\nrunwayml/stable-diffusion-v1-5\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/specific-inpaint-basic.png)\n\nrunwayml/stable-diffusion-inpainting\n\nThe trade-off of using a non-inpaint specific checkpoint is the overall image quality may be lower, but it generally tends to preserve the mask area (that is why you can see the mask outline)."", ""The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it's going to be used for. For example, here we are loading the `camembert-base` checkpoint in the `fill-mask` pipeline, which is completely fine. But if we were to load this checkpoint in the `text-classification` pipeline, the results would not make any sense because the head of `camembert-base` is not suitable for this task! We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:\n\n![The task selector on the web interface.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/tasks.png)\n\nYou can also instantiate the checkpoint using the model architecture directly:\n{#if fw === 'pt'}\n```py\nfrom transformers import CamembertTokenizer, CamembertForMaskedLM\ntokenizer = CamembertTokenizer.from\\_pretrained(\""camembert-base\"")\nmodel = CamembertForMaskedLM.from\\_pretrained(\""camembert-base\"")\n```\nHowever, we recommend using the [`Auto\\*` classes](https://huggingface.co/transformers/model\\_doc/auto.html?highlight=auto#auto-classes) instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the `Auto\\*` classes makes switching checkpoints simple:\n```py\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from\\_pretrained(\""camembert-base\"")\nmodel = AutoModelForMaskedLM.from\\_pretrained(\""camembert-base\"")\n```\n{:else}\n```py\nfrom transformers import CamembertTokenizer, TFCamembertForMaskedLM\ntokenizer = CamembertTokenizer.from\\_pretrained(\""camembert-base\"")\nmodel = TFCamembertForMaskedLM.from\\_pretrained(\""camembert-base\"")\n```\nHowever, we recommend using the [`TFAuto\\*` classes](https://huggingface.co/transformers/model\\_doc/auto.html?highlight=auto#auto-classes) instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the `TFAuto\\*` classes makes switching checkpoints simple:\n```py\nfrom transformers import AutoTokenizer, TFAutoModelForMaskedLM\ntokenizer = AutoTokenizer.from\\_pretrained(\""camembert-base\"")\nmodel = TFAutoModelForMaskedLM.from\\_pretrained(\""camembert-base\"")\n```\n{/if}\nWhen using a pretrained model, make sure to check how it was trained, on which datasets, its limits, and its biases. All of this information should be indicated on its model card."", ""When using\n`DiffusionPipeline.from\\_pretrained(...)`, checkpoints are loaded into\ntheir default pipeline. The default pipeline for the IF is the\ntext-to-image [`IFPipeline`](https://huggingface.co/docs/diffusers/v0.16.0/en/api/pipelines/if#diffusers.IFPipeline). When loading checkpoints\nwith a non-default pipeline, the pipeline must be explicitly specified. ``` python\nfrom diffusers import IFImg2ImgPipeline\npipe = IFImg2ImgPipeline.from\\_pretrained(\n\""DeepFloyd/IF-I-XL-v1.0\"",\ntext\\_encoder=text\\_encoder,\nunet=None,\ndevice\\_map=\""auto\""\n)\n```\nLet\\'s turn our salesman into an anime character. ``` python\nprompt = \""anime style\""\n```\nAs before, we create the text embeddings with T5\n``` python\nprompt\\_embeds, negative\\_embeds = pipe.encode\\_prompt(prompt)\n```\nand free GPU and CPU memory. First, remove the Python pointers\n``` python\ndel text\\_encoder\ndel pipe\n```\nand then free the memory\n``` python\nflush()\n```\n### 2.2 Stage 1: The main diffusion process\nNext, we only load the stage 1 UNet weights into the pipeline object,\njust like we did in the previous section. ``` python\npipe = IFImg2ImgPipeline.from\\_pretrained(\n\""DeepFloyd/IF-I-XL-v1.0\"",\ntext\\_encoder=None,\nvariant=\""fp16\"",\ntorch\\_dtype=torch.float16,\ndevice\\_map=\""auto\""\n)\n```\nThe image variation pipeline requires both the original image and the\nprompt embeddings."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\nðŸ§¨ Diffusers offers many pipelines, models, and schedulers for generative tasks. To make loading these components as simple as possible, we provide a single and unified method - `from\\_pretrained()` - that loads any of these components from either the Hugging Face [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) or your local machine. Whenever you load a pipeline or model, the latest files are automatically downloaded and cached so you can quickly reuse them next time without redownloading the files. This section will show you everything you need to know about loading pipelines, how to load different components in a pipeline, how to load checkpoint variants, and how to load community pipelines. You'll also learn how to load schedulers and compare the speed and quality trade-offs of using different schedulers. Finally, you'll see how to convert and load KerasCV checkpoints so you can use them in PyTorch with ðŸ§¨ Diffusers."", ""For example, if you're using a checkpoint for text-to-image and you want to use it again for image-to-image, use the [`~AutoPipelineForImage2Image.from\\_pipe`] method. This method creates a new pipeline from the components of a previously loaded pipeline at no additional memory cost. The [`~AutoPipelineForImage2Image.from\\_pipe`] method detects the original pipeline class and maps it to the new pipeline class corresponding to the task you want to do. For example, if you load a `\""stable-diffusion\""` class pipeline for text-to-image:\n```py\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch\npipeline\\_text2img = AutoPipelineForText2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n)\nprint(type(pipeline\\_text2img))\n\""\""\n```\nThen [`~AutoPipelineForImage2Image.from\\_pipe`] maps the original `\""stable-diffusion\""` pipeline class to [`StableDiffusionImg2ImgPipeline`]:\n```py\npipeline\\_img2img = AutoPipelineForImage2Image.from\\_pipe(pipeline\\_text2img)\nprint(type(pipeline\\_img2img))\n\""\""\n```\nIf you passed an optional argument - like disabling the safety checker - to the original pipeline, this argument is also passed on to the new pipeline:\n```py\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch\npipeline\\_text2img = AutoPipelineForText2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"",\ntorch\\_dtype=torch.float16,\nuse\\_safetensors=True,\nrequires\\_safety\\_checker=False,\n).to(\""cuda\"")\npipeline\\_img2img = AutoPipelineForImage2Image.from\\_pipe(pipeline\\_text2img)\nprint(pipeline\\_img2img.config.requires\\_safety\\_checker)\n\""False\""\n```\nYou can overwrite any of the arguments and even configuration from the original pipeline if you want to change the behavior of the new pipeline. For example, to turn the safety checker back on and add the `strength` argument:\n```py\npipeline\\_img2img = AutoPipelineForImage2Image.from\\_pipe(pipeline\\_text2img, requires\\_safety\\_checker=True, strength=0.3)\nprint(pipeline\\_img2img.config.requires\\_safety\\_checker)\n\""True\""\n```"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# AutoPipeline\nðŸ¤— Diffusers is able to complete many different tasks, and you can often reuse the same pretrained weights for multiple tasks such as text-to-image, image-to-image, and inpainting. If you're new to the library and diffusion models though, it may be difficult to know which pipeline to use for a task. For example, if you're using the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint for text-to-image, you might not know that you could also use it for image-to-image and inpainting by loading the checkpoint with the [`StableDiffusionImg2ImgPipeline`] and [`StableDiffusionInpaintPipeline`] classes respectively."", ""Code is available at https://github.com/CompVis/latent-diffusion.\\*\nMake sure to check out the Stable Diffusion [Tips](overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently! If you're interested in using one of the official checkpoints for a task, explore the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations! ## StableDiffusionPipeline\n[[autodoc]] StableDiffusionPipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_attention\\_slicing\n- disable\\_attention\\_slicing\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_xformers\\_memory\\_efficient\\_attention\n- disable\\_xformers\\_memory\\_efficient\\_attention\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n- load\\_textual\\_inversion\n- from\\_single\\_file\n- load\\_lora\\_weights\n- save\\_lora\\_weights\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput\n## FlaxStableDiffusionPipeline\n[[autodoc]] FlaxStableDiffusionPipeline\n- all\n- \\_\\_call\\_\\_\n## FlaxStableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.FlaxStableDiffusionPipelineOutput"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# AutoPipeline\n`AutoPipeline` is designed to:\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from\\_pretrained()` method. To seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from\\_pipe()` method to transfer the components from the original pipeline to the new one. ```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nprompt = \""Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\""\nimage = pipeline(prompt, num\\_inference\\_steps=25).images[0]\n```\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API! `AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n- [Stable Diffusion](./stable\\_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable\\_diffusion/stable\\_diffusion\\_xl)\n- [DeepFloyd IF](./deepfloyd\\_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky\\_v22)\n## AutoPipelineForText2Image\n[[autodoc]] AutoPipelineForText2Image\n- all\n- from\\_pretrained\n- from\\_pipe\n## AutoPipelineForImage2Image\n[[autodoc]] AutoPipelineForImage2Image\n- all\n- from\\_pretrained\n- from\\_pipe\n## AutoPipelineForInpainting\n[[autodoc]] AutoPipelineForInpainting\n- all\n- from\\_pretrained\n- from\\_pipe"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Load pipelines, models, and schedulers\n[[open-in-colab]]\nHaving an easy way to use a diffusion system for inference is essential to ðŸ§¨ Diffusers. Diffusion systems often consist of multiple components like parameterized models, tokenizers, and schedulers that interact in complex ways. That is why we designed the [`DiffusionPipeline`] to wrap the complexity of the entire diffusion system into an easy-to-use API, while remaining flexible enough to be adapted for other use cases, such as loading each component individually as building blocks to assemble your own diffusion system. Everything you need for inference or training is accessible with the `from\\_pretrained()` method. This guide will show you how to load:\n- pipelines from the Hub and locally\n- different components into a pipeline\n- checkpoint variants such as different floating point types or non-exponential mean averaged (EMA) weights\n- models and schedulers\n## Diffusion Pipeline\nðŸ’¡ Skip to the [DiffusionPipeline explained](#diffusionpipeline-explained) section if you are interested in learning in more detail about how the [`DiffusionPipeline`] class works. The [`DiffusionPipeline`] class is the simplest and most generic way to load the latest trending diffusion model from the [Hub](https://huggingface.co/models?library=diffusers&sort=trending). The [`DiffusionPipeline.from\\_pretrained`] method automatically detects the correct pipeline class from the checkpoint, downloads, and caches all the required configuration and weight files, and returns a pipeline instance ready for inference. ```python\nfrom diffusers import DiffusionPipeline\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\npipe = DiffusionPipeline.from\\_pretrained(repo\\_id, use\\_safetensors=True)\n```\nYou can also load a checkpoint with its specific pipeline class. The example above loaded a Stable Diffusion model; to get the same result, use the [`StableDiffusionPipeline`] class:\n```python\nfrom diffusers import StableDiffusionPipeline\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\npipe = StableDiffusionPipeline.from\\_pretrained(repo\\_id, use\\_safetensors=True)\n```\nA checkpoint (such as [`CompVis/stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4) or [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)) may also be used for more than one task, like text-to-image or image-to-image. To differentiate what task you want to use the checkpoint for, you have to load it directly with its corresponding task-specific pipeline class:\n```python\nfrom diffusers import StableDiffusionImg2ImgPipeline\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\npipe = StableDiffusionImg2ImgPipeline.from\\_pretrained(repo\\_id)\n```\n### Local pipeline\nTo load a diffusion pipeline locally, use [`git-lfs`](https://git-lfs.github.com/) to manually download the checkpoint (in this case, [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)) to your local disk. This creates a local folder, `./stable-diffusion-v1-5`, on your disk:\n```bash\ngit-lfs install\ngit clone https://huggingface.co/runwayml/stable-diffusion-v1-5\n```\nThen pass the local path to [`~DiffusionPipeline.from\\_pretrained`]:\n```python\nfrom diffusers import DiffusionPipeline\nrepo\\_id = \""./stable-diffusion-v1-5\""\nstable\\_diffusion = DiffusionPipeline.from\\_pretrained(repo\\_id, use\\_safetensors=True)\n```\nThe [`~DiffusionPipeline.from\\_pretrained`] method won't download any files from the Hub when it detects a local path, but this also means it won't download and cache the latest changes to a checkpoint.""]",The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically retrieves the relevant pipeline based on the name or path to the pretrained weights.,from_pretrained()
"What is the purpose of Diffusers library?
","[""ðŸ§¨ Diffusers Experimental\nWe are adding experimental code to support novel applications and usages of the Diffusers library."", ""This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\nis very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline. ## Tweakable, contributor-friendly over abstraction\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t\\_repeat\\_yourself). In short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers. Functions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable. \\*\\*However\\*\\*, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions. - Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions. - Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel. At Hugging Face, we call this design the \\*\\*single-file policy\\*\\* which means that almost all of the code of a certain class should be written in a single, self-contained file."", ""For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.\\*\\*\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!"", ""--\ntitle: Swift ðŸ§¨Diffusers - Fast Stable Diffusion for Mac\nthumbnail: /blog/assets/fast-mac-diffusers/thumbnail.png\nauthors:\n- user: pcuenq\n- user: reach-vb\n---\n# Swift ðŸ§¨Diffusers: Fast Stable Diffusion for Mac\nTransform your text into stunning images with ease using Diffusers for Mac, a native app powered by state-of-the-art diffusion models. It leverages a bouquet of SoTA Text-to-Image models contributed by the community to the Hugging Face Hub, and converted to Core ML for blazingly fast performance. Our latest version, 1.1, is now available on the [Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) with significant performance upgrades and user-friendly interface tweaks. It's a solid foundation for future feature updates. Plus, the app is fully open source with a permissive [license](https://github.com/huggingface/swift-coreml-diffusers/blob/main/LICENSE), so you can build on it too! Check out our GitHub repository at https://github.com/huggingface/swift-coreml-diffusers for more information. ![Screenshot showing Diffusers for Mac UI](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-mac-diffusers/UI.png)\n## What exactly is ðŸ§¨Diffusers for Mac anyway? The Diffusers app ([App Store](https://apps.apple.com/app/diffusers/id1666309574), [source code](https://github.com/huggingface/swift-coreml-diffusers)) is the Mac counterpart to our [ðŸ§¨`diffusers` library](https://github.com/huggingface/diffusers). This library is written in Python with PyTorch, and uses a modular design to train and run diffusion models."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n## Preamble\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training. Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library. The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups."", ""## Credits\nThis library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We'd like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:\n- @CompVis' latent diffusion models library, available [here](https://github.com/CompVis/latent-diffusion)\n- @hojonathanho original DDPM implementation, available [here](https://github.com/hojonathanho/diffusion) as well as the extremely useful translation into PyTorch by @pesser, available [here](https://github.com/pesser/pytorch\\_diffusion)\n- @ermongroup's DDIM implementation, available [here](https://github.com/ermongroup/ddim)\n- @yang-song's Score-VE and Score-VP implementations, available [here](https://github.com/yang-song/score\\_sde\\_pytorch)\nWe also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available [here](https://github.com/heejkoo/Awesome-Diffusion-Models) as well as @crowsonkb and @rromb for useful discussions and insights. ## Citation\n```bibtex\n@misc{von-platen-etal-2022-diffusers,\nauthor = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},\ntitle = {Diffusers: State-of-the-art diffusion models},\nyear = {2022},\npublisher = {GitHub},\njournal = {GitHub repository},\nhowpublished = {\\url{https://github.com/huggingface/diffusers}}\n}\n```"", ""Research projects\nThis folder contains various research projects using ðŸ§¨ Diffusers. They are not really maintained by the core maintainers of this library and often require a specific version of Diffusers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice. If you need help with any of those, please open an issue where you directly ping the author(s), as indicated at the top of the README of each folder."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Using Diffusers with other modalities\nDiffusers is in the process of expanding to modalities other than images. Example type | Colab | Pipeline |\n:-------------------------:|:-------------------------:|:-------------------------:|\n[Molecule conformation](https://www.nature.com/subjects/molecular-conformation#:~:text=Definition,to%20changes%20in%20their%20environment.) generation | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/geodiff\\_molecule\\_conformation.ipynb) | âŒ\nMore coming soon!"", ""To read more about the philosophy, you can have a look\nat [this blog post](https://huggingface.co/blog/transformers-design-philosophy). In Diffusers, we follow this philosophy for both pipelines and schedulers, but only partly for diffusion models. The reason we don't follow this design fully for diffusion models is because almost all diffusion pipelines, such\nas [DDPM](https://huggingface.co/docs/diffusers/api/pipelines/ddpm), [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable\\_diffusion/overview#stable-diffusion-pipelines), [unCLIP (DALLÂ·E 2)](https://huggingface.co/docs/diffusers/api/pipelines/unclip) and [Imagen](https://imagen.research.google/) all rely on the same diffusion model, the [UNet](https://huggingface.co/docs/diffusers/api/models/unet2d-cond). Great, now you should have generally understood why ðŸ§¨ Diffusers is designed the way it is ðŸ¤—. We try to apply these design principles consistently across the library. Nevertheless, there are some minor exceptions to the philosophy or some unlucky design choices. If you have feedback regarding the design, we would â¤ï¸ to hear it [directly on GitHub](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=). ## Design Philosophy in Details\nNow, let's look a bit into the nitty-gritty details of the design philosophy. Diffusers essentially consists of three major classes: [pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines), [models](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models), and [schedulers](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers)."", ""!---\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# ðŸ§¨ Diffusers Examples\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\nfor a variety of use cases involving training or fine-tuning. \\*\\*Note\\*\\*: If you are looking for \\*\\*official\\*\\* examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines)."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Philosophy\nðŸ§¨ Diffusers provides \\*\\*state-of-the-art\\*\\* pretrained diffusion models across multiple modalities. Its purpose is to serve as a \\*\\*modular toolbox\\*\\* for both inference and training. We aim at building a library that stands the test of time and therefore take API design very seriously. In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch's Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let's go over the most important ones:\n## Usability over Performance\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library. - Diffusers aims to be a \\*\\*light-weight\\*\\* package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages. - Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired. ## Simple over easy\nAs PyTorch states, \\*\\*explicit is better than implicit\\*\\* and \\*\\*simple is better than complex\\*\\*."", ""If you liked this topic and want to learn more, we recommend the following resources:\n- Our [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). - The [Getting Started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers\\_intro.ipynb) notebook, that gives a broader overview on Diffusion systems. - The [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) blog post. - Our [code in GitHub](https://github.com/huggingface/diffusers) where we'd be more than happy if you leave a â­ if `diffusers` is useful to you! ### Citation:\n```\n@article{patil2022stable,\nauthor = {Patil, Suraj and Cuenca, Pedro and Lambert, Nathan and von Platen, Patrick},\ntitle = {Stable Diffusion with ðŸ§¨ Diffusers},\njournal = {Hugging Face Blog},\nyear = {2022},\nnote = {[https://huggingface.co/blog/rlhf](https://huggingface.co/blog/stable\\_diffusion)},\n}\n```"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\nThe APIs in this section are more experimental and prone to breaking changes. Most of them are used internally for development, but they may also be useful to you if you're interested in building a diffusion model with some custom parts or if you're interested in some of our helper utilities for working with ðŸ¤— Diffusers."", ""!---\nCopyright 2022 - The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![](https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg)\n\n[![GitHub](https://img.shields.io/github/license/huggingface/datasets.svg?color=blue)](https://github.com/huggingface/diffusers/blob/main/LICENSE)\n[![GitHub release](https://img.shields.io/github/release/huggingface/diffusers.svg)](https://github.com/huggingface/diffusers/releases)\n[![GitHub release](https://static.pepy.tech/badge/diffusers/month)](https://pepy.tech/project/diffusers)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CODE_OF_CONDUCT.md)\n[![X account](https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&label=Follow%20%40diffuserslib)](https://twitter.com/diffuserslib)\n\nðŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, ðŸ¤— Diffusers is a modular toolbox that supports both."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\nWelcome to ðŸ§¨ Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how ðŸ§¨ Diffusers is meant to be used. You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want. After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications. Feel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers! Let's start diffusing!"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n\n![](https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg)\n\n# Diffusers\nðŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, ðŸ¤— Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). The library has three main components:\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in ðŸ¤— Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve. - Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality. - Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. [Tutorials\n\nLearn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you're using ðŸ¤— Diffusers for the first time!](./tutorials/tutorial_overview)\n[How-to guides\n\nPractical guides for helping you load pipelines, models, and schedulers. You'll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.](./using-diffusers/loading_overview)\n[Conceptual guides\n\nUnderstand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.](./conceptual/philosophy)\n[Reference\n\nTechnical descriptions of how ðŸ¤— Diffusers classes and methods work.](./api/models/overview)"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Philosophy\nðŸ§¨ Diffusers provides \\*\\*state-of-the-art\\*\\* pretrained diffusion models across multiple modalities. Its purpose is to serve as a \\*\\*modular toolbox\\*\\* for both inference and training. We aim at building a library that stands the test of time and therefore take API design very seriously. In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch's Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let's go over the most important ones:\n## Usability over Performance\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library. - Diffusers aims to be a \\*\\*light-weight\\*\\* package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages. - Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired. ## Simple over easy\nAs PyTorch states, \\*\\*explicit is better than implicit\\*\\* and \\*\\*simple is better than complex\\*\\*."", ""This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\nis very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline. ## Tweakable, contributor-friendly over abstraction\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t\\_repeat\\_yourself). In short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers. Functions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable. \\*\\*However\\*\\*, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions. - Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions. - Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel. At Hugging Face, we call this design the \\*\\*single-file policy\\*\\* which means that almost all of the code of a certain class should be written in a single, self-contained file.""]","The purpose of the ðŸ¤— Diffusers library is to provide state-of-the-art pretrained diffusion models across multiple modalities. It serves as a modular toolbox for both inference and training, allowing users to generate images, audio, and even 3D structures of molecules. The library is designed to be user-friendly and customizable, making it suitable for both simple inference solutions and the training of custom diffusion models.",To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.
"What method does the EulerAncestralDiscreteScheduler use for sampling?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# EulerAncestralDiscreteScheduler\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling\\_euler\\_ancestral\\_discrete.EulerAncestralDiscreteSchedulerOutput"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Schedulers\nðŸ¤— Diffusers provides many scheduler functions for the diffusion process. A scheduler takes a model's output (the sample which the diffusion process is iterating on) and a timestep to return a denoised sample. The timestep is important because it dictates where in the diffusion process the step is; data is generated by iterating forward \\*n\\* timesteps and inference occurs by propagating backward through the timesteps. Based on the timestep, a scheduler may be \\*discrete\\* in which case the timestep is an `int` or \\*continuous\\* in which case the timestep is a `float`. Depending on the context, a scheduler defines how to iteratively add noise to an image or how to update a sample based on a model's output:\n- during \\*training\\*, a scheduler adds noise (there are different algorithms for how to add noise) to a sample to train a diffusion model\n- during \\*inference\\*, a scheduler defines how to update a sample based on a pretrained model's output\nMany schedulers are implemented from the [k-diffusion](https://github.com/crowsonkb/k-diffusion) library by [Katherine Crowson](https://github.com/crowsonkb/), and they're also widely used in A1111. To help you map the schedulers from k-diffusion and A1111 to the schedulers in ðŸ¤— Diffusers, take a look at the table below:\n| A1111/k-diffusion | ðŸ¤— Diffusers | Usage |\n|---------------------|-------------------------------------|---------------------------------------------------------------------------------------------------------------|\n| DPM++ 2M | [`DPMSolverMultistepScheduler`] | |\n| DPM++ 2M Karras | [`DPMSolverMultistepScheduler`] | init with `use\\_karras\\_sigmas=True` |\n| DPM++ 2M SDE | [`DPMSolverMultistepScheduler`] | init with `algorithm\\_type=\""sde-dpmsolver++\""` |\n| DPM++ 2M SDE Karras | [`DPMSolverMultistepScheduler`] | init with `use\\_karras\\_sigmas=True` and `algorithm\\_type=\""sde-dpmsolver++\""` |\n| DPM++ 2S a | N/A | very similar to `DPMSolverSinglestepScheduler` |\n| DPM++ 2S a Karras | N/A | very similar to `DPMSolverSinglestepScheduler(use\\_karras\\_sigmas=True, ...)` |\n| DPM++ SDE | [`DPMSolverSinglestepScheduler`] | |\n| DPM++ SDE Karras | [`DPMSolverSinglestepScheduler`] | init with `use\\_karras\\_sigmas=True` |\n| DPM2 | [`KDPM2DiscreteScheduler`] | |\n| DPM2 Karras | [`KDPM2DiscreteScheduler`] | init with `use\\_karras\\_sigmas=True` |\n| DPM2 a | [`KDPM2AncestralDiscreteScheduler`] | |\n| DPM2 a Karras | [`KDPM2AncestralDiscreteScheduler`] | init with `use\\_karras\\_sigmas=True` |\n| DPM adaptive | N/A | |\n| DPM fast | N/A | |\n| Euler | [`EulerDiscreteScheduler`] | |\n| Euler a | [`EulerAncestralDiscreteScheduler`] | |\n| Heun | [`HeunDiscreteScheduler`] | |\n| LMS | [`LMSDiscreteScheduler`] | |\n| LMS Karras | [`LMSDiscreteScheduler`] | init with `use\\_karras\\_sigmas=True` |\n| N/A | [`DEISMultistepScheduler`] | |\n| N/A | [`UniPCMultistepScheduler`] | |\nAll schedulers are built from the base [`SchedulerMixin`] class which implements low level utilities shared by all schedulers. ## SchedulerMixin\n[[autodoc]] SchedulerMixin\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling\\_utils.SchedulerOutput\n## KarrasDiffusionSchedulers\n[`KarrasDiffusionSchedulers`] are a broad generalization of schedulers in ðŸ¤— Diffusers."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# EulerDiscreteScheduler\nThe Euler scheduler (Algorithm 2) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\_diffusion/sampling.py#L51) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerDiscreteScheduler\n[[autodoc]] EulerDiscreteScheduler\n## EulerDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling\\_euler\\_discrete.EulerDiscreteSchedulerOutput""]",The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling.,Ancestral sampling with Euler method steps.
"What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?
","[""Also, thanks to Niels Rogge, Younes Belkada, and Suraj Patil, among many others at Hugging Face, who laid out the foundations for increasing the use of multi-modal models from Transformers.\\*"", ""The abstract from the paper is the following:\n\\*While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a \""red bounding box\"" or \""pointed arrow\"". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available.\\*\nTips:\n- The architecture is similar than llava architecture except that the multi-modal projector takes a set of concatenated vision hidden states and has an additional layernorm layer on that module."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Image tasks with IDEFICS\n[[open-in-colab]]\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach\nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. For instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can\nsolve image-text tasks with a large multimodal model called IDEFICS. [IDEFICS](../model\\_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198),\na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image\nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content,\ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b)\nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the ðŸ¤— Hub."", ""Models such as [SimVLM](https://arxiv.org/abs/2108.10904) and [VirTex](https://arxiv.org/abs/2006.06666v3) use this pre-training objective and feature a unified multi-modal architecture consisting of a transformer encoder and transformer decoder, similar to that of an autoregressive language model. Letâ€™s break this down and see how this works. Language models with a prefix objective predict the next token given an input text as the prefix. For example, given the sequence â€œA man is standing at the cornerâ€, we can use â€œA man is standing at theâ€ as the prefix and train the model with the objective of predicting the next token - â€œcornerâ€ or another plausible continuation of the prefix. Visual transformers (ViT) apply the same concept of the prefix to images by dividing each image into a number of patches and sequentially feeding these patches to the model as inputs. Leveraging this idea, SimVLM features an architecture where the encoder receives a concatenated image patch sequence and prefix text sequence as the prefix input, and the decoder then predicts the continuation of the textual sequence. The diagram above depicts this idea. The SimVLM model is first pre-trained on a text dataset without image patches present in the prefix and then on an aligned image-text dataset. These models are used for image-conditioned text generation/captioning and VQA tasks. Models that leverage a unified multi-modal architecture to fuse visual information into a language model (LM) for image-guided tasks show impressive capabilities. However, models that solely use the PrefixLM strategy can be limited in terms of application areas as they are mainly designed for image captioning or visual question-answering downstream tasks. For example, given an image of a group of people, we can query the image to write a description of the image (e.g., â€œA group of people is standing together in front of a building and smilingâ€) or query it with questions that require visual reasoning: â€œHow many people are wearing red t-shirts?â€. On the other hand, models that learn multi-modal representations or adopt hybrid approaches can be adapted for various other downstream tasks, such as object detection and image segmentation. #### Frozen PrefixLM\n\n![Frozen PrefixLM](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/frozen_prefixlm.png)  \n*Frozen PrefixLM pre-training strategy ([image source](https://lilianweng.github.io/posts/2022-06-09-vlm))*\n\nWhile fusing visual information into a language model is highly effective, being able to use a pre-trained language model (LM) without the need for fine-tuning would be much more efficient. Hence, another pre-training objective in vision-language models is learning image embeddings that are aligned with a frozen language model. Models such as [Frozen](https://arxiv.org/abs/2106.13884) and [ClipCap](https://arxiv.org/abs/2111.09734) use this Frozen PrefixLM pre-training objective. They only update the parameters of the image encoder during training to generate image embeddings that can be used as a prefix to the pre-trained, frozen language model in a similar fashion to the PrefixLM objective discussed above. Both Frozen and ClipCap are trained on aligned image-text (caption) datasets with the objective of generating the next token in the caption, given the image embeddings and the prefix text. Finally, models such as [MAPL](https://arxiv.org/abs/2210.07179) and [Flamingo](https://arxiv.org/abs/2204.14198) keep both the pre-trained vision encoder and language model frozen. Flamingo sets a new state-of-the-art in few-shot learning on a wide range of open-ended vision and language tasks by adding Perceiver Resampler modules on top of the pre-trained frozen vision model and inserting new cross-attention layers between existing pre-trained and frozen LM layers to condition the LM on visual data. A nifty advantage of the Frozen PrefixLM pre-training objective is it enables training with limited aligned image-text data, which is particularly useful for domains where aligned multi-modal datasets are not available. ### 3) Multi-modal Fusing with Cross Attention\n\n![Cross Attention Fusing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/cross_attention_fusing.png)  \n *Fusing visual information with a cross-attention mechanism as shown ([image source](https://www.semanticscholar.org/paper/VisualGPT%3A-Data-efficient-Adaptation-of-Pretrained-Chen-Guo/616e0ed02ca024a8c1d4b86167f7486ea92a13d9))*\n\nAnother approach to leveraging pre-trained language models for multi-modal tasks is to directly fuse visual information into the layers of a language model decoder using a cross-attention mechanism instead of using images as additional prefixes to the language model."", ""Models such as [VisualGPT](https://arxiv.org/abs/2102.10407), [VC-GPT](https://arxiv.org/abs/2201.12723), and [Flamingo](https://arxiv.org/abs/2204.14198) use this pre-training strategy and are trained on image captioning and visual question-answering tasks. The main goal of such models is to balance the mixture of text generation capacity and visual information efficiently, which is highly important in the absence of large multi-modal datasets. Models such as VisualGPT use a visual encoder to embed images and feed the visual embeddings to the cross-attention layers of a pre-trained language decoder module to generate plausible captions. A more recent work, [FIBER](http://arxiv.org/abs/2206.07643), inserts cross-attention layers with a gating mechanism into both vision and language backbones, for more efficient multi-modal fusing and enables various other downstream tasks, such as image-text retrieval and open vocabulary object detection. ### 4) Masked-Language Modeling / Image-Text Matching\nAnother line of vision-language models uses a combination of Masked-Language Modeling (MLM) and Image-Text Matching (ITM) objectives to align specific parts of images with text and enable various downstream tasks such as visual question answering, visual commonsense reasoning, text-based image retrieval, and text-guided object detection. Models that follow this pre-training setup include [VisualBERT](https://arxiv.org/abs/1908.03557), [FLAVA](https://arxiv.org/abs/2112.04482), [ViLBERT](https://arxiv.org/abs/1908.02265), [LXMERT](https://arxiv.org/abs/1908.07490) and [BridgeTower](https://arxiv.org/abs/2206.08657). ![MLM / ITM](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/mlm_itm.png)  \n *Aligning parts of images with text ([image source](https://arxiv.org/abs/1908.02265))*\n\nLetâ€™s break down what MLM and ITM objectives mean. Given a partially masked caption, the MLM objective is to predict the masked words based on the corresponding image. Note that the MLM objective requires either using a richly annotated multi-modal dataset with bounding boxes or using an object detection model to generate object region proposals for parts of the input text. For the ITM objective, given an image and caption pair, the task is to predict whether the caption matches the image or not.""]",The large multimodal model that can solve image-text tasks and is based on Flamingo is called IDEFICS.,IDEFICS
"What is the purpose of the `gradio.Blocks` API?
","[""Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our \\*\\*newly developed\\*\\* low-level API called `gradio.Blocks`. Now, what's the difference between `Interface` and `Blocks`? - âš¡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs. - ðŸ§± `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \""building blocks\""). ### Why Blocks ðŸ§±?[[why-blocks-]]\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\n- Group together related demos as multiple tabs in one web application\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a dropdown) or its visibility based on user input\nWe will explore all of these concepts below. ### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook. ```py\nimport gradio as gr\ndef flip\\_text(x):\nreturn x[::-1]\ndemo = gr.Blocks()\nwith demo:\ngr.Markdown(\n\""\""\""\n# Flip Text! Start typing below to see the output. \""\""\""\n)\ninput = gr.Textbox(placeholder=\""Flip this text\"")\noutput = gr.Textbox()\ninput.change(fn=flip\\_text, inputs=input, outputs=output)\ndemo.launch()\n```\nThis simple example above introduces 4 concepts that underlie Blocks:\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context."", ""Gradio Demo: json\\_component\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.JSON(value={\""Key 1\"": \""Value 1\"", \""Key 2\"": {\""Key 3\"": \""Value 2\"", \""Key 4\"": \""Value 3\""}, \""Key 5\"": [\""Item 1\"", \""Item 2\"", \""Item 3\""]})\ndemo.launch()\n```"", ""### 7. Select all the steps necessary for adding state to your Gradio interface\n### 8. Which of the following are components included in the Gradio library? ### 9. What does Gradio `Blocks` allow you to do? ### 10. You can share a public link to a `Blocks` demo and host a `Blocks` demo on Hugging Face spaces."", ""Gradio Blocks Party[[gradio-blocks-party]]\nAlong with the release of the Gradio chapter of the course, Hugging Face hosted a community event on building cool machine learning demos using the new Gradio Blocks feature. You can find all the demos that the community created under the [`Gradio-Blocks`](https://huggingface.co/Gradio-Blocks) organisation on the Hub. Here's a few examples from the winners:\n\\*\\*Natural language to SQL\\*\\*"", ""ä½¿ç”¨ Gradio å—åƒå‡½æ•°ä¸€æ ·\nTags: TRANSLATION, HUB, SPACES\n\\*\\*å…ˆå†³æ¡ä»¶\\*\\*: æœ¬æŒ‡å—æ˜¯åœ¨å—ä»‹ç»çš„åŸºç¡€ä¸Šæž„å»ºçš„ã€‚è¯·ç¡®ä¿[å…ˆé˜…è¯»è¯¥æŒ‡å—](https://gradio.app/quickstart/#blocks-more-flexibility-and-control)ã€‚\n## ä»‹ç»\nä½ çŸ¥é“å—ï¼Œé™¤äº†ä½œä¸ºä¸€ä¸ªå…¨æ ˆæœºå™¨å­¦ä¹ æ¼”ç¤ºï¼ŒGradio å—åº”ç”¨å…¶å®žä¹Ÿæ˜¯ä¸€ä¸ªæ™®é€šçš„ Python å‡½æ•°ï¼ï¼Ÿ\nè¿™æ„å‘³ç€å¦‚æžœä½ æœ‰ä¸€ä¸ªåä¸º `demo` çš„ Gradio å—ï¼ˆæˆ–ç•Œé¢ï¼‰åº”ç”¨ï¼Œä½ å¯ä»¥åƒä½¿ç”¨ä»»ä½• Python å‡½æ•°ä¸€æ ·ä½¿ç”¨ `demo`ã€‚\næ‰€ä»¥ï¼Œåƒ `output = demo(\""Hello\"", \""friend\"")` è¿™æ ·çš„æ“ä½œä¼šåœ¨è¾“å…¥ä¸º \""Hello\"" å’Œ \""friend\"" çš„æƒ…å†µä¸‹è¿è¡Œ `demo` ä¸­å®šä¹‰çš„ç¬¬ä¸€ä¸ªäº‹ä»¶ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨å˜é‡ `output` ä¸­ã€‚\nå¦‚æžœä»¥ä¸Šå†…å®¹è®©ä½ æ‰“çžŒç¡ ðŸ¥±ï¼Œè¯·å¿è€ä¸€ä¸‹ï¼é€šè¿‡å°†åº”ç”¨ç¨‹åºåƒå‡½æ•°ä¸€æ ·ä½¿ç”¨ï¼Œä½ å¯ä»¥è½»æ¾åœ°ç»„åˆ Gradio åº”ç”¨ã€‚\næŽ¥ä¸‹æ¥çš„éƒ¨åˆ†å°†å±•ç¤ºå¦‚ä½•å®žçŽ°ã€‚\n## å°†å—è§†ä¸ºå‡½æ•°\nå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå°†è‹±æ–‡æ–‡æœ¬ç¿»è¯‘ä¸ºå¾·æ–‡æ–‡æœ¬çš„æ¼”ç¤ºå—ã€‚\n$code\\_english\\_translator\næˆ‘å·²ç»å°†å®ƒæ‰˜ç®¡åœ¨ Hugging Face Spaces ä¸Šçš„ [gradio/english\\_translator](https://huggingface.co/spaces/gradio/english\\_translator)ã€‚\nä½ ä¹Ÿå¯ä»¥åœ¨ä¸‹é¢çœ‹åˆ°æ¼”ç¤ºï¼š\n$demo\\_english\\_translator\nçŽ°åœ¨ï¼Œå‡è®¾ä½ æœ‰ä¸€ä¸ªç”Ÿæˆè‹±æ–‡æ–‡æœ¬çš„åº”ç”¨ç¨‹åºï¼Œä½†ä½ è¿˜æƒ³é¢å¤–ç”Ÿæˆå¾·æ–‡æ–‡æœ¬ã€‚\nä½ å¯ä»¥é€‰æ‹©ï¼š\n1. å°†æˆ‘çš„è‹±å¾·ç¿»è¯‘çš„æºä»£ç å¤åˆ¶ç²˜è´´åˆ°ä½ çš„åº”ç”¨ç¨‹åºä¸­ã€‚\n2. åœ¨ä½ çš„åº”ç”¨ç¨‹åºä¸­åŠ è½½æˆ‘çš„è‹±å¾·ç¿»è¯‘ï¼Œå¹¶å°†å…¶å½“ä½œæ™®é€šçš„ Python å‡½æ•°å¤„ç†ã€‚\né€‰é¡¹ 1 ä»ŽæŠ€æœ¯ä¸Šè®²æ€»æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å®ƒç»å¸¸å¼•å…¥ä¸å¿…è¦çš„å¤æ‚æ€§ã€‚\né€‰é¡¹ 2 å…è®¸ä½ å€Ÿç”¨æ‰€éœ€çš„åŠŸèƒ½ï¼Œè€Œä¸ä¼šè¿‡äºŽç´§å¯†åœ°è€¦åˆæˆ‘ä»¬çš„åº”ç”¨ç¨‹åºã€‚\nä½ åªéœ€è¦åœ¨æºæ–‡ä»¶ä¸­è°ƒç”¨ `Blocks.load` ç±»æ–¹æ³•å³å¯ã€‚\nä¹‹åŽï¼Œä½ å°±å¯ä»¥åƒä½¿ç”¨æ™®é€šçš„ Python å‡½æ•°ä¸€æ ·ä½¿ç”¨æˆ‘çš„ç¿»è¯‘åº”ç”¨ç¨‹åºäº†ï¼\nä¸‹é¢çš„ä»£ç ç‰‡æ®µå’Œæ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `Blocks.load`ã€‚\nè¯·æ³¨æ„ï¼Œå˜é‡ `english\\_translator` æ˜¯æˆ‘çš„è‹±å¾·ç¿»è¯‘åº”ç”¨ç¨‹åºï¼Œä½†å®ƒåœ¨ `generate\\_text` ä¸­åƒæ™®é€šå‡½æ•°ä¸€æ ·ä½¿ç”¨ã€‚\n$code\\_generate\\_english\\_german\n$demo\\_generate\\_english\\_german\n## å¦‚ä½•æŽ§åˆ¶ä½¿ç”¨åº”ç”¨ç¨‹åºä¸­çš„å“ªä¸ªå‡½æ•°\nå¦‚æžœä½ æ­£åœ¨åŠ è½½çš„åº”ç”¨ç¨‹åºå®šä¹‰äº†å¤šä¸ªå‡½æ•°ï¼Œä½ å¯ä»¥ä½¿ç”¨ `fn\\_index` å’Œ `api\\_name` å‚æ•°æŒ‡å®šè¦ä½¿ç”¨çš„å‡½æ•°ã€‚\nåœ¨è‹±å¾·æ¼”ç¤ºçš„ä»£ç ä¸­ï¼Œä½ ä¼šçœ‹åˆ°ä»¥ä¸‹ä»£ç è¡Œï¼š\ntranslate\\_btn.click(translate, inputs=english, outputs=german, api\\_name=\""translate-to-german\"")\nè¿™ä¸ª `api\\_name` åœ¨æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºä¸­ç»™è¿™ä¸ªå‡½æ•°ä¸€ä¸ªå”¯ä¸€çš„åç§°ã€‚ä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªåç§°å‘Šè¯‰ Gradio ä½ æƒ³ä½¿ç”¨\nä¸Šæ¸¸ç©ºé—´ä¸­çš„å“ªä¸ªå‡½æ•°ï¼š\nenglish\\_generator(text, api\\_name=\""translate-to-german\"")[0][\""generated\\_text\""]\nä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ `fn\\_index` å‚æ•°ã€‚\nå‡è®¾æˆ‘çš„åº”ç”¨ç¨‹åºè¿˜å®šä¹‰äº†ä¸€ä¸ªè‹±è¯­åˆ°è¥¿ç­ç‰™è¯­çš„ç¿»è¯‘å‡½æ•°ã€‚\nä¸ºäº†åœ¨æˆ‘ä»¬çš„æ–‡æœ¬ç”Ÿæˆåº”ç”¨ç¨‹åºä¸­ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š\nenglish\\_generator(text, fn\\_index=1)[0][\""generated\\_text\""]\nGradio ç©ºé—´ä¸­çš„å‡½æ•°æ˜¯ä»Žé›¶å¼€å§‹ç´¢å¼•çš„ï¼Œæ‰€ä»¥è¥¿ç­ç‰™è¯­ç¿»è¯‘å™¨å°†æ˜¯æˆ‘çš„ç©ºé—´ä¸­çš„ç¬¬äºŒä¸ªå‡½æ•°ï¼Œ\nå› æ­¤ä½ ä¼šä½¿ç”¨ç´¢å¼• 1ã€‚\n## ç»“è¯­\næˆ‘ä»¬å±•ç¤ºäº†å°†å—åº”ç”¨è§†ä¸ºæ™®é€š Python å‡½æ•°çš„æ–¹æ³•ï¼Œè¿™æœ‰åŠ©äºŽåœ¨ä¸åŒçš„åº”ç”¨ç¨‹åºä¹‹é—´ç»„åˆåŠŸèƒ½ã€‚\nä»»ä½•å—åº”ç”¨ç¨‹åºéƒ½å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªå‡½æ•°ï¼Œä½†ä¸€ä¸ªå¼ºå¤§çš„æ¨¡å¼æ˜¯åœ¨å°†å…¶è§†ä¸ºå‡½æ•°ä¹‹å‰ï¼Œ\nåœ¨[è‡ªå·±çš„åº”ç”¨ç¨‹åºä¸­åŠ è½½](https://huggingface.co/spaces)æ‰˜ç®¡åœ¨[Hugging Face Spaces](https://huggingface.co/spaces)ä¸Šçš„åº”ç”¨ç¨‹åºã€‚\næ‚¨ä¹Ÿå¯ä»¥åŠ è½½æ‰˜ç®¡åœ¨[Hugging Face Model Hub](https://huggingface.co/models)ä¸Šçš„æ¨¡åž‹â€”â€”æœ‰å…³ç¤ºä¾‹ï¼Œè¯·å‚é˜…[ä½¿ç”¨ Hugging Face é›†æˆ](/using\\_hugging\\_face\\_integrations)æŒ‡å—ã€‚\n### å¼€å§‹æž„å»ºï¼âš’ï¸\n## Parting Remarks\næˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°† Blocks åº”ç”¨ç¨‹åºè§†ä¸ºå¸¸è§„ Python å‡½æ•°ï¼Œä»¥ä¾¿åœ¨ä¸åŒçš„åº”ç”¨ç¨‹åºä¹‹é—´ç»„åˆåŠŸèƒ½ã€‚\nä»»ä½• Blocks åº”ç”¨ç¨‹åºéƒ½å¯ä»¥è¢«è§†ä¸ºå‡½æ•°ï¼Œä½†æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ¨¡å¼æ˜¯åœ¨å°†å…¶è§†ä¸ºè‡ªå·±åº”ç”¨ç¨‹åºçš„å‡½æ•°ä¹‹å‰ï¼Œå…ˆ`åŠ è½½`æ‰˜ç®¡åœ¨[Hugging Face Spaces](https://huggingface.co/spaces)ä¸Šçš„åº”ç”¨ç¨‹åºã€‚\næ‚¨è¿˜å¯ä»¥åŠ è½½æ‰˜ç®¡åœ¨[Hugging Face Model Hub](https://huggingface.co/models)ä¸Šçš„æ¨¡åž‹-è¯·å‚è§[ä½¿ç”¨ Hugging Face é›†æˆæŒ‡å—](/using\\_hugging\\_face\\_integrations)ä¸­çš„ç¤ºä¾‹ã€‚\n### Happy building!"", ""Gradio Demo: blocks\\_scroll\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndemo = gr.Blocks()\nwith demo:\ninp = gr.Textbox(placeholder=\""Enter text.\"")\nscroll\\_btn = gr.Button(\""Scroll\"")\nno\\_scroll\\_btn = gr.Button(\""No Scroll\"")\nbig\\_block = gr.HTML(\""\""\""\""\""\"")\nout = gr.Textbox()\nscroll\\_btn.click(lambda x: x,\ninputs=inp,\noutputs=out,\nscroll\\_to\\_output=True)\nno\\_scroll\\_btn.click(lambda x: x,\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""How to Create a Custom Chatbot with Gradio Blocks\nTags: NLP, TEXT, CHAT\nRelated spaces: https://huggingface.co/spaces/gradio/chatbot\\_streaming, https://huggingface.co/spaces/project-baize/Baize-7B,\n## Introduction\n\\*\\*Important Note\\*\\*: if you are getting started, we recommend using the `gr.ChatInterface` to create chatbots -- its a high-level abstraction that makes it possible to create beautiful chatbot applications fast, often with a single line of code. [Read more about it here](/guides/creating-a-chatbot-fast). This tutorial will show how to make chatbot UIs from scratch with Gradio's low-level Blocks API. This will give you full control over your Chatbot UI. You'll start by first creating a a simple chatbot to display text, a second one to stream text responses, and finally a chatbot that can handle media files as well. The chatbot interface that we create will look something like this:\n$demo\\_chatbot\\_streaming\n\\*\\*Prerequisite\\*\\*: We'll be using the `gradio.Blocks` class to build our Chatbot demo. You can [read the Guide to Blocks first](https://gradio.app/quickstart/#blocks-more-flexibility-and-control) if you are not already familiar with it. Also please make sure you are using the \\*\\*latest version\\*\\* version of Gradio: `pip install --upgrade gradio`. ## A Simple Chatbot Demo\nLet's start with recreating the simple demo above. As you may have noticed, our bot simply randomly responds \""How are you?\"", \""I love you\"", or \""I'm very hungry\"" to any input. Here's the code to create this with Gradio:\n$code\\_chatbot\\_simple\nThere are three Gradio components here:\n- A `Chatbot`, whose value stores the entire history of the conversation, as a list of response pairs between the user and bot. - A `Textbox` where the user can type their message, and then hit enter/submit to trigger the chatbot response\n- A `ClearButton` button to clear the Textbox and entire Chatbot history\nWe have a single function, `respond()`, which takes in the entire history of the chatbot, appends a random message, waits 1 second, and then returns the updated chat history."", ""Gradio Demo: blocks\\_update\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Markdown(\n\""\""\""\n# Animal Generator\nOnce you select a species, the detail panel should be visible."", ""### An Overview of Gradio\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do? #### Chatbots with `gr.ChatInterface`\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast). #### Custom Demos with `gr.Blocks`\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction â€” still all in Python. You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners). #### The Gradio Python & JavaScript Ecosystem\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\\* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio\\_client`): query any Gradio app programmatically in Python. \\* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript. \\* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \\* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications â€” for free!"", ""Meaning that this can be useful if you decide to [contribute to Gradio itself](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md) âœ…\n## Jupyter Notebook Magic ðŸ”®\nWhat about if you use Jupyter Notebooks (or Colab Notebooks, etc.) to develop code? We got something for you too! We've developed a \\*\\*magic command\\*\\* that will create and run a Blocks demo for you. To use this, load the gradio extension at the top of your notebook:\n`%load\\_ext gradio`\nThen, in the cell that you are developing your Gradio demo, simply write the magic command \\*\\*`%%blocks`\\*\\* at the top, and then write the layout and components like you would normally:\n```py\n%%blocks\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# Greetings {args.name}!\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\n```\nNotice that:\n- You do not need to launch your demo â€” Gradio does that for you automatically! - Every time you rerun the cell, Gradio will re-render your app on the same port and using the same underlying web server."", ""Blocks and Event Listeners\nWe briefly descirbed the Blocks class in the [Quickstart](/main/guides/quickstart#custom-demos-with-gr-blocks) as a way to build custom demos. Let's dive deeper. ## Blocks Structure\nTake a look at the demo below. $code\\_hello\\_blocks\n$demo\\_hello\\_blocks\n- First, note the `with gr.Blocks() as demo:` clause. The Blocks app code will be contained within this clause. - Next come the Components. These are the same Components used in `Interface`. However, instead of being passed to some constructor, Components are automatically added to the Blocks as they are created within the `with` clause. - Finally, the `click()` event listener. Event listeners define the data flow within the app. In the example above, the listener ties the two Textboxes together. The Textbox `name` acts as the input and Textbox `output` acts as the output to the `greet` method. This dataflow is triggered when the Button `greet\\_btn` is clicked. Like an Interface, an event listener can take multiple inputs or outputs. You can also attach event listeners using decorators - skip the `fn` argument and assign `inputs` and `outputs` directly:\n$code\\_hello\\_blocks\\_decorator\n## Event Listeners and Interactivity\nIn the example above, you'll notice that you are able to edit Textbox `name`, but not Textbox `output`. This is because any Component that acts as an input to an event listener is made interactive. However, since Textbox `output` acts only as an output, Gradio determines that it should not be made interactive. You can override the default behavior and directly configure the interactivity of a Component with the boolean `interactive` keyword argument. ```python\noutput = gr.Textbox(label=\""Output\"", interactive=True)\n```\n\\_Note\\_: What happens if a Gradio component is neither an input nor an output?"", ""Using Gradio Blocks Like Functions\nTags: TRANSLATION, HUB, SPACES\n\\*\\*Prerequisite\\*\\*: This Guide builds on the Blocks Introduction. Make sure to [read that guide first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control). ## Introduction\nDid you know that apart from being a full-stack machine learning demo, a Gradio Blocks app is also a regular-old python function!? This means that if you have a gradio Blocks (or Interface) app called `demo`, you can use `demo` like you would any python function. So doing something like `output = demo(\""Hello\"", \""friend\"")` will run the first event defined in `demo` on the inputs \""Hello\"" and \""friend\"" and store it\nin the variable `output`. If I put you to sleep ðŸ¥±, please bear with me! By using apps like functions, you can seamlessly compose Gradio apps. The following section will show how. ## Treating Blocks like functions\nLet's say we have the following demo that translates english text to german text. $code\\_english\\_translator\nI already went ahead and hosted it in Hugging Face spaces at [gradio/english\\_translator](https://huggingface.co/spaces/gradio/english\\_translator). You can see the demo below as well:\n$demo\\_english\\_translator\nNow, let's say you have an app that generates english text, but you wanted to additionally generate german text. You could either:\n1. Copy the source code of my english-to-german translation and paste it in your app. 2. Load my english-to-german translation in your app and treat it like a normal python function. Option 1 technically always works, but it often introduces unwanted complexity. Option 2 lets you borrow the functionality you want without tightly coupling our apps. All you have to do is call the `Blocks.load` class method in your source file. After that, you can use my translation app like a regular python function! The following code snippet and demo shows how to use `Blocks.load`. Note that the variable `english\\_translator` is my english to german app, but its used in `generate\\_text` like a regular function. $code\\_generate\\_english\\_german\n$demo\\_generate\\_english\\_german\n## How to control which function in the app to use\nIf the app you are loading defines more than one function, you can specify which function to use\nwith the `fn\\_index` and `api\\_name` parameters. In the code for our english to german demo, you'll see the following line:\n```python\ntranslate\\_btn.click(translate, inputs=english, outputs=german, api\\_name=\""translate-to-german\"")\n```\nThe `api\\_name` gives this function a unique name in our app. You can use this name to tell gradio which\nfunction in the upstream space you want to use:\n```python\nenglish\\_generator(text, api\\_name=\""translate-to-german\"")[0][\""generated\\_text\""]\n```\nYou can also use the `fn\\_index` parameter. Imagine my app also defined an english to spanish translation function. In order to use it in our text generation app, we would use the following code:\n```python\nenglish\\_generator(text, fn\\_index=1)[0][\""generated\\_text\""]\n```\nFunctions in gradio spaces are zero-indexed, so since the spanish translator would be the second function in my space,\nyou would use index 1."", ""Gradio Demo: dataframe\\_block-ui-test\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\ncount = gr.Slider(minimum=1, maximum=10, step=1, label=\""count\"")\ndata = gr.DataFrame(\nheaders=[\""A\"", \""B\""], col\\_count=(2, \""fixed\""), type=\""array\"", interactive=True\n)\nbtn = gr.Button(value=\""click\"")\nbtn.click(\nfn=lambda cnt: [[str(2 \\* i), str(2 \\* i + 1)] for i in range(int(cnt))],\ninputs=[count],\noutputs=[data],\n)\ndemo.launch()\n```"", ""--\ntitle: \""Gradio 3.0 is Out!\""\nthumbnail: /blog/assets/68\\_gradio\\_blocks/block-party.png\nauthors:\n- user: abidlabs\n---\n# Gradio 3.0 is Out! ### Machine Learning Demos\nMachine learning demos are an increasingly vital part of releasing a model. Demos allow anyone â€” not just ML engineers â€” to try out a model in the browser, give feedback on predictions, and build trust in the model if it performs well. More than 600,000 ML demos have been built with the Gradio library since its first version in 2019, and today, we are thrilled to announce \\*\\*Gradio 3.0\\*\\*: a ground-up redesign of the Gradio library ðŸ¥³\n### What's New in Gradio 3.0? ðŸ”¥ A complete redesign of the frontend, based on the feedback we're hearing from Gradio users:\n\\* We've switched to modern technologies (like [Svelte](https://svelte.dev/)) to build the Gradio frontend. We're seeing much smaller payloads and much faster page loads as a result! \\* We've also embranced a much cleaner design that will allow Gradio demos to fit in visually in more settings (such as being [embedded](https://discuss.huggingface.co/t/gradio-iframe-embedding/13021/9?u=abidlabs) in blog posts). ![](/blog/assets/68_gradio_blocks/lion.jpg)\n\\* We've revamped our existing components, like `Dataframe` to be more user-friendly (try dragging-and-dropping a CSV file into a Dataframe) as well as added new components, such as the `Gallery`, to allow you to build the right UI for your model. ![](/blog/assets/68_gradio_blocks/dalle.jpg)\n\\* We've added a `TabbedInterface` class which allows you to group together related demos as multiple tabs in one web app\n![](/blog/assets/68_gradio_blocks/tts.png)\nCheck out all the components you can use [on our (redesigned) docs](http://www.gradio.app/docs) ðŸ¤—! ðŸ”¥ We've created a new low-level language called \\*\\*Gradio Blocks\\*\\* that lets you build complex custom web apps, right in Python:\n![](/blog/assets/68_gradio_blocks/mindseye-lite.jpg)\nWhy did we create Blocks? Gradio demos are very easy to build, but what if you want more control over the layout of your demo, or more flexibility on how the data flows? For example, you might want to:\n\\* Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n\\* Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n\\* Change a component's properties (for example, the choices in a Dropdown) or its visibilty based on user input\nThe low-level Blocks API allows you to do all of this, right in Python. Here's an example of a Blocks demo that creates two simple demos and uses tabs to group them together:\n```python\nimport numpy as np\nimport gradio as gr\ndef flip\\_text(x):\nreturn x[::-1]\ndef flip\\_image(x):\nreturn np.fliplr(x)\nwith gr.Blocks() as demo:\ngr.Markdown(\""Flip text or image files using this demo.\"")\nwith gr.Tabs():\nwith gr.TabItem(\""Flip Text\""):\ntext\\_input = gr.Textbox()\ntext\\_output = gr.Textbox()\n# this demo runs whenever the input textbox changes\ntext\\_input.change(flip\\_text, inputs=text\\_input, outputs=text\\_output)\nwith gr.TabItem(\""Flip Image\""):\nwith gr.Row():\nimage\\_input = gr.Image()\nimage\\_output = gr.Image()\nbutton = gr.Button(\""Flip\"")\n# this demo runs whenever the button is clicked\nbutton.click(flip\\_image, inputs=image\\_input, outputs=image\\_output)\ndemo.launch()\n```\nOnce you run `launch()`, the following demo will appear:\n![](/blog/assets/68_gradio_blocks/flipper.png)\nFor a step-by-step introduction to Blocks, check out [the dedicated Blocks Guide](https://www.gradio.app/introduction\\_to\\_blocks/)\n### The Gradio Blocks Party\nWe're very excited about Gradio Blocks -- and we'd love for you to try it out -- so we are organizing a competition, \\*\\*the Gradio Blocks Party\\*\\* (ðŸ˜‰), to see who can build the best demos with Blocks."", ""Gradio Demo: hello\\_blocks\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndef greet(name):\nreturn \""Hello \"" + name + \""!\""\nwith gr.Blocks() as demo:\nname = gr.Textbox(label=\""Name\"")\noutput = gr.Textbox(label=\""Output Box\"")\ngreet\\_btn = gr.Button(\""Greet\"")\ngreet\\_btn.click(fn=greet, inputs=name, outputs=output, api\\_name=\""greet\"")\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: blocks\\_layout\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndemo = gr.Blocks()\nwith demo:\nwith gr.Row():\ngr.Image(interactive=True, scale=2)\ngr.Image()\nwith gr.Row():\ngr.Textbox(label=\""Text\"")\ngr.Number(label=\""Count\"", scale=2)\ngr.Radio(choices=[\""One\"", \""Two\""])\nwith gr.Row():\ngr.Button(\""500\"", scale=0, min\\_width=500)\ngr.Button(\""A\"", scale=0)\ngr.Button(\""grow\"")\nwith gr.Row():\ngr.Textbox()\ngr.Textbox()\ngr.Button()\nwith gr.Row():\nwith gr.Row():\nwith gr.Column():\ngr.Textbox(label=\""Text\"")\ngr.Number(label=\""Count\"")\ngr.Radio(choices=[\""One\"", \""Two\""])\ngr.Image()\nwith gr.Column():\ngr.Image(interactive=True)\ngr.Image()\ngr.Image()\ngr.Textbox(label=\""Text\"")\ngr.Number(label=\""Count\"")\ngr.Radio(choices=[\""One\"", \""Two\""])\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: blocks\\_static\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndemo = gr.Blocks()\nwith demo:\ngr.Image(\n\""https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=387&q=80\""\n)\ngr.Textbox(\""hi\"")\ngr.Number(3)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""## An Overview of Gradio\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do? ### Chatbots with `gr.ChatInterface`\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight to [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast). ### Custom Demos with `gr.Blocks`\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction â€” still all in Python. You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners). ### The Gradio Python & JavaScript Ecosystem\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\\* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio\\_client`): query any Gradio app programmatically in Python. \\* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript. \\* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \\* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications â€” for free!""]","The `gradio.Blocks` API is a low-level API that allows you to have full control over the data flows and layout of your application. It provides more flexibility compared to the high-level `Interface` API, enabling you to:

- Group together related demos as multiple tabs in one web application.
- Change the layout of your demo, specifying where the inputs and outputs are located.
- Create multi-step interfaces, where the output of one model becomes the input to the next model, or have more flexible data flows in general.
- Modify a component's properties (for example, the choices in a dropdown) or its visibility based on user input.

Overall, `gradio.Blocks` allows you to build very complex, custom applications using Python.","The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications."
"What is the purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents""?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# unCLIP\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in ðŸ¤— Diffusers comes from kakaobrain's [karlo](https://github.com/kakaobrain/karlo). The abstract from the paper is following:\n\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\*\nYou can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines."", ""Note, however, that the number of prompts we used to compute the CLIP scores is quite low. For a more practical evaluation, this number should be way higher, and the prompts should be diverse. By construction, there are some limitations in this score. The captions in the training dataset\nwere crawled from the web and extracted from `alt` and similar tags associated an image on the internet. They are not necessarily representative of what a human being would use to describe an image. Hence we\nhad to \""engineer\"" some prompts here. ### Image-conditioned text-to-image generation\nIn this case, we condition the generation pipeline with an input image as well as a text prompt. Let's take the [`StableDiffusionInstructPix2PixPipeline`], as an example. It takes an edit instruction as an input prompt and an input image to be edited. Here is one example:\n![edit-instruction](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation\\_diffusion\\_models/edit-instruction.png)\nOne strategy to evaluate such a model is to measure the consistency of the change between the two images (in [CLIP](https://huggingface.co/docs/transformers/model\\_doc/clip) space) with the change between the two image captions (as shown in [CLIP-Guided Domain Adaptation of Image Generators](https://arxiv.org/abs/2108.00946)). This is referred to as the \""\\*\\*CLIP directional similarity\\*\\*\"". - Caption 1 corresponds to the input image (image 1) that is to be edited. - Caption 2 corresponds to the edited image (image 2). It should reflect the edit instruction. Following is a pictorial overview:\n![edit-consistency](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation\\_diffusion\\_models/edit-consistency.png)\nWe have prepared a mini dataset to implement this metric. Let's first load the dataset. ```python\nfrom datasets import load\\_dataset\ndataset = load\\_dataset(\""sayakpaul/instructpix2pix-demo\"", split=\""train\"")\ndataset.features\n```\n```bash\n{'input': Value(dtype='string', id=None),\n'edit': Value(dtype='string', id=None),\n'output': Value(dtype='string', id=None),\n'image': Image(decode=True, id=None)}\n```\nHere we have:\n- `input` is a caption corresponding to the `image`."", ""It can be\ninstructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing\nfor the task, similarly to the zero-shot capabilities of GPT-2 and 3. The abstract from the paper is the following:\n\\*State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This\nrestricted form of supervision limits their generality and usability since additional labeled data is needed to specify\nany other visual concept. Learning directly from raw text about images is a promising alternative which leverages a\nmuch broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes\nwith which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400\nmillion (image, text) pairs collected from the internet. After pre-training, natural language is used to reference\nlearned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study\nthe performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks\nsuch as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need\nfor any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot\nwithout needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained\nmodel weights at this https URL.\\*\nThis model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/openai/CLIP). ## Usage tips and example\nCLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image\nclassification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text\nfeatures."", ""!--Copyright 2023 The GLIGEN Authors and The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# GLIGEN (Grounded Language-to-Image Generation)\nThe GLIGEN model was created by researchers and engineers from [University of Wisconsin-Madison, Columbia University, and Microsoft](https://github.com/gligen/GLIGEN). The [`StableDiffusionGLIGENPipeline`] and [`StableDiffusionGLIGENTextImagePipeline`] can generate photorealistic images conditioned on grounding inputs. Along with text and bounding boxes with [`StableDiffusionGLIGENPipeline`], if input images are given, [`StableDiffusionGLIGENTextImagePipeline`] can insert objects described by text at the region defined by bounding boxes. Otherwise, it'll generate an image described by the caption/prompt and insert objects described by text at the region defined by bounding boxes. It's trained on COCO2014D and COCO2014CD datasets, and the model uses a frozen CLIP ViT-L/14 text encoder to condition itself on grounding inputs. The abstract from the [paper](https://huggingface.co/papers/2301.07093) is:\n\\*Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGENâ€™s zeroshot performance on COCO and LVIS outperforms existing supervised layout-to-image baselines by a large margin.\\*\nMake sure to check out the Stable Diffusion [Tips](https://huggingface.co/docs/diffusers/en/api/pipelines/stable\\_diffusion/overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality and how to reuse pipeline components efficiently!"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Prior Transformer\nThe Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process. The abstract from the paper is:\n\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\*\n## PriorTransformer\n[[autodoc]] PriorTransformer\n## PriorTransformerOutput\n[[autodoc]] models.prior\\_transformer.PriorTransformerOutput"", ""Both the text and visual features are then projected to a latent space with identical dimension. The dot\nproduct between the projected image and text features is then used as a similar score. To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-overlapping patches,\nwhich are then linearly embedded. A [CLS] token is added to serve as representation of an entire image. The authors\nalso add absolute position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. The [`CLIPImageProcessor`] can be used to resize (or rescale) and normalize images for the model. The [`CLIPTokenizer`] is used to encode the text. The [`CLIPProcessor`] wraps\n[`CLIPImageProcessor`] and [`CLIPTokenizer`] into a single instance to both\nencode the text and prepare the images. The following example shows how to get the image-text similarity scores using\n[`CLIPProcessor`] and [`CLIPModel`]. ```python\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import CLIPProcessor, CLIPModel\n>>> model = CLIPModel.from\\_pretrained(\""openai/clip-vit-base-patch32\"")\n>>> processor = CLIPProcessor.from\\_pretrained(\""openai/clip-vit-base-patch32\"")\n>>> url = \""http://images.cocodataset.org/val2017/000000039769.jpg\""\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = processor(text=[\""a photo of a cat\"", \""a photo of a dog\""], images=image, return\\_tensors=\""pt\"", padding=True)\n>>> outputs = model(\\*\\*inputs)\n>>> logits\\_per\\_image = outputs.logits\\_per\\_image # this is the image-text similarity score\n>>> probs = logits\\_per\\_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n## Resources\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with CLIP. - [Fine tuning CLIP with Remote Sensing (Satellite) images and captions](https://huggingface.co/blog/fine-tune-clip-rsicd), a blog post about how to fine-tune CLIP with [RSICD dataset](https://github.com/201528014227051/RSICD\\_optimal) and comparison of performance changes due to data augmentation. - This [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text) shows how to train a CLIP-like vision-text dual encoder model using a pre-trained vision and text encoder using [COCO dataset](https://cocodataset.org/#home). - A [notebook](https://colab.research.google.com/drive/1tuoAC5F4sC7qid56Z0ap-stR3rwdk0ZV?usp=sharing) on how to use a pretrained CLIP for inference with beam search for image captioning. ðŸŒŽ\n\\*\\*Image retrieval\\*\\*\n- A [notebook](https://colab.research.google.com/drive/1bLVwVKpAndpEDHqjzxVPr\\_9nGrSbuOQd?usp=sharing) on image retrieval using pretrained CLIP and computing MRR(Mean Reciprocal Rank) score. ðŸŒŽ\n- A [notebook](https://colab.research.google.com/github/deep-diver/image\\_search\\_with\\_natural\\_language/blob/main/notebooks/Image\\_Search\\_CLIP.ipynb) on image retrieval and showing the similarity score. ðŸŒŽ\n- A [notebook](https://colab.research.google.com/drive/1xO-wC\\_m\\_GNzgjIBQ4a4znvQkvDoZJvH4?usp=sharing) on how to map images and texts to the same vector space using Multilingual CLIP. ðŸŒŽ\n- A [notebook](https://colab.research.google.com/github/vivien000/clip-demo/blob/master/clip.ipynb#scrollTo=uzdFhRGqiWkR) on how to run CLIP on semantic image search using [Unsplash](https://unsplash.com) and [TMBD](https://www.themoviedb.org/) datasets. ðŸŒŽ\n\\*\\*Explainability\\*\\*\n- A [notebook](https://colab.research.google.com/github/hila-chefer/Transformer-MM-Explainability/blob/main/CLIP\\_explainability.ipynb) on how to visualize similarity between input token and image segment."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Stable unCLIP\nStable unCLIP checkpoints are finetuned from [Stable Diffusion 2.1](./stable\\_diffusion/stable\\_diffusion\\_2) checkpoints to condition on CLIP image embeddings. Stable unCLIP still conditions on text embeddings. Given the two separate conditionings, stable unCLIP can be used\nfor text guided image variation. When combined with an unCLIP prior, it can also be used for full text to image generation. The abstract from the paper is:\n\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\*\n## Tips\nStable unCLIP takes `noise\\_level` as input during inference which determines how much noise is added to the image embeddings.""]","The purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents"" is to leverage robust representations of images learned by contrastive models like CLIP for image generation. The model consists of a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. This approach aims to improve image diversity with minimal loss in photorealism and caption similarity. Additionally, the decoders can produce variations of an image that preserve both its semantics and style while varying non-essential details absent from the image representation. The joint embedding space of CLIP also enables language-guided image manipulations in a zero-shot fashion.",The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.
"What command is used to install the requirements for a research project using ðŸ¤— Transformers?
","[""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Research projects\nThis folder contains various research projects using ðŸ¤— Transformers. They are not maintained and require a specific\nversion of ðŸ¤— Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice. If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder."", ""\\* [TensorFlow 2.0](https://www.tensorflow.org/install/pip) installation instructions. \\* [Flax](https://flax.readthedocs.io/en/latest/) installation instructions. ## Install with pip\nYou should install ðŸ¤— Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies. Start by creating a virtual environment in your project directory:\n```bash\npython -m venv .env\n```\nActivate the virtual environment. On Linux and MacOs:\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n```bash\n.env/Scripts/activate\n```\nNow you're ready to install ðŸ¤— Transformers with the following command:\n```bash\npip install transformers\n```\nFor CPU-support only, you can conveniently install ðŸ¤— Transformers and a deep learning library in one line. For example, install ðŸ¤— Transformers and PyTorch with:\n```bash\npip install 'transformers[torch]'\n```\nðŸ¤— Transformers and TensorFlow 2.0:\n```bash\npip install 'transformers[tf-cpu]'\n```\nM1 / ARM Users\nYou will need to install the following before installing TensorFLow 2.0\n```\nbrew install cmake\nbrew install pkg-config\n```\nðŸ¤— Transformers and Flax:\n```bash\npip install 'transformers[flax]'\n```\nFinally, check if ðŸ¤— Transformers has been properly installed by running the following command. It will download a pretrained model:\n```bash\npython -c \""from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\""\n```\nThen print out the label and score:\n```bash\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```\n## Install from source\nInstall ðŸ¤— Transformers from source with the following command:\n```bash\npip install git+https://github.com/huggingface/transformers\n```\nThis command installs the bleeding edge `main` version rather than the latest `stable` version."", ""## Additional resources\n\\* Transformers [library](https://github.com/huggingface/transformers). \\* Transformers [docs](https://huggingface.co/docs/transformers/index). \\* Share a model [guide](https://huggingface.co/docs/transformers/model\\_sharing)."", ""Set up a development environment, for instance by running the\nfollowing command:\n```bash\npython -m venv .env\nsource .env/bin/activate\npip install -e \"".[dev]\""\n```\nand return to the parent directory\n```bash\ncd .. ```\n4. We recommend adding the PyTorch version of \\*[camelcase name of model]\\* to\nTransformers. To install PyTorch, please follow the instructions [here](https://pytorch.org/get-started/locally/). \\*\\*Note:\\*\\* You don't need to have CUDA installed. Making the new model\nwork on CPU is sufficient. 5. To port \\*[camelcase name of model]\\*, you will also need access to its\noriginal repository:\n```bash\ngit clone [link to original repo].git\ncd [lowercase name of model]\npip install -e . ```\nNow you have set up a development environment to port \\*[camelcase name of model]\\*\nto ðŸ¤— Transformers. ### Run a pretrained checkpoint using the original repository\n\\*\\*3. Set up debugging environment\\*\\*\nAt first, you will work on the original \\*[camelcase name of model]\\* repository. Often, the original implementation is very \""researchy\"". Meaning that\ndocumentation might be lacking and the code can be difficult to\nunderstand. But this should be exactly your motivation to reimplement\n\\*[camelcase name of model]\\*. At Hugging Face, one of our main goals is to \\*make\npeople stand on the shoulders of giants\\* which translates here very well\ninto taking a working model and rewriting it to make it as \\*\\*accessible,\nuser-friendly, and beautiful\\*\\* as possible. This is the number-one\nmotivation to re-implement models into ðŸ¤— Transformers - trying to make\ncomplex new NLP technology accessible to \\*\\*everybody\\*\\*."", ""In all likelihood, you will need to adapt one of the example scripts, so we recommend forking and cloning the ðŸ¤— Transformers repository as follows. Doing so will allow you to share your fork of the Transformers library with your team members so that the team effectively works on the same code base. It will also automatically install the newest versions of `flax`, `jax` and `optax`. 1. Fork the [repository](https://github.com/huggingface/transformers) by\nclicking on the 'Fork' button on the repository's page. This creates a copy of the code\nunder your GitHub user account. 2. Clone your fork to your local disk, and add the base repository as a remote:\n```bash\n$ git clone https://github.com//transformers.git\n$ cd transformers\n$ git remote add upstream https://github.com/huggingface/transformers.git\n```\n3. Create a new branch to hold your development changes. This is especially useful to share code changes with your team:\n```bash\n$ git checkout -b a-descriptive-name-for-my-project\n```\n4. Set up a flax environment by running the following command in a virtual environment:\n```bash\n$ pip install -e \"".[flax]\""\n```\n(If transformers was already installed in the virtual environment, remove\nit with `pip uninstall transformers` before reinstalling it in editable\nmode with the `-e` flag.)\nIf you have already cloned that repo, you might need to `git pull` to get the most recent changes in the `datasets`\nlibrary. Running this command will automatically install `flax`, `jax` and `optax`. Next, you should also install the ðŸ¤— Datasets library. We strongly recommend installing the\nlibrary from source to profit from the most current additions during the community week. Simply run the following steps:\n```\n$ cd ~/\n$ git clone https://github.com/huggingface/datasets.git\n$ cd datasets\n$ pip install -e \"".[streaming]\""\n```\nIf you plan on contributing a specific dataset during\nthe community week, please fork the datasets repository and follow the instructions\n[here](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-create-a-pull-request)."", ""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\nSetup transformers following instructions in README.md, (I would fork first). ```bash\ngit clone git@github.com:huggingface/transformers.git\ncd transformers\npip install -e . pip install pandas GitPython wget\n```\nGet required metadata\n```\ncurl https://cdn-datasets.huggingface.co/language\\_codes/language-codes-3b2.csv > language-codes-3b2.csv\ncurl https://cdn-datasets.huggingface.co/language\\_codes/iso-639-3.csv > iso-639-3.csv\n```\nInstall Tatoeba-Challenge repo inside transformers\n```bash\ngit clone git@github.com:Helsinki-NLP/Tatoeba-Challenge.git\n```\nTo convert a few models, call the conversion script from command line:\n```bash\npython src/transformers/models/marian/convert\\_marian\\_tatoeba\\_to\\_pytorch.py --models heb-eng eng-heb --save\\_dir converted\n```\nTo convert lots of models you can pass your list of Tatoeba model names to `resolver.convert\\_models` in a python client or script. ```python\nfrom transformers.convert\\_marian\\_tatoeba\\_to\\_pytorch import TatoebaConverter\nresolver = TatoebaConverter(save\\_dir='converted')\nresolver.convert\\_models(['heb-eng', 'eng-heb'])\n```\n### Upload converted models\nSince version v3.5.0, the model sharing workflow is switched to git-based system . Refer to [model sharing doc](https://huggingface.co/transformers/main/model\\_sharing.html#model-sharing-and-uploading) for more details. To upload all converted models,\n1. Install [git-lfs](https://git-lfs.github.com/)."", ""Note that unlike the main examples these are not actively maintained, and may require specific older versions of dependencies in order to run. While we strive to present as many use cases as possible, the example scripts are just that - examples. It is expected that they won't work out-of-the-box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs. To help you with that, most of the examples fully expose the preprocessing of the data, allowing you to tweak and edit them as required. Please discuss on the [forum](https://discuss.huggingface.co/) or in an [issue](https://github.com/huggingface/transformers/issues) a feature you would like to implement in an example before submitting a PR; we welcome bug fixes, but since we want to keep the examples as simple as possible it's unlikely that we will merge a pull request adding more functionality at the cost of readability. ## Important note\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, you have to \\*\\*install the library from source\\*\\* and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/transformers\ncd transformers\npip install . ```\nThen cd in the example folder of your choice and run\n```bash\npip install -r requirements.txt\n```\nTo browse the examples corresponding to released versions of ðŸ¤— Transformers, click on the line below and then on your desired version of the library:\nExamples for older versions of ðŸ¤— Transformers\n\n* [v4.21.0](https://github.com/huggingface/transformers/tree/v4.21.0/examples)\n* [v4.20.1](https://github.com/huggingface/transformers/tree/v4.20.1/examples)\n* [v4.19.4](https://github.com/huggingface/transformers/tree/v4.19.4/examples)\n* [v4.18.0](https://github.com/huggingface/transformers/tree/v4.18.0/examples)\n* [v4.17.0](https://github.com/huggingface/transformers/tree/v4.17.0/examples)\n* [v4.16.2](https://github.com/huggingface/transformers/tree/v4.16.2/examples)\n* [v4.15.0](https://github.com/huggingface/transformers/tree/v4.15.0/examples)\n* [v4.14.1](https://github.com/huggingface/transformers/tree/v4.14.1/examples)\n* [v4.13.0](https://github.com/huggingface/transformers/tree/v4.13.0/examples)\n* [v4.12.5](https://github.com/huggingface/transformers/tree/v4.12.5/examples)\n* [v4.11.3](https://github.com/huggingface/transformers/tree/v4.11.3/examples)\n* [v4.10.3](https://github.com/huggingface/transformers/tree/v4.10.3/examples)\n* [v4.9.2](https://github.com/huggingface/transformers/tree/v4.9.2/examples)\n* [v4.8.2](https://github.com/huggingface/transformers/tree/v4.8.2/examples)\n* [v4.7.0](https://github.com/huggingface/transformers/tree/v4.7.0/examples)\n* [v4.6.1](https://github.com/huggingface/transformers/tree/v4.6.1/examples)\n* [v4.5.1](https://github.com/huggingface/transformers/tree/v4.5.1/examples)\n* [v4.4.2](https://github.com/huggingface/transformers/tree/v4.4.2/examples)\n* [v4.3.3](https://github.com/huggingface/transformers/tree/v4.3.3/examples)\n* [v4.2.2](https://github.com/huggingface/transformers/tree/v4.2.2/examples)\n* [v4.1.1](https://github.com/huggingface/transformers/tree/v4.1.1/examples)\n* [v4.0.1](https://github.com/huggingface/transformers/tree/v4.0.1/examples)\n* [v3.5.1](https://github.com/huggingface/transformers/tree/v3.5.1/examples)\n* [v3.4.0](https://github.com/huggingface/transformers/tree/v3.4.0/examples)\n* [v3.3.1](https://github.com/huggingface/transformers/tree/v3.3.1/examples)\n* [v3.2.0](https://github.com/huggingface/transformers/tree/v3.2.0/examples)\n* [v3.1.0](https://github.com/huggingface/transformers/tree/v3.1.0/examples)\n* [v3.0.2](https://github.com/huggingface/transformers/tree/v3.0.2/examples)\n* [v2.11.0](https://github.com/huggingface/transformers/tree/v2.11.0/examples)\n* [v2.10.0](https://github.com/huggingface/transformers/tree/v2.10.0/examples)\n* [v2.9.1](https://github.com/huggingface/transformers/tree/v2.9.1/examples)\n* [v2.8.0](https://github.com/huggingface/transformers/tree/v2.8.0/examples)\n* [v2.7.0](https://github.com/huggingface/transformers/tree/v2.7.0/examples)\n* [v2.6.0](https://github.com/huggingface/transformers/tree/v2.6.0/examples)\n* [v2.5.1](https://github.com/huggingface/transformers/tree/v2.5.1/examples)\n* [v2.4.0](https://github.com/huggingface/transformers/tree/v2.4.0/examples)\n* [v2.3.0](https://github.com/huggingface/transformers/tree/v2.3.0/examples)\n* [v2.2.0](https://github.com/huggingface/transformers/tree/v2.2.0/examples)\n* [v2.1.1](https://github.com/huggingface/transformers/tree/v2.1.0/examples)\n* [v2.0.0](https://github.com/huggingface/transformers/tree/v2.0.0/examples)\n* [v1.2.0](https://github.com/huggingface/transformers/tree/v1.2.0/examples)\n* [v1.1.0](https://github.com/huggingface/transformers/tree/v1.1.0/examples)\n* [v1.0.0](https://github.com/huggingface/transformers/tree/v1.0.0/examples)\nAlternatively, you can switch your cloned ðŸ¤— Transformers to a specific version (for instance with v3.5.1) with\n```bash\ngit checkout tags/v3.5.1\n```\nand run the example command as usual afterward. ## Running the Examples on Remote Hardware with Auto-Setup\n[run\\_on\\_remote.py](./run\\_on\\_remote.py) is a script that launches any example on remote self-hosted hardware,\nwith automatic hardware and environment setup."", ""Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh. 1. æƒ³è¦è²¢ç»æ–°çš„æ¨¡åž‹ï¼Ÿæˆ‘å€‘é€™è£¡æœ‰ä¸€ä»½\\*\\*è©³ç´°æŒ‡å¼•å’Œæ¨¡æ¿\\*\\*ä¾†å¼•å°Žä½ åŠ å…¥æ–°çš„æ¨¡åž‹ã€‚ä½ å¯ä»¥åœ¨ [`templates`](./templates) ç›®éŒ„ä¸­æ‰¾åˆ°å®ƒå€‘ã€‚è¨˜å¾—æŸ¥çœ‹[è²¢ç»æŒ‡å¼•](./CONTRIBUTING.md)ä¸¦åœ¨é–‹å§‹å¯« PR å‰è¯ç¹«ç¶­è­·äººå“¡æˆ–é–‹ä¸€å€‹æ–°çš„ issue ä¾†ç²å¾— feedbacksã€‚\nè¦æª¢æŸ¥æŸå€‹æ¨¡åž‹æ˜¯å¦å·²æœ‰ Flaxã€PyTorch æˆ– TensorFlow çš„å¯¦ä½œï¼Œæˆ–å…¶æ˜¯å¦åœ¨ðŸ¤— Tokenizers å‡½å¼åº«ä¸­æœ‰å°æ‡‰çš„ tokenizerï¼Œæ•¬è«‹åƒé–±[æ­¤è¡¨](https://huggingface.co/docs/transformers/index#supported-frameworks)ã€‚\né€™äº›å¯¦ä½œå‡å·²æ–¼å¤šå€‹è³‡æ–™é›†æ¸¬è©¦ï¼ˆè«‹åƒé–±ç¯„ä¾‹è…³æœ¬ï¼‰ä¸¦æ‡‰èˆ‡åŽŸç‰ˆå¯¦ä½œè¡¨ç¾ç›¸ç•¶ã€‚ä½ å¯ä»¥åœ¨ç¯„ä¾‹æ–‡ä»¶çš„[æ­¤ç¯€](https://huggingface.co/docs/transformers/examples)ä¸­äº†è§£å¯¦ä½œçš„ç´°ç¯€ã€‚\n## äº†è§£æ›´å¤š\n| ç« ç¯€ | æè¿° |\n|-|-|\n| [æ–‡ä»¶](https://huggingface.co/transformers/) | å®Œæ•´çš„ API æ–‡ä»¶å’Œæ•™å­¸ |\n| [ä»»å‹™æ¦‚è¦½](https://huggingface.co/docs/transformers/task\\_summary) | ðŸ¤— Transformers æ”¯æ´çš„ä»»å‹™ |\n| [é è™•ç†æ•™å­¸](https://huggingface.co/docs/transformers/preprocessing) | ä½¿ç”¨ `Tokenizer` ä¾†ç‚ºæ¨¡åž‹æº–å‚™è³‡æ–™ |\n| [è¨“ç·´å’Œå¾®èª¿](https://huggingface.co/docs/transformers/training) | ä½¿ç”¨ PyTorch/TensorFlow çš„å…§å»ºçš„è¨“ç·´æ–¹å¼æˆ–æ–¼ `Trainer` API ä¸­ä½¿ç”¨ ðŸ¤— Transformers æä¾›çš„æ¨¡åž‹ |\n| [å¿«é€Ÿä¸Šæ‰‹ï¼šå¾®èª¿å’Œç¯„ä¾‹è…³æœ¬](https://github.com/huggingface/transformers/tree/main/examples) | ç‚ºå„ç¨®ä»»å‹™æä¾›çš„ç¯„ä¾‹è…³æœ¬ |\n| [æ¨¡åž‹åˆ†äº«å’Œä¸Šå‚³](https://huggingface.co/docs/transformers/model\\_sharing) | ä¸Šå‚³ä¸¦èˆ‡ç¤¾ç¾¤åˆ†äº«ä½ å¾®èª¿çš„æ¨¡åž‹ |\n| [é·ç§»](https://huggingface.co/docs/transformers/migration) | å¾ž `pytorch-transformers` æˆ– `pytorch-pretrained-bert` é·ç§»åˆ° ðŸ¤— Transformers |\n## å¼•ç”¨\næˆ‘å€‘å·²å°‡æ­¤å‡½å¼åº«çš„[è«–æ–‡](https://www.aclweb.org/anthology/2020.emnlp-demos.6/)æ­£å¼ç™¼è¡¨ã€‚å¦‚æžœä½ ä½¿ç”¨äº† ðŸ¤— Transformers å‡½å¼åº«ï¼Œå¯ä»¥å¼•ç”¨ï¼š\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\ntitle = \""Transformers: State-of-the-Art Natural Language Processing\"",\nauthor = \""Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\"",\nbooktitle = \""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\"",\nmonth = oct,\nyear = \""2020\"",\naddress = \""Online\"",\npublisher = \""Association for Computational Linguistics\"",\nurl = \""https://www.aclweb.org/anthology/2020.emnlp-demos.6\"",\npages = \""38--45\""\n}\n```"", ""4. Set up a development environment by running the following command in a virtual environment:\n```bash\npip install -e \"".[dev]\""\n```\nIf ðŸ¤— Transformers was already installed in the virtual environment, remove\nit with `pip uninstall transformers` before reinstalling it in editable\nmode with the `-e` flag. Depending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\nfailure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n(PyTorch, TensorFlow and/or Flax) then do:\n```bash\npip install -e \"".[quality]\""\n```\nwhich should be enough for most use cases. 5. Develop the features in your branch. As you work on your code, you should make sure the test suite\npasses. Run the tests impacted by your changes like this:\n```bash\npytest tests/.py\n```\nFor more information about tests, check out the\n[Testing](https://huggingface.co/docs/transformers/testing) guide. ðŸ¤— Transformers relies on `black` and `ruff` to format its source code\nconsistently. After you make changes, apply automatic style corrections and code verifications\nthat can't be automated in one go with:\n```bash\nmake fixup\n```\nThis target is also optimized to only work with files modified by the PR you're working on. If you prefer to run the checks one after the other, the following command applies the\nstyle corrections:\n```bash\nmake style\n```\nðŸ¤— Transformers also uses `ruff` and a few custom scripts to check for coding mistakes. Quality\ncontrols are run by the CI, but you can run the same checks with:\n```bash\nmake quality\n```\nFinally, we have a lot of scripts to make sure we don't forget to update\nsome files when adding a new model. You can run these scripts with:\n```bash\nmake repo-consistency\n```\nTo learn more about those checks and how to fix any issues with them, check out the\n[Checks on a Pull Request](https://huggingface.co/docs/transformers/pr\\_checks) guide. If you're modifying documents under the `docs/source` directory, make sure the documentation can still be built. This check will also run in the CI when you open a pull request. To run a local check\nmake sure you install the documentation builder:\n```bash\npip install \"".[docs]\""\n```\nRun the following command from the root of the repository:\n```bash\ndoc-builder build transformers docs/source/en --build\\_dir ~/tmp/test-build\n```\nThis will build the documentation in the `~/tmp/test-build` folder where you can inspect the generated\nMarkdown files with your favorite editor."", ""!---\nCopyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Installation\nInstall ðŸ¤— Transformers for whichever deep learning library you're working with, setup your cache, and optionally configure ðŸ¤— Transformers to run offline. ðŸ¤— Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:\n\\* [PyTorch](https://pytorch.org/get-started/locally/) installation instructions."", ""- Dozens of architectures with over 60,000 pretrained models across all modalities. 1. Choose the right framework for every part of a model's lifetime:\n- Train state-of-the-art models in 3 lines of code. - Move a single model between TF2.0/PyTorch/JAX frameworks at will. - Seamlessly pick the right framework for training, evaluation, and production. 1. Easily customize a model or an example to your needs:\n- We provide examples for each architecture to reproduce the results published by its original authors. - Model internals are exposed as consistently as possible. - Model files can be used independently of the library for quick experiments. ## Why shouldn't I use transformers? - This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files. - The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops, you should use another library (possibly, [Accelerate](https://huggingface.co/docs/accelerate)). - While we strive to present as many use cases as possible, the scripts in our [examples folder](https://github.com/huggingface/transformers/tree/main/examples) are just that: examples. It is expected that they won't work out-of-the-box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs. ## Installation\n### With pip\nThis repository is tested on Python 3.8+, Flax 0.4.1+, PyTorch 1.10+, and TensorFlow 2.6+. You should install ðŸ¤— Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). First, create a virtual environment with the version of Python you're going to use and activate it. Then, you will need to install at least one of Flax, PyTorch, or TensorFlow. Please refer to [TensorFlow installation page](https://www.tensorflow.org/install/), [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation) installation pages regarding the specific installation command for your platform. When one of those backends has been installed, ðŸ¤— Transformers can be installed using pip as follows:\n```bash\npip install transformers\n```\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must [install the library from source](https://huggingface.co/docs/transformers/installation#installing-from-source). ### With conda\nSince Transformers version v4.0.0, we now have a conda channel: `huggingface`. ðŸ¤— Transformers can be installed using conda as follows:\n```shell script\nconda install -c huggingface transformers\n```\nFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Train with a script\nAlong with the ðŸ¤— Transformers [notebooks](./noteboks/README), there are also example scripts demonstrating how to train a model for a task with [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch), [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow), or [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax). You will also find scripts we've used in our [research projects](https://github.com/huggingface/transformers/tree/main/examples/research\\_projects) and [legacy examples](https://github.com/huggingface/transformers/tree/main/examples/legacy) which are mostly community contributed. These scripts are not actively maintained and require a specific version of ðŸ¤— Transformers that will most likely be incompatible with the latest version of the library. The example scripts are not expected to work out-of-the-box on every problem, and you may need to adapt the script to the problem you're trying to solve. To help you with this, most of the scripts fully expose how data is preprocessed, allowing you to edit it as necessary for your use case. For any feature you'd like to implement in an example script, please discuss it on the [forum](https://discuss.huggingface.co/) or in an [issue](https://github.com/huggingface/transformers/issues) before submitting a Pull Request. While we welcome bug fixes, it is unlikely we will merge a Pull Request that adds more functionality at the cost of readability. This guide will show you how to run an example summarization training script in [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) and [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization). All examples are expected to work with both frameworks unless otherwise specified. ## Setup\nTo successfully run the latest version of the example scripts, you have to \\*\\*install ðŸ¤— Transformers from source\\*\\* in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/transformers\ncd transformers\npip install . ```\nFor older versions of the example scripts, click on the toggle below:\nExamples for older versions of ðŸ¤— Transformers\n\n* [v4.5.1](https://github.com/huggingface/transformers/tree/v4.5.1/examples)\n* [v4.4.2](https://github.com/huggingface/transformers/tree/v4.4.2/examples)\n* [v4.3.3](https://github.com/huggingface/transformers/tree/v4.3.3/examples)\n* [v4.2.2](https://github.com/huggingface/transformers/tree/v4.2.2/examples)\n* [v4.1.1](https://github.com/huggingface/transformers/tree/v4.1.1/examples)\n* [v4.0.1](https://github.com/huggingface/transformers/tree/v4.0.1/examples)\n* [v3.5.1](https://github.com/huggingface/transformers/tree/v3.5.1/examples)\n* [v3.4.0](https://github.com/huggingface/transformers/tree/v3.4.0/examples)\n* [v3.3.1](https://github.com/huggingface/transformers/tree/v3.3.1/examples)\n* [v3.2.0](https://github.com/huggingface/transformers/tree/v3.2.0/examples)\n* [v3.1.0](https://github.com/huggingface/transformers/tree/v3.1.0/examples)\n* [v3.0.2](https://github.com/huggingface/transformers/tree/v3.0.2/examples)\n* [v2.11.0](https://github.com/huggingface/transformers/tree/v2.11.0/examples)\n* [v2.10.0](https://github.com/huggingface/transformers/tree/v2.10.0/examples)\n* [v2.9.1](https://github.com/huggingface/transformers/tree/v2.9.1/examples)\n* [v2.8.0](https://github.com/huggingface/transformers/tree/v2.8.0/examples)\n* [v2.7.0](https://github.com/huggingface/transformers/tree/v2.7.0/examples)\n* [v2.6.0](https://github.com/huggingface/transformers/tree/v2.6.0/examples)\n* [v2.5.1](https://github.com/huggingface/transformers/tree/v2.5.1/examples)\n* [v2.4.0](https://github.com/huggingface/transformers/tree/v2.4.0/examples)\n* [v2.3.0](https://github.com/huggingface/transformers/tree/v2.3.0/examples)\n* [v2.2.0](https://github.com/huggingface/transformers/tree/v2.2.0/examples)\n* [v2.1.1](https://github.com/huggingface/transformers/tree/v2.1.0/examples)\n* [v2.0.0](https://github.com/huggingface/transformers/tree/v2.0.0/examples)\n* [v1.2.0](https://github.com/huggingface/transformers/tree/v1.2.0/examples)\n* [v1.1.0](https://github.com/huggingface/transformers/tree/v1.1.0/examples)\n* [v1.0.0](https://github.com/huggingface/transformers/tree/v1.0.0/examples)\nThen switch your current clone of ðŸ¤— Transformers to a specific version, like v3.5.1 for example:\n```bash\ngit checkout tags/v3.5.1\n```\nAfter you've setup the correct library version, navigate to the example folder of your choice and install the example specific requirements:\n```bash\npip install -r requirements.txt\n```\n## Run a script\n\nThe example script downloads and preprocesses a dataset from the ðŸ¤— [Datasets](https://huggingface.co/docs/datasets/) library."", ""We strongly recommend to make use of the provided JAX/Flax examples scripts in [transformers/examples/flax](https://github.com/huggingface/transformers/tree/main/examples/flax) even if you want to train a JAX/Flax model of another github repository that is not integrated into ðŸ¤— Transformers. In all likelihood, you will need to adapt one of the example scripts, so we recommend forking and cloning the ðŸ¤— Transformers repository as follows. Doing so will allow you to share your fork of the Transformers library with your team members so that the team effectively works on the same code base. It will also automatically install the newest versions of `flax`, `jax` and `optax`. 1. Fork the [repository](https://github.com/huggingface/transformers) by\nclicking on the 'Fork' button on the repository's page. This creates a copy of the code\nunder your GitHub user account. 2. Clone your fork to your local disk, and add the base repository as a remote:\n```bash\n$ git clone https://github.com//transformers.git\n$ cd transformers\n$ git remote add upstream https://github.com/huggingface/transformers.git\n```\n3. Create a new branch to hold your development changes. This is especially useful to share code changes with your team:\n```bash\n$ git checkout -b a-descriptive-name-for-my-project\n```\n4. Set up a flax environment by running the following command in a virtual environment:\n```bash\n$ pip install -e \"".[flax]\""\n```\n(If transformers was already installed in the virtual environment, remove\nit with `pip uninstall transformers` before reinstalling it in editable\nmode with the `-e` flag.)\nIf you have already cloned that repo, you might need to `git pull` to get the most recent changes in the `datasets`\nlibrary. Running this command will automatically install `flax`, `jax` and `optax`. Next, you should also install the ðŸ¤— Datasets library. We strongly recommend installing the\nlibrary from source to profit from the most current additions during the community week. Simply run the following steps:\n```\n$ cd ~/\n$ git clone https://github.com/huggingface/datasets.git\n$ cd datasets\n$ pip install -e \"".[streaming]\""\n```\nIf you plan on contributing a specific dataset during\nthe community week, please fork the datasets repository and follow the instructions\n[here](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-create-a-pull-request)."", ""4. Set up a development environment by running the following command in a virtual environment:\n```bash\npip install -e \"".[dev]\""\n```\nIf ðŸ¤— Transformers was already installed in the virtual environment, remove\nit with `pip uninstall transformers` before reinstalling it in editable\nmode with the `-e` flag. Depending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\nfailure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n(PyTorch, TensorFlow and/or Flax) then do:\n```bash\npip install -e \"".[quality]\""\n```\nwhich should be enough for most use cases. 5. Develop the features in your branch. As you work on your code, you should make sure the test suite\npasses. Run the tests impacted by your changes like this:\n```bash\npytest tests/.py\n```\nFor more information about tests, check out the\n[Testing](https://huggingface.co/docs/transformers/testing) guide. ðŸ¤— Transformers relies on `black` and `ruff` to format its source code\nconsistently. After you make changes, apply automatic style corrections and code verifications\nthat can't be automated in one go with:\n```bash\nmake fixup\n```\nThis target is also optimized to only work with files modified by the PR you're working on. If you prefer to run the checks one after the other, the following command applies the\nstyle corrections:\n```bash\nmake style\n```\nðŸ¤— Transformers also uses `ruff` and a few custom scripts to check for coding mistakes. Quality\ncontrols are run by the CI, but you can run the same checks with:\n```bash\nmake quality\n```\nFinally, we have a lot of scripts to make sure we don't forget to update\nsome files when adding a new model. You can run these scripts with:\n```bash\nmake repo-consistency\n```\nTo learn more about those checks and how to fix any issues with them, check out the\n[Checks on a Pull Request](https://huggingface.co/docs/transformers/pr\\_checks) guide. If you're modifying documents under the `docs/source` directory, make sure the documentation can still be built. This check will also run in the CI when you open a pull request. To run a local check\nmake sure you install the documentation builder:\n```bash\npip install \"".[docs]\""\n```\nRun the following command from the root of the repository:\n```bash\ndoc-builder build transformers docs/source/en --build\\_dir ~/tmp/test-build\n```\nThis will build the documentation in the `~/tmp/test-build` folder where you can inspect the generated\nMarkdown files with your favorite editor.""]","To install the requirements for a research project using ðŸ¤— Transformers, you should run the following command inside the folder of your choice:

```bash
pip install -r requirements.txt
```",pip install -r requirements.txt
"What task does the `roberta-large-mnli` checkpoint perform?
","[""Its primary objective is to reduce the model's trainable parameters. LoRA's operation involves\nlearning a low rank update matrix while keeping the pre-trained weights frozen. ![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral/lora.png)\n## Setup\nRoBERTa has a limitatiom of maximum sequence length of 512, so we set the `MAX\\_LEN=512` for all models to ensure a fair comparison. ```python\nMAX\\_LEN = 512\nroberta\\_checkpoint = \""roberta-large\""\nmistral\\_checkpoint = \""mistralai/Mistral-7B-v0.1\""\nllama\\_checkpoint = \""meta-llama/Llama-2-7b-hf\""\n```\n## Data preparation\n### Data loading\nWe will load the dataset from Hugging Face:\n```python\nfrom datasets import load\\_dataset\ndataset = load\\_dataset(\""mehdiiraqui/twitter\\_disaster\"")\n```\nNow, let's split the dataset into training and validation datasets. Then add the test set:\n```python\nfrom datasets import Dataset\n# Split the dataset into training and validation datasets\ndata = dataset['train'].train\\_test\\_split(train\\_size=0.8, seed=42)\n# Rename the default \""test\"" split to \""validation\""\ndata['val'] = data.pop(\""test\"")\n# Convert the test dataframe to HuggingFace dataset and add it into the first dataset\ndata['test'] = dataset['test']\n```\nHere's an overview of the dataset:\n```bash\nDatasetDict({\ntrain: Dataset({\nfeatures: ['id', 'keyword', 'location', 'text', 'target'],\nnum\\_rows: 6090\n})\nval: Dataset({\nfeatures: ['id', 'keyword', 'location', 'text', 'target'],\nnum\\_rows: 1523\n})\ntest: Dataset({\nfeatures: ['id', 'keyword', 'location', 'text', 'target'],\nnum\\_rows: 3263\n})\n})\n```\nLet's check the data distribution:\n```python\nimport pandas as pd\ndata['train'].to\\_pandas().info()\ndata['test'].to\\_pandas().info()\n```\n- Train dataset\n```\nRangeIndex: 7613 entries, 0 to 7612\nData columns (total 5 columns):\n# Column Non-Null Count Dtype\n--- ------ -------------- -----\n0 id 7613 non-null int64\n1 keyword 7552 non-null object\n2 location 5080 non-null object\n3 text 7613 non-null object\n4 target 7613 non-null int64\ndtypes: int64(2), object(3)\nmemory usage: 297.5+ KB\n```\n- Test dataset\n```\nRangeIndex: 3263 entries, 0 to 3262\nData columns (total 5 columns):\n# Column Non-Null Count Dtype\n--- ------ -------------- -----\n0 id 3263 non-null int64\n1 keyword 3237 non-null object\n2 location 2158 non-null object\n3 text 3263 non-null object\n4 target 3263 non-null int64\ndtypes: int64(2), object(3)\nmemory usage: 127.6+ KB\n```\n\\*\\*Target distribution in the train dataset\\*\\*\n```\ntarget\n0 4342\n1 3271\nName: count, dtype: int64\n```\nAs the classes are not balanced, we will compute the positive and negative weights and use them for loss calculation later:\n```python\npos\\_weights = len(data['train'].to\\_pandas()) / (2 \\* data['train'].to\\_pandas().target.value\\_counts()[1])\nneg\\_weights = len(data['train'].to\\_pandas()) / (2 \\* data['train'].to\\_pandas().target.value\\_counts()[0])\n```\nThe final weights are:\n```\nPOS\\_WEIGHT, NEG\\_WEIGHT = (1.1637114032405993, 0.8766697374481806)\n```\nThen, we compute the maximum length of the column text:\n```python\n# Number of Characters\nmax\\_char = data['train'].to\\_pandas()['text'].str.len().max()\n# Number of Words\nmax\\_words = data['train'].to\\_pandas()['text'].str.split().str.len().max()\n```\n```\nThe maximum number of characters is 152."", ""!-- DISABLE-FRONTMATTER-SECTIONS -->\n# End-of-chapter quiz[[end-of-chapter-quiz]]\nThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood. First, though, let's test what you learned in this chapter! ### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform? roberta-large-mnli page.\""\n},\n{\ntext: \""Text classification\"",\nexplain: \""More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) â€” a task also called *natural language inference*.\"",\ncorrect: true\n},\n{\ntext: \""Text generation\"",\nexplain: \""Look again on the [roberta-large-mnli page](\\\""https://huggingface.co/roberta-large-mnli\\\"").\""\n}\n]}\n/>\n### 2. What will the following code return? ```py\nfrom transformers import pipeline\nner = pipeline(\""ner\"", grouped\\_entities=True)\nner(\""My name is Sylvain and I work at Hugging Face in Brooklyn.\"")\n```\nsentiment-analysis pipeline.\""\n},\n{\ntext: \""It will return a generated text completing this sentence.\"",\nexplain: \""This is incorrect â€” it would be a `text-generation` pipeline.\"",\n},\n{\ntext: \""It will return the words representing persons, organizations or locations.\"",\nexplain: \""Furthermore, with `grouped_entities=True`, it will group together the words belonging to the same entity, like \\\""Hugging Face\\\"".\"",\ncorrect: true\n}\n]}\n/>\n### 3."", ""From the performance results, we can see that RoBERTa is outperforming Mistral 7B and Llama 2 by a large margin. This raises the question about whether we really need a complex and large LLM for tasks like short-sequence binary classification? One learning we can draw from this study is that one should account for the specific project requirements, available resources, and performance needs to choose the LLMs model to use. Also, for relatively \\*simple\\* prediction tasks with short sequences base models such as RoBERTa remain competitive. Finally, we showcase that LoRa method can be applied to both encoder (RoBERTa) and decoder (Llama 2 and Mistral 7B) models. ## Resources\n1. You can find the code script in the following [Github project](https://github.com/mehdiir/Roberta-Llama-Mistral/). 2. You can check the hyper-param search results in the following Weight&Bias reports:\n- [RoBERTa](https://api.wandb.ai/links/mehdi-iraqui/505c22j1)\n- [Mistral 7B](https://api.wandb.ai/links/mehdi-iraqui/24vveyxp)\n- [Llama 2](https://api.wandb.ai/links/mehdi-iraqui/qq8beod0)"", ""It is identical to using the `--encoder-normalize-before` flag in [fairseq](https://fairseq.readthedocs.io/). The abstract from the paper is the following:\n\\*fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs.\\*\nThis model was contributed by [andreasmaden](https://huggingface.co/andreasmadsen). The original code can be found [here](https://github.com/princeton-nlp/DinkyTrain). ## Usage tips\n- The implementation is the same as [Roberta](roberta) except instead of using \\_Add and Norm\\_ it does \\_Norm and Add\\_. \\_Add\\_ and \\_Norm\\_ refers to the Addition and LayerNormalization as described in [Attention Is All You Need](https://arxiv.org/abs/1706.03762). - This is identical to using the `--encoder-normalize-before` flag in [fairseq](https://fairseq.readthedocs.io/). ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Causal language modeling task guide](../tasks/language\\_modeling)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## RobertaPreLayerNormConfig\n[[autodoc]] RobertaPreLayerNormConfig\n\n## RobertaPreLayerNormModel\n[[autodoc]] RobertaPreLayerNormModel\n- forward\n## RobertaPreLayerNormForCausalLM\n[[autodoc]] RobertaPreLayerNormForCausalLM\n- forward\n## RobertaPreLayerNormForMaskedLM\n[[autodoc]] RobertaPreLayerNormForMaskedLM\n- forward\n## RobertaPreLayerNormForSequenceClassification\n[[autodoc]] RobertaPreLayerNormForSequenceClassification\n- forward\n## RobertaPreLayerNormForMultipleChoice\n[[autodoc]] RobertaPreLayerNormForMultipleChoice\n- forward\n## RobertaPreLayerNormForTokenClassification\n[[autodoc]] RobertaPreLayerNormForTokenClassification\n- forward\n## RobertaPreLayerNormForQuestionAnswering\n[[autodoc]] RobertaPreLayerNormForQuestionAnswering\n- forward\n\n## TFRobertaPreLayerNormModel\n[[autodoc]] TFRobertaPreLayerNormModel\n- call\n## TFRobertaPreLayerNormForCausalLM\n[[autodoc]] TFRobertaPreLayerNormForCausalLM\n- call\n## TFRobertaPreLayerNormForMaskedLM\n[[autodoc]] TFRobertaPreLayerNormForMaskedLM\n- call\n## TFRobertaPreLayerNormForSequenceClassification\n[[autodoc]] TFRobertaPreLayerNormForSequenceClassification\n- call\n## TFRobertaPreLayerNormForMultipleChoice\n[[autodoc]] TFRobertaPreLayerNormForMultipleChoice\n- call\n## TFRobertaPreLayerNormForTokenClassification\n[[autodoc]] TFRobertaPreLayerNormForTokenClassification\n- call\n## TFRobertaPreLayerNormForQuestionAnswering\n[[autodoc]] TFRobertaPreLayerNormForQuestionAnswering\n- call\n\n## FlaxRobertaPreLayerNormModel\n[[autodoc]] FlaxRobertaPreLayerNormModel\n- \\_\\_call\\_\\_\n## FlaxRobertaPreLayerNormForCausalLM\n[[autodoc]] FlaxRobertaPreLayerNormForCausalLM\n- \\_\\_call\\_\\_\n## FlaxRobertaPreLayerNormForMaskedLM\n[[autodoc]] FlaxRobertaPreLayerNormForMaskedLM\n- \\_\\_call\\_\\_\n## FlaxRobertaPreLayerNormForSequenceClassification\n[[autodoc]] FlaxRobertaPreLayerNormForSequenceClassification\n- \\_\\_call\\_\\_\n## FlaxRobertaPreLayerNormForMultipleChoice\n[[autodoc]] FlaxRobertaPreLayerNormForMultipleChoice\n- \\_\\_call\\_\\_\n## FlaxRobertaPreLayerNormForTokenClassification\n[[autodoc]] FlaxRobertaPreLayerNormForTokenClassification\n- \\_\\_call\\_\\_\n## FlaxRobertaPreLayerNormForQuestionAnswering\n[[autodoc]] FlaxRobertaPreLayerNormForQuestionAnswering\n- \\_\\_call\\_\\_"", ""As a result, the first word of the sentence is encoded differently if it is not preceded by a white space. To ensure the first word includes a space, we set `add\\_prefix\\_space=True`. Also, to maintain consistent pre-processing for all three models, we set the parameter to 'True' for Llama 2 and Mistral 7b. - Define the preprocessing function for converting one row of the dataframe:\n```python\ndef roberta\\_preprocessing\\_function(examples):\nreturn roberta\\_tokenizer(examples['text'], truncation=True, max\\_length=MAX\\_LEN)\n```\nBy applying the preprocessing function to the first example of our training dataset, we have the tokenized inputs (`input\\_ids`) and the attention mask:\n```python\nroberta\\_preprocessing\\_function(data['train'][0])\n```\n```\n{'input\\_ids': [0, 127, 2373, 2490, 4, 1205, 640, 90, 4, 876, 73, 118, 725, 398, 13083, 329, 398, 119, 1343, 246, 2], 'attention\\_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n```\n- Now, let's apply the preprocessing function to the entire dataset:\n```python\ncol\\_to\\_delete = ['id', 'keyword','location', 'text']\n# Apply the preprocessing function and remove the undesired columns\nroberta\\_tokenized\\_datasets = data.map(roberta\\_preprocessing\\_function, batched=True, remove\\_columns=col\\_to\\_delete)\n# Rename the target to label as for HugginFace standards\nroberta\\_tokenized\\_datasets = roberta\\_tokenized\\_datasets.rename\\_column(\""target\"", \""label\"")\n# Set to torch format\nroberta\\_tokenized\\_datasets.set\\_format(\""torch\"")\n```\n\\*\\*Note:\\*\\* we deleted the undesired columns from our data: id, keyword, location and text. We have deleted the text because we have already converted it into the inputs ids and the attention mask:\nWe can have a look into our tokenized training dataset:\n```python\nroberta\\_tokenized\\_datasets['train'][0]\n```\n```\n{'label': tensor(0),\n'input\\_ids': tensor([ 0, 127, 2373, 2490, 4, 1205, 640, 90, 4, 876,\n73, 118, 725, 398, 13083, 329, 398, 119, 1343, 246,\n2]),\n'attention\\_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n```\n- For generating the training batches, we also need to pad the rows of a given batch to the maximum length found in the batch. For that, we will use the `DataCollatorWithPadding` class:\n```python\n# Data collator for padding a batch of examples to the maximum length seen in the batch\nfrom transformers import DataCollatorWithPadding\nroberta\\_data\\_collator = DataCollatorWithPadding(tokenizer=roberta\\_tokenizer)\n```\nYou can follow the same steps for preparing the data for Mistral 7B and Llama 2 models:\n\\*\\*Note\\*\\* that Llama 2 and Mistral 7B don't have a default `pad\\_token\\_id`. So, we use the `eos\\_token\\_id` for padding as well. - Mistral 7B:\n```python\n# Load Mistral 7B Tokenizer\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nmistral\\_tokenizer = AutoTokenizer.from\\_pretrained(mistral\\_checkpoint, add\\_prefix\\_space=True)\nmistral\\_tokenizer.pad\\_token\\_id = mistral\\_tokenizer.eos\\_token\\_id\nmistral\\_tokenizer.pad\\_token = mistral\\_tokenizer.eos\\_token\ndef mistral\\_preprocessing\\_function(examples):\nreturn mistral\\_tokenizer(examples['text'], truncation=True, max\\_length=MAX\\_LEN)\nmistral\\_tokenized\\_datasets = data.map(mistral\\_preprocessing\\_function, batched=True, remove\\_columns=col\\_to\\_delete)\nmistral\\_tokenized\\_datasets = mistral\\_tokenized\\_datasets.rename\\_column(\""target\"", \""label\"")\nmistral\\_tokenized\\_datasets.set\\_format(\""torch\"")\n# Data collator for padding a batch of examples to the maximum length seen in the batch\nmistral\\_data\\_collator = DataCollatorWithPadding(tokenizer=mistral\\_tokenizer)\n```\n- Llama 2:\n```python\n# Load Llama 2 Tokenizer\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nllama\\_tokenizer = AutoTokenizer.from\\_pretrained(llama\\_checkpoint, add\\_prefix\\_space=True)\nllama\\_tokenizer.pad\\_token\\_id = llama\\_tokenizer.eos\\_token\\_id\nllama\\_tokenizer.pad\\_token = llama\\_tokenizer.eos\\_token\ndef llama\\_preprocessing\\_function(examples):\nreturn llama\\_tokenizer(examples['text'], truncation=True, max\\_length=MAX\\_LEN)\nllama\\_tokenized\\_datasets = data.map(llama\\_preprocessing\\_function, batched=True, remove\\_columns=col\\_to\\_delete)\nllama\\_tokenized\\_datasets = llama\\_tokenized\\_datasets.rename\\_column(\""target\"", \""label\"")\nllama\\_tokenized\\_datasets.set\\_format(\""torch\"")\n# Data collator for padding a batch of examples to the maximum length seen in the batch\nllama\\_data\\_collator = DataCollatorWithPadding(tokenizer=llama\\_tokenizer)\n```\nNow that we have prepared the tokenized datasets, the next section will showcase how to load the pre-trained LLMs checkpoints and how to set the LoRa weights. ## Models\n### RoBERTa\n#### Load RoBERTa Checkpoints for the Classification Task\nWe load the pre-trained RoBERTa model with a sequence classification head using the Hugging Face `AutoModelForSequenceClassification` class:\n```python\nfrom transformers import AutoModelForSequenceClassification\nroberta\\_model = AutoModelForSequenceClassification.from\\_pretrained(roberta\\_checkpoint, num\\_labels=2)\n```\n#### LoRA setup for RoBERTa classifier\nWe import LoRa configuration and set some parameters for RoBERTa classifier:\n- TaskType: Sequence classification\n- r(rank): Rank for our decomposition matrices\n- lora\\_alpha: Alpha parameter to scale the learned weights."", ""- Uses RoBERTa tricks on the XLM approach, but does not use the translation language modeling objective. It only uses masked language modeling on sentences coming from one language. ## Resources\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with XLM-RoBERTa. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. - A blog post on how to [finetune XLM RoBERTa for multiclass classification with Habana Gaudi on AWS](https://www.philschmid.de/habana-distributed-training)\n- [`XLMRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\_classification.ipynb). - [`TFXLMRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\_classification-tf.ipynb). - [`FlaxXLMRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\_classification\\_flax.ipynb). - [Text classification](https://huggingface.co/docs/transformers/tasks/sequence\\_classification) chapter of the ðŸ¤— Hugging Face Task Guides. - [Text classification task guide](../tasks/sequence\\_classification)\n- [`XLMRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token\\_classification.ipynb). - [`TFXLMRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token\\_classification-tf.ipynb). - [`FlaxXLMRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification). - [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the ðŸ¤— Hugging Face Course. - [Token classification task guide](../tasks/token\\_classification)\n- [`XLMRobertaForCausalLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language\\_modeling.ipynb). - [Causal language modeling](https://huggingface.co/docs/transformers/tasks/language\\_modeling) chapter of the ðŸ¤— Hugging Face Task Guides. - [Causal language modeling task guide](../tasks/language\\_modeling)\n- [`XLMRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language\\_modeling.ipynb). - [`TFXLMRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run\\_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language\\_modeling-tf.ipynb). - [`FlaxXLMRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked\\_language\\_modeling\\_flax.ipynb). - [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the ðŸ¤— Hugging Face Course. - [Masked language modeling](../tasks/masked\\_language\\_modeling)\n- [`XLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question\\_answering.ipynb). - [`TFXLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question\\_answering-tf.ipynb). - [`FlaxXLMRobertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering). - [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the ðŸ¤— Hugging Face Course. - [Question answering task guide](../tasks/question\\_answering)\n\\*\\*Multiple choice\\*\\*\n- [`XLMRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice.ipynb). - [`TFXLMRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice-tf.ipynb). - [Multiple choice task guide](../tasks/multiple\\_choice)\nðŸš€ Deploy\n- A blog post on how to [Deploy Serverless XLM RoBERTa on AWS Lambda](https://www.philschmid.de/multilingual-serverless-xlm-roberta-with-huggingface)."", ""--\ntitle: \""Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with Lora\""\nthumbnail: /blog/assets/Lora-for-sequence-classification-with-Roberta-Llama-Mistral/Thumbnail.png\nauthors:\n- user: mehdiiraqui\nguest: true\n---\n# Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with Lora\n- [Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with LoRA](#comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-2-and-mistral-for-disaster-tweets-analysis-with-lora)\n- [Introduction](#introduction)\n- [Hardware Used](#hardware-used)\n- [Goals](#goals)\n- [Dependencies](#dependencies)\n- [Pre-trained Models](#pre-trained-models)\n- [RoBERTa](#roberta)\n- [Llama 2](#llama-2)\n- [Mistral 7B](#mistral-7b)\n- [LoRA](#lora)\n- [Setup](#setup)\n- [Data preparation](#data-preparation)\n- [Data loading](#data-loading)\n- [Data Processing](#data-processing)\n- [Models](#models)\n- [RoBERTa](#roberta)\n- [Load RoBERTA Checkpoints for the Classification Task](#load-roberta-checkpoints-for-the-classification-task)\n- [LoRA setup for RoBERTa classifier](#lora-setup-for-roberta-classifier)\n- [Mistral](#mistral)\n- [Load checkpoints for the classfication model](#load-checkpoints-for-the-classfication-model)\n- [LoRA setup for Mistral 7B classifier](#lora-setup-for-mistral-7b-classifier)\n- [Llama 2](#llama-2)\n- [Load checkpoints for the classification mode](#load-checkpoints-for-the-classfication-mode)\n- [LoRA setup for Llama 2 classifier](#lora-setup-for-llama-2-classifier)\n- [Setup the trainer](#setup-the-trainer)\n- [Evaluation Metrics](#evaluation-metrics)\n- [Custom Trainer for Weighted Loss](#custom-trainer-for-weighted-loss)\n- [Trainer Setup](#trainer-setup)\n- [RoBERTa](#roberta)\n- [Mistral-7B](#mistral-7b)\n- [Llama 2](#llama-2)\n- [Hyperparameter Tuning](#hyperparameter-tuning)\n- [Results](#results)\n- [Conclusion](#conclusion)\n- [Resources](#resources)\n## Introduction\nIn the fast-moving world of Natural Language Processing (NLP), we often find ourselves comparing different language models to see which one works best for specific tasks."", ""Refer to this page for usage examples. ## Resources\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with RoBERTa. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. - A blog on [Getting Started with Sentiment Analysis on Twitter](https://huggingface.co/blog/sentiment-analysis-twitter) using RoBERTa and the [Inference API](https://huggingface.co/inference-api). - A blog on [Opinion Classification with Kili and Hugging Face AutoTrain](https://huggingface.co/blog/opinion-classification-with-kili) using RoBERTa. - A notebook on how to [finetune RoBERTa for sentiment analysis](https://colab.research.google.com/github/DhavalTaunk08/NLP\\_scripts/blob/master/sentiment\\_analysis\\_using\\_roberta.ipynb). ðŸŒŽ\n- [`RobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\_classification.ipynb). - [`TFRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\_classification-tf.ipynb). - [`FlaxRobertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text\\_classification\\_flax.ipynb). - [Text classification task guide](../tasks/sequence\\_classification)\n- [`RobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token\\_classification.ipynb). - [`TFRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token\\_classification-tf.ipynb). - [`FlaxRobertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification). - [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the ðŸ¤— Hugging Face Course. - [Token classification task guide](../tasks/token\\_classification)\n- A blog on [How to train a new language model from scratch using Transformers and Tokenizers](https://huggingface.co/blog/how-to-train) with RoBERTa. - [`RobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language\\_modeling.ipynb). - [`TFRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run\\_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language\\_modeling-tf.ipynb). - [`FlaxRobertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked\\_language\\_modeling\\_flax.ipynb).""]","The `roberta-large-mnli` checkpoint performs text classification, specifically classifying if two sentences are logically linked across three labels: contradiction, neutral, and entailment. This task is also known as natural language inference.",Text classification
"What service is replacing the Paid tier of the Inference API at Hugging Face?
","[""Access ðŸ¤— Inference Endpoints\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account. You can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method. There are two pricing plans:\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. - There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested! After you've added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying!"", ""The ability to use Hugging Faceâ€™s comprehensive hub of foundation models means that users can pick the models that best align with their business needs without having to invest in the resources required to train them. This integration is a significant step forward in making foundation models more accessible to enterprises around the world. \\_If youâ€™re interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!\\_"", ""Benefit from higher rate limits, custom accelerated endpoints for the latest models, and early access to features. If you've built some exciting projects with the Inference API or are looking for a model not available in Inference for PROs, please [use this discussion](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/13). [Enterprise users](https://huggingface.co/enterprise) also benefit from PRO Inference API on top of other features, such as SSO. ## FAQ\n\n**Does this affect the free Inference API?**\n\nNo. We still expose thousands of models through free APIs that allow people to prototype and explore model capabilities quickly. **Does this affect Enterprise users?**\n\nUsers with an Enterprise subscription also benefit from accelerated inference API for curated models. **Can I use my own models with PRO Inference API?**\n\nThe free Inference API already supports a wide range of small and medium models from a variety of libraries (such as diffusers, transformers, and sentence transformers). If you have a custom model or custom inference logic, we recommend using [Inference Endpoints](https://ui.endpoints.huggingface.co/catalog)."", ""## Can I send large volumes of requests? Can I get accelerated APIs? If you are interested in accelerated inference, higher volumes of requests, or an SLA, please contact us at `api-enterprise at huggingface.co`. ## How can I see my usage? You can head to the [Inference API dashboard](https://api-inference.huggingface.co/dashboard/). Learn more about it in the [Inference API documentation](https://huggingface.co/docs/api-inference/usage). ## Is there programmatic access to the Inference API? Yes, the `huggingface\\_hub` library has a client wrapper documented [here](https://huggingface.co/docs/huggingface\\_hub/how-to-inference)."", ""--\ntitle: Introducing our new pricing\nthumbnail: /blog/assets/114\\_pricing-update/thumbnail.png\nauthors:\n- user: sbrandeis\n- user: pierric\n---\n# Introducing our new pricing\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently. First of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints). Along with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations). Your billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records. \\*\\*TL;DR\\*\\*: \\*\\*At HF we monetize by providing simple access to compute for AI\\*\\*, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub. [Read more](https://huggingface.co/docs/hub/billing) about our pricing and billing system. If you have any questions, feel free to reach out. We welcome your feedback ðŸ”¥"", ""Inference Endpoints\nInference Endpoints provides a secure production solution to easily deploy models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the [Hub](https://huggingface.co/models). This page is a reference for `huggingface\\_hub`'s integration with Inference Endpoints. For more information about the Inference Endpoints product, check out its [official documentation](https://huggingface.co/docs/inference-endpoints/index). Check out the [related guide](../guides/inference\\_endpoints) to learn how to use `huggingface\\_hub` to manage your Inference Endpoints programmatically. Inference Endpoints can be fully managed via API. The endpoints are documented with [Swagger](https://api.endpoints.huggingface.cloud/). The [`InferenceEndpoint`] class is a simple wrapper built on top on this API. ## Methods\nA subset of the Inference Endpoint features are implemented in [`HfApi`]:\n- [`get\\_inference\\_endpoint`] and [`list\\_inference\\_endpoints`] to get information about your Inference Endpoints\n- [`create\\_inference\\_endpoint`], [`update\\_inference\\_endpoint`] and [`delete\\_inference\\_endpoint`] to deploy and manage Inference Endpoints\n- [`pause\\_inference\\_endpoint`] and [`resume\\_inference\\_endpoint`] to pause and resume an Inference Endpoint\n- [`scale\\_to\\_zero\\_inference\\_endpoint`] to manually scale an Endpoint to 0 replicas\n## InferenceEndpoint\nThe main dataclass is [`InferenceEndpoint`]."", ""Once I'm done testing, I delete the endpoints that I've created to avoid unwanted charges. I also delete the VPC Endpoint in the AWS console. Hugging Face customers are already using Inference Endpoints. For example, [Phamily](https://phamily.com/), the #1 in-house chronic care management & proactive care platform, [told us](https://www.youtube.com/watch?v=20C9X5OYO2Q) that Inference Endpoints is helping them simplify and accelerate HIPAA-compliant Transformer deployments. ### Now it's your turn! Thanks to Inference Endpoints, you can deploy production-grade, scalable, secure endpoints in minutes, in just a few clicks. Why don't you [give it a try](https://ui.endpoints.huggingface.co/new)? We have plenty of ideas to make the service even better, and we'd love to hear your feedback in the [Hugging Face forum](https://discuss.huggingface.co/). Thank you for reading and have fun with Inference Endpoints!"", ""Firstly, we want a managed solution to deployment, we donâ€™t have a dedicated MLOPs team (yet), so weâ€™re looking for a solution that helps us minimize the time we spend on deploying models, even if it costs a little more than handling the deployments ourselves. Inference Endpoints are more expensive that what we were doing before, thereâ€™s an increased cost of between 24% and 50%. At the scale weâ€™re currently operating, this additional cost, a difference of ~$60 a month for a large CPU instance is nothing compared to the time and cognitive load we are saving by not having to worry about APIs, and containers. If we were deploying 100s of ML microservices we would probably want to think again, but that is probably true of many approaches to hosting. ## Some notes and caveats:\n- You can find pricing for Inference Endpoints [here](https://huggingface.co/pricing#endpoints), but a different number is displayed when you deploy a new endpoint from the [GUI](https://ui.endpoints.huggingface.co/new). Iâ€™ve used the latter, which is higher. - The values that I present in the table for ECS + Fargate are an underestimate, but probably not by much. I extracted them from the [fargate pricing page](https://aws.amazon.com/fargate/pricing/) and it includes just the cost of hosting the instance. Iâ€™m not including the data ingress/egress (probably the biggest thing is downloading the model from Hugging Face hub), nor have I included the costs related to ECR. ## Other considerations\n### Deployment Options\nCurrently you can deploy an Inference Endpoint from the [GUI](https://ui.endpoints.huggingface.co/new) or using a [RESTful API](https://huggingface.co/docs/inference-endpoints/api\\_reference). You can also make use of our command line tool [hugie](https://github.com/MantisAI/hfie) (which will be the subject of a future blog) to launch Inference Endpoints in one line of code by passing a configuration, itâ€™s really this simple:\n```bash\nhugie endpoint create example/development.json\n```\nFor me, whatâ€™s lacking is a [custom terraform provider](https://www.hashicorp.com/blog/writing-custom-terraform-providers). Itâ€™s all well and good deploying an inference endpoint from a [GitHub action](https://github.com/features/actions) using hugie, as we do, but it would be better if we could use the awesome state machine that is terraform to keep track of these. Iâ€™m pretty sure that someone (if not Hugging Face) will write one soon enough â€” if not, we will. ### Hosting multiple models on a single endpoint\nPhilipp Schmid posted a really nice blog about how to write a custom [Endpoint Handler](https://www.philschmid.de/multi-model-inference-endpoints) class to allow you to host multiple models on a single endpoint, potentially saving you quite a bit of money. His blog was about GPU inference, and the only real limitation is how many models you can fit into the GPU memory. I assume this will also work for CPU instances, though Iâ€™ve not tried yet. ## To concludeâ€¦\nWe find Hugging Face Inference Endpoints to be a very simple and convenient way to deploy transformer (and [sklearn](https://huggingface.co/scikit-learn)) models into an endpoint so they can be consumed by an application. Whilst they cost a little more than the ECS approach we were using before, itâ€™s well worth it because it saves us time on thinking about deployment, we can concentrate on the thing we want to: building NLP solutions for our clients to help solve their problems. \\_If youâ€™re interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!\\_\n\\_This article was originally published on February 15, 2023 [in Medium](https://medium.com/mantisnlp/why-were-switching-to-hugging-face-inference-endpoints-and-maybe-you-should-too-829371dcd330).\\_"", ""--\ntitle: Getting Started with Hugging Face Inference Endpoints\nthumbnail: /blog/assets/109\\_inference\\_endpoints/endpoints05.png\nauthors:\n- user: juliensimon\n---\n# Getting Started with Hugging Face Inference Endpoints\nTraining machine learning models has become quite simple, especially with the rise of pre-trained models and transfer learning. OK, sometimes it's not \\*that\\* simple, but at least, training models will never break critical applications, and make customers unhappy about your quality of service. Deploying models, however... Yes, we've all been there. Deploying models in production usually requires jumping through a series of hoops. Packaging your model in a container, provisioning the infrastructure, creating your prediction API, securing it, scaling it, monitoring it, and more. Let's face it: building all this plumbing takes valuable time away from doing actual machine learning work. Unfortunately, it can also go awfully wrong. We strive to fix this problem with the newly launched Hugging Face [Inference Endpoints](https://huggingface.co/inference-endpoints). In the spirit of making machine learning ever simpler without compromising on state-of-the-art quality, we've built a service that lets you deploy machine learning models directly from the [Hugging Face hub](https://huggingface.co) to managed infrastructure on your favorite cloud in just a few clicks."", ""Each task in the [`InferenceClient`] comes with a recommended\nmodel. Be aware that the HF recommendation can change over time without prior notice. Therefore it is best to explicitly\nset a model once you are decided. Also, in most cases you'll be interested in finding a model specific to \\_your\\_ needs. Visit the [Models](https://huggingface.co/models) page on the Hub to explore your possibilities. ### Using a specific URL\nThe examples we saw above use the free-hosted Inference API. This proves to be very useful for prototyping\nand testing things quickly. Once you're ready to deploy your model to production, you'll need to use a dedicated infrastructure. That's where [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) comes into play. It allows you to deploy\nany model and expose it as a private API. Once deployed, you'll get a URL that you can connect to using exactly the same\ncode as before, changing only the `model` parameter:\n```python\n>>> from huggingface\\_hub import InferenceClient\n>>> client = InferenceClient(model=\""https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/deepfloyd-if\"")\n# or\n>>> client = InferenceClient()\n>>> client.text\\_to\\_image(..., model=\""https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/deepfloyd-if\"")\n```\n### Authentication\nCalls made with the [`InferenceClient`] can be authenticated using a [User Access Token](https://huggingface.co/docs/hub/security-tokens)."", ""The model is loaded on-demand on our servers and unloaded when it's not needed anymore. You don't have to write any code and the feature is free. What's not to love? ## Free Inference API\nThe [Inference API](https://huggingface.co/docs/api-inference/) is what powers the Inference widget under the hood. With a simple HTTP request, you can load any hub model and predict your data with it in seconds. The model URL and a valid hub token are all you need. Here's how I can load and predict with the `xlm-roberta-base` [model](https://huggingface.co/xlm-roberta-base) in a single line:\n```\ncurl https://api-inference.huggingface.co/models/xlm-roberta-base \\\n-X POST \\\n-d '{\""inputs\"": \""The answer to the universe is .\""}' \\\n-H \""Authorization: Bearer HF\\_TOKEN\""\n```\nThe Inference API is the simplest way to build a prediction service that you can immediately call from your application during development and tests. No need for a bespoke API, or a model server. In addition, you can instantly switch from one model to the next and compare their performance in your application. And guess what? The Inference API is free to use. As rate limiting is enforced, we don't recommend using the Inference API for production. Instead, you should consider Inference Endpoints. ## Production with Inference Endpoints\nOnce you're happy with the performance of your ML model, it's time to deploy it for production. Unfortunately, when leaving the sandbox, everything becomes a concern: security, scaling, monitoring, etc. This is where a lot of ML stumble and sometimes fall. We built [Inference Endpoints](https://huggingface.co/inference-endpoints) to solve this problem. In just a few clicks, Inference Endpoints let you deploy any hub model on secure and scalable infrastructure, hosted in your AWS or Azure region of choice. Additional settings include CPU and GPU hosting, built-in auto-scaling, and more. This makes finding the appropriate cost/performance ratio easy, with [pricing](https://huggingface.co/pricing#endpoints) starting as low as $0.06 per hour. Inference Endpoints support three security levels:\n\\* Public: the endpoint runs in a public Hugging Face subnet, and anyone on the Internet can access it without any authentication. \\* Protected: the endpoint runs in a public Hugging Face subnet, and anyone on the Internet with the appropriate Hugging Face token can access it. \\* Private: the endpoint runs in a private Hugging Face subnet and is not accessible on the Internet. It's only available through a private connection in your AWS or Azure account."", ""ðŸ¤— Inference Endpoints\nðŸ¤— Inference Endpoints offers a secure production solution to easily deploy any ðŸ¤— Transformers, Sentence-Transformers and Diffusion models from the Hub on dedicated and autoscaling infrastructure managed by Hugging Face. A Hugging Face Endpoint is built from a [Hugging Face Model Repository](https://huggingface.co/models). When an Endpoint is created, the service creates image artifacts that are either built from the model you select or a custom-provided container image. The image artifacts are completely decoupled from the Hugging Face Hub source repositories to ensure the highest security and reliability levels. ðŸ¤— Inference Endpoints support all of the [ðŸ¤— Transformers, Sentence-Transformers and Diffusion tasks](/docs/inference-endpoints/supported\\_tasks) as well as [custom tasks](/docs/inference-endpoints/guides/custom\\_handler) not supported by ðŸ¤— Transformers yet like speaker diarization and diffusion. In addition, ðŸ¤— Inference Endpoints gives you the option to use a custom container image managed on an external service, for instance, [Docker Hub](https://hub.docker.com/), [AWS ECR](https://aws.amazon.com/ecr/?nc1=h\\_ls), [Azure ACR](https://azure.microsoft.com/de-de/services/container-registry/), or [Google GCR](https://cloud.google.com/container-registry?hl=de). ![creation-flow](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/creation\\_flow.png)\n## Documentation and Examples\n\\* [Security & Compliance](/docs/inference-endpoints/security)\n\\* [Supported Transformers Task](/docs/inference-endpoints/supported\\_tasks)\n\\* [API Reference](/docs/inference-endpoints/api\\_reference)\n\\* [Autoscaling](/docs/inference-endpoints/autoscaling)\n\\* [FAQ](/docs/inference-endpoints/faq)\n\\* [Help & Support](/docs/inference-endpoints/support)\n### Guides\n\\* [Access the solution (UI)](/docs/inference-endpoints/guides/access)\n\\* [Create your first Endpoint](/docs/inference-endpoints/guides/create\\_endpoint)\n\\* [Send Requests to Endpoints](/docs/inference-endpoints/guides/test\\_endpoint)\n\\* [Update your Endpoint](/docs/inference-endpoints/guides/update\\_endpoint)\n\\* [Advanced Setup (Instance Types, Auto Scaling, Versioning)](/docs/inference-endpoints/guides/advanced)\n\\* [Create a Private Endpoint with AWS PrivateLink](/docs/inference-endpoints/guides/private\\_link)\n\\* [Add custom Dependencies](/docs/inference-endpoints/guides/custom\\_dependencies)\n\\* [Create custom Inference Handler](/docs/inference-endpoints/guides/custom\\_handler)\n\\* [Use a custom Container Image](/docs/inference-endpoints/guides/custom\\_container)\n\\* [Access and read Logs](/docs/inference-endpoints/guides/logs)\n\\* [Access and view Metrics](/docs/inference-endpoints/guides/metrics)\n\\* [Change Organization or Account](/docs/inference-endpoints/guides/change\\_organization)\n### Others\n\\* [Inference Endpoints Versions](/docs/inference-endpoints/others/runtime)\n\\* [Serialization & Deserialization for Requests](/docs/inference-endpoints/others/serialization)"", ""--\ntitle: \""Why weâ€™re switching to Hugging Face Inference Endpoints, and maybe you should too\""\nthumbnail: /blog/assets/78\\_ml\\_director\\_insights/mantis1.png\nauthors:\n- user: mattupson\nguest: true\n---\n# Why weâ€™re switching to Hugging Face Inference Endpoints, and maybe you should too\nHugging Face recently launched [Inference Endpoints](https://huggingface.co/inference-endpoints); which as they put it: solves transformers in production. Inference Endpoints is a managed service that allows you to:\n- Deploy (almost) any model on Hugging Face Hub\n- To any cloud (AWS, and Azure, GCP on the way)\n- On a range of instance types (including GPU)\n- Weâ€™re switching some of our Machine Learning (ML) models that do inference on a CPU to this new service. This blog is about why, and why you might also want to consider it. ## What were we doing? The models that we have switched over to Inference Endpoints were previously managed internally and were running on AWS [Elastic Container Service](https://aws.amazon.com/ecs/) (ECS) backed by [AWS Fargate](https://aws.amazon.com/fargate/). This gives you a serverless cluster which can run container based tasks. Our process was as follows:\n- Train model on a GPU instance (provisioned by [CML](https://cml.dev/), trained with [transformers](https://huggingface.co/docs/transformers/main/))\n- Upload to [Hugging Face Hub](https://huggingface.co/models)\n- Build API to serve model [(FastAPI)](https://fastapi.tiangolo.com/)\n- Wrap API in container [(Docker)](https://www.docker.com/)\n- Upload container to AWS [Elastic Container Repository](https://aws.amazon.com/ecr/) (ECR)\n- Deploy model to ECS Cluster\nNow, you can reasonably argue that ECS was not the best approach to serving ML models, but it served us up until now, and also allowed ML models to sit alongside other container based services, so it reduced cognitive load. ## What do we do now? With Inference Endpoints, our flow looks like this:\n- Train model on a GPU instance (provisioned by [CML](https://cml.dev/), trained with [transformers](https://huggingface.co/docs/transformers/main/))\n- Upload to [Hugging Face Hub](https://huggingface.co/models)\n- Deploy using Hugging Face Inference Endpoints. So this is significantly easier. We could also use another managed service such as [SageMaker](https://aws.amazon.com/es/sagemaker/), [Seldon](https://www.seldon.io/), or [Bento ML](https://www.bentoml.com/), etc., but since we are already uploading our model to Hugging Face hub to act as a model registry, and weâ€™re pretty invested in Hugging Faceâ€™s other tools (like transformers, and [AutoTrain](https://huggingface.co/autotrain)) using Inference Endpoints makes a lot of sense for us. ## What about Latency and Stability? Before switching to Inference Endpoints we tested different CPU endpoints types using [ab](https://httpd.apache.org/docs/2.4/programs/ab.html). For ECS we didnâ€™t test so extensively, but we know that a large container had a latency of about ~200ms from an instance in the same region. The tests we did for Inference Endpoints we based on text classification model fine tuned on [RoBERTa](https://huggingface.co/roberta-base) with the following test parameters:\n- Requester region: eu-east-1\n- Requester instance size: t3-medium\n- Inference endpoint region: eu-east-1\n- Endpoint Replicas: 1\n- Concurrent connections: 1\n- Requests: 1000 (1000 requests in 1â€“2 minutes even from a single connection would represent very heavy use for this particular application)\nThe following table shows latency (ms Â± standard deviation and time to complete test in seconds) for four Intel Ice Lake equipped CPU endpoints.""]",The Paid tier of the Inference API service is being replaced by the new solution called Inference Endpoints.,Inference Endpoints
"What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# SqueezeBERT\n## Overview\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers."", ""But, is it even necessary to compute the full intermediate matrix \\\\( \\mathbf{Y}\\_\\text{int} \\\\) ? Not really, because relevant is only the output matrix \\\\( \\mathbf{Y}\\_\\text{out} \\\\). To trade memory for speed, one can thus chunk the linear layers computation to only process one chunk at the time. Defining `config.chunk\\_size\\_feed\\_forward` as \\\\( c\\_{f} \\\\), chunked linear layers are defined as \\\\( \\mathbf{Y}\\_{\\text{out}} = \\left[\\mathbf{Y}\\_{\\text{out}, 1: c\\_{f}}, \\ldots, \\mathbf{Y}\\_{\\text{out}, (n - c\\_{f}): n}\\right] \\\\) with \\\\( \\mathbf{Y}\\_{\\text{out}, (c\\_{f} \\* i): (i \\* c\\_{f} + i)} = \\text{Linear}\\_{\\text{out}}(\\text{Linear}\\_{\\text{int}}(\\mathbf{\\overline{Z}}\\_{(c\\_{f} \\* i): (i \\* c\\_{f} + i)})) \\\\). In practice, it just means that the output is incrementally computed and concatenated to avoid having to store the whole intermediate tensor \\\\( \\mathbf{Y}\\_{\\text{int}} \\\\) in memory. Assuming \\\\( c\\_{f}=1 \\\\) for our example we can illustrate the incremental computation of the output for position \\\\( i=9 \\\\) as follows. ![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific\\_images/master/reformer\\_benchmark/chunked\\_feed\\_forward.png)\nBy processing the inputs in chunks of size 1, the only tensors that have to be stored in memory at the same time are \\\\( \\mathbf{Y}\\_\\text{out} \\\\) of a maximum size of \\\\( 16 \\times d\\_{h} \\\\), \\\\( \\mathbf{y}\\_{\\text{int}, i} \\\\) of size \\\\( d\\_{f} \\\\) and the input \\\\( \\mathbf{\\overline{Z}} \\\\) of size \\\\( 16 \\times d\\_{h} \\\\), with \\\\( d\\_{h} \\\\) being `config.hidden\\_size` \\\\(^{3}\\\\). Finally, it is important to remember that \\*chunked linear layers\\* yield a mathematically equivalent output to conventional linear layers and can therefore be applied to all transformer linear layers. Making use of `config.chunk\\_size\\_feed\\_forward` therefore allows a better trade-off between memory and speed in certain use cases. ---\n\\\\( {}^1 \\\\) For a simpler explanation, the layer norm layer which is normally applied to \\\\( \\mathbf{\\overline{Z}} \\\\) before being processed by the feed forward layers is omitted for now. \\\\( {}^2 \\\\) In `bert-base-uncased`, \\*e.g.\\* the intermediate dimension \\\\( d\\_{f} \\\\) is with 3072 four times larger than the output dimension \\\\( d\\_{h} \\\\). \\\\( {}^3 \\\\) As a reminder, the output `config.num\\_attention\\_heads` is assumed to be 1 for the sake of clarity and illustration in this notebook, so that the output of the self-attention layers can be assumed to be of size `config.hidden\\_size`."", ""1. \\*\\*[SegFormer](https://huggingface.co/docs/transformers/model\\_doc/segformer)\\*\\* (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo. 1. \\*\\*[Segment Anything](https://huggingface.co/docs/transformers/model\\_doc/sam)\\*\\* (Meta AI à¤¸à¥‡) Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick. à¤¦à¥à¤µà¤¾à¤°à¤¾à¤…à¤¨à¥à¤¸à¤‚à¤§à¤¾à¤¨ à¤ªà¤¤à¥à¤° [Segment Anything](https://arxiv.org/pdf/2304.02643v1.pdf) à¤•à¥‡ à¤¸à¤¾à¤¥ à¤œà¤¾à¤°à¥€ à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾\n1. \\*\\*[SEW](https://huggingface.co/docs/transformers/model\\_doc/sew)\\*\\* (ASAPP à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤¦à¥‡à¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡à¤ªà¤° [à¤­à¤¾à¤·à¤£ à¤ªà¤¹à¤šà¤¾à¤¨ à¤•à¥‡ à¤²à¤¿à¤ à¤…à¤¨à¤¸à¥à¤ªà¤°à¤µà¤¾à¤‡à¤œà¥à¤¡ à¤ªà¥à¤°à¥€-à¤Ÿà¥à¤°à¥‡à¤¨à¤¿à¤‚à¤— à¤®à¥‡à¤‚ à¤ªà¤°à¤«à¥‰à¤°à¥à¤®à¥‡à¤‚à¤¸-à¤à¤«à¤¿à¤¶à¤¿à¤à¤‚à¤¸à¥€ à¤Ÿà¥à¤°à¥‡à¤¡-à¤‘à¤«à¥à¤¸](https ://arxiv.org/abs/2109.06870) à¤«à¥‡à¤²à¤¿à¤•à¥à¤¸ à¤µà¥‚, à¤•à¥à¤µà¤¾à¤‚à¤—à¤¯à¥à¤¨ à¤•à¤¿à¤®, à¤œà¤¿à¤‚à¤— à¤ªà¥ˆà¤¨, à¤•à¥à¤¯à¥‚ à¤¹à¤¾à¤¨, à¤•à¤¿à¤²à¤¿à¤¯à¤¨ à¤•à¥à¤¯à¥‚. à¤µà¥‡à¤¨à¤¬à¤°à¥à¤—à¤°, à¤¯à¥‹à¤µ à¤†à¤°à¥à¤Ÿà¤œà¤¼à¥€ à¤¦à¥à¤µà¤¾à¤°à¤¾à¥¤\n1. \\*\\*[SEW-D](https://huggingface.co/docs/transformers/model\\_doc/sew\\_d)\\*\\* (ASAPP à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤ªà¥‡à¤ªà¤° [à¤­à¤¾à¤·à¤£ à¤ªà¤¹à¤šà¤¾à¤¨ à¤•à¥‡ à¤²à¤¿à¤ à¤…à¤¨à¤¸à¥à¤ªà¤°à¤µà¤¾à¤‡à¤œà¥à¤¡ à¤ªà¥à¤°à¥€-à¤Ÿà¥à¤°à¥‡à¤¨à¤¿à¤‚à¤— à¤®à¥‡à¤‚ à¤ªà¤°à¤«à¥‰à¤°à¥à¤®à¥‡à¤‚à¤¸-à¤à¤«à¤¿à¤¶à¤¿à¤à¤‚à¤¸à¥€ à¤Ÿà¥à¤°à¥‡à¤¡-à¤‘à¤«à¥à¤¸] (https://arxiv.org/abs/2109.06870) à¤«à¥‡à¤²à¤¿à¤•à¥à¤¸ à¤µà¥‚, à¤•à¥à¤µà¤¾à¤‚à¤—à¤¯à¥à¤¨ à¤•à¤¿à¤®, à¤œà¤¿à¤‚à¤— à¤ªà¥ˆà¤¨, à¤•à¥à¤¯à¥‚ à¤¹à¤¾à¤¨, à¤•à¤¿à¤²à¤¿à¤¯à¤¨ à¤•à¥à¤¯à¥‚. à¤µà¥‡à¤¨à¤¬à¤°à¥à¤—à¤°, à¤¯à¥‹à¤†à¤µ à¤†à¤°à¥à¤Ÿà¤œà¤¼à¥€ à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤ªà¥‹à¤¸à¥à¤Ÿ à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾à¥¤\n1. \\*\\*[SpeechT5](https://huggingface.co/docs/transformers/model\\_doc/speecht5)\\*\\* (from Microsoft Research) released with the paper [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei. 1. \\*\\*[SpeechToTextTransformer](https://huggingface.co/docs/transformers/model\\_doc/speech\\_to\\_text)\\*\\* (à¤«à¥‡à¤¸à¤¬à¥à¤• à¤¸à¥‡), à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤ªà¥‡à¤ªà¤° [à¤«à¥‡à¤¯à¤°à¤¸à¥‡à¤• S2T: à¤«à¤¾à¤¸à¥à¤Ÿ à¤¸à¥à¤ªà¥€à¤š-à¤Ÿà¥‚-à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤®à¥‰à¤¡à¤²à¤¿à¤‚à¤— à¤µà¤¿à¤¦ à¤«à¥‡à¤¯à¤°à¤¸à¥‡à¤•](https: //arxiv.org/abs/2010.05171) à¤šà¤¾à¤‚à¤—à¤¹à¤¾à¤¨ à¤µà¤¾à¤‚à¤—, à¤¯à¥‚à¤‚ à¤¤à¤¾à¤‚à¤—, à¤œà¥à¤¤à¤¾à¤ˆ à¤®à¤¾, à¤à¤¨à¥€ à¤µà¥‚, à¤¦à¤¿à¤®à¤¿à¤¤à¥à¤°à¥‹ à¤“à¤–à¥‹à¤¨à¤•à¥‹, à¤œà¥à¤†à¤¨ à¤ªà¤¿à¤¨à¥‹ à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤ªà¥‹à¤¸à¥à¤Ÿ à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ã€‚\n1. \\*\\*[SpeechToTextTransformer2](https://huggingface.co/docs/transformers/model\\_doc/speech\\_to\\_text\\_2)\\*\\* (à¤«à¥‡à¤¸à¤¬à¥à¤• à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤ªà¥‡à¤ªà¤° [à¤²à¤¾à¤°à¥à¤œ-à¤¸à¥à¤•à¥‡à¤² à¤¸à¥‡à¤²à¥à¤«- à¤à¤‚à¤¡ à¤¸à¥‡à¤®à¥€-à¤¸à¥à¤ªà¤°à¤µà¤¾à¤‡à¤œà¥à¤¡ à¤²à¤°à¥à¤¨à¤¿à¤‚à¤— à¤«à¥‰à¤° à¤¸à¥à¤ªà¥€à¤š à¤Ÿà¥à¤°à¤¾à¤‚à¤¸à¤²à¥‡à¤¶à¤¨](https://arxiv.org/abs/2104.06678) à¤šà¤¾à¤‚à¤—à¤¹à¤¾à¤¨ à¤µà¤¾à¤‚à¤—, à¤à¤¨à¥€ à¤µà¥‚, à¤œà¥à¤†à¤¨ à¤ªà¤¿à¤¨à¥‹, à¤à¤²à¥‡à¤•à¥à¤¸à¥€ à¤¬à¥‡à¤µà¤¸à¥à¤•à¥€, à¤®à¤¾à¤‡à¤•à¤² à¤”à¤²à¥€, à¤à¤²à¥‡à¤•à¥à¤¸à¤¿à¤¸ à¤¦à¥à¤µà¤¾à¤°à¤¾ Conneau à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤ªà¥‹à¤¸à¥à¤Ÿ à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾à¥¤\n1. \\*\\*[Splinter](https://huggingface.co/docs/transformers/model\\_doc/splinter)\\*\\* (à¤¤à¥‡à¤² à¤…à¤µà¥€à¤µ à¤¯à¥‚à¤¨à¤¿à¤µà¤°à¥à¤¸à¤¿à¤Ÿà¥€ à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤ªà¥‡à¤ªà¤° [à¤¸à¥à¤ªà¥ˆà¤¨ à¤¸à¤¿à¤²à¥‡à¤•à¥à¤¶à¤¨ à¤•à¥‹ à¤ªà¥à¤°à¥€-à¤Ÿà¥à¤°à¥‡à¤¨à¤¿à¤‚à¤— à¤•à¤°à¤•à¥‡ à¤•à¥à¤›-à¤¶à¥‰à¤Ÿ à¤•à¥à¤µà¥‡à¤¶à¥à¤šà¤¨ à¤†à¤‚à¤¸à¤°à¤¿à¤‚à¤—](https:// arxiv.org/abs/2101.00438) à¤“à¤°à¤¿ à¤°à¤¾à¤®, à¤¯à¥à¤µà¤² à¤•à¤°à¥à¤¸à¥à¤Ÿà¤¨, à¤œà¥‹à¤¨à¤¾à¤¥à¤¨ à¤¬à¥‡à¤°à¥‡à¤‚à¤Ÿ, à¤…à¤®à¥€à¤° à¤—à¥à¤²à¥‹à¤¬à¤°à¥à¤¸à¤¨, à¤“à¤®à¤° à¤²à¥‡à¤µà¥€ à¤¦à¥à¤µà¤¾à¤°à¤¾à¥¤\n1. \\*\\*[SqueezeBERT](https://huggingface.co/docs/transformers/model\\_doc/squeezebert)\\*\\* (à¤¬à¤°à¥à¤•à¤²à¥‡ à¤¸à¥‡) à¤•à¤¾à¤—à¤œ à¤•à¥‡ à¤¸à¤¾à¤¥ [SqueezeBERT: à¤•à¥à¤¶à¤² à¤¤à¤‚à¤¤à¥à¤°à¤¿à¤•à¤¾ à¤¨à¥‡à¤Ÿà¤µà¤°à¥à¤• à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ NLP à¤•à¥‹ à¤•à¤‚à¤ªà¥à¤¯à¥‚à¤Ÿà¤° à¤µà¤¿à¤œà¤¼à¤¨ à¤•à¥à¤¯à¤¾ à¤¸à¤¿à¤–à¤¾ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ?](https: //arxiv.org/abs/2006.11316) à¤«à¥‰à¤°à¥‡à¤¸à¥à¤Ÿ à¤à¤¨."", ""The abstract from the paper is the following:\n\\*Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets,\nlarge computing systems, and better neural network models, natural language processing (NLP) technology has made\nsignificant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant\nopportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. In particular, we\nconsider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today's\nhighly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with\nBERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. In this work, we observe that methods\nsuch as grouped convolutions have yielded significant speedups for computer vision networks, but many of these\ntechniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\nset. The SqueezeBERT code will be released.\\*\nThis model was contributed by [forresti](https://huggingface.co/forresti). ## Usage tips\n- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\nrather than the left. - SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\nefficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\nwith a causal language modeling (CLM) objective are better in that regard. - For best results when finetuning on sequence classification tasks, it is recommended to start with the\n\\*squeezebert/squeezebert-mnli-headless\\* checkpoint. ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## SqueezeBertConfig\n[[autodoc]] SqueezeBertConfig\n## SqueezeBertTokenizer\n[[autodoc]] SqueezeBertTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## SqueezeBertTokenizerFast\n[[autodoc]] SqueezeBertTokenizerFast\n## SqueezeBertModel\n[[autodoc]] SqueezeBertModel\n## SqueezeBertForMaskedLM\n[[autodoc]] SqueezeBertForMaskedLM\n## SqueezeBertForSequenceClassification\n[[autodoc]] SqueezeBertForSequenceClassification\n## SqueezeBertForMultipleChoice\n[[autodoc]] SqueezeBertForMultipleChoice\n## SqueezeBertForTokenClassification\n[[autodoc]] SqueezeBertForTokenClassification\n## SqueezeBertForQuestionAnswering\n[[autodoc]] SqueezeBertForQuestionAnswering""]","SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers.",Grouped convolutions
"What type of license is the HuggingFace Team's software distributed under?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction"", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Scenes\nUnder construction ðŸš§."", ""Hugging Face Hub documentation\nThe Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. The Hub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine Learning. Are you ready to join the path towards open source Machine Learning? ðŸ¤—\n\nRepositories\n\n[Introduction](./repositories)\n[Getting Started](./repositories-getting-started)\n[Repository Settings](./repositories-settings)\n[Pull requests and Discussions](./repositories-pull-requests-discussions)\n[Notifications](./notifications)\n[Collections](./collections)\n[Webhooks](./webhooks)\n[Next Steps](./repositories-next-steps)\n[Licenses](./repositories-licenses)\n\nModels\n\n[Introduction](./models)\n[The Model Hub](./models-the-hub)\n[Model Cards](./model-cards)\n[Gated Models](./models-gated)\n[Uploading Models](./models-uploading)\n[Downloading Models](./models-downloading)\n[Libraries](./models-libraries)\n[Tasks](./models-tasks)\n[Widgets](./models-widgets)\n[Inference API](./models-inference)\n[Download Stats](./models-download-stats)\n\nDatasets\n\n[Introduction](./datasets)\n[Datasets Overview](./datasets-overview)\n[Dataset Cards](./datasets-cards)\n[Gated Datasets](./datasets-gated)\n[Uploading Datasets](./datasets-adding)\n[Downloading Datasets](./datasets-downloading)\n[Libraries](./datasets-libraries)\n[Dataset Viewer](./datasets-viewer)\n[Download Stats](./datasets-download-stats)\n[Data files Configuration](./datasets-data-files-configuration)\n\nSpaces\n\n[Introduction](./spaces)\n[Spaces Overview](./spaces-overview)\n[Gradio Spaces](./spaces-sdks-gradio)\n[Streamlit Spaces](./spaces-sdks-streamlit)\n[Static HTML Spaces](./spaces-sdks-static)\n[Docker Spaces](./spaces-sdks-docker)\n[Embed your Space](./spaces-embed)\n[Run with Docker](./spaces-run-with-docker)\n[Reference](./spaces-config-reference)\n[Changelog](./spaces-changelog)\n[Advanced Topics](./spaces-advanced)\n[Sign in with HF](./spaces-oauth)\n\nOther\n\n[Organizations](./organizations)\n[Enterprise Hub](./enterprise-hub)\n[Billing](./billing)\n[Security](./security)\n[Moderation](./moderation)\n[Paper Pages](./paper-pages)\n[Search](./search)\n[Digital Object Identifier (DOI)](./doi)\n[Hub API Endpoints](./api)\n[Sign in with HF](./oauth)\n[Contributor Code of Conduct](https://huggingface.co/code-of-conduct)\n[Content Guidelines](https://huggingface.co/content-guidelines)\n\n## What's the Hugging Face Hub?"", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction""]","The HuggingFace Team's software is distributed under the Apache License, Version 2.0.","Apache License, Version 2.0"
"What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?
","[""It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\n\\*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\*\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# AdaLoRA\n[AdaLoRA](https://hf.co/papers/2303.10512) is a method for optimizing the number of trainable parameters to assign to weight matrices and layers, unlike LoRA, which distributes parameters evenly across all modules. More parameters are budgeted for important weight matrices and layers while less important ones receive fewer parameters. The abstract from the paper is:\n\\*Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings. Our code is publicly available at https://github.com/QingruZhang/AdaLoRA\\*. ## AdaLoraConfig\n[[autodoc]] tuners.adalora.config.AdaLoraConfig\n## AdaLoraModel\n[[autodoc]] tuners.adalora.model.AdaLoraModel"", ""Movement Pruning: Adaptive Sparsity by Fine-Tuning\nAuthor: @VictorSanh\n\\*Magnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of \\*movement pruning\\*, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3% of the model parameters:\\*\n| Fine-pruning+Distillation  \n(Teacher=BERT-base fine-tuned) | BERT base  \nfine-tuned | Remaining  \nWeights (%) | Magnitude Pruning | L0 Regularization | Movement Pruning | Soft Movement Pruning |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| SQuAD - Dev  \nEM/F1 | 80.4/88.1 | 10%  \n3% | 70.2/80.1  \n45.5/59.6 | 72.4/81.9  \n64.3/75.8 | 75.6/84.3  \n67.5/78.0 | \\*\\*76.6/84.9\\*\\*  \n\\*\\*72.7/82.3\\*\\* |\n| MNLI - Dev  \nacc/MM acc | 84.5/84.9 | 10%  \n3% | 78.3/79.3  \n69.4/70.6 | 78.7/79.7  \n76.0/76.2 | 80.1/80.4  \n76.5/77.4 | \\*\\*81.2/81.8\\*\\*  \n\\*\\*79.5/80.1\\*\\* |\n| QQP - Dev  \nacc/F1 | 91.4/88.4 | 10%  \n3% | 79.8/65.0  \n72.4/57.8 | 88.1/82.8  \n87.0/81.9 | 89.7/86.2  \n86.1/81.5 | \\*\\*90.2/86.8\\*\\*  \n\\*\\*89.1/85.5\\*\\* |\nThis page contains information on how to fine-prune pre-trained models such as `BERT` to obtain extremely sparse models with movement pruning."", ""## ëª¨ë¸ êµ¬ì¡°\n\\*\\*ðŸ¤— Transformersê°€ ì œê³µí•˜ëŠ” [ëª¨ë“  ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸](https://huggingface.co/models)\\*\\* ëŠ” huggingface.co [ëª¨ë¸ í—ˆë¸Œ](https://huggingface.co)ì— ì™„ë²½ížˆ ì—°ë™ë˜ì–´ ìžˆìŠµë‹ˆë‹¤. [ê°œì¸](https://huggingface.co/users)ê³¼ [ê¸°ê´€](https://huggingface.co/organizations)ì´ ëª¨ë¸ í—ˆë¸Œì— ì§ì ‘ ì—…ë¡œë“œí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. í˜„ìž¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì˜ ê°œìˆ˜: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)\nðŸ¤— TransformersëŠ” ë‹¤ìŒ ëª¨ë¸ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤ (ê° ëª¨ë¸ì˜ ìš”ì•½ì€ [ì—¬ê¸°](https://huggingface.co/docs/transformers/model\\_summary)ì„œ í™•ì¸í•˜ì„¸ìš”):\n1. \\*\\*[ALBERT](https://huggingface.co/docs/transformers/model\\_doc/albert)\\*\\* (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. 1. \\*\\*[ALIGN](https://huggingface.co/docs/transformers/model\\_doc/align)\\*\\* (Google Research ì—ì„œ ì œê³µ)ì€ Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.ì˜ [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918)ë…¼ë¬¸ê³¼ í•¨ê»˜ ë°œí‘œí–ˆìŠµë‹ˆë‹¤."", ""The answer appears to be yes. The main ingredients are: adapters and 8bit matrix multiplication! Let us cover these topics in the following sections:\n### 8-bit matrix multiplication\nEfficient 8-bit matrix multiplication is a method that has been first introduced in the paper LLM.int8() and aims to solve the performance degradation issue when quantizing large-scale models. The proposed method breaks down the matrix multiplications that are applied under the hood in Linear layers in two stages: the outlier hidden states part that is going to be performed in float16 & the â€œnon-outlierâ€ part that is performed in int8. | ![8bit-matmul](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/133\\_trl\\_peft/8bit-matmul.png) |\n|:--:|\n| **Efficient 8-bit matrix multiplication is a method that has been first introduced in the paper [LLM.int8()](https://arxiv.org/abs/2208.07339) and aims to solve the performance degradation issue when quantizing large-scale models. The proposed method breaks down the matrix multiplications that are applied under the hood in Linear layers in two stages: the outlier hidden states part that is going to be performed in float16 & the â€œnon-outlierâ€ part that is performed in int8.** |\nIn a nutshell, you can reduce the size of a full-precision model by 4 (thus, by 2 for half-precision models) if you use 8-bit matrix multiplication. ### Low rank adaptation and PEFT\nIn 2021, a paper called LoRA: Low-Rank Adaption of Large Language Models demonstrated that fine tuning of large language models can be performed by freezing the pretrained weights and creating low rank versions of the query and value layers attention matrices. These low rank matrices have far fewer parameters than the original model, enabling fine-tuning with far less GPU memory. The authors demonstrate that fine-tuning of low-rank adapters achieved comparable results to fine-tuning the full pretrained model. | ![lora-gif](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/133\\_trl\\_peft/lora-animated.gif) |\n|:--:|\n| **The output activations original (frozen) pretrained weights (left) are augmented by a low rank adapter comprised of weight matrics A and B (right).** |\nThis technique allows the fine tuning of LLMs using a fraction of the memory requirements."", ""However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model\\_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model\\_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model\\_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix. ### Decoder[[nlp-decoder]]\n[GPT-2](model\\_doc/gpt2) is a decoder-only Transformer that predicts the next word in the sequence."", ""With each merge/commit, it can be more difficult to trace both the data used (as a number of released datasets are compilations of other datasets) and the models' history, as highly performing models are fine-tuned versions of fine-tuned versions of similar models (see Mistral's \""child models tree\"" [here](https://huggingface.co/spaces/davanstrien/mistral-graph)). In this summary, we haven't had the time yet to talk about this amazing technique, so let's spend a couple of final words on it. But what does it mean to merge a model? \\*\\*Model merging\\*\\* is a way to fuse the weights of different models together in a single model to (ideally) combine the respective strengths of each model in a unified single model. A few techniques exist to do so that have been extended and often published mostly in community forums, a striking case of fully decentralized research happening all over the world between a community of practitioners, researchers, and hobbyists. One of the simplest published methods consists in averaging the parameters of a set of models sharing a common architecture ([example 1](https://huggingface.co/papers/2204.03044), [example 2](https://huggingface.co/papers/2109.01903)) but more complex parameter combinations exist, such as determining which parameters are the most influential in each model for a given task ([weighted averaging](https://huggingface.co/papers/2111.09832)), or considering parameters interference between models before selecting which parameters to keep when merging ([ties merging](https://huggingface.co/papers/2306.01708)). These techniques allow anybody to easily generate combinations of models and are made especially easy by the fact that most models are nowadays variations on the same architecture. That's the reason some models submitted to the [open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open\\_llm\\_leaderboard) have names such as `llama2-zephyr-orca-ultra`. This particular example is likely a merge of `llama2` and `zephyr` models, fine-tuned on orca and ultra datasets. Usually, more details are to be found in the respective model card on the Hugging Face hub. ### PEFT: Personalization at the tip of your fingers\nSometimes, you may want more controlled personalization, without enough memory to load a whole model in memory to fine tune it. Did you know that you don't need to use an entire model when fine-tuning? You might want to use what is called \\*\\*parameter efficient fine-tuning\\*\\* (PEFT). This technique first freezes up the parameters of your pretrained model of interest, then adds a number of new parameters on top of it, called the adapters."", ""- Learn how to use the HuggingFace APIs ([transformers](https://huggingface.co/docs/transformers/index), [peft](https://huggingface.co/docs/peft/index), and [datasets](https://huggingface.co/docs/datasets/index)). - Setup the hyperparameter tuning and experiment logging using [Weights & Biases](https://wandb.ai). ## Dependencies\n```bash\ndatasets\nevaluate\npeft\nscikit-learn\ntorch\ntransformers\nwandb\n```\nNote: For reproducing the reported results, please check the pinned versions in the [wandb reports](#resources). ## Pre-trained Models\n### [RoBERTa](https://arxiv.org/abs/1907.11692)\nRoBERTa (Robustly Optimized BERT Approach) is an advanced variant of the BERT model proposed by Meta AI research team. BERT is a transformer-based language model using self-attention mechanisms for contextual word representations and trained with a masked language model objective. Note that BERT is an encoder only model used for natural language understanding tasks (such as sequence classification and token classification). RoBERTa is a popular model to fine-tune and appropriate as a baseline for our experiments. For more information, you can check the Hugging Face model [card](https://huggingface.co/docs/transformers/model\\_doc/roberta). ### [Llama 2](https://arxiv.org/abs/2307.09288)\nLlama 2 models, which stands for Large Language Model Meta AI, belong to the family of large language models (LLMs) introduced by Meta AI. The Llama 2 models vary in size, with parameter counts ranging from 7 billion to 65 billion. Llama 2 is an auto-regressive language model, based on the transformer decoder architecture. To generate text, Llama 2 processes a sequence of words as input and iteratively predicts the next token using a sliding window. Llama 2 architecture is slightly different from models like GPT-3. For instance, Llama 2 employs the SwiGLU activation function rather than ReLU and opts for rotary positional embeddings in place of absolute learnable positional embeddings. The recently released Llama 2 introduced architectural refinements to better leverage very long sequences by extending the context length to up to 4096 tokens, and using grouped-query attention (GQA) decoding. ### [Mistral 7B](https://arxiv.org/abs/2310.06825)\nMistral 7B v0.1, with 7.3 billion parameters, is the first LLM introduced by Mistral AI. The main novel techniques used in Mistral 7B's architecture are:\n- Sliding Window Attention: Replace the full attention (square compute cost) with a sliding window based attention where each token can attend to at most 4,096 tokens from the previous layer (linear compute cost). This mechanism enables Mistral 7B to handle longer sequences, where higher layers can access historical information beyond the window size of 4,096 tokens. - Grouped-query Attention: used in Llama 2 as well, the technique optimizes the inference process (reduce processing time) by caching the key and value vectors for previously decoded tokens in the sequence. ## [LoRA](https://arxiv.org/abs/2106.09685)\nPEFT, Parameter Efficient Fine-Tuning, is a collection of techniques (p-tuning, prefix-tuning, IA3, Adapters, and LoRa) designed to fine-tune large models using a much smaller set of training parameters while preserving the performance levels typically achieved through full fine-tuning. LoRA, Low-Rank Adaptation, is a PEFT method that shares similarities with Adapter layers."", ""There are, however, some downsides. The forward and backward pass is approximately twice as slow, due to the additional matrix multiplications in the adapter layers. ### What is PEFT? [Parameter-Efficient Fine-Tuning (PEFT)](https://github.com/huggingface/peft), is a Hugging Face library, created to support the creation and fine tuning of adapter layers on LLMs.`peft` is seamlessly integrated with ðŸ¤— Accelerate for large scale models leveraging DeepSpeed and Big Model Inference. The library supports many state of the art models and has an extensive set of examples, including:\n- Causal language modeling\n- Conditional generation\n- Image classification\n- 8-bit int8 training\n- Low Rank adaption of Dreambooth models\n- Semantic segmentation\n- Sequence classification\n- Token classification\nThe library is still under extensive and active development, with many upcoming features to be announced in the coming months. ## Fine-tuning 20B parameter models with Low Rank Adapters\nNow that the prerequisites are out of the way, let us go through the entire pipeline step by step, and explain with figures how you can fine-tune a 20B parameter LLM with RL using the tools mentioned above on a single 24GB GPU! ### Step 1: Load your active model in 8-bit precision\n| ![step1](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/133\\_trl\\_peft/step1.png) |\n|:--:|\n|  **Loading a model in 8-bit precision can save up to 4x memory compared to full precision model**|\nA â€œfree-lunchâ€ memory reduction of a LLM using `transformers` is to load your model in 8-bit precision using the method described in LLM.int8. This can be performed by simply adding the flag `load\\_in\\_8bit=True` when calling the `from\\_pretrained` method (you can read more about that [here](https://huggingface.co/docs/transformers/main/en/main\\_classes/quantization)). As stated in the previous section, a â€œhackâ€ to compute the amount of GPU memory you should need to load your model is to think in terms of â€œbillions of parametersâ€. As one byte needs 8 bits, you need 4GB per billion parameters for a full-precision model (32bit = 4bytes), 2GB per billion parameters for a half-precision model, and 1GB per billion parameters for an int8 model.""]","The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:

1. Splitting the embedding matrix into two smaller matrices.
2. Using repeating layers split among groups.",Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.
"What are the three main steps for fine-tuning a model with the ðŸ¤— Datasets library?
","[""Introduction[[introduction]]\nIn [Chapter 3](/course/chapter3) you got your first taste of the ðŸ¤— Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics. But this is just scratching the surface of what ðŸ¤— Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\\* What do you do when your dataset is not on the Hub? \\* How can you slice and dice a dataset? (And what if you \\_really\\_ need to use Pandas?)\n\\* What do you do when your dataset is huge and will melt your laptop's RAM? \\* What the heck are \""memory mapping\"" and Apache Arrow? \\* How can you create your own dataset and push it to the Hub? The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!"", ""To verify that all libraries are correctly installed, you can run the following command in a Python shell. It verifies that both `transformers` and `datasets` have been correclty installed. ```python\nfrom transformers import AutoModelForCTC, AutoProcessor\nfrom datasets import load\\_dataset\ndummy\\_dataset = load\\_dataset(\""common\\_voice\"", \""ab\"", split=\""test\"")\nmodel = AutoModelForCTC.from\\_pretrained(\""hf-internal-testing/tiny-random-wav2vec2\"")\nmodel.to(\""cuda\"")\nprocessor = AutoProcessor.from\\_pretrained(\""hf-internal-testing/tiny-random-wav2vec2\"")\ninput\\_values = processor(dummy\\_dataset[0][\""audio\""][\""array\""], return\\_tensors=\""pt\"", sampling\\_rate=16\\_000).input\\_values\ninput\\_values = input\\_values.to(\""cuda\"")\nlogits = model(input\\_values).logits\nassert logits.shape[-1] == 32\n```\n## How to finetune an acoustic model\nIn this section, we show you how to fine-tune a pre-trained [XLS-R Model](https://huggingface.co/docs/transformers/model\\_doc/xls\\_r) on the [Common Voice 7 dataset](https://huggingface.co/datasets/mozilla-foundation/common\\_voice\\_7\\_0)."", ""!---\nCopyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Token classification with LayoutLMv3 (PyTorch version)\nThis directory contains a script, `run\\_funsd\\_cord.py`, that can be used to fine-tune (or evaluate) LayoutLMv3 on form understanding datasets, such as [FUNSD](https://guillaumejaume.github.io/FUNSD/) and [CORD](https://github.com/clovaai/cord). The script `run\\_funsd\\_cord.py` leverages the ðŸ¤— Datasets library and the Trainer API. You can easily customize it to your needs. ## Fine-tuning on FUNSD\nFine-tuning LayoutLMv3 for token classification on [FUNSD](https://guillaumejaume.github.io/FUNSD/) can be done as follows:\n```bash\npython run\\_funsd\\_cord.py \\\n--model\\_name\\_or\\_path microsoft/layoutlmv3-base \\\n--dataset\\_name funsd \\\n--output\\_dir layoutlmv3-test \\\n--do\\_train \\\n--do\\_eval \\\n--max\\_steps 1000 \\\n--evaluation\\_strategy steps \\\n--eval\\_steps 100 \\\n--learning\\_rate 1e-5 \\\n--load\\_best\\_model\\_at\\_end \\\n--metric\\_for\\_best\\_model \""eval\\_f1\"" \\\n--push\\_to\\_hub \\\n--push\\_to\\_hubÂ°model\\_id layoutlmv3-finetuned-funsd\n```\nðŸ‘€ The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Fine-tune a pretrained model\n[[open-in-colab]]\nThere are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ðŸ¤— Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:\n\\* Fine-tune a pretrained model with ðŸ¤— Transformers [`Trainer`]. \\* Fine-tune a pretrained model in TensorFlow with Keras. \\* Fine-tune a pretrained model in native PyTorch. ## Prepare a dataset\nBefore you can fine-tune a pretrained model, download a dataset and prepare it for training. The previous tutorial showed you how to process data for training, and now you get an opportunity to put those skills to the test! Begin by loading the [Yelp Reviews](https://huggingface.co/datasets/yelp\\_review\\_full) dataset:\n```py\n>>> from datasets import load\\_dataset\n>>> dataset = load\\_dataset(\""yelp\\_review\\_full\"")\n>>> dataset[\""train\""][100]\n{'label': 0,\n'text': 'My expectations for McDonalds are t rarely high."", ""2. Use your fine-tuned model for inference. The task illustrated in this tutorial is supported by the following model architectures:\n[BEiT](../model\\_doc/beit), [BiT](../model\\_doc/bit), [ConvNeXT](../model\\_doc/convnext), [ConvNeXTV2](../model\\_doc/convnextv2), [CvT](../model\\_doc/cvt), [Data2VecVision](../model\\_doc/data2vec-vision), [DeiT](../model\\_doc/deit), [DiNAT](../model\\_doc/dinat), [DINOv2](../model\\_doc/dinov2), [EfficientFormer](../model\\_doc/efficientformer), [EfficientNet](../model\\_doc/efficientnet), [FocalNet](../model\\_doc/focalnet), [ImageGPT](../model\\_doc/imagegpt), [LeViT](../model\\_doc/levit), [MobileNetV1](../model\\_doc/mobilenet\\_v1), [MobileNetV2](../model\\_doc/mobilenet\\_v2), [MobileViT](../model\\_doc/mobilevit), [MobileViTV2](../model\\_doc/mobilevitv2), [NAT](../model\\_doc/nat), [Perceiver](../model\\_doc/perceiver), [PoolFormer](../model\\_doc/poolformer), [PVT](../model\\_doc/pvt), [RegNet](../model\\_doc/regnet), [ResNet](../model\\_doc/resnet), [SegFormer](../model\\_doc/segformer), [SwiftFormer](../model\\_doc/swiftformer), [Swin Transformer](../model\\_doc/swin), [Swin Transformer V2](../model\\_doc/swinv2), [VAN](../model\\_doc/van), [ViT](../model\\_doc/vit), [ViT Hybrid](../model\\_doc/vit\\_hybrid), [ViTMSN](../model\\_doc/vit\\_msn)\nBefore you begin, make sure you have all the necessary libraries installed:\n```bash\npip install transformers datasets evaluate\n```\nWe encourage you to log in to your Hugging Face account to upload and share your model with the community. When prompted, enter your token to log in:\n```py\n>>> from huggingface\\_hub import notebook\\_login\n>>> notebook\\_login()\n```\n## Load Food-101 dataset\nStart by loading a smaller subset of the Food-101 dataset from the ðŸ¤— Datasets library. This will give you a chance to\nexperiment and make sure everything works before spending more time training on the full dataset. ```py\n>>> from datasets import load\\_dataset\n>>> food = load\\_dataset(\""food101\"", split=\""train[:5000]\"")\n```\nSplit the dataset's `train` split into a train and test set with the [`~datasets.Dataset.train\\_test\\_split`] method:\n```py\n>>> food = food.train\\_test\\_split(test\\_size=0.2)\n```\nThen take a look at an example:\n```py\n>>> food[\""train\""][0]\n{'image': ,\n'label': 79}\n```\nEach example in the dataset has two fields:\n- `image`: a PIL image of the food item\n- `label`: the label class of the food item\nTo make it easier for the model to get the label name from the label id, create a dictionary that maps the label name\nto an integer and vice versa:\n```py\n>>> labels = food[\""train\""].features[\""label\""].names\n>>> label2id, id2label = dict(), dict()\n>>> for i, label in enumerate(labels):\n... label2id[label] = str(i)\n... id2label[str(i)] = label\n```\nNow you can convert the label id to a label name:\n```py\n>>> id2label[str(79)]\n'prime\\_rib'\n```\n## Preprocess\nThe next step is to load a ViT image processor to process the image into a tensor:\n```py\n>>> from transformers import AutoImageProcessor\n>>> checkpoint = \""google/vit-base-patch16-224-in21k\""\n>>> image\\_processor = AutoImageProcessor.from\\_pretrained(checkpoint)\n```\n\nApply some image transformations to the images to make the model more robust against overfitting. Here you'll use torchvision's [`transforms`](https://pytorch.org/vision/stable/transforms.html) module, but you can also use any image library you like. Crop a random part of the image, resize it, and normalize it with the image mean and standard deviation:\n```py\n>>> from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n>>> normalize = Normalize(mean=image\\_processor.image\\_mean, std=image\\_processor.image\\_std)\n>>> size = (\n... image\\_processor.size[\""shortest\\_edge\""]\n...""]","The three main steps for fine-tuning a model with the ðŸ¤— Datasets library are:
1. Load a dataset from the Hugging Face Hub.
2. Preprocess the data with `Dataset.map()`.
3. Load and compute metrics.",1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.
"What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?
","[""It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark. Figure 4. Latency (Batch=1, Physical Cores=2)\n\n---\n## Conclusion\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput. Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens. The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial)\n## Resources\n\\* [Hugging Face Infinity](https://huggingface.co/infinity)\n\\* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\n\\* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i)\n\\* [DistilBERT](https://huggingface.co/docs/transformers/model\\_doc/distilbert)\n\\* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\\* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\n\\* [ðŸ¤— Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)"", ""--\ntitle: \""How we sped up transformer inference 100x for ðŸ¤— API customers\""\nthumbnail: /blog/assets/09\\_accelerated\\_inference/thumbnail.png\n---\n# How we sped up transformer inference 100x for ðŸ¤— API customers\nðŸ¤— Transformers has become the default library for data scientists all around the world to explore state of the art NLP models and build new NLP features. With over 5,000 pre-trained and fine-tuned models available, in over 250 languages, it is a rich playground, easily accessible whichever framework you are working in. While experimenting with models in ðŸ¤— Transformers is easy, deploying these large models into production with maximum performance, and managing them into an architecture that scales with usage is a \\*\\*hard engineering challenge\\*\\* for any Machine Learning Engineer. This 100x performance gain and built-in scalability is why subscribers of our hosted [Accelerated Inference API](https://huggingface.co/pricing) chose to build their NLP features on top of it. To get to the \\*\\*last 10x of performance\\*\\* boost, the optimizations need to be low-level, specific to the model, and to the target hardware. This post shares some of our approaches squeezing every drop of compute juice for our customers. ðŸ‹\n## Getting to the first 10x speedup\nThe first leg of the optimization journey is the most accessible, all about using the best combination of techniques offered by the [Hugging Face libraries](https://github.com/huggingface/), independent of the target hardware. We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main\\_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:\n-| Naive version | Optimized version |\n-|:---------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------:|\n-|![](/blog/assets/09\\_accelerated\\_inference/unoptimized\\_graph.png)|![](/blog/assets/09\\_accelerated\\_inference/optimized\\_graph.png)|\nTokenization is often a bottleneck for efficiency during inference. We use the most efficient methods from the [ðŸ¤— Tokenizers](https://github.com/huggingface/tokenizers/) library, leveraging the Rust implementation of the model tokenizer in combination with smart caching to get up to 10x speedup for the overall latency. Leveraging the latest features of the Hugging Face libraries, we achieve a reliable 10x speed up compared to an out-of-box deployment for a given model/hardware pair. As new releases of Transformers and Tokenizers typically ship every month, our API customers do not need to constantly adapt to new optimization opportunities, their models just keep running faster."", ""You can test it and get a feeling for the performance of Infinity. Below you will find a `curl` command on how to send a request to the hosted endpoint. The API returns a `x-compute-time` HTTP Header, which contains the duration of the end-to-end pipeline. ```bash\ncurl --request POST `-i` \\\n--url https://infinity.huggingface.co/cpu/distilbert-base-uncased-emotion \\\n--header 'Content-Type: application/json' \\\n--data '{\""inputs\"":\""I like you. I love you\""}'\n```\n### Throughput\nBelow you can find the throughput comparison for running infinity on 2 physical cores with batch size 1, compared with vanilla transformers. Figure 3. Throughput: Infinity vs Transformers\n\n| Sequence Length | Infinity | Transformers | improvement |\n|-----------------|-------------|--------------|-------------|\n| 8 | 248 req/sec | 49 req/sec | +506% |\n| 16 | 212 req/sec | 50 req/sec | +424% |\n| 32 | 150 req/sec | 40 req/sec | +375% |\n| 64 | 97 req/sec | 28 req/sec | +346% |\n| 128 | 55 req/sec | 18 req/sec | +305% |\n| 256 | 27 req/sec | 9 req/sec | +300% |\n| 384 | 17 req/sec | 5 req/sec | +340% |\n| 512 | 12 req/sec | 4 req/sec | +300% |\n### Latency\nBelow, you can find the latency results for an experiment running Hugging Face Infinity on 2 Physical Cores with Batch Size 1."", ""--\ntitle: \""Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\""\nthumbnail: /blog/assets/46\\_infinity\\_cpu\\_performance/thumbnail.png\nauthors:\n- user: philschmid\n- user: jeffboudier\n- user: mfuntowicz\n---\n# Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\n\nDecember 2022 Update: Infinity is no longer offered by Hugging Face as a commercial inference solution. To deploy and accelerate your models, we recommend the following new solutions:\n\\* [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) to easily deploy models on dedicated infrastructure managed by Hugging Face. \\* Our open-source optimization libraries, [ðŸ¤— Optimum Intel](https://huggingface.co/blog/openvino) and [ðŸ¤— Optimum ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview), to get the highest efficiency out of training and running models for inference. \\* Hugging Face [Expert Acceleration Program](https://huggingface.co/support), a commercial service for Hugging Face experts to work directly with your team to accelerate your Machine Learning roadmap and models. ## Introduction\nTransfer learning has changed Machine Learning by reaching new levels of accuracy from Natural Language Processing (NLP) to Audio and Computer Vision tasks. At Hugging Face, we work hard to make these new complex models and large checkpoints as easily accessible and usable as possible. But while researchers and data scientists have converted to the new world of Transformers, few companies have been able to deploy these large, complex models in production at scale. The main bottleneck is the latency of predictions which can make large deployments expensive to run and real-time use cases impractical. Solving this is a difficult engineering challenge for any Machine Learning Engineering team and requires the use of advanced techniques to optimize models all the way down to the hardware. With [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments. ## What is Hugging Face Infinity\nHugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure. Hugging Face Infinity consists of 2 main services:\n\\* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container. \\* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container. The Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference. Figure 1. Infinity Overview\n\nAn Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learning tasks as defined in the [Transformers Pipelines documentation](https://huggingface.co/docs/transformers/master/en/main\\_classes/pipelines)."", ""In addition to superior performance for machine learning workloads, the Intel Ice Lake C6i instances offer great cost-performance and are our recommendation to deploy Infinity on Amazon Web Services. To learn more, visit the [EC2 C6i instance](https://aws.amazon.com/ec2/instance-types/c6i) page. ### Methodologies\nWhen it comes to benchmarking BERT-like models, two metrics are most adopted:\n\\* \\*\\*Latency\\*\\*: Time it takes for a single prediction of the model (pre-process, prediction, post-process)\n\\* \\*\\*Throughput\\*\\*: Number of executions performed in a fixed amount of time for one benchmark configuration, respecting Physical CPU cores, Sequence Length, and Batch Size\nThese two metrics will be used to benchmark Hugging Face Infinity across different setups to understand the benefits and tradeoffs in this blog post. ---\n## Results\nTo run the benchmark, we created an infinity container for the [EC2 C6i instance](https://aws.amazon.com/ec2/instance-types/c6i) (Ice-lake) and optimized a [DistilBERT](https://huggingface.co/docs/transformers/model\\_doc/distilbert) model for sequence classification using Infinity Multiverse. This ice-lake optimized Infinity Container can achieve up to 34% better latency & throughput compared to existing cascade-lake-based instances, and up to 800% better latency & throughput compared to vanilla transformers running on ice-lake. The Benchmark we created consists of 192 different experiments and configurations. We ran experiments for:\n\\* Physical CPU cores: 1, 2, 4, 8\n\\* Sequence length: 8, 16, 32, 64, 128, 256, 384, 512\n\\* Batch\\_size: 1, 2, 4, 8, 16, 32\nIn each experiment, we collect numbers for:\n\\* Throughput (requests per second)\n\\* Latency (min, max, avg, p90, p95, p99)\nYou can find the full data of the benchmark in this google spreadsheet: [ðŸ¤— Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing). In this blog post, we will highlight a few results of the benchmark including the best latency and throughput configurations. In addition to this, we deployed the [DistilBERT](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion) model we used for the benchmark as an API endpoint on 2 physical cores.""]",Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.,+800%
"What is the command to upload a spaCy pipeline to the Hugging Face Hub?
","[""\\* Launch [blog post](https://huggingface.co/blog/spacy)\n\\* spaCy v 3.1 [Announcement](https://explosion.ai/blog/spacy-v3-1#huggingface-hub)\n\\* spaCy [documentation](https://spacy.io/universe/project/spacy-huggingface-hub/)"", ""You can install spacy-huggingface-hub from pip:\n```bash\npip install spacy-huggingface-hub\n```\nYou can then check if the command has been registered successfully\n```bash\npython -m spacy huggingface-hub --help\n```\nTo push with the CLI, you can use the `huggingface-hub push` command as seen below. ```bash\npython -m spacy huggingface-hub push [whl\\_path] [--org] [--msg] [--local-repo] [--verbose]\n```\n| Argument | Type | Description |\n| -------------------- | ------------ | ----------------------------------------------------------------------------------------------------------------------------- |\n| `whl\\_path` | str / `Path` | The path to the `.whl` file packaged with [`spacy package`](https://spacy.io/api/cli#package). |\n| `--org`, `-o` | str | Optional name of organization to which the pipeline should be uploaded. |\n| `--msg`, `-m` | str | Commit message to use for update. Defaults to `\""Update spaCy pipeline\""`. |\n| `--local-repo`, `-l` | str / `Path` | Local path to the model repository (will be created if it doesn't exist)."", ""Using spaCy at Hugging Face\n`spaCy` is a popular library for advanced Natural Language Processing used widely across industry. `spaCy` makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text. ## Exploring spaCy models in the Hub\nThe official models from `spaCy` 3.3 are in the `spaCy` [Organization Page](https://huggingface.co/spacy). Anyone in the community can also share their `spaCy` models, which you can find by filtering at the left of the [models page](https://huggingface.co/models?library=spacy). All models on the Hub come up with useful features\n1. An automatically generated model card with label scheme, metrics, components, and more. 2. An evaluation sections at top right where you can look at the metrics. 3. Metadata tags that help for discoverability and contain information such as license and language. 4. An interactive widget you can use to play out with the model directly in the browser\n5. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_widget.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_widget-dark.png)\n\n## Using existing models\nAll `spaCy` models from the Hub can be directly installed using pip install. ```bash\npip install https://huggingface.co/spacy/en\\_core\\_web\\_sm/resolve/main/en\\_core\\_web\\_sm-any-py3-none-any.whl\n```\nTo find the link of interest, you can go to a repository with a `spaCy` model. When you open the repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model! ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_snippet.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_snippet-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_snippet2.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-spacy_snippet2-dark.png)\n\nOnce installed, you can load the model as any spaCy pipeline. ```python\n# Using spacy.load(). import spacy\nnlp = spacy.load(\""en\\_core\\_web\\_sm\"")\n# Importing as module. import en\\_core\\_web\\_sm\nnlp = en\\_core\\_web\\_sm.load()\n```\n## Sharing your models\n### Using the spaCy CLI (recommended)\nThe `spacy-huggingface-hub` library extends `spaCy` native CLI so people can easily push their packaged models to the Hub."", ""Make sure you are logged into Hugging Face before you begin:\n```bash\nhuggingface-cli login\n```\nThen add the `push\\_to\\_hub` argument to the script. This argument will create a repository with your Hugging Face username and the folder name specified in `output\\_dir`. To give your repository a specific name, use the `push\\_to\\_hub\\_model\\_id` argument to add it. The repository will be automatically listed under your namespace. The following example shows how to upload a model with a specific repository name:\n```bash\npython examples/pytorch/summarization/run\\_summarization.py\n--model\\_name\\_or\\_path t5-small \\\n--do\\_train \\\n--do\\_eval \\\n--dataset\\_name cnn\\_dailymail \\\n--dataset\\_config \""3.0.0\"" \\\n--source\\_prefix \""summarize: \"" \\\n--push\\_to\\_hub \\\n--push\\_to\\_hub\\_model\\_id finetuned-t5-cnn\\_dailymail \\\n--output\\_dir /tmp/tst-summarization \\\n--per\\_device\\_train\\_batch\\_size=4 \\\n--per\\_device\\_eval\\_batch\\_size=4 \\\n--overwrite\\_output\\_dir \\\n--predict\\_with\\_generate\n```"", ""Defaults to `hub` in the current working directory. |\n| `--verbose`, `-V` | bool | Output additional info for debugging, e.g. the full generated hub metadata. |\nYou can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`. ```bash\nhuggingface-cli login\npython -m spacy package ./en\\_ner\\_fashion ./output --build wheel\ncd ./output/en\\_ner\\_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en\\_ner\\_fashion-0.0.0-py3-none-any.whl\n```\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card. The command will output two things:\n\\* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en\\_core\\_web\\_sm\n\\* And how to install the pipeline directly from the Hub! ### From a Python script\nYou can use the `push` function from Python. It returns a dictionary containing the `\""url\""` and \""`whl\\_url`\"" of the published model and the wheel file, which you can later install with `pip install`. ```py\nfrom spacy\\_huggingface\\_hub import push\nresult = push(\""./en\\_ner\\_fashion-0.0.0-py3-none-any.whl\"")\nprint(result[\""url\""])\n```\n## Additional resources\n\\* spacy-huggingface-hub [library](https://github.com/explosion/spacy-huggingface-hub)."", ""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Upload files to the Hub\nSharing your files and work is an important aspect of the Hub. The `huggingface\\_hub` offers several options for uploading your files to the Hub. You can use these functions independently or integrate them into your library, making it more convenient for your users to interact with the Hub. This guide will show you how to push files:\n- without using Git. - that are very large with [Git LFS](https://git-lfs.github.com/). - with the `commit` context manager. - with the [`~Repository.push\\_to\\_hub`] function. Whenever you want to upload files to the Hub, you need to log in to your Hugging Face account. For more details about authentication, check out [this section](../quick-start#authentication). ## Upload a file\nOnce you've created a repository with [`create\\_repo`], you can upload a file to your repository using [`upload\\_file`]. Specify the path of the file to upload, where you want to upload the file to in the repository, and the name of the repository you want to add the file to. Depending on your repository type, you can optionally set the repository type as a `dataset`, `model`, or `space`. ```py\n>>> from huggingface\\_hub import HfApi\n>>> api = HfApi()\n>>> api.upload\\_file(\n..."", ""The Model Hub\n## What is the Model Hub? The Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing. Download pre-trained models with the [`huggingface\\_hub` client library](https://huggingface.co/docs/huggingface\\_hub/index), with ðŸ¤— [`Transformers`](https://huggingface.co/docs/transformers/index) for fine-tuning and other usages or with any of the over [15 integrated libraries](./models-libraries). You can even leverage the [Inference API](./models-inference) to use models in production settings. You can refer to the following video for a guide on navigating the Model Hub:\nTo learn how to upload models to the Hub, you can refer to the [Repositories Getting Started Guide](./repositories-getting-started)."", ""--\ntitle: \""Welcome spaCy to the Hugging Face Hub\""\nthumbnail: /blog/assets/23\\_spacy/thumbnail.png\nauthors:\n- user: osanseviero\n- user: ines\n---\n# Welcome spaCy to the Hugging Face Hub\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text. Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production! ### Finding models\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy. ### Widgets\nThis integration includes support for NER widgets, so all models with a NER component will have this out of the box! Coming soon there will be support for text classification and POS. [`spacy/en_core_web_sm`](/spacy/en_core_web_sm)\n\nHosted inference API\n\nToken Classification\n\nCompute\n\nThis model is currently loaded and running on the Inference API. JSON Output \nMaximize\n\n### Using existing models\nAll models from the Hub can be directly installed using `pip install`. ```bash\npip install https://huggingface.co/spacy/en\\_core\\_web\\_sm/resolve/main/en\\_core\\_web\\_sm-any-py3-none-any.whl\n```\n```python\n# Using spacy.load(). import spacy\nnlp = spacy.load(\""en\\_core\\_web\\_sm\"")\n# Importing as module. import en\\_core\\_web\\_sm\nnlp = en\\_core\\_web\\_sm.load()\n```\nWhen you open a repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model!""]","To upload a spaCy pipeline to the Hugging Face Hub, you can use the following command:

```bash
python -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]
```

Here, `whl_path` is the path to the `.whl` file packaged with `spacy package`. You can also include optional arguments such as `--org` for the organization name, `--msg` for a commit message, `--local-repo` for the local path to the model repository, and `--verbose` for additional debugging information.",python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
"What is the time and memory complexity of the NystrÃ¶mformer's approximation of self-attention?
","[""--\ntitle: \""NystrÃ¶mformer: Approximating self-attention in linear time and memory via the NystrÃ¶m method\""\nthumbnail: /blog/assets/86\\_nystromformer/thumbnail.png\nauthors:\n- user: asi\nguest: true\n---\n# NystrÃ¶mformer: Approximating self-attention in linear time and memory via the NystrÃ¶m method\n## Introduction\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. The [NystrÃ¶mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. NystrÃ¶mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the NystrÃ¶m method and how it can be adapted to approximate self-attention. ## NystrÃ¶m method for matrix approximation\nAt the heart of NystrÃ¶mformer is the NystrÃ¶m method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the NystrÃ¶m method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:\n\nRepresenting P as a block matrix\n\nWe now have four submatrices: \\\\(A\\_P, B\\_P, F\\_P,\\\\) and \\\\(C\\_P\\\\), with sizes \\\\(m \\times m, m \\times (n - m), (n - m) \\times m\\\\) and\n\\\\((n - m) \\times (n - m)\\\\) respectively. The \\\\(m\\\\) sampled columns are contained in \\\\(A\\_P\\\\) and \\\\(F\\_P\\\\), whereas the \\\\(m\\\\) sampled rows are contained in \\\\(A\\_P\\\\) and \\\\(B\\_P\\\\). So, the entries of \\\\(A\\_P, B\\_P,\\\\) and \\\\(F\\_P\\\\) are known to us, and we will estimate \\\\(C\\_P\\\\). According to the NystrÃ¶m method, \\\\(C\\_P\\\\) is given by:\n$$C\\_P = F\\_P A\\_P^+ B\\_P$$\nHere, \\\\(+\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). Thus, the NystrÃ¶m approximation of \\\\(P, \\hat{P}\\\\) can be written as:\n\nNystrÃ¶m approximation of P\n\nAs shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices."", ""Only at longer sequence lengths of 4096, a slight decrease in memory usage can be seen. Let's see what happens to the memory peak usage if we increase the size of the feed forward layer by a factor of 4 and reduce the number of attention heads also by a factor of 4 so that the feed forward layer becomes the memory bottleneck. ```\nconfig\\_no\\_chunk = ReformerConfig.from\\_pretrained(\""google/reformer-enwik8\"", chunk\\_size\\_feed\\_forward=0, num\\_attention\\_{h}eads=2, feed\\_forward\\_size=16384) # no chuck\nconfig\\_chunk = ReformerConfig.from\\_pretrained(\""google/reformer-enwik8\"", chunk\\_size\\_feed\\_forward=1, num\\_attention\\_{h}eads=2, feed\\_forward\\_size=16384) # feed forward chunk\nbenchmark\\_args = PyTorchBenchmarkArguments(sequence\\_lengths=[1024, 2048, 4096], batch\\_sizes=[8], models=[\""Reformer-No-Chunk\"", \""Reformer-Chunk\""], no\\_speed=True, no\\_env\\_print=True)\nbenchmark = PyTorchBenchmark(configs=[config\\_no\\_chunk, config\\_chunk], args=benchmark\\_args)\nresult = benchmark.run()\n```\n1 / 2\n2 / 2\n==================== INFERENCE - MEMORY - RESULT ====================\n--------------------------------------------------------------------------------\nModel Name Batch Size Seq Length Memory in MB\n--------------------------------------------------------------------------------\nReformer-No-Chunk 8 1024 3743\nReformer-No-Chunk 8 2048 5539\nReformer-No-Chunk 8 4096 9087\nReformer-Chunk 8 1024 2973\nReformer-Chunk 8 2048 3999\nReformer-Chunk 8 4096 6011\n--------------------------------------------------------------------------------\nNow a clear decrease in peak memory usage can be seen for longer input sequences."", ""Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap\n[Transformer-XL (2019)](https://arxiv.org/abs/1901.02860) showed that caching previously computed layer activations in a memory can boost performance on language modeling tasks (such as \\*enwik8\\*). Instead of just attending the current \\\\(n\\\\) input tokens, the model can also attend to the past \\\\(n\\_m\\\\) tokens, with \\\\(n\\_m\\\\) being the memory size of the model. Transformer-XL has a memory complexity of \\\\(O(n^2+ n n\\_m)\\\\), which shows that memory cost can increase significantly for very large \\\\(n\\_m\\\\). Hence, Transformer-XL has to eventually discard past activations from the memory when the number of cached activations gets larger than \\\\(n\\_m\\\\). Compressive Transformer addresses this problem by adding an additional compressed memory to efficiently cache past activations that would have otherwise eventually been discarded. This way the model can learn better long-range sequence dependencies having access to significantly more past activations. ![Compressive Tranformer recurrence](/blog/assets/14_long_range_transformers/CompressiveTransformer.png)\n\nFigure taken from Compressive Transfomer\n\nA compression factor \\\\(c\\\\) (equal to 3 in the illustration) is chosen to decide the rate at which past activations are compressed."", ""For more detail see Appendix A of the [paper](https://arxiv.org/pdf/2001.04451.pdf). For our example, multi-round LSH self-attention can be illustrated as follows. ![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific\\_images/master/reformer\\_benchmark/lsh\\_attention\\_4.png)\nGreat. That's it. Now we know how LSH self-attention works in Reformer. Regarding the memory complexity, we now have two terms that compete which each other to be the memory bottleneck: the dot-product: \\\\( \\mathcal{O}(n\\_{h} \\* n\\_{c} \\* l\\_{c}^2) = \\mathcal{O}(n \\* n\\_{h} \\* l\\_{c}) \\\\) and the required memory for LSH bucketing: \\\\( \\mathcal{O}(n \\* n\\_{h} \\* \\frac{n\\_{b}}{2}) \\\\) with \\\\( l\\_{c} \\\\) being the chunk length. Because for large \\\\( n \\\\), the number of buckets \\\\( \\frac{n\\_{b}}{2} \\\\) grows much faster than the chunk length \\\\( l\\_{c} \\\\), the user can again factorize the number of buckets `config.num\\_buckets` as explained [here](https://huggingface.co/transformers/model\\_doc/reformer.html#lsh-self-attention)."", ""The Authors' code can be found [here](https://github.com/allenai/longformer). ## Usage tips\n- Since the Longformer is based on RoBERTa, it doesn't have `token\\_type\\_ids`. You don't need to indicate which\ntoken belongs to which segment. Just separate your segments with the separation token `tokenizer.sep\\_token` (or\n``). - A transformer model replacing the attention matrices by sparse matrices to go faster. Often, the local context (e.g., what are the two tokens left and right?) is enough to take action for a given token. Some preselected input tokens are still given global attention, but the attention matrix has way less parameters, resulting in a speed-up. See the local attention section for more information. ## Longformer Self Attention\nLongformer self attention employs self attention on both a \""local\"" context and a \""global\"" context. Most tokens only\nattend \""locally\"" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\n`config.attention\\_window`. Note that `config.attention\\_window` can be of type `List` to define a\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \""globally\"" to all other tokens, as it is\nconventionally done for all tokens in `BertSelfAttention`. Note that \""locally\"" and \""globally\"" attending tokens are projected by different query, key and value matrices. Also note\nthat every \""locally\"" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \""globally\""\nattending tokens so that global attention is \\*symmetric\\*. The user can define which tokens attend \""locally\"" and which tokens attend \""globally\"" by setting the tensor\n`global\\_attention\\_mask` at run-time appropriately. All Longformer models employ the following logic for\n`global\\_attention\\_mask`:\n- 0: the token attends \""locally\"",\n- 1: the token attends \""globally\"". For more information please also refer to [`~LongformerModel.forward`] method. Using Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n\\_s \\times n\\_s)\\\\) to\n\\\\(\\mathcal{O}(n\\_s \\times w)\\\\), with \\\\(n\\_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize."", ""The abstract from the paper is the following:\n\\*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\ncommunity. To address this limitation, we propose NystrÃ¶mformer -- a model that exhibits favorable scalability as a\nfunction of sequence length. Our idea is based on adapting the NystrÃ¶m method to approximate standard self-attention\nwith O(n) complexity. The scalability of NystrÃ¶mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\nsequence length, and find that our NystrÃ¶mformer performs comparably, or in a few cases, even slightly better, than\nstandard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, NystrÃ¶mformer performs\nfavorably relative to other efficient self-attention methods. Our code is available at this https URL.\\*\nThis model was contributed by [novice03](https://huggingface.co/novice03). The original code can be found [here](https://github.com/mlpen/Nystromformer). ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## NystromformerConfig\n[[autodoc]] NystromformerConfig\n## NystromformerModel\n[[autodoc]] NystromformerModel\n- forward\n## NystromformerForMaskedLM\n[[autodoc]] NystromformerForMaskedLM\n- forward\n## NystromformerForSequenceClassification\n[[autodoc]] NystromformerForSequenceClassification\n- forward\n## NystromformerForMultipleChoice\n[[autodoc]] NystromformerForMultipleChoice\n- forward\n## NystromformerForTokenClassification\n[[autodoc]] NystromformerForTokenClassification\n- forward\n## NystromformerForQuestionAnswering\n[[autodoc]] NystromformerForQuestionAnswering\n- forward"", ""In tasks that require the model to process a large input sequence, long sequence models do not have to cut the input sequence to avoid memory overflow and thus have been shown to outperform standard \""BERT\""-like models \\*cf.\\* [Beltagy et al. (2020)](https://arxiv.org/abs/2004.05150). The Reformer pushes the limit of longe sequence modeling by its ability to process up to half a million tokens at once as shown in this [demo](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch\\_Reformer.ipynb). As a comparison, a conventional `bert-base-uncased` model limits the input length to only 512 tokens. In Reformer, each part of the standard transformer architecture is re-engineered to optimize for minimal memory requirement without a significant drop in performance. The memory improvements can be attributed to \\*\\*4\\*\\* features which the Reformer authors introduced to the transformer world:\n1. \\*\\*Reformer Self-Attention Layer\\*\\* - \\*How to efficiently implement self-attention without being restricted to a local context?\\*\n2. \\*\\*Chunked Feed Forward Layers\\*\\* - \\*How to get a better time-memory trade-off for large feed forward layers?\\*\n3. \\*\\*Reversible Residual Layers\\*\\* - \\*How to drastically reduce memory consumption in training by a smart residual architecture?\\*\n4. \\*\\*Axial Positional Encodings\\*\\* - \\*How to make positional encodings usable for extremely large input sequences?\\*\nThe goal of this blog post is to give the reader an \\*\\*in-depth\\*\\* understanding of each of the four Reformer features mentioned above."", ""\\* The FAVOR+ algorithm is the combination of multiple components. It is not clear which of these components have the most empirical impact on the performance, especially in view of the variety of modalities considered in this work. ## Reading group discussion\nThe developments in pre-trained transformer-based language models for natural language understanding and generation are impressive. Making these systems efficient for production purposes has become a very active research area. This emphasizes that we still have much to learn and build both on the methodological and practical sides to enable efficient and general deep learning based systems, in particular for applications that require modeling long-range inputs. The four papers above offer different ways to deal with the quadratic memory complexity of the self-attention mechanism, usually by reducing it to linear complexity. Linformer and Longformer both rely on the observation that the self-attention matrix does not contain \\\\(n Ã— n\\\\) worth of information (the attention matrix is low-rank and sparse). Performer gives a principled method to approximate the softmax-attention kernel (and any kernelizable attention mechanisms beyond softmax). Compressive Transformer offers an orthogonal approach to model long range dependencies based on recurrence. These different inductive biases have implications in terms of computational speed and generalization beyond the training setup. In particular, Linformer and Longformer lead to different trade-offs: Longformer explicitly designs the sparse attention patterns of the self-attention (fixed patterns) while Linformer learns the low-rank matrix factorization of the self-attention matrix. In our experiments, Longformer is less efficient than Linformer, and is currently highly dependent on implementation details. On the other hand, Linformerâ€™s decomposition only works for fixed context length (fixed at training) and cannot generalize to longer sequences without specific adaptation. Moreover, it cannot cache previous activations which can be extremely useful in the generative setup. Interestingly, Performer is conceptually different: it learns to approximate the softmax attention kernel without relying on any sparsity or low-rank assumption."", ""Next, let's look into how we can improve computational and memory efficiency by using better algorithms and an improved model architecture. ## 2. Flash Attention\nToday's top-performing LLMs share more or less the same fundamental architecture that consists of feed-forward layers, activation layers, layer normalization layers, and most crucially, self-attention layers. Self-attention layers are central to Large Language Models (LLMs) in that they enable the model to understand the contextual relationships between input tokens. However, the peak GPU memory consumption for self-attention layers grows \\*quadratically\\* both in compute and memory complexity with number of input tokens (also called \\*sequence length\\*) that we denote in the following by \\\\( N \\\\) . While this is not really noticeable for shorter input sequences (of up to 1000 input tokens), it becomes a serious problem for longer input sequences (at around 16000 input tokens). Let's take a closer look. The formula to compute the output \\\\( \\mathbf{O} \\\\) of a self-attention layer for an input \\\\( \\mathbf{X} \\\\) of length \\\\( N \\\\) is:\n$$ \\textbf{O} = \\text{Attn}(\\mathbf{X}) = \\mathbf{V} \\times \\text{Softmax}(\\mathbf{QK}^T) \\text{ with } \\mathbf{Q} = \\mathbf{W}\\_q \\mathbf{X}, \\mathbf{V} = \\mathbf{W}\\_v \\mathbf{X}, \\mathbf{K} = \\mathbf{W}\\_k \\mathbf{X} $$\n\\\\( \\mathbf{X} = (\\mathbf{x}\\_1, ... \\mathbf{x}\\_{N}) \\\\) is thereby the input sequence to the attention layer. The projections \\\\( \\mathbf{Q} \\\\) and \\\\( \\mathbf{K} \\\\) will each consist of \\\\( N \\\\) vectors resulting in the \\\\( \\mathbf{QK}^T \\\\) being of size \\\\( N^2 \\\\) . LLMs usually have multiple attention heads, thus doing multiple self-attention computations in parallel. Assuming, the LLM has 40 attention heads and runs in bfloat16 precision, we can calculate the memory requirement to store the \\\\( \\mathbf{QK^T} \\\\) matrices to be \\\\( 40 \\* 2 \\* N^2 \\\\) bytes. For \\\\( N=1000 \\\\) only around 50 MB of VRAM are needed, however, for \\\\( N=16000 \\\\) we would need 19 GB of VRAM, and for \\\\( N=100,000 \\\\) we would need almost 1TB just to store the \\\\( \\mathbf{QK}^T \\\\) matrices."", ""Also, notice that there is a DConv box. This corresponds to a skip connection added to the values using a 1D depthwise convolution. ## How is NystrÃ¶mformer implemented? The original implementation of NystrÃ¶mformer can be found [here](https://github.com/mlpen/Nystromformer) and the HuggingFace implementation can be found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/nystromformer/modeling\\_nystromformer.py). Let's take a look at a few lines of code (with some comments added) from the HuggingFace implementation. Note that some details such as normalization, attention masking, and depthwise convolution are avoided for simplicity. ```python\nkey\\_layer = self.transpose\\_for\\_scores(self.key(hidden\\_states)) # K\nvalue\\_layer = self.transpose\\_for\\_scores(self.value(hidden\\_states)) # V\nquery\\_layer = self.transpose\\_for\\_scores(mixed\\_query\\_layer) # Q\nq\\_landmarks = query\\_layer.reshape(\n-1,\nself.num\\_attention\\_heads,\nself.num\\_landmarks,\nself.seq\\_len // self.num\\_landmarks,\nself.attention\\_head\\_size,\n).mean(dim=-2) # \\tilde{Q}\nk\\_landmarks = key\\_layer.reshape(\n-1,\nself.num\\_attention\\_heads,\nself.num\\_landmarks,\nself.seq\\_len // self.num\\_landmarks,\nself.attention\\_head\\_size,\n).mean(dim=-2) # \\tilde{K}\nkernel\\_1 = torch.nn.functional.softmax(torch.matmul(query\\_layer, k\\_landmarks.transpose(-1, -2)), dim=-1) # \\tilde{F}\nkernel\\_2 = torch.nn.functional.softmax(torch.matmul(q\\_landmarks, k\\_landmarks.transpose(-1, -2)), dim=-1) # \\tilde{A} before pseudo-inverse\nattention\\_scores = torch.matmul(q\\_landmarks, key\\_layer.transpose(-1, -2)) # \\tilde{B} before softmax\nkernel\\_3 = nn.functional.softmax(attention\\_scores, dim=-1) # \\tilde{B}\nattention\\_probs = torch.matmul(kernel\\_1, self.iterative\\_inv(kernel\\_2)) # \\tilde{F} \\* \\tilde{A}\nnew\\_value\\_layer = torch.matmul(kernel\\_3, value\\_layer) # \\tilde{B} \\* V\ncontext\\_layer = torch.matmul(attention\\_probs, new\\_value\\_layer) # \\tilde{F} \\* \\tilde{A} \\* \\tilde{B} \\* V\n```\n## Using NystrÃ¶mformer with HuggingFace\nNystrÃ¶mformer for Masked Language Modeling (MLM) is available on HuggingFace. Currently, there are 4 checkpoints, corresponding to various sequence lengths: [`nystromformer-512`](https://huggingface.co/uw-madison/nystromformer-512), [`nystromformer-1024`](https://huggingface.co/uw-madison/nystromformer-1024), [`nystromformer-2048`](https://huggingface.co/uw-madison/nystromformer-2048), and [`nystromformer-4096`](https://huggingface.co/uw-madison/nystromformer-4096). The number of landmarks, \\\\(m\\\\), can be controlled using the `num\\_landmarks` parameter in the [`NystromformerConfig`](https://huggingface.co/docs/transformers/v4.18.0/en/model\\_doc/nystromformer#transformers.NystromformerConfig). Let's take a look at a minimal example of NystrÃ¶mformer for MLM:\n```python\nfrom transformers import AutoTokenizer, NystromformerForMaskedLM\nimport torch\ntokenizer = AutoTokenizer.from\\_pretrained(\""uw-madison/nystromformer-512\"")\nmodel = NystromformerForMaskedLM.from\\_pretrained(\""uw-madison/nystromformer-512\"")\ninputs = tokenizer(\""Paris is the [MASK] of France.\"", return\\_tensors=\""pt\"")\nwith torch.no\\_grad():\nlogits = model(\\*\\*inputs).logits\n# retrieve index of [MASK]\nmask\\_token\\_index = (inputs.input\\_ids == tokenizer.mask\\_token\\_id)[0].nonzero(as\\_tuple=True)[0]\npredicted\\_token\\_id = logits[0, mask\\_token\\_index].argmax(axis=-1)\ntokenizer.decode(predicted\\_token\\_id)\n```\n\nOutput:\n----------------------------------------------------------------------------------------------------\ncapital\n\nAlternatively, we can use the [pipeline API](https://huggingface.co/docs/transformers/main\\_classes/pipelines) (which handles all the complexity for us):\n```python\nfrom transformers import pipeline\nunmasker = pipeline('fill-mask', model='uw-madison/nystromformer-512')\nunmasker(\""Paris is the [MASK] of France.\"")\n```\n\nOutput:\n----------------------------------------------------------------------------------------------------\n[{'score': 0.829957902431488,\n'token': 1030,\n'token\\_str': 'capital',\n'sequence': 'paris is the capital of france.'},\n{'score': 0.022157637402415276,\n'token': 16081,\n'token\\_str': 'birthplace',\n'sequence': 'paris is the birthplace of france.'},\n{'score': 0.01904447190463543,\n'token': 197,\n'token\\_str': 'name',\n'sequence': 'paris is the name of france.'},\n{'score': 0.017583081498742104,\n'token': 1107,\n'token\\_str': 'kingdom',\n'sequence': 'paris is the kingdom of france.'},\n{'score': 0.005948934704065323,\n'token': 148,\n'token\\_str': 'city',\n'sequence': 'paris is the city of france.'}]\n\n## Conclusion\nNystrÃ¶mformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other linear self-attention schemes. In this blog post, we went over a high-level overview of the NystrÃ¶m method and how it can be leveraged for self-attention."", ""For this a `google/reformer-enwik8` model using global attention, a sequence length of over 16K results in a memory overflow. Now, let's activate \\*local\\* and \\*LSH\\* self-attention by using the model's default parameters. ```\nconfig = ReformerConfig.from\\_pretrained(\""google/reformer-enwik8\"")\nbenchmark\\_args = PyTorchBenchmarkArguments(sequence\\_lengths=[2048, 4096, 8192, 16384, 32768, 65436], batch\\_sizes=[1], models=[\""Reformer\""], no\\_speed=True, no\\_env\\_print=True)\nbenchmark = PyTorchBenchmark(configs=[config], args=benchmark\\_args)\nresult = benchmark.run()\n```\n1 / 1\nDoesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 GiB total capacity; 7.85 GiB already allocated; 1.74 GiB free; 9.06 GiB reserved in total by PyTorch)\nDoesn't fit on GPU. CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 11.17 GiB total capacity; 6.56 GiB already allocated; 3.99 GiB free; 6.81 GiB reserved in total by PyTorch)\n==================== INFERENCE - MEMORY - RESULT ====================\n--------------------------------------------------------------------------------\nModel Name Batch Size Seq Length Memory in MB\n--------------------------------------------------------------------------------\nReformer 1 2048 1785\nReformer 1 4096 2621\nReformer 1 8192 4281\nReformer 1 16384 7607\nReformer 1 32768 N/A\nReformer 1 65436 N/A\n--------------------------------------------------------------------------------\nAs expected using local and LSH self-attention is much more memory efficient for longer input sequences, so that the model runs out of memory only at 16K tokens for a 11GB RAM GPU in this notebook."", ""Next, let's look into how we can improve computational and memory efficiency by using better algorithms and an improved model architecture. # 2. Flash Attention: A Leap Forward\nToday's top-performing LLMs share more or less the same fundamental architecture that consists of feed-forward layers, activation layers, layer normalization layers, and most crucially, self-attention layers. Self-attention layers are central to Large Language Models (LLMs) in that they enable the model to understand the contextual relationships between input tokens. However, the peak GPU memory consumption for self-attention layers grows \\*quadratically\\* both in compute and memory complexity with number of input tokens (also called \\*sequence length\\*) that we denote in the following by \\\\( N \\\\) . While this is not really noticeable for shorter input sequences (of up to 1000 input tokens), it becomes a serious problem for longer input sequences (at around 16000 input tokens). Let's take a closer look. The formula to compute the output \\\\( \\mathbf{O} \\\\) of a self-attention layer for an input \\\\( \\mathbf{X} \\\\) of length \\\\( N \\\\) is:\n$$ \\textbf{O} = \\text{Attn}(\\mathbf{X}) = \\mathbf{V} \\times \\text{Softmax}(\\mathbf{QK}^T) \\text{ with } \\mathbf{Q} = \\mathbf{W}\\_q \\mathbf{X}, \\mathbf{V} = \\mathbf{W}\\_v \\mathbf{X}, \\mathbf{K} = \\mathbf{W}\\_k \\mathbf{X} $$\n\\\\( \\mathbf{X} = (\\mathbf{x}\\_1, ... \\mathbf{x}\\_{N}) \\\\) is thereby the input sequence to the attention layer. The projections \\\\( \\mathbf{Q} \\\\) and \\\\( \\mathbf{K} \\\\) will each consist of \\\\( N \\\\) vectors resulting in the \\\\( \\mathbf{QK}^T \\\\) being of size \\\\( N^2 \\\\) . LLMs usually have multiple attention heads, thus doing multiple self-attention computations in parallel. Assuming, the LLM has 40 attention heads and runs in bfloat16 precision, we can calculate the memory requirement to store the \\\\( \\mathbf{QK^T} \\\\) matrices to be \\\\( 40 \\* 2 \\* N^2 \\\\) bytes. For \\\\( N=1000 \\\\) only around 50 MB of VRAM are needed, however, for \\\\( N=16000 \\\\) we would need 19 GB of VRAM, and for \\\\( N=100,000 \\\\) we would need almost 1TB just to store the \\\\( \\mathbf{QK}^T \\\\) matrices.""]","The NystrÃ¶mformer approximates standard self-attention with a time and memory complexity of \(O(n)\), where \(n\) is the length of the input sequence. This is a significant improvement over the standard self-attention mechanism, which has a complexity of \(O(n^2)\).",O(n)
"What is the goal of the Named Entity Recognition task in token classification?
","[""These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. Two common types of token classification are:\n\\* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names. \\* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb). ```py\n>>> from transformers import pipeline\n>>> classifier = pipeline(task=\""ner\"")\n>>> preds = classifier(\""Hugging Face is a French company based in New York City.\"")\n>>> preds = [\n... {\n... \""entity\"": pred[\""entity\""],\n... \""score\"": round(pred[\""score\""], 4),\n... \""index\"": pred[\""index\""],\n... \""word\"": pred[\""word\""],\n... \""start\"": pred[\""start\""],\n... \""end\"": pred[\""end\""],\n..."", ""p align=\""center\""> ![FSNER LOGO](http://sayef.tech:8082/uploads/FSNER-LOGO-2.png) \n\nImplemented by  [sayef](https://huggingface.co/sayef) . ## Overview\nThe FSNER model was proposed in [Example-Based Named Entity Recognition](https://arxiv.org/abs/2008.10570) by Morteza Ziyadi, Yuting Sun, Abhishek Goswami, Jade Huang, Weizhu Chen. To identify entity spans in a new domain, it uses a train-free few-shot learning approach inspired by question-answering. ## Abstract\n----\n> We present a novel approach to named entity recognition (NER) in the presence of scarce data that we call example-based NER. Our train-free few-shot learning approach takes inspiration from question-answering to identify entity spans in a new and unseen domain. In comparison with the current state-of-the-art, the proposed method performs significantly better, especially when using a low number of support examples. ## Model Training Details\n-----\n| identifier | epochs | datasets |\n| ---------- |:----------:| :-----:|\n| [sayef/fsner-bert-base-uncased](https://huggingface.co/sayef/fsner-bert-base-uncased) | 10 | ontonotes5, conll2003, wnut2017, and fin (Alvarado et al.). |\n## Installation and Example Usage\n------\nYou can use the FSNER model in 3 ways:\n1. Install directly from PyPI: `pip install fsner` and import the model as shown in the code example below\nor\n2. Install from source: `python setup.py install` and import the model as shown in the code example below\nor\n3. Clone repo and change directory to `src` and import the model as shown in the code example below\n```python\nfrom fsner import FSNERModel, FSNERTokenizerUtils\nmodel = FSNERModel(\""sayef/fsner-bert-base-uncased\"")\ntokenizer = FSNERTokenizerUtils(\""sayef/fsner-bert-base-uncased\"")\n# size of query and supports must be the same."", ""The goal of this task is to classify each token in the input text into one of the following categories: person, organization, location, or none of them. - News Category Classification (NCC) on the Soham articles dataset from [IndicGLUE](https://aclanthology.org/2020.findings-emnlp.445/). The goal of this task is to predict the category to which belong the input text. We evaluated it during training on the NER task to check that everything was going well; as you can see on the following plot, this was indeed the case! Evaluation metrics of fine-tuned models on the NER task from different checkpoints of pre-trained models. At the end of training, we compared sahajBERT with three other pretrained language models: [XLM-R Large](https://arxiv.org/abs/1911.02116), [IndicBert](https://aclanthology.org/2020.findings-emnlp.445/), and [bnRoBERTa](https://huggingface.co/neuralspace-reverie/indic-transformers-bn-roberta). In the table below, you can see that our model has results comparable to the best Bengali language models available on HF Hub, even though our model has only ~18M trained parameters, while, for instance, XLM-R (a strong multilingual baseline), has ~559M parameters and was trained on several hundred V100 GPUs."", ""Named-Entity Recognition\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert\\_ner\\_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial\\_Analyst\\_AI\nTags: NER, TEXT, HIGHLIGHT\n## Introduction\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \""token\"") into different categories, such as names of people or names of locations, or different parts of speech. For example, given the sentence:\n> Does Chicago have any Pakistani restaurants? A named-entity recognition algorithm may identify:\n- \""Chicago\"" as a \\*\\*location\\*\\*\n- \""Pakistani\"" as an \\*\\*ethnicity\\*\\*\nand so on. Using `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team. Here is an example of a demo that you'll be able to build:\n$demo\\_ner\\_pipeline\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!"", ""# Token classification\nBased on the scripts [`run\\_ner.py`](https://github.com/huggingface/transformers/blob/main/examples/legacy/token-classification/run\\_ner.py). The following examples are covered in this section:\n\\* NER on the GermEval 2014 (German NER) dataset\n\\* Emerging and Rare Entities task: WNUTâ€™17 (English NER) dataset\nDetails and results for the fine-tuning provided by @stefan-it. ### GermEval 2014 (German NER) dataset\n#### Data (Download and pre-processing steps)\nData can be obtained from the [GermEval 2014](https://sites.google.com/site/germeval2014ner/data) shared task page. Here are the commands for downloading and pre-processing train, dev and test datasets. The original data format has four (tab-separated) columns, in a pre-processing step only the two relevant columns (token and outer span NER annotation) are extracted:\n```bash\ncurl -L 'https://drive.google.com/uc?export=download&id=1Jjhbal535VVz2ap4v4r\\_rN1UEHTdLK5P' \\\n| grep -v \""^#\"" | cut -f 2,3 | tr '\\t' ' ' > train.txt.tmp\ncurl -L 'https://drive.google.com/uc?export=download&id=1ZfRcQThdtAR5PPRjIDtrVP7BtXSCUBbm' \\\n| grep -v \""^#\"" | cut -f 2,3 | tr '\\t' ' ' > dev.txt.tmp\ncurl -L 'https://drive.google.com/uc?export=download&id=1u9mb7kNJHWQCWyweMDRMuTFoOHOfeBTH' \\\n| grep -v \""^#\"" | cut -f 2,3 | tr '\\t' ' ' > test.txt.tmp\n```\nThe GermEval 2014 dataset contains some strange \""control character\"" tokens like `'\\x96', '\\u200e', '\\x95', '\\xad' or '\\x80'`. One problem with these tokens is, that `BertTokenizer` returns an empty token for them, resulting in misaligned `InputExample`s."", ""The model config contains the label mapping in its id2label field. Using it, we can map every token to its corresponding label. The label O correspond to \""no entity\"", which is why we didn't see it in our results in the first slide. On top of the label and the probability, those results included the start and end character in the sentence. We will need to use the offset mapping of the tokenizer to get those (look at the video linked below if you don't know about them already). Then, looping through each token that has a label distinct from O, we can build the list of results we got with our first pipeline. The last step is to group together tokens that correspond to the same entity.This is why we had two labels for each type of entity: I-PER and B-PER for instance. It allows us to know if a token is in the same entity as the previous one.() Note that there are two ways of labelling used for token classification, one (in pink here) uses the B-PER label at the beginning of each new entity, but the other (in blue) only uses it to separate two adjacent entities of the same type. In both cases, we can flag a new entity each time we see a new label appearing (either with the I or B prefix) then take all the following tokens labelled the same, with an I-flag. This, coupled with the offset mapping to get the start and end characters allows us to get the span of texts for each entity."", ""3. The input embeddings are passed through multiple encoder layers to output some final hidden states. To use the pretrained model for text classification, add a sequence classification head on top of the base BERT model. The sequence classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and target to find the most likely label. Ready to try your hand at text classification? Check out our complete [text classification guide](tasks/sequence\\_classification) to learn how to finetune DistilBERT and use it for inference! ### Token classification\nTo use BERT for token classification tasks like named entity recognition (NER), add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and each token to find the most likely label. Ready to try your hand at token classification? Check out our complete [token classification guide](tasks/token\\_classification) to learn how to finetune DistilBERT and use it for inference! ### Question answering\nTo use BERT for question answering, add a span classification head on top of the base BERT model."", ""Then we calculate it for every class and take the overall average to evaluate our model. An example dataset used for this task is ConLL2003. Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels. You can extract important information from invoices using named entity recognition models, such as date, organization name or address. For more information about the Token classification task, check out the Hugging Face course. Question Answering video\nWelcome to the Hugging Face tasks series. In this video, we will take a look at the Question Answering task. Question answering is the task of extracting an answer in a given document. Question answering models take a context, which is the document you want to search in, and a question and return an answer. Note that the answer is not generated, but extracted from the context. This type of question answering is called extractive. The task is evaluated on two metrics, exact match and F1-Score. As the name implies, exact match looks for an exact match between the predicted answer and the correct answer. A common metric used is the F1-Score, which is calculated over tokens that are predicted correctly and incorrectly."", ""Convert the numbers to their label names to find out what the entities are:\n```py\n>>> label\\_list = wnut[\""train\""].features[f\""ner\\_tags\""].feature.names\n>>> label\\_list\n[\n\""O\"",\n\""B-corporation\"",\n\""I-corporation\"",\n\""B-creative-work\"",\n\""I-creative-work\"",\n\""B-group\"",\n\""I-group\"",\n\""B-location\"",\n\""I-location\"",\n\""B-person\"",\n\""I-person\"",\n\""B-product\"",\n\""I-product\"",\n]\n```\nThe letter that prefixes each `ner\\_tag` indicates the token position of the entity:\n- `B-` indicates the beginning of an entity. - `I-` indicates a token is contained inside the same entity (for example, the `State` token is a part of an entity like\n`Empire State Building`). - `0` indicates the token doesn't correspond to any entity. ## Preprocess\nThe next step is to load a DistilBERT tokenizer to preprocess the `tokens` field:\n```py\n>>> from transformers import AutoTokenizer\n>>> tokenizer = AutoTokenizer.from\\_pretrained(\""distilbert-base-uncased\"")\n```\nAs you saw in the example `tokens` field above, it looks like the input has already been tokenized. But the input actually hasn't been tokenized yet and you'll need to set `is\\_split\\_into\\_words=True` to tokenize the words into subwords. For example:\n```py\n>>> example = wnut[\""train\""][0]\n>>> tokenized\\_input = tokenizer(example[\""tokens\""], is\\_split\\_into\\_words=True)\n>>> tokens = tokenizer.convert\\_ids\\_to\\_tokens(tokenized\\_input[\""input\\_ids\""])\n>>> tokens\n['[CLS]', '@', 'paul', '##walk', 'it', \""'\"", 's', 'the', 'view', 'from', 'where', 'i', \""'\"", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']\n```\nHowever, this adds some special tokens `[CLS]` and `[SEP]` and the subword tokenization creates a mismatch between the input and labels. A single word corresponding to a single label may now be split into two subwords. You'll need to realign the tokens and labels by:\n1. Mapping all tokens to their corresponding word with the [`word\\_ids`](https://huggingface.co/docs/transformers/main\\_classes/tokenizer#transformers.BatchEncoding.word\\_ids) method. 2. Assigning the label `-100` to the special tokens `[CLS]` and `[SEP]` so they're ignored by the PyTorch loss function (see [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)). 3."", ""et's study how to preprocess a dataset for token classification! Token classification regroups any task that can be framed as labelling each word (or token) in a sentence, like identifying the persons, organizations and locations for instance. For our example, we will use the Conll dataset, in which we remove columns we won't use and rename the other ones to get to a dataset with just two columns: words and labels. If you have your own dataset for token classification, just make sure you clean your data to get to the same point, with one column containing words (as list of strings) and another containing labels (as integers spanning from to to your number of labels -1).() Make sure you have your label names stored somewhere - here we get them from the dataset features - so you are able to map the integers to some real labels when inspecting your data! Here we are doing named entity recognitions, so ours labels are either O for words that do not belong to any entity, LOC, for location, PER, for person, ORG for organization and MISC for miscellaneous. Each label has two versions: the B- labels indicate a word that begins an entity while the I- labels indicate a word that is inside an entity."", ""FrameworkSwitchCourse {fw} />\n# Token classification[[token-classification]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nThe first application we'll explore is token classification. This generic task encompasses any problem that can be formulated as \""attributing a label to each token in a sentence,\"" such as:\n- \\*\\*Named entity recognition (NER)\\*\\*: Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for \""no entity.\""\n- \\*\\*Part-of-speech tagging (POS)\\*\\*: Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.). - \\*\\*Chunking\\*\\*: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually `B-`) to any tokens that are at the beginning of a chunk, another label (usually `I-`) to tokens that are inside a chunk, and a third label (usually `O`) to tokens that don't belong to any chunk. Of course, there are many other types of token classification problem; those are just a few representative examples. In this section, we will fine-tune a model (BERT) on a NER task, which will then be able to compute predictions like this one:\n[![One-hot encoded labels for question answering.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner.png)\n![One-hot encoded labels for question answering.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner-dark.png)](/huggingface-course/bert-finetuned-ner)\nYou can find the model we'll train and upload to the Hub and double-check its predictions [here](https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn). ## Preparing the data[[preparing-the-data]]\nFirst things first, we need a dataset suitable for token classification. In this section we will use the [CoNLL-2003 dataset](https://huggingface.co/datasets/conll2003), which contains news stories from Reuters. ðŸ’¡ As long as your dataset consists of texts split into words with their corresponding labels, you will be able to adapt the data processing procedures described here to your own dataset. Refer back to [Chapter 5](/course/chapter5) if you need a refresher on how to load your own custom data in a `Dataset`. ### The CoNLL-2003 dataset[[the-conll-2003-dataset]]\nTo load the CoNLL-2003 dataset, we use the `load\\_dataset()` method from the ðŸ¤— Datasets library:\n```py\nfrom datasets import load\\_dataset\nraw\\_datasets = load\\_dataset(\""conll2003\"")\n```\nThis will download and cache the dataset, like we saw in [Chapter 3](/course/chapter3) for the GLUE MRPC dataset."", ""ote: the following transcripts are associated with Merve Noyan's videos in the Hugging Face Tasks playlist: https://www.youtube.com/playlist?list=PLo2EIpI\\_JMQtyEr-sLJSy5\\_SnLCb4vtQf\nToken Classification video\nWelcome to the Hugging Face tasks series! In this video weâ€™ll take a look at the token classification task. Token classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging. Letâ€™s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity. Another token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech. Token classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score."", ""> Named entities form the basis of many modern approaches to other tasks (like event clustering and summarization), but recall on\n> them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Six labels are available in the dataset. An overview can be found on this [page](http://noisy-text.github.io/2017/files/). #### Data (Download and pre-processing steps)\nThe dataset can be downloaded from the [official GitHub](https://github.com/leondz/emerging\\_entities\\_17) repository. The following commands show how to prepare the dataset for fine-tuning:\n```bash\nmkdir -p data\\_wnut\\_17\ncurl -L 'https://github.com/leondz/emerging\\_entities\\_17/raw/master/wnut17train.conll' | tr '\\t' ' ' > data\\_wnut\\_17/train.txt.tmp\ncurl -L 'https://github.com/leondz/emerging\\_entities\\_17/raw/master/emerging.dev.conll' | tr '\\t' ' ' > data\\_wnut\\_17/dev.txt.tmp\ncurl -L 'https://raw.githubusercontent.com/leondz/emerging\\_entities\\_17/master/emerging.test.annotated' | tr '\\t' ' ' > data\\_wnut\\_17/test.txt.tmp\n```\nLet's define some variables that we need for further pre-processing steps:\n```bash\nexport MAX\\_LENGTH=128\nexport BERT\\_MODEL=bert-large-cased\n```\nHere we use the English BERT large model for fine-tuning. The `preprocess.py` scripts splits longer sentences into smaller ones (once the max. subtoken length is reached):\n```bash\npython3 scripts/preprocess.py data\\_wnut\\_17/train.txt.tmp $BERT\\_MODEL $MAX\\_LENGTH > data\\_wnut\\_17/train.txt\npython3 scripts/preprocess.py data\\_wnut\\_17/dev.txt.tmp $BERT\\_MODEL $MAX\\_LENGTH > data\\_wnut\\_17/dev.txt\npython3 scripts/preprocess.py data\\_wnut\\_17/test.txt.tmp $BERT\\_MODEL $MAX\\_LENGTH > data\\_wnut\\_17/test.txt\n```\nIn the last pre-processing step, the `labels.txt` file needs to be generated. This file contains all available labels:\n```bash\ncat data\\_wnut\\_17/train.txt data\\_wnut\\_17/dev.txt data\\_wnut\\_17/test.txt | cut -d \"" \"" -f 2 | grep -v \""^$\""| sort | uniq > data\\_wnut\\_17/labels.txt\n```\n#### Run the Pytorch version\nFine-tuning with the PyTorch version can be started using the `run\\_ner.py` script. In this example we use a JSON-based configuration file. This configuration file looks like:\n```json\n{\n\""data\\_dir\"": \""./data\\_wnut\\_17\"",\n\""labels\"": \""./data\\_wnut\\_17/labels.txt\"",\n\""model\\_name\\_or\\_path\"": \""bert-large-cased\"",\n\""output\\_dir\"": \""wnut-17-model-1\"",\n\""max\\_seq\\_length\"": 128,\n\""num\\_train\\_epochs\"": 3,\n\""per\\_device\\_train\\_batch\\_size\"": 32,\n\""save\\_steps\"": 425,\n\""seed\"": 1,\n\""do\\_train\"": true,\n\""do\\_eval\"": true,\n\""do\\_predict\"": true,\n\""fp16\"": false\n}\n```\nIf your GPU supports half-precision training, please set `fp16` to `true`. Save this JSON-based configuration under `wnut\\_17.json`. The fine-tuning can be started with `python3 run\\_ner\\_old.py wnut\\_17.json`. #### Evaluation\nEvaluation on development dataset outputs the following:\n```bash\n05/29/2020 23:33:44 - INFO - \\_\\_main\\_\\_ - \\*\\*\\*\\*\\* Eval results \\*\\*\\*\\*\\*\n05/29/2020 23:33:44 - INFO - \\_\\_main\\_\\_ - eval\\_loss = 0.26505235286212275\n05/29/2020 23:33:44 - INFO - \\_\\_main\\_\\_ - eval\\_precision = 0.7008264462809918\n05/29/2020 23:33:44 - INFO - \\_\\_main\\_\\_ - eval\\_recall = 0.507177033492823\n05/29/2020 23:33:44 - INFO - \\_\\_main\\_\\_ - eval\\_f1 = 0.5884802220680084\n05/29/2020 23:33:44 - INFO - \\_\\_main\\_\\_ - epoch = 3.0\n```\nOn the test dataset the following results could be achieved:\n```bash\n05/29/2020 23:33:44 - INFO - transformers.trainer - \\*\\*\\*\\*\\* Running Prediction \\*\\*\\*\\*\\*\n05/29/2020 23:34:02 - INFO - \\_\\_main\\_\\_ - eval\\_loss = 0.30948806500973547\n05/29/2020 23:34:02 - INFO - \\_\\_main\\_\\_ - eval\\_precision = 0.5840108401084011\n05/29/2020 23:34:02 - INFO - \\_\\_main\\_\\_ - eval\\_recall = 0.3994439295644115\n05/29/2020 23:34:02 - INFO - \\_\\_main\\_\\_ - eval\\_f1 = 0.47440836543753434\n```\nWNUTâ€™17 is a very difficult task. Current state-of-the-art results on this dataset can be found [here](https://nlpprogress.com/english/named\\_entity\\_recognition.html)."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Token classification\n[[open-in-colab]]\nToken classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization. This guide will show you how to:\n1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [WNUT 17](https://huggingface.co/datasets/wnut\\_17) dataset to detect new entities."", ""et's have a look inside the token classification pipeline. In the pipeline video, we looked at the different applications the Transformers library supports out of the box, one of them being token classification, for instance predicting for each word in a sentence whether they correspond to a person, an organization or a location. We can even group together the tokens corresponding to the same entity, for instance all the tokens that formed the word Sylvain here, or Hugging and Face. The token classification pipeline works the same way as the text classification pipeline we studied in a previous video. There are three steps: the tokenization, the model, and the post processing. The first two steps are identical to the text classification pipeline, except we use an auto token classification model instead of a sequence classification one. We tokenize our text then feed it to the model. Instead of getting one number for each possible label for the whole sentence, we get one number for each of the possible 9 labels for every token in the sentence, here 19. Like all the other models of the Transformers library, our model outputs logits, which we turn into predictions by using a SoftMax. We also get the predicted label for each token by taking the maximum prediction (since the softmax function preserves the order, we could have done it on the logits if we had no need of the predictions)."", ""The label `B-XXX` indicates the token is at the beginning of an entity `XXX` and the label `I-XXX` indicates the token is inside the entity `XXX`. For instance, in the current example we would expect our model to classify the token `S` as `B-PER` (beginning of a person entity) and the tokens `##yl`, `##va` and `##in` as `I-PER` (inside a person entity). You might think the model was wrong in this case as it gave the label `I-PER` to all four of these tokens, but that's not entirely true. There are actually two formats for those `B-` and `I-` labels: \\*IOB1\\* and \\*IOB2\\*. The IOB2 format (in pink below), is the one we introduced whereas in the IOB1 format (in blue), the labels beginning with `B-` are only ever used to separate two adjacent entities of the same type. The model we are using was fine-tuned on a dataset using that format, which is why it assigns the label `I-PER` to the `S` token. ![IOB1 vs IOB2 format](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg)\n![IOB1 vs IOB2 format](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg)\n\nWith this map, we are ready to reproduce (almost entirely) the results of the first pipeline -- we can just grab the score and label of each token that was not classified as `O`:\n```py\nresults = []\ntokens = inputs.tokens()\nfor idx, pred in enumerate(predictions):\nlabel = model.config.id2label[pred]\nif label != \""O\"":\nresults.append(\n{\""entity\"": label, \""score\"": probabilities[idx][pred], \""word\"": tokens[idx]}\n)\nprint(results)\n```\n```python out\n[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},\n{'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},\n{'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},\n{'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},\n{'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},\n{'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},\n{'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},\n{'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]\n```\nThis is very similar to what we had before, with one exception: the pipeline also gave us information about the `start` and `end` of each entity in the original sentence."", ""The model config contains the label mapping in its id2label field. Using it, we can map every token to its corresponding label. The label O correspond to \""no entity\"", which is why we didn't see it in our results in the first slide. On top of the label and the probability, those results included the start and end character in the sentence. We will need to use the offset mapping of the tokenizer to get those (look at the video linked below if you don't know about them already). Then, looping through each token that has a label distinct from O, we can build the list of results we got with our first pipeline. The last step is to group together tokens that correspond to the same entity. This is why we had two labels for each type of entity: I-PER and B-PER for instance. It allows us to know if a token is in the same entity as the previous one.() Note that there are two ways of labelling used for token classification, one (in pink here) uses the B-PER label at the beginning of each new entity, but the other (in blue) only uses it to separate two adjacent entities of the same type. In both cases, we can flag a new entity each time we see a new label appearing (either with the I or B prefix) then take all the following tokens labelled the same, with an I-flag."", ""et's have a look inside the token classification pipeline. In the pipeline video, we looked at the different applications the Transformers library supports out of the box, one of them being token classification, for instance predicting for each word in a sentence whether they correspond to a person, an organization or a location. We can even group together the tokens corresponding to the same entity, for instance all the tokens that formed the word Sylvain here, or Hugging and Face. The token classification pipeline works the same way as the text classification pipeline we studied in a previous video. There are three steps: the tokenization, the model, and the post processing. The first two steps are identical to the text classification pipeline, except we use an auto token classification model instead of a sequence classification one. We tokenize our text then feed it to the model. Instead of getting one number for each possible label for the whole sentence, we get one number for each of the possible 9 labels for every token in the sentence, here 19. Like all the other models of the Transformers library, our model outputs logits, which we turn into predictions by using a SoftMax. We also get the predicted label for each token by taking the maximum prediction (since the softmax function preserves the order, we could have done it on the logits if we had no need of the predictions).""]","The goal of the Named Entity Recognition (NER) task in token classification is to identify and classify entities in a piece of text into predefined categories such as person, organization, location, or none of them. This involves labeling each token in the input text according to its corresponding entity category. For example, in the sentence ""Hugging Face is a French company based in New York City,"" the NER task would identify ""Hugging Face"" as an organization and ""New York City"" as a location.","The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization."
"What is the resolution of images used by the CLIPSeg model?
","[""For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm\\_source=hf&utm\\_medium=blog&utm\\_campaign=clipseg). Before we describe how to do that, letâ€™s first take a look at how CLIPSeg works. ## CLIP: the magic model behind CLIPSeg\n[CLIP](https://huggingface.co/docs/transformers/main/en/model\\_doc/clip), which stands for \\*\\*C\\*\\*ontrastive \\*\\*L\\*\\*anguageâ€“\\*\\*I\\*\\*mage \\*\\*P\\*\\*re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract \\*representation\\* of your input. This abstract representation, also called an \\*embedding\\*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together)."", ""cropping the image and darkening the background. ```python\nurl = \""https://i.imgur.com/mRSORqz.jpg\""\nalternative\\_prompt = Image.open(requests.get(url, stream=True).raw)\nalternative\\_prompt\n```\n\n```python\nencoded\\_alternative\\_prompt = processor(images=[alternative\\_prompt], return\\_tensors=\""pt\"")\n# predict\nwith torch.no\\_grad():\noutputs = model(\\*\\*encoded\\_image, conditional\\_pixel\\_values=encoded\\_alternative\\_prompt.pixel\\_values)\npreds = outputs.logits.unsqueeze(1)\npreds = torch.transpose(preds, 0, 1)\n```\n```python\n\\_, ax = plt.subplots(1, 2, figsize=(6, 4))\n[a.axis('off') for a in ax.flatten()]\nax[0].imshow(image)\nax[1].imshow(torch.sigmoid(preds[0]))\n```\n\nIn this case, the result is pretty much the same. This is probably\nbecause the coffee cup was already separated well from the background in\nthe original image. ## Using CLIPSeg to pre-label images on Segments.ai\nAs you can see, the results from CLIPSeg are a little fuzzy and very\nlow-res. If we want to obtain better results, you can fine-tune a\nstate-of-the-art segmentation model, as explained in [our previous\nblogpost](https://huggingface.co/blog/fine-tune-segformer). To finetune\nthe model, we\\'ll need labeled data. In this section, we\\'ll show you\nhow you can use CLIPSeg to create some rough segmentation masks and then\nrefine them on\n[Segments.ai](https://segments.ai/?utm\\_source=hf&utm\\_medium=blog&utm\\_campaign=clipseg),\na labeling platform with smart labeling tools for image segmentation."", ""While CLIPSeg is trained on simple object descriptions (e.g., â€œa carâ€), its CLIP backbone is pre-trained on engineered text templates (e.g., â€œan image of a carâ€, â€œa photo of a carâ€) and kept frozen during training. Once the inputs are preprocessed, we can perform inference to get a binary segmentation map of shape (height, width) for each text query. ```py\nimport torch\nwith torch.no\\_grad():\noutputs = model(\\*\\*inputs)\nlogits = outputs.logits\nprint(logits.shape)\n>>> torch.Size([3, 352, 352])\n```\nLetâ€™s visualize the results to see how well CLIPSeg performed (code is adapted from [this post](https://huggingface.co/blog/clipseg-zero-shot)). ```py\nimport matplotlib.pyplot as plt\nlogits = logits.unsqueeze(1)\n\\_, ax = plt.subplots(1, len(texts) + 1, figsize=(3\\*(len(texts) + 1), 12))\n[a.axis('off') for a in ax.flatten()]\nax[0].imshow(image)\n[ax[i+1].imshow(torch.sigmoid(logits[i][0])) for i in range(len(texts))];\n[ax[i+1].text(0, -15, prompt) for i, prompt in enumerate(texts)]\n```\n\n![CLIPSeg results](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/clipseg_result.png)\n\nAmazing, isnâ€™t it?"", ""This time, we\\'ll also\nscale up the outputs so that they match the input image\\'s size. ```python\nfrom torch import nn\ninputs = processor(text=category\\_names, images=[image] \\* len(category\\_names), padding=\""max\\_length\"", return\\_tensors=\""pt\"")\n# predict\nwith torch.no\\_grad():\noutputs = model(\\*\\*inputs)\n# resize the outputs\npreds = nn.functional.interpolate(\noutputs.logits.unsqueeze(1),\nsize=(image.size[1], image.size[0]),\nmode=\""bilinear\""\n)\n```\nAnd we can visualize the results again. ```python\nlen\\_cats = len(category\\_names)\n\\_, ax = plt.subplots(1, len\\_cats + 1, figsize=(3\\*(len\\_cats + 1), 4))\n[a.axis('off') for a in ax.flatten()]\nax[0].imshow(image)\n[ax[i+1].imshow(torch.sigmoid(preds[i][0])) for i in range(len\\_cats)];\n[ax[i+1].text(0, -15, category\\_name) for i, category\\_name in enumerate(category\\_names)];\n```\n\nNow we have to combine the predictions to a single segmented image. We\\'ll simply do this by taking the category with the greatest sigmoid\nvalue for each patch. We\\'ll also make sure that all the values under a\ncertain threshold do not count. ```python\nthreshold = 0.1\nflat\\_preds = torch.sigmoid(preds.squeeze()).reshape((preds.shape[0], -1))\n# Initialize a dummy \""unlabeled\"" mask with the threshold\nflat\\_preds\\_with\\_treshold = torch.full((preds.shape[0] + 1, flat\\_preds.shape[-1]), threshold)\nflat\\_preds\\_with\\_treshold[1:preds.shape[0]+1,:] = flat\\_preds\n# Get the top mask index for each pixel\ninds = torch.topk(flat\\_preds\\_with\\_treshold, 1, dim=0).indices.reshape((preds.shape[-2], preds.shape[-1]))\n```\nLet\\'s quickly visualize the result. ```python\nplt.imshow(inds)\n```\n\nLastly, we can upload the prediction to Segments.ai. To do that, we\\'ll\nfirst convert the bitmap to a png file, then we\\'ll upload this file to\nthe Segments, and finally we\\'ll add the label to the sample. ```python\nfrom segments.utils import bitmap2file\nimport numpy as np\ninds\\_np = inds.numpy().astype(np.uint32)\nunique\\_inds = np.unique(inds\\_np).tolist()\nf = bitmap2file(inds\\_np, is\\_segmentation\\_bitmap=True)\nasset = segments\\_client.upload\\_asset(f, \""clipseg\\_prediction.png\"")\nattributes = {\n'format\\_version': '0.1',\n'annotations': [{\""id\"": i, \""category\\_id\"": i} for i in unique\\_inds if i != 0],\n'segmentation\\_bitmap': { 'url': asset.url },\n}\nsegments\\_client.add\\_label(sample.uuid, 'ground-truth', attributes)\n```\nIf you take a look at the [uploaded prediction on\nSegments.ai](https://segments.ai/admin-tobias/clipseg/samples/71a80d39-8cf3-4768-a097-e81e0b677517/ground-truth),\nyou can see that it\\'s not perfect. However, you can manually correct\nthe biggest mistakes, and then you can use the corrected dataset to\ntrain a better model than CLIPSeg. ## Conclusion\nCLIPSeg is a zero-shot segmentation model that works with both text and image prompts. The model adds a decoder to CLIP and can segment almost anything. However, the output segmentation masks are still very low-res for now, so youâ€™ll probably still want to fine-tune a different segmentation model if accuracy is important. Note that there's more research on zero-shot segmentation currently being conducted, so you can expect more models to be added in the near future. One example is [GroupViT](https://huggingface.co/docs/transformers/model\\_doc/groupvit), which is already available in ðŸ¤— Transformers. To stay up to date with the latest news in segmentation research, you can follow us on Twitter: [@TobiasCornille](https://twitter.com/tobiascornille), [@NielsRogge](https://twitter.com/nielsrogge), and [@huggingface](https://twitter.com/huggingface). If youâ€™re interested in learning how to fine-tune a state-of-the-art segmentation model, check out our previous blog post: [https://huggingface.co/blog/fine-tune-segformer](https://huggingface.co/blog/fine-tune-segformer).""]","The CLIPSeg model uses images of 352 x 352 pixels, which results in low-resolution outputs.",352 x 352 pixels
"What can you use Gradio for?
","[""It uses Gradio's Audio and Label components."", ""Gradio Demo: image\\_segmentation\n### Simple image segmentation using gradio's AnnotatedImage component."", ""The Gradio Client is mostly used with apps hosted on [Hugging Face Spaces](https://hf.space), but your app can be hosted anywhere, such as your own server. \\*\\*Prequisites\\*\\*: To use the Gradio client, you do \\_not\\_ need to know the `gradio` library in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components. ## Installation\nThe lightweight `@gradio/client` package can be installed from the npm registry with a package manager of your choice and support node version 18 and above:\n```bash\nnpm i @gradio/client\n```\n## Connecting to a running Gradio App\nStart by connecting instantiating a `client` instance and connecting it to a Gradio app that is running on Hugging Face Spaces or generally anywhere on the web. ## Connecting to a Hugging Face Space\n```js\nimport { client } from \""@gradio/client\"";\nconst app = client(\""abidlabs/en2fr\""); // a Space that translates from English to French\n```\nYou can also connect to private Spaces by passing in your HF token with the `hf\\_token` property of the options parameter. You can get your HF token here: https://huggingface.co/settings/tokens\n```js\nimport { client } from \""@gradio/client\"";\nconst app = client(\""abidlabs/my-private-space\"", { hf\\_token=\""hf\\_...\"" })\n```\n## Duplicating a Space for private use\nWhile you can use any public Space as an API, you may get rate limited by Hugging Face if you make too many requests. For unlimited usage of a Space, simply duplicate the Space to create a private Space, and then use it to make as many requests as you'd like! The `@gradio/client` exports another function, `duplicate`, to make this process simple (you'll need to pass in your [Hugging Face token](https://huggingface.co/settings/tokens)). `duplicate` is almost identical to `client`, the only difference is under the hood:\n```js\nimport { client } from \""@gradio/client\"";\nconst response = await fetch(\n\""https://audio-samples.github.io/samples/mp3/blizzard\\_unconditional/sample-0.mp3\""\n);\nconst audio\\_file = await response.blob();\nconst app = await duplicate(\""abidlabs/whisper\"", { hf\\_token: \""hf\\_...\"" });\nconst transcription = app.predict(\""/predict\"", [audio\\_file]);\n```\nIf you have previously duplicated a Space, re-running `duplicate` will \\_not\\_ create a new Space."", ""Gradio Blocks Party[[gradio-blocks-party]]\nAlong with the release of the Gradio chapter of the course, Hugging Face hosted a community event on building cool machine learning demos using the new Gradio Blocks feature. You can find all the demos that the community created under the [`Gradio-Blocks`](https://huggingface.co/Gradio-Blocks) organisation on the Hub. Here's a few examples from the winners:\n\\*\\*Natural language to SQL\\*\\*"", ""Setting Up a Demo for Maximum Performance\nTags: CONCURRENCY, LATENCY, PERFORMANCE\nLet's say that your Gradio demo goes \\_viral\\_ on social media -- you have lots of users trying it out simultaneously, and you want to provide your users with the best possible experience or, in other words, minimize the amount of time that each user has to wait in the queue to see their prediction. How can you configure your Gradio demo to handle the most traffic? In this Guide, we dive into some of the parameters of Gradio's `.queue()` method as well as some other related parameters, and discuss how to set these parameters in a way that allows you to serve lots of users simultaneously with minimal latency. This is an advanced guide, so make sure you know the basics of Gradio already, such as [how to create and launch a Gradio Interface](https://gradio.app/guides/quickstart/). Most of the information in this Guide is relevant whether you are hosting your demo on [Hugging Face Spaces](https://hf.space) or on your own server. ## Overview of Gradio's Queueing System\nBy default, every Gradio demo includes a built-in queuing system that scales to thousands of requests."", ""Gradio-Lite: Serverless Gradio Running Entirely in Your Browser\nTags: SERVERLESS, BROWSER, PYODIDE\nGradio is a popular Python library for creating interactive machine learning apps. Traditionally, Gradio applications have relied on server-side infrastructure to run, which can be a hurdle for developers who need to host their applications. Enter Gradio-lite (`@gradio/lite`): a library that leverages [Pyodide](https://pyodide.org/en/stable/) to bring Gradio directly to your browser. In this blog post, we'll explore what `@gradio/lite` is, go over example code, and discuss the benefits it offers for running Gradio applications. ## What is `@gradio/lite`? `@gradio/lite` is a JavaScript library that enables you to run Gradio applications directly within your web browser. It achieves this by utilizing Pyodide, a Python runtime for WebAssembly, which allows Python code to be executed in the browser environment. With `@gradio/lite`, you can \\*\\*write regular Python code for your Gradio applications\\*\\*, and they will \\*\\*run seamlessly in the browser\\*\\* without the need for server-side infrastructure. ## Getting Started\nLet's build a \""Hello World\"" Gradio app in `@gradio/lite`\n### 1. Import JS and CSS\nStart by creating a new HTML file, if you don't have one already. Importing the JavaScript and CSS corresponding to the `@gradio/lite` package by using the following code:\n```html\n\n```\nNote that you should generally use the latest version of `@gradio/lite` that is available. You can see the [versions available here](https://www.jsdelivr.com/package/npm/@gradio/lite?tab=files)."", ""Key Features\nLet's go through some of the key features of Gradio. This guide is intended to be a high-level overview of various things that you should be aware of as you build your demo. Where appropriate, we link to more detailed guides on specific topics. 1. [Components](#components)\n2. [Queuing](#queuing)\n3. [Streaming outputs](#streaming-outputs)\n4. [Streaming inputs](#streaming-inputs)\n5. [Alert modals](#alert-modals)\n6. [Styling](#styling)\n7. [Progress bars](#progress-bars)\n8. [Batch functions](#batch-functions)\n## Components\nGradio includes more than 30 pre-built components (as well as many user-built \\_custom components\\_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on. Each component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[\""webcam\"", \""upload\""]`). \\*\\*Static and Interactive Components\\*\\*\nEvery component has a \\_static\\_ version that is designed to \\*display\\* data, and most components also have an \\_interactive\\_ version designed to let users input or modify the data. Typically, you don't need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports. \\*\\*Preprocessing and Postprocessing\\*\\*\nWhen a component is used as an input, Gradio automatically handles the \\_preprocessing\\_ needed to convert the data from a type sent by the user's browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array). Similarly, when a component is used as an output, Gradio automatically handles the \\_postprocessing\\_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (a gallery of images). Consider an example demo with three input components (`gr.Textbox`, `gr.Number`, and `gr.Image`) and two outputs (`gr.Number` and `gr.Gallery`) that serve as a UI for your image-to-image generation model. Below is a diagram of what our preprocessing will send to the model and what our postprocessing will require from it. ![](https://github.com/gradio-app/gradio/blob/main/guides/assets/dataflow.svg?raw=true)\nIn this image, the following preprocessing steps happen to send the data from the browser to your function:\n\\* The text in the textbox is converted to a Python `str` (essentially no preprocessing)\n\\* The number in the number input in converted to a Python `float` (essentially no preprocessing)\n\\* Most importantly, ihe image supplied by the user is converted to a `numpy.array` representation of the RGB values in the image\nImages are converted to NumPy arrays because they are a common format for machine learning workflows. You can control the \\_preprocessing\\_ using the component's parameters when constructing the component. For example, if you instantiate the `Image` component with the following parameters, it will preprocess the image to the `PIL` format instead:\n```py\nimg = gr.Image(type=\""pil\"")\n```\nPostprocessing is even simpler! Gradio automatically recognizes the format of the returned data (e.g. does the user's function return a `numpy` array or a `str` filepath for the `gr.Image` component?) and postprocesses it appropriately into a format that can be displayed by the browser. So in the image above, the following postprocessing steps happen to send the data returned from a user's function to the browser:\n\\* The `float` is displayed as a number and displayed directly to the user\n\\* The list of string filepaths (`list[str]`) is interpreted as a list of image filepaths and displayed as a gallery in the browser\nTake a look at the [Docs](https://gradio.app/docs) to see all the parameters for each Gradio component."", ""Gradio, check![[gradio-check]]\nThis wraps up the chapter on building cool ML demos with Gradio - we hope you enjoyed it! To recap, in this chapter we learned:\n- How to create Gradio demos with the high-level `Interface` API, and how to configure different input and output modalities. - Different ways to share Gradio demos, through temporary links and hosting on [Hugging Face Spaces](https://huggingface.co/spaces). - How to integrate Gradio demos with models and Spaces on the Hugging Face Hub. - Advanced features like storing state in a demo or providing authentication. - How to have full control of the data flow and layout of your demo with Gradio Blocks. If you'd like to test your understanding of the concepts covered in this chapter, check out the quiz in the next section! ## Where to next?[[where-to-next]]\nIf you want to learn more about Gradio you can\n- Take a look at [Demos](https://github.com/gradio-app/gradio/tree/main/demo) in the repo, there are quite a lot of examples there. - See the [Guides](https://gradio.app/guides/) page, where you can find guides about cool and advanced features."", ""### An Overview of Gradio\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do? #### Chatbots with `gr.ChatInterface`\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast). #### Custom Demos with `gr.Blocks`\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction â€” still all in Python. You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners). #### The Gradio Python & JavaScript Ecosystem\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\\* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio\\_client`): query any Gradio app programmatically in Python. \\* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript. \\* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \\* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications â€” for free!"", ""div align=\""center\"">\n[![gradio](readme_files/gradio.svg)](https://gradio.app)  \n[![gradio-backend](https://github.com/gradio-app/gradio/actions/workflows/backend.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/backend.yml)\n[![gradio-ui](https://github.com/gradio-app/gradio/actions/workflows/ui.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/ui.yml)\n[![PyPI](https://img.shields.io/pypi/v/gradio)](https://pypi.org/project/gradio/)\n[![PyPI downloads](https://img.shields.io/pypi/dm/gradio)](https://pypi.org/project/gradio/)\n![Python version](https://img.shields.io/badge/python-3.8+-important)\n[![Twitter follow](https://img.shields.io/twitter/follow/gradio?style=social&label=follow)](https://twitter.com/gradio)\n[Website](https://gradio.app)\n| [Documentation](https://gradio.app/docs/)\n| [Guides](https://gradio.app/guides/)\n| [Getting Started](https://gradio.app/getting\\_started/)\n| [Examples](demo/)\n| [ä¸­æ–‡](readme\\_files/zh-cn#readme)\n# Gradio: Build Machine Learning Web Apps â€” in Python\n$getting\\_started\n## Questions?"", ""Getting Started with the Gradio JavaScript client\nTags: CLIENT, API, SPACES\nThe Gradio JavaScript client makes it very easy to use any Gradio app as an API. As an example, consider this [Hugging Face Space that transcribes audio files](https://huggingface.co/spaces/abidlabs/whisper) that are recorded from the microphone. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/whisper-screenshot.jpg)\nUsing the `@gradio/client` library, we can easily use the Gradio as an API to transcribe audio files programmatically. Here's the entire code to do it:\n```js\nimport { client } from \""@gradio/client\"";\nconst response = await fetch(\n\""https://github.com/audio-samples/audio-samples.github.io/raw/master/samples/wav/ted\\_speakers/SalmanKhan/sample-1.wav\""\n);\nconst audio\\_file = await response.blob();\nconst app = await client(\""abidlabs/whisper\"");\nconst transcription = await app.predict(\""/predict\"", [audio\\_file]);\nconsole.log(transcription.data);\n// [ \""I said the same phrase 30 times.\"" ]\n```\nThe Gradio client works with any hosted Gradio app, whether it be an image generator, a text summarizer, a stateful chatbot, a tax calculator, or anything else!""]","Gradio can be used to build interactive machine learning applications quickly and easily. It allows you to create demos for various tasks, such as:

1. **Image Processing**: You can create applications that handle image inputs and outputs, such as image classification, segmentation, or generation.

2. **Text Processing**: Gradio can be used for applications that involve text, such as text summarization, translation, or sentiment analysis.

3. **Audio Processing**: You can build applications that work with audio files, such as transcription or audio classification.

4. **Chatbots**: Gradio provides a `gr.ChatInterface` class specifically designed for creating chatbot UIs.

5. **Custom Demos**: With the `gr.Blocks` class, you can design more complex applications with flexible layouts and data flows.

6. **API Integration**: Gradio apps can be used as APIs, allowing you to integrate them into other applications or services.

Overall, Gradio simplifies the process of creating and sharing machine learning models and applications, making it accessible for developers and researchers.","Create a demo for your machine learning model, share your machine learning model with others, and debug your model."
"What TensorFlow API function is used to load a saved tensor file?
","[""Tensorflow API\n[[autodoc]] safetensors.tensorflow.load\\_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save\\_file\n[[autodoc]] safetensors.tensorflow.save"", ""--\ntitle: Deploying TensorFlow Vision Models in Hugging Face with TF Serving\nthumbnail: /blog/assets/90\\_tf\\_serving\\_vision/thumbnail.png\nauthors:\n- user: sayakpaul\nguest: true\n---\n# Deploying TensorFlow Vision Models in Hugging Face with TF Serving\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/111_tf_serving_vision.ipynb)\nIn the past few months, the Hugging Face team and external contributors\nadded a variety of vision models in TensorFlow to Transformers. This\nlist is growing comprehensively and already includes state-of-the-art\npre-trained models like [Vision Transformer](https://huggingface.co/docs/transformers/main/en/model\\_doc/vit),\n[Masked Autoencoders](https://huggingface.co/docs/transformers/model\\_doc/vit\\_mae),\n[RegNet](https://huggingface.co/docs/transformers/main/en/model\\_doc/regnet),\n[ConvNeXt](https://huggingface.co/docs/transformers/model\\_doc/convnext),\nand many others! When it comes to deploying TensorFlow models, you have got a variety of\noptions. Depending on your use case, you may want to expose your model\nas an endpoint or package it in an application itself. TensorFlow\nprovides tools that cater to each of these different scenarios. In this post, you'll see how to deploy a Vision Transformer (ViT) model (for image classification)\nlocally using [TensorFlow Serving](https://www.tensorflow.org/tfx/tutorials/serving/rest\\_simple)\n(TF Serving). This will allow developers to expose the model either as a\nREST or gRPC endpoint. Moreover, TF Serving supports many\ndeployment-specific features off-the-shelf such as model warmup,\nserver-side batching, etc. To get the complete working code shown throughout this post, refer to\nthe Colab Notebook shown at the beginning. # Saving the Model\nAll TensorFlow models in ðŸ¤— Transformers have a method named\n`save\\_pretrained()`. With it, you can serialize the model weights in\nthe h5 format as well as in the standalone [SavedModel format](https://www.tensorflow.org/guide/saved\\_model). TF Serving needs a model to be present in the SavedModel format. So, let's first\nload a Vision Transformer model and save it:\n```py\nfrom transformers import TFViTForImageClassification\ntemp\\_model\\_dir = \""vit\""\nckpt = \""google/vit-base-patch16-224\""\nmodel = TFViTForImageClassification.from\\_pretrained(ckpt)\nmodel.save\\_pretrained(temp\\_model\\_dir, saved\\_model=True)\n```\nBy default, `save\\_pretrained()` will first create a version directory\ninside the path we provide to it. So, the path ultimately becomes:\n`{temp\\_model\\_dir}/saved\\_model/{version}`. We can inspect the serving signature of the SavedModel like so:\n```bash\nsaved\\_model\\_cli show --dir {temp\\_model\\_dir}/saved\\_model/1 --tag\\_set serve --signature\\_def serving\\_default\n```\nThis should output:\n```bash\nThe given SavedModel SignatureDef contains the following input(s):\ninputs['pixel\\_values'] tensor\\_info:\ndtype: DT\\_FLOAT\nshape: (-1, -1, -1, -1)\nname: serving\\_default\\_pixel\\_values:0\nThe given SavedModel SignatureDef contains the following output(s):\noutputs['logits'] tensor\\_info:\ndtype: DT\\_FLOAT\nshape: (-1, 1000)\nname: StatefulPartitionedCall:0\nMethod name is: tensorflow/serving/predict\n```\nAs can be noticed the model accepts single 4-d inputs (namely\n`pixel\\_values`) which has the following axes: `(batch\\_size,\nnum\\_channels, height, width)`."", ""!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-logo-light.svg)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-logo-dark.svg)\n\n# Safetensors\nSafetensors is a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy). Safetensors is really [fast ðŸš€](./speed). ## Installation\nwith pip:\n```\npip install safetensors\n```\nwith conda:\n```\nconda install -c huggingface safetensors\n```\n## Usage\n### Load tensors\n```python\nfrom safetensors import safe\\_open\ntensors = {}\nwith safe\\_open(\""model.safetensors\"", framework=\""pt\"", device=0) as f:\nfor k in f.keys():\ntensors[k] = f.get\\_tensor(k)\n```\nLoading only part of the tensors (interesting when running on multiple GPU)\n```python\nfrom safetensors import safe\\_open\ntensors = {}\nwith safe\\_open(\""model.safetensors\"", framework=\""pt\"", device=0) as f:\ntensor\\_slice = f.get\\_slice(\""embedding\"")\nvocab\\_size, hidden\\_dim = tensor\\_slice.get\\_shape()\ntensor = tensor\\_slice[:, :hidden\\_dim]\n```\n### Save tensors\n```python\nimport torch\nfrom safetensors.torch import save\\_file\ntensors = {\n\""embedding\"": torch.zeros((2, 2)),\n\""attention\"": torch.zeros((2, 3))\n}\nsave\\_file(tensors, \""model.safetensors\"")\n```\n## Format\nLet's say you have safetensors file named `model.safetensors`, then `model.safetensors` will have the following internal format:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-format.svg)\n\n## Featured Projects\nSafetensors is being used widely at leading AI enterprises, such as [Hugging Face](https://huggingface.co/), [EleutherAI](https://www.eleuther.ai/), and [StabilityAI](https://stability.ai/). Here is a non-exhaustive list of projects that are using safetensors:\n\\* [huggingface/transformers](https://github.com/huggingface/transformers)\n\\* [AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)\n\\* [Llama-cpp](https://github.com/ggerganov/llama.cpp/blob/e6a46b0ed1884c77267dc70693183e3b7164e0e0/convert.py#L537)\n\\* [microsoft/TaskMatrix](https://github.com/microsoft/TaskMatrix)\n\\* [hpcaitech/ColossalAI](https://github.com/hpcaitech/ColossalAI)\n\\* [huggingface/pytorch-image-models](https://github.com/huggingface/pytorch-image-models)\n\\* [CivitAI](https://civitai.com/)\n\\* [huggingface/diffusers](https://github.com/huggingface/diffusers)\n\\* [coreylowman/dfdx](https://github.com/coreylowman/dfdx)\n\\* [invoke-ai/InvokeAI](https://github.com/invoke-ai/InvokeAI)\n\\* [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)\n\\* [Sanster/lama-cleaner](https://github.com/Sanster/lama-cleaner)\n\\* [PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n\\* [AIGC-Audio/AudioGPT](https://github.com/AIGC-Audio/AudioGPT)\n\\* [brycedrennan/imaginAIry](https://github.com/brycedrennan/imaginAIry)\n\\* [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)\n\\* [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE)\n\\* [alvarobartt/safejax](https://github.com/alvarobartt/safejax)\n\\* [MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)\n\\* [LaurentMazare/tch-rs](https://github.com/LaurentMazare/tch-rs)\n\\* [chainyo/tensorshare](https://github.com/chainyo/tensorshare)""]",The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`.,safetensors.tensorflow.load_file
"Where can you access the logs of your Endpoints in Hugging Face Endpoints?
","[""Access and view Metrics\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the â€œAnalyticsâ€ tab of your Endpoints. ![metric dashboard](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png)\n## Access Metrics via API\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api\\_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints."", ""Access and read Logs\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the â€œLogsâ€ tab of your Endpoint. You will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference. ![select logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png)\nThe Container Logs are only available when your Endpoint is in the â€œRunningâ€ state. \\_Note: If your Endpoint creation is in the â€œFailedâ€ state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc.\\_\n\\*\\*Build Logs:\\*\\*\n![build logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png)\n\\*\\*Container Logs:\\*\\*\n![container logs](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png)"", ""Security & Compliance\nðŸ¤— Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place. ## Data Security/Privacy\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit. We also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet. Hugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan. ## Model Security/Privacy:\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub. ## Inference Endpoints and Hub Security\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\n## Inference Endpoint Security level\nWe currently offer three types of endpoints, in order or increasing security level:\n- \\*\\*Public\\*\\*: A Public Endpoint is available from the internet, secured with TLS/SSL, and requires no authentication."", ""Those endpoints are not officially supported in `huggingface\\_hub` or `huggingface.js` yet but [this code snippet](https://github.com/huggingface/huggingface\\_hub/issues/1535#issuecomment-1614693412) (in Python) might help you getting started. NEW! There's an [open PR](https://github.com/huggingface/huggingface\\_hub/pull/1905) in `huggingface\\_hub` to add official support from our Python library. ### Download access report\nYou can download a report of all access requests for a gated model with the \\*\\*download user access report\\*\\* button. Click on it to download a json file with a list of users. For each entry, you have:\n- \\*\\*user\\*\\*: the user id. Example: \\*julien-c\\*. - \\*\\*fullname\\*\\*: name of the user on the Hub. Example: \\*Julien Chaumond\\*. - \\*\\*status\\*\\*: status of the request. Either `\""pending\""`, `\""accepted\""` or `\""rejected\""`. - \\*\\*email\\*\\*: email of the user. - \\*\\*time\\*\\*: datetime when the user initially made the request. ### Customize requested information\nBy default, users landing on your gated model will be asked to share their contact information (email and username) by clicking the \\*\\*Agree and send request to access repo\\*\\* button.""]","You can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the â€œLogsâ€ tab of your Endpoint. You will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference. The Container Logs are only available when your Endpoint is in the â€œRunningâ€ state. If your Endpoint creation is in the â€œFailedâ€ state, you can check the Build Logs to see what the reason was.","In the ""Logs"" tab of your Endpoint through the UI."
"What is the latest task added to Hugging Face AutoTrain for Computer Vision?
","[""--\ntitle: Image Classification with AutoTrain\nthumbnail: /blog/assets/105\\_autotrain-image-classification/thumbnail.png\nauthors:\n- user: nimaboscarino\n---\n# Image Classification with AutoTrain\nSo youâ€™ve heard all about the cool things that are happening in the machine learning world, and you want to join in. Thereâ€™s just one problem â€“ you donâ€™t know how to code! ðŸ˜± Or maybe youâ€™re a seasoned software engineer who wants to add some ML to your side-project, but you donâ€™t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. Thatâ€™s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature weâ€™ve just added, weâ€™re making â€œno-codeâ€ machine learning better than ever. Best of all, you can create your first project for âœ¨ free! âœ¨\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with \\*\\*zero\\*\\* configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer ðŸ¤¯ Weâ€™ve been expanding the number of tasks that we support, and weâ€™re proud to announce that \\*\\*you can now use AutoTrain for Computer Vision\\*\\*!"", ""Image Classification is the latest task weâ€™ve added, with more on the way. But what does this mean for you? [Image Classification](https://huggingface.co/tasks/image-classification) models learn to \\*categorize\\* images, meaning that you can train one of these models to label any image. Do you want a model that can recognize signatures? Distinguish bird species? Identify plant diseases? As long as you can find an appropriate dataset, an image classification model has you covered. ## How can you train your own image classifier? If you havenâ€™t [created a Hugging Face account](https://huggingface.co/join) yet, nowâ€™s the time! Following that, make your way over to the [AutoTrain homepage](https://huggingface.co/autotrain) and click on â€œCreate new projectâ€ to get started. Youâ€™ll be asked to fill in some basic info about your project. In the screenshot below youâ€™ll see that I created a project named `butterflies-classification`, and I chose the â€œImage Classificationâ€ task. Iâ€™ve also chosen the â€œAutomaticâ€ model option, since I want to let AutoTrain do the work of finding the best model architectures for my project.Once AutoTrain creates your project, you just need to connect your data. If you have the data locally, you can drag and drop the folder into the window. Since we can also use [any of the image classification datasets on the Hugging Face Hub](https://huggingface.co/datasets?task\\_categories=task\\_categories:image-classification), in this example Iâ€™ve decided to use the [NimaBoscarino/butterflies](https://huggingface.co/datasets/NimaBoscarino/butterflies) dataset. You can select separate training and validation datasets if available, or you can ask AutoTrain to split the data for you.Once the data has been added, simply choose the number of model candidates that youâ€™d like AutoModel to try out, review the expected training cost (training with 5 candidate models and less than 500 images is free ðŸ¤©), and start training!In the screenshots above you can see that my project started 5 different models, which each reached different accuracy scores.""]",The latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification.,Image Classification
"What is the default repository type created by the `create_repo` function on Hugging Face Hub?
","[""Firstly, there are a few methods to manage repository creation, deletion, and others:\n```python no-format\nfrom huggingface\\_hub import (\n# User management\nlogin,\nlogout,\nwhoami,\n# Repository creation and management\ncreate\\_repo,\ndelete\\_repo,\nupdate\\_repo\\_visibility,\n# And some methods to retrieve/change information about the content\nlist\\_models,\nlist\\_datasets,\nlist\\_metrics,\nlist\\_repo\\_files,\nupload\\_file,\ndelete\\_file,\n)\n```\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them. The `create\\_repo` method can be used to create a new repository on the hub:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"")\n```\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"", organization=\""huggingface\"")\n```\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization."", ""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# åˆ›å»ºå’Œç®¡ç†å­˜å‚¨åº“\nHugging Face Hubæ˜¯ä¸€ç»„ Git å­˜å‚¨åº“ã€‚[Git](https://git-scm.com/)æ˜¯è½¯ä»¶å¼€å‘ä¸­å¹¿æ³›ä½¿ç”¨çš„å·¥å…·ï¼Œå¯ä»¥åœ¨åä½œå·¥ä½œæ—¶è½»æ¾å¯¹é¡¹ç›®è¿›è¡Œç‰ˆæœ¬æŽ§åˆ¶ã€‚æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä¸Ž Hub ä¸Šçš„å­˜å‚¨åº“è¿›è¡Œäº¤äº’ï¼Œç‰¹åˆ«å…³æ³¨ä»¥ä¸‹å†…å®¹ï¼š\n- åˆ›å»ºå’Œåˆ é™¤å­˜å‚¨åº“\n- ç®¡ç†åˆ†æ”¯å’Œæ ‡ç­¾\n- é‡å‘½åæ‚¨çš„å­˜å‚¨åº“\n- æ›´æ–°æ‚¨çš„å­˜å‚¨åº“å¯è§æ€§\n- ç®¡ç†å­˜å‚¨åº“çš„æœ¬åœ°å‰¯æœ¬\nå¦‚æžœæ‚¨ä¹ æƒ¯äºŽä½¿ç”¨ç±»ä¼¼äºŽGitLab/GitHub/Bitbucketç­‰å¹³å°ï¼Œæ‚¨å¯èƒ½é¦–å…ˆæƒ³åˆ°ä½¿ç”¨ `git`å‘½ä»¤è¡Œå·¥å…·æ¥å…‹éš†å­˜å‚¨åº“ï¼ˆ`git clone`ï¼‰ã€æäº¤æ›´æ”¹ï¼ˆ`git add` , ` git commit`ï¼‰å¹¶æŽ¨é€å®ƒä»¬ï¼ˆ`git push`ï¼‰ã€‚åœ¨ä½¿ç”¨ Hugging Face Hub æ—¶ï¼Œè¿™æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œè½¯ä»¶å·¥ç¨‹å’Œæœºå™¨å­¦ä¹ å¹¶ä¸å…·æœ‰ç›¸åŒçš„è¦æ±‚å’Œå·¥ä½œæµç¨‹ã€‚æ¨¡åž‹å­˜å‚¨åº“å¯èƒ½ä¼šç»´æŠ¤å¤§é‡æ¨¡åž‹æƒé‡æ–‡ä»¶ä»¥é€‚åº”ä¸åŒçš„æ¡†æž¶å’Œå·¥å…·ï¼Œå› æ­¤å…‹éš†å­˜å‚¨åº“ä¼šå¯¼è‡´æ‚¨ç»´æŠ¤å¤§é‡å ç”¨ç©ºé—´çš„æœ¬åœ°æ–‡ä»¶å¤¹ã€‚å› æ­¤ï¼Œä½¿ç”¨æˆ‘ä»¬çš„è‡ªå®šä¹‰HTTPæ–¹æ³•å¯èƒ½æ›´æœ‰æ•ˆã€‚æ‚¨å¯ä»¥é˜…è¯»æˆ‘ä»¬çš„[gitä¸ŽHTTPç›¸æ¯”è¾ƒ](../concepts/git\\_vs\\_http)è§£é‡Šé¡µé¢ä»¥èŽ·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯\nå¦‚æžœä½ æƒ³åœ¨Hubä¸Šåˆ›å»ºå’Œç®¡ç†ä¸€ä¸ªä»“åº“ï¼Œä½ çš„è®¡ç®—æœºå¿…é¡»å¤„äºŽç™»å½•çŠ¶æ€ã€‚å¦‚æžœå°šæœªç™»å½•ï¼Œè¯·å‚è€ƒ[æ­¤éƒ¨åˆ†](../quick-start#login)ã€‚åœ¨æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å‡è®¾ä½ çš„è®¡ç®—æœºå·²ç™»å½•\n## ä»“åº“åˆ›å»ºå’Œåˆ é™¤\nç¬¬ä¸€æ­¥æ˜¯äº†è§£å¦‚ä½•åˆ›å»ºå’Œåˆ é™¤ä»“åº“ã€‚ä½ åªèƒ½ç®¡ç†ä½ æ‹¥æœ‰çš„ä»“åº“ï¼ˆåœ¨ä½ çš„ç”¨æˆ·åå‘½åç©ºé—´ä¸‹ï¼‰æˆ–è€…ä½ å…·æœ‰å†™å…¥æƒé™çš„ç»„ç»‡ä¸­çš„ä»“åº“\n### åˆ›å»ºä¸€ä¸ªä»“åº“\nä½¿ç”¨ [`create\\_repo`] åˆ›å»ºä¸€ä¸ªç©ºä»“åº“ï¼Œå¹¶é€šè¿‡ `repo\\_id`å‚æ•°ä¸ºå…¶å‘½å `repo\\_id`æ˜¯ä½ çš„å‘½åç©ºé—´ï¼ŒåŽé¢è·Ÿç€ä»“åº“åç§°ï¼š`username\\_or\\_org/repo\\_name`\nè¿è¡Œä»¥ä¸‹ä»£ç ï¼Œä»¥åˆ›å»ºä»“åº“ï¼š\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-model\"")\n'https://huggingface.co/lysandre/test-model'\n```\né»˜è®¤æƒ…å†µä¸‹ï¼Œ[`create\\_repo`] ä¼šåˆ›å»ºä¸€ä¸ªæ¨¡åž‹ä»“åº“ã€‚ä½†æ˜¯ä½ å¯ä»¥ä½¿ç”¨ `repo\\_type`å‚æ•°æ¥æŒ‡å®šå…¶ä»–ä»“åº“ç±»åž‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœä½ æƒ³åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ä»“åº“\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-dataset\"", repo\\_type=\""dataset\"")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```\nåˆ›å»ºä»“åº“æ—¶ï¼Œä½ å¯ä»¥ä½¿ç”¨ `private`å‚æ•°è®¾ç½®ä»“åº“çš„å¯è§æ€§\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç \n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-private\"", private=True)\n```\nå¦‚æžœä½ æƒ³åœ¨ä»¥åŽæ›´æ”¹ä»“åº“çš„å¯è§æ€§ï¼Œä½ å¯ä»¥ä½¿ç”¨[`update\\_repo\\_visibility`] å‡½æ•°\n### åˆ é™¤ä¸€ä¸ªä»“åº“\nä½¿ç”¨ [`delete\\_repo`] åˆ é™¤ä¸€ä¸ªä»“åº“ã€‚ç¡®ä¿ä½ ç¡®å®žæƒ³è¦åˆ é™¤ä»“åº“ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªä¸å¯é€†è½¬çš„è¿‡ç¨‹ï¼åšå®Œä¸Šè¿°è¿‡ç¨‹åŽï¼ŒæŒ‡å®šä½ æƒ³è¦åˆ é™¤çš„ä»“åº“çš„ `repo\\_id`\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> delete\\_repo(repo\\_id=\""lysandre/my-corrupted-dataset\"", repo\\_type=\""dataset\"")\n```\n### å…‹éš†ä¸€ä¸ªä»“åº“ï¼ˆä»…é€‚ç”¨äºŽ Spacesï¼‰\nåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½æƒ³è¦å¤åˆ¶åˆ«äººçš„ä»“åº“å¹¶æ ¹æ®è‡ªå·±çš„ç”¨ä¾‹è¿›è¡Œè°ƒæ•´ã€‚å¯¹äºŽ Spacesï¼Œä½ å¯ä»¥ä½¿ç”¨ [`duplicate\\_space`] æ–¹æ³•æ¥å®žçŽ°ã€‚å®ƒå°†å¤åˆ¶æ•´ä¸ªä»“åº“ã€‚\nä½ ä»ç„¶éœ€è¦é…ç½®è‡ªå·±çš„è®¾ç½®ï¼ˆç¡¬ä»¶å’Œå¯†é’¥ï¼‰ã€‚æŸ¥çœ‹æˆ‘ä»¬çš„[ç®¡ç†ä½ çš„SpaceæŒ‡å—](./manage-spaces)ä»¥èŽ·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import duplicate\\_space\n>>> duplicate\\_space(\""multimodalart/dreambooth-training\"", private=False)\nRepoUrl('https://huggingface.co/spaces/nateraw/dreambooth-training',...)\n```\n## ä¸Šä¼ å’Œä¸‹è½½æ–‡ä»¶\næ—¢ç„¶æ‚¨å·²ç»åˆ›å»ºäº†æ‚¨çš„å­˜å‚¨åº“ï¼Œæ‚¨çŽ°åœ¨ä¹Ÿå¯ä»¥æŽ¨é€æ›´æ”¹è‡³å…¶ä¸­å¹¶ä»Žä¸­ä¸‹è½½æ–‡ä»¶\nè¿™ä¸¤ä¸ªä¸»é¢˜æœ‰å®ƒä»¬è‡ªå·±çš„æŒ‡å—ã€‚è¯·[ä¸Šä¼ æŒ‡å—](./upload) å’Œ[ä¸‹è½½æŒ‡å—](./download)æ¥å­¦ä¹ å¦‚ä½•ä½¿ç”¨æ‚¨çš„å­˜å‚¨åº“ã€‚\n## åˆ†æ”¯å’Œæ ‡ç­¾\nGitå­˜å‚¨åº“é€šå¸¸ä½¿ç”¨åˆ†æ”¯æ¥å­˜å‚¨åŒä¸€å­˜å‚¨åº“çš„ä¸åŒç‰ˆæœ¬ã€‚æ ‡ç­¾ä¹Ÿå¯ä»¥ç”¨äºŽæ ‡è®°å­˜å‚¨åº“çš„ç‰¹å®šçŠ¶æ€ï¼Œä¾‹å¦‚ï¼Œåœ¨å‘å¸ƒç‰ˆæœ¬è¿™ä¸ªæƒ…å†µä¸‹ã€‚æ›´ä¸€èˆ¬åœ°è¯´ï¼Œåˆ†æ”¯å’Œæ ‡ç­¾è¢«ç§°ä¸º[gitå¼•ç”¨](https://git-scm.com/book/en/v2/Git-Internals-Git-References). ### åˆ›å»ºåˆ†æ”¯å’Œæ ‡ç­¾\nä½ å¯ä»¥ä½¿ç”¨[`create\\_branch`]å’Œ[`create\\_tag`]æ¥åˆ›å»ºæ–°çš„åˆ†æ”¯å’Œæ ‡ç­¾:\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import create\\_branch, create\\_tag\n# Create a branch on a Space repo from `main` branch\n>>> create\\_branch(\""Matthijs/speecht5-tts-demo\"", repo\\_type=\""space\"", branch=\""handle-dog-speaker\"")\n# Create a tag on a Dataset repo from `v0.1-release` branch\n>>> create\\_branch(\""bigcode/the-stack\"", repo\\_type=\""dataset\"", revision=\""v0.1-release\"", tag=\""v0.1.1\"", tag\\_message=\""Bump release version.\"")\n```\nåŒæ—¶,ä½ å¯ä»¥ä»¥ç›¸åŒçš„æ–¹å¼ä½¿ç”¨ [`delete\\_branch`] å’Œ [`delete\\_tag`] å‡½æ•°æ¥åˆ é™¤åˆ†æ”¯æˆ–æ ‡ç­¾\n### åˆ—å‡ºæ‰€æœ‰çš„åˆ†æ”¯å’Œæ ‡ç­¾\nä½ è¿˜å¯ä»¥ä½¿ç”¨ [`list\\_repo\\_refs`] åˆ—å‡ºå­˜å‚¨åº“ä¸­çš„çŽ°æœ‰ Git å¼•ç”¨\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import list\\_repo\\_refs\n>>> api.list\\_repo\\_refs(\""bigcode/the-stack\"", repo\\_type=\""dataset\"")\nGitRefs(\nbranches=[\nGitRefInfo(name='main', ref='refs/heads/main', target\\_commit='18edc1591d9ce72aa82f56c4431b3c969b210ae3'),\nGitRefInfo(name='v1.1.a1', ref='refs/heads/v1.1.a1', target\\_commit='f9826b862d1567f3822d3d25649b0d6d22ace714')\n],\nconverts=[],\ntags=[\nGitRefInfo(name='v1.0', ref='refs/tags/v1.0', target\\_commit='c37a8cd1e382064d8aced5e05543c5f7753834da')\n]\n)\n```\n## ä¿®æ”¹å­˜å‚¨åº“è®¾ç½®\nå­˜å‚¨åº“å…·æœ‰ä¸€äº›å¯é…ç½®çš„è®¾ç½®ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ‚¨é€šå¸¸ä¼šåœ¨æµè§ˆå™¨ä¸­çš„å­˜å‚¨åº“è®¾ç½®é¡µé¢ä¸Šæ‰‹åŠ¨é…ç½®è¿™äº›è®¾ç½®ã€‚è¦é…ç½®å­˜å‚¨åº“ï¼Œæ‚¨å¿…é¡»å…·æœ‰å¯¹å…¶çš„å†™è®¿é—®æƒé™ï¼ˆæ‹¥æœ‰å®ƒæˆ–å±žäºŽç»„ç»‡ï¼‰ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ `huggingface\\_hub` åœ¨ç¼–ç¨‹æ–¹å¼ä¸Šé…ç½®çš„è®¾ç½®ã€‚\nä¸€äº›è®¾ç½®æ˜¯ç‰¹å®šäºŽ Spacesï¼ˆç¡¬ä»¶ã€çŽ¯å¢ƒå˜é‡ç­‰ï¼‰çš„ã€‚è¦é…ç½®è¿™äº›è®¾ç½®ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„[ç®¡ç†Spaces](../guides/manage-spaces)æŒ‡å—ã€‚\n### æ›´æ–°å¯è§æ€§\nä¸€ä¸ªå­˜å‚¨åº“å¯ä»¥æ˜¯å…¬å…±çš„æˆ–ç§æœ‰çš„ã€‚ç§æœ‰å­˜å‚¨åº“ä»…å¯¹æ‚¨æˆ–å­˜å‚¨åº“æ‰€åœ¨ç»„ç»‡çš„æˆå‘˜å¯è§ã€‚\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç å°†å­˜å‚¨åº“æ›´æ”¹ä¸ºç§æœ‰ï¼š\n```py\n>>> from huggingface\\_hub import update\\_repo\\_visibility\n>>> update\\_repo\\_visibility(repo\\_id=repo\\_id, private=True)\n```\n### é‡å‘½åæ‚¨çš„å­˜å‚¨åº“\næ‚¨å¯ä»¥ä½¿ç”¨ [`move\\_repo`] åœ¨ Hub ä¸Šé‡å‘½åæ‚¨çš„å­˜å‚¨åº“ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæ‚¨è¿˜å¯ä»¥å°†å­˜å‚¨åº“ä»Žä¸€ä¸ªç”¨æˆ·ç§»åŠ¨åˆ°ä¸€ä¸ªç»„ç»‡ã€‚åœ¨è¿™æ ·åšæ—¶ï¼Œæœ‰ä¸€äº›[é™åˆ¶](https://hf.co/docs/hub/repositories-settings#renaming-or-transferring-a-repo)éœ€è¦æ³¨æ„ã€‚ä¾‹å¦‚ï¼Œæ‚¨ä¸èƒ½å°†å­˜å‚¨åº“è½¬ç§»åˆ°å¦ä¸€ä¸ªç”¨æˆ·ã€‚\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import move\\_repo\n>>> move\\_repo(from\\_id=\""Wauplin/cool-model\"", to\\_id=\""huggingface/cool-model\"")\n```\n## ç®¡ç†å­˜å‚¨åº“çš„æœ¬åœ°å‰¯æœ¬\nä¸Šè¿°æ‰€æœ‰æ“ä½œéƒ½å¯ä»¥é€šè¿‡HTTPè¯·æ±‚å®Œæˆã€‚ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½å¸Œæœ›åœ¨æœ¬åœ°æ‹¥æœ‰å­˜å‚¨åº“çš„å‰¯æœ¬ï¼Œå¹¶ä½¿ç”¨æ‚¨ç†Ÿæ‚‰çš„Gitå‘½ä»¤ä¸Žä¹‹äº¤äº’ã€‚\n[`Repository`] ç±»å…è®¸æ‚¨ä½¿ç”¨ç±»ä¼¼äºŽGitå‘½ä»¤çš„å‡½æ•°ä¸ŽHubä¸Šçš„æ–‡ä»¶å’Œå­˜å‚¨åº“è¿›è¡Œäº¤äº’ã€‚å®ƒæ˜¯å¯¹Gitå’ŒGit-LFSæ–¹æ³•çš„åŒ…è£…ï¼Œä»¥ä½¿ç”¨æ‚¨å·²ç»äº†è§£å’Œå–œçˆ±çš„Gitå‘½ä»¤ã€‚åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…Git-LFSï¼ˆè¯·å‚é˜…[æ­¤å¤„](https://git-lfs.github.com/)èŽ·å–å®‰è£…è¯´æ˜Žï¼‰ã€‚\n### ä½¿ç”¨æœ¬åœ°å­˜å‚¨åº“\nä½¿ç”¨æœ¬åœ°å­˜å‚¨åº“è·¯å¾„å®žä¾‹åŒ–ä¸€ä¸ª [`Repository`] å¯¹è±¡ï¼š\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(local\\_dir=\""//\"")\n```\n### å…‹éš†\n`clone\\_from`å‚æ•°å°†ä¸€ä¸ªå­˜å‚¨åº“ä»ŽHugging Faceå­˜å‚¨åº“IDå…‹éš†åˆ°ç”± `local\\_dir`å‚æ•°æŒ‡å®šçš„æœ¬åœ°ç›®å½•ï¼š\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(local\\_dir=\""w2v2\"", clone\\_from=\""facebook/wav2vec2-large-960h-lv60\"")\n```\n`clone\\_from`è¿˜å¯ä»¥ä½¿ç”¨URLå…‹éš†å­˜å‚¨åº“ï¼š\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> repo = Repository(local\\_dir=\""huggingface-hub\"", clone\\_from=\""https://huggingface.co/facebook/wav2vec2-large-960h-lv60\"")\n```\nä½ å¯ä»¥å°†`clone\\_from`å‚æ•°ä¸Ž[`create\\_repo`]ç»“åˆä½¿ç”¨ï¼Œä»¥åˆ›å»ºå¹¶å…‹éš†ä¸€ä¸ªå­˜å‚¨åº“ï¼š\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> repo\\_url = create\\_repo(repo\\_id=\""repo\\_name\"")\n>>> repo = Repository(local\\_dir=\""repo\\_local\\_path\"", clone\\_from=repo\\_url)\n```\nå½“ä½ å…‹éš†ä¸€ä¸ªå­˜å‚¨åº“æ—¶ï¼Œé€šè¿‡åœ¨å…‹éš†æ—¶æŒ‡å®š`git\\_user`å’Œ`git\\_email`å‚æ•°ï¼Œä½ è¿˜å¯ä»¥ä¸ºå…‹éš†çš„å­˜å‚¨åº“é…ç½®Gitç”¨æˆ·åå’Œç”µå­é‚®ä»¶ã€‚å½“ç”¨æˆ·æäº¤åˆ°è¯¥å­˜å‚¨åº“æ—¶ï¼ŒGitå°†çŸ¥é“æäº¤çš„ä½œè€…æ˜¯è°ã€‚\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n```py\n>>> repo = Repository(\n..."", ""It's a model repo by default. Parameters:\n- `type`: Type of repo (dataset or space; model by default). - `name`: Name of repo. - `organization`: Name of organization (optional). Payload:\n```js\npayload = {\n\""type\"": \""model\"",\n\""name\"": \""name\"",\n\""organization\"": \""organization\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.delete\\_repo()`. ### PUT /api/repos/{repo\\_type}/{repo\\_id}/settings\nUpdate repo visibility. Payload:\n```js\npayload = {\n\""private\"": \""private\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.update\\_repo\\_visibility()`. ### POST /api/repos/move\nMove a repository (rename within the same namespace or transfer from user to organization). Parameters:\n- `fromRepo`: repo to rename. - `toRepo`: new name of the repo. - `type`: Type of repo (dataset or space; model by default). Payload:\n```js\npayload = {\n\""fromRepo\"" : \""namespace/repo\\_name\"",\n\""toRepo\"" : \""namespace2/repo\\_name2\"",\n\""type\"": \""model\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.move\\_repo()`. ## User API\nThe following endpoint gets information about a user. ### GET /api/whoami-v2\nGet username and organizations the user belongs to. Payload:\n```js\nheaders = { \""authorization\"" : \""Bearer $token\"" }\n```\nThis is equivalent to `huggingface\\_hub.whoami()`."", ""More details in [this guide](https://huggingface.co/docs/huggingface\\_hub/en/guides/manage-cache). ### Login\nThe Hugging Face Hub uses tokens to authenticate applications (see [docs](https://huggingface.co/docs/hub/security-tokens)). To login your machine, run the following CLI:\n```bash\nhuggingface-cli login\n# or using an environment variable\nhuggingface-cli login --token $HUGGINGFACE\\_TOKEN\n```\n### Create a repository\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(repo\\_id=\""super-cool-model\"")\n```\n### Upload files\nUpload a single file\n```py\nfrom huggingface\\_hub import upload\\_file\nupload\\_file(\npath\\_or\\_fileobj=\""/home/lysandre/dummy-test/README.md\"",\npath\\_in\\_repo=\""README.md\"",\nrepo\\_id=\""lysandre/test-model\"",\n)\n```\nOr an entire folder\n```py\nfrom huggingface\\_hub import upload\\_folder\nupload\\_folder(\nfolder\\_path=\""/path/to/local/space\"",\nrepo\\_id=\""username/my-cool-space\"",\nrepo\\_type=\""space\"",\n)\n```\nFor details in the [upload guide](https://huggingface.co/docs/huggingface\\_hub/en/guides/upload)."", ""Documentation for valid parameters and descriptions can be found [here](https://huggingface.co/docs/hub/endpoints). ### Advanced programmatic repository management\nThe `Repository` class helps manage both offline Git repositories and Hugging\nFace Hub repositories. Using the `Repository` class requires `git` and `git-lfs`\nto be installed. Instantiate a `Repository` object by calling it with a path to a local Git\nclone/repository:\n```python\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(\""//\"")\n```\nThe `Repository` takes a `clone\\_from` string as parameter. This can stay as\n`None` for offline management, but can also be set to any URL pointing to a Git\nrepo to clone that repository in the specified directory:\n```python\n>>> repo = Repository(\""huggingface-hub\"", clone\\_from=\""https://github.com/huggingface/huggingface\\_hub\"")\n```\nThe `clone\\_from` method can also take any Hugging Face model ID as input, and\nwill clone that repository:\n```python\n>>> repo = Repository(\""w2v2\"", clone\\_from=\""facebook/wav2vec2-large-960h-lv60\"")\n```\nIf the repository you're cloning is one of yours or one of your organisation's,\nthen having the ability to commit and push to that repository is important. In\norder to do that, you should make sure to be logged-in using `huggingface-cli\nlogin`, and to have the `token` parameter set to `True` (the default)\nwhen instantiating the `Repository` object:\n```python\n>>> repo = Repository(\""my-model\"", clone\\_from=\""/\"", token=True)\n```\nThis works for models, datasets and spaces repositories; but you will need to\nexplicitely specify the type for the last two options:\n```python\n>>> repo = Repository(\""my-dataset\"", clone\\_from=\""/\"", token=True, repo\\_type=\""dataset\"")\n```\nYou can also change between branches:\n```python\n>>> repo = Repository(\""huggingface-hub\"", clone\\_from=\""/\"", revision='branch1')\n>>> repo.git\\_checkout(\""branch2\"")\n```\nThe `clone\\_from` method can also take any Hugging Face model ID as input, and\nwill clone that repository:\n```python\n>>> repo = Repository(\""w2v2\"", clone\\_from=\""facebook/wav2vec2-large-960h-lv60\"")\n```\nFinally, you can choose to specify the Git username and email attributed to that\nclone directly by using the `git\\_user` and `git\\_email` parameters."", ""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Create and manage a repository\nThe Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\ndevelopment to easily version projects when working collaboratively. This guide will show you how to interact with the\nrepositories on the Hub, especially:\n- Create and delete a repository. - Manage branches and tags. - Rename your repository. - Update your repository visibility. - Manage a local copy of your repository. If you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct\nmight be to use `git` CLI to clone your repo (`git clone`), commit changes (`git add, git commit`) and push them\n(`git push`). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do\nnot share the same requirements and workflows. Model repositories might maintain large model weight files for different\nframeworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As\na result, it may be more efficient to use our custom HTTP methods. You can read our [Git vs HTTP paradigm](../concepts/git\\_vs\\_http)\nexplanation page for more details. If you want to create and manage a repository on the Hub, your machine must be logged in. If you are not, please refer to\n[this section](../quick-start#authentication). In the rest of this guide, we will assume that your machine is logged in. ## Repo creation and deletion\nThe first step is to know how to create and delete repositories. You can only manage repositories that you own (under\nyour username namespace) or from organizations in which you have write permissions. ### Create a repository\nCreate an empty repository with [`create\\_repo`] and give it a name with the `repo\\_id` parameter. The `repo\\_id` is your namespace followed by the repository name: `username\\_or\\_org/repo\\_name`. ```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-model\"")\n'https://huggingface.co/lysandre/test-model'\n```\nBy default, [`create\\_repo`] creates a model repository. But you can use the `repo\\_type` parameter to specify another repository type. For example, if you want to create a dataset repository:\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-dataset\"", repo\\_type=\""dataset\"")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```\nWhen you create a repository, you can set your repository visibility with the `private` parameter. ```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-private\"", private=True)\n```\nIf you want to change the repository visibility at a later time, you can use the [`update\\_repo\\_visibility`] function."", ""There are three kinds of repositories on the Hub, and in this guide you'll be creating a \\*\\*model repository\\*\\* for demonstration purposes. For information on creating and managing models, datasets, and Spaces, refer to their respective documentation. 1. To create a new repository, visit [huggingface.co/new](http://huggingface.co/new):\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new_repo.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new_repo-dark.png)\n\n2. Specify the owner of the repository: this can be either you or any of the organizations youâ€™re affiliated with. 3. Enter your modelâ€™s name. This will also be the name of the repository. 4. Specify whether you want your model to be public or private. 5. Specify the license. You can leave the \\*License\\* field blank for now. To learn about licenses, visit the [\\*\\*Licenses\\*\\*](repositories-licenses) documentation. After creating your model repository, you should see a page like this:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/empty_repo.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/empty_repo-dark.png)\n\nNote that the Hub prompts you to create a \\*Model Card\\*, which you can learn about in the [\\*\\*Model Cards documentation\\*\\*](./model-cards). Including a Model Card in your model repo is best practice, but since we're only making a test repo at the moment we can skip this. ## Adding files to a repository (Web UI)\nTo add files to your repository via the web UI, start by selecting the \\*\\*Files\\*\\* tab, navigating to the desired directory, and then clicking \\*\\*Add file\\*\\*. You'll be given the option to create a new file or upload a file directly from your computer. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repositories-add_file.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repositories-add_file-dark.png)\n\n### Creating a new file\nChoosing to create a new file will take you to the following editor screen, where you can choose a name for your file, add content, and save your file with a message that summarizes your changes. Instead of directly committing the new file to your repo's `main` branch, you can select `Open as a pull request` to create a [Pull Request](./repositories-pull-requests-discussions). ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repositories-create_file.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repositories-create_file-dark.png)\n\n### Uploading a file\nIf you choose \\_Upload file\\_ you'll be able to choose a local file to upload, along with a message summarizing your changes to the repo. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repositories-upload_file.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/repositories-upload_file-dark.png)\n\nAs with creating new files, you can select `Open as a pull request` to create a [Pull Request](./repositories-pull-requests-discussions) instead of adding your changes directly to the `main` branch of your repo. ## Adding files to a repository (terminal)[[terminal]]\n### Cloning repositories\nDownloading repositories to your local machine is called \\*cloning\\*. You can use the following commands to load your repo and navigate to it:\n```bash\ngit clone https://huggingface.co//\ncd \n```\nYou can clone over SSH with the following command:\n```bash\ngit clone git@hf.co:/\ncd \n```\nYou'll need to add your SSH public key to [your user settings](https://huggingface.co/settings/keys) to push changes or access private repositories. ### Set up\nNow's the time, you can add any files you want to the repository! ðŸ”¥\nDo you have files larger than 10MB? Those files should be tracked with `git-lfs`, which you can initialize with:\n```bash\ngit lfs install\n```\nNote that if your files are larger than \\*\\*5GB\\*\\* you'll also need to run:\n```bash\nhuggingface-cli lfs-enable-largefiles . ```\nWhen you use Hugging Face to create a repository, Hugging Face automatically provides a list of common file extensions for common Machine Learning large files in the `.gitattributes` file, which `git-lfs` uses to efficiently track changes to your large files.""]",The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository.,model
"How many splits does the ""duorc"" dataset have?
","[""List splits and configurations\nDatasets typically have splits and may also have configurations. A \\_split\\_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A \\_configuration\\_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you're interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load\\_hub)! This guide shows you how to use Datasets Server's `/splits` endpoint to retrieve a dataset's splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\nThe `/splits` endpoint accepts the dataset name as its query parameter:\n\n```python\nimport requests\nheaders = {\""Authorization\"": f\""Bearer {API\\_TOKEN}\""}\nAPI\\_URL = \""https://datasets-server.huggingface.co/splits?dataset=duorc\""\ndef query():\nresponse = requests.get(API\\_URL, headers=headers)\nreturn response.json()\ndata = query()\n```\n\n```js\nimport fetch from \""node-fetch\"";\nasync function query(data) {\nconst response = await fetch(\n\""https://datasets-server.huggingface.co/splits?dataset=duorc\"",\n{\nheaders: { Authorization: `Bearer ${API\\_TOKEN}` },\nmethod: \""GET\""\n}\n);\nconst result = await response.json();\nreturn result;\n}\nquery().then((response) => {\nconsole.log(JSON.stringify(response));\n});\n```\n\n```curl\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\n-X GET \\\n-H \""Authorization: Bearer ${API\\_TOKEN}\""\n```\nThe endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n```json\n{\n\""splits\"": [\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""test\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""test\"" }\n],\n\""pending\"": [],\n\""failed\"": []\n}\n```"", ""### Download and extract the data. It consists of two compressed folders: one with images, and the otherâ€”with associated image captions. Note that the compressed images folder is 13GB in size. ```bash\nwget http://images.cocodataset.org/annotations/annotations\\_trainval2014.zip\nwget http://images.cocodataset.org/zips/train2014.zip\nunzip annotations\\_trainval2014.zip\nunzip train2014.zip\nmkdir coco\\_dataset\nmv train2014 coco\\_dataset/\nmv annotations coco\\_dataset/\n```\n### Prepare dataset files and split the dataset. ```python\nimport json\nimport collections\nimages\\_dir = \""coco\\_dataset/train2014\""\nannotation\\_file = \""coco\\_dataset/annotations/captions\\_train2014.json\""\nwith open(annotation\\_file, \""r\"") as f:\nannotations = json.load(f)[\""annotations\""]\nimage\\_path\\_to\\_caption = collections.defaultdict(list)\nfor element in annotations:\ncaption = f\""{element['caption'].lower().rstrip('.')}\""\nimage\\_path = images\\_dir + \""/COCO\\_train2014\\_\"" + \""%012d.jpg\"" % (element[\""image\\_id\""])\nimage\\_path\\_to\\_caption[image\\_path].append(caption)\nlines = []\nfor image\\_path, captions in image\\_path\\_to\\_caption.items():\nlines.append(json.dumps({\""image\\_path\"": image\\_path, \""captions\"": captions}))\ntrain\\_lines = lines[:-8000]\nvalid\\_line = lines[-8000:]\nwith open(\""coco\\_dataset/train\\_dataset.json\"", \""w\"") as f:\nf.write(\""\\n\"".join(train\\_lines))\nwith open(\""coco\\_dataset/valid\\_dataset.json\"", \""w\"") as f:\nf.write(\""\\n\"".join(valid\\_line))\n```\n> Note: The data loading and processing part of this script can still be improved for maximum performance."", ""### Distributed\nTo split your dataset across your training nodes, you can use [`datasets.distributed.split\\_dataset\\_by\\_node`]:\n```python\nimport os\nfrom datasets.distributed import split\\_dataset\\_by\\_node\nds = split\\_dataset\\_by\\_node(ds, rank=int(os.environ[\""RANK\""]), world\\_size=int(os.environ[\""WORLD\\_SIZE\""]))\n```\nThis works for both map-style datasets and iterable datasets. The dataset is split for the node at rank `rank` in a pool of nodes of size `world\\_size`. For map-style datasets:\nEach node is assigned a chunk of data, e.g. rank 0 is given the first chunk of the dataset. For iterable datasets:\nIf the dataset has a number of shards that is a factor of `world\\_size` (i.e. if `dataset.n\\_shards % world\\_size == 0`),\nthen the shards are evenly assigned across the nodes, which is the most optimized. Otherwise, each node keeps 1 example out of `world\\_size`, skipping the other examples. This can also be combined with a `torch.utils.data.DataLoader` if you want each node to use multiple workers to load the data."", ""- The slice of `rows` of a dataset and the content contained in each column of a specific row. For example, here are the `features` and the slice of `rows` of the `duorc`/`SelfRC` train split from 150 to 151:\n```json\n// https://datasets-server.huggingface.co/rows?dataset=duorc&config=SelfRC&split=train&offset=150&length=2\n{\n\""features\"": [\n{\n\""feature\\_idx\"": 0,\n\""name\"": \""plot\\_id\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 1,\n\""name\"": \""plot\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 2,\n\""name\"": \""title\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 3,\n\""name\"": \""question\\_id\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 4,\n\""name\"": \""question\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 5,\n\""name\"": \""answers\"",\n\""type\"": {\n\""feature\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" },\n\""\\_type\"": \""Sequence\""\n}\n},\n{\n\""feature\\_idx\"": 6,\n\""name\"": \""no\\_answer\"",\n\""type\"": { \""dtype\"": \""bool\"", \""\\_type\"": \""Value\"" }\n}\n],\n\""rows\"": [\n{\n\""row\\_idx\"": 150,\n\""row\"": {\n\""plot\\_id\"": \""/m/03wj\\_q\"",\n\""plot\"": \""The film is centered on Mortal Kombat, a fighting tournament between the representatives of the realms of Earth and Outworld conceived by the Elder Gods amid looming invasion of the Earth by Outworld."", ""for example in shuffled\\_dataset:\n... ... ```\n## Split dataset\nYou can split your dataset one of two ways:\n- [`IterableDataset.take`] returns the first `n` examples in a dataset:\n```py\n>>> dataset = load\\_dataset('oscar', \""unshuffled\\_deduplicated\\_en\"", split='train', streaming=True)\n>>> dataset\\_head = dataset.take(2)\n>>> list(dataset\\_head)\n[{'id': 0, 'text': 'Mtendere Village was...'}, {'id': 1, 'text': 'Lily James cannot fight the music...'}]\n```\n- [`IterableDataset.skip`] omits the first `n` examples in a dataset and returns the remaining examples:\n```py\n>>> train\\_dataset = shuffled\\_dataset.skip(1000)\n```\n`take` and `skip` prevent future calls to `shuffle` because they lock in the order of the shards. You should `shuffle` your dataset before splitting it. ## Interleave\n[`interleave\\_datasets`] can combine an [`IterableDataset`] with other datasets. The combined dataset returns alternating examples from each of the original datasets. ```py\n>>> from datasets import interleave\\_datasets\n>>> en\\_dataset = load\\_dataset('oscar', \""unshuffled\\_deduplicated\\_en\"", split='train', streaming=True, trust\\_remote\\_code=True)\n>>> fr\\_dataset = load\\_dataset('oscar', \""unshuffled\\_deduplicated\\_fr\"", split='train', streaming=True, trust\\_remote\\_code=True)\n>>> multilingual\\_dataset = interleave\\_datasets([en\\_dataset, fr\\_dataset])\n>>> list(multilingual\\_dataset.take(2))\n[{'text': 'Mtendere Village was inspired by the vision...'}, {'text': \""MÃ©dia de dÃ©bat d'idÃ©es, de culture et de littÃ©rature...\""}]\n```\nDefine sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the `probabilities` argument with your desired sampling probabilities:\n```py\n>>> multilingual\\_dataset\\_with\\_oversampling = interleave\\_datasets([en\\_dataset, fr\\_dataset], probabilities=[0.8, 0.2], seed=42)\n>>> list(multilingual\\_dataset\\_with\\_oversampling.take(2))\n[{'text': 'Mtendere Village was inspired by the vision...'}, {'text': 'Lily James cannot fight the music...'}]\n```\nAround 80% of the final dataset is made of the `en\\_dataset`, and 20% of the `fr\\_dataset`."", ""If your dataset splits aren't loading as expected, it may be due to an incorrect pattern. ### Split name keywords\nThere are several ways to name splits. Validation splits are sometimes called \""dev\"", and test splits may be referred to as \""eval\"". These other split names are also supported, and the following keywords are equivalent:\n- train, training\n- validation, valid, val, dev\n- test, testing, eval, evaluation\nThe structure below is a valid repository:\n```\nmy\\_dataset\\_repository/\nâ”œâ”€â”€ README.md\nâ””â”€â”€ data/\nâ”œâ”€â”€ training.csv\nâ”œâ”€â”€ eval.csv\nâ””â”€â”€ valid.csv\n```\n### Multiple files per split\nIf one of your splits comprises several files, ðŸ¤— Datasets can still infer whether it is the train, validation, and test split from the file name. For example, if your train and test splits span several files:\n```\nmy\\_dataset\\_repository/\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ train\\_0.csv\nâ”œâ”€â”€ train\\_1.csv\nâ”œâ”€â”€ train\\_2.csv\nâ”œâ”€â”€ train\\_3.csv\nâ”œâ”€â”€ test\\_0.csv\nâ””â”€â”€ test\\_1.csv\n```\nMake sure all the files of your `train` set have \\*train\\* in their names (same for test and validation). Even if you add a prefix or suffix to `train` in the file name (like `my\\_train\\_file\\_00001.csv` for example),\nðŸ¤— Datasets can still infer the appropriate split."", ""Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package\\_reference/main\\_classes#datasets.DatasetInfo) object of the `datasets` library. ```json\n{\n\""dataset\\_info\"": {\n\""description\"": \""DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\n\"",\n\""citation\"": \""@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = {2018}\\n}\\n\"",\n\""homepage\"": \""https://duorc.github.io/\"",\n\""license\"": \""https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\"",\n\""features\"": {\n\""plot\\_id\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""plot\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""title\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""question\\_id\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""question\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""answers\"": {\n\""feature\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""\\_type\"": \""Sequence\""\n},\n\""no\\_answer\"": {\n\""dtype\"": \""bool\"",\n\""\\_type\"": \""Value\""\n}\n},\n\""builder\\_name\"": \""duorc\"",\n\""config\\_name\"": \""SelfRC\"",\n\""version\"": {\n\""version\\_str\"": \""1.0.0\"",\n\""major\"": 1,\n\""minor\"": 0,\n\""patch\"": 0\n},\n\""splits\"": {\n\""train\"": {\n\""name\"": \""train\"",\n\""num\\_bytes\"": 239852729,\n\""num\\_examples\"": 60721,\n\""dataset\\_name\"": \""duorc\""\n},\n\""validation\"": {\n\""name\"": \""validation\"",\n\""num\\_bytes\"": 51662519,\n\""num\\_examples\"": 12961,\n\""dataset\\_name\"": \""duorc\""\n},\n\""test\"": {\n\""name\"": \""test\"",\n\""num\\_bytes\"": 49142710,\n\""num\\_examples\"": 12559,\n\""dataset\\_name\"": \""duorc\""\n}\n},\n\""download\\_checksums\"": {\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_train.json\"": {\n\""num\\_bytes\"": 24388192,\n\""checksum\"": null\n},\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_dev.json\"": {\n\""num\\_bytes\"": 5051240,\n\""checksum\"": null\n},\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_test.json\"": {\n\""num\\_bytes\"": 5023228,\n\""checksum\"": null\n}\n},\n\""download\\_size\"": 34462660,\n\""dataset\\_size\"": 340657958,\n\""size\\_in\\_bytes\"": 375120618\n}\n}\n```"", ""## Offline\nEven if you don't have an internet connection, it is still possible to load a dataset. As long as you've downloaded a dataset from the Hub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline. If you know you won't have internet access, you can run ðŸ¤— Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, ðŸ¤— Datasets will look directly in the cache. Set the environment variable `HF\\_DATASETS\\_OFFLINE` to `1` to enable full offline mode. ## Slice splits\nYou can also choose only to load specific slices of a split. There are two options for slicing a split: using strings or the [`ReadInstruction`] API. Strings are more compact and readable for simple cases, while [`ReadInstruction`] is easier to use with variable slicing parameters. Concatenate a `train` and `test` split by:\n```py\n>>> train\\_test\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=\""train+test\"")\n===STRINGAPI-READINSTRUCTION-SPLIT===\n>>> ri = datasets.ReadInstruction(\""train\"") + datasets.ReadInstruction(\""test\"")\n>>> train\\_test\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=ri)\n```\nSelect specific rows of the `train` split:\n```py\n>>> train\\_10\\_20\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=\""train[10:20]\"")\n===STRINGAPI-READINSTRUCTION-SPLIT===\n>>> train\\_10\\_20\\_ds = datasets.load\\_dataset(\""bookcorpu\"", split=datasets.ReadInstruction(\""train\"", from\\_=10, to=20, unit=\""abs\""))\n```\nOr select a percentage of a split with:\n```py\n>>> train\\_10pct\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=\""train[:10%]\"")\n===STRINGAPI-READINSTRUCTION-SPLIT===\n>>> train\\_10\\_20\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=datasets.ReadInstruction(\""train\"", to=10, unit=\""%\""))\n```\nSelect a combination of percentages from each split:\n```py\n>>> train\\_10\\_80pct\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=\""train[:10%]+train[-80%:]\"")\n===STRINGAPI-READINSTRUCTION-SPLIT===\n>>> ri = (datasets.ReadInstruction(\""train\"", to=10, unit=\""%\"") + datasets.ReadInstruction(\""train\"", from\\_=-80, unit=\""%\""))\n>>> train\\_10\\_80pct\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=ri)\n```\nFinally, you can even create cross-validated splits. The example below creates 10-fold cross-validated splits. Each validation dataset is a 10% chunk, and the training dataset makes up the remaining complementary 90% chunk:\n```py\n>>> val\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=[f\""train[{k}%:{k+10}%]\"" for k in range(0, 100, 10)])\n>>> train\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=[f\""train[:{k}%]+train[{k+10}%:]\"" for k in range(0, 100, 10)])\n===STRINGAPI-READINSTRUCTION-SPLIT===\n>>> val\\_ds = datasets.load\\_dataset(\""bookcorpus\"", [datasets.ReadInstruction(\""train\"", from\\_=k, to=k+10, unit=\""%\"") for k in range(0, 100, 10)])\n>>> train\\_ds = datasets.load\\_dataset(\""bookcorpus\"", [(datasets.ReadInstruction(\""train\"", to=k, unit=\""%\"") + datasets.ReadInstruction(\""train\"", from\\_=k+10, unit=\""%\"")) for k in range(0, 100, 10)])\n```\n### Percent slicing and rounding\nThe default behavior is to round the boundaries to the nearest integer for datasets where the requested slice boundaries do not divide evenly by 100. As shown below, some slices may contain more examples than others. For instance, if the following train split includes 999 records, then:\n```py\n# 19 records, from 500 (included) to 519 (excluded). >>> train\\_50\\_52\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=\""train[50%:52%]\"")\n# 20 records, from 519 (included) to 539 (excluded). >>> train\\_52\\_54\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=\""train[52%:54%]\"")\n```\nIf you want equal sized splits, use `pct1\\_dropremainder` rounding instead. This treats the specified percentage boundaries as multiples of 1%. ```py\n# 18 records, from 450 (included) to 468 (excluded). >>> train\\_50\\_52pct1\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=datasets.ReadInstruction(\""train\"", from\\_=50, to=52, unit=\""%\"", rounding=\""pct1\\_dropremainder\""))\n# 18 records, from 468 (included) to 486 (excluded). >>> train\\_52\\_54pct1\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=datasets.ReadInstruction(\""train\"",from\\_=52, to=54, unit=\""%\"", rounding=\""pct1\\_dropremainder\""))\n# Or equivalently:\n>>> train\\_50\\_52pct1\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=\""train[50%:52%](pct1\\_dropremainder)\"")\n>>> train\\_52\\_54pct1\\_ds = datasets.load\\_dataset(\""bookcorpus\"", split=\""train[52%:54%](pct1\\_dropremainder)\"")\n```\n`pct1\\_dropremainder` rounding may truncate the last examples in a dataset if the number of examples in your dataset don't divide evenly by 100."", ""It provides the number of rows, the number of colums (where applicable) and the size in bytes for the different forms of the data: original files, size in memory (RAM) and auto-converted parquet files. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has 187.213 rows along all its configurations and splits, for a total of 97MB. ```json\n{\n\""size\"": {\n\""dataset\"": {\n\""dataset\"": \""duorc\"",\n\""num\\_bytes\\_original\\_files\"": 97383710,\n\""num\\_bytes\\_parquet\\_files\"": 58710973,\n\""num\\_bytes\\_memory\"": 1059067116,\n\""num\\_rows\"": 187213\n},\n\""configs\"": [\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""num\\_bytes\\_original\\_files\"": 62921050,\n\""num\\_bytes\\_parquet\\_files\"": 37709127,\n\""num\\_bytes\\_memory\"": 718409158,\n\""num\\_rows\"": 100972,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""num\\_bytes\\_original\\_files\"": 34462660,\n\""num\\_bytes\\_parquet\\_files\"": 21001846,\n\""num\\_bytes\\_memory\"": 340657958,\n\""num\\_rows\"": 86241,\n\""num\\_columns\"": 7\n}\n],\n\""splits\"": [\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""train\"",\n\""num\\_bytes\\_parquet\\_files\"": 26005668,\n\""num\\_bytes\\_memory\"": 496682909,\n\""num\\_rows\"": 69524,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""validation\"",\n\""num\\_bytes\\_parquet\\_files\"": 5566868,\n\""num\\_bytes\\_memory\"": 106510489,\n\""num\\_rows\"": 15591,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""test\"",\n\""num\\_bytes\\_parquet\\_files\"": 6136591,\n\""num\\_bytes\\_memory\"": 115215760,\n\""num\\_rows\"": 15857,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""train\"",\n\""num\\_bytes\\_parquet\\_files\"": 14851720,\n\""num\\_bytes\\_memory\"": 239852729,\n\""num\\_rows\"": 60721,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""validation\"",\n\""num\\_bytes\\_parquet\\_files\"": 3114390,\n\""num\\_bytes\\_memory\"": 51662519,\n\""num\\_rows\"": 12961,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""test\"",\n\""num\\_bytes\\_parquet\\_files\"": 3035736,\n\""num\\_bytes\\_memory\"": 49142710,\n\""num\\_rows\"": 12559,\n\""num\\_columns\"": 7\n}\n]\n},\n\""pending\"": [],\n\""failed\"": [],\n\""partial\"": false\n}\n```\nIf the size has `partial: true` it means that the actual size of the dataset couldn't been determined because it's too big. In that case the number of rows and bytes can be inferior to the actual numbers."", ""Download slices of rows\nDatasets Server provides a `/rows` endpoint for visualizing any slice of rows of a dataset. This will let you walk-through and inspect the data contained in a dataset. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets-server/oasst1_light.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets-server/oasst1_dark.png)\n\nCurrently, only {\"" \""}\n[datasets with parquet exports](./parquet)\nare supported so Datasets Server can extract any slice of rows without downloading the\nwhole dataset. This guide shows you how to use Datasets Server's `/rows` endpoint to download slices of a dataset. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-32d6a8be-b800-446a-8cee-f6b5ca1710df),\n[RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api),\nor [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listFirstRows). The `/rows` endpoint accepts five query parameters:\n- `dataset`: the dataset name, for example `glue` or `mozilla-foundation/common\\_voice\\_10\\_0`\n- `config`: the configuration name, for example `cola`\n- `split`: the split name, for example `train`\n- `offset`: the offset of the slice, for example `150`\n- `length`: the length of the slice, for example `10` (maximum: `100`)\n\n```python\nimport requests\nheaders = {\""Authorization\"": f\""Bearer {API\\_TOKEN}\""}\nAPI\\_URL = \""https://datasets-server.huggingface.co/rows?dataset=duorc&config=SelfRC&split=train&offset=150&length=10\""\ndef query():\nresponse = requests.get(API\\_URL, headers=headers)\nreturn response.json()\ndata = query()\n```\n\n```js\nimport fetch from \""node-fetch\"";\nasync function query(data) {\nconst response = await fetch(\n\""https://datasets-server.huggingface.co/rows?dataset=duorc&config=SelfRC&split=train&offset=150&length=10\"",\n{\nheaders: { Authorization: `Bearer ${API\\_TOKEN}` },\nmethod: \""GET\""\n}\n);\nconst result = await response.json();\nreturn result;\n}\nquery().then((response) => {\nconsole.log(JSON.stringify(response));\n});\n```\n\n```curl\ncurl https://datasets-server.huggingface.co/rows?dataset=duorc&config=SelfRC&split=train&offset=150&length=10 \\\n-X GET \\\n-H \""Authorization: Bearer ${API\\_TOKEN}\""\n```\nThe endpoint response is a JSON containing two keys:\n- The [`features`](https://huggingface.co/docs/datasets/about\\_dataset\\_features) of a dataset, including the column's name and data type."", ""Splits and configurations\nMachine learning datasets are commonly organized in \\*splits\\* and they may also have \\*configurations\\*. These internal structures provide the scaffolding for building out a dataset, and determines how a dataset should be split and organized. Understanding a dataset's structure can help you create your own dataset, and know which subset of data you should use when during model training and evaluation. ![split-configs-server](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/split-configs-server.gif)\n## Splits\nEvery processed and cleaned dataset contains \\*splits\\*, specific subsets of data reserved for specific needs. The most common splits are:\n\\* `train`: data used to train a model; this data is exposed to the model\n\\* `validation`: data reserved for evaluation and improving model hyperparameters; this data is hidden from the model\n\\* `test`: data reserved for evaluation only; this data is completely hidden from the model and ourselves\nThe `validation` and `test` sets are especially important to ensure a model is actually learning instead of \\*overfitting\\*, or just memorizing the data."", ""Preview a dataset\nDatasets Server provides a `/first-rows` endpoint for visualizing the first 100 rows of a dataset. This'll give you a good idea of the data types and example data contained in a dataset. ![dataset-viewer](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dataset-viewer.png)\nThis guide shows you how to use Datasets Server's `/first-rows` endpoint to preview a dataset. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-32d6a8be-b800-446a-8cee-f6b5ca1710df), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listFirstRows). The `/first-rows` endpoint accepts three query parameters:\n- `dataset`: the dataset name, for example `glue` or `mozilla-foundation/common\\_voice\\_10\\_0`\n- `config`: the configuration name, for example `cola`\n- `split`: the split name, for example `train`\n\n```python\nimport requests\nheaders = {\""Authorization\"": f\""Bearer {API\\_TOKEN}\""}\nAPI\\_URL = \""https://datasets-server.huggingface.co/first-rows?dataset=duorc&config=SelfRC&split=train\""\ndef query():\nresponse = requests.get(API\\_URL, headers=headers)\nreturn response.json()\ndata = query()\n```\n\n```js\nimport fetch from \""node-fetch\"";\nasync function query(data) {\nconst response = await fetch(\n\""https://datasets-server.huggingface.co/first-rows?dataset=duorc&config=SelfRC&split=train\"",\n{\nheaders: { Authorization: `Bearer ${API\\_TOKEN}` },\nmethod: \""GET\""\n}\n);\nconst result = await response.json();\nreturn result;\n}\nquery().then((response) => {\nconsole.log(JSON.stringify(response));\n});\n```\n\n```curl\ncurl https://datasets-server.huggingface.co/first-rows?dataset=duorc&config=SelfRC&split=train \\\n-X GET \\\n-H \""Authorization: Bearer ${API\\_TOKEN}\""\n```\nThe endpoint response is a JSON containing two keys:\n- The [`features`](https://huggingface.co/docs/datasets/about\\_dataset\\_features) of a dataset, including the column's name and data type. - The first 100 `rows` of a dataset and the content contained in each column of a specific row. For example, here are the `features` and the first 100 `rows` of the `duorc`/`SelfRC` train split:\n```json\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""train\"",\n\""features\"": [\n{\n\""feature\\_idx\"": 0,\n\""name\"": \""plot\\_id\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 1,\n\""name\"": \""plot\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 2,\n\""name\"": \""title\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 3,\n\""name\"": \""question\\_id\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 4,\n\""name\"": \""question\"",\n\""type\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" }\n},\n{\n\""feature\\_idx\"": 5,\n\""name\"": \""answers\"",\n\""type\"": {\n\""feature\"": { \""dtype\"": \""string\"", \""\\_type\"": \""Value\"" },\n\""\\_type\"": \""Sequence\""\n}\n},\n{\n\""feature\\_idx\"": 6,\n\""name\"": \""no\\_answer\"",\n\""type\"": { \""dtype\"": \""bool\"", \""\\_type\"": \""Value\"" }\n}\n],\n\""rows\"": [\n{\n\""row\\_idx\"": 0,\n\""row\"": {\n\""plot\\_id\"": \""/m/03vyhn\"",\n\""plot\"": \""200 years in the future, Mars has been colonized by a high-tech company.\\nMelanie Ballard (Natasha Henstridge) arrives by train to a Mars mining camp which has cut all communication links with the company headquarters.""]","The ""duorc"" dataset has six splits: three for the ""ParaphraseRC"" configuration (train, validation, test) and three for the ""SelfRC"" configuration (train, validation, test).",Six
"What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Fully Sharded Data Parallel\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or \\*rank\\*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class. Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer. ```bash\npip install accelerate\n```\n## FSDP configuration\nTo start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package\\_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`. ```bash\naccelerate config\n```\nWhen you run `accelerate config`, you'll be prompted with a series of options to configure your training environment."", ""This would result in the CPU RAM getting out of memory leading to processes being terminated. 2. Saving entire intermediate checkpoints using `FULL\\_STATE\\_DICT` with CPU offloading on rank 0 takes a lot of time and often results in NCCL Timeout errors due to indefinite hanging during broadcasting. However, at the end of training, we want the whole model state dict instead of the sharded state dict which is only compatible with FSDP. 3. We need to improve the speed and reduce the VRAM usage to train faster and save compute costs. Letâ€™s look at how to solve the above challenges and fine-tune a 70B model! Before we get started, here's all the required resources to reproduce our results:\n1. Codebase:\nhttps://github.com/pacman100/DHS-LLM-Workshop/tree/main/chat\\_assistant/training with flash-attn V2 monkey patch\n2. FSDP config: https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat\\_assistant/training/configs/fsdp\\_config.yaml\n3. SLURM script `launch.slurm`: https://gist.github.com/pacman100/1cb1f17b2f1b3139a63b764263e70b25\n4. Model: `meta-llama/Llama-2-70b-chat-hf`\n5. Dataset: [smangrul/code-chat-assistant-v1](https://huggingface.co/datasets/smangrul/code-chat-assistant-v1) (mix of LIMA+GUANACO with proper formatting in a ready-to-train format)\n### Pre-requisites\nFirst follow these steps to install Flash Attention V2: Dao-AILab/flash-attention: Fast and memory-efficient exact attention (github.com)."", ""In terms of train time, DDP with mixed precision is the fastest followed by FSDP using ZERO Stage 2 and Stage 3, respectively. As the task of causal language modelling always has fixed context sequence length (--block\\_size), the train time speedup with FSDP wasnâ€™t that great. For applications with dynamic batching, FSDP which enables larger batch sizes will likely have considerable speed up in terms of train time. FSDP mixed precision support currently has few [issues](https://github.com/pytorch/pytorch/issues/75676) with transformer. Once this is supported, the training time speed up will further improve considerably. ### CPU Offloading to enable training humongous models that wonâ€™t fit the GPU memory\nCommand for training GPT-2 XL Model (1.5B parameters):\n```bash\nexport BS=#`try with different batch sizes till you don't get OOM error,\n#i.e., start with larger batch size and go on decreasing till it fits on GPU`\ntime accelerate launch run\\_clm\\_no\\_trainer.py \\\n--model\\_name\\_or\\_path gpt2-xl \\\n--dataset\\_name wikitext \\\n--dataset\\_config\\_name wikitext-2-raw-v1 \\\n--per\\_device\\_train\\_batch\\_size $BS\n--per\\_device\\_eval\\_batch\\_size $BS\n--num\\_train\\_epochs 1\n--block\\_size 12\n```\n| Method | Batch Size Max ($BS) | Num GPUs | Approx Train Time (Hours) | Notes |\n| --- | --- | --- | --- | --- |\n| DDP | 1 | 1 | NA | OOM Error RuntimeError: CUDA out of memory."", ""Why PEFT? Full fine-tuning is expensive. Letâ€™s have some numbers to put things in perspective:\nMinimum GPU memory required for full fine-tuning:\n1. Weight: 2 bytes (Mixed-Precision training)\n2. Weight gradient: 2 bytes\n3. Optimizer state when using Adam: 4 bytes for original FP32 weight + 8 bytes for first and second moment estimates\n4. Cost per parameter adding all of the above: 16 bytes per parameter\n5. \\*\\*15.5B model -> 248GB of GPU memory without even considering huge memory requirements for storing intermediate activations -> minimum 4X A100 80GB GPUs required\\*\\*\nSince the hardware requirements are huge, we'll be using parameter-efficient fine-tuning using [QLoRA](https://arxiv.org/abs/2305.14314). Here are the minimal GPU memory requirements for fine-tuning StarCoder using QLoRA:\n> trainable params: 110,428,160 || all params: 15,627,884,544 || trainable%: 0.7066097761926236\n1. Base model Weight: 0.5 bytes \\* 15.51B frozen params = 7.755 GB\n2. Adapter weight: 2 bytes \\* 0.11B trainable params = 0.22GB\n3. Weight gradient: 2 bytes \\* 0.11B trainable params = 0.12GB\n4. Optimizer state when using Adam: 4 bytes \\* 0.11B trainable params \\* 3 = 1.32GB\n5. \\*\\*Adding all of the above -> 9.51 GB ~10GB -> 1 A100 40GB GPU required\\*\\* ðŸ¤¯. The reason for A100 40GB GPU is that the intermediate activations for long sequence lengths of 2048 and batch size of 4 for training lead to higher memory requirements. As we will see below, GPU memory required is 26GB which can be accommodated on A100 40GB GPU. Also, A100 GPUs have better compatibilty with Flash Attention 2. In the above calculations, we didn't consider memory required for intermediate activation checkpointing which is considerably huge. We leverage Flash Attention V2 and Gradient Checkpointing to overcome this issue. 1. For QLoRA along with flash attention V2 and gradient checkpointing, the total memory occupied by the model on a single A100 40GB GPU is \\*\\*26 GB\\*\\* with a \\*\\*batch size of 4\\*\\*. 2. For full fine-tuning using FSDP along with Flash Attention V2 and Gradient Checkpointing, the memory occupied per GPU ranges between \\*\\*70 GB to 77.6 GB\\*\\* with a \\*\\*per\\_gpu\\_batch\\_size of 1\\*\\*. Please refer to the [model-memory-usage](https://huggingface.co/spaces/hf-accelerate/model-memory-usage) to easily calculate how much vRAM is needed to train and perform big model inference on a model hosted on the ðŸ¤— Hugging Face Hub. ## Full Finetuning\nWe will look at how to do full fine-tuning of `bigcode/starcoder` (15B params) on 8 A100 80GB GPUs using PyTorch Fully Sharded Data Parallel (FSDP) technique. For more information on FSDP, please refer to [Fine-tuning Llama 2 70B using PyTorch FSDP](https://huggingface.co/blog/ram-efficient-pytorch-fsdp) and [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface.co/blog/pytorch-fsdp)."", ""This section covers some of the most important FSDP options. To learn more about the other available FSDP options, take a look at the [fsdp\\_config](https://huggingface.co/docs/transformers/main\\_classes/trainer#transformers.TrainingArguments.fsdp\\_config) parameters. ### Sharding strategy\nFSDP offers a number of sharding strategies to select from:\n\\* `FULL\\_SHARD` - shards model parameters, gradients and optimizer states across workers; select `1` for this option\n\\* `SHARD\\_GRAD\\_OP`- shard gradients and optimizer states across workers; select `2` for this option\n\\* `NO\\_SHARD` - don't shard anything (this is equivalent to DDP); select `3` for this option\n\\* `HYBRID\\_SHARD` - shard model parameters, gradients and optimizer states within each worker where each worker also has a full copy; select `4` for this option\n\\* `HYBRID\\_SHARD\\_ZERO2` - shard gradients and optimizer states within each worker where each worker also has a full copy; select `5` for this option\nThis is enabled by the `fsdp\\_sharding\\_strategy` flag."", ""After your dataset is prepared, and all the necessary training components are loaded, the script checks if you're using the `fsdp\\_plugin`. PyTorch offers two ways for wrapping model layers in FSDP, automatically or manually. The simplest method is to allow FSDP to automatically recursively wrap model layers without changing any other code. You can choose to wrap the model layers based on the layer name or on the size (number of parameters). In the FSDP configuration file, it uses the `TRANSFORMER\\_BASED\\_WRAP` option to wrap the [`T5Block`] layer. ```py\nif getattr(accelerator.state, \""fsdp\\_plugin\"", None) is not None:\naccelerator.state.fsdp\\_plugin.auto\\_wrap\\_policy = fsdp\\_auto\\_wrap\\_policy(model)\n```\nNext, use ðŸ¤— Accelerate's [`~accelerate.Accelerator.prepare`] function to prepare the model, datasets, optimizer, and scheduler for training. ```py\nmodel, train\\_dataloader, eval\\_dataloader, optimizer, lr\\_scheduler = accelerator.prepare(\nmodel, train\\_dataloader, eval\\_dataloader, optimizer, lr\\_scheduler\n)\n```\nFrom here, the remainder of the script handles the training loop, evaluation, and sharing your model to the Hub. ## Train\nRun the following command to launch the training script. Earlier, you saved the configuration file to `fsdp\\_config.yaml`, so you'll need to pass the path to the launcher with the `--config\\_file` argument like this:\n```bash\naccelerate launch --config\\_file fsdp\\_config.yaml examples/peft\\_lora\\_seq2seq\\_accelerate\\_fsdp.py\n```\nOnce training is complete, the script returns the accuracy and compares the predictions to the labels."", ""4. 3D parallelism [3]: Employs Data Parallelism using ZERO + Tensor Parallelism + Pipeline Parallelism to train humongous models in the order of 100s of Billions of parameters. For instance, BigScience 176B parameters Language Model employ this [6]. In this post we will look at Data Parallelism using ZeRO and more specifically the latest PyTorch feature \\*\\*[FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)\\*\\*. \\*\\*[DeepSpeed](https://github.com/microsoft/deepspeed)\\*\\* and \\*\\*[FairScale](https://github.com/facebookresearch/fairscale/)\\*\\* have implemented the core ideas of the ZERO paper. These have already been integrated in `transformers` Trainer and accompanied by great blog [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale) [10]. PyTorch recently upstreamed the Fairscale FSDP into PyTorch Distributed with additional optimizations. # Accelerate ðŸš€: Leverage PyTorch FSDP without any code changes\nWe will look at the task of Causal Language Modelling using GPT-2 Large (762M) and XL (1.5B) model variants. Below is the code for pre-training GPT-2 model. It is similar to the official causal language modeling example [here](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run\\_clm\\_no\\_trainer.py) with the addition of 2 arguments `n\\_train` (2000) and `n\\_val` (500) to prevent preprocessing/training on entire data in order to perform quick proof of concept benchmarks. [run\\_clm\\_no\\_trainer.py](./assets/62_pytorch_fsdp/run_clm_no_trainer.py)\nSample FSDP config after running the command `accelerate config`:\n```bash\ncompute\\_environment: LOCAL\\_MACHINE\ndeepspeed\\_config: {}\ndistributed\\_type: FSDP\nfsdp\\_config:\nmin\\_num\\_params: 2000\noffload\\_params: false\nsharding\\_strategy: 1\nmachine\\_rank: 0\nmain\\_process\\_ip: null\nmain\\_process\\_port: null\nmain\\_training\\_function: main\nmixed\\_precision: 'no'\nnum\\_machines: 1\nnum\\_processes: 2\nuse\\_cpu: false\n```\n## Multi-GPU FSDP\nHere, we experiment on the Single-Node Multi-GPU setting."", ""--\ntitle: \""Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\""\nthumbnail: /blog/assets/62\\_pytorch\\_fsdp/fsdp-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\nIn this post we will look at how we can leverage \\*\\*[Accelerate](https://github.com/huggingface/accelerate)\\*\\* Library for training large models which enables users to leverage the latest features of \\*\\*[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)\\*\\*. # Motivation ðŸ¤—\n\\*\\*With the ever increasing scale, size and parameters of the Machine Learning (ML) models, ML practitioners are finding it difficult to train or even load such large models on their hardware.\\*\\* On one hand, it has been found that large models learn quickly (data and compute efficient) and are significantly more performant when compared to smaller models [1]; on the other hand, it becomes prohibitive to train such models on most of the available hardware."", "">\nThis is because parameter groups created before wrapping will have no meaning post wrapping due parameter flattening of nested FSDP modules into 1D arrays (which can consume many layers). For instance, below are the named parameters of FSDP model on GPU 0 (When using 2 GPUs. Around 55M (110M/2) params in 1D arrays as this will have the 1st shard of the parameters). Here, if one has applied no weight decay for [bias, LayerNorm.weight] named parameters of unwrapped BERT-Base model, it canâ€™t be applied to the below FSDP wrapped model as there are no named parameters with either of those strings and the parameters of those layers are concatenated with parameters of various other layers. More details mentioned in this [issue](https://github.com/pytorch/pytorch/issues/76501) (`The original model parameters' .grads are not set, meaning that they cannot be optimized separately (which is why we cannot support multiple parameter groups)`). ```\n{\n'\\_fsdp\\_wrapped\\_module.flat\\_param': torch.Size([494209]),\n'\\_fsdp\\_wrapped\\_module.\\_fpw\\_module.bert.embeddings.word\\_embeddings.\\_fsdp\\_wrapped\\_module.flat\\_param': torch.Size([11720448]),\n'\\_fsdp\\_wrapped\\_module.\\_fpw\\_module.bert.encoder.\\_fsdp\\_wrapped\\_module.flat\\_param': torch.Size([42527232])\n}\n```\n- In case of multiple models, it is necessary to prepare the models before creating optimizers else it will throw an error. - Mixed precision is currently not supported with FSDP as we wait for PyTorch to fix support for it. # How it works ðŸ“\n![FSDP Workflow](./assets/62\\_pytorch\\_fsdp/FSDP\\_workflow.png)\n(Source: [link](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/))\nThe above workflow gives an overview of what happens behind the scenes when FSDP is activated. Let's first understand how DDP works and how FSDP improves it. In DDP, each worker/accelerator/GPU has a replica of the entire model parameters, gradients and optimizer states. Each worker gets a different batch of data, it goes through the forwards pass, a loss is computed followed by the backward pass to generate gradients. Now, an all-reduce operation is performed wherein each worker gets the gradients from the remaining workers and averaging is done. In this way, each worker now has the same global gradients which are used by the optimizer to update the model parameters. We can see that having full replicas consume a lot of redundant memory on each GPU, which limits the batch size as well as the size of the models. FSDP precisely addresses this by sharding the optimizer states, gradients and model parameters across the data parallel workers. It further facilitates CPU offloading of all those tensors, thereby enabling loading large models which won't fit the available GPU memory. Similar to DDP, each worker gets a different batch of data. During the forward pass, if the CPU offload is enabled, the parameters of the local shard are first copied to the GPU/accelerator. Then, each worker performs all-gather operation for a given FSDP wrapped module/layer(s) to all get the needed parameters, computation is performed followed by releasing/emptying the parameter shards of other workers. This continues for all the FSDP modules. The loss gets computed after the forward pass and during the backward pass, again an all-gather operation is performed to get all the needed parameters for a given FSDP module, computation is performed to get local gradients followed by releasing the shards of other workers."", ""--\ntitle: \""Fine-tuning Llama 2 70B using PyTorch FSDP\""\nthumbnail: /blog/assets/160\\_fsdp\\_llama/thumbnail.jpg\nauthors:\n- user: smangrul\n- user: sgugger\n- user: lewtun\n- user: philschmid\n---\n# Fine-tuning Llama 2 70B using PyTorch FSDP\n## Introduction\nIn this blog post, we will look at how to fine-tune Llama 2 70B using PyTorch FSDP and related best practices. We will be leveraging Hugging Face Transformers, Accelerate and TRL. We will also learn how to use Accelerate with SLURM. Fully Sharded Data Parallelism (FSDP) is a paradigm in which the optimizer states, gradients and parameters are sharded across devices. During the forward pass, each FSDP unit performs an \\_all-gather operation\\_ to get the complete weights, computation is performed followed by discarding the shards from other devices. After the forward pass, the loss is computed followed by the backward pass. In the backward pass, each FSDP unit performs an all-gather operation to get the complete weights, with computation performed to get the local gradients. These local gradients are averaged and sharded across the devices via a \\_reduce-scatter operation\\_ so that each device can update the parameters of its shard. For more information on what PyTorch FSDP is, please refer to this blog post: [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface.co/blog/pytorch-fsdp). ![FSDP Workflow](./assets/62\\_pytorch\\_fsdp/FSDP\\_workflow.png)\n(Source: [link](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/))\n## Hardware Used\nNumber of nodes: 2. Minimum required is 1. Number of GPUs per node: 8\nGPU type: A100\nGPU memory: 80GB\nintra-node connection: NVLink\nRAM per node: 1TB\nCPU cores per node: 96\ninter-node connection: Elastic Fabric Adapter\n## Challenges with fine-tuning LLaMa 70B\nWe encountered three main challenges when trying to fine-tune LLaMa 70B with FSDP:\n1. FSDP wraps the model after loading the pre-trained model. If each process/rank within a node loads the Llama-70B model, it would require 70\\\\*4\\\\*8 GB ~ 2TB of CPU RAM, where 4 is the number of bytes per parameter and 8 is the number of GPUs on each node."", ""By sharding the model parameters, optimizer and gradient states, and even offloading them to the CPU when they're inactive, FSDP can reduce the high cost of large-scale training. If you're interested in learning more, the following may be helpful:\n\\* Follow along with the more in-depth Accelerate guide for [FSDP](https://huggingface.co/docs/accelerate/usage\\_guides/fsdp). \\* Read the [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) blog post. \\* Read the [Scaling PyTorch models on Cloud TPUs with FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/) blog post."", ""Tried to allocate 40.00 MiB (GPU 0; 23.65 GiB total capacity; 22.27 GiB already allocated; 20.31 MiB free; 22.76 GiB reserved in total by PyTorch) |\n| DDP | 1 | 2 | NA | OOM Error RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.65 GiB total capacity; 22.27 GiB already allocated; 20.31 MiB free; 22.76 GiB reserved in total by PyTorch) |\n| DDP + FP16 | 1 | 1 | NA | OOM Error RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.65 GiB total capacity; 22.27 GiB already allocated; 20.31 MiB free; 22.76 GiB reserved in total by PyTorch) |\n| FSDP with min\\_num\\_params = 2K | 5 | 2 | 0.6 | |\n| FSDP with min\\_num\\_params = 2K + Offload to CPU | 10 | 1 | 3 | |\n| FSDP with min\\_num\\_params = 2K + Offload to CPU | 14 | 2 | 1.16 | |\nTable 2: Benchmarking FSDP on GPT-2 XL (1.5B) model\nFrom Table 2, we can observe that DDP (w and w/o fp16) isnâ€™t even able to run with batch size of 1 and results in CUDA OOM error. FSDP with Zero-Stage 3 is able to be run on 2 GPUs with batch size of 5 (effective batch size =10 (5 X 2)). FSDP with CPU offload can further increase the max batch size to 14 per GPU when using 2 GPUs. \\*\\*FSDP with CPU offload enables training GPT-2 1.5B model on a single GPU with a batch size of 10\\*\\*. This enables ML practitioners with minimal compute resources to train such large models, thereby democratizing large model training. ## Capabilities and limitations of the FSDP Integration\nLetâ€™s dive into the current support that Accelerate provides for FSDP integration and the known limitations. \\*\\*Required PyTorch version for FSDP support\\*\\*: PyTorch Nightly (or 1.12.0 if you read this after it has been released) as the model saving with FSDP activated is only available with recent fixes."", ""### CPU offload\nYou could also offload parameters and gradients when they are not in use to the CPU to save even more GPU memory and help you fit large models where even FSDP may not be sufficient. This is enabled by setting `fsdp\\_offload\\_params: true` when running `accelerate config`. ### Wrapping policy\nFSDP is applied by wrapping each layer in the network. The wrapping is usually applied in a nested way where the full weights are discarded after each forward pass to save memory for use in the next layer. The \\*auto wrapping\\* policy is the simplest way to implement this and you don't need to change any code. You should select `fsdp\\_auto\\_wrap\\_policy: TRANSFORMER\\_BASED\\_WRAP` to wrap a Transformer layer and `fsdp\\_transformer\\_layer\\_cls\\_to\\_wrap` to specify which layer to wrap (for example `BertLayer`). Otherwise, you can choose a size-based wrapping policy where FSDP is applied to a layer if it exceeds a certain number of parameters. This is enabled by setting `fsdp\\_wrap\\_policy: SIZE\\_BASED\\_WRAP` and `min\\_num\\_param` to the desired size threshold. ### Checkpointing\nIntermediate checkpoints should be saved with `fsdp\\_state\\_dict\\_type: SHARDED\\_STATE\\_DICT` because saving the full state dict with CPU offloading on rank 0 takes a lot of time and often results in `NCCL Timeout` errors due to indefinite hanging during broadcasting. You can resume training with the sharded state dicts with the [`~accelerate.Accelerator.load\\_state`]` method. ```py\n# directory containing checkpoints\naccelerator.load\\_state(\""ckpt\"")\n```\nHowever, when training ends, you want to save the full state dict because sharded state dict is only compatible with FSDP. ```py\nif trainer.is\\_fsdp\\_enabled:\ntrainer.accelerator.state.fsdp\\_plugin.set\\_state\\_dict\\_type(\""FULL\\_STATE\\_DICT\"")\ntrainer.save\\_model(script\\_args.output\\_dir)\n```\n### TPU\n[PyTorch XLA](https://pytorch.org/xla/release/2.1/index.html) supports FSDP training for TPUs and it can be enabled by modifying the FSDP configuration file generated by `accelerate config`. In addition to the sharding strategies and wrapping options specified above, you can add the parameters shown below to the file. ```yaml\nxla: True # must be set to True to enable PyTorch/XLA\nxla\\_fsdp\\_settings: # XLA-specific FSDP parameters\nxla\\_fsdp\\_grad\\_ckpt: True # use gradient checkpointing\n```\nThe [`xla\\_fsdp\\_settings`](https://github.com/pytorch/xla/blob/2e6e183e0724818f137c8135b34ef273dea33318/torch\\_xla/distributed/fsdp/xla\\_fully\\_sharded\\_data\\_parallel.py#L128) allow you to configure additional XLA-specific parameters for FSDP. ## Launch training\nAn example FSDP configuration file may look like:\n```yaml\ncompute\\_environment: LOCAL\\_MACHINE\ndebug: false\ndistributed\\_type: FSDP\ndowncast\\_bf16: 'no'\nfsdp\\_config:\nfsdp\\_auto\\_wrap\\_policy: TRANSFORMER\\_BASED\\_WRAP\nfsdp\\_backward\\_prefetch\\_policy: BACKWARD\\_PRE\nfsdp\\_cpu\\_ram\\_efficient\\_loading: true\nfsdp\\_forward\\_prefetch: false\nfsdp\\_offload\\_params: true\nfsdp\\_sharding\\_strategy: 1\nfsdp\\_state\\_dict\\_type: SHARDED\\_STATE\\_DICT\nfsdp\\_sync\\_module\\_states: true\nfsdp\\_transformer\\_layer\\_cls\\_to\\_wrap: BertLayer\nfsdp\\_use\\_orig\\_params: true\nmachine\\_rank: 0\nmain\\_training\\_function: main\nmixed\\_precision: bf16\nnum\\_machines: 1\nnum\\_processes: 2\nrdzv\\_backend: static\nsame\\_network: true\ntpu\\_env: []\ntpu\\_use\\_cluster: false\ntpu\\_use\\_sudo: false\nuse\\_cpu: false\n```\nTo launch training, run the [`accelerate launch`](https://huggingface.co/docs/accelerate/package\\_reference/cli#accelerate-launch) command and it'll automatically use the configuration file you previously created with `accelerate config`. ```bash\naccelerate launch my-trainer-script.py\n```\n```bash\naccelerate launch --fsdp=\""full shard\"" --fsdp\\_config=\""path/to/fsdp\\_config/ my-trainer-script.py\n```\n## Next steps\nFSDP can be a powerful tool for training really large models and you have access to more than one GPU or TPU."", ""`SHARDED\\_STATE\\_DICT` saves shard per GPU separately which makes it quick to save or resume training from intermediate checkpoint. When `FULL\\_STATE\\_DICT` is used, first process (rank 0) gathers the whole model on CPU and then saving it in a standard format. Letâ€™s create the accelerate config via below command:\n```\naccelerate config --config\\_file \""fsdp\\_config.yaml\""\n```\n![fsdp\\_config](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/160\\_ram\\_efficient\\_fsdp/fsdp\\_config.jpg)\nThe resulting config is available here: [fsdp\\_config.yaml](https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat\\_assistant/training/configs/fsdp\\_config.yaml). Here, the sharding strategy is `FULL\\_SHARD`. We are using `TRANSFORMER\\_BASED\\_WRAP` for auto wrap policy and it uses `\\_no\\_split\\_module` to find the Transformer block name for nested FSDP auto wrap. We use `SHARDED\\_STATE\\_DICT` to save the intermediate checkpoints and optimizer states in this format recommended by the PyTorch team. Make sure to enable broadcasting module parameters from rank 0 at the start as mentioned in the above paragraph on addressing Challenge 1. We are enabling `bf16` mixed precision training. For final checkpoint being the whole model state dict, below code snippet is used:\n```python\nif trainer.is\\_fsdp\\_enabled:\ntrainer.accelerator.state.fsdp\\_plugin.set\\_state\\_dict\\_type(\""FULL\\_STATE\\_DICT\"")\ntrainer.save\\_model(script\\_args.output\\_dir) # alternatively, trainer.push\\_to\\_hub() if the whole ckpt is below 50GB as the LFS limit per file is 50GB\n```\n### Addressing Challenge 3\nFlash Attention and enabling gradient checkpointing are required for faster training and reducing VRAM usage to enable fine-tuning and save compute costs. The codebase currently uses monkey patching and the implementation is at [chat\\_assistant/training/llama\\_flash\\_attn\\_monkey\\_patch.py](https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat\\_assistant/training/llama\\_flash\\_attn\\_monkey\\_patch.py). [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135.pdf) introduces a way to compute exact attention while being faster and memory-efficient by leveraging the knowledge of the memory hierarchy of the underlying hardware/GPUs - The higher the bandwidth/speed of the memory, the smaller its capacity as it becomes more expensive. If we follow the blog [Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr\\_intro.html), we can figure out that `Attention` module on current hardware is `memory-bound/bandwidth-bound`. The reason being that Attention \\*\\*mostly consists of elementwise operations\\*\\* as shown below on the left hand side. We can observe that masking, softmax and dropout operations take up the bulk of the time instead of matrix multiplications which consists of the bulk of FLOPs. ![Attention Bottlenecks](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/160\\_ram\\_efficient\\_fsdp/attention\\_bottleneck.png)\n(Source: [link](https://arxiv.org/pdf/2205.14135.pdf))\nThis is precisely the problem that Flash Attention addresses. The idea is to \\*\\*remove redundant HBM reads/writes.\\*\\* It does so by keeping everything in SRAM, perform all the intermediate steps and only then write the final result back to HBM, also known as \\*\\*Kernel Fusion\\*\\*. Below is an illustration of how this overcomes the memory-bound bottleneck. ![kernel\\_fusion](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/160\\_ram\\_efficient\\_fsdp/kernel\\_fusion.webp)\n(Source: [link](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad))\n\\*\\*Tiling\\*\\* is used during forward and backward passes to chunk the NxN softmax/scores computation into blocks to overcome the limitation of SRAM memory size. To enable tiling, online softmax algorithm is used."", ""Now, the local gradients are averaged and sharded to each relevant workers using reduce-scatter operation. This allows each worker to update the parameters of its local shard. If CPU offload is activated, the gradients are passed to CPU for updating parameters directly on CPU. Please refer [7, 8, 9] for all the in-depth details on the workings of the PyTorch FSDP and the extensive experimentation carried out using this feature. # Issues\nIf you encounter any issues with the integration part of PyTorch FSDP, please open an Issue in [accelerate](https://github.com/huggingface/accelerate/issues). But if you have problems with PyTorch FSDP configuration, and deployment - you need to ask the experts in their domains, therefore, please, open a [PyTorch Issue](https://github.com/pytorch/pytorch/issues) instead. # References\n[1] [Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers](http://nlp.cs.berkeley.edu/pubs/Li-Wallace-Shen-Lin-Keutzer-Klein-Gonzalez\\_2020\\_Transformers\\_paper.pdf)\n[2] [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054v3.pdf)\n[3] [DeepSpeed: Extreme-scale model training for everyone - Microsoft Research](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)\n[4] [Megatron-LM: Training Multi-Billion Parameter Language Models Using\nModel Parallelism](https://arxiv.org/pdf/1909.08053.pdf)\n[5] [Introducing GPipe, an Open Source Library for Efficiently Training Large-scale Neural Network Models](https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html)\n[6] [Which hardware do you need to train a 176B parameters model?](https://bigscience.huggingface.co/blog/which-hardware-to-train-a-176b-parameters-model)\n[7] [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API | PyTorch](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)\n[8] [Getting Started with Fully Sharded Data Parallel(FSDP) â€” PyTorch Tutorials 1.11.0+cu102 documentation](https://pytorch.org/tutorials/intermediate/FSDP\\_tutorial.html)\n[9] [Training a 1 Trillion Parameter Model With PyTorch Fully Sharded Data Parallel on AWS | by PyTorch | PyTorch | Mar, 2022 | Medium](https://medium.com/pytorch/training-a-1-trillion-parameter-model-with-pytorch-fully-sharded-data-parallel-on-aws-3ac13aa96cff)\n[10] [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale)"", ""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Fully Sharded Data Parallel\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes. Currently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates. FSDP is supported in ðŸ¤— Accelerate, and you can use it with ðŸ¤— PEFT. This guide will help you learn how to use our FSDP [training script](https://github.com/huggingface/peft/blob/main/examples/conditional\\_generation/peft\\_lora\\_seq2seq\\_accelerate\\_fsdp.py). You'll configure the script to train a large model for conditional generation. ## Configuration\nBegin by running the following command to [create a FSDP configuration file](https://huggingface.co/docs/accelerate/main/en/usage\\_guides/fsdp) with ðŸ¤— Accelerate. Use the `--config\\_file` flag to save the configuration file to a specific location, otherwise it is saved as a `default\\_config.yaml` file in the ðŸ¤— Accelerate cache. The configuration file is used to set the default options when you launch the training script. ```bash\naccelerate config --config\\_file fsdp\\_config.yaml\n```\nYou'll be asked a few questions about your setup, and configure the following arguments. For this example, make sure you fully shard the model parameters, gradients, optimizer states, leverage the CPU for offloading, and wrap model layers based on the Transformer layer class name. ```bash\n`Sharding Strategy`: [1] FULL\\_SHARD (shards optimizer states, gradients and parameters), [2] SHARD\\_GRAD\\_OP (shards optimizer states and gradients), [3] NO\\_SHARD\n`Offload Params`: Decides Whether to offload parameters and gradients to CPU\n`Auto Wrap Policy`: [1] TRANSFORMER\\_BASED\\_WRAP, [2] SIZE\\_BASED\\_WRAP, [3] NO\\_WRAP\n`Transformer Layer Class to Wrap`: When using `TRANSFORMER\\_BASED\\_WRAP`, user specifies comma-separated string of transformer layer class names (case-sensitive) to wrap ,e.g,\n`BertLayer`, `GPTJBlock`, `T5Block`, `BertLayer,BertEmbeddings,BertSelfOutput`... `Min Num Params`: minimum number of parameters when using `SIZE\\_BASED\\_WRAP`\n`Backward Prefetch`: [1] BACKWARD\\_PRE, [2] BACKWARD\\_POST, [3] NO\\_PREFETCH\n`State Dict Type`: [1] FULL\\_STATE\\_DICT, [2] LOCAL\\_STATE\\_DICT, [3] SHARDED\\_STATE\\_DICT\n```\nFor example, your FSDP configuration file may look like the following:\n```yaml\ncommand\\_file: null\ncommands: null\ncompute\\_environment: LOCAL\\_MACHINE\ndeepspeed\\_config: {}\ndistributed\\_type: FSDP\ndowncast\\_bf16: 'no'\ndynamo\\_backend: 'NO'\nfsdp\\_config:\nfsdp\\_auto\\_wrap\\_policy: TRANSFORMER\\_BASED\\_WRAP\nfsdp\\_backward\\_prefetch\\_policy: BACKWARD\\_PRE\nfsdp\\_offload\\_params: true\nfsdp\\_sharding\\_strategy: 1\nfsdp\\_state\\_dict\\_type: FULL\\_STATE\\_DICT\nfsdp\\_transformer\\_layer\\_cls\\_to\\_wrap: T5Block\ngpu\\_ids: null\nmachine\\_rank: 0\nmain\\_process\\_ip: null\nmain\\_process\\_port: null\nmain\\_training\\_function: main\nmegatron\\_lm\\_config: {}\nmixed\\_precision: 'no'\nnum\\_machines: 1\nnum\\_processes: 2\nrdzv\\_backend: static\nsame\\_network: true\ntpu\\_name: null\ntpu\\_zone: null\nuse\\_cpu: false\n```\n## The important parts\nLet's dig a bit deeper into the training script to understand how it works."", ""We compare the performance of Distributed Data Parallel (DDP) and FSDP in various configurations. First, GPT-2 Large(762M) model is used wherein DDP works with certain batch sizes without throwing Out Of Memory (OOM) errors. Next, GPT-2 XL (1.5B) model is used wherein DDP fails with OOM error even on batch size of 1. We observe that FSDP enables larger batch sizes for GPT-2 Large model and it enables training the GPT-2 XL model with decent batch size unlike DDP. \\*\\*Hardware setup\\*\\*: 2X24GB NVIDIA Titan RTX GPUs. Command for training GPT-2 Large Model (762M parameters):\n```bash\nexport BS=#`try with different batch sizes till you don't get OOM error,\n#i.e., start with larger batch size and go on decreasing till it fits on GPU`\ntime accelerate launch run\\_clm\\_no\\_trainer.py \\\n--model\\_name\\_or\\_path gpt2-large \\\n--dataset\\_name wikitext \\\n--dataset\\_config\\_name wikitext-2-raw-v1 \\\n--per\\_device\\_train\\_batch\\_size $BS\n--per\\_device\\_eval\\_batch\\_size $BS\n--num\\_train\\_epochs 1\n--block\\_size 12\n```\nSample FSDP Run:\n![Sample FSDP Run](./assets/62\\_pytorch\\_fsdp/sample\\_fsdp\\_run.png)\n| Method | Batch Size Max ($BS) | Approx Train Time (minutes) | Notes |\n| --- | --- | --- | --- |\n| DDP (Distributed Data Parallel) | 7 | 15 | |\n| DDP + FP16 | 7 | 8 | |\n| FSDP with SHARD\\_GRAD\\_OP | 11 | 11 | |\n| FSDP with min\\_num\\_params = 1M + FULL\\_SHARD | 15 | 12 | |\n| FSDP with min\\_num\\_params = 2K + FULL\\_SHARD | 15 | 13 | |\n| FSDP with min\\_num\\_params = 1M + FULL\\_SHARD + Offload to CPU | 20 | 23 | |\n| FSDP with min\\_num\\_params = 2K + FULL\\_SHARD + Offload to CPU | 22 | 24 | |\nTable 1: Benchmarking FSDP on GPT-2 Large (762M) model\nWith respect to DDP, from Table 1 we can observe that FSDP \\*\\*enables larger batch sizes\\*\\*, up to \\*\\*2X-3X\\*\\* without and with CPU offload setting, respectively.""]","Fully Sharded Data Parallel (FSDP) is designed for distributed training of large pretrained models by sharding the model parameters, gradients, and optimizer states across data parallel processes. This approach improves memory efficiency, allowing practitioners to scale training to larger batch or model sizes. By offloading sharded model parameters to a CPU, FSDP helps reduce the high memory costs associated with training large-scale models, enabling the training of models that may not fit into the available GPU memory.","FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes."
"What file format is used to save and store PyTorch model weights more securely than `.bin` files?
","[""Convert weights to safetensors\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`. The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository. For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights. Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space."", ""Pickle Scanning\nPickle is a widely used serialization format in ML. Most notably, it is the default format for PyTorch model weights. There are dangerous arbitrary code execution attacks that can be perpetrated when you load a pickle file. We suggest loading models from users and organizations you trust, relying on signed commits, and/or loading models from TF or Jax formats with the `from\\_tf=True` auto-conversion mechanism. We also alleviate this issue by displaying/\""vetting\"" the list of imports in any pickled file, directly on the Hub. Finally, we are experimenting with a new, simple serialization format for weights called [`safetensors`](https://github.com/huggingface/safetensors). ## What is a pickle? From the [official docs](https://docs.python.org/3/library/pickle.html) :\n> The `pickle` module implements binary protocols for serializing and de-serializing a Python object structure. What this means is that pickle is a serializing protocol, something you use to efficiently share data amongst parties. We call a pickle the binary file that was generated while pickling. At its core, the pickle is basically a stack of instructions or opcodes. As you probably have guessed, itâ€™s not human readable. The opcodes are generated when pickling and read sequentially at unpickling. Based on the opcode, a given action is executed. Hereâ€™s a small example:\n```python\nimport pickle\nimport pickletools\nvar = \""data I want to share with a friend\""\n# store the pickle data in a file named 'payload.pkl'\nwith open('payload.pkl', 'wb') as f:\npickle.dump(var, f)\n# disassemble the pickle\n# and print the instructions to the command line\nwith open('payload.pkl', 'rb') as f:\npickletools.dis(f)\n```\nWhen you run this, it will create a pickle file and print the following instructions in your terminal:\n```python\n0: \\x80 PROTO 4\n2: \\x95 FRAME 48\n11: \\x8c SHORT\\_BINUNICODE 'data I want to share with a friend'\n57: \\x94 MEMOIZE (as 0)\n58: ."", ""We are also taking steps to make sure the existing PyTorch files are not malicious, but the best we can do is flag suspicious-looking files. Of course, there are other file formats out there, but\nnone seemed to meet the full set of [ideal requirements](https://github.com/huggingface/safetensors#yet-another-format-) our team identified. In addition to being safe, `safetensors` allows lazy loading and generally faster loads (around 100x faster on CPU). Lazy loading means loading only part of a tensor in an efficient manner. This particular feature enables arbitrary sharding with efficient inference libraries, such as [text-generation-inference](https://github.com/huggingface/text-generation-inference), to load LLMs (such as LLaMA, StarCoder, etc.) on various types of hardware\nwith maximum efficiency. Because it loads so fast and is framework agnostic, we can even use the format\nto load models from the same file in PyTorch or TensorFlow. ## The security audit\nSince `safetensors` main asset is providing safety guarantees, we wanted to make sure\nit actually delivered. That's why Hugging Face, EleutherAI, and Stability AI teamed up to get an external\nsecurity audit to confirm it. Important findings:\n- No critical security flaw leading to arbitrary code execution was found. - Some imprecisions in the spec format were detected and fixed. - Some missing validation allowed [polyglot files](https://en.wikipedia.org/wiki/Polyglot\\_(computing)), which was fixed. - Lots of improvements to the test suite were proposed and implemented. In the name of openness and transparency, all companies agreed to make the report\nfully public. [Full report](https://huggingface.co/datasets/safetensors/trail\\_of\\_bits\\_audit\\_repot/resolve/main/SOW-TrailofBits-EleutherAI\\_HuggingFace-v1.2.pdf)\nOne import thing to note is that the library is written in Rust."", ""There are several reasons for using safetensors:\n- Safety is the number one reason for using safetensors. As open-source and model distribution grows, it is important to be able to trust the model weights you downloaded don't contain any malicious code. The current size of the header in safetensors prevents parsing extremely large JSON files. - Loading speed between switching models is another reason to use safetensors, which performs zero-copy of the tensors. It is especially fast compared to `pickle` if you're loading the weights to CPU (the default case), and just as fast if not faster when directly loading the weights to GPU. You'll only notice the performance difference if the model is already loaded, and not if you're downloading the weights or loading the model for the first time. The time it takes to load the entire pipeline:\n```py\nfrom diffusers import StableDiffusionPipeline\npipeline = StableDiffusionPipeline.from\\_pretrained(\""stabilityai/stable-diffusion-2-1\"", use\\_safetensors=True)\n\""Loaded in safetensors 0:00:02.033658\""\n\""Loaded in PyTorch 0:00:02.663379\""\n```\nBut the actual time it takes to load 500MB of the model weights is only:\n```bash\nsafetensors: 3.4873ms\nPyTorch: 172.7537ms\n```\n- Lazy loading is also supported in safetensors, which is useful in distributed settings to only load some of the tensors. This format allowed the [BLOOM](https://huggingface.co/bigscience/bloom) model to be loaded in 45 seconds on 8 GPUs instead of 10 minutes with regular PyTorch weights."", ""## Mitigation Strategies\n\\*\\*\\*Donâ€™t use pickle\\*\\*\\*\nSound advice Luc, but pickle is used profusely and isnâ€™t going anywhere soon: finding a new format everyone is happy with and initiating the change will take some time. So what can we do for now? ### Load files from users and organizations you trust\nOn the Hub, you have the ability to [sign your commits with a GPG key](./security-gpg). This does \\*\\*not\\*\\* guarantee that your file is safe, but it does guarantee the origin of the file. If you know and trust user A and the commit that includes the file on the Hub is signed by user Aâ€™s GPG key, itâ€™s pretty safe to assume that you can trust the file. ### Load model weights from TF or Flax\nTensorFlow and Flax checkpoints are not affected, and can be loaded within PyTorch architectures using the `from\\_tf` and `from\\_flax` kwargs for the `from\\_pretrained` method to circumvent this issue. E.g.:\n```python\nfrom transformers import AutoModel\nmodel = AutoModel.from\\_pretrained(\""bert-base-cased\"", from\\_flax=True)\n```\n### Use your own serialization format\n- [MsgPack](https://msgpack.org/index.html)\n- [Protobuf](https://developers.google.com/protocol-buffers)\n- [Cap'n'proto](https://capnproto.org/)\n- [Avro](https://avro.apache.org/)\n- [safetensors](https://github.com/huggingface/safetensors)\nThis last format, `safetensors`, is a simple serialization format that we are working on and experimenting with currently!"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Load safetensors\n[[open-in-colab]]\n[safetensors](https://github.com/huggingface/safetensors) is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or \\*pickled\\* into a `.bin` file with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights. This guide will show you how you load `.safetensor` files, and how to convert Stable Diffusion model weights stored in other formats to `.safetensor`. Before you start, make sure you have safetensors installed:\n```py\n# uncomment to install the necessary libraries in Colab\n#!pip install safetensors\n```\nIf you look at the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main) repository, you'll see weights inside the `text\\_encoder`, `unet` and `vae` subfolders are stored in the `.safetensors` format. By default, ðŸ¤— Diffusers automatically loads these `.safetensors` files from their subfolders if they're available in the model repository. For more explicit control, you can optionally set `use\\_safetensors=True` (if `safetensors` is not installed, you'll get an error message asking you to install it):\n```py\nfrom diffusers import DiffusionPipeline\npipeline = DiffusionPipeline.from\\_pretrained(\""runwayml/stable-diffusion-v1-5\"", use\\_safetensors=True)\n```\nHowever, model weights are not necessarily stored in separate subfolders like in the example above. Sometimes, all the weights are stored in a single `.safetensors` file. In this case, if the weights are Stable Diffusion weights, you can load the file directly with the [`~diffusers.loaders.FromSingleFileMixin.from\\_single\\_file`] method:\n```py\nfrom diffusers import StableDiffusionPipeline\npipeline = StableDiffusionPipeline.from\\_single\\_file(\n\""https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\""\n)\n```\n## Convert to safetensors\nNot all weights on the Hub are available in the `.safetensors` format, and you may encounter weights stored as `.bin`."", ""When you're using `transformers`, if `safetensors` is installed, then those files will already\nbe used preferentially in order to prevent issues, which means that\n```\npip install safetensors\n```\nis likely to be the only thing needed to run `safetensors` files safely. Going forward and thanks to the validation of the library, `safetensors` will now be installed in `transformers` by\ndefault. The next step is saving models in `safetensors` by default. We are thrilled to see that the `safetensors` library is already seeing use in the ML ecosystem, including:\n- [Civitai](https://civitai.com/)\n- [Stable Diffusion Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)\n- [dfdx](https://github.com/coreylowman/dfdx)\n- [LLaMA.cpp](https://github.com/ggerganov/llama.cpp/blob/e6a46b0ed1884c77267dc70693183e3b7164e0e0/convert.py#L537)\n## Why create something new? The creation of this library was driven by the fact that PyTorch uses `pickle` under\nthe hood, which is inherently unsafe. (Sources: [1](https://huggingface.co/docs/hub/security-pickle), [2, video](https://www.youtube.com/watch?v=2ethDz9KnLk), [3](https://github.com/pytorch/pytorch/issues/52596))\nWith pickle, it is possible to write a malicious file posing as a model\nthat gives full control of a user's computer to an attacker without the user's knowledge,\nallowing the attacker to steal all their bitcoins ðŸ˜“. While this vulnerability in pickle is widely known in the computer security world (and is acknowledged in the PyTorch [docs](https://pytorch.org/docs/stable/generated/torch.load.html)), itâ€™s not common knowledge in the broader ML community. Since the Hugging Face Hub is a platform where anyone can upload and share models, it is important to make efforts\nto prevent users from getting infected by malware."", ""--\ntitle: \""ðŸ¶Safetensors audited as really safe and becoming the default\""\nthumbnail: /blog/assets/142\\_safetensors\\_official/thumbnail.png\nauthors:\n- user: Narsil\n- user: stellaathena\nguest: true\n---\n# Audit shows that safetensors is safe and ready to become the default\n[Hugging Face](https://huggingface.co/), in close collaboration with [EleutherAI](https://www.eleuther.ai/) and [Stability AI](https://stability.ai/), has ordered\nan external security audit of the `safetensors` library, the results of which allow\nall three organizations to move toward making the library the default format\nfor saved models. The full results of the security audit, performed by [Trail of Bits](https://www.trailofbits.com/),\ncan be found here: [Report](https://huggingface.co/datasets/safetensors/trail\\_of\\_bits\\_audit\\_repot/resolve/main/SOW-TrailofBits-EleutherAI\\_HuggingFace-v1.2.pdf). The following blog post explains the origins of the library, why these audit results are important,\nand the next steps. ## What is safetensors? ðŸ¶[Safetensors](https://github.com/huggingface/safetensors) is a library\nfor saving and loading tensors in the most common frameworks (including PyTorch, TensorFlow, JAX, PaddlePaddle, and NumPy). For a more concrete explanation, we'll use PyTorch. ```python\nimport torch\nfrom safetensors.torch import load\\_file, save\\_file\nweights = {\""embeddings\"": torch.zeros((10, 100))}\nsave\\_file(weights, \""model.safetensors\"")\nweights2 = load\\_file(\""model.safetensors\"")\n```\nIt also has a number of [cool features](https://github.com/huggingface/safetensors#yet-another-format-) compared to other formats, most notably that loading files is \\_safe\\_, as we'll see later.""]",The file format used to save and store PyTorch model weights more securely than `.bin` files is called `.safetensors`.,`.safetensors`
"What type of security certification does Hugging Face have?
","[""Security\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)"", ""The certification process\nThe certification process is \\*\\*completely free\\*\\*:\n- To get a \\*certificate of completion\\*: you need \\*\\*to pass 80% of the assignments\\*\\*. - To get a \\*certificate of excellence\\*: you need \\*\\*to pass 100% of the assignments\\*\\*. There's \\*\\*no deadlines, the course is self-paced\\*\\*. ![Course certification](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit0/certification.jpg)\nWhen we say pass, \\*\\*we mean that your model must be pushed to the Hub and get a result equal or above the minimal requirement\\*\\*. To check your progression and which unit you passed/not passed: https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course\nNow that you're ready for the certification process, you need to:\n1. Go here: https://huggingface.co/spaces/huggingface-projects/Deep-RL-Course-Certification/\n2. Type your \\*hugging face username\\*, your \\*first name\\*, \\*last name\\*\n3. Click on \""Generate my certificate\"". - If you passed 80% of the assignments, \\*\\*congratulations\\*\\* you've just got the certificate of completion. - If you passed 100% of the assignments, \\*\\*congratulations\\*\\* you've just got the excellence certificate. - If you are below 80%, don't be discouraged!"", ""| Version | Supported |\n|---------|--------------------|\n| 1.x.x | :white\\_check\\_mark: |\n| 2.x.x | :white\\_check\\_mark: |\n## Reporting a Vulnerability\nTo report a security vulnerability, please contact: security@huggingface.co"", ""--\ntitle: \""Hugging Face Selected for the French Data Protection Agency Enhanced Support Program\""\nthumbnail: /blog/assets/146\\_cnil-accompaniment/logo.png\nauthors:\n- user: yjernite\n- user: julien-c\n- user: annatrdj\n- user: Ima1\n---\n# Hugging Face Selected for the French Data Protection Agency Enhanced Support Program\n\\*This blog post was originally published on [LinkedIn on 05/15/2023](https://www.linkedin.com/pulse/accompagnement-renforc%25C3%25A9-de-la-cnil-et-protection-des-donn%25C3%25A9es/)\\*\nWe are happy to announce that Hugging Face has been selected by the [CNIL](https://www.cnil.fr/en/home) (French Data Protection Authority) to benefit from its [Enhanced Support program](https://www.cnil.fr/en/enhanced-support-cnil-selects-3-digital-companies-strong-potential)! This new program picked three companies with â€œstrong potential for economic developmentâ€ out of over 40 candidates, who will receive support in understanding and implementing their duties with respect to data protection -\na daunting and necessary endeavor in the context of the rapidly evolving field of Artificial Intelligence."", ""Single Sign-On (SSO)\nThe Hugging Face Hub gives you the ability to implement mandatory Single Sign-On (SSO) for your organization. We support both SAML 2.0 and OpenID Connect (OIDC) protocols. This feature is part of the [Enterprise Hub](https://huggingface.co/enterprise). ## How does it work? When Single Sign-On is enabled, the members of your organization must authenticate through your Identity Provider (IdP) to access any content under the organization's namespace. Public content will still be available to users who are not members of the organization. \\*\\*We use email addresses to identify SSO users. Make sure that your organizational email address (e.g. your company email) has been added to [your user account](https://huggingface.co/settings/account).\\*\\*\nWhen users log in, they will be prompted to complete the Single Sign-On authentication flow with a banner similar to the following:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-sso-prompt.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-sso-prompt-dark.png)\n\nSingle Sign-On only applies to your organization. Members may belong to other organizations on Hugging Face. We support [role mapping](#role-mapping): you can automatically assign [roles](./organizations-security#access-control-in-organizations) to organization members based on attributes provided by your Identity Provider. ### Supported Identity Providers\nYou can easily integrate Hugging Face Hub with a variety of Identity Providers, such as Okta, OneLogin or Azure Active Directory (Azure AD). Hugging Face Hub can work with any OIDC-compliant or SAML Identity Provider. ## How to configure OIDC/SAML provider in the Hub\nWe have some guides available to help with configuring based on your chosen SSO provider, or to take inspiration from:\n- [How to configure OIDC with Okta in the Hub](./security-sso-okta-oidc)\n- [How to configure SAML with Okta in the Hub](./security-sso-okta-saml)\n- [How to configure SAML with Azure in the Hub](./security-sso-azure-saml)\n### Users Management\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-settings-users.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/sso/sso-settings-users-dark.png)\n\n#### Session Timeout\nThis value sets the duration of the session for members of your organization."", ""Security Policy\n## Reporting a Vulnerability\nðŸ¤— We have our bug bounty program set up with HackerOne. Please feel free to submit vulnerability reports to our private program at https://hackerone.com/hugging\\_face. Note that you'll need to be invited to our program, so send us a quick email at security@huggingface.co if you've found a vulnerability."", ""Security & Compliance\nðŸ¤— Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place. ## Data Security/Privacy\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit. We also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet. Hugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan. ## Model Security/Privacy:\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub. ## Inference Endpoints and Hub Security\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\n## Inference Endpoint Security level\nWe currently offer three types of endpoints, in order or increasing security level:\n- \\*\\*Public\\*\\*: A Public Endpoint is available from the internet, secured with TLS/SSL, and requires no authentication."", ""| Version | Supported |\n| ------- | ------------------ |\n| 1.x.x | :white\\_check\\_mark: |\n## Reporting a Vulnerability\nTo report a security vulnerability, please contact: feedback@huggingface.co"", ""| Version | Supported |\n| ------- | ------------------ |\n| 1.x.x | :white\\_check\\_mark: |\n## Reporting a Vulnerability\nTo report a security vulnerability, please contact: security@huggingface.co""]","Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses.",SOC2 Type 2 certified
"What do RAG models combine to generate outputs?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# RAG\n\n[![Models](https://img.shields.io/badge/All_model_pages-rag-blueviolet)](https://huggingface.co/models?filter=rag)\n\n## Overview\nRetrieval-augmented generation (\""RAG\"") models combine the powers of pretrained dense retrieval (DPR) and\nsequence-to-sequence models."", ""If this path already exists, the script will use saved predictions to calculate metrics. Add `--recalculate` parameter to force the script to perform inference from scratch. An example e2e evaluation run could look as follows:\n```bash\npython examples/research\\_projects/rag/eval\\_rag.py \\\n--model\\_name\\_or\\_path facebook/rag-sequence-nq \\\n--model\\_type rag\\_sequence \\\n--evaluation\\_set path/to/test.source \\\n--gold\\_data\\_path path/to/gold\\_data \\\n--predictions\\_path path/to/e2e\\_preds.txt \\\n--eval\\_mode e2e \\\n--gold\\_data\\_mode qa \\\n--n\\_docs 5 \\ # You can experiment with retrieving different number of documents at evaluation time\n--print\\_predictions \\\n--recalculate \\ # adding this parameter will force recalculating predictions even if predictions\\_path already exists\n```\n# Use your own knowledge source\nBy default, RAG uses the English Wikipedia as a knowledge source, known as the 'wiki\\_dpr' dataset. With `use\\_custom\\_knowledge\\_dataset.py` you can build your own knowledge source, \\*e.g.\\* for RAG. For instance, if documents are serialized as tab-separated csv files with the columns \""title\"" and \""text\"", one can use `use\\_own\\_knowledge\\_dataset.py` as follows:\n```bash\npython examples/research\\_projects/rag/use\\_own\\_knowledge\\_dataset.py \\\n--csv\\_path path/to/my\\_csv \\\n--output\\_dir path/to/my\\_knowledge\\_dataset \\\n```\nThe created outputs in `path/to/my\\_knowledge\\_dataset` can then be used to finetune RAG as follows:\n```bash\npython examples/research\\_projects/rag/finetune\\_rag.py \\\n--data\\_dir $DATA\\_DIR \\\n--output\\_dir $OUTPUT\\_DIR \\\n--model\\_name\\_or\\_path $MODEL\\_NAME\\_OR\\_PATH \\\n--model\\_type rag\\_sequence \\\n--fp16 \\\n--gpus 8\n--index\\_name custom\n--passages\\_path path/to/data/my\\_knowledge\\_dataset\n--index\\_path path/to/my\\_knowledge\\_dataset\\_hnsw\\_index.faiss\n```"", ""To get those metrics, we'll need to generate outputs from the model and convert them to strings. Let's build some lists of labels and predictions for the ROUGE metric to compare (note that if you get import errors for this section, you may need to`!pip install tqdm`). We're also going to use a trick that dramatically increases performance - compiling our generation code with [XLA](https://www.tensorflow.org/xla), TensorFlow's accelerated linear algebra compiler. XLA applies various optimizations to the model's computation graph, and results in significant improvements to speed and memory usage. As described in the Hugging Face [blog](https://huggingface.co/blog/tf-xla-generate), XLA works best when our input shapes don't vary too much. To handle this, we'll pad our inputs to multiples of 128, and make a new dataset with the padding collator, and then we'll apply the `@tf.function(jit\\_compile=True)` decorator to our generation function, which marks the whole function for compilation with XLA. ```python\nfrom tqdm import tqdm\nimport numpy as np\ngeneration\\_data\\_collator = DataCollatorForSeq2Seq(\ntokenizer, model=model, return\\_tensors=\""tf\"", pad\\_to\\_multiple\\_of=320\n)\ntf\\_generate\\_dataset = model.prepare\\_tf\\_dataset(\ntokenized\\_datasets[\""validation\""],\ncollate\\_fn=generation\\_data\\_collator,\nshuffle=False,\nbatch\\_size=8,\ndrop\\_remainder=True,\n)\n@tf.function(jit\\_compile=True)\ndef generate\\_with\\_xla(batch):\nreturn model.generate(\ninput\\_ids=batch[\""input\\_ids\""],\nattention\\_mask=batch[\""attention\\_mask\""],\nmax\\_new\\_tokens=32,\n)\nall\\_preds = []\nall\\_labels = []\nfor batch, labels in tqdm(tf\\_generate\\_dataset):\npredictions = generate\\_with\\_xla(batch)\ndecoded\\_preds = tokenizer.batch\\_decode(predictions, skip\\_special\\_tokens=True)\nlabels = labels.numpy()\nlabels = np.where(labels != -100, labels, tokenizer.pad\\_token\\_id)\ndecoded\\_labels = tokenizer.batch\\_decode(labels, skip\\_special\\_tokens=True)\ndecoded\\_preds = [\""\\n\"".join(sent\\_tokenize(pred.strip())) for pred in decoded\\_preds]\ndecoded\\_labels = [\""\\n\"".join(sent\\_tokenize(label.strip())) for label in decoded\\_labels]\nall\\_preds.extend(decoded\\_preds)\nall\\_labels.extend(decoded\\_labels)\n```\nOnce we have our lists of label and prediction strings, computing the ROUGE score is easy:\n```python\nresult = rouge\\_score.compute(\npredictions=decoded\\_preds, references=decoded\\_labels, use\\_stemmer=True\n)\nresult = {key: value.mid.fmeasure \\* 100 for key, value in result.items()}\n{k: round(v, 4) for k, v in result.items()}\n```\n```\n{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}\n```\n{/if}\n{#if fw === 'pt'}\n## Fine-tuning mT5 with ðŸ¤— Accelerate[[fine-tuning-mt5-with-accelerate]]\nFine-tuning our model with ðŸ¤— Accelerate is very similar to the text classification example we encountered in [Chapter 3](/course/chapter3)."", ""2. \\*\\*PyTorch specific\\*\\*: The document retrieval process group had to latch onto the existing process group used for training, meaning that PyTorch had to be used for training as well. ![alt\\_text](assets/12\\_ray\\_rag/ray\\_arch\\_updated.png \""image\\_tooltip\"")\n\\_Document retrieval with the Ray implementation\\_\nTo overcome these limitations, we introduced a novel implementation of distributed retrieval based on Ray. With [Rayâ€™s stateful actor abstractions](https://docs.ray.io/en/master/actors.html), multiple processes that are separate from the training processes are used to load the index and handle the retrieval queries. With multiple Ray actors, retrieval is no longer a bottleneck and PyTorch is no longer a requirement for RAG. And as you can see below, using the [Ray](https://docs.ray.io/en/master/) based implementation leads to better retrieval performance for multi-GPU fine-tuning. The following results show the seconds per retrieval call and we can see that as we increase the number of GPUs that we train on, using Ray has comparatively better performance than `torch.distributed`. Also, if we increase the number of Ray processes that perform retrieval, we also get better performance with more training workers since a single retrieval process is no longer a bottleneck. |  |  |  |  |\n| --- | --- | --- | --- |\n|  | 2 GPU | 3 GPU | 4 GPU |\n| torch.distributed | 2.12 sec/retrieval | 2.62 sec/retrieve | 3.438 sec/retrieve |\n| Ray 2 retrieval processes | 1.49 sec/retrieve | 1.539 sec/retrieve | 2.029 sec/retrieve |\n| Ray 4 retrieval processes | 1.145 sec/retrieve | 1.484 sec/retrieve | 1.66 sec/retrieve |\n\n\\_A performance comparison of different retrieval implementations. For each document retrieval implementation, we run 500 training steps with a per-GPU batch size of 8, and measure the time it takes to retrieve the contextual documents for each batch on the rank 0 training worker. As the results show, using multiple retrieval processes improves performance, especially as we scale training to multiple GPUs.\\_\n### How do I use it? [Huggingface](https://huggingface.co/) provides a [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) based [fine tuning script](https://github.com/huggingface/transformers/tree/master/examples/research\\_projects/rag), and we extended it to add the Ray retrieval implementation as an option. To try it out, first install the necessary requirements\n```bash\npip install ray\npip install transformers\npip install -r transformers/examples/research\\_projects/rag/requirements.txt\n```\nThen, you can specify your data paths and other configurations and run [finetune-rag-ray.sh](https://github.com/huggingface/transformers/blob/master/examples/research\\_projects/rag/finetune\\_rag\\_ray.sh)! ```bash\n# Sample script to finetune RAG using Ray for distributed retrieval. # Add parent directory to python path to access lightning\\_base.py\nexport PYTHONPATH=\""../\"":\""${PYTHONPATH}\""\n# Start a single-node Ray cluster. ray start --head\n# A sample finetuning run, you need to specify data\\_dir, output\\_dir and model\\_name\\_or\\_path\n# run ./examples/rag/finetune\\_rag\\_ray.sh --help to see all the possible options\npython examples/rag/finetune\\_rag.py \\\n--data\\_dir $DATA\\_DIR \\\n--output\\_dir $OUTPUT\\_DIR \\\n--model\\_name\\_or\\_path $MODEL\\_NAME\\_OR\\_PATH \\\n--model\\_type rag\\_sequence \\\n--fp16 \\\n--gpus 8 \\\n--profile \\\n--do\\_train \\\n--do\\_predict \\\n--n\\_val -1 \\\n--train\\_batch\\_size 8 \\\n--eval\\_batch\\_size 1 \\\n--max\\_source\\_length 128 \\\n--max\\_target\\_length 25 \\\n--val\\_max\\_target\\_length 25 \\\n--test\\_max\\_target\\_length 25 \\\n--label\\_smoothing 0.1 \\\n--dropout 0.1 \\\n--attention\\_dropout 0.1 \\\n--weight\\_decay 0.001 \\\n--adam\\_epsilon 1e-08 \\\n--max\\_grad\\_norm 0.1 \\\n--lr\\_scheduler polynomial \\\n--learning\\_rate 3e-05 \\\n--num\\_train\\_epochs 100 \\\n--warmup\\_steps 500 \\\n--gradient\\_accumulation\\_steps 1 \\\n--distributed\\_retriever ray \\\n--num\\_retrieval\\_workers 4\n# Stop the Ray cluster. ray stop\n```\n## Whatâ€™s next? Using RAG with [Huggingface transformers](https://github.com/huggingface/transformers/tree/master/examples/research\\_projects/rag) and the [Ray retrieval implementation](https://github.com/huggingface/transformers/blob/master/examples/research\\_projects/rag/finetune\\_rag\\_ray.sh) for faster distributed fine-tuning, you can leverage RAG for retrieval-based generation on your own knowledge-intensive tasks. Also, hyperparameter tuning is another aspect of transformer fine tuning and can have [huge impacts on accuracy](https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b). For scalable and easy hyperparameter tuning, check out the [Ray Tune](https://docs.ray.io/en/latest/tune/) library. By using [Ray Tuneâ€™s integration with PyTorch Lightning](https://medium.com/distributed-computing-with-ray/scaling-up-pytorch-lightning-hyperparameter-tuning-with-ray-tune-4bd9e1ff9929), or the [built-in integration with Huggingface transformers](https://huggingface.co/blog/ray-tune), you can run experiments to find the perfect hyperparameters for your RAG model. And lastly, stay tuned for a potential Tensorflow implementation of [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models) on [Huggingface](https://huggingface.co/)! If you plan to try RAG+Ray integration out, please feel free to share your experiences on the [Ray Discourse](https://discuss.ray.io/) or join the [Ray community Slack](https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform) for further discussion -- weâ€™d love to hear from you! > Also published at https://medium.com/distributed-computing-with-ray/retrieval-augmented-generation-with-huggingface-transformers-and-ray-b09b56161b1e"", ""[REALM](https://arxiv.org/abs/2002.08909) and [MARGE](https://arxiv.org/abs/2006.15020)). To start training, use the bash script (finetune\\_rag\\_ray\\_end2end.sh) in this folder. This script also includes descriptions on each command-line argument used. # Latest Update\nâš ï¸ Updated the rag-end2end-retriever to be compatible with PL==1.6.4 and RAY==1.13.0 (latest versions to the date 2022-June-11)\n# Note\nâš ï¸ This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability\n# Testing\nThe following two bash scripts can be used to quickly test the implementation. 1. sh ./test\\_run/test\\_finetune.sh script\n- Tests the full end-to-end fine-tuning ability with a dummy knowlendge-base and dummy training dataset (check test\\_dir directory). - Users can replace the dummy dataset and knowledge-base with their own to do their own finetuning. - Please read the comments in the test\\_finetune.sh file. 2. sh ./test\\_run/test\\_rag\\_new\\_features.sh\n- Tests the newly added functions (set\\_context\\_encoder and set\\_context\\_encoder\\_tokenizer) related to modeling rag. - This is sufficient to check the model's ability to use the set functions correctly. # Comparison of end2end RAG (including DPR finetuning) VS original-RAG\nWe conducted a simple experiment to investigate the effectiveness of this end2end training extension using the SQuAD dataset."", ""This information retrieval step allows [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) to make use of multiple sources of knowledge -- those that are baked into the model parameters and the information that is contained in the contextual passages, allowing it to outperform other state-of-the-art models in tasks like question answering. You can try it for yourself using this [demo provided by Huggingface](https://huggingface.co/rag/)! ### Scaling up fine-tuning\nThis retrieval of contextual documents is crucial for RAG's state-of-the-art results but introduces an extra layer of complexity. When scaling up the training process via a data-parallel training routine, a naive implementation of the document lookup can become a bottleneck for training. Further, the \\*\\*document index\\*\\* used in the retrieval component is often quite large, making it infeasible for each training worker to load its own replicated copy of the index. The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) communication package for the document retrieval portion."", ""The model retrieves contextual documents from an external dataset as part of its execution. These contextual documents are used in conjunction with the original input to produce an output. The GIF is taken from [Facebook's original blog post](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models).\\_\nRecently, [Huggingface](https://huggingface.co/) partnered with [Facebook AI](https://ai.facebook.com/) to introduce the [RAG](https://twitter.com/huggingface/status/1310597560906780680) model as part of its Transformers library. [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) acts just like any other [seq2seq model](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html). However, [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) has an intermediate component that retrieves contextual documents from an external knowledge base (like a Wikipedia text corpus). These documents are then used in conjunction with the input sequence and passed into the underlying seq2seq [generator](https://huggingface.co/blog/how-to-generate)."", ""End-to-End finetuning of RAG (including DPR retriever) for Question Answering. This finetuning script is actively maintained by [Shamane Siri](https://github.com/shamanez). Feel free to ask questions on the [Forum](https://discuss.huggingface.co/) or post an issue on [GitHub](https://github.com/huggingface/transformers/issues/new/choose) and tag @shamanez. Others that helped out: Patrick von Platen (@patrickvonplaten), Quentin Lhoest (@lhoestq), and Rivindu Weerasekera (@rivinduw)\nThe original RAG implementation is able to train the question encoder and generator end-to-end. This extension enables complete end-to-end training of RAG including the context encoder in the retriever component. Please read the [accompanying blog post](https://shamanesiri.medium.com/how-to-finetune-the-entire-rag-architecture-including-dpr-retriever-4b4385322552) for details on this implementation. The original RAG code has also been modified to work with the latest versions of pytorch lightning (version 1.2.10) and RAY (version 1.3.0). All other implementation details remain the same as the [original RAG code](https://github.com/huggingface/transformers/tree/main/examples/research\\_projects/rag). Read more about RAG at https://arxiv.org/abs/2005.11401. This code can be modified to experiment with other research on retrival augmented models which include training of the retriever (e.g."", ""RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate\noutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\nboth retrieval and generation to adapt to downstream tasks. It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, Douwe Kiela. The abstract from the paper is the following:\n\\*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve\nstate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely\nmanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind\ntask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge\nremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametric\nmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a\ngeneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) â€” models which combine pre-trained\nparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a\npre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages\nacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our\nmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,\noutperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation\ntasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art\nparametric-only seq2seq baseline.\\*\nThis model was contributed by [ola13](https://huggingface.co/ola13). ## Usage tips\nRetrieval-augmented generation (\""RAG\"") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq\nmodules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt\nto downstream tasks. ## RagConfig\n[[autodoc]] RagConfig\n## RagTokenizer\n[[autodoc]] RagTokenizer\n## Rag specific outputs\n[[autodoc]] models.rag.modeling\\_rag.RetrievAugLMMarginOutput\n[[autodoc]] models.rag.modeling\\_rag.RetrievAugLMOutput\n## RagRetriever\n[[autodoc]] RagRetriever\n\n## RagModel\n[[autodoc]] RagModel\n- forward\n## RagSequenceForGeneration\n[[autodoc]] RagSequenceForGeneration\n- forward\n- generate\n## RagTokenForGeneration\n[[autodoc]] RagTokenForGeneration\n- forward\n- generate\n\n## TFRagModel\n[[autodoc]] TFRagModel\n- call\n## TFRagSequenceForGeneration\n[[autodoc]] TFRagSequenceForGeneration\n- call\n- generate\n## TFRagTokenForGeneration\n[[autodoc]] TFRagTokenForGeneration\n- call\n- generate"", ""Intro\nAuthors: @patrickvonplaten and @lhoestq\nAimed at tackling the knowledge-intensive NLP tasks (think tasks a human wouldn't be expected to solve without access to external knowledge sources), RAG models are seq2seq models with access to a retrieval mechanism providing relevant context documents at training and evaluation time. A RAG model encapsulates two core components: a question encoder and a generator. During a forward pass, we encode the input with the question encoder and pass it\nto the retriever to extract relevant context documents. The documents are then prepended to the input. Such contextualized inputs are passed to the generator. Read more about RAG at https://arxiv.org/abs/2005.11401. # Note\nâš ï¸ This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability\n# Finetuning\nOur finetuning logic is based on scripts from [`examples/legacy/seq2seq`](https://github.com/huggingface/transformers/tree/main/examples/legacy/seq2seq). We accept training data in the same format as specified there - we expect a directory consisting of 6 text files:\n```bash\ntrain.source\ntrain.target\nval.source\nval.target\ntest.source\ntest.target\n```\nA sample finetuning command (run ` ./examples/research\\_projects/rag/finetune\\_rag.py --help` to list all available options):\n```bash\npython examples/research\\_projects/rag/finetune\\_rag.py \\\n--data\\_dir $DATA\\_DIR \\\n--output\\_dir $OUTPUT\\_DIR \\\n--model\\_name\\_or\\_path $MODEL\\_NAME\\_OR\\_PATH \\\n--model\\_type rag\\_sequence \\\n--fp16 \\\n--gpus 8\n```\nWe publish two `base` models which can serve as a starting point for finetuning on downstream tasks (use them as `model\\_name\\_or\\_path`):\n- [`facebook/rag-sequence-base`](https://huggingface.co/facebook/rag-sequence-base) - a base for finetuning `RagSequenceForGeneration` models,\n- [`facebook/rag-token-base`](https://huggingface.co/facebook/rag-token-base) - a base for finetuning `RagTokenForGeneration` models. The `base` models initialize the question encoder with [`facebook/dpr-question\\_encoder-single-nq-base`](https://huggingface.co/facebook/dpr-question\\_encoder-single-nq-base) and the generator with [`facebook/bart-large`](https://huggingface.co/facebook/bart-large). If you would like to initialize finetuning with a base model using different question encoder and generator architectures, you can build it with a consolidation script, e.g.:\n```\npython examples/research\\_projects/rag/consolidate\\_rag\\_checkpoint.py \\\n--model\\_type rag\\_sequence \\\n--generator\\_name\\_or\\_path facebook/bart-large-cnn \\\n--question\\_encoder\\_name\\_or\\_path facebook/dpr-question\\_encoder-single-nq-base \\\n--dest path/to/checkpoint\n```\nYou will then be able to pass `path/to/checkpoint` as `model\\_name\\_or\\_path` to the `finetune\\_rag.py` script. ## Document Retrieval\nWhen running distributed fine-tuning, each training worker needs to retrieve contextual documents\nfor its input by querying a index loaded into memory. RAG provides two implementations for document retrieval,\none with [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) communication package and the other\nwith [`Ray`](https://docs.ray.io/en/master/). This option can be configured with the `--distributed\\_retriever` flag which can either be set to `pytorch` or `ray`. By default this flag is set to `pytorch`. For the Pytorch implementation, only training worker 0 loads the index into CPU memory, and a gather/scatter pattern is used\nto collect the inputs from the other training workers and send back the corresponding document embeddings. For the Ray implementation, the index is loaded in \\*separate\\* process(es). The training workers randomly select which\nretriever worker to query. To use Ray for distributed retrieval, you have to set the `--distributed\\_retriever` arg to `ray`. To configure the number of retrieval workers (the number of processes that load the index), you can set the `num\\_retrieval\\_workers` flag."", ""--\ntitle: \""Retrieval Augmented Generation with Huggingface Transformers and Ray\""\nthumbnail: /blog/assets/12\\_ray\\_rag/ray\\_arch\\_updated.png\nauthors:\n- user: ray-project\nguest: true\n---\n# Retrieval Augmented Generation with Huggingface Transformers and Ray\n##### A guest blog post by [Amog Kamsetty](/amogkam) from the Anyscale team\n[Huggingface Transformers](https://huggingface.co/) recently added the [Retrieval Augmented Generation (RAG)](https://twitter.com/huggingface/status/1310597560906780680) model, a new NLP architecture that leverages external documents (like Wikipedia) to augment its knowledge and achieve state of the art results on knowledge-intensive tasks. In this blog post, we introduce the integration of [Ray](https://docs.ray.io/en/master/), a library for building scalable applications, into the RAG contextual document retrieval mechanism. This speeds up retrieval calls by 2x and improves the scalability of RAG distributed [fine-tuning](https://github.com/huggingface/transformers/tree/master/examples/research\\_projects/rag). ### What is Retrieval Augmented Generation (RAG)? ![alt\\_text](assets/12\\_ray\\_rag/rag\\_gif.gif \""image\\_tooltip\"")\n\\_An overview of RAG.""]",RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs.,Pretrained dense retrieval (DPR) and sequence-to-sequence models.
"What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# MarkupLM\n## Overview\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm). The model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838\\_From\\_one\\_tree\\_to\\_a\\_forest\\_a\\_unified\\_solution\\_for\\_structured\\_web\\_data\\_extraction), a dataset\nfor information extraction from web pages (basically named-entity recogntion on web pages)\nThe abstract from the paper is the following:\n\\*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\ntasks. The pre-trained model and code will be publicly available.\\*\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm). ## Usage tips\n- In addition to `input\\_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath\\_tags\\_seq` and `xpath\\_subs\\_seq`. These are the XPATH tags and subscripts respectively for each token in the input sequence. - One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info. ![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg)\n MarkupLM architecture. Taken from the [original paper.](https://arxiv.org/abs/2110.08518) \n## Usage: MarkupLMProcessor\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input\\_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks. ```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\nfeature\\_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from\\_pretrained(\""microsoft/markuplm-base\"")\nprocessor = MarkupLMProcessor(feature\\_extractor, tokenizer)\n```\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input\\_ids`, `attention\\_mask`, `token\\_type\\_ids`, `xpath\\_subs\\_seq`, `xpath\\_tags\\_seq`. Optionally, one can provide node labels to the processor, which are turned into token-level `labels`. [`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]."", ""In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs). \\*\\*Use case 1: web page classification (training, inference) + token classification (inference), parse\\_html = True\\*\\*\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML. ```python\n>>> from transformers import MarkupLMProcessor\n>>> processor = MarkupLMProcessor.from\\_pretrained(\""microsoft/markuplm-base\"")\n>>> html\\_string = \""\""\""\n... ... ... ... Hello world\n... ... ... # Welcome\n\n... Here is my website. ... ... \""\""\""\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\n>>> encoding = processor(html\\_string, return\\_tensors=\""pt\"")\n>>> print(encoding.keys())\ndict\\_keys(['input\\_ids', 'token\\_type\\_ids', 'attention\\_mask', 'xpath\\_tags\\_seq', 'xpath\\_subs\\_seq'])\n```\n\\*\\*Use case 2: web page classification (training, inference) + token classification (inference), parse\\_html=False\\*\\*\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse\\_html` to `False`. ```python\n>>> from transformers import MarkupLMProcessor\n>>> processor = MarkupLMProcessor.from\\_pretrained(\""microsoft/markuplm-base\"")\n>>> processor.parse\\_html = False\n>>> nodes = [\""hello\"", \""world\"", \""how\"", \""are\""]\n>>> xpaths = [\""/html/body/div/li[1]/div/span\"", \""/html/body/div/li[1]/div/span\"", \""html/body\"", \""html/body/div\""]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return\\_tensors=\""pt\"")\n>>> print(encoding.keys())\ndict\\_keys(['input\\_ids', 'token\\_type\\_ids', 'attention\\_mask', 'xpath\\_tags\\_seq', 'xpath\\_subs\\_seq'])\n```\n\\*\\*Use case 3: token classification (training), parse\\_html=False\\*\\*\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\ncorresponding node labels in order to train a model.""]","MarkupLMFeatureExtractor uses Beautiful Soup, a Python library for pulling data out of HTML and XML files, to extract data from HTML and XML documents.",Beautiful Soup
"What is the file size limit for syncing to HF Spaces without using Git-LFS?
","[""Managing Spaces with CircleCI Workflows\nYou can keep your app in sync with your GitHub repository with a \\*\\*CircleCI workflow\\*\\*. [CircleCI](https://circleci.com) is a continuous integration and continuous delivery (CI/CD) platform that helps automate the software development process. A [CircleCI workflow](https://circleci.com/docs/workflows/) is a set of automated tasks defined in a configuration file, orchestrated by CircleCI, to streamline the process of building, testing, and deploying software applications. \\*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\\*\nFirst, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository. ```bash\ngit remote add space https://huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME\n```\nThen force push to sync everything for the first time:\n```bash\ngit push --force space main\n```\nNext, set up a [CircleCI workflow](https://circleci.com/docs/workflows/) to push your `main` git branch to Spaces. In the example below:\n\\* Replace `HF\\_USERNAME` with your username and `SPACE\\_NAME` with your Space name. \\* [Create a context in CircleCI](https://circleci.com/docs/contexts/) and add an env variable into it called \\*HF\\_PERSONAL\\_TOKEN\\* (you can give it any name, use the key you create in place of HF\\_PERSONAL\\_TOKEN) and the value as your Hugging Face API token. You can find your Hugging Face API token under \\*\\*API Tokens\\*\\* on [your Hugging Face profile](https://huggingface.co/settings/tokens). ```yaml\nversion: 2.1\nworkflows:\nmain:\njobs:\n- sync-to-huggingface:\ncontext:\n- HuggingFace\nfilters:\nbranches:\nonly:\n- main\njobs:\nsync-to-huggingface:\ndocker:\n- image: alpine\nresource\\_class: small\nsteps:\n- run:\nname: install git\ncommand: apk update && apk add openssh-client git\n- checkout\n- run:\nname: push to Huggingface hub\ncommand: |\ngit config user.email \""\""\ngit config user.name \""\""\ngit push -f https://HF\\_USERNAME:${HF\\_PERSONAL\\_TOKEN}@huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME main\n```"", ""Repository limitations and recommendations\nThere are some limitations to be aware of when dealing with a large amount of data in your repo. Given the time it takes to stream the data,\ngetting an upload/push to fail at the end of the process or encountering a degraded experience, be it on hf.co or when working locally, can be very annoying. ## Recommendations\nWe gathered a list of tips and recommendations for structuring your repo. If you are looking for more practical tips, check out [this guide](https://huggingface.co/docs/huggingface\\_hub/main/en/guides/upload#practical-tips) on how to upload large amount of data using the Python library. | Characteristic | Recommended | Tips |\n| ---------------- | ------------------ | ------------------------------------------------------ |\n| Repo size | - | contact us for large repos (TBs of data) |\n| Files per repo | <100k | merge data into fewer files |\n| Entries per folder | <10k | use subdirectories in repo |\n| File size | <5GB | split data into chunked files |\n| Commit size | <100 files\\* | upload files in multiple commits |\n| Commits per repo | - | upload multiple files per commit and/or squash history |\n\\_\\* Not relevant when using `git` CLI directly\\_\nPlease read the next section to understand better those limits and how to deal with them."", ""Managing Spaces with Github Actions\nYou can keep your app in sync with your GitHub repository with \\*\\*Github Actions\\*\\*. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup. First, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository. ```bash\ngit remote add space https://huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME\n```\nThen force push to sync everything for the first time:\n```bash\ngit push --force space main\n```\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:\n\\* Replace `HF\\_USERNAME` with your username and `SPACE\\_NAME` with your Space name. \\* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF\\_TOKEN`. You can find your Hugging Face API token under \\*\\*API Tokens\\*\\* on your Hugging Face profile. ```yaml\nname: Sync to Hugging Face hub\non:\npush:\nbranches: [main]\n# to run this workflow manually from the Actions tab\nworkflow\\_dispatch:\njobs:\nsync-to-hub:\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@v3\nwith:\nfetch-depth: 0\nlfs: true\n- name: Push to hub\nenv:\nHF\\_TOKEN: ${{ secrets.HF\\_TOKEN }}\nrun: git push https://HF\\_USERNAME:$HF\\_TOKEN@huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME main\n```\nFinally, create an Action that automatically checks the file size of any new pull request:\n```yaml\nname: Check file size\non: # or directly `on: [push]` to run the action on every push on any branch\npull\\_request:\nbranches: [main]\n# to run this workflow manually from the Actions tab\nworkflow\\_dispatch:\njobs:\nsync-to-hub:\nruns-on: ubuntu-latest\nsteps:\n- name: Check large files\nuses: ActionsDesk/lfs-warning@v2.0\nwith:\nfilesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\n```"", ""## Explanations\nWhat are we talking about when we say \""large uploads\"", and what are their associated limitations? Large uploads can be\nvery diverse, from repositories with a few huge files (e.g. model weights) to repositories with thousands of small files\n(e.g. an image dataset). Under the hood, the Hub uses Git to version the data, which has structural implications on what you can do in your repo. If your repo is crossing some of the numbers mentioned in the previous section, \\*\\*we strongly encourage you to check out [`git-sizer`](https://github.com/github/git-sizer)\\*\\*,\nwhich has very detailed documentation about the different factors that will impact your experience. Here is a TL;DR of factors to consider:\n- \\*\\*Repository size\\*\\*: The total size of the data you're planning to upload. There is no hard limit on a Hub repository size. However, if you plan to upload hundreds of GBs or even TBs of data, we would appreciate it if you could let us know in advance so we can better help you if you have any questions during the process. You can contact us at datasets@huggingface.co or on [our Discord](http://hf.co/join/discord). - \\*\\*Number of files\\*\\*:\n- For optimal experience, we recommend keeping the total number of files under 100k. Try merging the data into fewer files if you have more. For example, json files can be merged into a single jsonl file, or large datasets can be exported as Parquet files. - The maximum number of files per folder cannot exceed 10k files per folder. A simple solution is to\ncreate a repository structure that uses subdirectories. For example, a repo with 1k folders from `000/` to `999/`, each containing at most 1000 files, is already enough. - \\*\\*File size\\*\\*: In the case of uploading large files (e.g. model weights), we strongly recommend splitting them \\*\\*into chunks of around 5GB each\\*\\*.""]","The file size limit for syncing to HF Spaces without using Git-LFS is less than 10MB. If you have files larger than this, you will need to use Git-LFS or review your files and check your history.",10MB
"What is the title of the paper introducing the ByT5 model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ByT5\n## Overview\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, Colin Raffel."", "")\n>>> labels\\_dict = tokenizer(\n... [\""La vie est comme une boÃ®te de chocolat.\"", \""Aujourd'hui c'est lundi.\""], padding=\""longest\"", return\\_tensors=\""pt\""\n... )\n>>> labels = labels\\_dict.input\\_ids\n>>> loss = model(\\*\\*model\\_inputs, labels=labels).loss\n>>> loss.item()\n17.9\n```\nSimilar to [T5](t5), ByT5 was trained on the span-mask denoising task. However,\nsince the model works directly on characters, the pretraining task is a bit\ndifferent. Let's corrupt some characters of the\ninput sentence `\""The dog chases a ball in the park.\""` and ask ByT5 to predict them\nfor us. ```python\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n>>> import torch\n>>> tokenizer = AutoTokenizer.from\\_pretrained(\""google/byt5-base\"")\n>>> model = AutoModelForSeq2SeqLM.from\\_pretrained(\""google/byt5-base\"")\n>>> input\\_ids\\_prompt = \""The dog chases a ball in the park.\""\n>>> input\\_ids = tokenizer(input\\_ids\\_prompt).input\\_ids\n>>> # Note that we cannot add \""{extra\\_id\\_...}\"" to the string directly\n>>> # as the Byte tokenizer would incorrectly merge the tokens\n>>> # For ByT5, we need to work directly on the character level\n>>> # Contrary to T5, ByT5 does not use sentinel tokens for masking, but instead\n>>> # uses final utf character ids."", ""The abstract from the paper is the following:\n\\*Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from\nthe model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they\ncan process text in any language out of the box, they are more robust to noise, and they minimize technical debt by\nremoving complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token\nsequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of\noperating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with\nminimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count,\ntraining FLOPs, and inference speed, and show that byte-level models are competitive with their token-level\ncounterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on\ntasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of\npre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our\nexperiments.\\*\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/byt5). ByT5's architecture is based on the T5v1.1 model, refer to [T5v1.1's documentation page](t5v1.1) for the API reference. They\nonly differ in how inputs should be prepared for the model, see the code examples below. Since ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix. ## Usage example\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\n```python\n>>> from transformers import T5ForConditionalGeneration\n>>> import torch\n>>> model = T5ForConditionalGeneration.from\\_pretrained(\""google/byt5-small\"")\n>>> num\\_special\\_tokens = 3\n>>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5. >>> # => Need to shift utf-8 character encodings by 3 before passing ids to model. >>> input\\_ids = torch.tensor([list(\""Life is like a box of chocolates.\"".encode(\""utf-8\""))]) + num\\_special\\_tokens\n>>> labels = torch.tensor([list(\""La vie est comme une boÃ®te de chocolat.\"".encode(\""utf-8\""))]) + num\\_special\\_tokens\n>>> loss = model(input\\_ids, labels=labels).loss\n>>> loss.item()\n2.66\n```\nFor batched inference and training it is however recommended to make use of the tokenizer:\n```python\n>>> from transformers import T5ForConditionalGeneration, AutoTokenizer\n>>> model = T5ForConditionalGeneration.from\\_pretrained(\""google/byt5-small\"")\n>>> tokenizer = AutoTokenizer.from\\_pretrained(\""google/byt5-small\"")\n>>> model\\_inputs = tokenizer(\n... [\""Life is like a box of chocolates.\"", \""Today is Monday.\""], padding=\""longest\"", return\\_tensors=\""pt\""\n...""]","The title of the paper introducing the ByT5 model is ""ByT5: Towards a token-free future with pre-trained byte-to-byte models.""",ByT5: Towards a token-free future with pre-trained byte-to-byte models
"What is the dimension of the feature vector for the base BERT model?
","[""n this video, we'll study the encoder architecture. An example of a popular encoder-only architecture is BERT, which is the most popular model of its kind. Let's first start by understanding how it works. We'll use a small example, using three words. We use these as inputs, and pass them through the encoder. We retrieve a numerical representation of each word. Here, for example, the encoder converts the three words â€œWelcome to NYCâ€ in these three sequences of numbers. The encoder outputs exactly one sequence of numbers per input word. This numerical representation can also be called a \""Feature vector\"", or \""Feature tensor\"". Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \""to\"", isn't the representation of only the \""to\"" word. It also takes into account the words around it, which we call the â€œcontextâ€.As in, it looks to the left context, the word on the left of the one we're studying (here the word \""Welcome\"") and the context on the right (here the word \""NYC\"") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \""meaning\"" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence."", ""|\n| Transformer Layers: | Number of Transformer blocks. A transformer block transforms a sequence of word representations to a sequence of contextualized words (numbered representations). |\n| Hidden Size: | Layers of mathematical functions, located between the input and output, that assign weights (to words) to produce a desired result. |\n| Attention Heads: | The size of a Transformer block. |\n| Processing: | Type of processing unit used to train the model. |\n| Length of Training: | Time it took to train the model. Hereâ€™s how many of the above ML architecture parts BERTbase and BERTlarge has:\n| | Transformer Layers | Hidden Size | Attention Heads | Parameters | Processing | Length of Training |\n|-----------|--------------------|-------------|-----------------|------------|------------|--------------------|\n| BERTbase | 12 | 768 | 12 | 110M | 4 TPUs | 4 days |\n| BERTlarge | 24 | 1024 | 16 | 340M | 16 TPUs | 4 days |\nLetâ€™s take a look at how BERTlargeâ€™s additional layers, attention heads, and parameters have increased its performance across NLP tasks."", ""It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\n\\*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\*\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A.""]",The dimension of the feature vector for the base BERT model is 768.,768
"What special identifier does the WordPiece Model use for continuing subwords?
","[""In turn, the model will now be able to make sense of token in different situations. It will understand that the words token, tokens, tokenizing, and tokenization are linked and have a similar meaning. It will also understand that tokenization, modernization, and immunization, which all have the same suffixes, are probably used in the same syntactic situations. Subword-based tokenizers generally have a way to identify which tokens are start of words, and which tokens complete start of words: token as the start of a word. ##ization as completing a word. Here the ## prefix indicates that ization is part of a word rather than the beginning of it. The ## comes from the BERT tokenizer, based on the WordPiece algorithm. Other tokenizers use other prefixes, which can be placed to indicate part of words like seen here, or start of words instead! There are a lot of different algorithms that can be used for subword tokenization, and most models obtaining state-of-the-art results in English today use some kind of subword-tokenization algorithm."", ""This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |\n## Post-Processors\nAfter the whole pipeline, we sometimes want to insert some special\ntokens before feed a tokenized string into a model like \""[CLS] My\nhorse is amazing [SEP]\"". The `PostProcessor` is the component doing\njust that. | Name | Description | Example |\n| :--- | :--- | :--- |\n| TemplateProcessing | Letâ€™s you easily template the post processing, adding special tokens, and specifying the `type\\_id` for each sequence/special token. The template is given two strings representing the single sequence and the pair of sequences, as well as a set of special tokens to use. | Example, when specifying a template with these values:  \n\n* single: `\""[CLS] $A [SEP]\""`\n* pair: `\""[CLS] $A [SEP] $B [SEP]\""`\n* special tokens:\n  + `\""[CLS]\""`\n  + `\""[SEP]\""`\n\n Input: `(\""I like this\"", \""but not this\"")`   \n Output: `\""[CLS] I like this [SEP] but not this [SEP]\""` |\n## Decoders\nThe Decoder knows how to go from the IDs used by the Tokenizer, back to\na readable piece of text. Some `Normalizer` and `PreTokenizer` use\nspecial characters or identifiers that need to be reverted for example. | Name | Description |\n| :--- | :--- |\n| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTokenizer encodes at the byte-level, using a set of visible Unicode characters to represent each byte, so we need a Decoder to revert this process and get something readable again. |\n| Metaspace | Reverts the Metaspace PreTokenizer. This PreTokenizer uses a special identifer `â–` to identify whitespaces, and so this Decoder helps with decoding these. |\n| WordPiece | Reverts the WordPiece Model. This model uses a special identifier `##` for continuing subwords, and so this Decoder helps with decoding these. |"", ""WordPiece tokenization[[wordpiece-tokenization]]\nWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It's very similar to BPE in terms of the training, but the actual tokenization is done differently. ðŸ’¡ This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm. ## Training algorithm[[training-algorithm]]\nâš ï¸ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate. Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\""word\""` gets split like this:\n```\nw ##o ##r ##d\n```\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix. Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\n$$\\mathrm{score} = (\\mathrm{freq\\\\_of\\\\_pair}) / (\\mathrm{freq\\\\_of\\\\_first\\\\_element} \\times \\mathrm{freq\\\\_of\\\\_second\\\\_element})$$\nBy dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary."", ""ðŸ’¡ Using `train\\_new\\_from\\_iterator()` on the same corpus won't result in the exact same vocabulary. This is because the ðŸ¤— Tokenizers library does not implement WordPiece for the training (since we are not completely sure of its internals), but uses BPE instead. To tokenize a new text, we pre-tokenize it, split it, then apply the tokenization algorithm on each word. That is, we look for the biggest subword starting at the beginning of the first word and split it, then we repeat the process on the second part, and so on for the rest of that word and the following words in the text:\n```python\ndef encode\\_word(word):\ntokens = []\nwhile len(word) > 0:\ni = len(word)\nwhile i > 0 and word[:i] not in vocab:\ni -= 1\nif i == 0:\nreturn [\""[UNK]\""]\ntokens.append(word[:i])\nword = word[i:]\nif len(word) > 0:\nword = f\""##{word}\""\nreturn tokens\n```\nLet's test it on one word that's in the vocabulary, and another that isn't:\n```python\nprint(encode\\_word(\""Hugging\""))\nprint(encode\\_word(\""HOgging\""))\n```\n```python out\n['Hugg', '##i', '##n', '##g']\n['[UNK]']\n```\nNow, let's write a function that tokenizes a text:\n```python\ndef tokenize(text):\npre\\_tokenize\\_result = tokenizer.\\_tokenizer.pre\\_tokenizer.pre\\_tokenize\\_str(text)\npre\\_tokenized\\_text = [word for word, offset in pre\\_tokenize\\_result]\nencoded\\_words = [encode\\_word(word) for word in pre\\_tokenized\\_text]\nreturn sum(encoded\\_words, [])\n```\nWe can try it on any text:\n```python\ntokenize(\""This is the Hugging Face course!\"")\n```\n```python out\n['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',\n'##e', '[UNK]']\n```\nThat's it for the WordPiece algorithm! Now let's take a look at Unigram."", ""The pattern can be inverted if necessary. * pattern should be either a custom string or regexp. * behavior should be one of:\n  + removed\n  + isolated\n  + mergedWithPrevious\n  + mergedWithNext\n  + contiguous\n* invert should be a boolean flag. | Example with pattern = ` `, behavior = `\""isolated\""`, invert = `False`:   \n Input: `\""Hello, how are you?\""`   \n Output: `\""Hello,\"", \"" \"", \""how\"", \"" \"", \""are\"", \"" \"", \""you?\""` |\n| Sequence | Lets you compose multiple `PreTokenizer` that will be run in the given order | |\n## Models\nModels are the core algorithms used to actually tokenize, and therefore,\nthey are the only mandatory component of a Tokenizer. | Name | Description |\n| :--- | :--- |\n| WordLevel | This is the â€œclassicâ€ tokenization algorithm. It letâ€™s you simply map words to IDs without anything fancy. This has the advantage of being really simple to use and understand, but it requires extremely large vocabularies for a good coverage. Using this `Model` requires the use of a `PreTokenizer`. No choice will be made by this model directly, it simply maps input tokens to IDs. |\n| BPE | One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding works by starting with characters, while merging those that are the most frequently seen together, thus creating new tokens. It then works iteratively to build new tokens out of the most frequent pairs it sees in a corpus. BPE is able to build words it has never seen by using multiple subword tokens, and thus requires smaller vocabularies, with less chances of having â€œunkâ€ (unknown) tokens. |\n| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words donâ€™t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word). |\n| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence."", ""For instance,\nthe word `\""bug\""` would be tokenized to `[\""b\"", \""ug\""]` but `\""mug\""` would be tokenized as `[\""\"", \""ug\""]` since\nthe symbol `\""m\""` is not in the base vocabulary. In general, single letters such as `\""m\""` are not replaced by the\n`\""\""` symbol because the training data usually includes at least one occurrence of each letter, but it is likely\nto happen for very special characters like emojis. As mentioned earlier, the vocabulary size, \\*i.e.\\* the base vocabulary size + the number of merges, is a hyperparameter\nto choose. For instance [GPT](model\\_doc/gpt) has a vocabulary size of 40,478 since they have 478 base characters\nand chose to stop training after 40,000 merges. #### Byte-level BPE\nA base vocabulary that includes all possible base characters can be quite large if \\*e.g.\\* all unicode characters are\nconsidered as base characters. To have a better base vocabulary, [GPT-2](https://cdn.openai.com/better-language-models/language\\_models\\_are\\_unsupervised\\_multitask\\_learners.pdf) uses bytes\nas the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that\nevery base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2's\ntokenizer can tokenize every text without the need for the  symbol. [GPT-2](model\\_doc/gpt) has a vocabulary\nsize of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned\nwith 50,000 merges. ### WordPiece\nWordPiece is the subword tokenization algorithm used for [BERT](model\\_doc/bert), [DistilBERT](model\\_doc/distilbert), and [Electra](model\\_doc/electra). The algorithm was outlined in [Japanese and Korean\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary. So what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is\nequivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by\nits second symbol is the greatest among all symbol pairs. \\*E.g.\\* `\""u\""`, followed by `\""g\""` would have only been\nmerged if the probability of `\""ug\""` divided by `\""u\""`, `\""g\""` would have been greater than for any other symbol\npair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it \\_loses\\_ by merging two symbols\nto ensure it's \\_worth it\\_. ### Unigram\nUnigram is a subword tokenization algorithm introduced in [Subword Regularization: Improving Neural Network Translation\nModels with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). In contrast to BPE or\nWordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each\nsymbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and\nthe most common substrings. Unigram is not used directly for any of the models in the transformers, but it's used in\nconjunction with [SentencePiece](#sentencepiece). At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training\ndata given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm\ncomputes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then\nremoves p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, \\*i.e.\\* those\nsymbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has\nreached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized. Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of\ntokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:\n```\n[\""b\"", \""g\"", \""h\"", \""n\"", \""p\"", \""s\"", \""u\"", \""ug\"", \""un\"", \""hug\""],\n```\n`\""hugs\""` could be tokenized both as `[\""hug\"", \""s\""]`, `[\""h\"", \""ug\"", \""s\""]` or `[\""h\"", \""u\"", \""g\"", \""s\""]`. So which one\nto choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that\nthe probability of each possible tokenization can be computed after training."", ""et's take a look at subword-based tokenization. Understanding why subword-based tokenization is interesting requires understanding the flaws of word-based and character-based tokenization. If you haven't seen the first videos on word-based and character-based tokenization, we recommend you check them out before looking at this video. Subword-tokenization lies in between character-based and word-based tokenization algorithms. The idea is to find a middle ground between very large vocabularies, large quantity of out-of-vocabulary tokens, loss of meaning across very similar words, for word-based tokenizers, and very long sequences, less meaningful individual tokens for character-based tokenizers. These algorithms rely on the following principle: frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. An example is the word dog: we would like to have our tokenizer to have a single ID for the word dog, rather than splitting it into characters: d, o, and g. However, when encountering the word dogs, we would like our tokenizer to understand that at the root, this is still the word dog, with an added s while slightly changes the meaning while keeping the original idea. Another example is a complex word like tokenization, which can be split into meaningful subwords. The root of the word is token, and ization completes the root to give it a slightly different meaning. It makes sense to split the word into two: token, as the root of the word (labeled as the \""start\"" of the word). ization as additional information (labeled as a \""completion\"" of the word)."", ""We can see that the words `[\""i\"", \""have\"", \""a\"", \""new\""]` are present in the tokenizer's vocabulary, but the word `\""gpu\""` is not. Consequently, the\ntokenizer splits `\""gpu\""` into known subwords: `[\""gp\"" and \""##u\""]`. `\""##\""` means that the rest of the token should\nbe attached to the previous one, without space (for decoding or reversal of the tokenization). As another example, [`~transformers.XLNetTokenizer`] tokenizes our previously exemplary text as follows:\n```py\n>>> from transformers import XLNetTokenizer\n>>> tokenizer = XLNetTokenizer.from\\_pretrained(\""xlnet-base-cased\"")\n>>> tokenizer.tokenize(\""Don't you love ðŸ¤— Transformers? We sure do.\"")\n[\""â–Don\"", \""'\"", \""t\"", \""â–you\"", \""â–love\"", \""â–\"", \""ðŸ¤—\"", \""â–\"", \""Transform\"", \""ers\"", \""?\"", \""â–We\"", \""â–sure\"", \""â–do\"", \"".\""]\n```\nWe'll get back to the meaning of those `\""â–\""` when we look at [SentencePiece](#sentencepiece). As one can see,\nthe rare word `\""Transformers\""` has been split into the more frequent subwords `\""Transform\""` and `\""ers\""`. Let's now look at how the different subword tokenization algorithms work. Note that all of those tokenization\nalgorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained\non. ### Byte-Pair Encoding (BPE)\nByte-Pair Encoding (BPE) was introduced in [Neural Machine Translation of Rare Words with Subword Units (Sennrich et\nal., 2015)](https://arxiv.org/abs/1508.07909)."", ""For instance, it won't necessarily merge `(\""un\"", \""##able\"")` even if that pair occurs very frequently in the vocabulary, because the two pairs `\""un\""` and `\""##able\""` will likely each appear in a lot of other words and have a high frequency. In contrast, a pair like `(\""hu\"", \""##gging\"")` will probably be merged faster (assuming the word \""hugging\"" appears often in the vocabulary) since `\""hu\""` and `\""##gging\""` are likely to be less frequent individually. Let's look at the same vocabulary we used in the BPE training example:\n```\n(\""hug\"", 10), (\""pug\"", 5), (\""pun\"", 12), (\""bun\"", 4), (\""hugs\"", 5)\n```\nThe splits here will be:\n```\n(\""h\"" \""##u\"" \""##g\"", 10), (\""p\"" \""##u\"" \""##g\"", 5), (\""p\"" \""##u\"" \""##n\"", 12), (\""b\"" \""##u\"" \""##n\"", 4), (\""h\"" \""##u\"" \""##g\"" \""##s\"", 5)\n```\nso the initial vocabulary will be `[\""b\"", \""h\"", \""p\"", \""##g\"", \""##n\"", \""##s\"", \""##u\""]` (if we forget about special tokens for now). The most frequent pair is `(\""##u\"", \""##g\"")` (present 20 times), but the individual frequency of `\""##u\""` is very high, so its score is not the highest (it's 1 / 36). All pairs with a `\""##u\""` actually have that same score (1 / 36), so the best score goes to the pair `(\""##g\"", \""##s\"")` -- the only one without a `\""##u\""` -- at 1 / 20, and the first merge learned is `(\""##g\"", \""##s\"") -> (\""##gs\"")`. Note that when we merge, we remove the `##` between the two tokens, so we add `\""##gs\""` to the vocabulary and apply the merge in the words of the corpus:\n```\nVocabulary: [\""b\"", \""h\"", \""p\"", \""##g\"", \""##n\"", \""##s\"", \""##u\"", \""##gs\""]\nCorpus: (\""h\"" \""##u\"" \""##g\"", 10), (\""p\"" \""##u\"" \""##g\"", 5), (\""p\"" \""##u\"" \""##n\"", 12), (\""b\"" \""##u\"" \""##n\"", 4), (\""h\"" \""##u\"" \""##gs\"", 5)\n```\nAt this point, `\""##u\""` is in all the possible pairs, so they all end up with the same score. Let's say that in this case, the first pair is merged, so `(\""h\"", \""##u\"") -> \""hu\""`. This takes us to:\n```\nVocabulary: [\""b\"", \""h\"", \""p\"", \""##g\"", \""##n\"", \""##s\"", \""##u\"", \""##gs\"", \""hu\""]\nCorpus: (\""hu\"" \""##g\"", 10), (\""p\"" \""##u\"" \""##g\"", 5), (\""p\"" \""##u\"" \""##n\"", 12), (\""b\"" \""##u\"" \""##n\"", 4), (\""hu\"" \""##gs\"", 5)\n```\nThen the next best score is shared by `(\""hu\"", \""##g\"")` and `(\""hu\"", \""##gs\"")` (with 1/15, compared to 1/21 for all the other pairs), so the first pair with the biggest score is merged:\n```\nVocabulary: [\""b\"", \""h\"", \""p\"", \""##g\"", \""##n\"", \""##s\"", \""##u\"", \""##gs\"", \""hu\"", \""hug\""]\nCorpus: (\""hug\"", 10), (\""p\"" \""##u\"" \""##g\"", 5), (\""p\"" \""##u\"" \""##n\"", 12), (\""b\"" \""##u\"" \""##n\"", 4), (\""hu\"" \""##gs\"", 5)\n```\nand we continue like this until we reach the desired vocabulary size. âœï¸ \\*\\*Now your turn!\\*\\* What will the next merge rule be? ## Tokenization algorithm[[tokenization-algorithm]]\nTokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not the merge rules learned. Starting from the word to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it. For instance, if we use the vocabulary learned in the example above, for the word `\""hugs\""` the longest subword starting from the beginning that is inside the vocabulary is `\""hug\""`, so we split there and get `[\""hug\"", \""##s\""]`. We then continue with `\""##s\""`, which is in the vocabulary, so the tokenization of `\""hugs\""` is `[\""hug\"", \""##s\""]`. With BPE, we would have applied the merges learned in order and tokenized this as `[\""hu\"", \""##gs\""]`, so the encoding is different. As another example, let's see how the word `\""bugs\""` would be tokenized. `\""b\""` is the longest subword starting at the beginning of the word that is in the vocabulary, so we split there and get `[\""b\"", \""##ugs\""]`. Then `\""##u\""` is the longest subword starting at the beginning of `\""##ugs\""` that is in the vocabulary, so we split there and get `[\""b\"", \""##u, \""##gs\""]`. Finally, `\""##gs\""` is in the vocabulary, so this last list is the tokenization of `\""bugs\""`. When the tokenization gets to a stage where it's not possible to find a subword in the vocabulary, the whole word is tokenized as unknown -- so, for instance, `\""mug\""` would be tokenized as `[\""[UNK]\""]`, as would `\""bum\""` (even if we can begin with `\""b\""` and `\""##u\""`, `\""##m\""` is not the vocabulary, and the resulting tokenization will just be `[\""[UNK]\""]`, not `[\""b\"", \""##u\"", \""[UNK]\""]`). This is another difference from BPE, which would only classify the individual characters not in the vocabulary as unknown. âœï¸ \\*\\*Now your turn!\\*\\* How will the word `\""pugs\""` be tokenized? ## Implementing WordPiece[[implementing-wordpiece]]\nNow let's take a look at an implementation of the WordPiece algorithm."", ""Also note that it added a space by default at the beginning of the sentence (before `Hello`) and ignored the double space between `are` and `you`. Now that we've seen a little of how some different tokenizers process text, we can start to explore the underlying algorithms themselves. We'll begin with a quick look at the broadly widely applicable SentencePiece; then, over the next three sections, we'll examine how the three main algorithms used for subword tokenization work. ## SentencePiece[[sentencepiece]]\n[SentencePiece](https://github.com/google/sentencepiece) is a tokenization algorithm for the preprocessing of text that you can use with any of the models we will see in the next three sections. It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, `â–`. Used in conjunction with the Unigram algorithm (see [section 7](/course/chapter7/7)), it doesn't even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese). The other main feature of SentencePiece is \\*reversible tokenization\\*: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the `\\_`s with spaces -- this results in the normalized text. As we saw earlier, the BERT tokenizer removes repeating spaces, so its tokenization is not reversible. ## Algorithm overview[[algorithm-overview]]\nIn the following sections, we'll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here's a quick overview of how they each work. Don't hesitate to come back to this table after reading each of the next sections if it doesn't make sense to you yet. Model | BPE | WordPiece | Unigram\n:----:|:---:|:---------:|:------:\nTraining | Starts from a small vocabulary and learns rules to merge tokens | Starts from a small vocabulary and learns rules to merge tokens | Starts from a large vocabulary and learns rules to remove tokens\nTraining step | Merges the tokens corresponding to the most common pair | Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent | Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus\nLearns | Merge rules and a vocabulary | Just a vocabulary | A vocabulary with a score for each token\nEncoding | Splits a word into characters and applies the merges learned during training | Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word | Finds the most likely split into tokens, using the scores learned during training\nNow let's dive into BPE!""]",The WordPiece Model uses the special identifier `##` for continuing subwords.,##
"What is the purpose of the ðŸ§¨ Diffusers tutorials?
","[""Overview\nThese examples show how to run [Diffuser](https://arxiv.org/abs/2205.09991) in Diffusers. There are two ways to use the script, `run\\_diffuser\\_locomotion.py`. The key option is a change of the variable `n\\_guide\\_steps`. When `n\\_guide\\_steps=0`, the trajectories are sampled from the diffusion model, but not fine-tuned to maximize reward in the environment. By default, `n\\_guide\\_steps=2` to match the original implementation. You will need some RL specific requirements to run the examples:\n```\npip install -f https://download.pytorch.org/whl/torch\\_stable.html \\\nfree-mujoco-py \\\neinops \\\ngym==0.24.1 \\\nprotobuf==3.20.1 \\\ngit+https://github.com/rail-berkeley/d4rl.git \\\nmediapy \\\nPillow==9.0.0\n```"", ""For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.\\*\\*\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!"", ""--\ntitle: Swift ðŸ§¨Diffusers - Fast Stable Diffusion for Mac\nthumbnail: /blog/assets/fast-mac-diffusers/thumbnail.png\nauthors:\n- user: pcuenq\n- user: reach-vb\n---\n# Swift ðŸ§¨Diffusers: Fast Stable Diffusion for Mac\nTransform your text into stunning images with ease using Diffusers for Mac, a native app powered by state-of-the-art diffusion models. It leverages a bouquet of SoTA Text-to-Image models contributed by the community to the Hugging Face Hub, and converted to Core ML for blazingly fast performance. Our latest version, 1.1, is now available on the [Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) with significant performance upgrades and user-friendly interface tweaks. It's a solid foundation for future feature updates. Plus, the app is fully open source with a permissive [license](https://github.com/huggingface/swift-coreml-diffusers/blob/main/LICENSE), so you can build on it too! Check out our GitHub repository at https://github.com/huggingface/swift-coreml-diffusers for more information. ![Screenshot showing Diffusers for Mac UI](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-mac-diffusers/UI.png)\n## What exactly is ðŸ§¨Diffusers for Mac anyway? The Diffusers app ([App Store](https://apps.apple.com/app/diffusers/id1666309574), [source code](https://github.com/huggingface/swift-coreml-diffusers)) is the Mac counterpart to our [ðŸ§¨`diffusers` library](https://github.com/huggingface/diffusers). This library is written in Python with PyTorch, and uses a modular design to train and run diffusion models."", ""If you liked this topic and want to learn more, we recommend the following resources:\n- Our [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). - The [Getting Started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers\\_intro.ipynb) notebook, that gives a broader overview on Diffusion systems. - The [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) blog post. - Our [code in GitHub](https://github.com/huggingface/diffusers) where we'd be more than happy if you leave a â­ if `diffusers` is useful to you! ### Citation:\n```\n@article{patil2022stable,\nauthor = {Patil, Suraj and Cuenca, Pedro and Lambert, Nathan and von Platen, Patrick},\ntitle = {Stable Diffusion with ðŸ§¨ Diffusers},\njournal = {Hugging Face Blog},\nyear = {2022},\nnote = {[https://huggingface.co/blog/rlhf](https://huggingface.co/blog/stable\\_diffusion)},\n}\n```"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\nðŸ¤— Diffusers provides a collection of training scripts for you to train your own diffusion models. You can find all of our training scripts in [diffusers/examples](https://github.com/huggingface/diffusers/tree/main/examples). Each training script is:\n- \\*\\*Self-contained\\*\\*: the training script does not depend on any local files, and all packages required to run the script are installed from the `requirements.txt` file. - \\*\\*Easy-to-tweak\\*\\*: the training scripts are an example of how to train a diffusion model for a specific task and won't work out-of-the-box for every training scenario. You'll likely need to adapt the training script for your specific use-case. To help you with that, we've fully exposed the data preprocessing code and the training loop so you can modify it for your own use. - \\*\\*Beginner-friendly\\*\\*: the training scripts are designed to be beginner-friendly and easy to understand, rather than including the latest state-of-the-art methods to get the best and most competitive results."", ""Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). ðŸ¤— Diffusers offers three core components:\n- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code. - Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality. - Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. ## Installation\nWe recommend installing ðŸ¤— Diffusers in a virtual environment from PyPI or Conda. For more details about installing [PyTorch](https://pytorch.org/get-started/locally/) and [Flax](https://flax.readthedocs.io/en/latest/#installation), please refer to their official documentation. ### PyTorch\nWith `pip` (official package):\n```bash\npip install --upgrade diffusers[torch]\n```\nWith `conda` (maintained by the community):\n```sh\nconda install -c conda-forge diffusers\n```\n### Flax\nWith `pip` (official package):\n```bash\npip install --upgrade diffusers[flax]\n```\n### Apple Silicon (M1/M2) support\nPlease refer to the [How to use Stable Diffusion in Apple Silicon](https://huggingface.co/docs/diffusers/optimization/mps) guide. ## Quickstart\nGenerating outputs is super easy with ðŸ¤— Diffusers. To generate an image from text, use the `from\\_pretrained` method to load any pretrained diffusion model (browse the [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) for 16000+ checkpoints):\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\npipeline = DiffusionPipeline.from\\_pretrained(\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16)\npipeline.to(\""cuda\"")\npipeline(\""An image of a squirrel in Picasso style\"").images[0]\n```\nYou can also dig into the models and schedulers toolbox to build your own diffusion system:\n```python\nfrom diffusers import DDPMScheduler, UNet2DModel\nfrom PIL import Image\nimport torch\nscheduler = DDPMScheduler.from\\_pretrained(\""google/ddpm-cat-256\"")\nmodel = UNet2DModel.from\\_pretrained(\""google/ddpm-cat-256\"").to(\""cuda\"")\nscheduler.set\\_timesteps(50)\nsample\\_size = model.config.sample\\_size\nnoise = torch.randn((1, 3, sample\\_size, sample\\_size), device=\""cuda\"")\ninput = noise\nfor t in scheduler.timesteps:\nwith torch.no\\_grad():\nnoisy\\_residual = model(input, t).sample\nprev\\_noisy\\_sample = scheduler.step(noisy\\_residual, t, input).prev\\_sample\ninput = prev\\_noisy\\_sample\nimage = (input / 2 + 0.5).clamp(0, 1)\nimage = image.cpu().permute(0, 2, 3, 1).numpy()[0]\nimage = Image.fromarray((image \\* 255).round().astype(\""uint8\""))\nimage\n```\nCheck out the [Quickstart](https://huggingface.co/docs/diffusers/quicktour) to launch your diffusion journey today! ## How to navigate the documentation\n| \\*\\*Documentation\\*\\* | \\*\\*What can I learn?\\*\\* |\n|---------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Tutorial](https://huggingface.co/docs/diffusers/tutorials/tutorial\\_overview) | A basic crash course for learning how to use the library's most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model. |\n| [Loading](https://huggingface.co/docs/diffusers/using-diffusers/loading\\_overview) | Guides for how to load and configure all the components (pipelines, models, and schedulers) of the library, as well as how to use different schedulers. |\n| [Pipelines for inference](https://huggingface.co/docs/diffusers/using-diffusers/pipeline\\_overview) | Guides for how to use pipelines for different inference tasks, batched generation, controlling generated outputs and randomness, and how to contribute a pipeline to the library."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\nWelcome to ðŸ§¨ Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how ðŸ§¨ Diffusers is meant to be used. You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want. After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications. Feel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers! Let's start diffusing!"", ""!---\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# ðŸ§¨ Diffusers Examples\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\nfor a variety of use cases involving training or fine-tuning. \\*\\*Note\\*\\*: If you are looking for \\*\\*official\\*\\* examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines)."", ""--\ntitle: Stable Diffusion with ðŸ§¨ Diffusers\nthumbnail: /blog/assets/98\\_stable\\_diffusion/thumbnail.png\nauthors:\n- user: valhalla\n- user: pcuenq\n- user: natolambert\n- user: patrickvonplaten\n---\n# Stable Diffusion with ðŸ§¨ Diffusers\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb)\n# \\*\\*Stable Diffusion\\*\\* ðŸŽ¨\n\\*...using ðŸ§¨ Diffusers\\*\nStable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). It is trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database. \\*LAION-5B\\* is the largest, freely accessible multi-modal dataset that currently exists. In this post, we want to show how to use Stable Diffusion with the [ðŸ§¨ Diffusers library](https://github.com/huggingface/diffusers), explain how the model works and finally dive a bit deeper into how `diffusers` allows\none to customize the image generation pipeline. \\*\\*Note\\*\\*: It is highly recommended to have a basic understanding of how diffusion models work. If diffusion\nmodels are completely new to you, we recommend reading one of the following blog posts:\n- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n- [Getting started with ðŸ§¨ Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers\\_intro.ipynb)\nNow, let's get started by generating some images ðŸŽ¨. ## Running Stable Diffusion\n### License\nBefore using the model, you need to accept the model [license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) in order to download and use the weights."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n\n![](https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg)\n\n# Diffusers\nðŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, ðŸ¤— Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). The library has three main components:\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in ðŸ¤— Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve. - Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality. - Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. [Tutorials\n\nLearn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you're using ðŸ¤— Diffusers for the first time!](./tutorials/tutorial_overview)\n[How-to guides\n\nPractical guides for helping you load pipelines, models, and schedulers. You'll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.](./using-diffusers/loading_overview)\n[Conceptual guides\n\nUnderstand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.](./conceptual/philosophy)\n[Reference\n\nTechnical descriptions of how ðŸ¤— Diffusers classes and methods work.](./api/models/overview)""]","The purpose of the ðŸ§¨ Diffusers tutorials is to provide a gentle introduction to diffusion models and help users understand the library fundamentals. These beginner-friendly tutorials are designed to teach users how to use a pipeline for inference to rapidly generate outputs, and then deconstruct that pipeline to understand how to use the library as a modular toolbox for building their own diffusion systems. After completing the tutorials, users will gain the necessary skills to explore the library on their own and apply it to their own projects and applications.",To provide a gentle introduction to diffusion models and help understand the library fundamentals.
"What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?
","[""Using Flagging\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\nTags: FLAGGING, DATA\n## Introduction\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \""hard\"" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust. Gradio simplifies the collection of this data by including a \\*\\*Flag\\*\\* button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`. ## The \\*\\*Flag\\*\\* button in `gradio.Interface`\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked \\*\\*Flag\\*\\*. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file. There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail. - `allow\\_flagging`: this parameter can be set to either `\""manual\""` (default), `\""auto\""`, or `\""never\""`. - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked. - `auto`: users will not see a button to flag, but every sample will be flagged automatically. - `never`: users will not see a button to flag, and no sample will be flagged. - `flagging\\_options`: this parameter can be either `None` (default) or a list of strings. - If `None`, then the user simply clicks on the \\*\\*Flag\\*\\* button and no additional options are shown. - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\""Incorrect\"", \""Ambiguous\""]`, then buttons labeled \\*\\*Flag as Incorrect\\*\\* and \\*\\*Flag as Ambiguous\\*\\* appear. This only applies if `allow\\_flagging` is `\""manual\""`. - The chosen option is then logged along with the input and output. - `flagging\\_dir`: this parameter takes a string. - It represents what to name the directory where flagged data is stored. - `flagging\\_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n- Using this parameter allows you to write custom code that gets run when the flag button is clicked\n- By default, this is set to an instance of `gr.CSVLogger`\n- One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset."", ""- `description`: which accepts text, markdown or HTML and places it right under the title. - `article`: which also accepts text, markdown or HTML and places it below the interface. ![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)\nIf you're using the `Blocks` API instead, you can insert text, markdown, or HTML anywhere using the `gr.Markdown(...)` or `gr.HTML(...)` components, with descriptive content inside the `Component` constructor. Another useful keyword argument is `label=`, which is present in every `Component`. This modifies the label text at the top of each `Component`. You can also add the `info=` keyword argument to form elements like `Textbox` or `Radio` to provide further information on their usage. ```python\ngr.Number(label='Age', info='In years, must be greater than 0')\n```\n## Flagging\nBy default, an `Interface` will have \""Flag\"" button. When a user testing your `Interface` sees input with interesting output, such as erroneous or unexpected model behaviour, they can flag the input for you to review. Within the directory provided by the `flagging\\_dir=` argument to the `Interface` constructor, a CSV file will log the flagged inputs. If the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example, with the calculator interface shown above, we would have the flagged data stored in the flagged directory shown below:\n```directory\n+-- calculator.py\n+-- flagged/\n| +-- logs.csv\n```\n\\_flagged/logs.csv\\_\n```csv\nnum1,operation,num2,Output\n5,add,7,12\n6,subtract,1.5,4.5\n```\nWith the sepia interface shown earlier, we would have the flagged data stored in the flagged directory shown below:\n```directory\n+-- sepia.py\n+-- flagged/\n| +-- logs.csv\n| +-- im/\n| | +-- 0.png\n| | +-- 1.png\n| +-- Output/\n| | +-- 0.png\n| | +-- 1.png\n```\n\\_flagged/logs.csv\\_\n```csv\nim,Output\nim/0.png,Output/0.png\nim/1.png,Output/1.png\n```\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging\\_options` argument of Interface. Users will have to select one of the strings when flagging, which will be saved as an additional column to the CSV."", ""(See more below.)\n## What happens to flagged data? Within the directory provided by the `flagging\\_dir` argument, a CSV file will log the flagged data. Here's an example: The code below creates the calculator interface embedded below it:\n```python\nimport gradio as gr\ndef calculator(num1, operation, num2):\nif operation == \""add\"":\nreturn num1 + num2\nelif operation == \""subtract\"":\nreturn num1 - num2\nelif operation == \""multiply\"":\nreturn num1 \\* num2\nelif operation == \""divide\"":\nreturn num1 / num2\niface = gr.Interface(\ncalculator,\n[\""number\"", gr.Radio([\""add\"", \""subtract\"", \""multiply\"", \""divide\""]), \""number\""],\n\""number\"",\nallow\\_flagging=\""manual\""\n)\niface.launch()\n```\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged. ```directory\n+-- flagged/\n| +-- logs.csv\n```\n\\_flagged/logs.csv\\_\n```csv\nnum1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n```\nIf the interface involves file data, such as for Image and Audio components, folders will be created to store those flagged data as well. For example an `image` input to `image` output interface will create the following structure. ```directory\n+-- flagged/\n| +-- logs.csv\n| +-- image/\n| | +-- 0.png\n| | +-- 1.png\n| +-- Output/\n| | +-- 0.png\n| | +-- 1.png\n```\n\\_flagged/logs.csv\\_\n```csv\nim,Output timestamp\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\n```\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to the `flagging\\_options` argument of Interface. Users will have to select one of these choices when flagging, and the option will be saved as an additional column to the CSV. If we go back to the calculator example, the following code will create the interface embedded below it. ```python\niface = gr.Interface(\ncalculator,\n[\""number\"", gr.Radio([\""add\"", \""subtract\"", \""multiply\"", \""divide\""]), \""number\""],\n\""number\"",\nallow\\_flagging=\""manual\"",\nflagging\\_options=[\""wrong sign\"", \""off by one\"", \""other\""]\n)\niface.launch()\n```\nWhen users click the flag button, the csv file will now include a column indicating the selected option. \\_flagged/logs.csv\\_\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```\n## The HuggingFaceDatasetSaver Callback\nSometimes, saving the data to a local CSV file doesn't make sense."", ""ä½¿ç”¨æ ‡è®°\nç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\næ ‡ç­¾ï¼šæ ‡è®°ï¼Œæ•°æ®\n## ç®€ä»‹\nå½“æ‚¨æ¼”ç¤ºä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡åž‹æ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æ”¶é›†è¯•ç”¨æ¨¡åž‹çš„ç”¨æˆ·çš„æ•°æ®ï¼Œç‰¹åˆ«æ˜¯æ¨¡åž‹è¡Œä¸ºä¸å¦‚é¢„æœŸçš„æ•°æ®ç‚¹ã€‚æ•èŽ·è¿™äº›â€œå›°éš¾â€æ•°æ®ç‚¹æ˜¯æœ‰ä»·å€¼çš„ï¼Œå› ä¸ºå®ƒå…è®¸æ‚¨æ”¹è¿›æœºå™¨å­¦ä¹ æ¨¡åž‹å¹¶ä½¿å…¶æ›´å¯é å’Œç¨³å¥ã€‚\nGradio é€šè¿‡åœ¨æ¯ä¸ªâ€œç•Œé¢â€ä¸­åŒ…å«ä¸€ä¸ª\\*\\*æ ‡è®°\\*\\*æŒ‰é’®æ¥ç®€åŒ–è¿™äº›æ•°æ®çš„æ”¶é›†ã€‚è¿™ä½¿å¾—ç”¨æˆ·æˆ–æµ‹è¯•äººå‘˜å¯ä»¥è½»æ¾åœ°å°†æ•°æ®å‘é€å›žè¿è¡Œæ¼”ç¤ºçš„æœºå™¨ã€‚æ ·æœ¬ä¼šä¿å­˜åœ¨ä¸€ä¸ª CSV æ—¥å¿—æ–‡ä»¶ä¸­ï¼ˆé»˜è®¤æƒ…å†µä¸‹ï¼‰ã€‚å¦‚æžœæ¼”ç¤ºæ¶‰åŠå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘æˆ–å…¶ä»–ç±»åž‹çš„æ–‡ä»¶ï¼Œåˆ™è¿™äº›æ–‡ä»¶ä¼šå•ç‹¬ä¿å­˜åœ¨ä¸€ä¸ªå¹¶è¡Œç›®å½•ä¸­ï¼Œå¹¶ä¸”è¿™äº›æ–‡ä»¶çš„è·¯å¾„ä¼šä¿å­˜åœ¨ CSV æ–‡ä»¶ä¸­ã€‚\n## åœ¨ `gradio.Interface` ä¸­ä½¿ç”¨\\*\\*æ ‡è®°\\*\\*æŒ‰é’®\nä½¿ç”¨ Gradio çš„ `Interface` è¿›è¡Œæ ‡è®°ç‰¹åˆ«ç®€å•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨è¾“å‡ºç»„ä»¶ä¸‹æ–¹æœ‰ä¸€ä¸ªæ ‡è®°ä¸º\\*\\*æ ‡è®°\\*\\*çš„æŒ‰é’®ã€‚å½“ç”¨æˆ·æµ‹è¯•æ‚¨çš„æ¨¡åž‹æ—¶ï¼Œå¦‚æžœçœ‹åˆ°æœ‰è¶£çš„è¾“å‡ºï¼Œä»–ä»¬å¯ä»¥ç‚¹å‡»æ ‡è®°æŒ‰é’®å°†è¾“å…¥å’Œè¾“å‡ºæ•°æ®å‘é€å›žè¿è¡Œæ¼”ç¤ºçš„æœºå™¨ã€‚æ ·æœ¬ä¼šä¿å­˜åœ¨ä¸€ä¸ª CSV æ—¥å¿—æ–‡ä»¶ä¸­ï¼ˆé»˜è®¤æƒ…å†µä¸‹ï¼‰ã€‚å¦‚æžœæ¼”ç¤ºæ¶‰åŠå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘æˆ–å…¶ä»–ç±»åž‹çš„æ–‡ä»¶ï¼Œåˆ™è¿™äº›æ–‡ä»¶ä¼šå•ç‹¬ä¿å­˜åœ¨ä¸€ä¸ªå¹¶è¡Œç›®å½•ä¸­ï¼Œå¹¶ä¸”è¿™äº›æ–‡ä»¶çš„è·¯å¾„ä¼šä¿å­˜åœ¨ CSV æ–‡ä»¶ä¸­ã€‚\nåœ¨ `gradio.Interface` ä¸­æœ‰[å››ä¸ªå‚æ•°](https://gradio.app/docs/#interface-header)æŽ§åˆ¶æ ‡è®°çš„å·¥ä½œæ–¹å¼ã€‚æˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»å®ƒä»¬ã€‚\n- `allow\\_flagging`ï¼šæ­¤å‚æ•°å¯ä»¥è®¾ç½®ä¸º `\""manual\""`ï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œ`\""auto\""` æˆ– `\""never\""`ã€‚\n- `manual`ï¼šç”¨æˆ·å°†çœ‹åˆ°ä¸€ä¸ªæ ‡è®°æŒ‰é’®ï¼Œåªæœ‰åœ¨ç‚¹å‡»æŒ‰é’®æ—¶æ ·æœ¬æ‰ä¼šè¢«æ ‡è®°ã€‚\n- `auto`ï¼šç”¨æˆ·å°†ä¸ä¼šçœ‹åˆ°ä¸€ä¸ªæ ‡è®°æŒ‰é’®ï¼Œä½†æ¯ä¸ªæ ·æœ¬éƒ½ä¼šè‡ªåŠ¨è¢«æ ‡è®°ã€‚\n- `never`ï¼šç”¨æˆ·å°†ä¸ä¼šçœ‹åˆ°ä¸€ä¸ªæ ‡è®°æŒ‰é’®ï¼Œå¹¶ä¸”ä¸ä¼šæ ‡è®°ä»»ä½•æ ·æœ¬ã€‚\n- `flagging\\_options`ï¼šæ­¤å‚æ•°å¯ä»¥æ˜¯ `None`ï¼ˆé»˜è®¤å€¼ï¼‰æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ã€‚\n- å¦‚æžœæ˜¯ `None`ï¼Œåˆ™ç”¨æˆ·åªéœ€ç‚¹å‡»\\*\\*æ ‡è®°\\*\\*æŒ‰é’®ï¼Œä¸ä¼šæ˜¾ç¤ºå…¶ä»–é€‰é¡¹ã€‚\n- å¦‚æžœæä¾›äº†ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œåˆ™ç”¨æˆ·ä¼šçœ‹åˆ°å¤šä¸ªæŒ‰é’®ï¼Œå¯¹åº”äºŽæä¾›çš„æ¯ä¸ªå­—ç¬¦ä¸²ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæ­¤å‚æ•°çš„å€¼ä¸º`[\"" é”™è¯¯ \"", \"" æ¨¡ç³Š \""]`ï¼Œåˆ™ä¼šæ˜¾ç¤ºæ ‡è®°ä¸º\\*\\*æ ‡è®°ä¸ºé”™è¯¯\\*\\*å’Œ\\*\\*æ ‡è®°ä¸ºæ¨¡ç³Š\\*\\*çš„æŒ‰é’®ã€‚è¿™ä»…é€‚ç”¨äºŽ `allow\\_flagging` ä¸º `\""manual\""` çš„æƒ…å†µã€‚\n- æ‰€é€‰é€‰é¡¹å°†ä¸Žè¾“å…¥å’Œè¾“å‡ºä¸€èµ·è®°å½•ã€‚\n- `flagging\\_dir`ï¼šæ­¤å‚æ•°æŽ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚\n- å®ƒè¡¨ç¤ºæ ‡è®°æ•°æ®å­˜å‚¨çš„ç›®å½•åç§°ã€‚\n- `flagging\\_callback`ï¼šæ­¤å‚æ•°æŽ¥å— `FlaggingCallback` ç±»çš„å­ç±»çš„å®žä¾‹\n- ä½¿ç”¨æ­¤å‚æ•°å…è®¸æ‚¨ç¼–å†™åœ¨ç‚¹å‡»æ ‡è®°æŒ‰é’®æ—¶è¿è¡Œçš„è‡ªå®šä¹‰ä»£ç \n- é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒè®¾ç½®ä¸º `gr.CSVLogger` çš„ä¸€ä¸ªå®žä¾‹\n- ä¸€ä¸ªç¤ºä¾‹æ˜¯å°†å…¶è®¾ç½®ä¸º `gr.HuggingFaceDatasetSaver` çš„ä¸€ä¸ªå®žä¾‹ï¼Œè¿™æ ·æ‚¨å¯ä»¥å°†ä»»ä½•æ ‡è®°çš„æ•°æ®å¯¼å…¥åˆ° HuggingFace æ•°æ®é›†ä¸­ï¼ˆå‚è§ä¸‹æ–‡ï¼‰ã€‚\n## æ ‡è®°çš„æ•°æ®ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ\nåœ¨ `flagging\\_dir` å‚æ•°æä¾›çš„ç›®å½•ä¸­ï¼Œå°†è®°å½•æ ‡è®°çš„æ•°æ®çš„ CSV æ–‡ä»¶ã€‚\nä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼šä¸‹é¢çš„ä»£ç åˆ›å»ºäº†åµŒå…¥å…¶ä¸­çš„è®¡ç®—å™¨ç•Œé¢ï¼š\n```python\nimport gradio as gr\ndef calculator(num1, operation, num2):\nif operation == \""add\"":\nreturn num1 + num2\nelif operation == \""subtract\"":\nreturn num1 - num2\nelif operation == \""multiply\"":\nreturn num1 \\* num2\nelif operation == \""divide\"":\nreturn num1 / num2\niface = gr.Interface(\ncalculator,\n[\""number\"", gr.Radio([\""add\"", \""subtract\"", \""multiply\"", \""divide\""]), \""number\""],\n\""number\"",\nallow\\_flagging=\""manual\""\n)\niface.launch()\n```\nå½“æ‚¨ç‚¹å‡»ä¸Šé¢çš„æ ‡è®°æŒ‰é’®æ—¶ï¼Œå¯åŠ¨ç•Œé¢çš„ç›®å½•å°†åŒ…æ‹¬ä¸€ä¸ªæ–°çš„æ ‡è®°å­æ–‡ä»¶å¤¹ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ª CSV æ–‡ä»¶ã€‚è¯¥ CSV æ–‡ä»¶åŒ…æ‹¬æ‰€æœ‰è¢«æ ‡è®°çš„æ•°æ®ã€‚\n```directory\n+-- flagged/\n| +-- logs.csv\n```\n\\_flagged/logs.csv\\_\n```csv\nnum1,operation,num2,Output,timestamp\n5,add,7,12,2022-01-31 11:40:51.093412\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n```\nå¦‚æžœç•Œé¢æ¶‰åŠæ–‡ä»¶æ•°æ®ï¼Œä¾‹å¦‚å›¾åƒå’ŒéŸ³é¢‘ç»„ä»¶ï¼Œè¿˜å°†åˆ›å»ºæ–‡ä»¶å¤¹æ¥å­˜å‚¨è¿™äº›æ ‡è®°çš„æ•°æ®ã€‚ä¾‹å¦‚ï¼Œå°† `image` è¾“å…¥åˆ° `image` è¾“å‡ºç•Œé¢å°†åˆ›å»ºä»¥ä¸‹ç»“æž„ã€‚\n```directory\n+-- flagged/\n| +-- logs.csv\n| +-- image/\n| | +-- 0.png\n| | +-- 1.png\n| +-- Output/\n| | +-- 0.png\n| | +-- 1.png\n```\n\\_flagged/logs.csv\\_\n```csv\nim,Output timestamp\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\n```\nå¦‚æžœæ‚¨å¸Œæœ›ç”¨æˆ·ä¸ºæ ‡è®°æä¾›ä¸€ä¸ªåŽŸå› ï¼Œæ‚¨å¯ä»¥å°†å­—ç¬¦ä¸²åˆ—è¡¨ä¼ é€’ç»™ Interface çš„ `flagging\\_options` å‚æ•°ã€‚ç”¨æˆ·åœ¨æ ‡è®°æ—¶å¿…é¡»é€‰æ‹©å…¶ä¸­ä¸€é¡¹ï¼Œé€‰é¡¹å°†ä½œä¸ºé™„åŠ åˆ—ä¿å­˜åœ¨ CSV æ–‡ä»¶ä¸­ã€‚\nå¦‚æžœæˆ‘ä»¬å›žåˆ°è®¡ç®—å™¨ç¤ºä¾‹ï¼Œä¸‹é¢çš„ä»£ç å°†åˆ›å»ºåµŒå…¥å…¶ä¸­çš„ç•Œé¢ã€‚\n```python\niface = gr.Interface(\ncalculator,\n[\""number\"", gr.Radio([\""add\"", \""subtract\"", \""multiply\"", \""divide\""]), \""number\""],\n\""number\"",\nallow\\_flagging=\""manual\"",\nflagging\\_options=[\""wrong sign\"", \""off by one\"", \""other\""]\n)\niface.launch()\n```\nå½“ç”¨æˆ·ç‚¹å‡»æ ‡è®°æŒ‰é’®æ—¶ï¼ŒCSV æ–‡ä»¶çŽ°åœ¨å°†åŒ…æ‹¬æŒ‡ç¤ºæ‰€é€‰é€‰é¡¹çš„åˆ—ã€‚\n\\_flagged/logs.csv\\_\n```csv\nnum1,operation,num2,Output,flag,timestamp\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n```\n## HuggingFaceDatasetSaver å›žè°ƒ\næœ‰æ—¶ï¼Œå°†æ•°æ®ä¿å­˜åˆ°æœ¬åœ° CSV æ–‡ä»¶æ˜¯ä¸åˆç†çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨ Hugging Face Spaces ä¸Š\nï¼Œå¼€å‘è€…é€šå¸¸æ— æ³•è®¿é—®æ‰˜ç®¡ Gradio æ¼”ç¤ºçš„åº•å±‚ä¸´æ—¶æœºå™¨ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨ Hugging Face Space ä¸­å…³é—­æ ‡è®°çš„åŽŸå› ã€‚ç„¶è€Œï¼Œ\næ‚¨å¯èƒ½å¸Œæœ›å¯¹æ ‡è®°çš„æ•°æ®åšå…¶ä»–å¤„ç†ã€‚\nyou may want to do something else with the flagged data. é€šè¿‡ `flagging\\_callback` å‚æ•°ï¼Œæˆ‘ä»¬ä½¿è¿™å˜å¾—éžå¸¸ç®€å•ã€‚\nä¾‹å¦‚ï¼Œä¸‹é¢æˆ‘ä»¬å°†ä¼šå°†æ ‡è®°çš„æ•°æ®ä»Žæˆ‘ä»¬çš„è®¡ç®—å™¨ç¤ºä¾‹å¯¼å…¥åˆ° Hugging Face æ•°æ®é›†ä¸­ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥æž„å»ºä¸€ä¸ªâ€œä¼—åŒ…â€æ•°æ®é›†ï¼š\n```python\nimport os\nHF\\_TOKEN = os.getenv('HF\\_TOKEN')\nhf\\_writer = gr.HuggingFaceDatasetSaver(HF\\_TOKEN, \""crowdsourced-calculator-demo\"")\niface = gr.Interface(\ncalculator,\n[\""number\"", gr.Radio([\""add\"", \""subtract\"", \""multiply\"", \""divide\""]), \""number\""],\n\""number\"",\ndescription=\""Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\"",\nallow\\_flagging=\""manual\"",\nflagging\\_options=[\""wrong sign\"", \""off by one\"", \""other\""],\nflagging\\_callback=hf\\_writer\n)\niface.launch()\n```\næ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„ Hugging Face ä»¤ç‰Œå’Œ\nè¦ä¿å­˜æ ·æœ¬çš„æ•°æ®é›†çš„åç§°ï¼Œå®šä¹‰äº†æˆ‘ä»¬è‡ªå·±çš„\n`gradio.HuggingFaceDatasetSaver` çš„å®žä¾‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°† `allow\\_flagging=\""manual\""` è®¾ç½®ä¸ºäº†\nï¼Œå› ä¸ºåœ¨ Hugging Face Spaces ä¸­ï¼Œ`allow\\_flagging` é»˜è®¤è®¾ç½®ä¸º `\""never\""`ã€‚è¿™æ˜¯æˆ‘ä»¬çš„æ¼”ç¤ºï¼š\næ‚¨çŽ°åœ¨å¯ä»¥åœ¨è¿™ä¸ª[å…¬å…±çš„ Hugging Face æ•°æ®é›†](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)ä¸­çœ‹åˆ°ä¸Šé¢æ ‡è®°çš„æ‰€æœ‰ç¤ºä¾‹ã€‚\n![flagging callback hf](/assets/guides/flagging-callback-hf.png)\næˆ‘ä»¬åˆ›å»ºäº† `gradio.HuggingFaceDatasetSaver` ç±»ï¼Œä½†åªè¦å®ƒç»§æ‰¿è‡ª[æ­¤æ–‡ä»¶](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py)ä¸­å®šä¹‰çš„ `FlaggingCallback`ï¼Œæ‚¨å¯ä»¥ä¼ é€’è‡ªå·±çš„è‡ªå®šä¹‰ç±»ã€‚å¦‚æžœæ‚¨åˆ›å»ºäº†ä¸€ä¸ªå¾ˆæ£’çš„å›žè°ƒï¼Œè¯·å°†å…¶è´¡çŒ®ç»™è¯¥å­˜å‚¨åº“ï¼\n## ä½¿ç”¨ Blocks è¿›è¡Œæ ‡è®°\nå¦‚æžœæ‚¨æ­£åœ¨ä½¿ç”¨ `gradio.Blocks`ï¼Œåˆè¯¥æ€Žä¹ˆåŠžå‘¢ï¼Ÿä¸€æ–¹é¢ï¼Œä½¿ç”¨ Blocks æ‚¨æ‹¥æœ‰æ›´å¤šçš„çµæ´»æ€§\n--æ‚¨å¯ä»¥ç¼–å†™ä»»ä½•æ‚¨æƒ³åœ¨æŒ‰é’®è¢«ç‚¹å‡»æ—¶è¿è¡Œçš„ Python ä»£ç ï¼Œ\nå¹¶ä½¿ç”¨ Blocks ä¸­çš„å†…ç½®äº‹ä»¶åˆ†é…å®ƒã€‚\nåŒæ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ›ä½¿ç”¨çŽ°æœ‰çš„ `FlaggingCallback` æ¥é¿å…ç¼–å†™é¢å¤–çš„ä»£ç ã€‚\nè¿™éœ€è¦ä¸¤ä¸ªæ­¥éª¤ï¼š\n1. æ‚¨å¿…é¡»åœ¨ä»£ç ä¸­çš„æŸä¸ªä½ç½®è¿è¡Œæ‚¨çš„å›žè°ƒçš„ `.setup()` æ–¹æ³•\nåœ¨ç¬¬ä¸€æ¬¡æ ‡è®°æ•°æ®ä¹‹å‰\n2."", ""For example, on Hugging Face\nSpaces, developers typically don't have access to the underlying ephemeral machine hosting the Gradio\ndemo. That's why, by default, flagging is turned off in Hugging Face Space. However,\nyou may want to do something else with the flagged data. We've made this super easy with the `flagging\\_callback` parameter. For example, below we're going to pipe flagged data from our calculator example into a Hugging Face Dataset, e.g. so that we can build a \""crowd-sourced\"" dataset:\n```python\nimport os\nHF\\_TOKEN = os.getenv('HF\\_TOKEN')\nhf\\_writer = gr.HuggingFaceDatasetSaver(HF\\_TOKEN, \""crowdsourced-calculator-demo\"")\niface = gr.Interface(\ncalculator,\n[\""number\"", gr.Radio([\""add\"", \""subtract\"", \""multiply\"", \""divide\""]), \""number\""],\n\""number\"",\ndescription=\""Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\"",\nallow\\_flagging=\""manual\"",\nflagging\\_options=[\""wrong sign\"", \""off by one\"", \""other\""],\nflagging\\_callback=hf\\_writer\n)\niface.launch()\n```\nNotice that we define our own\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow\\_flagging=\""manual\""`\nbecause on Hugging Face Spaces, `allow\\_flagging` is set to `\""never\""` by default. Here's our demo:\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo). ![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)\nWe created the `gradio.HuggingFaceDatasetSaver` class, but you can pass your own custom class as long as it inherits from `FLaggingCallback` defined in [this file](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py). If you create a cool callback, contribute it to the repo! ## Flagging with Blocks\nWhat about if you are using `gradio.Blocks`? On one hand, you have even more flexibility\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\nand assign that using the built-in events in Blocks.""]","The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `""manual""`.","""manual"""
"Where can the full code for the Stable Diffusion demo be found?
","[""ote: This is a simplified version of the code needed to create the Stable Diffusion demo."", ""--\ntitle: ðŸ§¨ Stable Diffusion in JAX / Flax ! thumbnail: /blog/assets/108\\_stable\\_diffusion\\_jax/thumbnail.png\nauthors:\n- user: pcuenq\n- user: patrickvonplaten\n---\n# ðŸ§¨ Stable Diffusion in JAX / Flax ! [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_jax_how_to.ipynb)\n# \\*\\*Stable Diffusion in JAX / Flax\\*\\* ðŸš€\nðŸ¤— Hugging Face [Diffusers](https://github.com/huggingface/diffusers) supports Flax since version `0.5.1`! This allows for super fast inference on Google TPUs, such as those available in Colab, Kaggle or Google Cloud Platform. This post shows how to run inference using JAX / Flax. If you want more details about how Stable Diffusion works or want to run it in GPU, please refer to [this Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). If you want to follow along, click the button above to open this post as a Colab notebook. First, make sure you are using a TPU backend. If you are running this notebook in Colab, select `Runtime` in the menu above, then select the option \""Change runtime type\"" and then select `TPU` under the `Hardware accelerator` setting. Note that JAX is not exclusive to TPUs, but it shines on that hardware because each TPU server has 8 TPU accelerators working in parallel. ## Setup\n``` python\nimport jax\nnum\\_devices = jax.device\\_count()\ndevice\\_type = jax.devices()[0].device\\_kind\nprint(f\""Found {num\\_devices} JAX devices of type {device\\_type}.\"")\nassert \""TPU\"" in device\\_type, \""Available device is not a TPU, please select TPU from Edit > Notebook settings > Hardware accelerator\""\n```\n\\*Output\\*:\n```bash\nFound 8 JAX devices of type TPU v2."", ""Hence we call it ControlNet-XS. We provide the code for controlling StableDiffusion-XL [Podell et al., 2023] (Model B, 48M Parameters) and StableDiffusion 2.1 [Rombach et al. 2022] (Model B, 14M Parameters), all under openrail license.\\*\nThis model was contributed by [UmerHA](https://twitter.com/UmerHAdil). â¤ï¸\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## StableDiffusionControlNetXSPipeline\n[[autodoc]] StableDiffusionControlNetXSPipeline\n- all\n- \\_\\_call\\_\\_\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput"", ""--\ntitle: Using Stable Diffusion with Core ML on Apple Silicon\nthumbnail: /blog/assets/diffusers\\_coreml/thumbnail.png\nauthors:\n- user: pcuenq\n---\n# Using Stable Diffusion with Core ML on Apple Silicon\nThanks to Apple engineers, you can now run Stable Diffusion on Apple Silicon using Core ML! [This Apple repo](https://github.com/apple/ml-stable-diffusion) provides conversion scripts and inference code based on [ðŸ§¨ Diffusers](https://github.com/huggingface/diffusers), and we love it! To make it as easy as possible for you, we converted the weights ourselves and put the Core ML versions of the models in [the Hugging Face Hub](https://hf.co/apple). \\*\\*Update\\*\\*: some weeks after this post was written we created a native Swift app that you can use to run Stable Diffusion effortlessly on your own hardware. We released [an app in the Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) as well as [the source code to allow other projects to use it](https://github.com/huggingface/swift-coreml-diffusers). The rest of this post guides you on how to use the converted weights in your own code or convert additional weights yourself. ## Available Checkpoints\nThe official Stable Diffusion checkpoints are already converted and ready for use:\n- Stable Diffusion v1.4: [converted](https://hf.co/apple/coreml-stable-diffusion-v1-4) [original](https://hf.co/CompVis/stable-diffusion-v1-4)\n- Stable Diffusion v1.5: [converted](https://hf.co/apple/coreml-stable-diffusion-v1-5) [original](https://hf.co/runwayml/stable-diffusion-v1-5)\n- Stable Diffusion v2 base: [converted](https://hf.co/apple/coreml-stable-diffusion-2-base) [original](https://huggingface.co/stabilityai/stable-diffusion-2-base)\n- Stable Diffusion v2.1 base: [converted](https://hf.co/apple/coreml-stable-diffusion-2-1-base) [original](https://huggingface.co/stabilityai/stable-diffusion-2-1-base)\nCore ML supports all the compute units available in your device: CPU, GPU and Apple's Neural Engine (NE)."", ""Free solutions include many [spaces](https://huggingface.co/spaces) here on ðŸ¤— Hugging Face, such as the [Stable Diffusion 2.1 Demo](https://huggingface.co/spaces/stabilityai/stable-diffusion) or the [camemduru webui](https://huggingface.co/spaces/camenduru/webui). You can find a list of additional online services [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services). You can even use ðŸ¤— [Diffusers](https://huggingface.co/docs/diffusers/index) to write your own free solution! You can find a simple code example to get started [here](https://colab.research.google.com/drive/1HebngGyjKj7nLdXfj6Qi0N1nh7WvD74z?usp=sharing). \\*Note:\\* Parts of this series will use advanced features such as image2image, which may not be available on all online services. ### Generating Concept Art \nLet's generate some concept art. The steps are simple:\n1. Type what you want. 2. Click generate. ![Stable Diffusion Demo Space](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/sd-demo.png)\nBut, how do you get the results you actually want?"", ""They can be found through the following links:\n1. [Stable Diffusion v1.1](https://huggingface.co/CompVis/stable-diffusion-v1-1)\n2. [Stable Diffusion v1.2](https://huggingface.co/CompVis/stable-diffusion-v1-2)\n3. [Stable Diffusion v1.3](https://huggingface.co/CompVis/stable-diffusion-v1-3)\n4. [Stable Diffusion v1.4](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n```python\nfrom diffusers import DiffusionPipeline\nimport matplotlib.pyplot as plt\npipe = DiffusionPipeline.from\\_pretrained('CompVis/stable-diffusion-v1-4', custom\\_pipeline='suvadityamuk/StableDiffusionComparison')\npipe.enable\\_attention\\_slicing()\npipe = pipe.to('cuda')\nprompt = \""an astronaut riding a horse on mars\""\noutput = pipe(prompt)\nplt.subplots(2,2,1)\nplt.imshow(output.images[0])\nplt.title('Stable Diffusion v1.1')\nplt.axis('off')\nplt.subplots(2,2,2)\nplt.imshow(output.images[1])\nplt.title('Stable Diffusion v1.2')\nplt.axis('off')\nplt.subplots(2,2,3)\nplt.imshow(output.images[2])\nplt.title('Stable Diffusion v1.3')\nplt.axis('off')\nplt.subplots(2,2,4)\nplt.imshow(output.images[3])\nplt.title('Stable Diffusion v1.4')\nplt.axis('off')\nplt.show()\n```\nAs a result, you can look at a grid of all 4 generated images being shown together, that captures a difference the advancement of the training between the 4 checkpoints."", ""If you liked this topic and want to learn more, we recommend the following resources:\n- Our [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). - The [Getting Started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers\\_intro.ipynb) notebook, that gives a broader overview on Diffusion systems. - The [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) blog post. - Our [code in GitHub](https://github.com/huggingface/diffusers) where we'd be more than happy if you leave a â­ if `diffusers` is useful to you! ### Citation:\n```\n@article{patil2022stable,\nauthor = {Patil, Suraj and Cuenca, Pedro and Lambert, Nathan and von Platen, Patrick},\ntitle = {Stable Diffusion with ðŸ§¨ Diffusers},\njournal = {Hugging Face Blog},\nyear = {2022},\nnote = {[https://huggingface.co/blog/rlhf](https://huggingface.co/blog/stable\\_diffusion)},\n}\n```"", ""The whole framework involves a single forward of the diffusion model without any iterative optimization. We demonstrate that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity.\\*\nThe original codebase can be found at [Fantasy-Studio/Paint-by-Example](https://github.com/Fantasy-Studio/Paint-by-Example), and you can try it out in a [demo](https://huggingface.co/spaces/Fantasy-Studio/Paint-by-Example). ## Tips\nPaint by Example is supported by the official [Fantasy-Studio/Paint-by-Example](https://huggingface.co/Fantasy-Studio/Paint-by-Example) checkpoint. The checkpoint is warm-started from [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4) to inpaint partly masked images conditioned on example and reference images. Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## PaintByExamplePipeline\n[[autodoc]] PaintByExamplePipeline\n- all\n- \\_\\_call\\_\\_\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput"", ""Check out the Stable Diffusion demo for more: https://hf.co/spaces/stabilityai/stable-diffusion/"", ""Like in previous posts, I'm using an `r7iz.metal-16xl` instance (64 vCPU, 512GB RAM) with an Ubuntu 20.04 AMI (`ami-07cd3e6c4915b2d18`). Let's get started! Code samples are available on [Gitlab](https://gitlab.com/juliensimon/huggingface-demos/-/tree/main/optimum/stable\\_diffusion\\_intel). ## The Diffusers library\nThe [Diffusers](https://huggingface.co/docs/diffusers/index) library makes it extremely simple to generate images with Stable Diffusion models. If you're not familiar with these models, here's a great [illustrated introduction](https://jalammar.github.io/illustrated-stable-diffusion/). First, let's create a virtual environment with the required libraries: Transformers, Diffusers, Accelerate, and PyTorch. ```\nvirtualenv sd\\_inference\nsource sd\\_inference/bin/activate\npip install pip --upgrade\npip install transformers diffusers accelerate torch==1.13.1\n```\nThen, we write a simple benchmarking function that repeatedly runs inference, and returns the average latency for a single-image generation. ```python\nimport time\ndef elapsed\\_time(pipeline, prompt, nb\\_pass=10, num\\_inference\\_steps=20):\n# warmup\nimages = pipeline(prompt, num\\_inference\\_steps=10).images\nstart = time.time()\nfor \\_ in range(nb\\_pass):\n\\_ = pipeline(prompt, num\\_inference\\_steps=num\\_inference\\_steps, output\\_type=\""np\"")\nend = time.time()\nreturn (end - start) / nb\\_pass\n```\nNow, let's build a `StableDiffusionPipeline` with the default `float32` data type, and measure its inference latency. ```python\nfrom diffusers import StableDiffusionPipeline\nmodel\\_id = \""runwayml/stable-diffusion-v1-5\""\npipe = StableDiffusionPipeline.from\\_pretrained(model\\_id)\nprompt = \""sailing ship in storm by Rembrandt\""\nlatency = elapsed\\_time(pipe, prompt)\nprint(latency)\n```\nThe average latency is \\*\\*32.3 seconds\\*\\*."", ""See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main"", ""Code is available at https://github.com/CompVis/latent-diffusion.\\*\nMake sure to check out the Stable Diffusion [Tips](overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently! If you're interested in using one of the official checkpoints for a task, explore the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations! ## StableDiffusionPipeline\n[[autodoc]] StableDiffusionPipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_attention\\_slicing\n- disable\\_attention\\_slicing\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_xformers\\_memory\\_efficient\\_attention\n- disable\\_xformers\\_memory\\_efficient\\_attention\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n- load\\_textual\\_inversion\n- from\\_single\\_file\n- load\\_lora\\_weights\n- save\\_lora\\_weights\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput\n## FlaxStableDiffusionPipeline\n[[autodoc]] FlaxStableDiffusionPipeline\n- all\n- \\_\\_call\\_\\_\n## FlaxStableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.FlaxStableDiffusionPipelineOutput"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Load different Stable Diffusion formats\n[[open-in-colab]]\nStable Diffusion models are available in different formats depending on the framework they're trained and saved with, and where you download them from. Converting these formats for use in ðŸ¤— Diffusers allows you to use all the features supported by the library, such as [using different schedulers](schedulers) for inference, [building your custom pipeline](write\\_own\\_pipeline), and a variety of techniques and methods for [optimizing inference speed](../optimization/opt\\_overview)."", ""Gradio Demo: stable-diffusion\n### Note: This is a simplified version of the code needed to create the Stable Diffusion demo."", ""--\ntitle: Stable Diffusion with ðŸ§¨ Diffusers\nthumbnail: /blog/assets/98\\_stable\\_diffusion/thumbnail.png\nauthors:\n- user: valhalla\n- user: pcuenq\n- user: natolambert\n- user: patrickvonplaten\n---\n# Stable Diffusion with ðŸ§¨ Diffusers\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb)\n# \\*\\*Stable Diffusion\\*\\* ðŸŽ¨\n\\*...using ðŸ§¨ Diffusers\\*\nStable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). It is trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database. \\*LAION-5B\\* is the largest, freely accessible multi-modal dataset that currently exists. In this post, we want to show how to use Stable Diffusion with the [ðŸ§¨ Diffusers library](https://github.com/huggingface/diffusers), explain how the model works and finally dive a bit deeper into how `diffusers` allows\none to customize the image generation pipeline. \\*\\*Note\\*\\*: It is highly recommended to have a basic understanding of how diffusion models work. If diffusion\nmodels are completely new to you, we recommend reading one of the following blog posts:\n- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n- [Getting started with ðŸ§¨ Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers\\_intro.ipynb)\nNow, let's get started by generating some images ðŸŽ¨. ## Running Stable Diffusion\n### License\nBefore using the model, you need to accept the model [license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) in order to download and use the weights."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Stable Diffusion pipelines\nStable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). Latent diffusion applies the diffusion process over a lower dimensional latent space to reduce memory and compute complexity. This specific type of diffusion model was proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://huggingface.co/papers/2112.10752) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer. Stable Diffusion is trained on 512x512 images from a subset of the LAION-5B dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs. For more details about how Stable Diffusion works and how it differs from the base latent diffusion model, take a look at the Stability AI [announcement](https://stability.ai/blog/stable-diffusion-announcement) and our own [blog post](https://huggingface.co/blog/stable\\_diffusion#how-does-stable-diffusion-work) for more technical details. You can find the original codebase for Stable Diffusion v1.0 at [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion) and Stable Diffusion v2.0 at [Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion) as well as their original scripts for various tasks. Additional official checkpoints for the different Stable Diffusion versions and tasks can be found on the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations. Explore these organizations to find the best checkpoint for your use-case! The table below summarizes the available Stable Diffusion pipelines, their supported tasks, and an interactive demo:\n\n| Pipeline | Supported tasks | ðŸ¤— Space |\n| --- | --- | --- |\n| [StableDiffusion](./text2img) | text-to-image |  |\n| [StableDiffusionImg2Img](./img2img) | image-to-image |  |\n| [StableDiffusionInpaint](./inpaint) | inpainting |  |\n| [StableDiffusionDepth2Img](./depth2img) | depth-to-image |  |\n| [StableDiffusionImageVariation](./image_variation) | image variation |  |\n| [StableDiffusionPipelineSafe](./stable_diffusion_safe) | filtered text-to-image |  |\n| [StableDiffusion2](./stable_diffusion_2) | text-to-image, inpainting, depth-to-image, super-resolution |  |\n| [StableDiffusionXL](./stable_diffusion_xl) | text-to-image, image-to-image |  |\n| [StableDiffusionLatentUpscale](./latent_upscale) | super-resolution |  |\n| [StableDiffusionUpscale](./upscale) | super-resolution |\n| [StableDiffusionLDM3D](./ldm3d_diffusion) | text-to-rgb, text-to-depth, text-to-pano |  |\n| [StableDiffusionUpscaleLDM3D](./ldm3d_diffusion) | ldm3d super-resolution |\n\n## Tips\nTo help you get the most out of the Stable Diffusion pipelines, here are a few tips for improving performance and usability.""]",The full code for the Stable Diffusion demo can be found at this link: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main.,https://hf.co/spaces/stabilityai/stable-diffusion/tree/main
"What transformation does the FNet model use to replace the self-attention layer in a BERT model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# FNet\n## Overview\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:\n\\*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \""mix\"" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \""efficient\"" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.\\*\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f\\_net). ## Usage tips\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\nsequence length for fine-tuning and inference. ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## FNetConfig\n[[autodoc]] FNetConfig\n## FNetTokenizer\n[[autodoc]] FNetTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## FNetTokenizerFast\n[[autodoc]] FNetTokenizerFast\n## FNetModel\n[[autodoc]] FNetModel\n- forward\n## FNetForPreTraining\n[[autodoc]] FNetForPreTraining\n- forward\n## FNetForMaskedLM\n[[autodoc]] FNetForMaskedLM\n- forward\n## FNetForNextSentencePrediction\n[[autodoc]] FNetForNextSentencePrediction\n- forward\n## FNetForSequenceClassification\n[[autodoc]] FNetForSequenceClassification\n- forward\n## FNetForMultipleChoice\n[[autodoc]] FNetForMultipleChoice\n- forward\n## FNetForTokenClassification\n[[autodoc]] FNetForTokenClassification\n- forward\n## FNetForQuestionAnswering\n[[autodoc]] FNetForQuestionAnswering\n- forward"", ""The abstract from the paper is the following:\n\\*Pre-trained language models like BERT and its variants have recently achieved impressive performance in various\nnatural language understanding tasks. However, BERT heavily relies on the global self-attention block and thus suffers\nlarge memory footprint and computation cost. Although all its attention heads query on the whole input sequence for\ngenerating the attention map from a global perspective, we observe some heads only need to learn local dependencies,\nwhich means the existence of computation redundancy. We therefore propose a novel span-based dynamic convolution to\nreplace these self-attention heads to directly model local dependencies. The novel convolution heads, together with the\nrest self-attention heads, form a new mixed attention block that is more efficient at both global and local context\nlearning. We equip BERT with this mixed attention design and build a ConvBERT model. Experiments have shown that\nConvBERT significantly outperforms BERT and its variants in various downstream tasks, with lower training cost and\nfewer model parameters. Remarkably, ConvBERTbase model achieves 86.4 GLUE score, 0.7 higher than ELECTRAbase, while\nusing less than 1/4 training cost. Code and pre-trained models will be released.\\*\nThis model was contributed by [abhishek](https://huggingface.co/abhishek). The original implementation can be found\nhere: https://github.com/yitu-opensource/ConvBert\n## Usage tips\nConvBERT training tips are similar to those of BERT. For usage tips refer to [BERT documentation](bert). ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## ConvBertConfig\n[[autodoc]] ConvBertConfig\n## ConvBertTokenizer\n[[autodoc]] ConvBertTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## ConvBertTokenizerFast\n[[autodoc]] ConvBertTokenizerFast\n\n## ConvBertModel\n[[autodoc]] ConvBertModel\n- forward\n## ConvBertForMaskedLM\n[[autodoc]] ConvBertForMaskedLM\n- forward\n## ConvBertForSequenceClassification\n[[autodoc]] ConvBertForSequenceClassification\n- forward\n## ConvBertForMultipleChoice\n[[autodoc]] ConvBertForMultipleChoice\n- forward\n## ConvBertForTokenClassification\n[[autodoc]] ConvBertForTokenClassification\n- forward\n## ConvBertForQuestionAnswering\n[[autodoc]] ConvBertForQuestionAnswering\n- forward\n\n## TFConvBertModel\n[[autodoc]] TFConvBertModel\n- call\n## TFConvBertForMaskedLM\n[[autodoc]] TFConvBertForMaskedLM\n- call\n## TFConvBertForSequenceClassification\n[[autodoc]] TFConvBertForSequenceClassification\n- call\n## TFConvBertForMultipleChoice\n[[autodoc]] TFConvBertForMultipleChoice\n- call\n## TFConvBertForTokenClassification\n[[autodoc]] TFConvBertForTokenClassification\n- call\n## TFConvBertForQuestionAnswering\n[[autodoc]] TFConvBertForQuestionAnswering\n- call"", ""This is illustrated examplary for the second\nencoder block (red boxes at bottow) whose weight parameters\n\\\\(\\theta\\_{\\text{enc}}^{\\text{self-attn}, 2}\\\\) and\n\\\\(\\theta\\_{\\text{enc}}^{\\text{feed-forward}, 2}\\\\) are set to BERT\\'s weight\nparameters \\\\(\\theta\\_{\\text{BERT}}^{\\text{feed-forward}, 2}\\\\) and\n\\\\(\\theta\\_{\\text{BERT}}^{\\text{self-attn}, 2}\\\\), respectively at\ninitialization. Before fine-tuning, the encoder therefore behaves exactly like a\npre-trained BERT model. Assuming the input sequence\n\\\\(\\mathbf{x}\\_1, \\ldots, \\mathbf{x}\\_n\\\\) (shown in green) passed to the\nencoder is equal to the input sequence\n\\\\(\\mathbf{x}\\_1^{\\text{BERT}}, \\ldots, \\mathbf{x}\\_n^{\\text{BERT}}\\\\) (shown\nin grey) passed to BERT, this means that the respective output vector\nsequences \\\\(\\mathbf{\\overline{x}}\\_1, \\ldots, \\mathbf{\\overline{x}}\\_n\\\\)\n(shown in darker green) and\n\\\\(\\mathbf{\\overline{x}}\\_1^{\\text{BERT}}, \\ldots, \\mathbf{\\overline{x}}\\_n^{\\text{BERT}}\\\\)\n(shown in darker grey) also have to be equal. Next, let\\'s illustrate how the decoder is warm-started. ![texte du\nlien](https://raw.githubusercontent.com/patrickvonplaten/scientific\\_images/master/encoder\\_decoder/leverage\\_decoder.png)\nThe architecture of the decoder is different from BERT\\'s architecture\nin three ways. 1. First, the decoder has to be conditioned on the contextualized\nencoded sequence \\\\(\\mathbf{\\overline{X}}\\_{1:n}\\\\) by means of\ncross-attention layers. Consequently, randomly initialized\ncross-attention layers are added between the self-attention layer\nand the two feed-forward layers in each BERT block. This is\nrepresented exemplary for the second block by\n\\\\(+\\theta\\_{\\text{dec}}^{\\text{cross-attention, 2}}\\\\) and illustrated\nby the newly added fully connected graph in red in the lower red box\non the right. This necessarily changes the behavior of each modified\nBERT block so that an input vector, \\*e.g.\\* \\\\(\\mathbf{y'}\\_0\\\\) now\nyields a random output vector \\\\(\\mathbf{y''}\\_0\\\\) (highlighted by the\nred border around the output vector \\\\(\\mathbf{y''}\\_0\\\\)). 2. Second, BERT\\'s \\*bi-directional\\* self-attention layers have to be\nchanged to \\*uni-directional\\* self-attention layers to comply with\nauto-regressive generation. Because both the bi-directional and the\nuni-directional self-attention layer are based on the same \\*key\\*,\n\\*query\\* and \\*value\\* projection weights, the decoder\\'s\nself-attention layer weights can be initialized with BERT\\'s\nself-attention layer weights. \\*E.g.\\* the query, key and value weight\nparameters of the decoder\\'s uni-directional self-attention layer\nare initialized with those of BERT\\'s bi-directional self-attention\nlayer \\\\(\\theta\\_{\\text{BERT}}^{\\text{self-attn}, 2} = \\{\\mathbf{W}\\_{\\text{BERT}, k}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{BERT}, v}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{BERT}, q}^{\\text{self-attn}, 2} \\} \\to \\theta\\_{\\text{dec}}^{\\text{self-attn}, 2} = \\{\\mathbf{W}\\_{\\text{dec}, k}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{dec}, v}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{dec}, q}^{\\text{self-attn}, 2} \\}. \\\\) However, in \\*uni-directional\\* self-attention each token only\nattends to all previous tokens, so that the decoder\\'s\nself-attention layers yield different output vectors than BERT\\'s\nself-attention layers even though they share the same weights."", ""Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\n1. \\*\\*[FlauBERT](https://huggingface.co/docs/transformers/model\\_doc/flaubert)\\*\\* (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, LoÃ¯c Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, BenoÃ®t CrabbÃ©, Laurent Besacier, Didier Schwab. 1. \\*\\*[FLAVA](https://huggingface.co/docs/transformers/model\\_doc/flava)\\*\\* (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 1. \\*\\*[FNet](https://huggingface.co/docs/transformers/model\\_doc/fnet)\\*\\* (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. 1. \\*\\*[FocalNet](https://huggingface.co/docs/transformers/model\\_doc/focalnet)\\*\\* (from Microsoft Research) released with the paper [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. 1. \\*\\*[Funnel Transformer](https://huggingface.co/docs/transformers/model\\_doc/funnel)\\*\\* (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V."", ""Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\n1. \\*\\*[FlauBERT](https://huggingface.co/docs/transformers/model\\_doc/flaubert)\\*\\* (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, LoÃ¯c Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, BenoÃ®t CrabbÃ©, Laurent Besacier, Didier Schwab. 1. \\*\\*[FLAVA](https://huggingface.co/docs/transformers/model\\_doc/flava)\\*\\* (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 1. \\*\\*[FNet](https://huggingface.co/docs/transformers/model\\_doc/fnet)\\*\\* (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. 1. \\*\\*[FocalNet](https://huggingface.co/docs/transformers/model\\_doc/focalnet)\\*\\* (from Microsoft Research) released with the paper [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. 1. \\*\\*[Funnel Transformer](https://huggingface.co/docs/transformers/model\\_doc/funnel)\\*\\* (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V."", ""Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\n1. \\*\\*[FlauBERT](https://huggingface.co/docs/transformers/model\\_doc/flaubert)\\*\\* (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, LoÃ¯c Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, BenoÃ®t CrabbÃ©, Laurent Besacier, Didier Schwab. 1. \\*\\*[FLAVA](https://huggingface.co/docs/transformers/model\\_doc/flava)\\*\\* (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 1. \\*\\*[FNet](https://huggingface.co/docs/transformers/model\\_doc/fnet)\\*\\* (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. 1. \\*\\*[FocalNet](https://huggingface.co/docs/transformers/model\\_doc/focalnet)\\*\\* (from Microsoft Research) released with the paper [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. 1. \\*\\*[Funnel Transformer](https://huggingface.co/docs/transformers/model\\_doc/funnel)\\*\\* (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V."", ""Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\n1. \\*\\*[FlauBERT](https://huggingface.co/docs/transformers/model\\_doc/flaubert)\\*\\* (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, LoÃ¯c Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, BenoÃ®t CrabbÃ©, Laurent Besacier, Didier Schwab. 1. \\*\\*[FLAVA](https://huggingface.co/docs/transformers/model\\_doc/flava)\\*\\* (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 1. \\*\\*[FNet](https://huggingface.co/docs/transformers/model\\_doc/fnet)\\*\\* (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. 1. \\*\\*[FocalNet](https://huggingface.co/docs/transformers/model\\_doc/focalnet)\\*\\* (from Microsoft Research) released with the paper [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. 1. \\*\\*[Funnel Transformer](https://huggingface.co/docs/transformers/model\\_doc/funnel)\\*\\* (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V."", ""Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\n1. \\*\\*[FlauBERT](https://huggingface.co/docs/transformers/model\\_doc/flaubert)\\*\\* (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, LoÃ¯c Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, BenoÃ®t CrabbÃ©, Laurent Besacier, Didier Schwab. 1. \\*\\*[FLAVA](https://huggingface.co/docs/transformers/model\\_doc/flava)\\*\\* (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 1. \\*\\*[FNet](https://huggingface.co/docs/transformers/model\\_doc/fnet)\\*\\* (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. 1. \\*\\*[FocalNet](https://huggingface.co/docs/transformers/model\\_doc/focalnet)\\*\\* (from Microsoft Research) released with the paper [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. 1. \\*\\*[Funnel Transformer](https://huggingface.co/docs/transformers/model\\_doc/funnel)\\*\\* (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V."", ""Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\n1. \\*\\*[FlauBERT](https://huggingface.co/docs/transformers/model\\_doc/flaubert)\\*\\* (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, LoÃ¯c Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, BenoÃ®t CrabbÃ©, Laurent Besacier, Didier Schwab. 1. \\*\\*[FLAVA](https://huggingface.co/docs/transformers/model\\_doc/flava)\\*\\* (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 1. \\*\\*[FNet](https://huggingface.co/docs/transformers/model\\_doc/fnet)\\*\\* (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. 1. \\*\\*[FocalNet](https://huggingface.co/docs/transformers/model\\_doc/focalnet)\\*\\* (from Microsoft Research) released with the paper [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. 1. \\*\\*[Funnel Transformer](https://huggingface.co/docs/transformers/model\\_doc/funnel)\\*\\* (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V."", ""Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\n1. \\*\\*[FlauBERT](https://huggingface.co/docs/transformers/model\\_doc/flaubert)\\*\\* (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, LoÃ¯c Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, BenoÃ®t CrabbÃ©, Laurent Besacier, Didier Schwab. 1. \\*\\*[FLAVA](https://huggingface.co/docs/transformers/model\\_doc/flava)\\*\\* (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 1. \\*\\*[FNet](https://huggingface.co/docs/transformers/model\\_doc/fnet)\\*\\* (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. 1. \\*\\*[FocalNet](https://huggingface.co/docs/transformers/model\\_doc/focalnet)\\*\\* (from Microsoft Research) released with the paper [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao. 1. \\*\\*[Funnel Transformer](https://huggingface.co/docs/transformers/model\\_doc/funnel)\\*\\* (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V.""]",The FNet model replaces the self-attention layer in a BERT model with a Fourier transform that returns only the real parts of the transform.,Fourier transform
"What type of test should typically accompany a bug fix in Gradio's testing strategy?
","[""Test Strategy\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to. This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Objectives\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n1. Ensure that the Gradio library functions as we expect it to. 2. Enable the maintenance team to quickly identify both the presence and source of defects. 3. Prevent regressions, i.e. if we fix something it should stay fixed. 4. Improve the quality of the codebase in order to ease maintenance efforts. 5. Reduce the amount of manual testing required. ## Scope\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas. We should not focus on code coverage but on test coverage following the below criteria:\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n## Types of testing\nOur tests will broadly fall into one of three categories:\n- Static Quality checks\n- Dynamic 'Code' tests\n- Dynamic Functional tests\n### Static Quality checks\nStatic quality checks are generally very fast to run and do not require building the code base."", ""gradio\n## 4.11.0\n### Features\n- [#6842](https://github.com/gradio-app/gradio/pull/6842) [`846d52d`](https://github.com/gradio-app/gradio/commit/846d52d1c92d429077382ce494eea27fd062d9f6) - Fix md highlight. Thanks [@pngwn](https://github.com/pngwn)! - [#6831](https://github.com/gradio-app/gradio/pull/6831) [`f3abde8`](https://github.com/gradio-app/gradio/commit/f3abde80884d96ad69b825020c46486d9dd5cac5) - Add an option to enable header links for markdown. Thanks [@pngwn](https://github.com/pngwn)! - [#6814](https://github.com/gradio-app/gradio/pull/6814) [`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d) - Refactor queue so that there are separate queues for each concurrency id. Thanks [@aliabid94](https://github.com/aliabid94)! - [#6809](https://github.com/gradio-app/gradio/pull/6809) [`1401d99`](https://github.com/gradio-app/gradio/commit/1401d99ade46d87da75b5f5808a3354c49f1d1ea) - Fix `ImageEditor` interaction story. Thanks [@hannahblair](https://github.com/hannahblair)! - [#6803](https://github.com/gradio-app/gradio/pull/6803) [`77c9003`](https://github.com/gradio-app/gradio/commit/77c900311e2ba37b8f849ce088ceb48aa196af18) - Fixes issue 5781: Enables specifying a caching directory for Examples. Thanks [@cswamy](https://github.com/cswamy)! - [#6823](https://github.com/gradio-app/gradio/pull/6823) [`67a2b7f`](https://github.com/gradio-app/gradio/commit/67a2b7f12cb06355fcc41e40d47e8b2ad211d7d1) - Fixed duplicate word (\""this this\""). Thanks [@Cassini-chris](https://github.com/Cassini-chris)!"", ""Gradio Demo: queue\\_full\\_e2e\\_test\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nimport time\nimport random\nn\\_calls = 0\ndef get\\_random\\_number():\nglobal n\\_calls\nif n\\_calls == 1:\nn\\_calls += 1\nraise gr.Error(\""This is a gradio error\"")\nn\\_calls += 1\ntime.sleep(5)\nreturn random.randrange(1, 10)\nwith gr.Blocks() as demo:\nwith gr.Row():\nwith gr.Column():\nfirst = gr.Button(\""First Call\"")\nsecond = gr.Button(\""Second Call\"")\nthird = gr.Button(\""Third Call\"")\nfourth = gr.Button(\""Fourth Call\"")\nwith gr.Column():\nfirst\\_o = gr.Number(label=\""First Result\"")\nsecond\\_o = gr.Number(label=\""Second Result\"")\nthird\\_o = gr.Number(label=\""Third Result\"")\nfourth\\_o = gr.Number(label=\""Fourth Result\"")\nfirst.click(get\\_random\\_number, None, first\\_o, concurrency\\_id=\""f\"")\nsecond.click(get\\_random\\_number, None, second\\_o, concurrency\\_id=\""f\"")\nthird.click(get\\_random\\_number, None, third\\_o, concurrency\\_id=\""f\"")\nfourth.click(get\\_random\\_number, None, fourth\\_o, concurrency\\_id=\""f\"")\ndemo.queue(max\\_size=2)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""### Testing and Infrastructure Changes:\nNo changes to highlight. ### Breaking Changes:\nNo changes to highlight. ### Full Changelog:\nNo changes to highlight. ### Contributors Shoutout:\nNo changes to highlight. # 0.2.3\n### New Features:\nNo changes to highlight. ### Bug Fixes:\n- Fix example inputs for `gr.File(file\\_count='multiple')` output components by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4153](https://github.com/gradio-app/gradio/pull/4153)\n### Documentation Changes:\nNo changes to highlight. ### Testing and Infrastructure Changes:\nNo changes to highlight. ### Breaking Changes:\nNo changes to highlight. ### Full Changelog:\nNo changes to highlight. ### Contributors Shoutout:\nNo changes to highlight. # 0.2.2\n### New Features:\nNo changes to highlight. ### Bug Fixes:\n- Only send request to `/info` route if demo version is above `3.28.3` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4109](https://github.com/gradio-app/gradio/pull/4109)\n### Other Changes:\n- Fix bug in test from gradio 3.29.0 refactor by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4138](https://github.com/gradio-app/gradio/pull/4138)\n### Breaking Changes:\nNo changes to highlight. # 0.2.1\n### New Features:\nNo changes to highlight. ### Bug Fixes:\nRemoves extraneous `State` component info from the `Client.view\\_api()` method by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)\n### Documentation Changes:\nNo changes to highlight. ### Testing and Infrastructure Changes:\nSeparates flaky tests from non-flaky tests by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)\n### Breaking Changes:\nNo changes to highlight."", ""- [#5682](https://github.com/gradio-app/gradio/pull/5682) [`c57f1b75e`](https://github.com/gradio-app/gradio/commit/c57f1b75e272c76b0af4d6bd0c7f44743ff34f26) - Fix functional tests. Thanks [@abidlabs](https://github.com/abidlabs)! - [#5681](https://github.com/gradio-app/gradio/pull/5681) [`40de3d217`](https://github.com/gradio-app/gradio/commit/40de3d2178b61ebe424b6f6228f94c0c6f679bea) - add query parameters to the `gr.Request` object through the `query\\_params` attribute. Thanks [@DarhkVoyd](https://github.com/DarhkVoyd)! - [#5653](https://github.com/gradio-app/gradio/pull/5653) [`ea0e00b20`](https://github.com/gradio-app/gradio/commit/ea0e00b207b4b90a10e9d054c4202d4e705a29ba) - Prevent Clients from accessing API endpoints that set `api\\_name=False`. Thanks [@abidlabs](https://github.com/abidlabs)! - [#5639](https://github.com/gradio-app/gradio/pull/5639) [`e1874aff8`](https://github.com/gradio-app/gradio/commit/e1874aff814d13b23f3e59ef239cc13e18ad3fa7) - Add `gr.on` listener method. Thanks [@aliabid94](https://github.com/aliabid94)! - [#5652](https://github.com/gradio-app/gradio/pull/5652) [`2e25d4305`](https://github.com/gradio-app/gradio/commit/2e25d430582264945ae3316acd04c4453a25ce38) - Pause autoscrolling if a user scrolls up in a `gr.Textbox` and resume autoscrolling if they go all the way down."", ""gradio\\_test\n## 0.3.3\n### Patch Changes\n- Updated dependencies [[`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d), [`73268ee`](https://github.com/gradio-app/gradio/commit/73268ee2e39f23ebdd1e927cb49b8d79c4b9a144)]:\n- @gradio/statustracker@0.4.3\n- @gradio/atoms@0.4.1\n## 0.3.2\n### Patch Changes\n- Updated dependencies [[`4d1cbbc`](https://github.com/gradio-app/gradio/commit/4d1cbbcf30833ef1de2d2d2710c7492a379a9a00)]:\n- @gradio/atoms@0.4.0\n- @gradio/statustracker@0.4.2\n## 0.3.1\n### Patch Changes\n- Updated dependencies []:\n- @gradio/atoms@0.3.1\n- @gradio/statustracker@0.4.1\n## 0.3.0\n### Features\n- [#6532](https://github.com/gradio-app/gradio/pull/6532) [`96290d304`](https://github.com/gradio-app/gradio/commit/96290d304a61064b52c10a54b2feeb09ca007542) - tweak deps. Thanks [@pngwn](https://github.com/pngwn)! ## 0.2.3\n### Patch Changes\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8)]:\n- @gradio/atoms@0.3.0\n- @gradio/statustracker@0.4.0\n## 0.2.2\n### Patch Changes\n- Updated dependencies [[`f816136a0`](https://github.com/gradio-app/gradio/commit/f816136a039fa6011be9c4fb14f573e4050a681a)]:\n- @gradio/atoms@0.2.2\n- @gradio/statustracker@0.3.2\n## 0.2.1\n### Patch Changes\n- Updated dependencies [[`3cdeabc68`](https://github.com/gradio-app/gradio/commit/3cdeabc6843000310e1a9e1d17190ecbf3bbc780), [`fad92c29d`](https://github.com/gradio-app/gradio/commit/fad92c29dc1f5cd84341aae417c495b33e01245f)]:\n- @gradio/atoms@0.2.1\n- @gradio/statustracker@0.3.1\n## 0.2.0\n### Features\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - fix cc build."", ""### Bug Fixes:\n- Updated components with `info` attribute to update when `update()` is called on them. by [@jebarpg](https://github.com/jebarpg) in [PR 4715](https://github.com/gradio-app/gradio/pull/4715). - Ensure the `Image` components undo button works mode is `mask` or `color-sketch` by [@amyorz](https://github.com/AmyOrz) in [PR 4692](https://github.com/gradio-app/gradio/pull/4692)\n- Load the iframe resizer external asset asynchronously, by [@akx](https://github.com/akx) in [PR 4336](https://github.com/gradio-app/gradio/pull/4336)\n- Restored missing imports in `gr.components` by [@abidlabs](https://github.com/abidlabs) in [PR 4566](https://github.com/gradio-app/gradio/pull/4566)\n- Fix bug where `select` event was not triggered in `gr.Gallery` if `height` was set to be large with `allow_preview=False` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4551](https://github.com/gradio-app/gradio/pull/4551)\n- Fix bug where setting `visible=False` in `gr.Group` event did not work by [@abidlabs](https://github.com/abidlabs) in [PR 4567](https://github.com/gradio-app/gradio/pull/4567)\n- Fix `make_waveform` to work with paths that contain spaces [@akx](https://github.com/akx) in [PR 4570](https://github.com/gradio-app/gradio/pull/4570) & [PR 4578](https://github.com/gradio-app/gradio/pull/4578)\n- Send captured data in `stop_recording` event for `gr.Audio` and `gr.Video` components by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4554](https://github.com/gradio-app/gradio/pull/4554)\n- Fix bug in `gr.Gallery` where `height` and `object_fit` parameters where being ignored by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4576](https://github.com/gradio-app/gradio/pull/4576)\n- Fixes an HTML sanitization issue in DOMPurify where links in markdown were not opening in a new window by [@hannahblair] in [PR 4577](https://github.com/gradio-app/gradio/pull/4577)\n- Fixed Dropdown height rendering in Columns by [@aliabid94](https://github.com/aliabid94) in [PR 4584](https://github.com/gradio-app/gradio/pull/4584)\n- Fixed bug where `AnnotatedImage` css styling was causing the annotation masks to not be displayed correctly by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4628](https://github.com/gradio-app/gradio/pull/4628)\n- Ensure that Gradio does not silently fail when running on a port that is occupied by [@abidlabs](https://github.com/abidlabs) in [PR 4624](https://github.com/gradio-app/gradio/pull/4624). - Fix double upload bug that caused lag in file uploads by [@aliabid94](https://github.com/aliabid94) in [PR 4661](https://github.com/gradio-app/gradio/pull/4661)\n- `Progress` component now appears even when no `iterable` is specified in `tqdm` constructor by [@itrushkin](https://github.com/itrushkin) in [PR 4475](https://github.com/gradio-app/gradio/pull/4475)\n- Deprecation warnings now point at the user code using those deprecated features, instead of Gradio internals, by (https://github.com/akx) in [PR 4694](https://github.com/gradio-app/gradio/pull/4694)\n- Adapt column widths in gr.Examples based on content by [@pngwn](https://github.com/pngwn) & [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 4700](https://github.com/gradio-app/gradio/pull/4700)\n- The `plot` parameter deprecation warnings should now only be emitted for `Image` components by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4709](https://github.com/gradio-app/gradio/pull/4709)\n- Removed uncessessary `type` deprecation warning by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4709](https://github.com/gradio-app/gradio/pull/4709)\n- Ensure Audio autoplays works when `autoplay=True` and the video source is dynamically updated [@pngwn](https://github.com/pngwn) in [PR 4705](https://github.com/gradio-app/gradio/pull/4705)\n- When an error modal is shown in spaces, ensure we scroll to the top so it can be seen by [@pngwn](https://github.com/pngwn) in [PR 4712](https://github.com/gradio-app/gradio/pull/4712)\n- Update depedencies by [@pngwn](https://github.com/pngwn) in [PR 4675](https://github.com/gradio-app/gradio/pull/4675)\n- Fixes `gr.Dropdown` being cutoff at the bottom by [@abidlabs](https://github.com/abidlabs) in [PR 4691](https://github.com/gradio-app/gradio/pull/4691). - Scroll top when clicking \""View API\"" in spaces by [@pngwn](https://github.com/pngwn) in [PR 4714](https://github.com/gradio-app/gradio/pull/4714)\n- Fix bug where `show_label` was hiding the entire component for `gr.Label` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4713](https://github.com/gradio-app/gradio/pull/4713)\n- Don't crash when uploaded image has broken EXIF data, by [@akx](https://github.com/akx) in [PR 4764](https://github.com/gradio-app/gradio/pull/4764)\n- Place toast messages at the top of the screen by [@pngwn](https://github.com/pngwn) in [PR 4796](https://github.com/gradio-app/gradio/pull/4796)\n- Fix regressed styling of Login page when auth is enabled by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4797](https://github.com/gradio-app/gradio/pull/4797)\n- Prevent broken scrolling to output on Spaces by [@aliabid94](https://github.com/aliabid94) in [PR 4822](https://github.com/gradio-app/gradio/pull/4822)\n### Other Changes:\n- Add `.git-blame-ignore-revs` by [@akx](https://github.com/akx) in [PR 4586](https://github.com/gradio-app/gradio/pull/4586)\n- Update frontend dependencies in [PR 4601](https://github.com/gradio-app/gradio/pull/4601)\n- Use `typing.Literal` where possible in gradio library and client by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4608](https://github.com/gradio-app/gradio/pull/4608)\n- Remove unnecessary mock json files for frontend E2E tests by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 4625](https://github.com/gradio-app/gradio/pull/4625)\n- Update dependencies by [@pngwn](https://github.com/pngwn) in [PR 4643](https://github.com/gradio-app/gradio/pull/4643)\n- The theme builder now launches successfully, and the API docs are cleaned up."", ""Tests in this category could be browser-based end-to-end tests, accessibility tests, or performance tests. They are sometimes called acceptance tests. ## Testing tools\nWe currently use the following tools:\n### Static quality checks\n- Python type-checking (python)\n- Black linting (python)\n- ruff formatting (python)\n- prettier formatting (javascript/svelte)\n- TypeScript type-checking (javascript/svelte)\n- eslint linting (javascript/svelte) [in progress]\n### Dynamic code tests\n- pytest (python unit and integration tests)\n- vitest (node-based unit and integration tests)\n- playwright (browser-based unit and integration tests)\n### Functional/acceptance tests\n- playwright (full end to end testing)\n- chromatic (visual testing) [in progress]\n- Accessibility testing [to do]\n## Supported environments and versions\nAll operating systems refer to the current runner variants supported by GitHub actions. All unspecified version segments (`x`) refer to latest. | Software | Version(s) | Operating System(s) |\n| -------- | --------------------- | --------------------------------- |\n| Python | `3.8.x` | `ubuntu-latest`, `windows-latest` |\n| Node | `18.x.x` | `ubuntu-latest` |\n| Browser | `playwright-chrome-x` | `ubuntu-latest` |\n## Test execution\nTests need to be executed in a number of environments and at different stages of the development cycle in order to be useful. The requirements for tests are as follows:\n- \\*\\*Locally\\*\\*: it is important that developers can easily run most tests locally to ensure a passing suite before making a PR. There are some exceptions to this, certain tests may require access to secret values which we cannot make available to all possible contributors for practical security reasons. It is reasonable that it isn't possible to run these tests but they should be disabled by default when running locally. - \\*\\*CI\\*\\* - It is \\_critical\\_ that all tests run successfully in CI with no exceptions. Not every test is required to pass to satisfy CI checks for practical reasons but it is required that all tests should run in CI and notify us if something unexpected happens in order for the development team to take appropriate action. For instructions on how to write and run tests see the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Managing defects\nAs we formalise our testing strategy and bring / keep our test up to standard, it is important that we have some principles on managing defects as they occur/ are reported. For now we can have one very simple rule:\n- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should \\_typically\\_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule."", ""Thanks [@abidlabs](https://github.com/abidlabs)! - [#6079](https://github.com/gradio-app/gradio/pull/6079) [`3b2d9eaa3`](https://github.com/gradio-app/gradio/commit/3b2d9eaa3e84de3e4a0799e4585a94510d665f26) - fix cc build. Thanks [@pngwn](https://github.com/pngwn)! ### Fixes\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Pending events behavior. Thanks [@pngwn](https://github.com/pngwn)! - [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Reinstate types that were removed in error in #5832. Thanks [@pngwn](https://github.com/pngwn)! - [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Fixes: slider bar are too thin on FireFox. Thanks [@pngwn](https://github.com/pngwn)! - [#6146](https://github.com/gradio-app/gradio/pull/6146) [`40a171ea6`](https://github.com/gradio-app/gradio/commit/40a171ea60c74afa9519d6cb159def16ce68e1ca) - Fix image double change bug. Thanks [@pngwn](https://github.com/pngwn)! - [#6148](https://github.com/gradio-app/gradio/pull/6148) [`0000a1916`](https://github.com/gradio-app/gradio/commit/0000a191688c5480c977c80acdd0c9023865d57e) - fix dropdown arrow size. Thanks [@pngwn](https://github.com/pngwn)! - [#6067](https://github.com/gradio-app/gradio/pull/6067) [`bf38e5f06`](https://github.com/gradio-app/gradio/commit/bf38e5f06a7039be913614901c308794fea83ae0) - remove dupe component. Thanks [@pngwn](https://github.com/pngwn)! - [#6065](https://github.com/gradio-app/gradio/pull/6065) [`7d07001e8`](https://github.com/gradio-app/gradio/commit/7d07001e8e7ca9cbd2251632667b3a043de49f49) - fix storybook. Thanks [@pngwn](https://github.com/pngwn)! - [#5826](https://github.com/gradio-app/gradio/pull/5826) [`ce036c5d4`](https://github.com/gradio-app/gradio/commit/ce036c5d47e741e29812654bcc641ea6be876504) - Pending events behavior. Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)! - [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)! - [#6042](https://github.com/gradio-app/gradio/pull/6042) [`e27997fe6`](https://github.com/gradio-app/gradio/commit/e27997fe6c2bcfebc7015fc26100cee9625eb13a) - Fix `root` when user is unauthenticated so that login page appears correctly. Thanks [@abidlabs](https://github.com/abidlabs)! - [#6076](https://github.com/gradio-app/gradio/pull/6076) [`f3f98f923`](https://github.com/gradio-app/gradio/commit/f3f98f923c9db506284b8440e18a3ac7ddd8398b) - Lite error handler. Thanks [@whitphx](https://github.com/whitphx)! - [#5984](https://github.com/gradio-app/gradio/pull/5984) [`66549d8d2`](https://github.com/gradio-app/gradio/commit/66549d8d256b1845c8c5efa0384695b36cb46eab) - Fixes: slider bar are too thin on FireFox. Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)! ## 3.45.0-beta.13\n### Features\n- [#5964](https://github.com/gradio-app/gradio/pull/5964) [`5fbda0bd2`](https://github.com/gradio-app/gradio/commit/5fbda0bd2b2bbb2282249b8875d54acf87cd7e84) - Wasm release. Thanks [@pngwn](https://github.com/pngwn)! ## 3.45.0-beta.12\n### Features\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-app/gradio/commit/85ba6de136a45b3e92c74e410bb27e3cbe7138d7) - V4: Some misc fixes."", ""6. Fix audio streaming, which was previously choppy in [PR 2351](https://github.com/gradio-app/gradio/pull/2351). Big thanks to [@yannickfunk](https://github.com/yannickfunk) for the proposed solution. 7. Fix bug where new typeable slider doesn't respect the minimum and maximum values [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 2380](https://github.com/gradio-app/gradio/pull/2380)\n### Documentation Changes:\n1. New Guide: Connecting to a Database ðŸ—„ï¸\nA new guide by [@freddyaboulton](https://github.com/freddyaboulton) that explains how you can use Gradio to connect your app to a database. Read more [here](https://gradio.app/connecting_to_a_database/). 2. New Guide: Running Background Tasks ðŸ¥·\nA new guide by [@freddyaboulton](https://github.com/freddyaboulton) that explains how you can run background tasks from your gradio app. Read more [here](https://gradio.app/running_background_tasks/). 3. Small fixes to docs for `Image` component by [@abidlabs](https://github.com/abidlabs) in [PR 2372](https://github.com/gradio-app/gradio/pull/2372)\n### Testing and Infrastructure Changes:\nNo changes to highlight. ### Breaking Changes:\nNo changes to highlight. ### Full Changelog:\n- Create a guide on how to connect an app to a database hosted on the cloud by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2341](https://github.com/gradio-app/gradio/pull/2341)\n- Removes `analytics` dependency by [@abidlabs](https://github.com/abidlabs) in [PR 2347](https://github.com/gradio-app/gradio/pull/2347)\n- Add guide on launching background tasks from your app by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2350](https://github.com/gradio-app/gradio/pull/2350)\n- Fix typo in guide image path by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2357](https://github.com/gradio-app/gradio/pull/2357)\n- Raise error if Blocks has duplicate component with same IDs by [@abidlabs](https://github.com/abidlabs) in [PR 2359](https://github.com/gradio-app/gradio/pull/2359)\n- Hotfix: fix version back to 3.4 by [@abidlabs](https://github.com/abidlabs) in [PR 2361](https://github.com/gradio-app/gradio/pull/2361)\n- Change version.txt to 3.4 instead of 3.4.0 by [@aliabd](https://github.com/aliabd) in [PR 2363](https://github.com/gradio-app/gradio/pull/2363)\n- Catch the permission exception on the audio component by [@Ian-GL](https://github.com/Ian-GL) in [PR 2330](https://github.com/gradio-app/gradio/pull/2330)\n- Fix image_classifier_interface_load demo by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2365](https://github.com/gradio-app/gradio/pull/2365)\n- Small fixes to docs for `Image` component by [@abidlabs](https://github.com/abidlabs) in [PR 2372](https://github.com/gradio-app/gradio/pull/2372)\n- Automated Release Notes by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2306](https://github.com/gradio-app/gradio/pull/2306)\n- Fixed small typos in the docs [@julien-c](https://github.com/julien-c) in [PR 2373](https://github.com/gradio-app/gradio/pull/2373)\n- Adds ability to disable pre/post-processing for examples [@abidlabs](https://github.com/abidlabs) in [PR 2383](https://github.com/gradio-app/gradio/pull/2383)\n- Copy changelog file in website docker by [@aliabd](https://github.com/aliabd) in [PR 2384](https://github.com/gradio-app/gradio/pull/2384)\n- Lets users provide a `gr.update()` dictionary even if post-processing is disabled [@abidlabs](https://github.com/abidlabs) in [PR 2385](https://github.com/gradio-app/gradio/pull/2385)\n- Fix bug where errors would cause apps run in reload mode to hang forever by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2394](https://github.com/gradio-app/gradio/pull/2394)\n- Fix bug where new typeable slider doesn't respect the minimum and maximum values [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 2380](https://github.com/gradio-app/gradio/pull/2380)\n### Contributors Shoutout:\nNo changes to highlight."", ""Ex: specify `None` as the second value in the tuple, to add a single message in the chatbot from the \""bot\"" side. ```python\ngr.Chatbot([(\""Hi, I'm DialoGPT. Try asking me a question.\"", None)])\n```\nBy [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3165](https://github.com/gradio-app/gradio/pull/3165)\n- Fixes `gr.utils.delete_none` to only remove props whose values are `None` from the config by [@abidlabs](https://github.com/abidlabs) in [PR 3188](https://github.com/gradio-app/gradio/pull/3188)\n- Fix bug where embedded demos were not loading files properly by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3177](https://github.com/gradio-app/gradio/pull/3177)\n- The `change` event is now triggered when users click the 'Clear All' button of the multiselect DropDown component by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3195](https://github.com/gradio-app/gradio/pull/3195)\n- Stops File component from freezing when a large file is uploaded by [@aliabid94](https://github.com/aliabid94) in [PR 3191](https://github.com/gradio-app/gradio/pull/3191)\n- Support Chinese pinyin in Dataframe by [@aliabid94](https://github.com/aliabid94) in [PR 3206](https://github.com/gradio-app/gradio/pull/3206)\n- The `clear` event is now triggered when images are cleared by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3218](https://github.com/gradio-app/gradio/pull/3218)\n- Fix bug where auth cookies where not sent when connecting to an app via http by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3223](https://github.com/gradio-app/gradio/pull/3223)\n- Ensure latext CSS is always applied in light and dark mode by [@pngwn](https://github.com/pngwn) in [PR 3233](https://github.com/gradio-app/gradio/pull/3233)\n### Documentation Changes:\n- Sort components in docs by alphabetic order by [@aliabd](https://github.com/aliabd) in [PR 3152](https://github.com/gradio-app/gradio/pull/3152)\n- Changes to W&B guide by [@scottire](https://github.com/scottire) in [PR 3153](https://github.com/gradio-app/gradio/pull/3153)\n- Keep pnginfo metadata for gallery by [@wfng92](https://github.com/wfng92) in [PR 3150](https://github.com/gradio-app/gradio/pull/3150)\n- Add a section on how to run a Gradio app locally [@osanseviero](https://github.com/osanseviero) in [PR 3170](https://github.com/gradio-app/gradio/pull/3170)\n- Fixed typos in gradio events function documentation by [@vidalmaxime](https://github.com/vidalmaxime) in [PR 3168](https://github.com/gradio-app/gradio/pull/3168)\n- Added an example using Gradio's batch mode with the diffusers library by [@abidlabs](https://github.com/abidlabs) in [PR 3224](https://github.com/gradio-app/gradio/pull/3224)\n### Testing and Infrastructure Changes:\nNo changes to highlight."", ""- Fixes bug that the chatbot sample code does not work with certain input value by [@petrov826](https://github.com/petrov826) in [PR 3039](https://github.com/gradio-app/gradio/pull/3039). - Fix shadows for form element and ensure focus styles more visible in dark mode [@pngwn](https://github.com/pngwn) in [PR 3042](https://github.com/gradio-app/gradio/pull/3042). - Fixed bug where the Checkbox and Dropdown change events were not triggered in response to other component changes by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3045](https://github.com/gradio-app/gradio/pull/3045)\n- Fix bug where the queue was not properly restarted after launching a `closed` app by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3022](https://github.com/gradio-app/gradio/pull/3022)\n- Adding missing embedded components on docs by [@aliabd](https://github.com/aliabd) in [PR 3027](https://github.com/gradio-app/gradio/pull/3027)\n- Fixes bug where app would crash if the `file_types` parameter of `gr.File` or `gr.UploadButton` was not a list by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3048](https://github.com/gradio-app/gradio/pull/3048)\n- Ensure CSS mounts correctly regardless of how many Gradio instances are on the page [@pngwn](https://github.com/pngwn) in [PR 3059](https://github.com/gradio-app/gradio/pull/3059). - Fix bug where input component was not hidden in the frontend for `UploadButton` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3053](https://github.com/gradio-app/gradio/pull/3053)\n- Fixes issue where after clicking submit or undo, the sketch output wouldn't clear. [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3047](https://github.com/gradio-app/gradio/pull/3047)\n- Ensure spaces embedded via the web component always use the correct URLs for server requests and change ports for testing to avoid strange collisions when users are working with embedded apps locally by [@pngwn](https://github.com/pngwn) in [PR 3065](https://github.com/gradio-app/gradio/pull/3065)\n- Preserve selected image of Gallery through updated by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3061](https://github.com/gradio-app/gradio/pull/3061)\n- Fix bug where auth was not respected on HF spaces by [@freddyaboulton](https://github.com/freddyaboulton) and [@aliabid94](https://github.com/aliabid94) in [PR 3049](https://github.com/gradio-app/gradio/pull/3049)\n- Fixes bug where tabs selected attribute not working if manually change tab by [@tomchang25](https://github.com/tomchang25) in [3055](https://github.com/gradio-app/gradio/pull/3055)\n- Change chatbot to show dots on progress, and fix bug where chatbot would not stick to bottom in the case of images by [@aliabid94](https://github.com/aliabid94) in [PR 3067](https://github.com/gradio-app/gradio/pull/3079)\n### Documentation Changes:\n- SEO improvements to guides by[@aliabd](https://github.com/aliabd) in [PR 2915](https://github.com/gradio-app/gradio/pull/2915)\n- Use `gr.LinePlot` for the `blocks_kinematics` demo by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2998](https://github.com/gradio-app/gradio/pull/2998)\n- Updated the `interface_series_load` to include some inline markdown code by [@abidlabs](https://github.com/abidlabs) in [PR 3051](https://github.com/gradio-app/gradio/pull/3051)\n### Testing and Infrastructure Changes:\n- Adds a GitHub action to test if any large files (> 5MB) are present by [@abidlabs](https://github.com/abidlabs) in [PR 3013](https://github.com/gradio-app/gradio/pull/3013)\n### Breaking Changes:\nNo changes to highlight."", ""gradio\n## 4.11.0\n### Features\n- [#6842](https://github.com/gradio-app/gradio/pull/6842) [`846d52d`](https://github.com/gradio-app/gradio/commit/846d52d1c92d429077382ce494eea27fd062d9f6) - Fix md highlight. Thanks [@pngwn](https://github.com/pngwn)! - [#6831](https://github.com/gradio-app/gradio/pull/6831) [`f3abde8`](https://github.com/gradio-app/gradio/commit/f3abde80884d96ad69b825020c46486d9dd5cac5) - Add an option to enable header links for markdown. Thanks [@pngwn](https://github.com/pngwn)! - [#6814](https://github.com/gradio-app/gradio/pull/6814) [`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d) - Refactor queue so that there are separate queues for each concurrency id. Thanks [@aliabid94](https://github.com/aliabid94)! - [#6809](https://github.com/gradio-app/gradio/pull/6809) [`1401d99`](https://github.com/gradio-app/gradio/commit/1401d99ade46d87da75b5f5808a3354c49f1d1ea) - Fix `ImageEditor` interaction story. Thanks [@hannahblair](https://github.com/hannahblair)! - [#6803](https://github.com/gradio-app/gradio/pull/6803) [`77c9003`](https://github.com/gradio-app/gradio/commit/77c900311e2ba37b8f849ce088ceb48aa196af18) - Fixes issue 5781: Enables specifying a caching directory for Examples. Thanks [@cswamy](https://github.com/cswamy)! - [#6823](https://github.com/gradio-app/gradio/pull/6823) [`67a2b7f`](https://github.com/gradio-app/gradio/commit/67a2b7f12cb06355fcc41e40d47e8b2ad211d7d1) - Fixed duplicate word (\""this this\""). Thanks [@Cassini-chris](https://github.com/Cassini-chris)!"", ""### Testing and Infrastructure Changes:\nNo changes to highlight. ### Breaking Changes:\nNo changes to highlight. ### Full Changelog:\nNo changes to highlight. ### Contributors Shoutout:\nNo changes to highlight. # 0.2.3\n### New Features:\nNo changes to highlight. ### Bug Fixes:\n- Fix example inputs for `gr.File(file\\_count='multiple')` output components by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4153](https://github.com/gradio-app/gradio/pull/4153)\n### Documentation Changes:\nNo changes to highlight. ### Testing and Infrastructure Changes:\nNo changes to highlight. ### Breaking Changes:\nNo changes to highlight. ### Full Changelog:\nNo changes to highlight. ### Contributors Shoutout:\nNo changes to highlight. # 0.2.2\n### New Features:\nNo changes to highlight. ### Bug Fixes:\n- Only send request to `/info` route if demo version is above `3.28.3` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4109](https://github.com/gradio-app/gradio/pull/4109)\n### Other Changes:\n- Fix bug in test from gradio 3.29.0 refactor by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4138](https://github.com/gradio-app/gradio/pull/4138)\n### Breaking Changes:\nNo changes to highlight. # 0.2.1\n### New Features:\nNo changes to highlight. ### Bug Fixes:\nRemoves extraneous `State` component info from the `Client.view\\_api()` method by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)\n### Documentation Changes:\nNo changes to highlight. ### Testing and Infrastructure Changes:\nSeparates flaky tests from non-flaky tests by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)\n### Breaking Changes:\nNo changes to highlight."", ""- Fixes bug that the chatbot sample code does not work with certain input value by [@petrov826](https://github.com/petrov826) in [PR 3039](https://github.com/gradio-app/gradio/pull/3039). - Fix shadows for form element and ensure focus styles more visible in dark mode [@pngwn](https://github.com/pngwn) in [PR 3042](https://github.com/gradio-app/gradio/pull/3042). - Fixed bug where the Checkbox and Dropdown change events were not triggered in response to other component changes by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3045](https://github.com/gradio-app/gradio/pull/3045)\n- Fix bug where the queue was not properly restarted after launching a `closed` app by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3022](https://github.com/gradio-app/gradio/pull/3022)\n- Adding missing embedded components on docs by [@aliabd](https://github.com/aliabd) in [PR 3027](https://github.com/gradio-app/gradio/pull/3027)\n- Fixes bug where app would crash if the `file_types` parameter of `gr.File` or `gr.UploadButton` was not a list by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3048](https://github.com/gradio-app/gradio/pull/3048)\n- Ensure CSS mounts correctly regardless of how many Gradio instances are on the page [@pngwn](https://github.com/pngwn) in [PR 3059](https://github.com/gradio-app/gradio/pull/3059). - Fix bug where input component was not hidden in the frontend for `UploadButton` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3053](https://github.com/gradio-app/gradio/pull/3053)\n- Fixes issue where after clicking submit or undo, the sketch output wouldn't clear. [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3047](https://github.com/gradio-app/gradio/pull/3047)\n- Ensure spaces embedded via the web component always use the correct URLs for server requests and change ports for testing to avoid strange collisions when users are working with embedded apps locally by [@pngwn](https://github.com/pngwn) in [PR 3065](https://github.com/gradio-app/gradio/pull/3065)\n- Preserve selected image of Gallery through updated by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3061](https://github.com/gradio-app/gradio/pull/3061)\n- Fix bug where auth was not respected on HF spaces by [@freddyaboulton](https://github.com/freddyaboulton) and [@aliabid94](https://github.com/aliabid94) in [PR 3049](https://github.com/gradio-app/gradio/pull/3049)\n- Fixes bug where tabs selected attribute not working if manually change tab by [@tomchang25](https://github.com/tomchang25) in [3055](https://github.com/gradio-app/gradio/pull/3055)\n- Change chatbot to show dots on progress, and fix bug where chatbot would not stick to bottom in the case of images by [@aliabid94](https://github.com/aliabid94) in [PR 3067](https://github.com/gradio-app/gradio/pull/3079)\n### Documentation Changes:\n- SEO improvements to guides by[@aliabd](https://github.com/aliabd) in [PR 2915](https://github.com/gradio-app/gradio/pull/2915)\n- Use `gr.LinePlot` for the `blocks_kinematics` demo by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2998](https://github.com/gradio-app/gradio/pull/2998)\n- Updated the `interface_series_load` to include some inline markdown code by [@abidlabs](https://github.com/abidlabs) in [PR 3051](https://github.com/gradio-app/gradio/pull/3051)\n### Testing and Infrastructure Changes:\n- Adds a GitHub action to test if any large files (> 5MB) are present by [@abidlabs](https://github.com/abidlabs) in [PR 3013](https://github.com/gradio-app/gradio/pull/3013)\n### Breaking Changes:\nNo changes to highlight."", ""Ex: specify `None` as the second value in the tuple, to add a single message in the chatbot from the \""bot\"" side. ```python\ngr.Chatbot([(\""Hi, I'm DialoGPT. Try asking me a question.\"", None)])\n```\nBy [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3165](https://github.com/gradio-app/gradio/pull/3165)\n- Fixes `gr.utils.delete_none` to only remove props whose values are `None` from the config by [@abidlabs](https://github.com/abidlabs) in [PR 3188](https://github.com/gradio-app/gradio/pull/3188)\n- Fix bug where embedded demos were not loading files properly by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3177](https://github.com/gradio-app/gradio/pull/3177)\n- The `change` event is now triggered when users click the 'Clear All' button of the multiselect DropDown component by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3195](https://github.com/gradio-app/gradio/pull/3195)\n- Stops File component from freezing when a large file is uploaded by [@aliabid94](https://github.com/aliabid94) in [PR 3191](https://github.com/gradio-app/gradio/pull/3191)\n- Support Chinese pinyin in Dataframe by [@aliabid94](https://github.com/aliabid94) in [PR 3206](https://github.com/gradio-app/gradio/pull/3206)\n- The `clear` event is now triggered when images are cleared by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3218](https://github.com/gradio-app/gradio/pull/3218)\n- Fix bug where auth cookies where not sent when connecting to an app via http by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3223](https://github.com/gradio-app/gradio/pull/3223)\n- Ensure latext CSS is always applied in light and dark mode by [@pngwn](https://github.com/pngwn) in [PR 3233](https://github.com/gradio-app/gradio/pull/3233)\n### Documentation Changes:\n- Sort components in docs by alphabetic order by [@aliabd](https://github.com/aliabd) in [PR 3152](https://github.com/gradio-app/gradio/pull/3152)\n- Changes to W&B guide by [@scottire](https://github.com/scottire) in [PR 3153](https://github.com/gradio-app/gradio/pull/3153)\n- Keep pnginfo metadata for gallery by [@wfng92](https://github.com/wfng92) in [PR 3150](https://github.com/gradio-app/gradio/pull/3150)\n- Add a section on how to run a Gradio app locally [@osanseviero](https://github.com/osanseviero) in [PR 3170](https://github.com/gradio-app/gradio/pull/3170)\n- Fixed typos in gradio events function documentation by [@vidalmaxime](https://github.com/vidalmaxime) in [PR 3168](https://github.com/gradio-app/gradio/pull/3168)\n- Added an example using Gradio's batch mode with the diffusers library by [@abidlabs](https://github.com/abidlabs) in [PR 3224](https://github.com/gradio-app/gradio/pull/3224)\n### Testing and Infrastructure Changes:\nNo changes to highlight."", ""### Bug Fixes:\n- Updated components with `info` attribute to update when `update()` is called on them. by [@jebarpg](https://github.com/jebarpg) in [PR 4715](https://github.com/gradio-app/gradio/pull/4715). - Ensure the `Image` components undo button works mode is `mask` or `color-sketch` by [@amyorz](https://github.com/AmyOrz) in [PR 4692](https://github.com/gradio-app/gradio/pull/4692)\n- Load the iframe resizer external asset asynchronously, by [@akx](https://github.com/akx) in [PR 4336](https://github.com/gradio-app/gradio/pull/4336)\n- Restored missing imports in `gr.components` by [@abidlabs](https://github.com/abidlabs) in [PR 4566](https://github.com/gradio-app/gradio/pull/4566)\n- Fix bug where `select` event was not triggered in `gr.Gallery` if `height` was set to be large with `allow_preview=False` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4551](https://github.com/gradio-app/gradio/pull/4551)\n- Fix bug where setting `visible=False` in `gr.Group` event did not work by [@abidlabs](https://github.com/abidlabs) in [PR 4567](https://github.com/gradio-app/gradio/pull/4567)\n- Fix `make_waveform` to work with paths that contain spaces [@akx](https://github.com/akx) in [PR 4570](https://github.com/gradio-app/gradio/pull/4570) & [PR 4578](https://github.com/gradio-app/gradio/pull/4578)\n- Send captured data in `stop_recording` event for `gr.Audio` and `gr.Video` components by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4554](https://github.com/gradio-app/gradio/pull/4554)\n- Fix bug in `gr.Gallery` where `height` and `object_fit` parameters where being ignored by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4576](https://github.com/gradio-app/gradio/pull/4576)\n- Fixes an HTML sanitization issue in DOMPurify where links in markdown were not opening in a new window by [@hannahblair] in [PR 4577](https://github.com/gradio-app/gradio/pull/4577)\n- Fixed Dropdown height rendering in Columns by [@aliabid94](https://github.com/aliabid94) in [PR 4584](https://github.com/gradio-app/gradio/pull/4584)\n- Fixed bug where `AnnotatedImage` css styling was causing the annotation masks to not be displayed correctly by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4628](https://github.com/gradio-app/gradio/pull/4628)\n- Ensure that Gradio does not silently fail when running on a port that is occupied by [@abidlabs](https://github.com/abidlabs) in [PR 4624](https://github.com/gradio-app/gradio/pull/4624). - Fix double upload bug that caused lag in file uploads by [@aliabid94](https://github.com/aliabid94) in [PR 4661](https://github.com/gradio-app/gradio/pull/4661)\n- `Progress` component now appears even when no `iterable` is specified in `tqdm` constructor by [@itrushkin](https://github.com/itrushkin) in [PR 4475](https://github.com/gradio-app/gradio/pull/4475)\n- Deprecation warnings now point at the user code using those deprecated features, instead of Gradio internals, by (https://github.com/akx) in [PR 4694](https://github.com/gradio-app/gradio/pull/4694)\n- Adapt column widths in gr.Examples based on content by [@pngwn](https://github.com/pngwn) & [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 4700](https://github.com/gradio-app/gradio/pull/4700)\n- The `plot` parameter deprecation warnings should now only be emitted for `Image` components by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4709](https://github.com/gradio-app/gradio/pull/4709)\n- Removed uncessessary `type` deprecation warning by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4709](https://github.com/gradio-app/gradio/pull/4709)\n- Ensure Audio autoplays works when `autoplay=True` and the video source is dynamically updated [@pngwn](https://github.com/pngwn) in [PR 4705](https://github.com/gradio-app/gradio/pull/4705)\n- When an error modal is shown in spaces, ensure we scroll to the top so it can be seen by [@pngwn](https://github.com/pngwn) in [PR 4712](https://github.com/gradio-app/gradio/pull/4712)\n- Update depedencies by [@pngwn](https://github.com/pngwn) in [PR 4675](https://github.com/gradio-app/gradio/pull/4675)\n- Fixes `gr.Dropdown` being cutoff at the bottom by [@abidlabs](https://github.com/abidlabs) in [PR 4691](https://github.com/gradio-app/gradio/pull/4691). - Scroll top when clicking \""View API\"" in spaces by [@pngwn](https://github.com/pngwn) in [PR 4714](https://github.com/gradio-app/gradio/pull/4714)\n- Fix bug where `show_label` was hiding the entire component for `gr.Label` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4713](https://github.com/gradio-app/gradio/pull/4713)\n- Don't crash when uploaded image has broken EXIF data, by [@akx](https://github.com/akx) in [PR 4764](https://github.com/gradio-app/gradio/pull/4764)\n- Place toast messages at the top of the screen by [@pngwn](https://github.com/pngwn) in [PR 4796](https://github.com/gradio-app/gradio/pull/4796)\n- Fix regressed styling of Login page when auth is enabled by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4797](https://github.com/gradio-app/gradio/pull/4797)\n- Prevent broken scrolling to output on Spaces by [@aliabid94](https://github.com/aliabid94) in [PR 4822](https://github.com/gradio-app/gradio/pull/4822)\n### Other Changes:\n- Add `.git-blame-ignore-revs` by [@akx](https://github.com/akx) in [PR 4586](https://github.com/gradio-app/gradio/pull/4586)\n- Update frontend dependencies in [PR 4601](https://github.com/gradio-app/gradio/pull/4601)\n- Use `typing.Literal` where possible in gradio library and client by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4608](https://github.com/gradio-app/gradio/pull/4608)\n- Remove unnecessary mock json files for frontend E2E tests by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 4625](https://github.com/gradio-app/gradio/pull/4625)\n- Update dependencies by [@pngwn](https://github.com/pngwn) in [PR 4643](https://github.com/gradio-app/gradio/pull/4643)\n- The theme builder now launches successfully, and the API docs are cleaned up."", ""6. Fix audio streaming, which was previously choppy in [PR 2351](https://github.com/gradio-app/gradio/pull/2351). Big thanks to [@yannickfunk](https://github.com/yannickfunk) for the proposed solution. 7. Fix bug where new typeable slider doesn't respect the minimum and maximum values [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 2380](https://github.com/gradio-app/gradio/pull/2380)\n### Documentation Changes:\n1. New Guide: Connecting to a Database ðŸ—„ï¸\nA new guide by [@freddyaboulton](https://github.com/freddyaboulton) that explains how you can use Gradio to connect your app to a database. Read more [here](https://gradio.app/connecting_to_a_database/). 2. New Guide: Running Background Tasks ðŸ¥·\nA new guide by [@freddyaboulton](https://github.com/freddyaboulton) that explains how you can run background tasks from your gradio app. Read more [here](https://gradio.app/running_background_tasks/). 3. Small fixes to docs for `Image` component by [@abidlabs](https://github.com/abidlabs) in [PR 2372](https://github.com/gradio-app/gradio/pull/2372)\n### Testing and Infrastructure Changes:\nNo changes to highlight. ### Breaking Changes:\nNo changes to highlight. ### Full Changelog:\n- Create a guide on how to connect an app to a database hosted on the cloud by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2341](https://github.com/gradio-app/gradio/pull/2341)\n- Removes `analytics` dependency by [@abidlabs](https://github.com/abidlabs) in [PR 2347](https://github.com/gradio-app/gradio/pull/2347)\n- Add guide on launching background tasks from your app by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2350](https://github.com/gradio-app/gradio/pull/2350)\n- Fix typo in guide image path by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2357](https://github.com/gradio-app/gradio/pull/2357)\n- Raise error if Blocks has duplicate component with same IDs by [@abidlabs](https://github.com/abidlabs) in [PR 2359](https://github.com/gradio-app/gradio/pull/2359)\n- Hotfix: fix version back to 3.4 by [@abidlabs](https://github.com/abidlabs) in [PR 2361](https://github.com/gradio-app/gradio/pull/2361)\n- Change version.txt to 3.4 instead of 3.4.0 by [@aliabd](https://github.com/aliabd) in [PR 2363](https://github.com/gradio-app/gradio/pull/2363)\n- Catch the permission exception on the audio component by [@Ian-GL](https://github.com/Ian-GL) in [PR 2330](https://github.com/gradio-app/gradio/pull/2330)\n- Fix image_classifier_interface_load demo by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2365](https://github.com/gradio-app/gradio/pull/2365)\n- Small fixes to docs for `Image` component by [@abidlabs](https://github.com/abidlabs) in [PR 2372](https://github.com/gradio-app/gradio/pull/2372)\n- Automated Release Notes by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2306](https://github.com/gradio-app/gradio/pull/2306)\n- Fixed small typos in the docs [@julien-c](https://github.com/julien-c) in [PR 2373](https://github.com/gradio-app/gradio/pull/2373)\n- Adds ability to disable pre/post-processing for examples [@abidlabs](https://github.com/abidlabs) in [PR 2383](https://github.com/gradio-app/gradio/pull/2383)\n- Copy changelog file in website docker by [@aliabd](https://github.com/aliabd) in [PR 2384](https://github.com/gradio-app/gradio/pull/2384)\n- Lets users provide a `gr.update()` dictionary even if post-processing is disabled [@abidlabs](https://github.com/abidlabs) in [PR 2385](https://github.com/gradio-app/gradio/pull/2385)\n- Fix bug where errors would cause apps run in reload mode to hang forever by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2394](https://github.com/gradio-app/gradio/pull/2394)\n- Fix bug where new typeable slider doesn't respect the minimum and maximum values [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 2380](https://github.com/gradio-app/gradio/pull/2380)\n### Contributors Shoutout:\nNo changes to highlight."", ""Thanks [@abidlabs](https://github.com/abidlabs)! - [#6079](https://github.com/gradio-app/gradio/pull/6079) [`3b2d9eaa3`](https://github.com/gradio-app/gradio/commit/3b2d9eaa3e84de3e4a0799e4585a94510d665f26) - fix cc build. Thanks [@pngwn](https://github.com/pngwn)! ### Fixes\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Pending events behavior. Thanks [@pngwn](https://github.com/pngwn)! - [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Reinstate types that were removed in error in #5832. Thanks [@pngwn](https://github.com/pngwn)! - [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Fixes: slider bar are too thin on FireFox. Thanks [@pngwn](https://github.com/pngwn)! - [#6146](https://github.com/gradio-app/gradio/pull/6146) [`40a171ea6`](https://github.com/gradio-app/gradio/commit/40a171ea60c74afa9519d6cb159def16ce68e1ca) - Fix image double change bug. Thanks [@pngwn](https://github.com/pngwn)! - [#6148](https://github.com/gradio-app/gradio/pull/6148) [`0000a1916`](https://github.com/gradio-app/gradio/commit/0000a191688c5480c977c80acdd0c9023865d57e) - fix dropdown arrow size. Thanks [@pngwn](https://github.com/pngwn)! - [#6067](https://github.com/gradio-app/gradio/pull/6067) [`bf38e5f06`](https://github.com/gradio-app/gradio/commit/bf38e5f06a7039be913614901c308794fea83ae0) - remove dupe component. Thanks [@pngwn](https://github.com/pngwn)! - [#6065](https://github.com/gradio-app/gradio/pull/6065) [`7d07001e8`](https://github.com/gradio-app/gradio/commit/7d07001e8e7ca9cbd2251632667b3a043de49f49) - fix storybook. Thanks [@pngwn](https://github.com/pngwn)! - [#5826](https://github.com/gradio-app/gradio/pull/5826) [`ce036c5d4`](https://github.com/gradio-app/gradio/commit/ce036c5d47e741e29812654bcc641ea6be876504) - Pending events behavior. Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)! - [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)! - [#6042](https://github.com/gradio-app/gradio/pull/6042) [`e27997fe6`](https://github.com/gradio-app/gradio/commit/e27997fe6c2bcfebc7015fc26100cee9625eb13a) - Fix `root` when user is unauthenticated so that login page appears correctly. Thanks [@abidlabs](https://github.com/abidlabs)! - [#6076](https://github.com/gradio-app/gradio/pull/6076) [`f3f98f923`](https://github.com/gradio-app/gradio/commit/f3f98f923c9db506284b8440e18a3ac7ddd8398b) - Lite error handler. Thanks [@whitphx](https://github.com/whitphx)! - [#5984](https://github.com/gradio-app/gradio/pull/5984) [`66549d8d2`](https://github.com/gradio-app/gradio/commit/66549d8d256b1845c8c5efa0384695b36cb46eab) - Fixes: slider bar are too thin on FireFox. Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)! ## 3.45.0-beta.13\n### Features\n- [#5964](https://github.com/gradio-app/gradio/pull/5964) [`5fbda0bd2`](https://github.com/gradio-app/gradio/commit/5fbda0bd2b2bbb2282249b8875d54acf87cd7e84) - Wasm release. Thanks [@pngwn](https://github.com/pngwn)! ## 3.45.0-beta.12\n### Features\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-app/gradio/commit/85ba6de136a45b3e92c74e410bb27e3cbe7138d7) - V4: Some misc fixes."", ""- [#5682](https://github.com/gradio-app/gradio/pull/5682) [`c57f1b75e`](https://github.com/gradio-app/gradio/commit/c57f1b75e272c76b0af4d6bd0c7f44743ff34f26) - Fix functional tests. Thanks [@abidlabs](https://github.com/abidlabs)! - [#5681](https://github.com/gradio-app/gradio/pull/5681) [`40de3d217`](https://github.com/gradio-app/gradio/commit/40de3d2178b61ebe424b6f6228f94c0c6f679bea) - add query parameters to the `gr.Request` object through the `query\\_params` attribute. Thanks [@DarhkVoyd](https://github.com/DarhkVoyd)! - [#5653](https://github.com/gradio-app/gradio/pull/5653) [`ea0e00b20`](https://github.com/gradio-app/gradio/commit/ea0e00b207b4b90a10e9d054c4202d4e705a29ba) - Prevent Clients from accessing API endpoints that set `api\\_name=False`. Thanks [@abidlabs](https://github.com/abidlabs)! - [#5639](https://github.com/gradio-app/gradio/pull/5639) [`e1874aff8`](https://github.com/gradio-app/gradio/commit/e1874aff814d13b23f3e59ef239cc13e18ad3fa7) - Add `gr.on` listener method. Thanks [@aliabid94](https://github.com/aliabid94)! - [#5652](https://github.com/gradio-app/gradio/pull/5652) [`2e25d4305`](https://github.com/gradio-app/gradio/commit/2e25d430582264945ae3316acd04c4453a25ce38) - Pause autoscrolling if a user scrolls up in a `gr.Textbox` and resume autoscrolling if they go all the way down.""]",Every bug fix should typically be accompanied by a dynamic code test that failed before the fix and passes afterwards.,Dynamic code test
"How can you force mixed precision training when initializing the Accelerator in ðŸ¤— Accelerate?
","[""Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of ðŸ¤— Accelerate is:\n- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible. - to have the same functions work for any distributed setup, so only have to learn one API. ### How does it work? To see how the library works in practice, let's have a look at each line of code we need to add to a training loop. ```python\naccelerator = Accelerator()\n```\nOn top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script. ```python\nmodel, optim, data = accelerator.prepare(model, optim, data)\n```\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torch.nn.Module`), optimizers (`torch.optim.Optimizer`) and dataloaders (`torch.data.dataloader.DataLoader`). #### Model\nModel preparation include wrapping it in the proper container (for instance `DistributedDataParallel`) and putting it on the proper device. Like with a regular distributed training, you will need to unwrap your model for saving, or to access its specific methods, which can be done with `accelerator.unwrap\\_model(model)`. #### Optimizer\nThe optimizer is also wrapped in a special container that will perform the necessary operations in the step to make mixed precision work."", ""For details and\nbenchmarks, please, see [TensorFloat-32(TF32) on Ampere devices](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices). The document includes\ninstructions on how to disable this automatic conversion if for some reason you prefer not to use it. With the ðŸ¤— Trainer you can use `--tf32` to enable it, or disable it with `--tf32 0` or `--no\\_tf32`. By default the PyTorch default is used. ### Automatic Mixed Precision\nYou can use automatic mixed precision with either a pytorch-like AMP way or the apex-like way:\n### fp16\nTo configure pytorch AMP-like mode with fp16 (float16) set:\n```json\n{\n\""fp16\"": {\n\""enabled\"": \""auto\"",\n\""loss\\_scale\"": 0,\n\""loss\\_scale\\_window\"": 1000,\n\""initial\\_scale\\_power\"": 16,\n\""hysteresis\"": 2,\n\""min\\_loss\\_scale\"": 1\n}\n}\n```\nand the [`Trainer`] will automatically enable or disable it based on the value of\n`args.fp16\\_backend`. The rest of config values are up to you. This mode gets enabled when `--fp16 --fp16\\_backend amp` or `--fp16\\_full\\_eval` command line args are passed. You can also enable/disable this mode explicitly:\n```json\n{\n\""fp16\"": {\n\""enabled\"": true,\n\""loss\\_scale\"": 0,\n\""loss\\_scale\\_window\"": 1000,\n\""initial\\_scale\\_power\"": 16,\n\""hysteresis\"": 2,\n\""min\\_loss\\_scale\"": 1\n}\n}\n```\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\nconfiguration. Here is the [documentation](https://www.deepspeed.ai/docs/config-json/#fp16-training-options). ### bf16\nIf bf16 (bfloat16) is desired instead of fp16 then the following configuration section is to be used:\n```json\n{\n\""bf16\"": {\n\""enabled\"": \""auto\""\n}\n}\n```\nbf16 has the same dynamic range as fp32 and thus doesn't require loss scaling. This mode gets enabled when `--bf16` or `--bf16\\_full\\_eval` command line args are passed. You can also enable/disable this mode explicitly:\n```json\n{\n\""bf16\"": {\n\""enabled\"": true\n}\n}\n```\nAs of `deepspeed==0.6.0` the bf16 support is new and experimental. If you use [gradient accumulation](#gradient-accumulation) with bf16-enabled, you need to be aware that it'll accumulate gradients in bf16, which may not be what you want due to this format's low precision, as it may lead to a lossy accumulation. A work is being done to fix that and provide an option to use a higher precision `dtype` (fp16 or fp32). ### NCCL Collectives\nThere is the `dtype` of the training regime and there is a separate `dtype` that is used for communication collectives like various reduction and gathering/scattering operations. All gather/scatter ops are performed in the same `dtype` the data is in, so if you're using bf16 training regime it gets gathered in bf16 - gathering is a non-lossy operation."", ""It'll automatically configure your training setup based on your hardware and environment. Take a look at the ðŸ¤— Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more. Initialize an ðŸ¤— Accelerate environment:\n```bash\naccelerate config\n```\nTo setup a default ðŸ¤— Accelerate environment without choosing any configurations:\n```bash\naccelerate config default\n```\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n```bash\nfrom accelerate.utils import write\\_basic\\_config\nwrite\\_basic\\_config()\n```\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create\\_dataset) guide to learn how to create a dataset that works with the training script. The following sections highlight parts of the training script that are important for understanding how to modify it, but it doesn't cover every aspect of the script in detail. If you're interested in learning more, feel free to read through the [script](https://github.com/huggingface/diffusers/blob/main/examples/controlnet/train\\_controlnet.py) and let us know if you have any questions or concerns. ## Script parameters\nThe training script provides many parameters to help you customize your training run. All of the parameters and their descriptions are found in the [`parse\\_args()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/controlnet/train\\_controlnet.py#L231) function. This function provides default values for each parameter, such as the training batch size and learning rate, but you can also set your own values in the training command if you'd like. For example, to speedup training with mixed precision using the fp16 format, add the `--mixed\\_precision` parameter to the training command:\n```bash\naccelerate launch train\\_controlnet.py \\\n--mixed\\_precision=\""fp16\""\n```\nMany of the basic and important parameters are described in the [Text-to-image](text2image#script-parameters) training guide, so this guide just focuses on the relevant parameters for ControlNet:\n- `--max\\_train\\_samples`: the number of training samples; this can be lowered for faster training, but if you want to stream really large datasets, you'll need to include this parameter and the `--streaming` parameter in your training command\n- `--gradient\\_accumulation\\_steps`: number of update steps to accumulate before the backward pass; this allows you to train with a bigger batch size than your GPU memory can typically handle\n### Min-SNR weighting\nThe [Min-SNR](https://huggingface.co/papers/2303.09556) weighting strategy can help with training by rebalancing the loss to achieve faster convergence."", ""# Initialize accelerator and tensorboard logging\n... accelerator = Accelerator(\n... mixed\\_precision=config.mixed\\_precision,\n... gradient\\_accumulation\\_steps=config.gradient\\_accumulation\\_steps,\n... log\\_with=\""tensorboard\"",\n... project\\_dir=os.path.join(config.output\\_dir, \""logs\""),\n... )\n... if accelerator.is\\_main\\_process:\n... if config.output\\_dir is not None:\n... os.makedirs(config.output\\_dir, exist\\_ok=True)\n... if config.push\\_to\\_hub:\n... repo\\_id = create\\_repo(\n... repo\\_id=config.hub\\_model\\_id or Path(config.output\\_dir).name, exist\\_ok=True\n... ).repo\\_id\n... accelerator.init\\_trackers(\""train\\_example\"")\n... # Prepare everything\n... # There is no specific order to remember, you just need to unpack the\n... # objects in the same order you gave them to the prepare method. ... model, optimizer, train\\_dataloader, lr\\_scheduler = accelerator.prepare(\n... model, optimizer, train\\_dataloader, lr\\_scheduler\n... )\n... global\\_step = 0\n... # Now you train the model\n... for epoch in range(config.num\\_epochs):\n... progress\\_bar = tqdm(total=len(train\\_dataloader), disable=not accelerator.is\\_local\\_main\\_process)\n... progress\\_bar.set\\_description(f\""Epoch {epoch}\"")\n..."", ""upercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on which you can run your training: it could be on CPU, GPUs, TPUs. Distributed on one machine with several devices, or several machines (often called nodes) each with multiple devices. On top of that there are new tweaks to make your training faster or more memory efficient, like mixed precision and DeepSpeed. Each of those setups or training tweaks, requires you to change the code of your training loop in one way or another and to learn a new API. All those setups are handled by the Trainer API, and there are several third-party libraries that can also help you with that. The problem with those is that they can feel like a black box and that it might not be easy to implement the tweak to the training loop you need. Accelerate has been designed specifically to let you retain full control over your training loop and be as non-intrusive as possible. With just four lines to add to your training loop (here shown on the code of the training loop from the \""Raw training loop\"" video), Accelerate will handle all the setups and training tweaks mentioned on the first slide."", ""While gradient checkpointing may improve memory efficiency, it slows training by approximately 20%. ## Mixed precision training\n\\*\\*Mixed precision training\\*\\* is a technique that aims to optimize the computational efficiency of training models by\nutilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point\nprecision (fp32 or float32) to represent and process variables. However, not all variables require this high precision\nlevel to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like 16-bit\nfloating point (fp16 or float16), we can speed up the computations. Because in this approach some computations are performed\nin half-precision, while some are still in full precision, the approach is called mixed precision training. Most commonly mixed precision training is achieved by using fp16 (float16) data types, however, some GPU architectures\n(such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. Check\nout the [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/) to learn more about\nthe differences between these data types. ### fp16\nThe main advantage of mixed precision training comes from saving the activations in half precision (fp16). Although the gradients are also computed in half precision they are converted back to full precision for the optimization\nstep so no memory is saved here. While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes. This is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU). To enable mixed precision training, set the `fp16` flag to `True`:\n```py\ntraining\\_args = TrainingArguments(per\\_device\\_train\\_batch\\_size=4, fp16=True, \\*\\*default\\_args)\n```\nIf you prefer to use ðŸ¤— Accelerate, find the ðŸ¤— Accelerate example [further in this guide](#using-accelerate)."", ""Then we can enable gradient checkpointing by calling the model's [`~PreTrainedModel.gradient\\_checkpointing\\_enable`] method. When we initialize the [`Accelerator`](https://huggingface.co/docs/accelerate/package\\_reference/accelerator#accelerate.Accelerator)\nwe can specify if we want to use mixed precision training and it will take care of it for us in the [`prepare`] call. During the [`prepare`](https://huggingface.co/docs/accelerate/package\\_reference/accelerator#accelerate.Accelerator.prepare)\ncall the dataloader will also be distributed across workers should we use multiple GPUs. We use the same [8-bit optimizer](#8-bit-adam) from the earlier example. Finally, we can add the main training loop. Note that the `backward` call is handled by ðŸ¤— Accelerate. We can also see\nhow gradient accumulation works: we normalize the loss, so we get the average at the end of accumulation and once we have\nenough steps we run the optimization. Implementing these optimization techniques with ðŸ¤— Accelerate only takes a handful of lines of code and comes with the\nbenefit of more flexibility in the training loop. For a full documentation of all features have a look at the\n[Accelerate documentation](https://huggingface.co/docs/accelerate/index). ## Efficient Software Prebuilds\nPyTorch's [pip and conda builds](https://pytorch.org/get-started/locally/#start-locally) come prebuilt with the cuda toolkit\nwhich is enough to run PyTorch, but it is insufficient if you need to build cuda extensions."", ""Some adjustments are required to use DeepSpeed in a notebook; please take a look at the [corresponding guide](main\\_classes/deepspeed#deployment-in-notebooks). \\* If you prefer to use ðŸ¤— Accelerate, refer to [ðŸ¤— Accelerate DeepSpeed guide](https://huggingface.co/docs/accelerate/en/usage\\_guides/deepspeed). ## Using torch.compile\nPyTorch 2.0 introduced a new compile function that doesn't require any modification to existing PyTorch code but can\noptimize your code by adding a single line of code: `model = torch.compile(model)`. If using [`Trainer`], you only need `to` pass the `torch\\_compile` option in the [`TrainingArguments`]:\n```python\ntraining\\_args = TrainingArguments(torch\\_compile=True, \\*\\*default\\_args)\n```\n`torch.compile` uses Python's frame evaluation API to automatically create a graph from existing PyTorch programs. After\ncapturing the graph, different backends can be deployed to lower the graph to an optimized engine. You can find more details and benchmarks in [PyTorch documentation](https://pytorch.org/get-started/pytorch-2.0/). `torch.compile` has a growing list of backends, which can be found in by calling `torchdynamo.list\\_backends()`, each of which with its optional dependencies. Choose which backend to use by specifying it via `torch\\_compile\\_backend` in the [`TrainingArguments`]. Some of the most commonly used backends are:\n\\*\\*Debugging backends\\*\\*:\n\\* `dynamo.optimize(\""eager\"")` - Uses PyTorch to run the extracted GraphModule. This is quite useful in debugging TorchDynamo issues. \\* `dynamo.optimize(\""aot\\_eager\"")` - Uses AotAutograd with no compiler, i.e, just using PyTorch eager for the AotAutograd's extracted forward and backward graphs. This is useful for debugging, and unlikely to give speedups. \\*\\*Training & inference backends\\*\\*:\n\\* `dynamo.optimize(\""inductor\"")` - Uses TorchInductor backend with AotAutograd and cudagraphs by leveraging codegened Triton kernels [Read more](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)\n\\* `dynamo.optimize(\""nvfuser\"")` - nvFuser with TorchScript. [Read more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)\n\\* `dynamo.optimize(\""aot\\_nvfuser\"")` - nvFuser with AotAutograd. [Read more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)\n\\* `dynamo.optimize(\""aot\\_cudagraphs\"")` - cudagraphs with AotAutograd. [Read more](https://github.com/pytorch/torchdynamo/pull/757)\n\\*\\*Inference-only backend\\*\\*s:\n\\* `dynamo.optimize(\""ofi\"")` - Uses Torchscript optimize\\_for\\_inference. [Read more](https://pytorch.org/docs/stable/generated/torch.jit.optimize\\_for\\_inference.html)\n\\* `dynamo.optimize(\""fx2trt\"")` - Uses NVIDIA TensorRT for inference optimizations. [Read more](https://pytorch.org/TensorRT/tutorials/getting\\_started\\_with\\_fx\\_path.html)\n\\* `dynamo.optimize(\""onnxrt\"")` - Uses ONNXRT for inference on CPU/GPU. [Read more](https://onnxruntime.ai/)\n\\* `dynamo.optimize(\""ipex\"")` - Uses IPEX for inference on CPU. [Read more](https://github.com/intel/intel-extension-for-pytorch)\nFor an example of using `torch.compile` with ðŸ¤— Transformers, check out this [blog post on fine-tuning a BERT model for Text Classification using the newest PyTorch 2.0 features](https://www.philschmid.de/getting-started-pytorch-2-0-transformers)\n## Using ðŸ¤— Accelerate\nWith [ðŸ¤— Accelerate](https://huggingface.co/docs/accelerate/index) you can use the above methods while gaining full\ncontrol over the training loop and can essentially write the loop in pure PyTorch with some minor modifications. Suppose you have combined the methods in the [`TrainingArguments`] like so:\n```py\ntraining\\_args = TrainingArguments(\nper\\_device\\_train\\_batch\\_size=1,\ngradient\\_accumulation\\_steps=4,\ngradient\\_checkpointing=True,\nfp16=True,\n\\*\\*default\\_args,\n)\n```\nThe full example training loop with ðŸ¤— Accelerate is only a handful of lines of code long:\n```py\nfrom accelerate import Accelerator\nfrom torch.utils.data.dataloader import DataLoader\ndataloader = DataLoader(ds, batch\\_size=training\\_args.per\\_device\\_train\\_batch\\_size)\nif training\\_args.gradient\\_checkpointing:\nmodel.gradient\\_checkpointing\\_enable()\naccelerator = Accelerator(fp16=training\\_args.fp16)\nmodel, optimizer, dataloader = accelerator.prepare(model, adam\\_bnb\\_optim, dataloader)\nmodel.train()\nfor step, batch in enumerate(dataloader, start=1):\nloss = model(\\*\\*batch).loss\nloss = loss / training\\_args.gradient\\_accumulation\\_steps\naccelerator.backward(loss)\nif step % training\\_args.gradient\\_accumulation\\_steps == 0:\noptimizer.step()\noptimizer.zero\\_grad()\n```\nFirst we wrap the dataset in a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."", ""### BF16\nIf you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While\nbf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have\nis `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which\nis about the same as fp32 - because both have 8-bits used for the numerical range. You can enable BF16 in the ðŸ¤— Trainer with:\n```python\ntraining\\_args = TrainingArguments(bf16=True, \\*\\*default\\_args)\n```\n### TF32\nThe Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead\nof 23 bits precision it has only 10 bits (same as fp16) and uses only 19 bits in total. It's \""magical\"" in the sense that\nyou can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput\nimprovement. All you need to do is to add the following to your code:\n```\nimport torch\ntorch.backends.cuda.matmul.allow\\_tf32 = True\ntorch.backends.cudnn.allow\\_tf32 = True\n```\nCUDA will automatically switch to using tf32 instead of fp32 where possible, assuming that the used GPU is from the Ampere series. According to [NVIDIA research](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/), the\nmajority of machine learning training workloads show the same perplexity and convergence with tf32 training as with fp32. If you're already using fp16 or bf16 mixed precision it may help with the throughput as well. You can enable this mode in the ðŸ¤— Trainer:\n```python\nTrainingArguments(tf32=True, \\*\\*default\\_args)\n```\ntf32 can't be accessed directly via `tensor.to(dtype=torch.tf32)` because it is an internal CUDA data type. You need `torch>=1.7` to use tf32 data types. For additional information on tf32 vs other precisions, please refer to the following benchmarks:\n[RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803) and\n[A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189). ## Flash Attention 2\nYou can speedup the training throughput by using Flash Attention 2 integration in transformers. Check out the appropriate section in the [single GPU section](./perf\\_infer\\_gpu\\_one#Flash-Attention-2) to learn more about how to load a model with Flash Attention 2 modules. ## Optimizer choice\nThe most common optimizer used to train transformer models is Adam or AdamW (Adam with weight decay)."", ""Various reduce operations can be quite lossy, for example when gradients are averaged across multiple-gpus, if the communications are done in fp16 or bf16 the outcome is likely be lossy - since when one ads multiple numbers in low precision the result isn't exact. More so with bf16 as it has a lower precision than fp16. Often fp16 is good enough as the loss is minimal when averaging grads which are typically very small. Therefore, by default for half precision training fp16 is used as the default for reduction operations. But you have full control over this functionality and if you choose you can add a small overhead and ensure that reductions will be using fp32 as the accumulation dtype and only when the result is ready it'll get downcast to the half precision `dtype` you're training in. In order to override the default you simply add a new configuration entry:\n```json\n{\n\""communication\\_data\\_type\"": \""fp32\""\n}\n```\nThe valid values as of this writing are \""fp16\"", \""bfp16\"", \""fp32\"". note: stage zero 3 had a bug with regards to bf16 comm dtype that was fixed in `deepspeed==0.8.1`\n### apex\nTo configure apex AMP-like mode set:\n```json\n\""amp\"": {\n\""enabled\"": \""auto\"",\n\""opt\\_level\"": \""auto\""\n}\n```\nand the [`Trainer`] will automatically configure it based on the values of `args.fp16\\_backend` and\n`args.fp16\\_opt\\_level`. This mode gets enabled when `--fp16 --fp16\\_backend apex --fp16\\_opt\\_level 01` command line args are passed. You can also configure this mode explicitly:\n```json\n{\n\""amp\"": {\n\""enabled\"": true,\n\""opt\\_level\"": \""O1\""\n}\n}\n```\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\nconfiguration. Here is the [documentation](https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options). ### Batch Size\nTo configure batch size, use:\n```json\n{\n\""train\\_batch\\_size\"": \""auto\"",\n\""train\\_micro\\_batch\\_size\\_per\\_gpu\"": \""auto\""\n}\n```\nand the [`Trainer`] will automatically set `train\\_micro\\_batch\\_size\\_per\\_gpu` to the value of\n`args.per\\_device\\_train\\_batch\\_size` and `train\\_batch\\_size` to `args.world\\_size \\* args.per\\_device\\_train\\_batch\\_size \\* args.gradient\\_accumulation\\_steps`. You can also set the values explicitly:\n```json\n{\n\""train\\_batch\\_size\"": 12,\n\""train\\_micro\\_batch\\_size\\_per\\_gpu\"": 4\n}\n```\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\nconfiguration. ### Gradient Accumulation\nTo configure gradient accumulation set:\n```json\n{\n\""gradient\\_accumulation\\_steps\"": \""auto\""\n}\n```\nand the [`Trainer`] will automatically set it to the value of `args.gradient\\_accumulation\\_steps`. You can also set the value explicitly:\n```json\n{\n\""gradient\\_accumulation\\_steps\"": 3\n}\n```\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\nconfiguration. ### Gradient Clipping\nTo configure gradient gradient clipping set:\n```json\n{\n\""gradient\\_clipping\"": \""auto\""\n}\n```\nand the [`Trainer`] will automatically set it to the value of `args.max\\_grad\\_norm`. You can also set the value explicitly:\n```json\n{\n\""gradient\\_clipping\"": 1.0\n}\n```\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\nconfiguration. ### Getting The Model Weights Out\nAs long as you continue training and resuming using DeepSpeed you don't need to worry about anything. DeepSpeed stores\nfp32 master weights in its custom checkpoint optimizer files, which are `global\\_step\\*/\\*optim\\_states.pt` (this is glob\npattern), and are saved under the normal checkpoint. \\*\\*FP16 Weights:\\*\\*\nWhen a model is saved under ZeRO-2, you end up having the normal `pytorch\\_model.bin` file with the model weights, but\nthey are only the fp16 version of the weights. Under ZeRO-3, things are much more complicated, since the model weights are partitioned out over multiple GPUs,\ntherefore `\""stage3\\_gather\\_16bit\\_weights\\_on\\_model\\_save\"": true` is required to get the `Trainer` to save the fp16\nversion of the weights. If this setting is `False` `pytorch\\_model.bin` won't be created."", ""--\ntitle: \""Introducing ðŸ¤— Accelerate\""\nthumbnail: /blog/assets/20\\_accelerate\\_library/accelerate\\_diff.png\nauthors:\n- user: sgugger\n---\n# Introducing ðŸ¤— Accelerate\n## ðŸ¤— Accelerate\nRun your \\*\\*raw\\*\\* PyTorch training scripts on any kind of device. Most high-level libraries above PyTorch provide support for distributed training and mixed precision, but the abstraction they introduce require a user to learn a new API if they want to customize the underlying training loop. ðŸ¤— Accelerate was created for PyTorch users who like to have full control over their training loops but are reluctant to write (and maintain) the boilerplate code needed to use distributed training (for multi-GPU on one or several nodes, TPUs, ...) or mixed precision training. Plans forward include support for fairscale, deepseed, AWS SageMaker specific data-parallelism and model parallelism. It provides two things: a simple and consistent API that abstracts that boilerplate code and a launcher command to easily run those scripts on various setups. ### Easy integration! Let's first have a look at an example:\n```diff\nimport torch\nimport torch.nn.functional as F\nfrom datasets import load\\_dataset\n+ from accelerate import Accelerator\n+ accelerator = Accelerator()\n- device = 'cpu'\n+ device = accelerator.device\nmodel = torch.nn.Transformer().to(device)\noptim = torch.optim.Adam(model.parameters())\ndataset = load\\_dataset('my\\_dataset')\ndata = torch.utils.data.DataLoader(dataset, shuffle=True)\n+ model, optim, data = accelerator.prepare(model, optim, data)\nmodel.train()\nfor epoch in range(10):\nfor source, targets in data:\nsource = source.to(device)\ntargets = targets.to(device)\noptimizer.zero\\_grad()\noutput = model(source)\nloss = F.cross\\_entropy(output, targets)\n- loss.backward()\n+ accelerator.backward(loss)\noptimizer.step()\n```\nBy just adding five lines of code to any standard PyTorch training script, you can now run said script on any kind of distributed setting, as well as with or without mixed precision. ðŸ¤— Accelerate even handles the device placement for you, so you can simplify the training loop above even further:\n```diff\nimport torch\nimport torch.nn.functional as F\nfrom datasets import load\\_dataset\n+ from accelerate import Accelerator\n+ accelerator = Accelerator()\n- device = 'cpu'\n- model = torch.nn.Transformer().to(device)\n+ model = torch.nn.Transformer()\noptim = torch.optim.Adam(model.parameters())\ndataset = load\\_dataset('my\\_dataset')\ndata = torch.utils.data.DataLoader(dataset, shuffle=True)\n+ model, optim, data = accelerator.prepare(model, optim, data)\nmodel.train()\nfor epoch in range(10):\nfor source, targets in data:\n- source = source.to(device)\n- targets = targets.to(device)\noptimizer.zero\\_grad()\noutput = model(source)\nloss = F.cross\\_entropy(output, targets)\n- loss.backward()\n+ accelerator.backward(loss)\noptimizer.step()\n```\nIn contrast, here are the changes needed to have this code run with distributed training are the followings:\n```diff\n+ import os\nimport torch\nimport torch.nn.functional as F\nfrom datasets import load\\_dataset\n+ from torch.utils.data import DistributedSampler\n+ from torch.nn.parallel import DistributedDataParallel\n+ local\\_rank = int(os.environ.get(\""LOCAL\\_RANK\"", -1))\n- device = 'cpu'\n+ device = device = torch.device(\""cuda\"", local\\_rank)\nmodel = torch.nn.Transformer().to(device)\n+ model = DistributedDataParallel(model)\noptim = torch.optim.Adam(model.parameters())\ndataset = load\\_dataset('my\\_dataset')\n+ sampler = DistributedSampler(dataset)\n- data = torch.utils.data.DataLoader(dataset, shuffle=True)\n+ data = torch.utils.data.DataLoader(dataset, sampler=sampler)\nmodel.train()\nfor epoch in range(10):\n+ sampler.set\\_epoch(epoch)\nfor source, targets in data:\nsource = source.to(device)\ntargets = targets.to(device)\noptimizer.zero\\_grad()\noutput = model(source)\nloss = F.cross\\_entropy(output, targets)\nloss.backward()\noptimizer.step()\n```\nThese changes will make your training script work for multiple GPUs, but your script will then stop working on CPU or one GPU (unless you start adding if statements everywhere).""]","You can force mixed precision training when initializing the Accelerator in ðŸ¤— Accelerate by passing the `fp16=True` argument to the `Accelerator()` initialization. Here is an example:

```python
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)
``` 

This will enable mixed precision training using float16.",By passing `fp16=True` to the Accelerator init.
"What is the purpose of tokenizers in the NLP pipeline?
","[""p align=\""center\"">\n\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\n\n![Build](https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg)\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\n[![Doc](https://docs.rs/tokenizers/badge.svg)](https://docs.rs/tokenizers/)\n\nThe core of `tokenizers`, written in Rust. Provides an implementation of today's most used tokenizers, with a focus on performance and\nversatility. ## What is a Tokenizer\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`. The various steps of the pipeline are:\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\nthe [unicode normalization standards](https://unicode.org/reports/tr15/#Norm\\_Forms), such as `NFD` or `NFKC`. More details about how to use the `Normalizers` are available on the\n[Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\nsplitting text is simply on whitespace. 3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\n`BPE` or `WordPiece`."", ""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a PyTorch tensor."", ""These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. Two common types of token classification are:\n\\* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names. \\* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb). ```py\n>>> from transformers import pipeline\n>>> classifier = pipeline(task=\""ner\"")\n>>> preds = classifier(\""Hugging Face is a French company based in New York City.\"")\n>>> preds = [\n... {\n... \""entity\"": pred[\""entity\""],\n... \""score\"": round(pred[\""score\""], 4),\n... \""index\"": pred[\""index\""],\n... \""word\"": pred[\""word\""],\n... \""start\"": pred[\""start\""],\n... \""end\"": pred[\""end\""],\n..."", ""n these few videos, we'll take a look at the tokenizers. In Natural Language Processing, most of the data that we handle consists of raw text. However, machine learning models cannot read and understand text in its raw form they can only work with numbers. The tokenizer's objective will be to translate the text into numbers. There are several possible approaches to this conversion, and the objective is to find the most meaningful representation. We'll take a look at three distinct tokenization algorithms. We compare them one to one, so we recommend you look at the videos in the following order: Word-based, Character-based, and Subword-based."", ""And with that, you now know how to train your very own tokenizers!"", ""he tokenization pipeline involves several steps that convert raw text into numbers. In this video, we will see what happens during the pre-tokenization step. The pre-tokenization operation is the operation performed after the normalization of the text and before the application of the tokenization algorithm. This step consists in applying rules that do not need to be learned to perform a first division of the text. Let's look at how several tokenizers pre\\_tokenize this example. The gpt 2 pretokenization divides the text on spaces and some punctuation - but the apostrophe is not a division criterion for example. We also notice that spaces have been replaced by a capital G with a dot above. Albert's pre-tokenization divides the text at the level of spaces, adds a space at the beginning of the sentence and replaces spaces with a special underscore. Finally, Bert's pre-tokenization divides the text at the level of punctuation and spaces. Unlike the previous tokenizers, spaces are not transformed and integrated to the tokens produced with this pre-tokenizer. Through these 3 examples, we could observe the two main types of operations brought by the pre-tokenization: some changes on the text and the division of the string into tokens that can be associated to words. Finally, the \""backend\\_tokenizer\"" of the fast tokenizers also allows to test the pre-tokenization operation very easily thanks to its \""pre\\_tokenize\\_str\"" method. We notice that the output of this operation is composed of both tokens and offsets which allow to link the token to its position in the text given in input of the method."", ""The algorithm simply picks the most\nlikely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their\nprobabilities. Those probabilities are defined by the loss the tokenizer is trained on. Assuming that the training data consists of\nthe words \\\\(x\\_{1}, \\dots, x\\_{N}\\\\) and that the set of all possible tokenizations for a word \\\\(x\\_{i}\\\\) is\ndefined as \\\\(S(x\\_{i})\\\\), then the overall loss is defined as\n$$\\mathcal{L} = -\\sum\\_{i=1}^{N} \\log \\left ( \\sum\\_{x \\in S(x\\_{i})} p(x) \\right )$$\n### SentencePiece\nAll tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to\nseparate words. However, not all languages use spaces to separate words. One possible solution is to use language\nspecific pre-tokenizers, \\*e.g.\\* [XLM](model\\_doc/xlm) uses a specific Chinese, Japanese, and Thai pre-tokenizer). To solve this problem more generally, [SentencePiece: A simple and language independent subword tokenizer and\ndetokenizer for Neural Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) treats the input\nas a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram\nalgorithm to construct the appropriate vocabulary. The [`XLNetTokenizer`] uses SentencePiece for example, which is also why in the example earlier the\n`\""â–\""` character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be\nconcatenated and `\""â–\""` is replaced by a space. All transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models\nusing SentencePiece are [ALBERT](model\\_doc/albert), [XLNet](model\\_doc/xlnet), [Marian](model\\_doc/marian), and [T5](model\\_doc/t5)."", ""FrameworkSwitchCourse {fw} />\n# Tokenizers[[tokenizers]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. In NLP tasks, the data that is generally processed is raw text. Here's an example of such text:\n```\nJim Henson was a puppeteer\n```\nHowever, models can only process numbers, so we need to find a way to convert the raw text to numbers. That's what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation â€” that is, the one that makes the most sense to the model â€” and, if possible, the smallest representation. Let's take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization. ## Word-based[[word-based]]\nThe first type of tokenizer that comes to mind is \\_word-based\\_. It's generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:\n\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg)\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg)\n\nThere are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python's `split()` function:\n```py\ntokenized\\_text = \""Jim Henson was a puppeteer\"".split()\nprint(tokenized\\_text)\n```\n```python out\n['Jim', 'Henson', 'was', 'a', 'puppeteer']\n```\nThere are also variations of word tokenizers that have extra rules for punctuation."", ""Building a tokenizer, block by block[[building-a-tokenizer-block-by-block]]\nAs we've seen in the previous sections, tokenization comprises several steps:\n- Normalization (any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.)\n- Pre-tokenization (splitting the input into words)\n- Running the input through the model (using the pre-tokenized words to produce a sequence of tokens)\n- Post-processing (adding the special tokens of the tokenizer, generating the attention mask and token type IDs)\nAs a reminder, here's another look at the overall process:\n\n![The tokenization pipeline.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg)\n![The tokenization pipeline.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg)\n\nThe ðŸ¤— Tokenizers library has been built to provide several options for each of those steps, which you can mix and match together."", ""he fast tokenizers of the Transformers library are fast, but they also implement features that will be super useful for data pre-processing and post-processing. Let's have a look at them! First, let's have a look at the usual output of a tokenizer. We get input IDs that correspond to tokens, but we lose a lot of information in the process. For instance, here the tokenization is the same for the two sentences, even if one has several more spaces than the other. Just having the input IDs is thus not enough if we want to match some tokens with a span of text (something we will need to do when tackling question answering for instance). It's also difficult to know when two tokens belong to the same word or not: it looks easy when you just look at the output of a BERT tokenizer, we just need to look for the ##. But other tokenizers have different ways to tokenize parts of words. For instance RoBERTa adds this special G symbol to mark the tokens at the beginning of a word, and T5 uses this special underscore symbol for the same purpose."", ""First, we `Prepend` character `\\_` to the input string. Then we replace all spaces with `\\_`. There's a huge list of potential operations, they can be applied to regular expression matches, and they have to be performed in a very specific order. The code in the `tokenizers` library takes care of all these details for all the models in the Hub. In contrast, projects that use language models in other domains, such as Swift apps, usually resort to hardcoding these decisions as part of the app's source code. This is fine for a couple of models, but then it's difficult to replace a model with a different one, and it's easy to make mistakes. What we are doing in `swift-transformers` is replicate those abstractions in Swift, so we write them once and everybody can use them in their apps. We are just getting started, so coverage is still small. Feel free to open issues in the repo or contribute your own! Specifically, we currently support BPE (Byte-Pair Encoding) tokenizers, one of the three main families in use today. The GPT models, Falcon and Llama, all use this method. Support for Unigram and WordPiece tokenizers will come later. We haven't ported all the possible normalizers, pre-tokenizers and post-processors - just the ones we encountered during our conversions of Llama 2, Falcon and GPT models. This is how to use the `Tokenizers` module in Swift:\n```swift\nimport Tokenizers\nfunc testTokenizer() async throws {\nlet tokenizer = try await AutoTokenizer.from(pretrained: \""pcuenq/Llama-2-7b-chat-coreml\"")\nlet inputIds = tokenizer(\""Today she took a train to the West\"")\nassert(inputIds == [1, 20628, 1183, 3614, 263, 7945, 304, 278, 3122])\n}\n```\nHowever, you don't usually need to tokenize the input text yourself - the [`Generation` code](https://github.com/huggingface/swift-transformers/blob/17d4bfae3598482fc7ecf1a621aa77ab586d379a/Sources/Generation/Generation.swift#L82) will take care of it."", ""With this kind of tokenizer, we can end up with some pretty large \""vocabularies,\"" where a vocabulary is defined by the total number of independent tokens that we have in our corpus. Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word. If we want to completely cover a language with a word-based tokenizer, we'll need to have an identifier for each word in the language, which will generate a huge amount of tokens. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we'd need to keep track of that many IDs. Furthermore, words like \""dog\"" are represented differently from words like \""dogs\"", and the model will initially have no way of knowing that \""dog\"" and \""dogs\"" are similar: it will identify the two words as unrelated. The same applies to other similar words, like \""run\"" and \""running\"", which the model will not see as being similar initially. Finally, we need a custom token to represent words that are not in our vocabulary. This is known as the \""unknown\"" token, often represented as \""[UNK]\"" or \""<unk>\"". It's generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn't able to retrieve a sensible representation of a word and you're losing information along the way. The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token. One way to reduce the amount of unknown tokens is to go one level deeper, using a \\_character-based\\_ tokenizer. ## Character-based[[character-based]]\nCharacter-based tokenizers split the text into characters, rather than words. This has two primary benefits:\n- The vocabulary is much smaller. - There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters. But here too some questions arise concerning spaces and punctuation:\n\n![An example of character-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg)\n![An example of character-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg)\n\nThis approach isn't perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, it's less meaningful: each character doesn't mean a lot on its own, whereas that is the case with words. However, this again differs according to the language; in Chinese, for example, each character carries more information than a character in a Latin language. Another thing to consider is that we'll end up with a very large amount of tokens to be processed by our model: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters. To get the best of both worlds, we can use a third technique that combines the two approaches: \\*subword tokenization\\*. ## Subword tokenization[[subword-tokenization]]\nSubword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. For instance, \""annoyingly\"" might be considered a rare word and could be decomposed into \""annoying\"" and \""ly\"". These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of \""annoyingly\"" is kept by the composite meaning of \""annoying\"" and \""ly\"". Here is an example showing how a subword tokenization algorithm would tokenize the sequence \""Let's do tokenization!\"":\n\n![A subword tokenization algorithm.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg)\n![A subword tokenization algorithm.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg)\n\nThese subwords end up providing a lot of semantic meaning: for instance, in the example above \""tokenization\"" was split into \""token\"" and \""ization\"", two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word)."", ""## Pre-Tokenization\nPre-tokenization is the act of splitting a text into smaller objects\nthat give an upper bound to what your tokens will be at the end of\ntraining. A good way to think of this is that the pre-tokenizer will\nsplit your text into \""words\"" and then, your final tokens will be parts\nof those words. An easy way to pre-tokenize inputs is to split on spaces and\npunctuations, which is done by the\n`pre\\_tokenizers.Whitespace`\npre-tokenizer:\n\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_pipeline.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START setup\\_pre\\_tokenizer\"",\n\""end-before\"": \""END setup\\_pre\\_tokenizer\"",\n\""dedent\"": 8}\n\n{\""path\"": \""../../tokenizers/tests/documentation.rs\"",\n\""language\"": \""rust\"",\n\""start-after\"": \""START pipeline\\_setup\\_pre\\_tokenizer\"",\n\""end-before\"": \""END pipeline\\_setup\\_pre\\_tokenizer\"",\n\""dedent\"": 4}\n\n{\""path\"": \""../../bindings/node/examples/documentation/pipeline.test.ts\"",\n\""language\"": \""js\"",\n\""start-after\"": \""START setup\\_pre\\_tokenizer\"",\n\""end-before\"": \""END setup\\_pre\\_tokenizer\"",\n\""dedent\"": 8}\nThe output is a list of tuples, with each tuple containing one word and\nits span in the original sentence (which is used to determine the final\n`offsets` of our `Encoding`). Note that splitting on\npunctuation will split contractions like `\""I'm\""` in this example. You can combine together any `PreTokenizer` together. For instance, here is a pre-tokenizer that will\nsplit on space, punctuation and digits, separating numbers in their\nindividual digits:\n\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_pipeline.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START combine\\_pre\\_tokenizer\"",\n\""end-before\"": \""END combine\\_pre\\_tokenizer\"",\n\""dedent\"": 8}\n\n{\""path\"": \""../../tokenizers/tests/documentation.rs\"",\n\""language\"": \""rust\"",\n\""start-after\"": \""START pipeline\\_combine\\_pre\\_tokenizer\"",\n\""end-before\"": \""END pipeline\\_combine\\_pre\\_tokenizer\"",\n\""dedent\"": 4}\n\n{\""path\"": \""../../bindings/node/examples/documentation/pipeline.test.ts\"",\n\""language\"": \""js\"",\n\""start-after\"": \""START combine\\_pre\\_tokenizer\"",\n\""end-before\"": \""END combine\\_pre\\_tokenizer\"",\n\""dedent\"": 8}\nAs we saw in the `quicktour`, you can\ncustomize the pre-tokenizer of a `Tokenizer` by just changing the corresponding attribute:\n\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_pipeline.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START replace\\_pre\\_tokenizer\"",\n\""end-before\"": \""END replace\\_pre\\_tokenizer\"",\n\""dedent\"": 8}\n\n{\""path\"": \""../../tokenizers/tests/documentation.rs\"",\n\""language\"": \""rust\"",\n\""start-after\"": \""START pipeline\\_replace\\_pre\\_tokenizer\"",\n\""end-before\"": \""END pipeline\\_replace\\_pre\\_tokenizer\"",\n\""dedent\"": 4}\n\n{\""path\"": \""../../bindings/node/examples/documentation/pipeline.test.ts\"",\n\""language\"": \""js\"",\n\""start-after\"": \""START replace\\_pre\\_tokenizer\"",\n\""end-before\"": \""END replace\\_pre\\_tokenizer\"",\n\""dedent\"": 8}\nOf course, if you change the way the pre-tokenizer, you should probably\nretrain your tokenizer from scratch afterward. ## Model\nOnce the input texts are normalized and pre-tokenized, the\n`Tokenizer` applies the model on the\npre-tokens."", ""he tokenizer pipeline. In this video, we'll look at how a tokenizer converts raw text to numbers that a Transformer model can make sense of, like when we execute this code. Here is a quick overview of what happens inside the tokenizer object: first the text is split into tokens, which are words, parts of words, or punctuation symbols. Then the tokenizer adds potential special tokens and converts each token to their unique respective ID as defined by the tokenizer's vocabulary. As we'll see it doesn't actually happen in this order, but viewing it like this is better for understanding what happens. The first step is to split our input text into tokens with the tokenize method. To do this, the tokenizer may first perform some operations like lowercasing all words, then follow a set of rules to split the result in small chunks of text. Most of the Transformers models use a subword tokenization algorithm, which means that one given word can be split in several tokens, like tokenize here. Look at the \""Tokenization algorithms\"" videos linked below for more information! The ## prefix we see in front of ize is the convention used by BERT to indicate this token is not the beginning of a word. Other tokenizers may use different conventions however: for instance ALBERT tokenizers will add a long underscore in front of all the tokens that had a space before them, which is a convention used by sentencepiece tokenizers."", ""Components\nWhen building a Tokenizer, you can attach various types of components to\nthis Tokenizer in order to customize its behavior. This page lists most\nprovided components. ## Normalizers\nA `Normalizer` is in charge of pre-processing the input string in order\nto normalize it as relevant for a given use case. Some common examples\nof normalization are the Unicode normalization algorithms (NFD, NFKD,\nNFC & NFKC), lowercasing etc... The specificity of `tokenizers` is that\nwe keep track of the alignment while normalizing. This is essential to\nallow mapping from the generated tokens back to the input text. The `Normalizer` is optional. | Name | Description | Example |\n| :--- | :--- | :--- |\n| NFD | NFD unicode normalization | |\n| NFKD | NFKD unicode normalization | |\n| NFC | NFC unicode normalization | |\n| NFKC | NFKC unicode normalization | |\n| Lowercase | Replaces all uppercase to lowercase | Input: `HELLO á½ˆÎ”Î¥Î£Î£Î•ÎŽÎ£`   \n Output: `hello`á½€Î´Ï…ÏƒÏƒÎµÏÏ‚` |\n| Strip | Removes all whitespace characters on the specified sides (left, right or both) of the input | Input: `\""`hi`\""`   \n Output: `\""hi\""` |\n| StripAccents | Removes all accent symbols in unicode (to be used with NFD for consistency) | Input: `Ã©`   \n Ouput: `e` |\n| Replace | Replaces a custom string or regexp and changes it with given content | `Replace(\""a\"", \""e\"")` will behave like this:   \n Input: `\""banana\""`   \n Ouput: `\""benene\""` |\n| BertNormalizer | Provides an implementation of the Normalizer used in the original BERT. Options that can be set are:\n\n* clean\\_text\n* handle\\_chinese\\_chars\n* strip\\_accents\n* lowercase\n\n| |\n| Sequence | Composes multiple normalizers that will run in the provided order | `Sequence([NFKC(), Lowercase()])` |\n\n| Name | Description | Example |\n| :--- | :--- | :--- |\n| NFD | NFD unicode normalization | |\n| NFKD | NFKD unicode normalization | |\n| NFC | NFC unicode normalization | |\n| NFKC | NFKC unicode normalization | |\n| Lowercase | Replaces all uppercase to lowercase | Input: `HELLO á½ˆÎ”Î¥Î£Î£Î•ÎŽÎ£`   \n Output: `hello`á½€Î´Ï…ÏƒÏƒÎµÏÏ‚` |\n| Strip | Removes all whitespace characters on the specified sides (left, right or both) of the input | Input: `\""`hi`\""`   \n Output: `\""hi\""` |\n| StripAccents | Removes all accent symbols in unicode (to be used with NFD for consistency) | Input: `Ã©`   \n Ouput: `e` |\n| Replace | Replaces a custom string or regexp and changes it with given content | `Replace(\""a\"", \""e\"")` will behave like this:   \n Input: `\""banana\""`   \n Ouput: `\""benene\""` |\n| BertNormalizer | Provides an implementation of the Normalizer used in the original BERT. Options that can be set are:\n\n* clean\\_text\n* handle\\_chinese\\_chars\n* strip\\_accents\n* lowercase\n\n| |\n| Sequence | Composes multiple normalizers that will run in the provided order | `Sequence::new(vec![NFKC, Lowercase])` |\n\n| Name | Description | Example |\n| :--- | :--- | :--- |\n| NFD | NFD unicode normalization | |\n| NFKD | NFKD unicode normalization | |\n| NFC | NFC unicode normalization | |\n| NFKC | NFKC unicode normalization | |\n| Lowercase | Replaces all uppercase to lowercase | Input: `HELLO á½ˆÎ”Î¥Î£Î£Î•ÎŽÎ£`   \n Output: `hello`á½€Î´Ï…ÏƒÏƒÎµÏÏ‚` |\n| Strip | Removes all whitespace characters on the specified sides (left, right or both) of the input | Input: `\""`hi`\""`   \n Output: `\""hi\""` |\n| StripAccents | Removes all accent symbols in unicode (to be used with NFD for consistency) | Input: `Ã©`   \n Ouput: `e` |\n| Replace | Replaces a custom string or regexp and changes it with given content | `Replace(\""a\"", \""e\"")` will behave like this:   \n Input: `\""banana\""`   \n Ouput: `\""benene\""` |\n| BertNormalizer | Provides an implementation of the Normalizer used in the original BERT. Options that can be set are:\n\n* cleanText\n* handleChineseChars\n* stripAccents\n* lowercase\n\n| |\n| Sequence | Composes multiple normalizers that will run in the provided order | |\n## Pre-tokenizers\nThe `PreTokenizer` takes care of splitting the input according to a set\nof rules."", ""The tokenization pipeline\nWhen calling `Tokenizer.encode` or\n`Tokenizer.encode\\_batch`, the input\ntext(s) go through the following pipeline:\n- `normalization`\n- `pre-tokenization`\n- `model`\n- `post-processing`\nWe'll see in details what happens during each of those steps in detail,\nas well as when you want to `decode ` some token ids, and how the ðŸ¤— Tokenizers library allows you\nto customize each of those steps to your needs. If you're already\nfamiliar with those steps and want to learn by seeing some code, jump to\n`our BERT from scratch example `. For the examples that require a `Tokenizer` we will use the tokenizer we trained in the\n`quicktour`, which you can load with:\n\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_pipeline.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START reload\\_tokenizer\"",\n\""end-before\"": \""END reload\\_tokenizer\"",\n\""dedent\"": 12}\n\n{\""path\"": \""../../tokenizers/tests/documentation.rs\"",\n\""language\"": \""rust\"",\n\""start-after\"": \""START pipeline\\_reload\\_tokenizer\"",\n\""end-before\"": \""END pipeline\\_reload\\_tokenizer\"",\n\""dedent\"": 4}\n\n{\""path\"": \""../../bindings/node/examples/documentation/pipeline.test.ts\"",\n\""language\"": \""js\"",\n\""start-after\"": \""START reload\\_tokenizer\"",\n\""end-before\"": \""END reload\\_tokenizer\"",\n\""dedent\"": 8}\n## Normalization\nNormalization is, in a nutshell, a set of operations you apply to a raw\nstring to make it less random or \""cleaner\""."", ""!-- DISABLE-FRONTMATTER-SECTIONS -->\n# Tokenizers\nFast State-of-the-art tokenizers, optimized for both research and\nproduction\n[ðŸ¤— Tokenizers](https://github.com/huggingface/tokenizers) provides an\nimplementation of today's most used tokenizers, with a focus on\nperformance and versatility. These tokenizers are also used in [ðŸ¤— Transformers](https://github.com/huggingface/transformers). # Main features:\n- Train new vocabularies and tokenize, using today's most used tokenizers. - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server's CPU. - Easy to use, but also extremely versatile. - Designed for both research and production. - Full alignment tracking. Even with destructive normalization, it's always possible to get the part of the original sentence that corresponds to any token. - Does all the pre-processing: Truncation, Padding, add the special tokens your model needs."", ""p align=\""center\"">\n\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\n\n[![Build](https://badge.fury.io/js/tokenizers.svg)](https://badge.fury.io/js/tokenizers)\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\n\nNodeJS implementation of today's most used tokenizers, with a focus on performance and\nversatility. Bindings over the [Rust](https://github.com/huggingface/tokenizers/tree/master/tokenizers) implementation. If you are interested in the High-level design, you can go check it there. ## Main features\n- Train new vocabularies and tokenize using 4 pre-made tokenizers (Bert WordPiece and the 3\nmost common BPE versions). - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes\nless than 20 seconds to tokenize a GB of text on a server's CPU. - Easy to use, but also extremely versatile. - Designed for research and production. - Normalization comes with alignments tracking. It's always possible to get the part of the\noriginal sentence that corresponds to a given token. - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs. ## Installation\n```bash\nnpm install tokenizers@latest\n```\n## Basic example\n```ts\nimport { Tokenizer } from \""tokenizers\"";\nconst tokenizer = await Tokenizer.fromFile(\""tokenizer.json\"");\nconst wpEncoded = await tokenizer.encode(\""Who is John?\"");\nconsole.log(wpEncoded.getLength());\nconsole.log(wpEncoded.getTokens());\nconsole.log(wpEncoded.getIds());\nconsole.log(wpEncoded.getAttentionMask());\nconsole.log(wpEncoded.getOffsets());\nconsole.log(wpEncoded.getOverflowing());\nconsole.log(wpEncoded.getSpecialTokensMask());\nconsole.log(wpEncoded.getTypeIds());\nconsole.log(wpEncoded.getWordIds());\n```\n## License\n[Apache License 2.0](../../LICENSE)"", ""p align=\""center\"">\n\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\n\n![Build](https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg)\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue&cachedrop)](https://github.com/huggingface/tokenizers/blob/main/LICENSE)\n[![](https://pepy.tech/badge/tokenizers/week)](https://pepy.tech/project/tokenizers)\n\nProvides an implementation of today's most used tokenizers, with a focus on performance and\nversatility. ## Main features:\n- Train new vocabularies and tokenize, using today's most used tokenizers. - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes\nless than 20 seconds to tokenize a GB of text on a server's CPU. - Easy to use, but also extremely versatile. - Designed for research and production. - Normalization comes with alignments tracking. It's always possible to get the part of the\noriginal sentence that corresponds to a given token. - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs. ## Bindings\nWe provide bindings to the following languages (more to come!):\n- [Rust](https://github.com/huggingface/tokenizers/tree/main/tokenizers) (Original implementation)\n- [Python](https://github.com/huggingface/tokenizers/tree/main/bindings/python)\n- [Node.js](https://github.com/huggingface/tokenizers/tree/main/bindings/node)\n- [Ruby](https://github.com/ankane/tokenizers-ruby) (Contributed by @ankane, external repo)\n## Quick example using Python:\nChoose your model between Byte-Pair Encoding, WordPiece or Unigram and instantiate a tokenizer:\n```python\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\ntokenizer = Tokenizer(BPE())\n```\nYou can customize how pre-tokenization (e.g., splitting into words) is done:\n```python\nfrom tokenizers.pre\\_tokenizers import Whitespace\ntokenizer.pre\\_tokenizer = Whitespace()\n```\nThen training your tokenizer on a set of files just takes two lines of codes:\n```python\nfrom tokenizers.trainers import BpeTrainer\ntrainer = BpeTrainer(special\\_tokens=[\""[UNK]\"", \""[CLS]\"", \""[SEP]\"", \""[PAD]\"", \""[MASK]\""])\ntokenizer.train(files=[\""wiki.train.raw\"", \""wiki.valid.raw\"", \""wiki.test.raw\""], trainer=trainer)\n```\nOnce your tokenizer is trained, encode any text with just one line:\n```python\noutput = tokenizer.encode(\""Hello, y'all! How are you ðŸ˜ ?\"")\nprint(output.tokens)\n# [\""Hello\"", \"",\"", \""y\"", \""'\"", \""all\"", \""!\"", \""How\"", \""are\"", \""you\"", \""[UNK]\"", \""?\""]\n```\nCheck the [documentation](https://huggingface.co/docs/tokenizers/index)\nor the [quicktour](https://huggingface.co/docs/tokenizers/quicktour) to learn more!"", ""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a TensorFlow tensor.""]","The purpose of tokenizers in the NLP pipeline is to translate raw text into data that can be processed by machine learning models. Since models can only work with numbers, tokenizers convert text inputs into numerical data. This involves several steps, including normalization, pre-tokenization, and applying a tokenization model, ultimately resulting in a representation that the model can understand and work with. Tokenizers also handle tasks such as truncation, padding, and adding special tokens required by the model.",To translate text into data that can be processed by the model.
"What is the purpose of the Safety Checker in the Diffusers library?
","[""We will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback. ## Scope\nThe Diffusers community will apply the following ethical guidelines to the projectâ€™s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns. ## Ethical guidelines\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question. - \\*\\*Transparency\\*\\*: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions. - \\*\\*Consistency\\*\\*: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. - \\*\\*Simplicity\\*\\*: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the projectâ€™s goals lean and coherent. - \\*\\*Accessibility\\*\\*: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community. - \\*\\*Reproducibility\\*\\*: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library. - \\*\\*Responsibility\\*\\*: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers. ## Examples of implementations: Safety features and Mechanisms\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology."", ""\\*\\*Please consider the following guidelines when opening a new issue\\*\\*:\n- Make sure you have searched whether your issue has already been asked before (use the search bar on GitHub under Issues). - Please never report a new issue on another (related) issue. If another issue is highly related, please\nopen a new issue nevertheless and link to the related issue. - Make sure your issue is written in English. Please use one of the great, free online translation services, such as [DeepL](https://www.deepl.com/translator) to translate from your native language to English if you are not comfortable in English. - Check whether your issue might be solved by updating to the newest Diffusers version. Before posting your issue, please make sure that `python -c \""import diffusers; print(diffusers.\\_\\_version\\_\\_)\""` is higher or matches the latest Diffusers version. - Remember that the more effort you put into opening a new issue, the higher the quality of your answer will be and the better the overall quality of the Diffusers issues."", ""For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.\\*\\*\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!"", ""--\ntitle: Swift ðŸ§¨Diffusers - Fast Stable Diffusion for Mac\nthumbnail: /blog/assets/fast-mac-diffusers/thumbnail.png\nauthors:\n- user: pcuenq\n- user: reach-vb\n---\n# Swift ðŸ§¨Diffusers: Fast Stable Diffusion for Mac\nTransform your text into stunning images with ease using Diffusers for Mac, a native app powered by state-of-the-art diffusion models. It leverages a bouquet of SoTA Text-to-Image models contributed by the community to the Hugging Face Hub, and converted to Core ML for blazingly fast performance. Our latest version, 1.1, is now available on the [Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) with significant performance upgrades and user-friendly interface tweaks. It's a solid foundation for future feature updates. Plus, the app is fully open source with a permissive [license](https://github.com/huggingface/swift-coreml-diffusers/blob/main/LICENSE), so you can build on it too! Check out our GitHub repository at https://github.com/huggingface/swift-coreml-diffusers for more information. ![Screenshot showing Diffusers for Mac UI](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-mac-diffusers/UI.png)\n## What exactly is ðŸ§¨Diffusers for Mac anyway? The Diffusers app ([App Store](https://apps.apple.com/app/diffusers/id1666309574), [source code](https://github.com/huggingface/swift-coreml-diffusers)) is the Mac counterpart to our [ðŸ§¨`diffusers` library](https://github.com/huggingface/diffusers). This library is written in Python with PyTorch, and uses a modular design to train and run diffusion models."", ""Research projects\nThis folder contains various research projects using ðŸ§¨ Diffusers. They are not really maintained by the core maintainers of this library and often require a specific version of Diffusers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice. If you need help with any of those, please open an issue where you directly ping the author(s), as indicated at the top of the README of each folder."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\nWelcome to ðŸ§¨ Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how ðŸ§¨ Diffusers is meant to be used. You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want. After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications. Feel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers! Let's start diffusing!"", ""If you liked this topic and want to learn more, we recommend the following resources:\n- Our [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). - The [Getting Started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers\\_intro.ipynb) notebook, that gives a broader overview on Diffusion systems. - The [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) blog post. - Our [code in GitHub](https://github.com/huggingface/diffusers) where we'd be more than happy if you leave a â­ if `diffusers` is useful to you! ### Citation:\n```\n@article{patil2022stable,\nauthor = {Patil, Suraj and Cuenca, Pedro and Lambert, Nathan and von Platen, Patrick},\ntitle = {Stable Diffusion with ðŸ§¨ Diffusers},\njournal = {Hugging Face Blog},\nyear = {2022},\nnote = {[https://huggingface.co/blog/rlhf](https://huggingface.co/blog/stable\\_diffusion)},\n}\n```"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# ðŸ§¨ Diffusersâ€™ Ethical Guidelines\n## Preamble\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training. Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ€™ contributions, and usage of the Diffusers library. The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups."", ""!---\nCopyright 2022 - The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![](https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg)\n\n[![GitHub](https://img.shields.io/github/license/huggingface/datasets.svg?color=blue)](https://github.com/huggingface/diffusers/blob/main/LICENSE)\n[![GitHub release](https://img.shields.io/github/release/huggingface/diffusers.svg)](https://github.com/huggingface/diffusers/releases)\n[![GitHub release](https://static.pepy.tech/badge/diffusers/month)](https://pepy.tech/project/diffusers)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CODE_OF_CONDUCT.md)\n[![X account](https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&label=Follow%20%40diffuserslib)](https://twitter.com/diffuserslib)\n\nðŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, ðŸ¤— Diffusers is a modular toolbox that supports both."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# How to contribute to Diffusers ðŸ§¨\nWe â¤ï¸ contributions from the open-source community! Everyone is welcome, and all types of participation â€“not just codeâ€“ are valued and appreciated. Answering questions, helping others, reaching out, and improving the documentation are all immensely valuable to the community, so don't be afraid and get involved if you're up for it! Everyone is encouraged to start by saying ðŸ‘‹ in our public Discord channel. We discuss the latest trends in diffusion models, ask questions, show off personal projects, help each other with contributions, or just hang out â˜•. [![Join us on Discord](https://img.shields.io/discord/823813159592001537?color=5865F2&logo=Discord&logoColor=white)](https://discord.gg/G7tWnz98XR)\nWhichever way you choose to contribute, we strive to be part of an open, welcoming, and kind community. Please, read our [code of conduct](https://github.com/huggingface/diffusers/blob/main/CODE\\_OF\\_CONDUCT.md) and be mindful to respect it during your interactions. We also recommend you become familiar with the [ethical guidelines](https://huggingface.co/docs/diffusers/conceptual/ethical\\_guidelines) that guide our project and ask you to adhere to the same principles of transparency and responsibility. We enormously value feedback from the community, so please do not be afraid to speak up if you believe you have valuable feedback that can help improve the library - every message, comment, issue, and pull request (PR) is read and considered. ## Overview\nYou can contribute in many ways ranging from answering questions on issues to adding new diffusion models to\nthe core library. In the following, we give an overview of different ways to contribute, ranked by difficulty in ascending order."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n\n![](https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg)\n\n# Diffusers\nðŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, ðŸ¤— Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction). The library has three main components:\n- State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in ðŸ¤— Diffusers, check out the table in the pipeline [overview](api/pipelines/overview) for a complete list of available pipelines and the task they solve. - Interchangeable [noise schedulers](api/schedulers/overview) for balancing trade-offs between generation speed and quality. - Pretrained [models](api/models) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems. [Tutorials\n\nLearn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you're using ðŸ¤— Diffusers for the first time!](./tutorials/tutorial_overview)\n[How-to guides\n\nPractical guides for helping you load pipelines, models, and schedulers. You'll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.](./using-diffusers/loading_overview)\n[Conceptual guides\n\nUnderstand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.](./conceptual/philosophy)\n[Reference\n\nTechnical descriptions of how ðŸ¤— Diffusers classes and methods work.](./api/models/overview)"", ""\\*\\*Please consider the following guidelines when opening a new issue\\*\\*:\n- Make sure you have searched whether your issue has already been asked before (use the search bar on GitHub under Issues). - Please never report a new issue on another (related) issue. If another issue is highly related, please\nopen a new issue nevertheless and link to the related issue. - Make sure your issue is written in English. Please use one of the great, free online translation services, such as [DeepL](https://www.deepl.com/translator) to translate from your native language to English if you are not comfortable in English. - Check whether your issue might be solved by updating to the newest Diffusers version. Before posting your issue, please make sure that `python -c \""import diffusers; print(diffusers.\\_\\_version\\_\\_)\""` is higher or matches the latest Diffusers version. - Remember that the more effort you put into opening a new issue, the higher the quality of your answer will be and the better the overall quality of the Diffusers issues."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# How to contribute to Diffusers ðŸ§¨\nWe â¤ï¸ contributions from the open-source community! Everyone is welcome, and all types of participation â€“not just codeâ€“ are valued and appreciated. Answering questions, helping others, reaching out, and improving the documentation are all immensely valuable to the community, so don't be afraid and get involved if you're up for it! Everyone is encouraged to start by saying ðŸ‘‹ in our public Discord channel. We discuss the latest trends in diffusion models, ask questions, show off personal projects, help each other with contributions, or just hang out â˜•. [![Join us on Discord](https://img.shields.io/discord/823813159592001537?color=5865F2&logo=discord&logoColor=white)](https://Discord.gg/G7tWnz98XR)\nWhichever way you choose to contribute, we strive to be part of an open, welcoming, and kind community. Please, read our [code of conduct](https://github.com/huggingface/diffusers/blob/main/CODE\\_OF\\_CONDUCT.md) and be mindful to respect it during your interactions. We also recommend you become familiar with the [ethical guidelines](https://huggingface.co/docs/diffusers/conceptual/ethical\\_guidelines) that guide our project and ask you to adhere to the same principles of transparency and responsibility. We enormously value feedback from the community, so please do not be afraid to speak up if you believe you have valuable feedback that can help improve the library - every message, comment, issue, and pull request (PR) is read and considered. ## Overview\nYou can contribute in many ways ranging from answering questions on issues to adding new diffusion models to\nthe core library. In the following, we give an overview of different ways to contribute, ranked by difficulty in ascending order.""]","The Safety Checker in the Diffusers library is designed to mitigate potential ethical and social risks associated with diffusion technology. It helps to ensure that the generated content adheres to safety standards by filtering out harmful or inappropriate outputs, such as those that may contain explicit content or perpetuate harmful biases. This feature is part of the community's commitment to responsibility and transparency in the use of diffusion models.",The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.
"What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?
","[""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Interact with Discussions and Pull Requests\nThe `huggingface\\_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood. ## Retrieve Discussions and Pull Requests from the Hub\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(repo\\_id=\""bigscience/bloom\""):\n... print(f\""{discussion.num} - {discussion.title}, pr: {discussion.is\\_pull\\_request}\"")\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer\\_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]\n```\n`HfApi.get\\_repo\\_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(\n..."", ""-->\n# Git ä¸Ž HTTP èŒƒå¼\n`huggingface\\_hub`åº“æ˜¯ç”¨äºŽä¸ŽHugging Face Hubè¿›è¡Œäº¤äº’çš„åº“ï¼ŒHugging Face Hubæ˜¯ä¸€ç»„åŸºäºŽGitçš„å­˜å‚¨åº“ï¼ˆæ¨¡åž‹ã€æ•°æ®é›†æˆ–Spacesï¼‰ã€‚ä½¿ç”¨ `huggingface\\_hub`æœ‰ä¸¤ç§ä¸»è¦æ–¹å¼æ¥è®¿é—®Hubã€‚\nç¬¬ä¸€ç§æ–¹æ³•ï¼Œå³æ‰€è°“çš„â€œåŸºäºŽgitâ€çš„æ–¹æ³•ï¼Œç”±[`Repository`]ç±»é©±åŠ¨ã€‚è¿™ç§æ–¹æ³•ä½¿ç”¨äº†ä¸€ä¸ªåŒ…è£…å™¨ï¼Œå®ƒåœ¨ `git`å‘½ä»¤çš„åŸºç¡€ä¸Šå¢žåŠ äº†ä¸“é—¨ä¸ŽHubäº¤äº’çš„é¢å¤–å‡½æ•°ã€‚ç¬¬äºŒç§é€‰æ‹©ï¼Œç§°ä¸ºâ€œåŸºäºŽHTTPâ€çš„æ–¹æ³•ï¼Œæ¶‰åŠä½¿ç”¨[`HfApi`]å®¢æˆ·ç«¯è¿›è¡ŒHTTPè¯·æ±‚ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€çœ‹æ¯ç§æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚\n## å­˜å‚¨åº“ï¼šåŸºäºŽåŽ†å²çš„ Git æ–¹æ³•\næœ€åˆï¼Œ`huggingface\\_hub`ä¸»è¦å›´ç»• [`Repository`] ç±»æž„å»ºã€‚å®ƒä¸ºå¸¸è§çš„ `git` å‘½ä»¤ï¼ˆå¦‚ `\""git add\""`ã€`\""git commit\""`ã€`\""git push\""`ã€`\""git tag\""`ã€`\""git checkout\""` ç­‰ï¼‰æä¾›äº† Python åŒ…è£…å™¨\nè¯¥åº“è¿˜å¯ä»¥å¸®åŠ©è®¾ç½®å‡­æ®å’Œè·Ÿè¸ªå¤§åž‹æ–‡ä»¶ï¼Œè¿™äº›æ–‡ä»¶é€šå¸¸åœ¨æœºå™¨å­¦ä¹ å­˜å‚¨åº“ä¸­ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥åº“å…è®¸æ‚¨åœ¨åŽå°æ‰§è¡Œå…¶æ–¹æ³•ï¼Œä½¿å…¶åœ¨è®­ç»ƒæœŸé—´ä¸Šä¼ æ•°æ®å¾ˆæœ‰ç”¨ã€‚\nä½¿ç”¨ [`Repository`] çš„æœ€å¤§ä¼˜ç‚¹æ˜¯å®ƒå…è®¸ä½ åœ¨æœ¬åœ°æœºå™¨ä¸Šç»´æŠ¤æ•´ä¸ªå­˜å‚¨åº“çš„æœ¬åœ°å‰¯æœ¬ã€‚è¿™ä¹Ÿå¯èƒ½æ˜¯ä¸€ä¸ªç¼ºç‚¹ï¼Œå› ä¸ºå®ƒéœ€è¦ä½ ä¸æ–­æ›´æ–°å’Œç»´æŠ¤è¿™ä¸ªæœ¬åœ°å‰¯æœ¬ã€‚è¿™ç±»ä¼¼äºŽä¼ ç»Ÿè½¯ä»¶å¼€å‘ä¸­ï¼Œæ¯ä¸ªå¼€å‘äººå‘˜éƒ½ç»´æŠ¤è‡ªå·±çš„æœ¬åœ°å‰¯æœ¬ï¼Œå¹¶åœ¨å¼€å‘åŠŸèƒ½æ—¶æŽ¨é€æ›´æ”¹ã€‚ä½†æ˜¯ï¼Œåœ¨æœºå™¨å­¦ä¹ çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œè¿™å¯èƒ½å¹¶ä¸æ€»æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºç”¨æˆ·å¯èƒ½åªéœ€è¦ä¸‹è½½æŽ¨ç†æ‰€éœ€çš„æƒé‡ï¼Œæˆ–å°†æƒé‡ä»Žä¸€ç§æ ¼å¼è½¬æ¢ä¸ºå¦ä¸€ç§æ ¼å¼ï¼Œè€Œæ— éœ€å…‹éš†æ•´ä¸ªå­˜å‚¨åº“ã€‚\n## HfApi: ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ä¸”æ–¹ä¾¿çš„HTTPå®¢æˆ·ç«¯\n`HfApi` è¢«å¼€å‘ä¸ºæœ¬åœ° git å­˜å‚¨åº“çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå› ä¸ºæœ¬åœ° git å­˜å‚¨åº“åœ¨å¤„ç†å¤§åž‹æ¨¡åž‹æˆ–æ•°æ®é›†æ—¶å¯èƒ½ä¼šå¾ˆéº»çƒ¦ã€‚`HfApi` æä¾›ä¸ŽåŸºäºŽ git çš„æ–¹æ³•ç›¸åŒçš„åŠŸèƒ½ï¼Œä¾‹å¦‚ä¸‹è½½å’ŒæŽ¨é€æ–‡ä»¶ä»¥åŠåˆ›å»ºåˆ†æ”¯å’Œæ ‡ç­¾ï¼Œä½†æ— éœ€æœ¬åœ°æ–‡ä»¶å¤¹æ¥ä¿æŒåŒæ­¥ã€‚\n`HfApi`é™¤äº†æä¾› `git` å·²ç»æä¾›çš„åŠŸèƒ½å¤–ï¼Œè¿˜æä¾›å…¶ä»–åŠŸèƒ½ï¼Œä¾‹å¦‚ï¼š\n\\* ç®¡ç†å­˜å‚¨åº“\n\\* ä½¿ç”¨ç¼“å­˜ä¸‹è½½æ–‡ä»¶ä»¥è¿›è¡Œæœ‰æ•ˆçš„é‡å¤ä½¿ç”¨\n\\* åœ¨ Hub ä¸­æœç´¢å­˜å‚¨åº“å’Œå…ƒæ•°æ®\n\\* è®¿é—®ç¤¾åŒºåŠŸèƒ½ï¼Œå¦‚è®¨è®ºã€PRå’Œè¯„è®º\n\\* é…ç½®Spaces\n## æˆ‘åº”è¯¥ä½¿ç”¨ä»€ä¹ˆï¼Ÿä»¥åŠä½•æ—¶ä½¿ç”¨ï¼Ÿ\næ€»çš„æ¥è¯´ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œ`HTTP æ–¹æ³•`æ˜¯ä½¿ç”¨ huggingface\\_hub çš„æŽ¨èæ–¹æ³•ã€‚ä½†æ˜¯ï¼Œåœ¨ä»¥ä¸‹å‡ ç§æƒ…å†µä¸‹ï¼Œç»´æŠ¤æœ¬åœ° git å…‹éš†ï¼ˆä½¿ç”¨ `Repository`ï¼‰å¯èƒ½æ›´æœ‰ç›Šï¼š\nå¦‚æžœæ‚¨åœ¨æœ¬åœ°æœºå™¨ä¸Šè®­ç»ƒæ¨¡åž‹ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„ git å·¥ä½œæµç¨‹å¹¶å®šæœŸæŽ¨é€æ›´æ–°å¯èƒ½æ›´æœ‰æ•ˆã€‚`Repository` è¢«ä¼˜åŒ–ä¸ºæ­¤ç±»æƒ…å†µï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿåœ¨åŽå°è¿è¡Œã€‚\nå¦‚æžœæ‚¨éœ€è¦æ‰‹åŠ¨ç¼–è¾‘å¤§åž‹æ–‡ä»¶ï¼Œ`git `æ˜¯æœ€ä½³é€‰æ‹©ï¼Œå› ä¸ºå®ƒåªä¼šå°†æ–‡ä»¶çš„å·®å¼‚å‘é€åˆ°æœåŠ¡å™¨ã€‚ä½¿ç”¨ `HfAPI` å®¢æˆ·ç«¯ï¼Œæ¯æ¬¡ç¼–è¾‘éƒ½ä¼šä¸Šä¼ æ•´ä¸ªæ–‡ä»¶ã€‚è¯·è®°ä½ï¼Œå¤§å¤šæ•°å¤§åž‹æ–‡ä»¶æ˜¯äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå› æ­¤æ— æ³•ä»Ž git å·®å¼‚ä¸­å—ç›Šã€‚\nå¹¶éžæ‰€æœ‰ git å‘½ä»¤éƒ½é€šè¿‡ [`HfApi`] æä¾›ã€‚æœ‰äº›å¯èƒ½æ°¸è¿œä¸ä¼šè¢«å®žçŽ°ï¼Œä½†æˆ‘ä»¬ä¸€ç›´åœ¨åŠªåŠ›æ”¹è¿›å¹¶ç¼©å°å·®è·ã€‚å¦‚æžœæ‚¨æ²¡æœ‰çœ‹åˆ°æ‚¨çš„ç”¨ä¾‹è¢«è¦†ç›–ã€‚\nè¯·åœ¨[Github](https://github.com/huggingface/huggingface\\_hub)æ‰“å¼€ä¸€ä¸ª issueï¼æˆ‘ä»¬æ¬¢è¿Žåé¦ˆï¼Œä»¥å¸®åŠ©æˆ‘ä»¬ä¸Žæˆ‘ä»¬çš„ç”¨æˆ·ä¸€èµ·æž„å»º ðŸ¤— ç”Ÿæ€ç³»ç»Ÿã€‚"", ""Hugging Face Hub documentation\nThe Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. The Hub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine Learning. Are you ready to join the path towards open source Machine Learning? ðŸ¤—\n\nRepositories\n\n[Introduction](./repositories)\n[Getting Started](./repositories-getting-started)\n[Repository Settings](./repositories-settings)\n[Pull requests and Discussions](./repositories-pull-requests-discussions)\n[Notifications](./notifications)\n[Collections](./collections)\n[Webhooks](./webhooks)\n[Next Steps](./repositories-next-steps)\n[Licenses](./repositories-licenses)\n\nModels\n\n[Introduction](./models)\n[The Model Hub](./models-the-hub)\n[Model Cards](./model-cards)\n[Gated Models](./models-gated)\n[Uploading Models](./models-uploading)\n[Downloading Models](./models-downloading)\n[Libraries](./models-libraries)\n[Tasks](./models-tasks)\n[Widgets](./models-widgets)\n[Inference API](./models-inference)\n[Download Stats](./models-download-stats)\n\nDatasets\n\n[Introduction](./datasets)\n[Datasets Overview](./datasets-overview)\n[Dataset Cards](./datasets-cards)\n[Gated Datasets](./datasets-gated)\n[Uploading Datasets](./datasets-adding)\n[Downloading Datasets](./datasets-downloading)\n[Libraries](./datasets-libraries)\n[Dataset Viewer](./datasets-viewer)\n[Download Stats](./datasets-download-stats)\n[Data files Configuration](./datasets-data-files-configuration)\n\nSpaces\n\n[Introduction](./spaces)\n[Spaces Overview](./spaces-overview)\n[Gradio Spaces](./spaces-sdks-gradio)\n[Streamlit Spaces](./spaces-sdks-streamlit)\n[Static HTML Spaces](./spaces-sdks-static)\n[Docker Spaces](./spaces-sdks-docker)\n[Embed your Space](./spaces-embed)\n[Run with Docker](./spaces-run-with-docker)\n[Reference](./spaces-config-reference)\n[Changelog](./spaces-changelog)\n[Advanced Topics](./spaces-advanced)\n[Sign in with HF](./spaces-oauth)\n\nOther\n\n[Organizations](./organizations)\n[Enterprise Hub](./enterprise-hub)\n[Billing](./billing)\n[Security](./security)\n[Moderation](./moderation)\n[Paper Pages](./paper-pages)\n[Search](./search)\n[Digital Object Identifier (DOI)](./doi)\n[Hub API Endpoints](./api)\n[Sign in with HF](./oauth)\n[Contributor Code of Conduct](https://huggingface.co/code-of-conduct)\n[Content Guidelines](https://huggingface.co/content-guidelines)\n\n## What's the Hugging Face Hub?"", ""Documentation for valid parameters and descriptions can be found [here](https://huggingface.co/docs/hub/endpoints). ### Advanced programmatic repository management\nThe `Repository` class helps manage both offline Git repositories and Hugging\nFace Hub repositories. Using the `Repository` class requires `git` and `git-lfs`\nto be installed. Instantiate a `Repository` object by calling it with a path to a local Git\nclone/repository:\n```python\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(\""//\"")\n```\nThe `Repository` takes a `clone\\_from` string as parameter. This can stay as\n`None` for offline management, but can also be set to any URL pointing to a Git\nrepo to clone that repository in the specified directory:\n```python\n>>> repo = Repository(\""huggingface-hub\"", clone\\_from=\""https://github.com/huggingface/huggingface\\_hub\"")\n```\nThe `clone\\_from` method can also take any Hugging Face model ID as input, and\nwill clone that repository:\n```python\n>>> repo = Repository(\""w2v2\"", clone\\_from=\""facebook/wav2vec2-large-960h-lv60\"")\n```\nIf the repository you're cloning is one of yours or one of your organisation's,\nthen having the ability to commit and push to that repository is important. In\norder to do that, you should make sure to be logged-in using `huggingface-cli\nlogin`, and to have the `token` parameter set to `True` (the default)\nwhen instantiating the `Repository` object:\n```python\n>>> repo = Repository(\""my-model\"", clone\\_from=\""/\"", token=True)\n```\nThis works for models, datasets and spaces repositories; but you will need to\nexplicitely specify the type for the last two options:\n```python\n>>> repo = Repository(\""my-dataset\"", clone\\_from=\""/\"", token=True, repo\\_type=\""dataset\"")\n```\nYou can also change between branches:\n```python\n>>> repo = Repository(\""huggingface-hub\"", clone\\_from=\""/\"", revision='branch1')\n>>> repo.git\\_checkout(\""branch2\"")\n```\nThe `clone\\_from` method can also take any Hugging Face model ID as input, and\nwill clone that repository:\n```python\n>>> repo = Repository(\""w2v2\"", clone\\_from=\""facebook/wav2vec2-large-960h-lv60\"")\n```\nFinally, you can choose to specify the Git username and email attributed to that\nclone directly by using the `git\\_user` and `git\\_email` parameters."", ""--\ntitle: Introducing Pull Requests and Discussions ðŸ¥³\nthumbnail: /blog/assets/76\\_community\\_update/thumbnail.png\n---\n# Introducing Pull Requests and Discussions ðŸ¥³\n![Pull requests and discussions on the hub](assets/76\\_community\\_update/community-update.png)\nWe are thrilled to announce the release of our latest collaborative features: pull requests and discussions on the Hugging Face Hub! Pull requests and discussions are available today under the [community tab](https://huggingface.co/gpt2/discussions) for all repository types: models, datasets, and Spaces. Any member of the community can create and participate in discussions and pull requests, facilitating collaborations not only within teams, but also with everyone else in the community! It's the biggest update ever done to the Hub, and we can't wait to see the community members start collaborating with it ðŸ¤©. The new \""Community\"" tab also aligns with proposals in ethical ML throughout the years. Feedback and iterations have a central place in the development of ethical machine learning software. We really believe having it in the community's toolset will unlock new kinds of positive patterns in ML, collaborations, and progress. Some example use cases for discussions and pull requests:\n- Propose suggestions in model cards to improve disclosures of ethical biases. - Let users flag concerning generations of a given Space demo. - Provide a venue through which model and dataset authors can have a direct discussion with community members. - Allow others to improve your repositories! For example, users might want to provide TensorFlow weights! ## Discussions\n![Discussions on the Hugging Face Hub](assets/76\\_community\\_update/new-discussion.png)\n[Discussions](https://huggingface.co/gpt2/discussions?type=discussion) allow community members ask and answer questions as well as share their ideas and suggestions directly with the repository owners and the community. Anyone can create and participate in discussions in the community tab of a repository. ## Pull requests\n![Pull requests on the Hugging Face Hub](assets/76\\_community\\_update/new-pr.png)\n[Pull requests](https://huggingface.co/gpt2/discussions?type=pull\\_request) allow community members open, comment, merge, or close pull requests directly from the website. The easiest way to open a pull request is to use the \""Collaborate\"" button in the \""Files and versions\"" tab. It will let you do single file contributions very easily. Under the hood, our Pull requests do not use forks and branches, but instead, custom \""branches\"" called `refs` that are stored directly on the source repo. This approach to avoids the need to create a forks for each new version of the model/dataset. ## How is this different from other git hosts\nAt a high level, we aim to build a simpler version of other git hosts' (like GitHub's) PRs and Issues:\n- no forks are involved: contributors push to a special `ref` branch directly on the source repo\n- no hard distinction between issues and PRs: they are essentially the same so we display them in the same lists\n- streamlined for ML (i.e."", ""Next Steps\nThese next sections highlight features and additional information that you may find useful to make the most out of the Git repositories on the Hugging Face Hub. ## How to programmatically manage repositories\nHugging Face supports accessing repos with Python via the [`huggingface\\_hub` library](https://huggingface.co/docs/huggingface\\_hub/index). The operations that we've explored, such as downloading repositories and uploading files, are available through the library, as well as other useful functions! If you prefer to use git directly, please read the sections below. ## Learning more about Git\nA good place to visit if you want to continue learning about Git is [this Git tutorial](https://learngitbranching.js.org/). For even more background on Git, you can take a look at [GitHub's Git Guides](https://github.com/git-guides). ## How to use branches\nTo effectively use Git repos collaboratively and to work on features without releasing premature code you can use \\*\\*branches\\*\\*. Branches allow you to separate your \""work in progress\"" code from your \""production-ready\"" code, with the additional benefit of letting multiple people work on a project without frequently conflicting with each others' contributions. You can use branches to isolate experiments in their own branch, and even [adopt team-wide practices for managing branches](https://ericmjl.github.io/essays-on-data-science/workflow/gitflow/). To learn about Git branching, you can try out the [Learn Git Branching interactive tutorial](https://learngitbranching.js.org/). ## Using tags\nGit allows you to \\*tag\\* commits so that you can easily note milestones in your project. As such, you can use tags to mark commits in your Hub repos! To learn about using tags, you can visit [this DevConnected post](https://devconnected.com/how-to-create-git-tags/). Beyond making it easy to identify important commits in your repo's history, using Git tags also allows you to do A/B testing, [clone a repository at a specific tag](https://www.techiedelight.com/clone-specific-tag-with-git/), and more! The `huggingface\\_hub` library also supports working with tags, such as [downloading files from a specific tagged commit](https://huggingface.co/docs/huggingface\\_hub/main/en/how-to-downstream#hfhuburl)."", ""Pull requests and Discussions\nHub Pull requests and Discussions allow users to do community contributions to repositories. Pull requests and discussions work the same for all the repo types. At a high level, the aim is to build a simpler version of other git hosts' (like GitHub's) PRs and Issues:\n- no forks are involved: contributors push to a special `ref` branch directly on the source repo. - there's no hard distinction between discussions and PRs: they are essentially the same so they are displayed in the same lists. - they are streamlined for ML (i.e. models/datasets/spaces repos), not arbitrary repos. \\_Note, Pull Requests and discussions can be enabled or disabled from the [repository settings](./repositories-settings#disabling-discussions-pull-requests)\\_\n## List\nBy going to the community tab in any repository, you can see all Discussions and Pull requests. You can also filter to only see the ones that are open. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-list-dark.png)\n\n## View\nThe Discussion page allows you to see the comments from different users. If it's a Pull Request, you can see all the changes by going to the Files changed tab. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-view.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-view-dark.png)\n\n## Editing a Discussion / Pull request title\nIf you opened a PR or discussion, are the author of the repository, or have write access to it, you can edit the discussion title by clicking on the pencil button. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-edit-title.PNG)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-edit-title-dark.PNG)\n\n## Pin a Discussion / Pull Request\nIf you have write access to a repository, you can pin discussions and Pull Requests. Pinned discussions appear at the top of all the discussions. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pin.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pin-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pinned.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-pinned-dark.png)\n\n## Lock a Discussion / Pull Request\nIf you have write access to a repository, you can lock discussions or Pull Requests. Once a discussion is locked, previous comments are still visible and users won't be able to add new comments. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-lock.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-lock-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-locked.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-locked-dark.png)\n\n## Comment edition and moderation\nIf you wrote a comment or have write access to the repository, you can edit the content of the comment from the contextual menu in the top-right corner of the comment box. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-edit.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-menu-edit-dark.png)\n\nOnce the comment has been edited, a new link will appear above the comment. This link shows the edit history. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-link.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-link-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-history.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-edit-history-dark.png)\n\nYou can also hide a comment. Hiding a comment is irreversible, and nobody will be able to see its content nor edit it anymore. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-hidden.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-comment-hidden-dark.png)\n\nRead also [moderation](./moderation) to see how to report an abusive comment. ## Can I use Markdown and LaTeX in my comments and discussions? Yes! You can use Markdown to add formatting to your comments. Additionally, you can utilize LaTeX for mathematical typesetting, your formulas will be rendered with [KaTeX](https://katex.org/) before being parsed in Markdown."", ""p align=\""center\"">\n\n![huggingface_hub library logo](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingface_hub.svg)\n\n*The official Python client for the Huggingface Hub.*\n\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc)](https://huggingface.co/docs/huggingface_hub/en/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub/releases)\n[![PyPi version](https://img.shields.io/pypi/pyversions/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub)\n[![downloads](https://static.pepy.tech/badge/huggingface_hub/month)](https://pypi.org/project/huggingface-hub)\n[![Code coverage](https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL)](https://codecov.io/gh/huggingface/huggingface_hub)\n\n#### **English** | [Deutsch](https://github.com/huggingface/huggingface_hub/blob/main/README_de.md) | [à¤¹à¤¿à¤‚à¤¦à¥€](https://github.com/huggingface/huggingface_hub/blob/main/README_hi.md) | [í•œêµ­ì–´](https://github.com/huggingface/huggingface_hub/blob/main/README_ko.md) | [ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰](https://github.com/huggingface/huggingface_hub/blob/main/README_cn.md)\n\n---\n\\*\\*Documentation\\*\\*: <https://hf.co/docs/huggingface_hub>\n\\*\\*Source Code\\*\\*: <https://github.com/huggingface/huggingface_hub>\n---\n## Welcome to the huggingface\\_hub library\nThe `huggingface\\_hub` library allows you to interact with the [Hugging Face Hub](https://huggingface.co/), a platform democratizing open-source Machine Learning for creators and collaborators."", ""Repositories\nModels, Spaces, and Datasets are hosted on the Hugging Face Hub as [Git repositories](https://git-scm.com/about), which means that version control and collaboration are core elements of the Hub. In a nutshell, a repository (also known as a \\*\\*repo\\*\\*) is a place where code and assets can be stored to back up your work, share it with the community, and work in a team. In these pages, you will go over the basics of getting started with Git and interacting with repositories on the Hub. Once you get the hang of it, you can explore the best practices and next steps that we've compiled for effective repository usage. ## Contents\n- [Getting Started with Repositories](./repositories-getting-started)\n- [Settings](./repositories-settings)\n- [Pull Requests & Discussions](./repositories-pull-requests-discussions)\n- [Pull Requests advanced usage](./repositories-pull-requests-discussions#pull-requests-advanced-usage)\n- [Webhooks](./webhooks)\n- [Notifications](./notifications)\n- [Collections](./collections)\n- [Repository size recommendations](./repositories-recommendations)\n- [Next Steps](./repositories-next-steps)\n- [Licenses](./repositories-licenses)"", ""repo\\_id=\""username/repo\\_name\"",\n... metadata={\""tags\"": [\""computer-vision\"", \""awesome-model\""]},\n... create\\_pr=True,\n... )\n```\nYou can also use [`HfApi.create\\_discussion`] (respectively [`HfApi.create\\_pull\\_request`]) to create a Discussion (respectively a Pull Request) on a repo. Opening a Pull Request this way can be useful if you need to work on changes locally. Pull Requests opened this way will be in `\""draft\""` mode. ```python\n>>> from huggingface\\_hub import create\\_discussion, create\\_pull\\_request\n>>> create\\_discussion(\n... repo\\_id=\""username/repo-name\"",\n... title=\""Hi from the huggingface\\_hub library!\"",\n... token=\""\"",\n... )\nDiscussionWithDetails(...)\n>>> create\\_pull\\_request(\n... repo\\_id=\""username/repo-name\"",\n... title=\""Hi from the huggingface\\_hub library!\"",\n... token=\""\"",\n... )\nDiscussionWithDetails(..., is\\_pull\\_request=True)\n```\nManaging Pull Requests and Discussions can be done entirely with the [`HfApi`] class. For example:\n\\* [`comment\\_discussion`] to add comments\n\\* [`edit\\_discussion\\_comment`] to edit comments\n\\* [`rename\\_discussion`] to rename a Discussion or Pull Request\n\\* [`change\\_discussion\\_status`] to open or close a Discussion / Pull Request\n\\* [`merge\\_pull\\_request`] to merge a Pull Request\nVisit the [`HfApi`] documentation page for an exhaustive reference of all available methods. ## Push changes to a Pull Request\n\\*Coming soon !\\*\n## See also\nFor a more detailed reference, visit the [Discussions and Pull Requests](../package\\_reference/community) and the [hf\\_api](../package\\_reference/hf\\_api) documentation page."", ""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ðŸ¤— Hub client library\nThe `huggingface\\_hub` library allows you to interact with the [Hugging Face\nHub](https://hf.co), a machine learning platform for creators and collaborators. Discover pre-trained models and datasets for your projects or play with the hundreds of\nmachine learning apps hosted on the Hub. You can also create and share your own models\nand datasets with the community. The `huggingface\\_hub` library provides a simple way to\ndo all these things with Python. Read the [quick start guide](quick-start) to get up and running with the\n`huggingface\\_hub` library. You will learn how to download files from the Hub, create a\nrepository, and upload files to the Hub. Keep reading to learn more about how to manage\nyour repositories on the ðŸ¤— Hub, how to interact in discussions or even how to access\nthe Inference API. [How-to guides\n\nPractical guides to help you achieve a specific goal. Take a look at these guides to learn how to use huggingface\\_hub to solve real-world problems.](./guides/overview)\n[Reference\n\nExhaustive and technical description of huggingface\\_hub classes and methods.](./package_reference/overview)\n[Conceptual guides\n\nHigh-level explanations for building a better understanding of huggingface\\_hub philosophy.](./concepts/git_vs_http)\n\n## Contribute\nAll contributions to the `huggingface\\_hub` are welcomed and equally valued! ðŸ¤— Besides\nadding or fixing existing issues in the code, you can also help improve the\ndocumentation by making sure it is accurate and up-to-date, help answer questions on\nissues, and request new features you think will improve the library. Take a look at the\n[contribution\nguide](https://github.com/huggingface/huggingface\\_hub/blob/main/CONTRIBUTING.md) to\nlearn more about how to submit a new issue or feature request, how to submit a pull\nrequest, and how to test your contributions to make sure everything works as expected. Contributors should also be respectful of our [code of\nconduct](https://github.com/huggingface/huggingface\\_hub/blob/main/CODE\\_OF\\_CONDUCT.md) to\ncreate an inclusive and welcoming collaborative space for everyone."", ""However, in some cases you might be interested in having\na local copy of your repository and interact with it using the Git commands you are familiar with. The [`Repository`] class allows you to interact with files and repositories on the Hub with functions similar to Git commands. It is a wrapper over Git and Git-LFS methods to use the Git commands you already know and love. Before starting, please make sure you have Git-LFS installed (see [here](https://git-lfs.github.com/) for installation instructions). [`Repository`] is deprecated in favor of the http-based alternatives implemented in [`HfApi`]. Given its large adoption in legacy code, the complete removal of [`Repository`] will only happen in release `v1.0`. For more details, please read [this explanation page](./concepts/git\\_vs\\_http). ### Use a local repository\nInstantiate a [`Repository`] object with a path to a local repository:\n```py\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(local\\_dir=\""//\"")\n```\n### Clone\nThe `clone\\_from` parameter clones a repository from a Hugging Face repository ID to a local directory specified by the `local\\_dir` argument:\n```py\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(local\\_dir=\""w2v2\"", clone\\_from=\""facebook/wav2vec2-large-960h-lv60\"")\n```\n`clone\\_from` can also clone a repository using a URL:\n```py\n>>> repo = Repository(local\\_dir=\""huggingface-hub\"", clone\\_from=\""https://huggingface.co/facebook/wav2vec2-large-960h-lv60\"")\n```\nYou can combine the `clone\\_from` parameter with [`create\\_repo`] to create and clone a repository:\n```py\n>>> repo\\_url = create\\_repo(repo\\_id=\""repo\\_name\"")\n>>> repo = Repository(local\\_dir=\""repo\\_local\\_path\"", clone\\_from=repo\\_url)\n```\nYou can also configure a Git username and email to a cloned repository by specifying the `git\\_user` and `git\\_email` parameters when you clone a repository."", ""!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Interacting with Discussions and Pull Requests\nCheck the [`HfApi`] documentation page for the reference of methods enabling\ninteraction with Pull Requests and Discussions on the Hub. - [`get\\_repo\\_discussions`]\n- [`get\\_discussion\\_details`]\n- [`create\\_discussion`]\n- [`create\\_pull\\_request`]\n- [`rename\\_discussion`]\n- [`comment\\_discussion`]\n- [`edit\\_discussion\\_comment`]\n- [`change\\_discussion\\_status`]\n- [`merge\\_pull\\_request`]\n## Data structures\n[[autodoc]] Discussion\n[[autodoc]] DiscussionWithDetails\n[[autodoc]] DiscussionEvent\n[[autodoc]] DiscussionComment\n[[autodoc]] DiscussionStatusChange\n[[autodoc]] DiscussionCommit\n[[autodoc]] DiscussionTitleChange"", ""repo\\_id=\""bigscience/bloom\"",\n... author=\""ArthurZ\"",\n... discussion\\_type=\""pull\\_request\"",\n... discussion\\_status=\""open\"",\n... ):\n... print(f\""{discussion.num} - {discussion.title} by {discussion.author}, pr: {discussion.is\\_pull\\_request}\"")\n# 19 - Add Flax weights by ArthurZ, pr: True\n```\n`HfApi.get\\_repo\\_discussions` returns a [generator](https://docs.python.org/3.7/howto/functional.html#generators) that yields\n[`Discussion`] objects. To get all the Discussions in a single list, run:\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> discussions\\_list = list(get\\_repo\\_discussions(repo\\_id=\""bert-base-uncased\""))\n```\nThe [`Discussion`] object returned by [`HfApi.get\\_repo\\_discussions`] contains high-level overview of the\nDiscussion or Pull Request. You can also get more detailed information using [`HfApi.get\\_discussion\\_details`]:\n```python\n>>> from huggingface\\_hub import get\\_discussion\\_details\n>>> get\\_discussion\\_details(\n... repo\\_id=\""bigscience/bloom-1b3\"",\n... discussion\\_num=2\n... )\nDiscussionWithDetails(\nnum=2,\nauthor='cakiki',\ntitle='Update VRAM memory for the V100s',\nstatus='open',\nis\\_pull\\_request=True,\nevents=[\nDiscussionComment(type='comment', author='cakiki', ...),\nDiscussionCommit(type='commit', author='cakiki', summary='Update VRAM memory for the V100s', oid='1256f9d9a33fa8887e1c1bf0e09b4713da96773a', ...),\n],\nconflicting\\_files=[],\ntarget\\_branch='refs/heads/main',\nmerge\\_commit\\_oid=None,\ndiff='diff --git a/README.md b/README.md\\nindex a6ae3b9294edf8d0eda0d67c7780a10241242a7e..3a1814f212bc3f0d3cc8f74bdbd316de4ae7b9e3 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -132,7 +132,7 [...]',\n)\n```\n[`HfApi.get\\_discussion\\_details`] returns a [`DiscussionWithDetails`] object, which is a subclass of [`Discussion`]\nwith more detailed information about the Discussion or Pull Request. Information includes all the comments, status changes,\nand renames of the Discussion via [`DiscussionWithDetails.events`]. In case of a Pull Request, you can retrieve the raw git diff with [`DiscussionWithDetails.diff`]. All the commits of the\nPull Request are listed in [`DiscussionWithDetails.events`]. ## Create and edit a Discussion or Pull Request programmatically\nThe [`HfApi`] class also offers ways to create and edit Discussions and Pull Requests. You will need an [access token](https://huggingface.co/docs/hub/security-tokens) to create and edit Discussions\nor Pull Requests. The simplest way to propose changes on a repo on the Hub is via the [`create\\_commit`] API: just\nset the `create\\_pr` parameter to `True`. This parameter is also available on other methods that wrap [`create\\_commit`]:\n\\* [`upload\\_file`]\n\\* [`upload\\_folder`]\n\\* [`delete\\_file`]\n\\* [`delete\\_folder`]\n\\* [`metadata\\_update`]\n```python\n>>> from huggingface\\_hub import metadata\\_update\n>>> metadata\\_update(\n...""]",The `HfApi` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.,HfApi
"What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?
","[""--\ntitle: \""Introducing Skops\""\nthumbnail: /blog/assets/94\\_skops/introducing\\_skops.png\nauthors:\n- user: merve\n- user: adrin\n- user: BenjaminB\n---\n# Introducing Skops\n## Introducing Skops\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others. Let's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production. ```python\n# let's import the libraries first\nimport sklearn\nfrom sklearn.datasets import load\\_breast\\_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model\\_selection import train\\_test\\_split\n# Load the data and split\nX, y = load\\_breast\\_cancer(as\\_frame=True, return\\_X\\_y=True)\nX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(\nX, y, test\\_size=0.3, random\\_state=42\n)\n# Train the model\nmodel = DecisionTreeClassifier().fit(X\\_train, y\\_train)\n```\nYou can use any model filename and serialization method, like `pickle` or `joblib`.""]",The name of the new library introduced by Hugging Face for hosting scikit-learn models is Skops.,Skops
"What is the purpose of Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""However, we observed that fine-tuning the encoder produces better results. We experimented with this approach after seeing it used in other Dreambooth implementations, and the results are striking! Frozen text encoder\n![Frozen text encoder](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/f\\_froxen\\_encoder.jpg)\nFine-tuned text encoder\n![Fine-tuned text encoder](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/g\\_unfrozen\\_encoder.jpg)\nFine-tuning the text encoder produces the best results, especially with faces. It generates more realistic images, it's less prone to overfitting and it also achieves better prompt interpretability, being able to handle more complex prompts. ## Epilogue: Textual Inversion + Dreambooth\nWe also ran a final experiment where we combined [Textual Inversion](https://textual-inversion.github.io) with Dreambooth. Both techniques have a similar goal, but their approaches are different. In this experiment we first ran textual inversion for 2000 steps. From that model, we then ran Dreambooth for an additional 500 steps using a learning rate of `1e-6`. These are the results:\n![Textual Inversion + Dreambooth](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/h\\_textual\\_inversion\\_dreambooth.jpg)\nWe think the results are much better than doing plain Dreambooth but not as good as when we fine-tune the whole text encoder. It seems to copy the style of the training images a bit more, so it could be overfitting to them. We didn't explore this combination further, but it could be an interesting alternative to improve Dreambooth and still fit the process in a 16GB GPU. Feel free to explore and tell us about your results!"", ""ðŸŽ‰ To learn more about how to use your new model, the following guides may be helpful:\n- Learn how to [load Textual Inversion embeddings](../using-diffusers/loading\\_adapters) and also use them as negative embeddings. - Learn how to use [Textual Inversion](textual\\_inversion\\_inference) for inference with Stable Diffusion 1/2 and Stable Diffusion XL."", ""You need to duplicate it and assign a GPU so it runs fast. This process will save you from having to set up your own training environment and you'll be able to train your models in minutes! ## Other Methods\nThe quest for easy fine-tuning is not new. In addition to Dreambooth, [\\_textual inversion\\_](https://huggingface.co/docs/diffusers/main/en/training/text\\_inversion) is another popular method that attempts to teach new concepts to a trained Stable Diffusion Model. One of the main reasons for using Textual Inversion is that trained weights are also small and easy to share. However, they only work for a single subject (or a small handful of them), whereas LoRA can be used for general-purpose fine-tuning, meaning that it can be adapted to new domains or datasets. [Pivotal Tuning](https://arxiv.org/abs/2106.05744) is a method that tries to combine Textual Inversion with LoRA. First, you teach the model a new concept using Textual Inversion techniques, obtaining a new token embedding to represent it. Then, you train that token embedding using LoRA to get the best of both worlds. We haven't explored Pivotal Tuning with LoRA yet."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Textual Inversion\nTextual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder. [`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings. To learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading\\_adapters#textual-inversion) loading guide. ## TextualInversionLoaderMixin\n[[autodoc]] loaders.textual\\_inversion.TextualInversionLoaderMixin"", ""I.e. a few pictures of a person can be used to generate images of that person in different styles. ## Textual Inversion\n[Paper](https://arxiv.org/abs/2208.01618)\n[Textual Inversion](../training/text\\_inversion) fine-tunes a model to teach it about a new concept. I.e. a few pictures of a style of artwork can be used to generate images in that style. ## ControlNet\n[Paper](https://arxiv.org/abs/2302.05543)\n[ControlNet](../api/pipelines/controlnet) is an auxiliary network which adds an extra condition. There are 8 canonical pre-trained ControlNets trained on different conditionings such as edge detection, scribbles,\ndepth maps, and semantic segmentations. ## Prompt Weighting\n[Prompt weighting](../using-diffusers/weighted\\_prompts) is a simple technique that puts more attention weight on certain parts of the text\ninput. ## Custom Diffusion\n[Paper](https://arxiv.org/abs/2212.04488)\n[Custom Diffusion](../training/custom\\_diffusion) only fine-tunes the cross-attention maps of a pre-trained\ntext-to-image diffusion model. It also allows for additionally performing Textual Inversion. It supports\nmulti-concept training by design. Like DreamBooth and Textual Inversion, Custom Diffusion is also used to\nteach a pre-trained text-to-image diffusion model about new concepts to generate outputs involving the\nconcept(s) of interest. ## Model Editing\n[Paper](https://arxiv.org/abs/2303.08084)\nThe [text-to-image model editing pipeline](../api/pipelines/model\\_editing) helps you mitigate some of the incorrect implicit assumptions a pre-trained text-to-image\ndiffusion model might make about the subjects present in the input prompt."", ""This method works by training and finding new embeddings that represent the images you provide with a special word in the prompt. As a result, the diffusion model weights stay the same and the training process produces a relatively tiny (a few KBs) file. Because textual inversion creates embeddings, it cannot be used on its own like DreamBooth and requires another model. ```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16).to(\""cuda\"")\n```\nNow you can load the textual inversion embeddings with the [`~loaders.TextualInversionLoaderMixin.load\\_textual\\_inversion`] method and generate some images. Let's load the [sd-concepts-library/gta5-artwork](https://huggingface.co/sd-concepts-library/gta5-artwork) embeddings and you'll need to include the special word `` in your prompt to trigger it:\n```py\npipeline.load\\_textual\\_inversion(\""sd-concepts-library/gta5-artwork\"")\nprompt = \""A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration,  style\""\nimage = pipeline(prompt).images[0]\nimage\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_txt_embed.png)\n\nTextual inversion can also be trained on undesirable things to create \\*negative embeddings\\* to discourage a model from generating images with those undesirable things like blurry images or extra fingers on a hand. This can be an easy way to quickly improve your prompt. You'll also load the embeddings with [`~loaders.TextualInversionLoaderMixin.load\\_textual\\_inversion`], but this time, you'll need two more parameters:\n- `weight\\_name`: specifies the weight file to load if the file was saved in the ðŸ¤— Diffusers format with a specific name or if the file is stored in the A1111 format\n- `token`: specifies the special word to use in the prompt to trigger the embeddings\nLet's load the [sayakpaul/EasyNegative-test](https://huggingface.co/sayakpaul/EasyNegative-test) embeddings:\n```py\npipeline.load\\_textual\\_inversion(\n\""sayakpaul/EasyNegative-test\"", weight\\_name=\""EasyNegative.safetensors\"", token=\""EasyNegative\""\n)\n```\nNow you can use the `token` to generate an image with the negative embeddings:\n```py\nprompt = \""A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration, EasyNegative\""\nnegative\\_prompt = \""EasyNegative\""\nimage = pipeline(prompt, negative\\_prompt=negative\\_prompt, num\\_inference\\_steps=50).images[0]\nimage\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_neg_embed.png)\n\n## LoRA\n[Low-Rank Adaptation (LoRA)](https://huggingface.co/papers/2106.09685) is a popular training technique because it is fast and generates smaller file sizes (a couple hundred MBs)."", ""# Textual Inversion\n[Textual Inversion](https://hf.co/papers/2208.01618) is a training technique for personalizing image generation models with just a few example images of what you want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide. If you're training on a GPU with limited vRAM, you should try enabling the `gradient\\_checkpointing` and `mixed\\_precision` parameters in the training command. You can also reduce your memory footprint by using memory-efficient attention with [xFormers](../optimization/xformers). JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn't support gradient checkpointing or xFormers. With the same configuration and setup as PyTorch, the Flax training script should be at least ~70% faster! This guide will explore the [textual\\_inversion.py](https://github.com/huggingface/diffusers/blob/main/examples/textual\\_inversion/textual\\_inversion.py) script to help you become more familiar with it, and how you can adapt it for your own use-case. Before running the script, make sure you install the library from source:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install . ```\nNavigate to the example folder with the training script and install the required dependencies for the script you're using:\n\n```bash\ncd examples/textual\\_inversion\npip install -r requirements.txt\n```\n\n```bash\ncd examples/textual\\_inversion\npip install -r requirements\\_flax.txt\n```\n\nðŸ¤— Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the ðŸ¤— Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more. Initialize an ðŸ¤— Accelerate environment:\n```bash\naccelerate config\n```\nTo setup a default ðŸ¤— Accelerate environment without choosing any configurations:\n```bash\naccelerate config default\n```\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n```bash\nfrom accelerate.utils import write\\_basic\\_config\nwrite\\_basic\\_config()\n```\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create\\_dataset) guide to learn how to create a dataset that works with the training script."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Textual inversion\n[[open-in-colab]]\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer). This guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text\\_inversion) training guide. Import the necessary libraries:\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import make\\_image\\_grid\n```\n## Stable Diffusion 1 and 2\nPick a Stable Diffusion checkpoint and a pre-learned concept from the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer):\n```py\npretrained\\_model\\_name\\_or\\_path = \""runwayml/stable-diffusion-v1-5\""\nrepo\\_id\\_embeds = \""sd-concepts-library/cat-toy\""\n```\nNow you can load a pipeline, and pass the pre-learned concept to it:\n```py\npipeline = StableDiffusionPipeline.from\\_pretrained(\npretrained\\_model\\_name\\_or\\_path, torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\npipeline.load\\_textual\\_inversion(repo\\_id\\_embeds)\n```\nCreate a prompt with the pre-learned concept by using the special placeholder token ``, and choose the number of samples and rows of images you'd like to generate:\n```py\nprompt = \""a grafitti in a favela wall with a  on it\""\nnum\\_samples\\_per\\_row = 2\nnum\\_rows = 2\n```\nThen run the pipeline (feel free to adjust the parameters like `num\\_inference\\_steps` and `guidance\\_scale` to see how they affect image quality), save the generated images and visualize them with the helper function you created at the beginning:\n```py\nall\\_images = []\nfor \\_ in range(num\\_rows):\nimages = pipeline(prompt, num\\_images\\_per\\_prompt=num\\_samples\\_per\\_row, num\\_inference\\_steps=50, guidance\\_scale=7.5).images\nall\\_images.extend(images)\ngrid = make\\_image\\_grid(all\\_images, num\\_rows, num\\_samples\\_per\\_row)\ngrid\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/textual_inversion_inference.png)\n\n## Stable Diffusion XL\nStable Diffusion XL (SDXL) can also use textual inversion vectors for inference. In contrast to Stable Diffusion 1 and 2, SDXL has two text encoders so you'll need two textual inversion embeddings - one for each text encoder model. Let's download the SDXL textual inversion embeddings and have a closer look at it's structure:\n```py\nfrom huggingface\\_hub import hf\\_hub\\_download\nfrom safetensors.torch import load\\_file\nfile = hf\\_hub\\_download(\""dn118/unaestheticXL\"", filename=\""unaestheticXLv31.safetensors\"")\nstate\\_dict = load\\_file(file)\nstate\\_dict\n```\n```\n{'clip\\_g': tensor([[ 0.0077, -0.0112, 0.0065, ..., 0.0195, 0.0159, 0.0275],\n...,\n[-0.0170, 0.0213, 0.0143, ..., -0.0302, -0.0240, -0.0362]],\n'clip\\_l': tensor([[ 0.0023, 0.0192, 0.0213, ..., -0.0385, 0.0048, -0.0011],\n...,\n[ 0.0475, -0.0508, -0.0145, ..., 0.0070, -0.0089, -0.0163]],\n```\nThere are two tensors, `\""clip\\_g\""` and `\""clip\\_l\""`. `\""clip\\_g\""` corresponds to the bigger text encoder in SDXL and refers to\n`pipe.text\\_encoder\\_2` and `\""clip\\_l\""` refers to `pipe.text\\_encoder`. Now you can load each tensor separately by passing them along with the correct text encoder and tokenizer\nto [`~loaders.TextualInversionLoaderMixin.load\\_textual\\_inversion`]:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipe = AutoPipelineForText2Image.from\\_pretrained(\""stabilityai/stable-diffusion-xl-base-1.0\"", variant=\""fp16\"", torch\\_dtype=torch.float16)\npipe.to(\""cuda\"")\npipe.load\\_textual\\_inversion(state\\_dict[\""clip\\_g\""], token=\""unaestheticXLv31\"", text\\_encoder=pipe.text\\_encoder\\_2, tokenizer=pipe.tokenizer\\_2)\npipe.load\\_textual\\_inversion(state\\_dict[\""clip\\_l\""], token=\""unaestheticXLv31\"", text\\_encoder=pipe.text\\_encoder, tokenizer=pipe.tokenizer)\n# the embedding should be used as a negative embedding, so we pass it as a negative prompt\ngenerator = torch.Generator().manual\\_seed(33)\nimage = pipe(\""a woman standing in front of a mountain\"", negative\\_prompt=\""unaestheticXLv31\"", generator=generator).images[0]\nimage\n```"", ""Distillation for quantization on Textual Inversion models to personalize text2image\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images.\\_By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images\\_\nThe `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. We have enabled distillation for quantization in `textual\\_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method. ## Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n```bash\npip install -r requirements.txt\n```\n## Prepare Datasets\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n[![](https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg)](https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg)\n## Get a FP32 Textual Inversion model\nUse the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model. ```bash\nexport MODEL\\_NAME=\""CompVis/stable-diffusion-v1-4\""\nexport DATA\\_DIR=\""./dicoo\""\naccelerate launch textual\\_inversion.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--train\\_data\\_dir=$DATA\\_DIR \\\n--learnable\\_property=\""object\"" \\\n--placeholder\\_token=\""\"" --initializer\\_token=\""toy\"" \\\n--resolution=512 \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=4 \\\n--max\\_train\\_steps=3000 \\\n--learning\\_rate=5.0e-04 --scale\\_lr \\\n--lr\\_scheduler=\""constant\"" \\\n--lr\\_warmup\\_steps=0 \\\n--output\\_dir=\""dicoo\\_model\""\n```\n## Do distillation for quantization\nDistillation for quantization is a method that combines [intermediate layer knowledge distillation](https://github.com/intel/neural-compressor/blob/master/docs/source/distillation.md#intermediate-layer-knowledge-distillation) and [quantization aware training](https://github.com/intel/neural-compressor/blob/master/docs/source/quantization.md#quantization-aware-training) in the same training process to improve the performance of the quantized model."", ""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Training with Intel Extension for PyTorch\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \""textual\\_inversion.py\"". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual\\_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script. The example supports both single node and multi-node distributed training:\n### Single node training\n```bash\nexport MODEL\\_NAME=\""CompVis/stable-diffusion-v1-4\""\nexport DATA\\_DIR=\""path-to-dir-containing-dicoo-images\""\npython textual\\_inversion.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--train\\_data\\_dir=$DATA\\_DIR \\\n--learnable\\_property=\""object\"" \\\n--placeholder\\_token=\""\"" --initializer\\_token=\""toy\"" \\\n--seed=7 \\\n--resolution=512 \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=1 \\\n--max\\_train\\_steps=3000 \\\n--learning\\_rate=2.5e-03 --scale\\_lr \\\n--output\\_dir=\""textual\\_inversion\\_dicoo\""\n```\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. You may not get performance speedup without Bfloat16 support. ### Multi-node distributed training\nBefore running the scripts, make sure to install the library's training dependencies successfully:\n```bash\npython -m pip install oneccl\\_bind\\_pt==1.13 -f https://developer.intel.com/ipex-whl-stable-cpu\n```\n```bash\nexport MODEL\\_NAME=\""CompVis/stable-diffusion-v1-4\""\nexport DATA\\_DIR=\""path-to-dir-containing-dicoo-images\""\noneccl\\_bindings\\_for\\_pytorch\\_path=$(python -c \""from oneccl\\_bindings\\_for\\_pytorch import cwd; print(cwd)\"")\nsource $oneccl\\_bindings\\_for\\_pytorch\\_path/env/setvars.sh\npython -m intel\\_extension\\_for\\_pytorch.cpu.launch --distributed \\\n--hostfile hostfile --nnodes 2 --nproc\\_per\\_node 2 textual\\_inversion.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--train\\_data\\_dir=$DATA\\_DIR \\\n--learnable\\_property=\""object\"" \\\n--placeholder\\_token=\""\"" --initializer\\_token=\""toy\"" \\\n--seed=7 \\\n--resolution=512 \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=1 \\\n--max\\_train\\_steps=750 \\\n--learning\\_rate=2.5e-03 --scale\\_lr \\\n--output\\_dir=\""textual\\_inversion\\_dicoo\""\n```\nThe above is a simple distributed training usage on 2 nodes with 2 processes on each node."", ""# [Deprecated] Multi Token Textual Inversion\n\\*\\*IMPORTART: This research project is deprecated. Multi Token Textual Inversion is now supported natively in [the official textual inversion example](https://github.com/huggingface/diffusers/tree/main/examples/textual\\_inversion#running-locally-with-pytorch).\\*\\*\nThe author of this project is [Isamu Isozaki](https://github.com/isamu-isozaki) - please make sure to tag the author for issue and PRs as well as @patrickvonplaten. We add multi token support to textual inversion. I added\n1. num\\_vec\\_per\\_token for the number of used to reference that token\n2. progressive\\_tokens for progressively training the token from 1 token to 2 token etc\n3. progressive\\_tokens\\_max\\_steps for the max number of steps until we start full training\n4. vector\\_shuffle to shuffle vectors\nFeel free to add these options to your training! In practice num\\_vec\\_per\\_token around 10+vector shuffle works great! ## Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install . ```\nThen cd in the example folder and run\n```bash\npip install -r requirements.txt\n```\nAnd initialize an [ðŸ¤—Accelerate](https://github.com/huggingface/accelerate/) environment with:\n```bash\naccelerate config\n```\n### Cat toy example\nYou need to accept the model license before downloading or using the weights. In this example we'll use model version `v1-5`, so you'll need to visit [its card](https://huggingface.co/runwayml/stable-diffusion-v1-5), read the license and tick the checkbox if you agree. You have to be a registered user in ðŸ¤— Hugging Face Hub, and you'll also need to use an access token for the code to work. For more information on access tokens, please refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens). Run the following command to authenticate your token\n```bash\nhuggingface-cli login\n```\nIf you have already cloned the repo, then you won't need to go through these steps."", ""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install . ```\nThen cd in the example folder and run:\n```bash\npip install -r requirements.txt\n```\nAnd initialize an [ðŸ¤— Accelerate](https://github.com/huggingface/accelerate/) environment with:\n```bash\naccelerate config\n```\n### Cat toy example\nFirst, let's login so that we can upload the checkpoint to the Hub during training:\n```bash\nhuggingface-cli login\n```\nNow let's get our dataset. For this example we will use some cat images: https://huggingface.co/datasets/diffusers/cat\\_toy\\_example . Let's first download it locally:\n```py\nfrom huggingface\\_hub import snapshot\\_download\nlocal\\_dir = \""./cat\""\nsnapshot\\_download(\""diffusers/cat\\_toy\\_example\"", local\\_dir=local\\_dir, repo\\_type=\""dataset\"", ignore\\_patterns=\"".gitattributes\"")\n```\nThis will be our training data. Now we can launch the training using:\n\\*\\*\\_\\_\\_Note: Change the `resolution` to 768 if you are using the [stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2) 768x768 model.\\_\\_\\_\\*\\*\n```bash\nexport MODEL\\_NAME=\""runwayml/stable-diffusion-v1-5\""\nexport DATA\\_DIR=\""./cat\""\naccelerate launch textual\\_inversion.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--train\\_data\\_dir=$DATA\\_DIR \\\n--learnable\\_property=\""object\"" \\\n--placeholder\\_token=\""\"" \\\n--initializer\\_token=\""toy\"" \\\n--resolution=512 \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=4 \\\n--max\\_train\\_steps=3000 \\\n--learning\\_rate=5.0e-04 \\\n--scale\\_lr \\\n--lr\\_scheduler=\""constant\"" \\\n--lr\\_warmup\\_steps=0 \\\n--push\\_to\\_hub \\\n--output\\_dir=\""textual\\_inversion\\_cat\""\n```\nA full training run takes ~1 hour on one V100 GPU.""]","Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. It allows a model like Stable Diffusion to learn a new concept from just 3-5 sample images, enabling more control over the generated images and allowing users to tailor the model towards specific concepts. The file produced from training is extremely small (a few KBs), and the new embeddings can be loaded into the text encoder to generate images that reflect the learned concepts.",Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.
"What is the recommended multiple of batch size for fp16 data type on an A100 GPU?
","[""For instance, for fp16 data type a multiple of 8 is recommended, unless\nit's an A100 GPU, in which case use multiples of 64. For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup. ## Gradient Accumulation\nThe \\*\\*gradient accumulation\\*\\* method aims to calculate gradients in smaller increments instead of computing them for the\nentire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward\nand backward passes through the model and accumulating the gradients during the process. Once a sufficient number of\ngradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\nbecomes possible to increase the \\*\\*effective batch size\\*\\* beyond the limitations imposed by the GPU's memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can\nslow down the training process. You can enable gradient accumulation by adding the `gradient\\_accumulation\\_steps` argument to [`TrainingArguments`]:\n```py\ntraining\\_args = TrainingArguments(per\\_device\\_train\\_batch\\_size=1, gradient\\_accumulation\\_steps=4, \\*\\*default\\_args)\n```\nIn the above example, your effective batch size becomes 4."", ""It was run on a single NVIDIA A100-SXM4-80GB GPU with a prompt length of 512. The model we used was `meta-llama/Llama-2-13b-hf`. with batch size = 1:\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None |None|None |None |26.0 |36.958 |27.058 |29152.98 |\n|gptq |False |4 |128 |exllama|36.2 |33.711 |29.663 |10484.34 |\n|bitsandbytes|None |4|None |None |37.64 |52.00 |19.23 |11018.36 |\nwith batch size = 16:\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None |None|None |None |26.0 |69.94 |228.76 |53986.51 |\n|gptq |False |4 |128 |exllama|36.2 |95.41 |167.68 |34777.04 |\n|bitsandbytes|None |4|None |None |37.64 |113.98 |140.38 |35532.37 |\nFrom the benchmark, we can see that bitsandbyes and GPTQ are equivalent, with GPTQ being slightly faster for large batch size."", ""The impact of `fp16` on latency is less marked with `batch\\_size = 1`, but here it is of enormous interest as it can reduce latency by almost half, and almost double throughput! # Concluding remarks\nThis blog post showcased a few simple optimization tricks bundled in the ðŸ¤— ecosystem. Using anyone of these techniques, or a combination of all three, can greatly improve Bark inference speed and memory footprint. \\* You can use the large version of Bark without any performance degradation and a footprint of just 2GB instead of 5GB, 15% faster, \\*\\*using ðŸ¤— Better Transformer and CPU offload\\*\\*. \\* Do you prefer high throughput? \\*\\*Batch by 8 with ðŸ¤— Better Transformer and half-precision\\*\\*. \\* You can get the best of both worlds by using \\*\\*fp16, ðŸ¤— Better Transformer and CPU offload\\*\\*!"", ""Alternatively, use ðŸ¤— Accelerate to gain full control over the training loop. Find the ðŸ¤— Accelerate example\n[further down in this guide](#using-accelerate). While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can\nresult in a more pronounced training slowdown. Consider the following example. Let's say, the `per\\_device\\_train\\_batch\\_size=4`\nwithout gradient accumulation hits the GPU's limit. If you would like to train with batches of size 64, do not set the\n`per\\_device\\_train\\_batch\\_size` to 1 and `gradient\\_accumulation\\_steps` to 64. Instead, keep `per\\_device\\_train\\_batch\\_size=4`\nand set `gradient\\_accumulation\\_steps=16`. This results in the same effective batch size while making better use of\nthe available GPU resources. For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\nand [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957). ## Gradient Checkpointing\nSome large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used.""]","For fp16 data type on an A100 GPU, it is recommended to use multiples of 64.",64
"How do you run a Gradio Blocks app in reload mode using a Python IDE?
","[""Developing Faster with Auto-Reloading\n\\*\\*Prerequisite\\*\\*: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control). This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks. ## Why Auto-Reloading? When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes. To make it faster and more convenient to write your code, we've made it easier to \""reload\"" your Gradio apps instantly when you are developing in a \\*\\*Python IDE\\*\\* (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \""magic command\"" that allows you to re-run cells faster if you use \\*\\*Jupyter Notebooks\\*\\* (or any similar environment like Colab). This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster. ## Python IDE Reload ðŸ”¥\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Markdown(\""# Greetings from Gradio!\"")\ninp = gr.Textbox(placeholder=\""What is your name?\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""Welcome, {x}!\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`."", ""Gradio Demo: dataframe\\_block-ui-test\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\ncount = gr.Slider(minimum=1, maximum=10, step=1, label=\""count\"")\ndata = gr.DataFrame(\nheaders=[\""A\"", \""B\""], col\\_count=(2, \""fixed\""), type=\""array\"", interactive=True\n)\nbtn = gr.Button(value=\""click\"")\nbtn.click(\nfn=lambda cnt: [[str(2 \\* i), str(2 \\* i + 1)] for i in range(int(cnt))],\ninputs=[count],\noutputs=[data],\n)\ndemo.launch()\n```"", ""By default, the Gradio use UTF-8 encoding for scripts. \\*\\*For reload mode\\*\\*, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n1. Configure encoding declaration of python script, for example: `# -\\*- coding: cp1252 -\\*-`\n2. Confirm that your code editor has identified that encoding format. 3. Run like this: `gradio run.py --encoding cp1252`\nðŸ”¥ If your application accepts command line arguments, you can pass them in as well. Here's an example:\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# Greetings {args.name}!\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nWhich you could run like this: `gradio run.py --name Gretel`\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code."", ""WARNING: The --reload flag should not be used in production on Windows. ```\nè¿™é‡Œæœ€é‡è¦çš„ä¸€è¡Œæ˜¯ `æ­£åœ¨è§‚å¯Ÿ ...`ã€‚è¿™é‡Œå‘ç”Ÿçš„æƒ…å†µæ˜¯ Gradio å°†è§‚å¯Ÿ `run.py` æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ï¼Œå¦‚æžœæ–‡ä»¶å‘ç”Ÿæ›´æ”¹ï¼Œå®ƒå°†è‡ªåŠ¨ä¸ºæ‚¨é‡æ–°è¿è¡Œæ–‡ä»¶ã€‚å› æ­¤ï¼Œæ‚¨åªéœ€ä¸“æ³¨äºŽç¼–å†™ä»£ç ï¼ŒGradio æ¼”ç¤ºå°†è‡ªåŠ¨åˆ·æ–° ðŸ¥³\nâš ï¸ è­¦å‘Šï¼š`gradio` å‘½ä»¤ä¸ä¼šæ£€æµ‹ä¼ é€’ç»™ `launch()` æ–¹æ³•çš„å‚æ•°ï¼Œå› ä¸ºåœ¨é‡æ–°åŠ è½½æ¨¡å¼ä¸‹ä»Žæœªè°ƒç”¨ `launch()` æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œè®¾ç½® `launch()` ä¸­çš„ `auth` æˆ– `show\\_error` ä¸ä¼šåœ¨åº”ç”¨ç¨‹åºä¸­åæ˜ å‡ºæ¥ã€‚\nå½“æ‚¨ä½¿ç”¨é‡æ–°åŠ è½½æ¨¡å¼æ—¶ï¼Œè¯·è®°ä½ä¸€ä»¶é‡è¦çš„äº‹æƒ…ï¼šGradio ä¸“é—¨æŸ¥æ‰¾åä¸º `demo` çš„ Gradio Blocks/Interface æ¼”ç¤ºã€‚å¦‚æžœæ‚¨å°†æ¼”ç¤ºå‘½åä¸ºå…¶ä»–åç§°ï¼Œæ‚¨éœ€è¦åœ¨ä»£ç ä¸­çš„ç¬¬äºŒä¸ªå‚æ•°ä¸­ä¼ å…¥æ¼”ç¤ºçš„ FastAPI åº”ç”¨ç¨‹åºçš„åç§°ã€‚å¯¹äºŽ Gradio æ¼”ç¤ºï¼Œå¯ä»¥ä½¿ç”¨ `.app` å±žæ€§è®¿é—® FastAPI åº”ç”¨ç¨‹åºã€‚å› æ­¤ï¼Œå¦‚æžœæ‚¨çš„ `run.py` æ–‡ä»¶å¦‚ä¸‹æ‰€ç¤ºï¼š\n```python\nimport gradio as gr\nwith gr.Blocks() as my\\_demo:\ngr.Markdown(\""# æ¥è‡ªGradioçš„é—®å€™ï¼\"")\ninp = gr.Textbox(placeholder=\""æ‚¨å«ä»€ä¹ˆåå­—ï¼Ÿ\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""æ¬¢è¿Žï¼Œ{x}ï¼\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\nmy\\_demo.launch()\n```\né‚£ä¹ˆæ‚¨å¯ä»¥è¿™æ ·å¯åŠ¨å®ƒï¼š`gradio run.py my\\_demo.app`ã€‚\nGradioé»˜è®¤ä½¿ç”¨UTF-8ç¼–ç æ ¼å¼ã€‚å¯¹äºŽ\\*\\*é‡æ–°åŠ è½½æ¨¡å¼\\*\\*ï¼Œå¦‚æžœä½ çš„è„šæœ¬ä½¿ç”¨çš„æ˜¯é™¤UTF-8ä»¥å¤–çš„ç¼–ç ï¼ˆå¦‚GBKï¼‰ï¼š\n1. åœ¨Pythonè„šæœ¬çš„ç¼–ç å£°æ˜Žå¤„æŒ‡å®šä½ æƒ³è¦çš„ç¼–ç æ ¼å¼ï¼Œå¦‚ï¼š`# -\\*- coding: gbk -\\*-`\n2. ç¡®ä¿ä½ çš„ä»£ç ç¼–è¾‘å™¨è¯†åˆ«åˆ°è¯¥æ ¼å¼ã€‚\n3. æ‰§è¡Œï¼š`gradio run.py --encoding gbk`\nðŸ”¥ å¦‚æžœæ‚¨çš„åº”ç”¨ç¨‹åºæŽ¥å—å‘½ä»¤è¡Œå‚æ•°ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä¼ é€’å®ƒä»¬ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# æ¬¢è¿Ž {args.name}ï¼\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\næ‚¨å¯ä»¥åƒè¿™æ ·è¿è¡Œå®ƒï¼š`gradio run.py --name Gretel`\nä½œä¸ºä¸€ä¸ªå°æç¤ºï¼Œåªè¦æ›´æ”¹äº† `run.py` æºä»£ç æˆ– Gradio æºä»£ç ï¼Œè‡ªåŠ¨é‡æ–°åŠ è½½å°±ä¼šå‘ç”Ÿã€‚è¿™æ„å‘³ç€å¦‚æžœæ‚¨å†³å®š[ä¸º Gradio åšè´¡çŒ®](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md)ï¼Œè¿™å°†éžå¸¸æœ‰ç”¨ âœ…\n## Jupyter Notebook é­”æ³•å‘½ä»¤ðŸ”®\nå¦‚æžœæ‚¨ä½¿ç”¨ Jupyter Notebooksï¼ˆæˆ– Colab Notebooks ç­‰ï¼‰è¿›è¡Œå¼€å‘ï¼Œæˆ‘ä»¬ä¹Ÿä¸ºæ‚¨æä¾›äº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼\næˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ª \\*\\*magic command é­”æ³•å‘½ä»¤\\*\\*ï¼Œå¯ä»¥ä¸ºæ‚¨åˆ›å»ºå’Œè¿è¡Œä¸€ä¸ª Blocks æ¼”ç¤ºã€‚è¦ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œåœ¨ç¬”è®°æœ¬é¡¶éƒ¨åŠ è½½ gradio æ‰©å±•ï¼š\n`%load\\_ext gradio`\nç„¶åŽï¼Œåœ¨æ‚¨æ­£åœ¨å¼€å‘ Gradio æ¼”ç¤ºçš„å•å…ƒæ ¼ä¸­ï¼Œåªéœ€åœ¨é¡¶éƒ¨å†™å…¥é­”æ³•å‘½ä»¤\\*\\*`%%blocks`\\*\\*ï¼Œç„¶åŽåƒå¹³å¸¸ä¸€æ ·ç¼–å†™å¸ƒå±€å’Œç»„ä»¶ï¼š\n```py\n%%blocks\nimport gradio as gr\ngr.Markdown(\""# æ¥è‡ªGradioçš„é—®å€™ï¼\"")\ninp = gr.Textbox(placeholder=\""æ‚¨å«ä»€ä¹ˆåå­—ï¼Ÿ\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""æ¬¢è¿Žï¼Œ{x}ï¼\"",\ninputs=inp,\noutputs=out)\n```\nè¯·æ³¨æ„ï¼š\n- æ‚¨ä¸éœ€è¦æ”¾ç½®æ ·æ¿ä»£ç  `with gr.Blocks() as demo:` å’Œ `demo.launch()` â€” Gradio ä¼šè‡ªåŠ¨ä¸ºæ‚¨å®Œæˆï¼\n- æ¯æ¬¡é‡æ–°è¿è¡Œå•å…ƒæ ¼æ—¶ï¼ŒGradio éƒ½å°†åœ¨ç›¸åŒçš„ç«¯å£ä¸Šé‡æ–°å¯åŠ¨æ‚¨çš„åº”ç”¨ç¨‹åºï¼Œå¹¶ä½¿ç”¨ç›¸åŒçš„åº•å±‚ç½‘ç»œæœåŠ¡å™¨ã€‚è¿™æ„å‘³ç€æ‚¨å°†æ¯”æ­£å¸¸é‡æ–°è¿è¡Œå•å…ƒæ ¼æ›´å¿«åœ°çœ‹åˆ°å˜åŒ–ã€‚\nä¸‹é¢æ˜¯åœ¨ Jupyter Notebook ä¸­çš„ç¤ºä¾‹ï¼š\n![](https://i.ibb.co/nrszFws/Blocks.gif)\nðŸª„è¿™åœ¨ colab ç¬”è®°æœ¬ä¸­ä¹Ÿé€‚ç”¨ï¼[è¿™æ˜¯ä¸€ä¸ª colab ç¬”è®°æœ¬](https://colab.research.google.com/drive/1jUlX1w7JqckRHVE-nbDyMPyZ7fYD8488?authuser=1#scrollTo=zxHYjbCTTz\\_5)ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­çœ‹åˆ° Blocks é­”æ³•æ•ˆæžœã€‚å°è¯•è¿›è¡Œä¸€äº›æ›´æ”¹å¹¶é‡æ–°è¿è¡Œå¸¦æœ‰ Gradio ä»£ç çš„å•å…ƒæ ¼ï¼\nNotebook Magic çŽ°åœ¨æ˜¯ä½œè€…æž„å»º Gradio æ¼”ç¤ºçš„é¦–é€‰æ–¹å¼ã€‚æ— è®ºæ‚¨å¦‚ä½•ç¼–å†™ Python ä»£ç ï¼Œæˆ‘ä»¬éƒ½å¸Œæœ›è¿™ä¸¤ç§æ–¹æ³•éƒ½èƒ½ä¸ºæ‚¨æä¾›æ›´å¥½çš„ Gradio å¼€å‘ä½“éªŒã€‚\n---\n## ä¸‹ä¸€æ­¥\næ—¢ç„¶æ‚¨å·²ç»äº†è§£äº†å¦‚ä½•ä½¿ç”¨ Gradio å¿«é€Ÿå¼€å‘ï¼Œè¯·å¼€å§‹æž„å»ºè‡ªå·±çš„åº”ç”¨ç¨‹åºå§ï¼\nå¦‚æžœä½ æ­£åœ¨å¯»æ‰¾çµæ„Ÿï¼Œè¯·å°è¯•æµè§ˆå…¶ä»–äººç”¨ Gradio æž„å»ºçš„æ¼”ç¤ºï¼Œ[æµè§ˆ Hugging Face Spaces](http://hf.space/) ðŸ¤—"", ""This means you'll see your changes \\_much, much faster\\_ than if you were rerunning the cell normally. Here's what it looks like in a jupyter notebook:\n![](https://gradio-builds.s3.amazonaws.com/demo-files/jupyter\\_reload.gif)\nðŸª„ This works in colab notebooks too! [Here's a colab notebook](https://colab.research.google.com/drive/1zAuWoiTIb3O2oitbtVb2\\_ekv1K6ggtC1?usp=sharing) where you can see the Blocks magic in action. Try making some changes and re-running the cell with the Gradio code! The Notebook Magic is now the author's preferred way of building Gradio demos. Regardless of how you write Python code, we hope either of these methods will give you a much better development experience using Gradio. ---\n## Next Steps\nNow that you know how to develop quickly using Gradio, start building your own! If you are looking for inspiration, try exploring demos other people have built with Gradio, [browse public Hugging Face Spaces](http://hf.space/) ðŸ¤—"", ""Gradio Demo: blocks\\_js\\_load\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndef welcome(name):\nreturn f\""Welcome to Gradio, {name}!\""\njs = \""\""\""\nfunction createGradioAnimation() {\nvar container = document.createElement('div');\ncontainer.id = 'gradio-animation';\ncontainer.style.fontSize = '2em';\ncontainer.style.fontWeight = 'bold';\ncontainer.style.textAlign = 'center';\ncontainer.style.marginBottom = '20px';\nvar text = 'Welcome to Gradio!';\nfor (var i = 0; i < text.length; i++) {\n(function(i){\nsetTimeout(function(){\nvar letter = document.createElement('span');\nletter.style.opacity = '0';\nletter.style.transition = 'opacity 0.5s';\nletter.innerText = text[i];\ncontainer.appendChild(letter);\nsetTimeout(function() {\nletter.style.opacity = '1';\n}, 50);\n}, i \\* 250);\n})(i);\n}\nvar gradioContainer = document.querySelector('.gradio-container');\ngradioContainer.insertBefore(container, gradioContainer.firstChild);\nreturn 'Animation created';\n}\n\""\""\""\nwith gr.Blocks(js=js) as demo:\ninp = gr.Textbox(placeholder=\""What is your name?\"")\nout = gr.Textbox()\ninp.change(welcome, inp, out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Instead of doing this, you can run your code in \\*\\*reload mode\\*\\* by changing 1 word: `python` to `gradio`:\nIn the terminal, run `gradio run.py`. That's it! Now, you'll see that after you'll see something like this:\n```bash\nWatching: '/Users/freddy/sources/gradio/gradio', '/Users/freddy/sources/gradio/demo/'\nRunning on local URL: http://127.0.0.1:7860\n```\nThe important part here is the line that says `Watching...` What's happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically ðŸ¥³\nâš ï¸ Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show\\_error` in `launch()` will not be reflected in the app. There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n```python\nimport gradio as gr\nwith gr.Blocks() as my\\_demo:\ngr.Markdown(\""# Greetings from Gradio!\"")\ninp = gr.Textbox(placeholder=\""What is your name?\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""Welcome, {x}!\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\nmy\\_demo.launch()\n```\nThen you would launch it in reload mode like this: `gradio run.py my\\_demo`."", ""Gradio Demo: blocks\\_plug\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndef change\\_tab():\nreturn gr.Tabs(selected=2)\nidentity\\_demo, input\\_demo, output\\_demo = gr.Blocks(), gr.Blocks(), gr.Blocks()\nwith identity\\_demo:\ngr.Interface(lambda x: x, \""text\"", \""text\"")\nwith input\\_demo:\nt = gr.Textbox(label=\""Enter your text here\"")\nwith gr.Row():\nbtn = gr.Button(\""Submit\"")\nclr = gr.ClearButton(t)\nwith output\\_demo:\ngr.Textbox(\""This is a static output\"")\nwith gr.Blocks() as demo:\ngr.Markdown(\""Three demos in one!\"")\nwith gr.Tabs(selected=1) as tabs:\nwith gr.TabItem(\""Text Identity\"", id=0) as tab0:\ntab0.select(lambda: gr.Tabs(selected=0), None, tabs)\nidentity\\_demo.render()\nwith gr.TabItem(\""Text Input\"", id=1) as tab1:\ntab1.select(lambda: gr.Tabs(selected=1), None, tabs)\ninput\\_demo.render()\nwith gr.TabItem(\""Text Static\"", id=2) as tab2:\ntab2.select(lambda: gr.Tabs(selected=2), None, tabs)\noutput\\_demo.render()\nbtn = gr.Button(\""Change tab\"")\nbtn.click(inputs=None, outputs=tabs, fn=change\\_tab)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Meaning that this can be useful if you decide to [contribute to Gradio itself](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md) âœ…\n## Jupyter Notebook Magic ðŸ”®\nWhat about if you use Jupyter Notebooks (or Colab Notebooks, etc.) to develop code? We got something for you too! We've developed a \\*\\*magic command\\*\\* that will create and run a Blocks demo for you. To use this, load the gradio extension at the top of your notebook:\n`%load\\_ext gradio`\nThen, in the cell that you are developing your Gradio demo, simply write the magic command \\*\\*`%%blocks`\\*\\* at the top, and then write the layout and components like you would normally:\n```py\n%%blocks\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# Greetings {args.name}!\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\n```\nNotice that:\n- You do not need to launch your demo â€” Gradio does that for you automatically! - Every time you rerun the cell, Gradio will re-render your app on the same port and using the same underlying web server."", ""é€šè¿‡è‡ªåŠ¨é‡è½½å®žçŽ°æ›´å¿«çš„å¼€å‘\n\\*\\*å…ˆå†³æ¡ä»¶\\*\\*ï¼šæœ¬æŒ‡å—è¦æ±‚æ‚¨äº†è§£å—çš„çŸ¥è¯†ã€‚è¯·ç¡®ä¿[å…ˆé˜…è¯»å—æŒ‡å—](https://gradio.app/quickstart/#blocks-more-flexibility-and-control)ã€‚\næœ¬æŒ‡å—ä»‹ç»äº†è‡ªåŠ¨é‡æ–°åŠ è½½ã€åœ¨ Python IDE ä¸­é‡æ–°åŠ è½½ä»¥åŠåœ¨ Jupyter Notebooks ä¸­ä½¿ç”¨ gradio çš„æ–¹æ³•ã€‚\n## ä¸ºä»€ä¹ˆè¦ä½¿ç”¨è‡ªåŠ¨é‡è½½ï¼Ÿ\nå½“æ‚¨æž„å»º Gradio æ¼”ç¤ºæ—¶ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨ Blocks æž„å»ºæ—¶ï¼Œæ‚¨å¯èƒ½ä¼šå‘çŽ°åå¤è¿è¡Œä»£ç ä»¥æµ‹è¯•æ›´æ”¹å¾ˆéº»çƒ¦ã€‚\nä¸ºäº†æ›´å¿«é€Ÿã€æ›´ä¾¿æ·åœ°ç¼–å†™ä»£ç ï¼Œæˆ‘ä»¬å·²ç»ç®€åŒ–äº†åœ¨ \\*\\*Python IDE\\*\\*ï¼ˆå¦‚ VS Codeã€Sublime Textã€PyCharm ç­‰ï¼‰ä¸­å¼€å‘æˆ–ä»Žç»ˆç«¯è¿è¡Œ Python ä»£ç æ—¶â€œé‡æ–°åŠ è½½â€Gradio åº”ç”¨çš„æ–¹å¼ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªç±»ä¼¼çš„â€œé­”æ³•å‘½ä»¤â€ï¼Œä½¿æ‚¨å¯ä»¥æ›´å¿«é€Ÿåœ°é‡æ–°è¿è¡Œå•å…ƒæ ¼ï¼Œå¦‚æžœæ‚¨ä½¿ç”¨ Jupyter Notebooksï¼ˆæˆ–ç±»ä¼¼çš„çŽ¯å¢ƒï¼Œå¦‚ Colabï¼‰çš„è¯ã€‚\nè¿™ä¸ªç®€çŸ­çš„æŒ‡å—å°†æ¶µç›–è¿™ä¸¤ç§æ–¹æ³•ï¼Œæ‰€ä»¥æ— è®ºæ‚¨å¦‚ä½•ç¼–å†™ Python ä»£ç ï¼Œæ‚¨éƒ½å°†çŸ¥é“å¦‚ä½•æ›´å¿«åœ°æž„å»º Gradio åº”ç”¨ç¨‹åºã€‚\n## Python IDE é‡è½½ ðŸ”¥\nå¦‚æžœæ‚¨ä½¿ç”¨ Python IDE æž„å»º Gradio Blocksï¼Œé‚£ä¹ˆä»£ç æ–‡ä»¶ï¼ˆå‡è®¾å‘½åä¸º `run.py`ï¼‰å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Markdown(\""# æ¥è‡ªGradioçš„é—®å€™ï¼\"")\ninp = gr.Textbox(placeholder=\""æ‚¨å«ä»€ä¹ˆåå­—ï¼Ÿ\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""æ¬¢è¿Žï¼Œ{x}ï¼\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\né—®é¢˜åœ¨äºŽï¼Œæ¯å½“æ‚¨æƒ³è¦æ›´æ”¹å¸ƒå±€ã€äº‹ä»¶æˆ–ç»„ä»¶æ—¶ï¼Œéƒ½å¿…é¡»é€šè¿‡ç¼–å†™ `python run.py` æ¥å…³é—­å’Œé‡æ–°è¿è¡Œåº”ç”¨ç¨‹åºã€‚\nè€Œä¸æ˜¯è¿™æ ·åšï¼Œæ‚¨å¯ä»¥é€šè¿‡æ›´æ”¹ 1 ä¸ªå•è¯æ¥ä»¥\\*\\*é‡æ–°åŠ è½½æ¨¡å¼\\*\\*è¿è¡Œä»£ç ï¼šå°† `python` æ›´æ”¹ä¸º `gradio`ï¼š\nåœ¨ç»ˆç«¯ä¸­è¿è¡Œ `gradio run.py`ã€‚å°±æ˜¯è¿™æ ·ï¼\nçŽ°åœ¨ï¼Œæ‚¨å°†çœ‹åˆ°ç±»ä¼¼äºŽè¿™æ ·çš„å†…å®¹ï¼š\n```bash\nLaunching in \\*reload mode\\* on: http://127.0.0.1:7860 (Press CTRL+C to quit)\nWatching..."", ""Gradio Demo: blocks\\_scroll\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndemo = gr.Blocks()\nwith demo:\ninp = gr.Textbox(placeholder=\""Enter text.\"")\nscroll\\_btn = gr.Button(\""Scroll\"")\nno\\_scroll\\_btn = gr.Button(\""No Scroll\"")\nbig\\_block = gr.HTML(\""\""\""\""\""\"")\nout = gr.Textbox()\nscroll\\_btn.click(lambda x: x,\ninputs=inp,\noutputs=out,\nscroll\\_to\\_output=True)\nno\\_scroll\\_btn.click(lambda x: x,\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: code\\_component\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Code(\nvalue=\""\""\""def hello\\_world():\nreturn \""Hello, world!\""\nprint(hello\\_world())\""\""\"",\nlanguage=\""python\"",\ninteractive=True,\nshow\\_label=False,\n)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: theme\\_soft\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nimport time\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\ntextbox = gr.Textbox(label=\""Name\"")\nslider = gr.Slider(label=\""Count\"", minimum=0, maximum=100, step=1)\nwith gr.Row():\nbutton = gr.Button(\""Submit\"", variant=\""primary\"")\nclear = gr.Button(\""Clear\"")\noutput = gr.Textbox(label=\""Output\"")\ndef repeat(name, count):\ntime.sleep(3)\nreturn name \\* count\nbutton.click(repeat, [textbox, slider], output)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: blocks\\_static\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndemo = gr.Blocks()\nwith demo:\ngr.Image(\n\""https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=387&q=80\""\n)\ngr.Textbox(\""hi\"")\ngr.Number(3)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: hello\\_blocks\\_decorator\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\nname = gr.Textbox(label=\""Name\"")\noutput = gr.Textbox(label=\""Output Box\"")\ngreet\\_btn = gr.Button(\""Greet\"")\n@greet\\_btn.click(inputs=name, outputs=output)\ndef greet(name):\nreturn \""Hello \"" + name + \""!\""\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: blocks\\_layout\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndemo = gr.Blocks()\nwith demo:\nwith gr.Row():\ngr.Image(interactive=True, scale=2)\ngr.Image()\nwith gr.Row():\ngr.Textbox(label=\""Text\"")\ngr.Number(label=\""Count\"", scale=2)\ngr.Radio(choices=[\""One\"", \""Two\""])\nwith gr.Row():\ngr.Button(\""500\"", scale=0, min\\_width=500)\ngr.Button(\""A\"", scale=0)\ngr.Button(\""grow\"")\nwith gr.Row():\ngr.Textbox()\ngr.Textbox()\ngr.Button()\nwith gr.Row():\nwith gr.Row():\nwith gr.Column():\ngr.Textbox(label=\""Text\"")\ngr.Number(label=\""Count\"")\ngr.Radio(choices=[\""One\"", \""Two\""])\ngr.Image()\nwith gr.Column():\ngr.Image(interactive=True)\ngr.Image()\ngr.Image()\ngr.Textbox(label=\""Text\"")\ngr.Number(label=\""Count\"")\ngr.Radio(choices=[\""One\"", \""Two\""])\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: theme\\_extended\\_step\\_3\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nimport time\nwith gr.Blocks(\ntheme=gr.themes.Default(\nfont=[gr.themes.GoogleFont(\""Inconsolata\""), \""Arial\"", \""sans-serif\""]\n)\n) as demo:\ntextbox = gr.Textbox(label=\""Name\"")\nslider = gr.Slider(label=\""Count\"", minimum=0, maximum=100, step=1)\nwith gr.Row():\nbutton = gr.Button(\""Submit\"", variant=\""primary\"")\nclear = gr.Button(\""Clear\"")\noutput = gr.Textbox(label=\""Output\"")\ndef repeat(name, count):\ntime.sleep(3)\nreturn name \\* count\nbutton.click(repeat, [textbox, slider], output)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: hello\\_blocks\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndef greet(name):\nreturn \""Hello \"" + name + \""!\""\nwith gr.Blocks() as demo:\nname = gr.Textbox(label=\""Name\"")\noutput = gr.Textbox(label=\""Output Box\"")\ngreet\\_btn = gr.Button(\""Greet\"")\ngreet\\_btn.click(fn=greet, inputs=name, outputs=output, api\\_name=\""greet\"")\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: theme\\_extended\\_step\\_2\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nimport time\nwith gr.Blocks(theme=gr.themes.Default(spacing\\_size=\""sm\"", radius\\_size=\""none\"")) as demo:\ntextbox = gr.Textbox(label=\""Name\"")\nslider = gr.Slider(label=\""Count\"", minimum=0, maximum=100, step=1)\nwith gr.Row():\nbutton = gr.Button(\""Submit\"", variant=\""primary\"")\nclear = gr.Button(\""Clear\"")\noutput = gr.Textbox(label=\""Output\"")\ndef repeat(name, count):\ntime.sleep(3)\nreturn name \\* count\nbutton.click(repeat, [textbox, slider], output)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```""]","è¦åœ¨ Python IDE ä¸­ä»¥é‡æ–°åŠ è½½æ¨¡å¼è¿è¡Œ Gradio Blocks åº”ç”¨ç¨‹åºï¼Œæ‚¨åªéœ€å°†è¿è¡Œå‘½ä»¤ä»Ž `python run.py` æ›´æ”¹ä¸º `gradio run.py`ã€‚è¿™æ ·ï¼ŒGradio å°†è§‚å¯Ÿ `run.py` æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ï¼Œå¦‚æžœæ–‡ä»¶å‘ç”Ÿæ›´æ”¹ï¼Œå®ƒå°†è‡ªåŠ¨é‡æ–°è¿è¡Œæ–‡ä»¶ã€‚ä»¥ä¸‹æ˜¯å…·ä½“æ­¥éª¤ï¼š

1. ç¡®ä¿æ‚¨çš„ä»£ç æ–‡ä»¶ï¼ˆä¾‹å¦‚ `run.py`ï¼‰åŒ…å« Gradio Blocks çš„å®šä¹‰ã€‚
2. åœ¨ç»ˆç«¯ä¸­è¿è¡Œå‘½ä»¤ï¼š`gradio run.py`ã€‚
3. æ‚¨å°†çœ‹åˆ°ç±»ä¼¼äºŽä»¥ä¸‹å†…å®¹çš„è¾“å‡ºï¼š
   ```bash
   Watching: '/path/to/your/directory'
   Running on local URL: http://127.0.0.1:7860
   ```

è¿™æ ·ï¼Œæ‚¨å°±å¯ä»¥ä¸“æ³¨äºŽç¼–å†™ä»£ç ï¼Œè€Œ Gradio æ¼”ç¤ºå°†è‡ªåŠ¨åˆ·æ–°ã€‚è¯·æ³¨æ„ï¼Œ`gradio` å‘½ä»¤ä¸ä¼šæ£€æµ‹ä¼ é€’ç»™ `launch()` æ–¹æ³•çš„å‚æ•°ï¼Œå› ä¸ºåœ¨é‡æ–°åŠ è½½æ¨¡å¼ä¸‹ä»Žæœªè°ƒç”¨ `launch()` æ–¹æ³•ã€‚",Run `gradio run.py` in the terminal.
"How can you install the Hugging Face Unity API in your Unity project?
","[""--\ntitle: \""How to Install and Use the Hugging Face Unity API\""\nthumbnail: /blog/assets/124\\_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n# How to Install and Use the Hugging Face Unity API\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API. ## Installation\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif)\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens). 7. Test the API key by clicking `Test API key` in the API Wizard. 8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API\\_URL` field."", ""p align=\""center\"">\n\n![huggingface_hub library logo](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingface_hub.svg)\n\n*ê³µì‹ Huggingface Hub íŒŒì´ì¬ í´ë¼ì´ì–¸íŠ¸*\n\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc)](https://huggingface.co/docs/huggingface_hub/ko/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub/releases)\n[![PyPi version](https://img.shields.io/pypi/pyversions/huggingface_hub.svg)](https://github.com/huggingface/huggingface_hub)\n[![downloads](https://static.pepy.tech/badge/huggingface_hub/month)](https://pypi.org/project/huggingface-hub)\n[![Code coverage](https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL)](https://codecov.io/gh/huggingface/huggingface_hub)\n\n#### [English](https://github.com/huggingface/huggingface_hub/blob/main/README.md) | [Deutsch](https://github.com/huggingface/huggingface_hub/blob/main/README_de.md) | [à¤¹à¤¿à¤‚à¤¦à¥€](https://github.com/huggingface/huggingface_hub/blob/main/README_hi.md) | **í•œêµ­ì–´**| [ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰](https://github.com/huggingface/huggingface_hub/blob/main/README_cn.md)\n\n---\n\\*\\*ê¸°ìˆ  ë¬¸ì„œ\\*\\*: <https://hf.co/docs/huggingface_hub>\n\\*\\*ì†ŒìŠ¤ ì½”ë“œ\\*\\*: <https://github.com/huggingface/huggingface_hub>\n---\n## huggingface\\_hub ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°œìš”\n`huggingface\\_hub` ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” [Hugging Face Hub](https://huggingface.co/)ì™€ ìƒí˜¸ìž‘ìš©í•  ìˆ˜ ìžˆê²Œ í•´ì¤ë‹ˆë‹¤."", ""9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png)\nNow that the API is set up, you can make calls from your scripts to the API. Let's look at an example of performing a Sentence Similarity task:\n```\nusing HuggingFace.API;\n/\\* other code \\*/\n// Make a call to the API\nvoid Query() {\nstring inputText = \""I'm on my way to the forest.\"";\nstring[] candidates = {\n\""The player is going to the city\"",\n\""The player is going to the wilderness\"",\n\""The player is wandering aimlessly\""\n};\nHuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\n}\n// If successful, handle the result\nvoid OnSuccess(float[] result) {\nforeach(float value in result) {\nDebug.Log(value);\n}\n}\n// Otherwise, handle the error\nvoid OnError(string error) {\nDebug.LogError(error);\n}\n/\\* other code \\*/\n```\n## Supported Tasks and Custom Models\nThe Hugging Face Unity API also currently supports the following tasks:\n- [Conversation](https://huggingface.co/tasks/conversational)\n- [Text Generation](https://huggingface.co/tasks/text-generation)\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\n- [Text Classification](https://huggingface.co/tasks/text-classification)\n- [Question Answering](https://huggingface.co/tasks/question-answering)\n- [Translation](https://huggingface.co/tasks/translation)\n- [Summarization](https://huggingface.co/tasks/summarization)\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.""]","To install the Hugging Face Unity API in your Unity project, follow these steps:

1. Open your Unity project.
2. Go to `Window` -> `Package Manager`.
3. Click `+` and select `Add Package from git URL`.
4. Enter `https://github.com/huggingface/unity-api.git`.
5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.
6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).
7. Test the API key by clicking `Test API key` in the API Wizard.
8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.
9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`.
10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard. 

After completing these steps, you can make calls from your scripts to the API.","To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`."
"What is the pretraining objective of the Wav2Vec2 context network?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Wav2Vec2\n## Overview\nThe Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. The abstract from the paper is the following:\n\\*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\nrecognition with limited amounts of labeled data.\\*\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). ## Usage tips\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\nusing [`Wav2Vec2CTCTokenizer`]. ## Resources\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with Wav2Vec2."", ""The results of this run can be seen [here](https://wandb.ai/patrickvonplaten/pretraining-wav2vec2/reports/Wav2Vec2-Large--VmlldzoxMTAwODM4?accessToken=wm3qzcnldrwsa31tkvf2pdmilw3f63d4twtffs86ou016xjbyilh55uoi3mo1qzc) and the checkpoint pretrained for 120,000 steps can be accessed [here](https://huggingface.co/patrickvonplaten/wav2vec2-large-repro-960h-libri-120k-steps)"", ""We are interested in the most likely prediction of\nthe model and thus take the `argmax(...)` of the logits. Also, we\ntransform the encoded labels back to the original string by replacing\n`-100` with the `pad\\_token\\_id` and decoding the ids while making sure\nthat consecutive tokens are \\*\\*not\\*\\* grouped to the same token in CTC\nstyle \\\\({}^1\\\\). ```python\ndef compute\\_metrics(pred):\npred\\_logits = pred.predictions\npred\\_ids = np.argmax(pred\\_logits, axis=-1)\npred.label\\_ids[pred.label\\_ids == -100] = processor.tokenizer.pad\\_token\\_id\npred\\_str = processor.batch\\_decode(pred\\_ids)\n# we do not want to group tokens when computing the metrics\nlabel\\_str = processor.batch\\_decode(pred.label\\_ids, group\\_tokens=False)\nwer = wer\\_metric.compute(predictions=pred\\_str, references=label\\_str)\nreturn {\""wer\"": wer}\n```\nNow, we can load the pretrained `Wav2Vec2` checkpoint. The tokenizer\\'s\n`pad\\_token\\_id` must be to define the model\\'s `pad\\_token\\_id` or in the\ncase of `Wav2Vec2ForCTC` also CTC\\'s \\*blank token\\* \\\\({}^2\\\\). To save GPU\nmemory, we enable PyTorch\\'s [gradient\ncheckpointing](https://pytorch.org/docs/stable/checkpoint.html) and also\nset the loss reduction to \\\""\\*mean\\*\\\"". ```python\nfrom transformers import Wav2Vec2ForCTC\nmodel = Wav2Vec2ForCTC.from\\_pretrained(\n\""facebook/wav2vec2-base\"",\nctc\\_loss\\_reduction=\""mean\"",\npad\\_token\\_id=processor.tokenizer.pad\\_token\\_id,\n)\n```\n\\*\\*Print Output:\\*\\*\n```bash\nSome weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm\\_head.weight', 'lm\\_head.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."", ""Using the Buckwalter format, text is also logged in Arabic abjad. `--target\\_feature\\_extractor\\_sampling\\_rate` resamples audio to target feature extractor's sampling rate (16kHz). `--max\\_duration\\_in\\_seconds=\""15\""` filters out examples whose audio is longer than the specified limit,\nwhich helps with capping GPU memory usage. ### DeepSpeed Integration\nTo learn how to deploy Deepspeed Integration please refer to [this guide](https://huggingface.co/transformers/main/main\\_classes/deepspeed.html#deepspeed-trainer-integration). But to get started quickly all you need is to install:\n```\npip install deepspeed\n```\nand then use the default configuration files in this directory:\n\\* `ds\\_config\\_wav2vec2\\_zero2.json`\n\\* `ds\\_config\\_wav2vec2\\_zero3.json`\nHere are examples of how you can use DeepSpeed:\n(edit the value for `--num\\_gpus` to match the number of GPUs you have)\nZeRO-2:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 2 \\\nrun\\_asr.py \\\n--output\\_dir=output\\_dir --num\\_train\\_epochs=2 --per\\_device\\_train\\_batch\\_size=2 \\\n--per\\_device\\_eval\\_batch\\_size=2 --evaluation\\_strategy=steps --save\\_steps=500 --eval\\_steps=100 \\\n--logging\\_steps=5 --learning\\_rate=5e-4 --warmup\\_steps=3000 \\\n--model\\_name\\_or\\_path=patrickvonplaten/wav2vec2\\_tiny\\_random\\_robust \\\n--dataset\\_name=hf-internal-testing/librispeech\\_asr\\_dummy --dataset\\_config\\_name=clean \\\n--train\\_split\\_name=validation --validation\\_split\\_name=validation --orthography=timit \\\n--preprocessing\\_num\\_workers=1 --group\\_by\\_length --freeze\\_feature\\_extractor --verbose\\_logging \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero2.json\n```\nFor ZeRO-2 with more than 1 gpu you need to use (which is already in the example configuration file):\n```\n\""zero\\_optimization\"": {\n... \""find\\_unused\\_parameters\"": true,\n... }\n```\nZeRO-3:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 2 \\\nrun\\_asr.py \\\n--output\\_dir=output\\_dir --num\\_train\\_epochs=2 --per\\_device\\_train\\_batch\\_size=2 \\\n--per\\_device\\_eval\\_batch\\_size=2 --evaluation\\_strategy=steps --save\\_steps=500 --eval\\_steps=100 \\\n--logging\\_steps=5 --learning\\_rate=5e-4 --warmup\\_steps=3000 \\\n--model\\_name\\_or\\_path=patrickvonplaten/wav2vec2\\_tiny\\_random\\_robust \\\n--dataset\\_name=hf-internal-testing/librispeech\\_asr\\_dummy --dataset\\_config\\_name=clean \\\n--train\\_split\\_name=validation --validation\\_split\\_name=validation --orthography=timit \\\n--preprocessing\\_num\\_workers=1 --group\\_by\\_length --freeze\\_feature\\_extractor --verbose\\_logging \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero3.json\n```\n### Pretraining Wav2Vec2\nThe `run\\_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss objective (see official [paper](https://arxiv.org/abs/2006.11477) for more information). It is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed (please refer to [this guide](https://huggingface.co/transformers/main/main\\_classes/deepspeed.html#deepspeed-trainer-integration) for more information). Here is an example of how you can use DeepSpeed ZeRO-2 to pretrain a small Wav2Vec2 model:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 4 run\\_pretrain.py \\\n--output\\_dir=\""./wav2vec2-base-libri-100h\"" \\\n--num\\_train\\_epochs=\""3\"" \\\n--per\\_device\\_train\\_batch\\_size=\""32\"" \\\n--per\\_device\\_eval\\_batch\\_size=\""32\"" \\\n--gradient\\_accumulation\\_steps=\""2\"" \\\n--save\\_total\\_limit=\""3\"" \\\n--save\\_steps=\""500\"" \\\n--logging\\_steps=\""10\"" \\\n--learning\\_rate=\""5e-4\"" \\\n--weight\\_decay=\""0.01\"" \\\n--warmup\\_steps=\""3000\"" \\\n--model\\_name\\_or\\_path=\""patrickvonplaten/wav2vec2-base-libri-100h\"" \\\n--dataset\\_name=\""librispeech\\_asr\"" \\\n--dataset\\_config\\_name=\""clean\"" \\\n--train\\_split\\_name=\""train.100\"" \\\n--preprocessing\\_num\\_workers=\""4\"" \\\n--max\\_duration\\_in\\_seconds=\""10.0\"" \\\n--group\\_by\\_length \\\n--verbose\\_logging \\\n--fp16 \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero2.json \\\n```\n### Forced Alignment\nCharacter level forced alignment for audio and text pairs with wav2vec2 models finetuned on ASR task for a specific language. Inspired by [this](https://pytorch.org/tutorials/intermediate/forced\\_alignment\\_with\\_torchaudio\\_tutorial.html) Pytorch tutorial. #### Input Formats\nInput format in script.txt Input format in wavs directroy\n0000 sentence1 0000.wav\n0001 sentence2 0001.wav\n#### Output Format\nOutput directory will contain 0000.txt and 0001.txt. Each file will have format like below\nchar score start\\_ms end\\_ms\nh 0.25 1440 1520\n#### Run command\n```\npython alignment.py \\\n--model\\_name=\""arijitx/wav2vec2-xls-r-300m-bengali\"" \\\n--wav\\_dir=\""./wavs\""\n--text\\_file=\""script.txt\"" \\\n--input\\_wavs\\_sr=48000 \\\n--output\\_dir=\""./out\\_alignment\"" \\\n--cuda\n```"", ""Ready to try your hand at audio classification? Check out our complete [audio classification guide](tasks/audio\\_classification) to learn how to finetune Wav2Vec2 and use it for inference! ### Automatic speech recognition\nTo use the pretrained model for automatic speech recognition, add a language modeling head on top of the base Wav2Vec2 model for [connectionist temporal classification (CTC)](glossary#connectionist-temporal-classification-ctc). The language modeling head is a linear layer that accepts the encoder's hidden states and transforms them into logits. Each logit represents a token class (the number of tokens comes from the task vocabulary). The CTC loss is calculated between the logits and targets to find the most likely sequence of tokens, which are then decoded into a transcription. Ready to try your hand at automatic speech recognition? Check out our complete [automatic speech recognition guide](tasks/asr) to learn how to finetune Wav2Vec2 and use it for inference! ## Computer vision\nThere are two ways to approach computer vision tasks:\n1. Split an image into a sequence of patches and process them in parallel with a Transformer. 2. Use a modern CNN, like [ConvNeXT](model\\_doc/convnext), which relies on convolutional layers but adopts modern network designs. A third approach mixes Transformers with convolutions (for example, [Convolutional Vision Transformer](model\\_doc/cvt) or [LeViT](model\\_doc/levit)). We won't discuss those because they just combine the two approaches we examine here. ViT and ConvNeXT are commonly used for image classification, but for other vision tasks like object detection, segmentation, and depth estimation, we'll look at DETR, Mask2Former and GLPN, respectively; these models are better suited for those tasks. ### Image classification\nViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an attention mechanism while ConvNeXT uses convolutions."", ""Wav2Vec2 Contrastive Loss PreTraining examples\nThe following example showcases how to pretrain a wav2vec2 model using the JAX/Flax backend. Pretraining Wav2Vec2 is rather complex, so it is highly recommended to read the\n[official paper](https://arxiv.org/abs/2006.11477). JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU. Models written in JAX/Flax are \\*\\*immutable\\*\\* and updated in a purely functional\nway which enables simple and efficient model parallelism. `run\\_wav2vec2\\_pretrain\\_flax.py` is a lightweight example of how to download and preprocess a dataset from the ðŸ¤— Datasets library or use your own files (jsonlines or csv), then pretrain the wav2vec2 architectures above on it. For custom datasets in `jsonlines` format please see: [the Datasets documentation](https://huggingface.co/docs/datasets/loading\\_datasets#json-files) and you also will find examples of these below. Let's start by creating a model repository to save the trained model and logs. Here we call the model `\""wav2vec2-base-robust\""`, but you can change the model name as you like. You can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\nyou are logged in) or via the command line:\n```\nhuggingface-cli repo create wav2vec2-base-robust\n```\nNext we clone the model repository to add the tokenizer and model files. ```\ngit clone https://huggingface.co//wav2vec2-base-robust\n```\nTo ensure that all tensorboard traces will be uploaded correctly, we need to\ntrack them. You can run the following command inside your model repo to do so. ```\ncd wav2vec2-base-robust\ngit lfs track \""\\*tfevents\\*\""\n```\nGreat, we have set up our model repository. During training, we will automatically\npush the training logs and model weights to the repo. Next, let's add a symbolic link to the `run\\_wav2vec2\\_pretrain\\_flax`. ```bash\nexport MODEL\\_DIR=\""./wav2vec2-base-robust\""\nln -s ~/transformers/examples/research\\_projects/jax-projects/wav2vec2/run\\_wav2vec2\\_pretrain\\_flax.py ./\n```\n### Create the model configuration\nLet's first create the model configuration and store it in the model repository."", ""The\nsampling rate hereby plays an important role in that it defines how many\ndata points of the speech signal are measured per second. Therefore,\nsampling with a higher sampling rate results in a better approximation\nof the \\*real\\* speech signal but also necessitates more values per\nsecond. A pretrained checkpoint expects its input data to have been sampled more\nor less from the same distribution as the data it was trained on. The\nsame speech signals sampled at two different rates have a very different\ndistribution, \\*e.g.\\*, doubling the sampling rate results in data points\nbeing twice as long. Thus, before fine-tuning a pretrained checkpoint of\nan ASR model, it is crucial to verify that the sampling rate of the data\nthat was used to pretrain the model matches the sampling rate of the\ndataset used to fine-tune the model. Wav2Vec2 was pretrained on the audio data of\n[LibriSpeech](https://huggingface.co/datasets/librispeech\\_asr) and\nLibriVox which both were sampling with 16kHz. Our fine-tuning dataset,\n[Timit](hhtps://huggingface.co/datasets/timit\\_asr), was luckily also\nsampled with 16kHz. If the fine-tuning dataset would have been sampled\nwith a rate lower or higher than 16kHz, we first would have had to up or\ndownsample the speech signal to match the sampling rate of the data used\nfor pretraining. A Wav2Vec2 feature extractor object requires the following parameters to\nbe instantiated:\n- `feature\\_size`: Speech models take a sequence of feature vectors as\nan input. While the length of this sequence obviously varies, the\nfeature size should not. In the case of Wav2Vec2, the feature size\nis 1 because the model was trained on the raw speech signal \\\\({}^2\\\\) . - `sampling\\_rate`: The sampling rate at which the model is trained on. - `padding\\_value`: For batched inference, shorter inputs need to be\npadded with a specific value\n- `do\\_normalize`: Whether the input should be\n\\*zero-mean-unit-variance\\* normalized or not. Usually, speech models\nperform better when normalizing the input\n- `return\\_attention\\_mask`: Whether the model should make use of an\n`attention\\_mask` for batched inference. In general, models should\n\\*\\*always\\*\\* make use of the `attention\\_mask` to mask padded tokens. However, due to a very specific design choice of `Wav2Vec2`\\'s\n\\\""base\\\"" checkpoint, better results are achieved when using no\n`attention\\_mask`."", ""inputs = {k: v.to(\""cuda\"") for k, v in inputs.items()}\n... with torch.no\\_grad():\n... logits = model(\\*\\*inputs).logits\n... transcription = processor.batch\\_decode(logits.cpu().numpy(), pool).text\n... batch[\""transcription\""] = transcription\n... return batch\n>>> # note: pool should be instantiated \\*after\\* `Wav2Vec2ProcessorWithLM`. >>> # otherwise, the LM won't be available to the pool's sub-processes\n>>> # select number of processes and batch\\_size based on number of CPU cores available and on dataset size\n>>> with get\\_context(\""fork\"").Pool(processes=2) as pool:\n... result = dataset.map(\n... map\\_to\\_pred, batched=True, batch\\_size=2, fn\\_kwargs={\""pool\"": pool}, remove\\_columns=[\""speech\""]\n... )\n>>> result[\""transcription\""][:2]\n['MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL', \""NOR IS MISTER COULTER'S MANNER LESS INTERESTING THAN HIS MATTER\""]\n```\n## Wav2Vec2 specific outputs\n[[autodoc]] models.wav2vec2\\_with\\_lm.processing\\_wav2vec2\\_with\\_lm.Wav2Vec2DecoderWithLMOutput\n[[autodoc]] models.wav2vec2.modeling\\_wav2vec2.Wav2Vec2BaseModelOutput\n[[autodoc]] models.wav2vec2.modeling\\_wav2vec2.Wav2Vec2ForPreTrainingOutput\n[[autodoc]] models.wav2vec2.modeling\\_flax\\_wav2vec2.FlaxWav2Vec2BaseModelOutput\n[[autodoc]] models.wav2vec2.modeling\\_flax\\_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput\n\n## Wav2Vec2Model\n[[autodoc]] Wav2Vec2Model\n- forward\n## Wav2Vec2ForCTC\n[[autodoc]] Wav2Vec2ForCTC\n- forward\n- load\\_adapter\n## Wav2Vec2ForSequenceClassification\n[[autodoc]] Wav2Vec2ForSequenceClassification\n- forward\n## Wav2Vec2ForAudioFrameClassification\n[[autodoc]] Wav2Vec2ForAudioFrameClassification\n- forward\n## Wav2Vec2ForXVector\n[[autodoc]] Wav2Vec2ForXVector\n- forward\n## Wav2Vec2ForPreTraining\n[[autodoc]] Wav2Vec2ForPreTraining\n- forward\n\n## TFWav2Vec2Model\n[[autodoc]] TFWav2Vec2Model\n- call\n## TFWav2Vec2ForSequenceClassification\n[[autodoc]] TFWav2Vec2ForSequenceClassification\n- call\n## TFWav2Vec2ForCTC\n[[autodoc]] TFWav2Vec2ForCTC\n- call\n\n## FlaxWav2Vec2Model\n[[autodoc]] FlaxWav2Vec2Model\n- \\_\\_call\\_\\_\n## FlaxWav2Vec2ForCTC\n[[autodoc]] FlaxWav2Vec2ForCTC\n- \\_\\_call\\_\\_\n## FlaxWav2Vec2ForPreTraining\n[[autodoc]] FlaxWav2Vec2ForPreTraining\n- \\_\\_call\\_\\_"", ""However, the encoded labels should not be grouped when\ndecoding since they don\\'t correspond to the predicted tokens of the\nmodel, which is why the `group\\_tokens=False` parameter has to be passed. If we wouldn\\'t pass this parameter a word like `\""hello\""` would\nincorrectly be encoded, and decoded as `\""helo\""`. \\\\({}^2\\\\) The blank token allows the model to predict a word, such as\n`\""hello\""` by forcing it to insert the blank token between the two l\\'s. A CTC-conform prediction of `\""hello\""` of our model would be\n`[PAD] [PAD] \""h\"" \""e\"" \""e\"" \""l\"" \""l\"" [PAD] \""l\"" \""o\"" \""o\"" [PAD]`. ### Training\nTraining will take between 90 and 180 minutes depending on the GPU\nallocated to the google colab attached to this notebook. While the trained model yields satisfying\nresults on \\*Timit\\*\\'s test data, it is by no means an optimally\nfine-tuned model. The purpose of this notebook is to demonstrate how\nWav2Vec2\\'s [base](https://huggingface.co/facebook/wav2vec2-base),\n[large](https://huggingface.co/facebook/wav2vec2-large), and\n[large-lv60](https://huggingface.co/facebook/wav2vec2-large-lv60)\ncheckpoints can be fine-tuned on any English dataset."", ""!---\nCopyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Speech Recognition Pre-Training\n## Wav2Vec2 Speech Pre-Training\nThe script [`run\\_speech\\_wav2vec2\\_pretraining\\_no\\_trainer.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run\\_wav2vec2\\_pretraining\\_no\\_trainer.py) can be used to pre-train a [Wav2Vec2](https://huggingface.co/transformers/model\\_doc/wav2vec2.html?highlight=wav2vec2) model from scratch. In the script [`run\\_speech\\_wav2vec2\\_pretraining\\_no\\_trainer`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run\\_wav2vec2\\_pretraining\\_no\\_trainer.py), a Wav2Vec2 model is pre-trained on audio data alone using [Wav2Vec2's contrastive loss objective](https://arxiv.org/abs/2006.11477). The following examples show how to fine-tune a `\""base\""`-sized Wav2Vec2 model as well as a `\""large\""`-sized Wav2Vec2 model using [`accelerate`](https://github.com/huggingface/accelerate). ---\n\\*\\*NOTE 1\\*\\*\nWav2Vec2's pre-training is known to be quite unstable. It is advised to do a couple of test runs with a smaller dataset,\n\\*i.e.\\* `--dataset\\_config\\_names clean clean`, `--dataset\\_split\\_names validation test`\nto find good hyper-parameters for `learning\\_rate`, `batch\\_size`, `num\\_warmup\\_steps`,\nand the optimizer. A good metric to observe during training is the gradient norm which should ideally be between 0.5 and 2. ---\n---\n\\*\\*NOTE 2\\*\\*\nWhen training a model on large datasets it is recommended to run the data preprocessing\nin a first run in a \\*\\*non-distributed\\*\\* mode via `--preprocessing\\_only` so that\nwhen running the model in \\*\\*distributed\\*\\* mode in a second step the preprocessed data\ncan easily be loaded on each distributed device. ---\n### Demo\nIn this demo run we pre-train a `\""base-sized\""` Wav2Vec2 model simply only on the validation\nand test data of [librispeech\\_asr](https://huggingface.co/datasets/librispeech\\_asr). The demo is run on two Titan RTX (24 GB RAM each). In case you have less RAM available\nper device, consider reducing `--batch\\_size` and/or the `--max\\_duration\\_in\\_seconds`. ```bash\naccelerate launch run\\_wav2vec2\\_pretraining\\_no\\_trainer.py \\\n--dataset\\_name=\""librispeech\\_asr\"" \\\n--dataset\\_config\\_names clean clean \\\n--dataset\\_split\\_names validation test \\\n--model\\_name\\_or\\_path=\""patrickvonplaten/wav2vec2-base-v2\"" \\\n--output\\_dir=\""./wav2vec2-pretrained-demo\"" \\\n--max\\_train\\_steps=\""20000\"" \\\n--num\\_warmup\\_steps=\""32000\"" \\\n--gradient\\_accumulation\\_steps=\""8\"" \\\n--learning\\_rate=\""0.005\"" \\\n--weight\\_decay=\""0.01\"" \\\n--max\\_duration\\_in\\_seconds=\""20.0\"" \\\n--min\\_duration\\_in\\_seconds=\""2.0\"" \\\n--logging\\_steps=\""1\"" \\\n--saving\\_steps=\""10000\"" \\\n--per\\_device\\_train\\_batch\\_size=\""8\"" \\\n--per\\_device\\_eval\\_batch\\_size=\""8\"" \\\n--adam\\_beta1=\""0.9\"" \\\n--adam\\_beta2=\""0.98\"" \\\n--adam\\_epsilon=\""1e-06\"" \\\n--gradient\\_checkpointing \\\n--mask\\_time\\_prob=\""0.65\"" \\\n--mask\\_time\\_length=\""10\""\n```\nThe results of this run can be seen [here](https://wandb.ai/patrickvonplaten/wav2vec2-pretrained-demo/reports/Wav2Vec2-PreTraining-Demo-Run--VmlldzoxMDk3MjAw?accessToken=oa05s1y57lizo2ocxy3k01g6db1u4pt8m6ur2n8nl4cb0ug02ms2cw313kb8ruch). ### Base\nTo pre-train `\""base-sized\""` Wav2Vec2 model, \\*e.g.\\* [facebook/wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base)\non [librispeech\\_asr](https://huggingface.co/datasets/librispeech\\_asr), the following command can be run:\n```bash\naccelerate launch run\\_wav2vec2\\_pretraining\\_no\\_trainer.py \\\n--dataset\\_name=librispeech\\_asr \\\n--dataset\\_config\\_names clean clean other \\\n--dataset\\_split\\_names train.100 train.360 train.500 \\\n--model\\_name\\_or\\_path=\""patrickvonplaten/wav2vec2-base-v2\"" \\\n--output\\_dir=\""./wav2vec2-pretrained-demo\"" \\\n--max\\_train\\_steps=\""200000\"" \\\n--num\\_warmup\\_steps=\""32000\"" \\\n--gradient\\_accumulation\\_steps=\""4\"" \\\n--learning\\_rate=\""0.001\"" \\\n--weight\\_decay=\""0.01\"" \\\n--max\\_duration\\_in\\_seconds=\""20.0\"" \\\n--min\\_duration\\_in\\_seconds=\""2.0\"" \\\n--logging\\_steps=\""1\"" \\\n--saving\\_steps=\""10000\"" \\\n--per\\_device\\_train\\_batch\\_size=\""8\"" \\\n--per\\_device\\_eval\\_batch\\_size=\""8\"" \\\n--adam\\_beta1=\""0.9\"" \\\n--adam\\_beta2=\""0.98\"" \\\n--adam\\_epsilon=\""1e-06\"" \\\n--gradient\\_checkpointing \\\n--mask\\_time\\_prob=\""0.65\"" \\\n--mask\\_time\\_length=\""10\""\n```\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 4 days. In case you have more than 8 GPUs available for a higher effective `batch\\_size`,\nit is recommended to increase the `learning\\_rate` to `0.005` for faster convergence. The results of this run can be seen [here](https://wandb.ai/patrickvonplaten/test/reports/Wav2Vec2-Base--VmlldzoxMTUyODQ0?accessToken=rg6e8u9yizx964k8q47zctq1m4afpvtn1i3qi9exgdmzip6xwkfzvagfajpzj55n) and the checkpoint pretrained for 85,000 steps can be accessed [here](https://huggingface.co/patrickvonplaten/wav2vec2-base-repro-960h-libri-85k-steps)\n### Large\nTo pre-train `\""large-sized\""` Wav2Vec2 model, \\*e.g.\\* [facebook/wav2vec2-large-lv60](https://huggingface.co/facebook/wav2vec2-large-lv60),\non [librispeech\\_asr](https://huggingface.co/datasets/librispeech\\_asr), the following command can be run:\n```bash\naccelerate launch run\\_wav2vec2\\_pretraining\\_no\\_trainer.py \\\n--dataset\\_name=librispeech\\_asr \\\n--dataset\\_config\\_names clean clean other \\\n--dataset\\_split\\_names train.100 train.360 train.500 \\\n--output\\_dir=./test \\\n--max\\_train\\_steps=200000 \\\n--num\\_warmup\\_steps=32000 \\\n--gradient\\_accumulation\\_steps=8 \\\n--learning\\_rate=0.001 \\\n--weight\\_decay=0.01 \\\n--max\\_duration\\_in\\_seconds=20.0 \\\n--min\\_duration\\_in\\_seconds=2.0 \\\n--model\\_name\\_or\\_path=./\n--logging\\_steps=1 \\\n--saving\\_steps=10000 \\\n--per\\_device\\_train\\_batch\\_size=2 \\\n--per\\_device\\_eval\\_batch\\_size=4 \\\n--adam\\_beta1=0.9 \\\n--adam\\_beta2=0.98 \\\n--adam\\_epsilon=1e-06 \\\n--gradient\\_checkpointing \\\n--mask\\_time\\_prob=0.65 \\\n--mask\\_time\\_length=10\n```\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 7 days. In case you have more than 8 GPUs available for a higher effective `batch\\_size`,\nit is recommended to increase the `learning\\_rate` to `0.005` for faster convergence."", ""--\ntitle: \""Boosting Wav2Vec2 with n-grams in ðŸ¤— Transformers\""\nthumbnail: /blog/assets/44\\_boost\\_wav2vec2\\_ngram/wav2vec2\\_ngram.png\nauthors:\n- user: patrickvonplaten\n---\n# Boosting Wav2Vec2 with n-grams in ðŸ¤— Transformers\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Boosting_Wav2Vec2_with_n_grams_in_Transformers.ipynb)\n\\*\\*Wav2Vec2\\*\\* is a popular pre-trained model for speech recognition. Released in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Meta AI Research, the novel architecture catalyzed progress in\nself-supervised pretraining for speech recognition, \\*e.g.\\* [\\*G. Ng et\nal.\\*, 2021](https://arxiv.org/pdf/2104.03416.pdf), [\\*Chen et al\\*,\n2021](https://arxiv.org/abs/2110.13900), [\\*Hsu et al.\\*,\n2021](https://arxiv.org/abs/2106.07447) and [\\*Babu et al.\\*,\n2021](https://arxiv.org/abs/2111.09296). On the Hugging Face Hub,\nWav2Vec2's most popular pre-trained checkpoint currently amounts to\nover [\\*\\*250,000\\*\\* monthly\ndownloads](https://huggingface.co/facebook/wav2vec2-base-960h). Using Connectionist Temporal Classification (CTC), pre-trained\nWav2Vec2-like checkpoints are extremely easy to fine-tune on downstream\nspeech recognition tasks. In a nutshell, fine-tuning pre-trained\nWav2Vec2 checkpoints works as follows:\nA single randomly initialized linear layer is stacked on top of the\npre-trained checkpoint and trained to classify raw audio input to a\nsequence of letters."", ""The official results of the model can be found in Table 3 and Table 4 of the paper. The Wav2Vec2-Conformer weights were released by the Meta AI team within the [Fairseq library](https://github.com/pytorch/fairseq/blob/main/examples/wav2vec/README.md#pre-trained-models). This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be found [here](https://github.com/pytorch/fairseq/tree/main/examples/wav2vec). ## Usage tips\n- Wav2Vec2-Conformer follows the same architecture as Wav2Vec2, but replaces the \\*Attention\\*-block with a \\*Conformer\\*-block\nas introduced in [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100). - For the same number of layers, Wav2Vec2-Conformer requires more parameters than Wav2Vec2, but also yields\nan improved word error rate. - Wav2Vec2-Conformer uses the same tokenizer and feature extractor as Wav2Vec2. - Wav2Vec2-Conformer can use either no relative position embeddings, Transformer-XL-like position embeddings, or\nrotary position embeddings by setting the correct `config.position\\_embeddings\\_type`. ## Resources\n- [Audio classification task guide](../tasks/audio\\_classification)\n- [Automatic speech recognition task guide](../tasks/asr)\n## Wav2Vec2ConformerConfig\n[[autodoc]] Wav2Vec2ConformerConfig\n## Wav2Vec2Conformer specific outputs\n[[autodoc]] models.wav2vec2\\_conformer.modeling\\_wav2vec2\\_conformer.Wav2Vec2ConformerForPreTrainingOutput\n## Wav2Vec2ConformerModel\n[[autodoc]] Wav2Vec2ConformerModel\n- forward\n## Wav2Vec2ConformerForCTC\n[[autodoc]] Wav2Vec2ConformerForCTC\n- forward\n## Wav2Vec2ConformerForSequenceClassification\n[[autodoc]] Wav2Vec2ConformerForSequenceClassification\n- forward\n## Wav2Vec2ConformerForAudioFrameClassification\n[[autodoc]] Wav2Vec2ConformerForAudioFrameClassification\n- forward\n## Wav2Vec2ConformerForXVector\n[[autodoc]] Wav2Vec2ConformerForXVector\n- forward\n## Wav2Vec2ConformerForPreTraining\n[[autodoc]] Wav2Vec2ConformerForPreTraining\n- forward"", ""It does so by:\n1. extracting audio representations from the raw audio (using CNN\nlayers),\n2. processing the sequence of audio representations with a stack of\ntransformer layers, and,\n3. classifying the processed audio representations into a sequence of\noutput letters. Previously audio classification models required an additional language\nmodel (LM) and a dictionary to transform the sequence of classified audio\nframes to a coherent transcription. Wav2Vec2's architecture is based on\ntransformer layers, thus giving each processed audio representation\ncontext from all other audio representations. In addition, Wav2Vec2\nleverages the [CTC algorithm](https://distill.pub/2017/ctc/) for\nfine-tuning, which solves the problem of alignment between a varying\n\""input audio length\""-to-\""output text length\"" ratio. Having contextualized audio classifications and no alignment problems,\nWav2Vec2 does not require an external language model or dictionary to\nyield acceptable audio transcriptions. As can be seen in Appendix C of the [official\npaper](https://arxiv.org/abs/2006.11477), Wav2Vec2 gives impressive\ndownstream performances on [LibriSpeech](https://huggingface.co/datasets/librispeech\\_asr) without using a language model at\nall. However, from the appendix, it also becomes clear that using Wav2Vec2\nin combination with a language model can yield a significant\nimprovement, especially when the model was trained on only 10 minutes of\ntranscribed audio. Until recently, the ðŸ¤— Transformers library did not offer a simple user\ninterface to decode audio files with a fine-tuned Wav2Vec2 \\*\\*and\\*\\* a\nlanguage model. This has thankfully changed. ðŸ¤— Transformers now offers\nan easy-to-use integration with \\*Kensho Technologies'\\* [pyctcdecode\nlibrary](https://github.com/kensho-technologies/pyctcdecode)."", ""Note that many training parameters can be set in the model configuration including\nthe configuration about the masking distribution (`mask\\_time\\_length`, `mask\\_time\\_prob`),\ndropout (`attention\\_dropout`, ...), the trade-off between the contrastive loss and\nthe diversity loss, etc... Mostly likely you will need to change these parameters depending on your use case. Again, we highly recommend to read the [official paper](https://arxiv.org/abs/2006.11477)\nto better understand which parameters can be set for pretraining. For this example, we will be using a `\""base\""`-sized model of Wav2Vec2 with robust\nlayer norm and keep most of the default settings. ```python\nmodel\\_dir=\""./wav2vec2-base-robust\""\nfrom transformers import Wav2Vec2Config\nconfig = Wav2Vec2Config.from\\_pretrained(\n\""facebook/wav2vec2-base\"",\nmask\\_time\\_length=10,\nmask\\_time\\_prob=0.05,\ndiversity\\_loss\\_weight=0.1,\nnum\\_negatives=100,\ndo\\_stable\\_layer\\_norm=True,\nfeat\\_extract\\_norm=\""layer\"",\n)\nconfig.save\\_pretrained(model\\_dir)\n```\n### Create a feature extractor configuration\nBefore we can start the training, we need to define\na feature extractor that takes care of normalization, etc... Here we can also re-use the feature extractor of [wav2vec2-base-960h](https://huggingface.co/facebook/wav2vec2-base) while making sure that padding is allowed. ```python\nmodel\\_dir=\""./wav2vec2-base-robust\""\nfrom transformers import Wav2Vec2FeatureExtractor\nconfig = Wav2Vec2FeatureExtractor.from\\_pretrained(\""facebook/wav2vec2-base\"", return\\_attention\\_mask=True)\nconfig.save\\_pretrained(model\\_dir)\n```\n### Train the model\nFinally, we can run the example script to train the model:\n```bash\n./run\\_wav2vec2\\_pretrain\\_flax.py \\\n--output\\_dir=${MODEL\\_DIR} \\\n--num\\_train\\_epochs=\""5\"" \\\n--per\\_device\\_train\\_batch\\_size=\""32\"" \\\n--per\\_device\\_eval\\_batch\\_size=\""32\"" \\\n--learning\\_rate=\""5e-4\"" \\\n--weight\\_decay=\""0.01\"" \\\n--warmup\\_steps=\""2000\"" \\\n--model\\_name\\_or\\_path=${MODEL\\_DIR} \\\n--dataset\\_name=\""librispeech\\_asr\"" \\\n--dataset\\_config\\_name=\""clean\"" \\\n--train\\_split\\_name=\""train.100\"" \\\n--preprocessing\\_num\\_workers=\""4\"" \\\n--max\\_duration\\_in\\_seconds=\""10.0\"" \\\n--adam\\_beta1=\""0.9\"" \\\n--adam\\_beta2=\""0.98\"" \\\n--pad\\_to\\_multiple\\_of=\""16384\"" \\\n--push\\_to\\_hub\n```\nNote that this script is not fully tested yet, so we cannot ensure that\nthe above script leads to satisfying results."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# How ðŸ¤— Transformers solve tasks\nIn [What ðŸ¤— Transformers can do](task\\_summary), you learned about natural language processing (NLP), speech and audio, computer vision tasks, and some important applications of them. This page will look closely at how models solve these tasks and explain what's happening under the hood. There are many ways to solve a given task, some models may implement certain techniques or even approach the task from a new angle, but for Transformer models, the general idea is the same. Owing to its flexible architecture, most models are a variant of an encoder, decoder, or encoder-decoder structure. In addition to Transformer models, our library also has several convolutional neural networks (CNNs), which are still used today for computer vision tasks. We'll also explain how a modern CNN works. To explain how tasks are solved, we'll walk through what goes on inside the model to output useful predictions. - [Wav2Vec2](model\\_doc/wav2vec2) for audio classification and automatic speech recognition (ASR)\n- [Vision Transformer (ViT)](model\\_doc/vit) and [ConvNeXT](model\\_doc/convnext) for image classification\n- [DETR](model\\_doc/detr) for object detection\n- [Mask2Former](model\\_doc/mask2former) for image segmentation\n- [GLPN](model\\_doc/glpn) for depth estimation\n- [BERT](model\\_doc/bert) for NLP tasks like text classification, token classification and question answering that use an encoder\n- [GPT2](model\\_doc/gpt2) for NLP tasks like text generation that use a decoder\n- [BART](model\\_doc/bart) for NLP tasks like summarization and translation that use an encoder-decoder\nBefore you go further, it is good to have some basic knowledge of the original Transformer architecture. Knowing how encoders, decoders, and attention work will aid you in understanding how different Transformer models work. If you're just getting started or need a refresher, check out our [course](https://huggingface.co/course/chapter1/4?fw=pt) for more information! ## Speech and audio\n[Wav2Vec2](model\\_doc/wav2vec2) is a self-supervised model pretrained on unlabeled speech data and finetuned on labeled data for audio classification and automatic speech recognition. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/wav2vec2_architecture.png)\n\nThis model has four main components:\n1. A \\*feature encoder\\* takes the raw audio waveform, normalizes it to zero mean and unit variance, and converts it into a sequence of feature vectors that are each 20ms long. 2. Waveforms are continuous by nature, so they can't be divided into separate units like a sequence of text can be split into words. That's why the feature vectors are passed to a \\*quantization module\\*, which aims to learn discrete speech units. The speech unit is chosen from a collection of codewords, known as a \\*codebook\\* (you can think of this as the vocabulary). From the codebook, the vector or speech unit, that best represents the continuous audio input is chosen and forwarded through the model. 3. About half of the feature vectors are randomly masked, and the masked feature vector is fed to a \\*context network\\*, which is a Transformer encoder that also adds relative positional embeddings. 4. The pretraining objective of the context network is a \\*contrastive task\\*. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label). Now that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech recognition! ### Audio classification\nTo use the pretrained model for audio classification, add a sequence classification head on top of the base Wav2Vec2 model. The classification head is a linear layer that accepts the encoder's hidden states. The hidden states represent the learned features from each audio frame which can have varying lengths. To create one vector of fixed-length, the hidden states are pooled first and then transformed into logits over the class labels. The cross-entropy loss is calculated between the logits and target to find the most likely class."", ""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Wav2Vec2Phoneme\n## Overview\nThe Wav2Vec2Phoneme model was proposed in [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition (Xu et al.,\n2021](https://arxiv.org/abs/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli. The abstract from the paper is the following:\n\\*Recent progress in self-training, self-supervised pretraining and unsupervised learning enabled well performing speech\nrecognition systems without any labeled data. However, in many cases there is labeled data available for related\nlanguages which is not utilized by these methods. This paper extends previous work on zero-shot cross-lingual transfer\nlearning by fine-tuning a multilingually pretrained wav2vec 2.0 model to transcribe unseen languages. This is done by\nmapping phonemes of the training languages to the target language using articulatory features. Experiments show that\nthis simple method significantly outperforms prior work which introduced task-specific architectures and used only part\nof a monolingually pretrained model.\\*\nRelevant checkpoints can be found under https://huggingface.co/models?other=phoneme-recognition. This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten)\nThe original code can be found [here](https://github.com/pytorch/fairseq/tree/master/fairseq/models/wav2vec). ## Usage tips\n- Wav2Vec2Phoneme uses the exact same architecture as Wav2Vec2\n- Wav2Vec2Phoneme is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. - Wav2Vec2Phoneme model was trained using connectionist temporal classification (CTC) so the model output has to be\ndecoded using [`Wav2Vec2PhonemeCTCTokenizer`]. - Wav2Vec2Phoneme can be fine-tuned on multiple language at once and decode unseen languages in a single forward pass\nto a sequence of phonemes\n- By default, the model outputs a sequence of phonemes. In order to transform the phonemes to a sequence of words one\nshould make use of a dictionary and language model. Wav2Vec2Phoneme's architecture is based on the Wav2Vec2 model, for API reference, check out [`Wav2Vec2`](wav2vec2)'s documentation page\nexcept for the tokenizer. ## Wav2Vec2PhonemeCTCTokenizer\n[[autodoc]] Wav2Vec2PhonemeCTCTokenizer\n- \\_\\_call\\_\\_\n- batch\\_decode\n- decode\n- phonemize""]","The pretraining objective of the Wav2Vec2 context network is a contrastive task. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label).",The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.
"What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?
","[""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a TensorFlow tensor."", ""ÐœÑ‹ Ñ…Ð¾Ñ‚Ð¸Ð¼, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Transformers Ð¿Ð¾Ð·Ð²Ð¾Ð»Ð¸Ð» Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ°Ð¼, Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÐ¼, ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ð°Ð¼, Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¾Ñ€Ð°Ð¼, Ð¸Ð½Ð¶ÐµÐ½ÐµÑ€Ð°Ð¼ Ð¸ Ð²ÑÐµÐ¼ Ð¶ÐµÐ»Ð°ÑŽÑ‰Ð¸Ð¼\nÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ñ‹ ÑÐ²Ð¾ÐµÐ¹ Ð¼ÐµÑ‡Ñ‚Ñ‹. Ð§Ñ‚Ð¾Ð±Ñ‹ Ð¾Ñ‚Ð¿Ñ€Ð°Ð·Ð´Ð½Ð¾Ð²Ð°Ñ‚ÑŒ 100 Ñ‚Ñ‹ÑÑÑ‡ Ð·Ð²ÐµÐ·Ð´ Transformers, Ð¼Ñ‹ Ñ€ÐµÑˆÐ¸Ð»Ð¸ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ Ð°ÐºÑ†ÐµÐ½Ñ‚ Ð½Ð° ÑÐ¾Ð¾Ð±Ñ‰ÐµÑÑ‚Ð²Ðµ, Ð¸ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ [awesome-transformers](./awesome-transformers.md), Ð½Ð° ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ Ð¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»ÐµÐ½Ñ‹ 100\nÐ½ÐµÐ²ÐµÑ€Ð¾ÑÑ‚Ð½Ñ‹Ñ… Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¾Ð², ÑÐ¾Ð·Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ transformers. Ð•ÑÐ»Ð¸ Ð²Ñ‹ ÑÐ²Ð»ÑÐµÑ‚ÐµÑÑŒ Ð²Ð»Ð°Ð´ÐµÐ»ÑŒÑ†ÐµÐ¼ Ð¸Ð»Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹, Ð¿Ð¾ Ð²Ð°ÑˆÐµÐ¼Ñƒ Ð¼Ð½ÐµÐ½Ð¸ÑŽ, Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½ Ð² ÑÑ‚Ð¾Ñ‚ ÑÐ¿Ð¸ÑÐ¾Ðº, Ð¿Ð¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð¾Ñ‚ÐºÑ€Ð¾Ð¹Ñ‚Ðµ PR Ð´Ð»Ñ ÐµÐ³Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ! ## Ð•ÑÐ»Ð¸ Ð²Ñ‹ Ñ…Ð¾Ñ‚Ð¸Ñ‚Ðµ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½ÑƒÑŽ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÑƒ Ð¾Ñ‚ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹ Hugging Face\n[![HuggingFace Expert Acceleration Program](https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png)](https://huggingface.co/support)  \n## Ð‘Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ Ð³Ð°Ð¹Ð´\nÐ”Ð»Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° Ð·Ð°Ð´Ð°Ð½Ð½Ð¾Ð¼ Ð²Ñ…Ð¾Ð´Ðµ (Ñ‚ÐµÐºÑÑ‚, Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ, Ð·Ð²ÑƒÐº, ...) Ð¼Ñ‹ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ API `pipeline`. ÐšÐ¾Ð½Ð²ÐµÐ¹ÐµÑ€Ñ‹ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÑŽÑ‚ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³Ð¾Ð¼, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð»ÑÑ Ð¿Ñ€Ð¸ ÐµÐµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸. Ð’Ð¾Ñ‚ ÐºÐ°Ðº Ð¼Ð¾Ð¶Ð½Ð¾ Ð±Ñ‹ÑÑ‚Ñ€Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ€ Ð´Ð»Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð¸ Ð¾Ñ‚Ñ€Ð¸Ñ†Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ñ‚ÐµÐºÑÑ‚Ð¾Ð²:\n```python\n>>> from transformers import pipeline\n# Ð’Ñ‹Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ€Ð° Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ð¹\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('ÐœÑ‹ Ð¾Ñ‡ÐµÐ½ÑŒ Ñ€Ð°Ð´Ñ‹ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ€ Ð² transformers.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```\nÐ’Ñ‚Ð¾Ñ€Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° ÐºÐ¾Ð´Ð° Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð¸ ÐºÑÑˆÐ¸Ñ€ÑƒÐµÑ‚ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ÑƒÑŽ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ€Ð¾Ð¼, Ð° Ñ‚Ñ€ÐµÑ‚ÑŒÑ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÐµÑ‚ ÐµÐµ Ð½Ð° Ð·Ð°Ð´Ð°Ð½Ð½Ð¾Ð¼ Ñ‚ÐµÐºÑÑ‚Ðµ."", ""For instance, if the review was negative, I could create a feature which would contact the customer, apologize for the poor quality of service, and inform him/her that our support team will contact him/her as soon as possible to assist him and hopefully fix the problem. Since I don't plan to get more than 2,000 requests per month, I didn't impose any performance constraints regarding the time and the scalability. ## The Transformers library\nI was a bit confused at the beginning when I downloaded the .h5 file. I thought it would be compatible with `tensorflow.keras.models.load\\_model`, but this wasn't the case. After a few minutes of research I was able to figure out that the file was a weights checkpoint rather than a Keras model. After that, I tried out the API that Hugging Face offers and read a bit more about the pipeline feature they offer. Since the results of the API & the pipeline were great, I decided that I could serve the model through the pipeline on my own server. Below is the [official example](https://github.com/huggingface/transformers#quick-tour) from the Transformers GitHub page. ```python\nfrom transformers import pipeline\n# Allocate a pipeline for sentiment-analysis\nclassifier = pipeline('sentiment-analysis')\nclassifier('We are very happy to include pipeline into the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9978193640708923}]\n```\n## Deploy transformers to Google Cloud\n> GCP is chosen as it is the cloud environment I am using in my personal organization."", ""[SEP] [UNK] Transformers : State of the Art NLP\n[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\nquestion answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting-edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\nthen share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\ncan be modified to enable quick research experiments. Why should I use transformers? 1. Easy-to-use state-of-the-art models:\n- High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user-facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint:\n2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages."", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Philosophy\nðŸ¤— Transformers is an opinionated library built for:\n- machine learning researchers and educators seeking to use, study or extend large-scale Transformers models. - hands-on practitioners who want to fine-tune those models or serve them in production, or both. - engineers who just want to download a pretrained model and use it to solve a given machine learning task. The library was designed with two strong goals in mind:\n1. Be as easy and fast to use as possible:\n- We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions,\njust three standard classes required to use each model: [configuration](main\\_classes/configuration),\n[models](main\\_classes/model), and a preprocessing class ([tokenizer](main\\_classes/tokenizer) for NLP, [image processor](main\\_classes/image\\_processor) for vision, [feature extractor](main\\_classes/feature\\_extractor) for audio, and [processor](main\\_classes/processors) for multimodal inputs). - All of these classes can be initialized in a simple and unified way from pretrained instances by using a common\n`from\\_pretrained()` method which downloads (if needed), caches and\nloads the related class instance and associated data (configurations' hyperparameters, tokenizers' vocabulary,\nand models' weights) from a pretrained checkpoint provided on [Hugging Face Hub](https://huggingface.co/models) or your own saved checkpoint. - On top of those three base classes, the library provides two APIs: [`pipeline`] for quickly\nusing a model for inference on a given task and [`Trainer`] to quickly train or fine-tune a PyTorch model (all TensorFlow models are compatible with `Keras.fit`). - As a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to\nextend or build upon the library, just use regular Python, PyTorch, TensorFlow, Keras modules and inherit from the base\nclasses of the library to reuse functionalities like model loading and saving. If you'd like to learn more about our coding philosophy for models, check out our [Repeat Yourself](https://huggingface.co/blog/transformers-design-philosophy) blog post. 2. Provide state-of-the-art models with performances as close as possible to the original models:\n- We provide at least one example for each architecture which reproduces a result provided by the official authors\nof said architecture. - The code is usually as close to the original code base as possible which means some PyTorch code may be not as\n\\*pytorchic\\* as it could be as a result of being converted TensorFlow code and vice versa."", ""Transformers, what can they do?[[transformers-what-can-they-do]]\nIn this section, we will look at what Transformer models can do and use our first tool from the ðŸ¤— Transformers library: the `pipeline()` function. ðŸ‘€ See that *Open in Colab* button on the top right? Click on it to open a Google Colab notebook with all the code samples of this section. This button will be present in any section containing code examples. If you want to run the examples locally, we recommend taking a look at the [setup](/course/chapter0). ## Transformers are everywhere![[transformers-are-everywhere]]\nTransformer models are used to solve all kinds of NLP tasks, like the ones mentioned in the previous section. Here are some of the companies and organizations using Hugging Face and Transformer models, who also contribute back to the community by sharing their models:\n![Companies using Hugging Face](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG)\nThe [ðŸ¤— Transformers library](https://github.com/huggingface/transformers) provides the functionality to create and use those shared models. The [Model Hub](https://huggingface.co/models) contains thousands of pretrained models that anyone can download and use. You can also upload your own models to the Hub! âš ï¸ The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want! [Create a huggingface.co](https://huggingface.co/join) account to benefit from all available features! Before diving into how Transformer models work under the hood, let's look at a few examples of how they can be used to solve some interesting NLP problems. ## Working with pipelines[[working-with-pipelines]]\nThe most basic object in the ðŸ¤— Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\""sentiment-analysis\"")\nclassifier(\""I've been waiting for a HuggingFace course my whole life.\"")\n```\n```python out\n[{'label': 'POSITIVE', 'score': 0.9598047137260437}]\n```\nWe can even pass several sentences! ```python\nclassifier(\n[\""I've been waiting for a HuggingFace course my whole life.\"", \""I hate this so much!\""]\n)\n```\n```python out\n[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n{'label': 'NEGATIVE', 'score': 0.9994558095932007}]\n```\nBy default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English."", ""he pipeline function. The pipeline function is the most high-level API of the Transformers library. It regroups together all the steps to go from raw texts to usable predictions. The model used is at the core of a pipeline, but the pipeline also include all the necessary pre-processing (since the model does not expect texts, but numbers) as well as some post-processing to make the output of the model human-readable. Let's look at a first example with the sentiment analysis pipeline. This pipeline performs text classification on a given input, and determines if it's positive or negative. Here, it attributed the positive label on the given text, with a confidence of 95%. You can pass multiple texts to the same pipeline, which will be processed and passed through the model together, as a batch. The output is a list of individual results, in the same order as the input texts. Here we find the same label and score for the first text, and the second text is judged positive with a confidence of 99.99%. The zero-shot classification pipeline is a more general text-classification pipeline: it allows you to provide the labels you want."", ""We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\nelse to build their dream projects. In order to celebrate the 100,000 stars of transformers, we have decided to put the spotlight on the\ncommunity, and we have created the [awesome-transformers](./awesome-transformers.md) page which lists 100\nincredible projects built in the vicinity of transformers. If you own or use a project that you believe should be part of the list, please open a PR to add it! ## If you are looking for custom support from the Hugging Face team\n[![HuggingFace Expert Acceleration Program](https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png)](https://huggingface.co/support)  \n## Quick tour\nTo immediately use a model on a given input (text, image, audio, ...), we provide the `pipeline` API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts:\n```python\n>>> from transformers import pipeline\n# Allocate a pipeline for sentiment-analysis\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('We are very happy to introduce pipeline to the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```\nThe second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text."", ""!---\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\n\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\n\n#### **English** | [ç®€ä½“ä¸­æ–‡](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ç¹é«”ä¸­æ–‡](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [í•œêµ­ì–´](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [EspaÃ±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [æ—¥æœ¬èªž](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [à¤¹à¤¿à¤¨à¥à¤¦à¥€](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [Ð ortuguÃªs](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [à°¤à±†à°²à±à°—à±](https://github.com/huggingface/transformers//blob/main/README_te.md) |\n\n### Aprendizado de mÃ¡quina de Ãºltima geraÃ§Ã£o para JAX, PyTorch e TensorFlow\n\n### \n\nA biblioteca ðŸ¤— Transformers oferece milhares de modelos prÃ©-treinados para executar tarefas em diferentes modalidades, como texto, visÃ£o e Ã¡udio."", ""!---\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\n\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\n\n#### [English](https://github.com/huggingface/transformers/blob/main/README.md) | [ç®€ä½“ä¸­æ–‡](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ç¹é«”ä¸­æ–‡](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [í•œêµ­ì–´](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [EspaÃ±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [æ—¥æœ¬èªž](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [à¤¹à¤¿à¤¨à¥à¤¦à¥€](https://github.com/huggingface/transformers/blob/main/README_hd.md) | **Ð ÑƒÑÑÐºÐ¸Ð¹** [à°¤à±†à°²à±à°—à±](https://github.com/huggingface/transformers//blob/main/README_te.md) |\n\n### Ð¡Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ðµ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ JAX, PyTorch Ð¸ TensorFlow\n\n### \n\nðŸ¤— Transformers Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ñ‚Ñ‹ÑÑÑ‡Ð¸ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡, Ñ‚Ð°ÐºÐ¸Ñ… ÐºÐ°Ðº Ñ‚ÐµÐºÑÑ‚, Ð·Ñ€ÐµÐ½Ð¸Ðµ Ð¸ Ð°ÑƒÐ´Ð¸Ð¾."", ""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a PyTorch tensor.""]","The default checkpoint used by the sentiment analysis pipeline in the Transformers library is ""distilbert base uncased finetuned sst2 english.""",distilbert base uncased finetuned sst2 english
"What is the purpose of the notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi""?
","[""Note that while [CUDA graphs](https://developer.nvidia.com/blog/cuda-graphs/) are not currently compatible with model parallelism in DeepSpeed (DeepSpeed v0.8.2, see [here](https://github.com/microsoft/DeepSpeed/blob/v0.8.2/deepspeed/inference/engine.py#L158)), HPU graphs are supported in Habana's DeepSpeed fork. All benchmarks are doing [greedy generation](https://huggingface.co/blog/how-to-generate#greedy-search) of 100 token outputs. The input prompt is:\n> \""DeepSpeed is a machine learning framework\""\nwhich consists of 7 tokens with BLOOM's tokenizer. The results for inference latency are displayed in the table below (the unit is \\*seconds\\*). | Model | Number of devices | Gaudi2 latency (seconds) | A100-80GB latency (seconds) | First-gen Gaudi latency (seconds) |\n|:-----------:|:-----------------:|:-------------------------:|:-----------------:|:----------------------------------:|\n| BLOOMZ | 8 | 3.103 | 4.402 | / |\n| BLOOMZ-7B | 8 | 0.734 | 2.417 | 3.321 |\n| BLOOMZ-7B | 1 | 0.772 | 2.119 | 2.387 |\n\\*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, leading to a\\* x\\*1.42 speedup on BLOOMZ with Gaudi2 compared to A100.\\*\nThe Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. \\*\\*For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB\\*\\*. Smaller checkpoints present interesting results too. \\*\\*Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!\\*\\* It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device. We also ran these models on first-gen Gaudi. While it is slower than Gaudi2, it is interesting from a price perspective as a DL1 instance on AWS costs approximately 13\\$ per hour. Latency for BLOOMZ-7B on first-gen Gaudi is 2.387 seconds. Thus, \\*\\*first-gen Gaudi offers for the 7-billion checkpoint a better price-performance ratio than A100\\*\\* which costs more than 30\\$ per hour! We expect the Habana team will optimize the performance of these models in the upcoming SynapseAI releases. For example, in our last benchmark, we saw that [Gaudi2 performs Stable Diffusion inference 2.2x faster than A100](https://huggingface.co/blog/habana-gaudi-2-benchmark#generating-images-from-text-with-stable-diffusion) and this has since been improved further to 2.37x with the latest optimizations provided by Habana. We will update these numbers as new versions of SynapseAI are released and integrated within Optimum Habana."", ""--\ntitle: Getting Started with Transformers on Habana Gaudi\nthumbnail: /blog/assets/61\\_getting\\_started\\_habana/habana01.png\nauthors:\n- user: juliensimon\n---\n# Getting Started with Transformers on Habana Gaudi\nA couple of weeks ago, we've had the pleasure to [announce](https://huggingface.co/blog/habana) that [Habana Labs](https://habana.ai) and [Hugging Face](https://huggingface.co/) would partner to accelerate Transformer model training. Habana Gaudi accelerators deliver up to 40% better price performance for training machine learning models compared to the latest GPU-based Amazon EC2 instances. We are super excited to bring this price performance advantages to Transformers ðŸš€\nIn this hands-on post, I'll show you how to quickly set up a Habana Gaudi instance on Amazon Web Services, and then fine-tune a BERT model for text classification. As usual, all code is provided so that you may reuse it in your projects. Let's get started! ## Setting up an Habana Gaudi instance on AWS\nThe simplest way to work with Habana Gaudi accelerators is to launch an Amazon EC2 [DL1](https://aws.amazon.com/ec2/instance-types/dl1/) instance. These instances are equipped with 8 Habana Gaudi processors that can easily be put to work thanks to the [Habana Deep Learning Amazon Machine Image](https://aws.amazon.com/marketplace/server/procurement?productId=9a75c51a-a4d1-4470-884f-6be27933fcc8) (AMI)."", ""--\ntitle: \""Accelerate Large Model Training using DeepSpeed\""\nthumbnail: /blog/assets/83\\_accelerate\\_deepspeed/deepspeed-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n# Accelerate Large Model Training using DeepSpeed\nIn this post we will look at how we can leverage the \\*\\*[Accelerate](https://github.com/huggingface/accelerate)\\*\\* library for training large models which enables users to leverage the ZeRO features of \\*\\*[DeeSpeed](https://www.deepspeed.ai)\\*\\*. # Motivation ðŸ¤—\n\\*\\*Tired of Out of Memory (OOM) errors while trying to train large models? We've got you covered. Large models are very performant [1] but difficult to train with the available hardware. To get the most of the available hardware for training large models one can leverage Data Parallelism using ZeRO - Zero Redundancy Optimizer [2]\\*\\*. Below is a short description of Data Parallelism using ZeRO with diagram from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)\n![ZeRO Data Parallelism](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero.png)\n(Source: [link](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/))\na."", ""This final run is thus x1.41 faster than our base run on Gaudi2 \\*\\*simply adding 2 ready-to-use training arguments.\\*\\* It is also \\*\\*x1.41 faster than H100\\*\\* and \\*\\*x2.45 faster than A100\\*\\* with `dataloader\\_num\\_workers=2`! ### Reproducing this benchmark\nTo reproduce this benchmark, you first need to get access to Gaudi2 through the [Intel Developer Cloud](https://www.intel.com/content/www/us/en/secure/developer/devcloud/cloud-launchpad.html) (see [this guide](https://huggingface.co/blog/habana-gaudi-2-benchmark#how-to-get-access-to-gaudi2) for more information). Then, you need to install the latest version of Optimum Habana and run `run\\_bridgetower.py` which you can find [here](https://github.com/huggingface/optimum-habana/blob/main/examples/contrastive-image-text/run\\_bridgetower.py). Here is how to do it:\n```bash\npip install optimum[habana]\ngit clone https://github.com/huggingface/optimum-habana.git\ncd optimum-habana/examples/contrastive-image-text\npip install -r requirements.txt\n```\nThe base command line to run the script is:\n```bash\npython ../gaudi\\_spawn.py --use\\_mpi --world\\_size 8 run\\_bridgetower.py \\\n--output\\_dir /tmp/bridgetower-test \\\n--model\\_name\\_or\\_path BridgeTower/bridgetower-large-itm-mlm-itc \\\n--dataset\\_name jmhessel/newyorker\\_caption\\_contest --dataset\\_config\\_name matching \\\n--dataset\\_revision 3c6c4f6c0ff7e902833d3afa5f8f3875c2b036e6 \\\n--image\\_column image --caption\\_column image\\_description \\\n--remove\\_unused\\_columns=False \\\n--do\\_train --do\\_eval --do\\_predict \\\n--per\\_device\\_train\\_batch\\_size=\""40\"" --per\\_device\\_eval\\_batch\\_size=\""16\"" \\\n--num\\_train\\_epochs 5 \\\n--learning\\_rate=\""1e-5\"" \\\n--push\\_to\\_hub --report\\_to tensorboard --hub\\_model\\_id bridgetower\\\n--overwrite\\_output\\_dir \\\n--use\\_habana --use\\_lazy\\_mode --use\\_hpu\\_graphs\\_for\\_inference --gaudi\\_config\\_name Habana/clip \\\n--throughput\\_warmup\\_steps 3 \\\n--logging\\_steps 10\n```\nwhich corresponds to the case `--dataloader\\_num\\_workers 0`. You can then add `--dataloader\\_num\\_workers N` and `--mediapipe\\_dataloader` to test other configurations. To push your model and Tensorboard logs to the Hugging Face Hub, you will have to log in to your account beforehand with:\n```bash\nhuggingface-cli login\n```\nFor A100 and H100, you can use the same `run\\_bridgetower.py` script with a few small changes:\n- Replace `GaudiTrainer` and `GaudiTrainingArguments` with `Trainer` and `TrainingArguments` from Transformers\n- Remove references to `GaudiConfig`, `gaudi\\_config` and `HabanaDataloaderTrainer`\n- Import `set\\_seed` directly from Transformers: `from transformers import set\\_seed`\nThe results displayed in this benchmark were obtained with a Nvidia H100 Lambda instance and a Nvidia A100 80GB GCP instance both with 8 devices using [Nvidia's Docker images](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/index.html)."", ""Note that `--mediapipe\\_dataloader` is compatible with Gaudi2 only and will not work with A100/H100. Regarding fp8 results on H100 using [Transformer Engine](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/index.html), they are not available because the code crashes and would require modifying the modeling of BridgeTower in Transformers. We will revisit this comparison when fp8 is supported on Gaudi2. ## Conclusion\nWhen dealing with images, we presented two solutions to speed up your training workflows: allocating more resources to the dataloader, and decoding and augmenting images directly on accelerator devices rather than on CPU. We showed that it leads to dramatic speedups when training a SOTA vision-language model like BridgeTower: \\*\\*Habana Gaudi2 with Optimum Habana is about x1.4 faster than Nvidia H100 and x2.5 faster than Nvidia A100 80GB with Transformers!\\*\\*\nAnd this is super easy to use as you just need to provide a few additional training arguments. To go further, we are looking forward to using HPU graphs for training models even faster and to presenting how to use DeepSpeed ZeRO-3 on Gaudi2 to accelerate the training of your LLMs."", ""`![](assets/61_getting_started_habana/habana04.png)`\nAs a next step, I make sure that the instance allows incoming ```ssh``` traffic. I do not restrict the source address for simplicity, but you should definitely do it in your account. `![](assets/61_getting_started_habana/habana05.png)`\nBy default, this AMI will start an instance with 8GB of Amazon EBS storage, which won't be enough here. I bump storage to 50GB. `![](assets/61_getting_started_habana/habana08.png)`\nNext, I assign an Amazon IAM role to the instance. In real life, this role should have the minimum set of permissions required to run your training job, such as the ability to read data from one of your Amazon S3 buckets. This role is not needed here as the dataset will be downloaded from the Hugging Face hub. If you're not familiar with IAM, I highly recommend reading the [Getting Started](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started.html) documentation. Then, I ask EC2 to provision my instance as a [Spot Instance](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html), a great way to reduce the $13.11 per hour cost. `![](assets/61_getting_started_habana/habana06.png)`\nFinally, I launch the instance. A couple of minutes later, the instance is ready and I can connect to it with ```ssh```. Windows users can do the same with \\*PuTTY\\* by following the [documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html). ```\nssh -i ~/.ssh/julsimon-keypair.pem ubuntu@ec2-18-207-189-109.compute-1.amazonaws.com\n```\nOn this instance, the last setup step is to pull the Habana container for PyTorch, which is the framework I'll use to fine-tune my model. You can find information on other prebuilt containers and on how to build your own in the Habana [documentation](https://docs.habana.ai/en/latest/Installation\\_Guide/index.html). ```\ndocker pull \\\nvault.habana.ai/gaudi-docker/1.5.0/ubuntu20.04/habanalabs/pytorch-installer-1.11.0:1.5.0-610\n```\nOnce the image has been pulled to the instance, I run it in interactive mode. ```\ndocker run -it \\\n--runtime=habana \\\n-e HABANA\\_VISIBLE\\_DEVICES=all \\\n-e OMPI\\_MCA\\_btl\\_vader\\_single\\_copy\\_mechanism=none \\\n--cap-add=sys\\_nice \\\n--net=host \\\n--ipc=host vault.habana.ai/gaudi-docker/1.5.0/ubuntu20.04/habanalabs/pytorch-installer-1.11.0:1.5.0-610\n```\nI'm now ready to fine-tune my model. ## Fine-tuning a text classification model on Habana Gaudi\nI first clone the [Optimum Habana](https://github.com/huggingface/optimum-habana) repository inside the container I've just started. ```\ngit clone https://github.com/huggingface/optimum-habana.git\n```\nThen, I install the Optimum Habana package from source. ```\ncd optimum-habana\npip install . ```\nThen, I move to the subdirectory containing the text classification example and install the required Python packages. ```\ncd examples/text-classification\npip install -r requirements.txt\n```\nI can now launch the training job, which downloads the [bert-large-uncased-whole-word-masking](https://huggingface.co/bert-large-uncased-whole-word-masking) model from the Hugging Face hub, and fine-tunes it on the [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) task of the [GLUE](https://gluebenchmark.com/) benchmark."", ""The `100\\_000` steps with a global batch size of `256` took around 12,5 hours. BERT was originally pre-trained on [1 Million Steps](https://arxiv.org/pdf/1810.04805.pdf) with a global batch size of `256`:\n> We train with batch size of 256 sequences (256 sequences \\* 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40 epochs over the 3.3 billion word corpus. Meaning if we want to do a full pre-training it would take around 125h hours (12,5 hours \\* 10) and would cost us around ~$1,650 using Habana Gaudi on AWS, which is extremely cheap. For comparison, the DeepSpeed Team, who holds the record for the [fastest BERT-pretraining](https://www.deepspeed.ai/tutorials/bert-pretraining/), [reported](https://www.deepspeed.ai/tutorials/bert-pretraining/) that pre-training BERT on 1 [DGX-2](https://www.nvidia.com/en-us/data-center/dgx-2/) (powered by 16 NVIDIA V100 GPUs with 32GB of memory each) takes around 33,25 hours. To compare the cost we can use the [p3dn.24xlarge](https://aws.amazon.com/de/ec2/instance-types/p3/) as reference, which comes with 8x NVIDIA V100 32GB GPUs and costs ~31,22$/h. We would need two of these instances to have the same \""setup\"" as the one DeepSpeed reported, for now we are ignoring any overhead created to the multi-node setup (I/O, Network etc.). This would bring the cost of the DeepSpeed GPU based training on AWS to around ~$2,075, which is 25% more than what Habana Gaudi currently delivers. \\_Something to note here is that using [DeepSpeed](https://www.deepspeed.ai/tutorials/bert-pretraining/#deepspeed-single-gpu-throughput-results) in general improves the performance by a factor of ~1.5 - 2. A factor of ~1.5 - 2x, means that the same pre-training job without DeepSpeed would likely take twice as long and cost twice as much or ~$3-4k.\\_\nWe are looking forward on to do the experiment again once the [Gaudi DeepSpeed integration](https://docs.habana.ai/en/latest/PyTorch/DeepSpeed/DeepSpeed\\_User\\_Guide.html#deepspeed-configs) is more widely available. ## Conclusion\nThat's it for this Tutorial. Now you know the basics on how to pre-train BERT from scratch using Hugging Face Transformers and Habana Gaudi. You also saw how easy it is to migrate from the `Trainer` to the `GaudiTrainer`. We compared our implementation with the [fastest BERT-pretraining](https://www.deepspeed.ai/Tutorials/bert-pretraining/) results and saw that Habana Gaudi still delivers a 25% cost reduction and allows us to pre-train BERT for ~$1,650. Those results are incredible since it will allow companies to adapt their pre-trained models to their language and domain to [improve accuracy up to 10%](https://huggingface.co/pile-of-law/legalbert-large-1.7M-1#evaluation-results) compared to the general BERT models. If you are interested in training your own BERT or other Transformers models from scratch to reduce cost and improve accuracy, [contact our experts](mailto:expert-acceleration@huggingface.co) to learn about our [Expert Acceleration Program](https://huggingface.co/support). To learn more about Habana solutions, [read about our partnership and how to contact them](https://huggingface.co/hardware/habana). Code: [pre-training-bert.ipynb](https://github.com/philschmid/deep-learning-habana-huggingface/blob/master/pre-training/pre-training-bert.ipynb)\n---\nThanks for reading!"", ""Although these devices have quite a lot of memory, the model is so large that a single device is not enough to contain a single instance of BLOOMZ. To solve this issue, we are going to use [DeepSpeed](https://www.deepspeed.ai/), which is a deep learning optimization library that enables many memory and speed improvements to accelerate the model and make it fit the device. In particular, we rely here on [DeepSpeed-inference](https://arxiv.org/abs/2207.00032): it introduces several features such as [model (or pipeline) parallelism](https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism) to make the most of the available devices. For Gaudi2, we use [Habana's DeepSpeed fork](https://github.com/HabanaAI/deepspeed) that adds support for HPUs. ### Latency\nWe measured latencies (batch of one sample) for two different sizes of BLOOMZ, both with multi-billion parameters:\n- [176 billion](https://huggingface.co/bigscience/bloomz) parameters\n- [7 billion](https://huggingface.co/bigscience/bloomz-7b1) parameters\nRuns were performed with DeepSpeed-inference in 16-bit precision with 8 devices and using a [key-value cache](https://huggingface.co/docs/transformers/v4.27.1/en/model\\_doc/bloom#transformers.BloomForCausalLM.forward.use\\_cache)."", ""The explosion occurred at the SpaceX facility in Boca Chica, Texas. The Starship prototype was being tested for its ability to withstand the pressure of flight. The explosion occurred at']\n--------------------------------------------------------------------------------------------------\nBatch nÂ°4\nInput: ['Scalene is a high-performance CPU and memory profiler for Python.']\nOutput: ['Scalene is a high-performance CPU and memory profiler for Python. It is designed to be a lightweight, portable, and easy-to-use profiler. Scalene is a Python package that can be installed on any platform that supports Python. Scalene is a lightweight, portable, and easy-to-use profiler']\n--------------------------------------------------------------------------------------------------\nBatch nÂ°5\nInput: ['With the rise of cheap small \""Cube Satellites\"", startups are now']\nOutput: ['With the rise of cheap small \""Cube Satellites\"", startups are now able to launch their own satellites for a fraction of the cost of a traditional launch. This has led to a proliferation of small satellites, which are now being used for a wide range of applications. The most common use of small satellites is for communications,']\n```\nIn the next section, we explain how to use the script we wrote to perform this benchmark or to apply it on any dataset you like from the Hugging Face Hub! ### How to reproduce these results? The script used for benchmarking BLOOMZ on Gaudi2 and first-gen Gaudi is available [here](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation). Before running it, please make sure that the latest versions of SynapseAI and the Gaudi drivers are installed following [the instructions given by Habana](https://docs.habana.ai/en/latest/Installation\\_Guide/index.html). Then, run the following:\n```bash\ngit clone https://github.com/huggingface/optimum-habana.git\ncd optimum-habana && pip install . && cd examples/text-generation\npip install git+https://github.com/HabanaAI/DeepSpeed.git@1.9.0\n```\nFinally, you can launch the script as follows:\n```bash\npython ../gaudi\\_spawn.py --use\\_deepspeed --world\\_size 8 run\\_generation.py --model\\_name\\_or\\_path bigscience/bloomz --use\\_hpu\\_graphs --use\\_kv\\_cache --max\\_new\\_tokens 100\n```\nFor multi-node inference, you can follow [this guide](https://huggingface.co/docs/optimum/habana/usage\\_guides/multi\\_node\\_training) from the documentation of Optimum Habana."", ""--\ntitle: \""Faster Training and Inference: Habana GaudiÂ®2 vs Nvidia A100 80GB\""\nthumbnail: /blog/assets/habana-gaudi-2-benchmark/thumbnail.png\nauthors:\n- user: regisss\n---\n# Faster Training and Inference: Habana GaudiÂ®-2 vs Nvidia A100 80GB\nIn this article, you will learn how to use [HabanaÂ® GaudiÂ®2](https://habana.ai/training/gaudi2/) to accelerate model training and inference, and train bigger models with ðŸ¤— [Optimum Habana](https://huggingface.co/docs/optimum/habana/index). Then, we present several benchmarks including BERT pre-training, Stable Diffusion inference and T5-3B fine-tuning, to assess the performance differences between first generation Gaudi, Gaudi2 and Nvidia A100 80GB. Spoiler alert - Gaudi2 is about twice faster than Nvidia A100 80GB for both training and inference! [Gaudi2](https://habana.ai/training/gaudi2/) is the second generation AI hardware accelerator designed by Habana Labs. A single server contains 8 accelerator devices with 96GB of memory each (versus 32GB on first generation Gaudi and 80GB on A100 80GB). The Habana SDK, [SynapseAI](https://developer.habana.ai/), is common to both first-gen Gaudi and Gaudi2. That means that ðŸ¤— Optimum Habana, which offers a very user-friendly interface between the ðŸ¤— Transformers and ðŸ¤— Diffusers libraries and SynapseAI, \\*\\*works the exact same way on Gaudi2 as on first-gen Gaudi!\\*\\*\nSo if you already have ready-to-use training or inference workflows for first-gen Gaudi, we encourage you to try them on Gaudi2, as they will work without any single change. ## How to Get Access to Gaudi2? One of the easy, cost-efficient ways that Intel and Habana have made Gaudi2 available is on the Intel Developer Cloud. To start using Gaudi2 there, you should follow the following steps:\n1."", ""The Gaudi runs were performed in \\*bfloat16\\* precision and the A100 runs in \\*fp16\\* precision (more information [here](https://huggingface.co/docs/diffusers/optimization/fp16)). All runs were \\*single-device\\* runs. ### Fine-tuning T5-3B\nWith 96 GB of memory per device, Gaudi2 enables running much bigger models. For instance, we managed to fine-tune T5-3B (containing 3 billion parameters) with gradient checkpointing being the only applied memory optimization. This is not possible on first-gen Gaudi. [Here](https://huggingface.co/regisss/t5-3b-summarization-gaudi-2/tensorboard?scroll=1#scalars) are the logs of this run where the model was fine-tuned on the CNN DailyMail dataset for text summarization using [this script](https://github.com/huggingface/optimum-habana/tree/main/examples/summarization). The results we achieved are presented in the table below. \\*\\*Gaudi2 is x2.44 faster than A100 80GB.\\*\\* We observe that we cannot fit a batch size larger than 1 on Gaudi2 here. This is due to the memory space taken by the graph where operations are accumulated during the first iteration of the run. Habana is working on optimizing the memory footprint in future releases of SynapseAI. We are looking forward to expanding this benchmark using newer versions of Habana's SDK and also using [DeepSpeed](https://www.deepspeed.ai/) to see if the same trend holds. | | First-gen Gaudi | Gaudi2 (BS=1) | A100 (BS=16) |\n|:-:|:-------:|:--------------:|:------------:|\n| Throughput (samples/s) | N/A | 19.7 | 8.07 |\n| Speedup | / | x2.44 | x1.0 |\n\\*BS\\* is the batch size per device."", ""Below is a fun conversation with the chatbot ðŸ’¬. You can find snapshots of more conversations using different contexts [here](https://github.com/pacman100/accelerate-deepspeed-test/tree/main/src/chatbot\\_snapshots). ![Chatbot](./assets/83\\_accelerate\\_deepspeed/chatbot.png)\n---\n## CPU/Disk Offloading to enable training humongous models that wonâ€™t fit the GPU memory\nOn a single 24GB NVIDIA Titan RTX GPU, one cannot train GPT-XL Model (1.5B parameters) even with a batch size of 1. We will look at how we can use DeepSpeed ZeRO Stage-3 with CPU offloading of optimizer states, gradients and parameters to train GPT-XL Model. We will leverage the DeepSpeed Zero Stage-3 CPU offload config [zero3\\_offload\\_config\\_accelerate.json](https://github.com/pacman100/accelerate-deepspeed-test/blob/main/src/modeling/configs/zero3\\_offload\\_config\\_accelerate.json) (given below) for training. The rest of the process of using the config with ðŸ¤— `accelerate` is similar to the above experiment. ```json\n{\n\""fp16\"": {\n\""enabled\"": true,\n\""loss\\_scale\"": 0,\n\""loss\\_scale\\_window\"": 1000,\n\""initial\\_scale\\_power\"": 16,\n\""hysteresis\"": 2,\n\""min\\_loss\\_scale\"": 1\n},\n\""optimizer\"": {\n\""type\"": \""AdamW\"",\n\""params\"": {\n\""lr\"": \""auto\"",\n\""weight\\_decay\"": \""auto\""\n}\n},\n\""scheduler\"": {\n\""type\"": \""WarmupDecayLR\"",\n\""params\"": {\n\""warmup\\_min\\_lr\"": \""auto\"",\n\""warmup\\_max\\_lr\"": \""auto\"",\n\""warmup\\_num\\_steps\"": \""auto\"",\n\""total\\_num\\_steps\"": \""auto\""\n}\n},\n\""zero\\_optimization\"": {\n\""stage\"": 3,\n\""offload\\_optimizer\"": {\n\""device\"": \""cpu\"",\n\""pin\\_memory\"": true\n},\n\""offload\\_param\"": {\n\""device\"": \""cpu\"",\n\""pin\\_memory\"": true\n},\n\""overlap\\_comm\"": true,\n\""contiguous\\_gradients\"": true,\n\""reduce\\_bucket\\_size\"": \""auto\"",\n\""stage3\\_prefetch\\_bucket\\_size\"": \""auto\"",\n\""stage3\\_param\\_persistence\\_threshold\"": \""auto\"",\n\""sub\\_group\\_size\"": 1e9,\n\""stage3\\_max\\_live\\_parameters\"": 1e9,\n\""stage3\\_max\\_reuse\\_distance\"": 1e9,\n\""stage3\\_gather\\_16bit\\_weights\\_on\\_model\\_save\"": true\n},\n\""gradient\\_accumulation\\_steps\"": 1,\n\""gradient\\_clipping\"": \""auto\"",\n\""steps\\_per\\_print\"": 2000,\n\""train\\_batch\\_size\"": \""auto\"",\n\""train\\_micro\\_batch\\_size\\_per\\_gpu\"": \""auto\"",\n\""wall\\_clock\\_breakdown\"": false\n}\n```\n\\*\\*ZeRO Stage-3 CPU Offload DeepSpeed Config File Example\\*\\*\n```bash\ncompute\\_environment: LOCAL\\_MACHINE\ndeepspeed\\_config:\ndeepspeed\\_config\\_file: /path/to/zero3\\_offload\\_config\\_accelerate.json\nzero3\\_init\\_flag: true\ndistributed\\_type: DEEPSPEED\nfsdp\\_config: {}\nmachine\\_rank: 0\nmain\\_process\\_ip: null\nmain\\_process\\_port: null\nmain\\_training\\_function: main\nmixed\\_precision: fp16\nnum\\_machines: 1\nnum\\_processes: 2\nuse\\_cpu: false\n```\nNow, run below command for training:\n```bash\naccelerate launch run\\_clm\\_no\\_trainer.py \\\n--config\\_name \""gpt2-xl\"" \\\n--tokenizer\\_name \""gpt2-xl\"" \\\n--dataset\\_name \""wikitext\"" \\\n--dataset\\_config\\_name \""wikitext-2-raw-v1\"" \\\n--block\\_size 128 \\\n--output\\_dir \""/tmp/clm\\_deepspeed\\_stage3\\_offload\\_\\_accelerate\"" \\\n--learning\\_rate 5e-4 \\\n--per\\_device\\_train\\_batch\\_size 16 \\\n--per\\_device\\_eval\\_batch\\_size 1 \\\n--num\\_train\\_epochs 1 \\\n--with\\_tracking \\\n--report\\_to \""wandb\""\\\n```\n---\n| Method | Batch Size Max | Train time per epoch (seconds) | Notes |\n| --- | --- | --- | --- |\n| DDP (Distributed Data Parallel) | - | - | OOM Error\n| DeepSpeed ZeRO Stage 3 | \\*\\*16\\*\\* | 6608.35 | |\nTable 3: Benchmarking DeepSpeed ZeRO Stage-3 CPU Offload on GPT-XL (1.5B) model\n---\nDDP will result in OOM error even with batch size 1. On the other hand, with DeepSpeed ZeRO Stage-3 CPU offload, we can train with a batch size of 16. Finally, please, remember that, ðŸ¤— `Accelerate` only integrates DeepSpeed, therefore if you\nhave any problems or questions with regards to DeepSpeed usage, please, file an issue with [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed/issues). # References\n[1] [Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers](http://nlp.cs.berkeley.edu/pubs/Li-Wallace-Shen-Lin-Keutzer-Klein-Gonzalez\\_2020\\_Transformers\\_paper.pdf)\n[2] [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054v3.pdf)\n[3] [DeepSpeed: Extreme-scale model training for everyone - Microsoft Research](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)\n[4] [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale)\n[5] [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface.co/blog/pytorch-fsdp)\n[6] [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf)"", ""--\ntitle: \""Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator\""\nthumbnail: /blog/assets/habana-gaudi-2-bloom/thumbnail.png\nauthors:\n- user: regisss\n---\n# Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator\nThis article will show you how to easily deploy large language models with hundreds of billions of parameters like BLOOM on [HabanaÂ® GaudiÂ®2](https://habana.ai/training/gaudi2/) using ðŸ¤— [Optimum Habana](https://huggingface.co/docs/optimum/habana/index), which is the bridge between Gaudi2 and the ðŸ¤— Transformers library. As demonstrated in the benchmark presented in this post, this will enable you to \\*\\*run inference faster than with any GPU currently available on the market\\*\\*. As models get bigger and bigger, deploying them into production to run inference has become increasingly challenging. Both hardware and software have seen a lot of innovations to address these challenges, so let's dive in to see how to efficiently overcome them! ## BLOOMZ\n[BLOOM](https://arxiv.org/abs/2211.05100) is a 176-billion-parameter autoregressive model that was trained to complete sequences of text. It can handle 46 different languages and 13 programming languages. Designed and trained as part of the [BigScience](https://bigscience.huggingface.co/) initiative, BLOOM is an open-science project that involved a large number of researchers and engineers all over the world. More recently, another model with the exact same architecture was released: [BLOOMZ](https://arxiv.org/abs/2211.01786), which is a fine-tuned version of BLOOM on several tasks leading to better generalization and zero-shot[^1] capabilities. Such large models raise new challenges in terms of memory and speed for both [training](https://huggingface.co/blog/bloom-megatron-deepspeed) and [inference](https://huggingface.co/blog/bloom-inference-optimization). Even in 16-bit precision, one instance requires 352 GB to fit! You will probably struggle to find any device with so much memory at the moment, but state-of-the-art hardware like Habana Gaudi2 does make it possible to perform inference on BLOOM and BLOOMZ models with low latencies. ## Habana Gaudi2\n[Gaudi2](https://habana.ai/training/gaudi2/) is the second-generation AI hardware accelerator designed by Habana Labs. A single server contains 8 accelerator devices (called Habana Processing Units, or HPUs) with 96GB of memory each, which provides room to make very large models fit in. However, hosting the model is not very interesting if the computation is slow. Fortunately, Gaudi2 shines on that aspect: it differs from GPUs in that its architecture enables the accelerator to perform General Matrix Multiplication (GeMM) and other operations in parallel, which speeds up deep learning workflows. These features make Gaudi2 a great candidate for LLM training and inference. Habana's SDK, SynapseAIâ„¢, supports PyTorch and DeepSpeed for accelerating LLM training and inference. The [SynapseAI graph compiler](https://docs.habana.ai/en/latest/Gaudi\\_Overview/SynapseAI\\_Software\\_Suite.html#graph-compiler-and-runtime) will optimize the execution of the operations accumulated in the graph (e.g. operator fusion, data layout management, parallelization, pipelining and memory management, and graph-level optimizations). Moreover, support for [HPU graphs](https://docs.habana.ai/en/latest/PyTorch/Inference\\_on\\_PyTorch/Inference\\_Using\\_HPU\\_Graphs.html) and [DeepSpeed-inference](https://docs.habana.ai/en/latest/PyTorch/DeepSpeed/Inference\\_Using\\_DeepSpeed.html) have just recently been introduced in SynapseAI, and these are well-suited for latency-sensitive applications as shown in our benchmark below. All these features are integrated into the ðŸ¤— [Optimum Habana](https://github.com/huggingface/optimum-habana) library so that deploying your model on Gaudi is very simple. Check out the quick-start page [here](https://huggingface.co/docs/optimum/habana/quickstart). If you would like to get access to Gaudi2, go to the [Intel Developer Cloud](https://www.intel.com/content/www/us/en/secure/developer/devcloud/cloud-launchpad.html) and follow [this guide](https://huggingface.co/blog/habana-gaudi-2-benchmark#how-to-get-access-to-gaudi2). ## Benchmarks\nIn this section, we are going to provide an early benchmark of BLOOMZ on Gaudi2, first-generation Gaudi and Nvidia A100 80GB."", ""Gaudi2 and A100 runs were performed in fp32 with gradient checkpointing enabled. All runs were \\*distributed\\* runs on \\*8 devices\\*. ## Conclusion\nIn this article, we discuss our first experience with Gaudi2. The transition from first generation Gaudi to Gaudi2 is completely seamless since SynapseAI, Habana's SDK, is fully compatible with both. This means that new optimizations proposed by future releases will benefit both of them. You have seen that Habana Gaudi2 significantly improves performance over first generation Gaudi and delivers about twice the throughput speed as Nvidia A100 80GB for both training and inference. You also know now how to setup a Gaudi2 instance through the Intel Developer Zone. Check out the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples) you can easily run on it with ðŸ¤— Optimum Habana. If you are interested in accelerating your Machine Learning training and inference workflows using the latest AI hardware accelerators and software libraries, check out our [Expert Acceleration Program](https://huggingface.co/support). To learn more about Habana solutions, [read about our partnership here](https://huggingface.co/hardware/habana) and [contact them](https://habana.ai/contact-us/). To learn more about Hugging Face efforts to make AI hardware accelerators easy to use, check out our [Hardware Partner Program](https://huggingface.co/hardware). ### Related Topics\n- [Getting Started on Transformers with Habana Gaudi](https://huggingface.co/blog/getting-started-habana)\n- [Accelerate Transformer Model Training with Hugging Face and Habana Labs](https://developer.habana.ai/events/accelerate-transformer-model-training-with-hugging-face-and-habana-labs/)\n---\nThanks for reading!"", ""We are also looking forward to the performance benefits that will come with FP8 inference on Gaudi2. We also presented the results achieved with first-generation Gaudi. For smaller models, it can perform on par with or even better than A100 for almost a third of its price. It is a good alternative option to using GPUs for running inference with such a big model like BLOOMZ. If you are interested in accelerating your Machine Learning training and inference workflows using the latest AI hardware accelerators and software libraries, check out our [Expert Acceleration Program](https://huggingface.co/support). To learn more about Habana solutions, [read about our partnership and contact them here](https://huggingface.co/hardware/habana). To learn more about Hugging Face efforts to make AI hardware accelerators easy to use, check out our [Hardware Partner Program](https://huggingface.co/hardware). ### Related Topics\n- [Faster Training and Inference: Habana Gaudi-2 vs Nvidia A100 80GB](https://huggingface.co/blog/habana-gaudi-2-benchmark)\n- [Leverage DeepSpeed to Train Faster and Cheaper Large Scale Transformer Models with Hugging Face and Habana Labs Gaudi](https://developer.habana.ai/events/leverage-deepspeed-to-train-faster-and-cheaper-large-scale-transformer-models-with-hugging-face-and-habana-labs-gaudi/)\n---\nThanks for reading!""]","The notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi"" is designed to demonstrate how to leverage the DeepSpeed library in conjunction with Habana Gaudi hardware to efficiently train large-scale models that contain billions of parameters. It provides practical guidance on setting up the training environment, utilizing DeepSpeed's features for memory optimization and speed improvements, and effectively managing the training process for such large models. This resource aims to help users maximize the performance of their training workflows on Habana Gaudi accelerators.",To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.
"What command line module does PyTorch provide to run a script on multiple GPUs?
","[""\\* correct / len(test\\_loader.dataset)}')\n```\nTypically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook. However, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through \\*distributed\\* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\n## PyTorch Distributed Data Parallelism\nAs the name implies, `torch.distributed` is meant to work on \\*distributed\\* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both. To convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp\\_tutorial.html)\nFirst a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through\n> Note: for this section of the tutorial it should be assumed these are sent in python script files. Later on a launcher using Accelerate will be discussed that removes this necessity\n```python\nimport os\nimport torch.distributed as dist\ndef setup(rank, world\\_size):\n\""Sets up the process group and configuration for PyTorch Distributed Data Parallelism\""\nos.environ[\""MASTER\\_ADDR\""] = 'localhost'\nos.environ[\""MASTER\\_PORT\""] = \""12355\""\n# Initialize the process group\ndist.init\\_process\\_group(\""gloo\"", rank=rank, world\\_size=world\\_size)\ndef cleanup():\n\""Cleans up the distributed environment\""\ndist.destroy\\_process\\_group()\n```\nThe last piece of the puzzle is \\*how do I send my data and model to another GPU?\\*\nThis is where the `DistributedDataParallel` module comes into play. It will copy your model onto each GPU, and when `loss.backward()` is called the backpropagation is performed and the resulting gradients across all these copies of the model will be averaged/reduced. This ensures each device has the same weights post the optimizer step. Below is an example of our training setup, refactored as a function, with this capability:\n> Note: Here rank is the overall rank of the current GPU compared to all the other GPUs available, meaning they have a rank of `0 -> n-1`\n```python\nfrom torch.nn.parallel import DistributedDataParallel as DDP\ndef train(model, rank, world\\_size):\nsetup(rank, world\\_size)\nmodel = model.to(rank)\nddp\\_model = DDP(model, device\\_ids=[rank])\noptimizer = optim.AdamW(ddp\\_model.parameters(), lr=1e-3)\n# Train for one epoch\nmodel.train()\nfor batch\\_idx, (data, target) in enumerate(train\\_loader):\ndata, target = data.to(device), target.to(device)\noutput = model(data)\nloss = F.nll\\_loss(output, target)\nloss.backward()\noptimizer.step()\noptimizer.zero\\_grad()\ncleanup()\n```\nThe optimizer needs to be declared based on the model \\*on the specific device\\* (so `ddp\\_model` and not `model`) for all of the gradients to properly be calculated. Lastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\n```bash\ntorchrun --nproc\\_per\\_node=2 --nnodes=1 example\\_script.py\n```\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch."", "". ! - \\; \\: \\\"" â€œ % â€˜ â€ ï¿½ \\\n--fp16 \\\n--group\\_by\\_length \\\n--push\\_to\\_hub \\\n--do\\_train --do\\_eval\n```\nOn a single V100 GPU, this script should run in \\*ca.\\* 1 hour 20 minutes and yield a CTC loss of \\*\\*0.39\\*\\* and word error rate\nof \\*\\*0.35\\*\\*. ### Multi GPU CTC\nThe following command shows how to fine-tune [XLSR-Wav2Vec2](https://huggingface.co/transformers/main/model\\_doc/xlsr\\_wav2vec2.html) on [Common Voice](https://huggingface.co/datasets/common\\_voice) using 8 GPUs in half-precision. ```bash\ntorchrun \\\n--nproc\\_per\\_node 8 run\\_speech\\_recognition\\_ctc.py \\\n--dataset\\_name=\""common\\_voice\"" \\\n--model\\_name\\_or\\_path=\""facebook/wav2vec2-large-xlsr-53\"" \\\n--dataset\\_config\\_name=\""tr\"" \\\n--output\\_dir=\""./wav2vec2-common\\_voice-tr-demo-dist\"" \\\n--overwrite\\_output\\_dir \\\n--num\\_train\\_epochs=\""15\"" \\\n--per\\_device\\_train\\_batch\\_size=\""4\"" \\\n--learning\\_rate=\""3e-4\"" \\\n--warmup\\_steps=\""500\"" \\\n--evaluation\\_strategy=\""steps\"" \\\n--text\\_column\\_name=\""sentence\"" \\\n--length\\_column\\_name=\""input\\_length\"" \\\n--save\\_steps=\""400\"" \\\n--eval\\_steps=\""100\"" \\\n--logging\\_steps=\""1\"" \\\n--layerdrop=\""0.0\"" \\\n--save\\_total\\_limit=\""3\"" \\\n--freeze\\_feature\\_encoder \\\n--gradient\\_checkpointing \\\n--chars\\_to\\_ignore , ?"", ""Scripts\nA train, validation, inference, and checkpoint cleaning script included in the github root folder. Scripts are not currently packaged in the pip release. The training and validation scripts evolved from early versions of the [PyTorch Imagenet Examples](https://github.com/pytorch/examples). I have added significant functionality over time, including CUDA specific performance enhancements based on\n[NVIDIA's APEX Examples](https://github.com/NVIDIA/apex/tree/master/examples). ## Training Script\nThe variety of training args is large and not all combinations of options (or even options) have been fully tested. For the training dataset folder, specify the folder to the base that contains a `train` and `validation` folder. To train an SE-ResNet34 on ImageNet, locally distributed, 4 GPUs, one process per GPU w/ cosine schedule, random-erasing prob of 50% and per-pixel random value:\n```bash\n./distributed\\_train.sh 4 /data/imagenet --model seresnet34 --sched cosine --epochs 150 --warmup-epochs 5 --lr 0.4 --reprob 0.5 --remode pixel --batch-size 256 --amp -j 4\n```\nIt is recommended to use PyTorch 1.9+ w/ PyTorch native AMP and DDP instead of APEX AMP."", ""The file naming is up to you. It's recommended to use DeepSpeed's `add\\_config\\_arguments` utility to add the necessary command line arguments to your code. For more information please see [DeepSpeed's Argument Parsing](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) doc. You can use a launcher of your choice here. You can continue using the pytorch launcher:\n```bash\ntorch.distributed.run --nproc\\_per\\_node=2 your\\_program.py  --deepspeed ds\\_config.json\n```\nor use the launcher provided by `deepspeed`:\n```bash\ndeepspeed --num\\_gpus=2 your\\_program.py  --deepspeed ds\\_config.json\n```\nAs you can see the arguments aren't the same, but for most needs either of them works. The\nfull details on how to configure various nodes and GPUs can be found [here](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node). When you use the `deepspeed` launcher and you want to use all available gpus you can just omit the `--num\\_gpus` flag. Here is an example of running `run\\_translation.py` under DeepSpeed deploying all available GPUs:\n```bash\ndeepspeed examples/pytorch/translation/run\\_translation.py \\\n--deepspeed tests/deepspeed/ds\\_config\\_zero3.json \\\n--model\\_name\\_or\\_path t5-small --per\\_device\\_train\\_batch\\_size 1 \\\n--output\\_dir output\\_dir --overwrite\\_output\\_dir --fp16 \\\n--do\\_train --max\\_train\\_samples 500 --num\\_train\\_epochs 1 \\\n--dataset\\_name wmt16 --dataset\\_config \""ro-en\"" \\\n--source\\_lang en --target\\_lang ro\n```\nNote that in the DeepSpeed documentation you are likely to see `--deepspeed --deepspeed\\_config ds\\_config.json` - i.e. two DeepSpeed-related arguments, but for the sake of simplicity, and since there are already so many arguments to deal\nwith, we combined the two into a single argument. For some practical usage examples, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400). ### Deployment with one GPU\nTo deploy DeepSpeed with one GPU adjust the [`Trainer`] command line arguments as follows:\n```bash\ndeepspeed --num\\_gpus=1 examples/pytorch/translation/run\\_translation.py \\\n--deepspeed tests/deepspeed/ds\\_config\\_zero2.json \\\n--model\\_name\\_or\\_path t5-small --per\\_device\\_train\\_batch\\_size 1 \\\n--output\\_dir output\\_dir --overwrite\\_output\\_dir --fp16 \\\n--do\\_train --max\\_train\\_samples 500 --num\\_train\\_epochs 1 \\\n--dataset\\_name wmt16 --dataset\\_config \""ro-en\"" \\\n--source\\_lang en --target\\_lang ro\n```\nThis is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly to use just one GPU via\n`--num\\_gpus=1`. By default, DeepSpeed deploys all GPUs it can see on the given node. If you have only 1 GPU to start\nwith, then you don't need this argument. The following [documentation](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) discusses the launcher options."", ""### Older CUDA versions\nSometimes, older CUDA versions may refuse to build with newer compilers. For example, if you have `gcc-9` but CUDA wants `gcc-7`. Usually, installing the latest CUDA toolkit enables support for the newer compiler. You could also install an older version of the compiler in addition to the one you're currently using (or it may already be installed but it's not used by default and the build system can't see it). To resolve this, you can create a symlink to give the build system visibility to the older compiler. ```bash\n# adapt the path to your system\nsudo ln -s /usr/bin/gcc-7 /usr/local/cuda-10.2/bin/gcc\nsudo ln -s /usr/bin/g++-7 /usr/local/cuda-10.2/bin/g++\n```\n## Multi-GPU Network Issues Debug\nWhen training or inferencing with `DistributedDataParallel` and multiple GPU, if you run into issue of inter-communication between processes and/or nodes, you can use the following script to diagnose network issues. ```bash\nwget https://raw.githubusercontent.com/huggingface/transformers/main/scripts/distributed/torch-distributed-gpu-test.py\n```\nFor example to test how 2 GPUs interact do:\n```bash\npython -m torch.distributed.run --nproc\\_per\\_node 2 --nnodes 1 torch-distributed-gpu-test.py\n```\nIf both processes can talk to each and allocate GPU memory each will print an OK status."", ""It is more than linearly faster than run\\_eval.py in some cases\nbecause it uses SortishSampler to minimize padding. You can also use it on 1 GPU. `data\\_dir` must have\n`{type\\_path}.source` and `{type\\_path}.target`. Run `./run\\_distributed\\_eval.py --help` for all clargs. ```bash\ntorchrun --nproc\\_per\\_node=8 run\\_distributed\\_eval.py \\\n--model\\_name sshleifer/distilbart-large-xsum-12-3 \\\n--save\\_dir xsum\\_generations \\\n--data\\_dir xsum \\\n--fp16 # you can pass generate kwargs like num\\_beams here, just like run\\_eval.py\n```\nContributions that implement this command for other distributed hardware setups are welcome! #### Single-GPU Eval: Tips and Tricks\nWhen using `run\\_eval.py`, the following features can be useful:\n\\* if you running the script multiple times and want to make it easier to track what arguments produced that output, use `--dump-args`. Along with the results it will also dump any custom params that were passed to the script. For example if you used: `--num\\_beams 8 --early\\_stopping true`, the output will be:\n```\n{'bleu': 26.887, 'n\\_obs': 10, 'runtime': 1, 'seconds\\_per\\_sample': 0.1, 'num\\_beams': 8, 'early\\_stopping': True}\n```\n`--info` is an additional argument available for the same purpose of tracking the conditions of the experiment. It's useful to pass things that weren't in the argument list, e.g. a language pair `--info \""lang:en-ru\""`. But also if you pass `--info` without a value it will fallback to the current date/time string, e.g. `2020-09-13 18:44:43`. If using `--dump-args --info`, the output will be:\n```\n{'bleu': 26.887, 'n\\_obs': 10, 'runtime': 1, 'seconds\\_per\\_sample': 0.1, 'num\\_beams': 8, 'early\\_stopping': True, 'info': '2020-09-13 18:44:43'}\n```\nIf using `--dump-args --info \""pair:en-ru chkpt=best`, the output will be:\n```\n{'bleu': 26.887, 'n\\_obs': 10, 'runtime': 1, 'seconds\\_per\\_sample': 0.1, 'num\\_beams': 8, 'early\\_stopping': True, 'info': 'pair=en-ru chkpt=best'}\n```\n\\* if you need to perform a parametric search in order to find the best ones that lead to the highest BLEU score, let `run\\_eval\\_search.py` to do the searching for you."", ""Now we can fit very large models into a single GPU, but the training might still be very slow. The simplest strategy in this scenario is data parallelism: we replicate the same training setup into separate GPUs and pass different batches to each GPU. With this, you can parallelize the forward/backward passes of the model and scale with the number of GPUs. ![chapter10\\_ddp.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/chapter10\\_ddp.png)\nWe use either the `transformers.Trainer` or `accelerate`, which both support data parallelism without any code changes, by simply passing arguments when calling the scripts with `torchrun` or `accelerate launch`. The following runs a training script with 8 GPUs on a single machine with `accelerate` and `torchrun`, respectively. ```bash\naccelerate launch --multi\\_gpu --num\\_machines 1 --num\\_processes 8 my\\_accelerate\\_script.py\ntorchrun --nnodes 1 --nproc\\_per\\_node 8 my\\_torch\\_script.py\n```\n## Supervised fine-tuning\nBefore we start training reward models and tuning our model with RL, it helps if the model is already good in the domain we are interested in."", ""So the promise is very attractive - it runs a 30min simulation on the cluster of choice and it comes up with the best\nstrategy to utilise this specific environment. If you add/remove/replace any parts it'll run and re-optimize the plan\nfor that. And then you can train. A different setup will have its own custom optimization. ðŸ¤— Transformers status: Transformers models are FX-trace-able via [transformers.utils.fx](https://github.com/huggingface/transformers/blob/master/src/transformers/utils/fx.py),\nwhich is a prerequisite for FlexFlow, however, changes are required on the FlexFlow side to make it work with Transformers models. ## GPU selection\nWhen training on multiple GPUs, you can specify the number of GPUs to use and in what order. This can be useful for instance when you have GPUs with different computing power and want to use the faster GPU first. The selection process works for both [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) and [DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html) to use only a subset of the available GPUs, and you don't need Accelerate or the [DeepSpeed integration](./main\\_classes/deepspeed). ### Number of GPUs\nFor example, if you have 4 GPUs and you only want to use the first 2:\n\nUse the `--nproc\\_per\\_node` to select how many GPUs to use. ```bash\ntorchrun --nproc\\_per\\_node=2 trainer-program.py ... ```\n\nUse `--num\\_processes` to select how many GPUs to use. ```bash\naccelerate launch --num\\_processes 2 trainer-program.py ... ```\n\nUse `--num\\_gpus` to select how many GPUs to use. ```bash\ndeepspeed --num\\_gpus 2 trainer-program.py ... ```\n### Order of GPUs\nNow, to select which GPUs to use and their order, you'll use the `CUDA\\_VISIBLE\\_DEVICES` environment variable. It is easiest to set the environment variable in a `~/bashrc` or another startup config file. `CUDA\\_VISIBLE\\_DEVICES` is used to map which GPUs are used. For example, if you have 4 GPUs (0, 1, 2, 3) and you only want to run GPUs 0 and 2:\n```bash\nCUDA\\_VISIBLE\\_DEVICES=0,2 torchrun trainer-program.py ... ```\nOnly the 2 physical GPUs (0 and 2) are \""visible\"" to PyTorch and these are mapped to `cuda:0` and `cuda:1` respectively. You can also reverse the order of the GPUs to use 2 first. Now, the mapping is `cuda:1` for GPU 0 and `cuda:0` for GPU 2. ```bash\nCUDA\\_VISIBLE\\_DEVICES=2,0 torchrun trainer-program.py ... ```\nYou can also set the `CUDA\\_VISIBLE\\_DEVICES` environment variable to an empty value to create an environment without GPUs. ```bash\nCUDA\\_VISIBLE\\_DEVICES= python trainer-program.py ... ```\nAs with any environment variable, they can be exported instead of being added to the command line."", ""To enable both of these features:\n- Add the `fp16` argument to enable mixed precision. - Set the number of GPUs to use with the `nproc\\_per\\_node` argument. ```bash\ntorchrun \\\n--nproc\\_per\\_node 8 pytorch/summarization/run\\_summarization.py \\\n--fp16 \\\n--model\\_name\\_or\\_path t5-small \\\n--do\\_train \\\n--do\\_eval \\\n--dataset\\_name cnn\\_dailymail \\\n--dataset\\_config \""3.0.0\"" \\\n--source\\_prefix \""summarize: \"" \\\n--output\\_dir /tmp/tst-summarization \\\n--per\\_device\\_train\\_batch\\_size=4 \\\n--per\\_device\\_eval\\_batch\\_size=4 \\\n--overwrite\\_output\\_dir \\\n--predict\\_with\\_generate\n```\nTensorFlow scripts utilize a [`MirroredStrategy`](https://www.tensorflow.org/guide/distributed\\_training#mirroredstrategy) for distributed training, and you don't need to add any additional arguments to the training script. The TensorFlow script will use multiple GPUs by default if they are available. ## Run a script on a TPU\n\nTensor Processing Units (TPUs) are specifically designed to accelerate performance. PyTorch supports TPUs with the [XLA](https://www.tensorflow.org/xla) deep learning compiler (see [here](https://github.com/pytorch/xla/blob/master/README.md) for more details). To use a TPU, launch the `xla\\_spawn.py` script and use the `num\\_cores` argument to set the number of TPU cores you want to use. ```bash\npython xla\\_spawn.py --num\\_cores 8 \\\nsummarization/run\\_summarization.py \\\n--model\\_name\\_or\\_path t5-small \\\n--do\\_train \\\n--do\\_eval \\\n--dataset\\_name cnn\\_dailymail \\\n--dataset\\_config \""3.0.0\"" \\\n--source\\_prefix \""summarize: \"" \\\n--output\\_dir /tmp/tst-summarization \\\n--per\\_device\\_train\\_batch\\_size=4 \\\n--per\\_device\\_eval\\_batch\\_size=4 \\\n--overwrite\\_output\\_dir \\\n--predict\\_with\\_generate\n```\n\nTensor Processing Units (TPUs) are specifically designed to accelerate performance. TensorFlow scripts utilize a [`TPUStrategy`](https://www.tensorflow.org/guide/distributed\\_training#tpustrategy) for training on TPUs. To use a TPU, pass the name of the TPU resource to the `tpu` argument. ```bash\npython run\\_summarization.py \\\n--tpu name\\_of\\_tpu\\_resource \\\n--model\\_name\\_or\\_path t5-small \\\n--dataset\\_name cnn\\_dailymail \\\n--dataset\\_config \""3.0.0\"" \\\n--output\\_dir /tmp/tst-summarization \\\n--per\\_device\\_train\\_batch\\_size 8 \\\n--per\\_device\\_eval\\_batch\\_size 16 \\\n--num\\_train\\_epochs 3 \\\n--do\\_train \\\n--do\\_eval\n```\n## Run a script with ðŸ¤— Accelerate\nðŸ¤— [Accelerate](https://huggingface.co/docs/accelerate) is a PyTorch-only library that offers a unified method for training a model on several types of setups (CPU-only, multiple GPUs, TPUs) while maintaining complete visibility into the PyTorch training loop. Make sure you have ðŸ¤— Accelerate installed if you don't already have it:\n> Note: As Accelerate is rapidly developing, the git version of accelerate must be installed to run the scripts\n```bash\npip install git+https://github.com/huggingface/accelerate\n```\nInstead of the `run\\_summarization.py` script, you need to use the `run\\_summarization\\_no\\_trainer.py` script. ðŸ¤— Accelerate supported scripts will have a `task\\_no\\_trainer.py` file in the folder. Begin by running the following command to create and save a configuration file:\n```bash\naccelerate config\n```\nTest your setup to make sure it is configured correctly:\n```bash\naccelerate test\n```\nNow you are ready to launch the training:\n```bash\naccelerate launch run\\_summarization\\_no\\_trainer.py \\\n--model\\_name\\_or\\_path t5-small \\\n--dataset\\_name cnn\\_dailymail \\\n--dataset\\_config \""3.0.0\"" \\\n--source\\_prefix \""summarize: \"" \\\n--output\\_dir ~/tmp/tst-summarization\n```\n## Use a custom dataset\nThe summarization script supports custom datasets as long as they are a CSV or JSON Line file."", ""!---\nCopyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Multiple-choice training (e.g. SWAG)\nThis folder contains the `run\\_swag.py` script, showing an examples of \\*multiple-choice answering\\* with the\nðŸ¤— Transformers library. For straightforward use-cases you may be able to use these scripts without modification,\nalthough we have also included comments in the code to indicate areas that you may need to adapt to your own projects. ### Multi-GPU and TPU usage\nBy default, the script uses a `MirroredStrategy` and will use multiple GPUs effectively if they are available. TPUs\ncan also be used by passing the name of the TPU resource with the `--tpu` argument. ### Memory usage and data loading\nOne thing to note is that all data is loaded into memory in this script. Most multiple-choice datasets are small\nenough that this is not an issue, but if you have a very large dataset you will need to modify the script to handle\ndata streaming. This is particularly challenging for TPUs, given the stricter requirements and the sheer volume of data\nrequired to keep them fed. A full explanation of all the possible pitfalls is a bit beyond this example script and\nREADME, but for more information you can see the 'Input Datasets' section of\n[this document](https://www.tensorflow.org/guide/tpu). ### Example command\n```bash\npython run\\_swag.py \\\n--model\\_name\\_or\\_path distilbert-base-cased \\\n--output\\_dir output \\\n--do\\_eval \\\n--do\\_train\n```"", ""2. Next, take a look at the `run\\_common\\_voice.py` script to get an understanding of how it works. In short the script does the following:\n- Load the given common voice dataset\n- Create vocab for the language\n- Load the model with given hyperparameters\n- Pre-process the dataset to input into the model\n- Run training\n- Run evaluation\n3. The following examples show how you can launch fine-tuning for the common voice dataset. Here we will run the script on the \\*Turkish\\* Common Voice dataset for demonstration purposes. \\*\\*To lanuch fine-tuninig on a single GPU:\\*\\*\n```bash\npython run\\_common\\_voice.py \\\n--model\\_name\\_or\\_path=\""facebook/wav2vec2-large-xlsr-53\"" \\\n--dataset\\_config\\_name=\""tr\"" \\ # use this argument to specify the language code\n--output\\_dir=./wav2vec2-large-xlsr-turkish-demo \\\n--overwrite\\_output\\_dir \\\n--num\\_train\\_epochs=\""5\"" \\\n--per\\_device\\_train\\_batch\\_size=\""16\"" \\\n--learning\\_rate=\""3e-4\"" \\\n--warmup\\_steps=\""500\"" \\\n--evaluation\\_strategy=\""steps\"" \\\n--save\\_steps=\""400\"" \\\n--eval\\_steps=\""400\"" \\\n--logging\\_steps=\""400\"" \\\n--save\\_total\\_limit=\""3\"" \\\n--freeze\\_feature\\_extractor \\\n--feat\\_proj\\_dropout=\""0.0\"" \\\n--layerdrop=\""0.1\"" \\\n--gradient\\_checkpointing \\\n--fp16 \\\n--group\\_by\\_length \\\n--do\\_train --do\\_eval\n```\n\\*\\*To lanuch fine-tuninig on multiple GPUs:\\*\\*\n```bash\npython -m torch.distributed.launch \\\n--nproc\\_per\\_node 4 run\\_common\\_voice.py \\\n--model\\_name\\_or\\_path=\""facebook/wav2vec2-large-xlsr-53\"" \\\n--dataset\\_config\\_name=\""tr\"" \\ # use this argument to specify the language code\n--output\\_dir=./wav2vec2-large-xlsr-turkish-demo \\\n--overwrite\\_output\\_dir \\\n--num\\_train\\_epochs=\""5\"" \\\n--per\\_device\\_train\\_batch\\_size=\""16\"" \\\n--learning\\_rate=\""3e-4\"" \\\n--warmup\\_steps=\""500\"" \\\n--evaluation\\_strategy=\""steps\"" \\\n--save\\_steps=\""400\"" \\\n--eval\\_steps=\""400\"" \\\n--logging\\_steps=\""400\"" \\\n--save\\_total\\_limit=\""3\"" \\\n--freeze\\_feature\\_extractor \\\n--feat\\_proj\\_dropout=\""0.0\"" \\\n--layerdrop=\""0.1\"" \\\n--gradient\\_checkpointing \\\n--fp16 \\\n--group\\_by\\_length \\\n--do\\_train --do\\_eval\n```\nThe above command will launch the training on 4 GPUs."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nâš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Efficient Training on Multiple GPUs\nIf training a model on a single GPU is too slow or if the model's weights do not fit in a single GPU's memory, transitioning\nto a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered\nin the [Methods and tools for efficient training on a single GPU](perf\\_train\\_gpu\\_one) as they are universally applicable\nto model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your\ncase on a single GPU, consider moving to multiple GPUs. Transitioning from a single GPU to multiple GPUs requires the introduction of some form of parallelism, as the workload\nmust be distributed across the resources. Multiple techniques can be employed to achieve parallelism, such as data\nparallelism, tensor parallelism, and pipeline parallelism. It's important to note that there isn't a one-size-fits-all\nsolution, and the optimal settings depend on the specific hardware configuration you are using. This guide offers an in-depth overview of individual types of parallelism, as well as guidance on ways to combine\ntechniques and choosing an appropriate approach. For step-by-step tutorials on distributed training, please refer to\nthe [ðŸ¤— Accelerate documentation](https://huggingface.co/docs/accelerate/index). While the main concepts discussed in this guide are likely applicable across frameworks, here we focus on\nPyTorch-based implementations. Before diving deeper into the specifics of each technique, let's go over the rough decision process when training\nlarge models on a large infrastructure. ## Scalability strategy\nBegin by estimating how much vRAM is required to train your model. For models hosted on the ðŸ¤— Hub, use our\n[Model Memory Calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage), which gives you\naccurate calculations within a few percent margin. \\*\\*Parallelization strategy for a single Node / multi-GPU setup\\*\\*\nWhen training a model on a single node with multiple GPUs, your choice of parallelization strategy can significantly\nimpact performance. Here's a breakdown of your options:\n\\*\\*Case 1: Your model fits onto a single GPU\\*\\*\nIf your model can comfortably fit onto a single GPU, you have two primary options:\n1. DDP - Distributed DataParallel\n2. ZeRO - depending on the situation and configuration used, this method may or may not be faster, however, it's worth experimenting with it. \\*\\*Case 2: Your model doesn't fit onto a single GPU:\\*\\*\nIf your model is too large for a single GPU, you have several alternatives to consider:\n1."", ""## PyTorch Distributed\nPyTorch supports [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) which enables data parallelism. To start, create a Python file and import `torch.distributed` and `torch.multiprocessing` to set up the distributed process group and to spawn the processes for inference on each GPU. You should also initialize a [`DiffusionPipeline`]:\n```py\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom diffusers import DiffusionPipeline\nsd = DiffusionPipeline.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n)\n```\nYou'll want to create a function to run inference; [`init\\_process\\_group`](https://pytorch.org/docs/stable/distributed.html?highlight=init\\_process\\_group#torch.distributed.init\\_process\\_group) handles creating a distributed environment with the type of backend to use, the `rank` of the current process, and the `world\\_size` or the number of processes participating. If you're running inference in parallel over 2 GPUs, then the `world\\_size` is 2. Move the [`DiffusionPipeline`] to `rank` and use `get\\_rank` to assign a GPU to each process, where each process handles a different prompt:\n```py\ndef run\\_inference(rank, world\\_size):\ndist.init\\_process\\_group(\""nccl\"", rank=rank, world\\_size=world\\_size)\nsd.to(rank)\nif torch.distributed.get\\_rank() == 0:\nprompt = \""a dog\""\nelif torch.distributed.get\\_rank() == 1:\nprompt = \""a cat\""\nimage = sd(prompt).images[0]\nimage.save(f\""./{'\\_'.join(prompt)}.png\"")\n```\nTo run the distributed inference, call [`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn) to run the `run\\_inference` function on the number of GPUs defined in `world\\_size`:\n```py\ndef main():\nworld\\_size = 2\nmp.spawn(run\\_inference, args=(world\\_size,), nprocs=world\\_size, join=True)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\nmain()\n```\nOnce you've completed the inference script, use the `--nproc\\_per\\_node` argument to specify the number of GPUs to use and call `torchrun` to run the script:\n```bash\ntorchrun run\\_distributed.py --nproc\\_per\\_node=2\n```"", ""--\ntitle: \""Introducing ðŸ¤— Accelerate\""\nthumbnail: /blog/assets/20\\_accelerate\\_library/accelerate\\_diff.png\nauthors:\n- user: sgugger\n---\n# Introducing ðŸ¤— Accelerate\n## ðŸ¤— Accelerate\nRun your \\*\\*raw\\*\\* PyTorch training scripts on any kind of device. Most high-level libraries above PyTorch provide support for distributed training and mixed precision, but the abstraction they introduce require a user to learn a new API if they want to customize the underlying training loop. ðŸ¤— Accelerate was created for PyTorch users who like to have full control over their training loops but are reluctant to write (and maintain) the boilerplate code needed to use distributed training (for multi-GPU on one or several nodes, TPUs, ...) or mixed precision training. Plans forward include support for fairscale, deepseed, AWS SageMaker specific data-parallelism and model parallelism. It provides two things: a simple and consistent API that abstracts that boilerplate code and a launcher command to easily run those scripts on various setups. ### Easy integration! Let's first have a look at an example:\n```diff\nimport torch\nimport torch.nn.functional as F\nfrom datasets import load\\_dataset\n+ from accelerate import Accelerator\n+ accelerator = Accelerator()\n- device = 'cpu'\n+ device = accelerator.device\nmodel = torch.nn.Transformer().to(device)\noptim = torch.optim.Adam(model.parameters())\ndataset = load\\_dataset('my\\_dataset')\ndata = torch.utils.data.DataLoader(dataset, shuffle=True)\n+ model, optim, data = accelerator.prepare(model, optim, data)\nmodel.train()\nfor epoch in range(10):\nfor source, targets in data:\nsource = source.to(device)\ntargets = targets.to(device)\noptimizer.zero\\_grad()\noutput = model(source)\nloss = F.cross\\_entropy(output, targets)\n- loss.backward()\n+ accelerator.backward(loss)\noptimizer.step()\n```\nBy just adding five lines of code to any standard PyTorch training script, you can now run said script on any kind of distributed setting, as well as with or without mixed precision. ðŸ¤— Accelerate even handles the device placement for you, so you can simplify the training loop above even further:\n```diff\nimport torch\nimport torch.nn.functional as F\nfrom datasets import load\\_dataset\n+ from accelerate import Accelerator\n+ accelerator = Accelerator()\n- device = 'cpu'\n- model = torch.nn.Transformer().to(device)\n+ model = torch.nn.Transformer()\noptim = torch.optim.Adam(model.parameters())\ndataset = load\\_dataset('my\\_dataset')\ndata = torch.utils.data.DataLoader(dataset, shuffle=True)\n+ model, optim, data = accelerator.prepare(model, optim, data)\nmodel.train()\nfor epoch in range(10):\nfor source, targets in data:\n- source = source.to(device)\n- targets = targets.to(device)\noptimizer.zero\\_grad()\noutput = model(source)\nloss = F.cross\\_entropy(output, targets)\n- loss.backward()\n+ accelerator.backward(loss)\noptimizer.step()\n```\nIn contrast, here are the changes needed to have this code run with distributed training are the followings:\n```diff\n+ import os\nimport torch\nimport torch.nn.functional as F\nfrom datasets import load\\_dataset\n+ from torch.utils.data import DistributedSampler\n+ from torch.nn.parallel import DistributedDataParallel\n+ local\\_rank = int(os.environ.get(\""LOCAL\\_RANK\"", -1))\n- device = 'cpu'\n+ device = device = torch.device(\""cuda\"", local\\_rank)\nmodel = torch.nn.Transformer().to(device)\n+ model = DistributedDataParallel(model)\noptim = torch.optim.Adam(model.parameters())\ndataset = load\\_dataset('my\\_dataset')\n+ sampler = DistributedSampler(dataset)\n- data = torch.utils.data.DataLoader(dataset, shuffle=True)\n+ data = torch.utils.data.DataLoader(dataset, sampler=sampler)\nmodel.train()\nfor epoch in range(10):\n+ sampler.set\\_epoch(epoch)\nfor source, targets in data:\nsource = source.to(device)\ntargets = targets.to(device)\noptimizer.zero\\_grad()\noutput = model(source)\nloss = F.cross\\_entropy(output, targets)\nloss.backward()\noptimizer.step()\n```\nThese changes will make your training script work for multiple GPUs, but your script will then stop working on CPU or one GPU (unless you start adding if statements everywhere)."", ""The main difference is that this\nscript exposes the bare training loop, to allow you to quickly experiment and add any customization you would like. It offers less options than the script with `Trainer` (for instance you can easily change the options for the optimizer\nor the dataloaders directly in the script) but still run in a distributed setup, on TPU and supports mixed precision by\nthe mean of the [ðŸ¤— `Accelerate`](https://github.com/huggingface/accelerate) library. You can use the script normally\nafter installing it:\n```bash\npip install git+https://github.com/huggingface/accelerate\n```\nthen\n```bash\nexport TASK\\_NAME=mrpc\npython run\\_glue\\_no\\_trainer.py \\\n--model\\_name\\_or\\_path bert-base-cased \\\n--task\\_name $TASK\\_NAME \\\n--max\\_length 128 \\\n--per\\_device\\_train\\_batch\\_size 32 \\\n--learning\\_rate 2e-5 \\\n--num\\_train\\_epochs 3 \\\n--output\\_dir /tmp/$TASK\\_NAME/\n```\nYou can then use your usual launchers to run in it in a distributed environment, but the easiest way is to run\n```bash\naccelerate config\n```\nand reply to the questions asked. Then\n```bash\naccelerate test\n```\nthat will check everything is ready for training. Finally, you can launch training with\n```bash\nexport TASK\\_NAME=mrpc\naccelerate launch run\\_glue\\_no\\_trainer.py \\\n--model\\_name\\_or\\_path bert-base-cased \\\n--task\\_name $TASK\\_NAME \\\n--max\\_length 128 \\\n--per\\_device\\_train\\_batch\\_size 32 \\\n--learning\\_rate 2e-5 \\\n--num\\_train\\_epochs 3 \\\n--output\\_dir /tmp/$TASK\\_NAME/\n```\nThis command is the same and will work for:\n- a CPU-only setup\n- a setup with one GPU\n- a distributed training with several GPUs (single or multi node)\n- a training on TPUs\nNote that this library is in alpha release so your feedback is more than welcome if you encounter any problem using it."", "". ! - \\; \\: \\\"" â€œ % â€˜ â€ ï¿½ \\\n--fp16 \\\n--group\\_by\\_length \\\n--push\\_to\\_hub \\\n--do\\_train --do\\_eval\n```\nOn 8 V100 GPUs, this script should run in \\*ca.\\* 18 minutes and yield a CTC loss of \\*\\*0.39\\*\\* and word error rate\nof \\*\\*0.36\\*\\*. ### Multi GPU CTC with Dataset Streaming\nThe following command shows how to use [Dataset Streaming mode](https://huggingface.co/docs/datasets/dataset\\_streaming)\nto fine-tune [XLS-R](https://huggingface.co/transformers/main/model\\_doc/xls\\_r.html)\non [Common Voice](https://huggingface.co/datasets/common\\_voice) using 4 GPUs in half-precision. Streaming mode imposes several constraints on training:\n1. We need to construct a tokenizer beforehand and define it via `--tokenizer\\_name\\_or\\_path`. 2. `--num\\_train\\_epochs` has to be replaced by `--max\\_steps`. Similarly, all other epoch-based arguments have to be\nreplaced by step-based ones. 3. Full dataset shuffling on each epoch is not possible, since we don't have the whole dataset available at once. However, the `--shuffle\\_buffer\\_size` argument controls how many examples we can pre-download before shuffling them. ```bash\n\\*\\*torchrun \\\n--nproc\\_per\\_node 4 run\\_speech\\_recognition\\_ctc\\_streaming.py \\\n--dataset\\_name=\""common\\_voice\"" \\\n--model\\_name\\_or\\_path=\""facebook/wav2vec2-xls-r-300m\"" \\\n--tokenizer\\_name\\_or\\_path=\""anton-l/wav2vec2-tokenizer-turkish\"" \\\n--dataset\\_config\\_name=\""tr\"" \\\n--train\\_split\\_name=\""train+validation\"" \\\n--eval\\_split\\_name=\""test\"" \\\n--output\\_dir=\""wav2vec2-xls-r-common\\_voice-tr-ft\"" \\\n--overwrite\\_output\\_dir \\\n--max\\_steps=\""5000\"" \\\n--per\\_device\\_train\\_batch\\_size=\""8\"" \\\n--gradient\\_accumulation\\_steps=\""2\"" \\\n--learning\\_rate=\""5e-4\"" \\\n--warmup\\_steps=\""500\"" \\\n--evaluation\\_strategy=\""steps\"" \\\n--text\\_column\\_name=\""sentence\"" \\\n--save\\_steps=\""500\"" \\\n--eval\\_steps=\""500\"" \\\n--logging\\_steps=\""1\"" \\\n--layerdrop=\""0.0\"" \\\n--eval\\_metrics wer cer \\\n--save\\_total\\_limit=\""1\"" \\\n--mask\\_time\\_prob=\""0.3\"" \\\n--mask\\_time\\_length=\""10\"" \\\n--mask\\_feature\\_prob=\""0.1\"" \\\n--mask\\_feature\\_length=\""64\"" \\\n--freeze\\_feature\\_encoder \\\n--chars\\_to\\_ignore , ?""]","PyTorch provides the `torchrun` command line module to run a script on multiple GPUs. You can use it with the `--nproc_per_node` argument to specify the number of GPUs to use. For example:

```bash
torchrun --nproc_per_node=2 example_script.py
```",torchrun
"What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?
","[""Vision Transformers å›¾åƒåˆ†ç±»\nç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/abidlabs/vision-transformer\næ ‡ç­¾ï¼šVISION, TRANSFORMERS, HUB\n## ç®€ä»‹\nå›¾åƒåˆ†ç±»æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„é‡è¦ä»»åŠ¡ã€‚æž„å»ºæ›´å¥½çš„åˆ†ç±»å™¨ä»¥ç¡®å®šå›¾åƒä¸­å­˜åœ¨çš„å¯¹è±¡æ˜¯å½“å‰ç ”ç©¶çš„çƒ­ç‚¹é¢†åŸŸï¼Œå› ä¸ºå®ƒåœ¨ä»Žäººè„¸è¯†åˆ«åˆ°åˆ¶é€ è´¨é‡æŽ§åˆ¶ç­‰æ–¹é¢éƒ½æœ‰åº”ç”¨ã€‚\næœ€å…ˆè¿›çš„å›¾åƒåˆ†ç±»å™¨åŸºäºŽ \\_transformers\\_ æž¶æž„ï¼Œè¯¥æž¶æž„æœ€åˆåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å¾ˆå—æ¬¢è¿Žã€‚è¿™ç§æž¶æž„é€šå¸¸è¢«ç§°ä¸º vision transformers (ViT)ã€‚è¿™äº›æ¨¡åž‹éžå¸¸é€‚åˆä¸Ž Gradio çš„\\*å›¾åƒ\\*è¾“å…¥ç»„ä»¶ä¸€èµ·ä½¿ç”¨ï¼Œå› æ­¤åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æž„å»ºä¸€ä¸ªä½¿ç”¨ Gradio è¿›è¡Œå›¾åƒåˆ†ç±»çš„ Web æ¼”ç¤ºã€‚æˆ‘ä»¬åªéœ€ç”¨\\*\\*ä¸€è¡Œ Python ä»£ç \\*\\*å³å¯æž„å»ºæ•´ä¸ª Web åº”ç”¨ç¨‹åºï¼Œå…¶æ•ˆæžœå¦‚ä¸‹ï¼ˆè¯•ç”¨ä¸€ä¸‹ç¤ºä¾‹ä¹‹ä¸€ï¼ï¼‰ï¼š\nè®©æˆ‘ä»¬å¼€å§‹å§ï¼\n### å…ˆå†³æ¡ä»¶\nç¡®ä¿æ‚¨å·²ç»[å®‰è£…](/getting\\_started)äº† `gradio` Python åŒ…ã€‚\n## æ­¥éª¤ 1 - é€‰æ‹© Vision å›¾åƒåˆ†ç±»æ¨¡åž‹\né¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå›¾åƒåˆ†ç±»æ¨¡åž‹ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[Hugging Face Model Hub](https://huggingface.co/models?pipeline\\_tag=image-classification)ä¸Šçš„ä¸€ä¸ªæ¨¡åž‹ã€‚è¯¥ Hub åŒ…å«æ•°åƒä¸ªæ¨¡åž‹ï¼Œæ¶µç›–äº†å¤šç§ä¸åŒçš„æœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚\nåœ¨å·¦ä¾§è¾¹æ ä¸­å±•å¼€ Tasks ç±»åˆ«ï¼Œå¹¶é€‰æ‹©æˆ‘ä»¬æ„Ÿå…´è¶£çš„â€œImage Classificationâ€ä½œä¸ºæˆ‘ä»¬çš„ä»»åŠ¡ã€‚ç„¶åŽï¼Œæ‚¨å°†çœ‹åˆ° Hub ä¸Šä¸ºå›¾åƒåˆ†ç±»è®¾è®¡çš„æ‰€æœ‰æ¨¡åž‹ã€‚\nåœ¨æ’°å†™æ—¶ï¼Œæœ€å—æ¬¢è¿Žçš„æ¨¡åž‹æ˜¯ `google/vit-base-patch16-224`ï¼Œè¯¥æ¨¡åž‹åœ¨åˆ†è¾¨çŽ‡ä¸º 224x224 åƒç´ çš„ ImageNet å›¾åƒä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚æˆ‘ä»¬å°†åœ¨æ¼”ç¤ºä¸­ä½¿ç”¨æ­¤æ¨¡åž‹ã€‚\n## æ­¥éª¤ 2 - ä½¿ç”¨ Gradio åŠ è½½ Vision Transformer æ¨¡åž‹\nå½“ä½¿ç”¨ Hugging Face Hub ä¸Šçš„æ¨¡åž‹æ—¶ï¼Œæˆ‘ä»¬æ— éœ€ä¸ºæ¼”ç¤ºå®šä¹‰è¾“å…¥æˆ–è¾“å‡ºç»„ä»¶ã€‚åŒæ ·ï¼Œæˆ‘ä»¬ä¸éœ€è¦å…³å¿ƒé¢„å¤„ç†æˆ–åŽå¤„ç†çš„ç»†èŠ‚ã€‚æ‰€æœ‰è¿™äº›éƒ½å¯ä»¥ä»Žæ¨¡åž‹æ ‡ç­¾ä¸­è‡ªåŠ¨æŽ¨æ–­å‡ºæ¥ã€‚\né™¤äº†å¯¼å…¥è¯­å¥å¤–ï¼Œæˆ‘ä»¬åªéœ€è¦ä¸€è¡Œä»£ç å³å¯åŠ è½½å¹¶å¯åŠ¨æ¼”ç¤ºã€‚\næˆ‘ä»¬ä½¿ç”¨ `gr.Interface.load()` æ–¹æ³•ï¼Œå¹¶ä¼ å…¥åŒ…å« `huggingface/` çš„æ¨¡åž‹è·¯å¾„ï¼Œä»¥æŒ‡å®šå®ƒæ¥è‡ª Hugging Face Hubã€‚\n```python\nimport gradio as gr\ngr.Interface.load(\n\""huggingface/google/vit-base-patch16-224\"",\nexamples=[\""alligator.jpg\"", \""laptop.jpg\""]).launch()\n```\nè¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ª `examples` å‚æ•°ï¼Œå…è®¸æˆ‘ä»¬ä½¿ç”¨ä¸€äº›é¢„å®šä¹‰çš„ç¤ºä¾‹é¢„å¡«å……æˆ‘ä»¬çš„ç•Œé¢ã€‚\nè¿™å°†ç”Ÿæˆä»¥ä¸‹æŽ¥å£ï¼Œæ‚¨å¯ä»¥ç›´æŽ¥åœ¨æµè§ˆå™¨ä¸­å°è¯•ã€‚å½“æ‚¨è¾“å…¥å›¾åƒæ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨è¿›è¡Œé¢„å¤„ç†å¹¶å‘é€åˆ° Hugging Face Hub APIï¼Œé€šè¿‡æ¨¡åž‹å¤„ç†ï¼Œå¹¶ä»¥äººç±»å¯è§£é‡Šçš„é¢„æµ‹ç»“æžœè¿”å›žã€‚å°è¯•ä¸Šä¼ æ‚¨è‡ªå·±çš„å›¾åƒï¼\n---\nå®Œæˆï¼åªéœ€ä¸€è¡Œä»£ç ï¼Œæ‚¨å°±å»ºç«‹äº†ä¸€ä¸ªå›¾åƒåˆ†ç±»å™¨çš„ Web æ¼”ç¤ºã€‚å¦‚æžœæ‚¨æƒ³ä¸Žä»–äººåˆ†äº«ï¼Œè¯·åœ¨ `launch()` æŽ¥å£æ—¶è®¾ç½® `share=True`ã€‚"", ""Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. 1. \\*\\*[SwiftFormer](https://huggingface.co/docs/transformers/model\\_doc/swiftformer)\\*\\* (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan. 1. \\*\\*[Swin Transformer](https://huggingface.co/docs/transformers/model\\_doc/swin)\\*\\* (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. 1. \\*\\*[Swin Transformer V2](https://huggingface.co/docs/transformers/model\\_doc/swinv2)\\*\\* (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo. 1. \\*\\*[Swin2SR](https://huggingface.co/docs/transformers/model\\_doc/swin2sr)\\*\\* (from University of WÃ¼rzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V."", ""Note that one should\nuse [`DeiTImageProcessor`] in order to prepare images for the model. - [BEiT](beit) (BERT pre-training of Image Transformers) by Microsoft Research. BEiT models outperform supervised pre-trained\nvision transformers using a self-supervised method inspired by BERT (masked image modeling) and based on a VQ-VAE. - DINO (a method for self-supervised training of Vision Transformers) by Facebook AI. Vision Transformers trained using\nthe DINO method show very interesting properties not seen with convolutional models. They are capable of segmenting\nobjects, without having ever been trained to do so. DINO checkpoints can be found on the [hub](https://huggingface.co/models?other=dino). - [MAE](vit\\_mae) (Masked Autoencoders) by Facebook AI. By pre-training Vision Transformers to reconstruct pixel values for a high portion\n(75%) of masked patches (using an asymmetric encoder-decoder architecture), the authors show that this simple method outperforms\nsupervised pre-training after fine-tuning."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Transformer2D\nA Transformer model for image-like data from [CompVis](https://huggingface.co/CompVis) that is based on the [Vision Transformer](https://huggingface.co/papers/2010.11929) introduced by Dosovitskiy et al. The [`Transformer2DModel`] accepts discrete (classes of vector embeddings) or continuous (actual embeddings) inputs. When the input is \\*\\*continuous\\*\\*:\n1. Project the input and reshape it to `(batch\\_size, sequence\\_length, feature\\_dimension)`. 2. Apply the Transformer blocks in the standard way. 3. Reshape to image. When the input is \\*\\*discrete\\*\\*:\nIt is assumed one of the input classes is the masked latent pixel. The predicted classes of the unnoised image don't contain a prediction for the masked pixel because the unnoised image cannot be masked."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Pyramid Vision Transformer (PVT)\n## Overview\nThe PVT model was proposed in\n[Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/abs/2102.12122)\nby Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao. The PVT is a type of\nvision transformer that utilizes a pyramid structure to make it an effective backbone for dense prediction tasks."", ""For Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage. AutoTrain also enables [automatic model evaluation](https://huggingface.co/spaces/autoevaluate/model-evaluator). This application allows you to evaluate ðŸ¤— Transformers [models](https://huggingface.co/models?library=transformers&sort=downloads) across a wide variety of [datasets](https://huggingface.co/datasets) on the Hub. The results of your evaluation will be displayed on the [public leaderboards](https://huggingface.co/spaces/autoevaluate/leaderboards). You can check [this blog post](https://huggingface.co/blog/eval-on-the-hub) for more details. ## The technical philosophy\nIn this section, we wanted to share our philosophy behind adding support for Computer Vision in ðŸ¤— Transformers so that the community is aware of the design choices specific to this area. Even though Transformers started with NLP, we support multiple modalities today, for example â€“ vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:\n- Easy model download with a single line of code with `from\\_pretrained()`\n- Easy model upload with `push\\_to\\_hub()`\n- Support for loading huge checkpoints with efficient checkpoint sharding techniques\n- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))\n- Initialization from model configurations\n- Support for both PyTorch and TensorFlow (non-exhaustive)\n- and many more\nUnlike tokenizers, we have preprocessors (such as [this](https://huggingface.co/docs/transformers/model\\_doc/vit#transformers.ViTImageProcessor)) that take care of preparing data for the vision models. We have worked hard to ensure the user experience of using a vision model still feels easy and similar:\n```py\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nimport torch\nfrom datasets import load\\_dataset\ndataset = load\\_dataset(\""huggingface/cats-image\"")\nimage = dataset[\""test\""][\""image\""][0]\nimage\\_processor  = ViTImageProcessor.from\\_pretrained(\""google/vit-base-patch16-224\"")\nmodel = ViTForImageClassification.from\\_pretrained(\""google/vit-base-patch16-224\"")\ninputs = image\\_processor(image, return\\_tensors=\""pt\"")\nwith torch.no\\_grad():\nlogits = model(\\*\\*inputs).logits\n# model predicts one of the 1000 ImageNet classes\npredicted\\_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted\\_label])\n# Egyptian cat\n```\nEven for a difficult task like object detection, the user experience doesnâ€™t change very much:\n```py\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\nfrom PIL import Image\nimport requests\nurl = \""http://images.cocodataset.org/val2017/000000039769.jpg\""\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\\_processor = AutoImageProcessor.from\\_pretrained(\""microsoft/conditional-detr-resnet-50\"")\nmodel = AutoModelForObjectDetection.from\\_pretrained(\""microsoft/conditional-detr-resnet-50\"")\ninputs = image\\_processor(images=image, return\\_tensors=\""pt\"")\noutputs = model(\\*\\*inputs)\n# convert outputs (bounding boxes and class logits) to COCO API\ntarget\\_sizes = torch.tensor([image.size[::-1]])\nresults = image\\_processor.post\\_process\\_object\\_detection(\noutputs, threshold=0.5, target\\_sizes=target\\_sizes\n)[0]\nfor score, label, box in zip(results[\""scores\""], results[\""labels\""], results[\""boxes\""]):\nbox = [round(i, 2) for i in box.tolist()]\nprint(\nf\""Detected {model.config.id2label[label.item()]} with confidence \""\nf\""{round(score.item(), 3)} at location {box}\""\n)\n```\nLeads to:\n```bash\nDetected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\nDetected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\nDetected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\nDetected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\nDetected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\n```\n## Zero-shot models for vision\nThereâ€™s been a surge of models that reformulate core vision tasks like segmentation and detection in interesting ways and introduce even more flexibility."", ""The abstract from the paper is the following:\n\\*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its\napplications to computer vision remain limited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional networks while keeping their overall\nstructure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),\nVision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring\nsubstantially fewer computational resources to train.\\*\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code (written in JAX) can be\nfound [here](https://github.com/google-research/vision\\_transformer). ## Resources\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with ViT Hybrid. - [`ViTHybridForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image\\_classification.ipynb). - See also: [Image classification task guide](../tasks/image\\_classification)\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. ## ViTHybridConfig\n[[autodoc]] ViTHybridConfig\n## ViTHybridImageProcessor\n[[autodoc]] ViTHybridImageProcessor\n- preprocess\n## ViTHybridModel\n[[autodoc]] ViTHybridModel\n- forward\n## ViTHybridForImageClassification\n[[autodoc]] ViTHybridForImageClassification\n- forward"", ""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Vision Transformer (ViT)\n## Overview\nThe Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\nat Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, Neil Houlsby."", ""With so many Transformer variants available, it can be easy to miss the bigger picture. What all these models have in common is they're based on the original Transformer architecture. Some models only use the encoder or decoder, while others use both. This provides a useful taxonomy to categorize and examine the high-level differences within models in the Transformer family, and it'll help you understand Transformers you haven't encountered before. If you aren't familiar with the original Transformer model or need a refresher, check out the [How do Transformers work](https://huggingface.co/course/chapter1/4?fw=pt) chapter from the Hugging Face course.## Computer vision\n### Convolutional network\nFor a long time, convolutional networks (CNNs) were the dominant paradigm for computer vision tasks until the [Vision Transformer](https://arxiv.org/abs/2010.11929) demonstrated its scalability and efficiency. Even then, some of a CNN's best qualities, like translation invariance, are so powerful (especially for certain tasks) that some Transformers incorporate convolutions in their architecture. [ConvNeXt](model\\_doc/convnext) flipped this exchange around and incorporated design choices from Transformers to modernize a CNN. For example, ConvNeXt uses non-overlapping sliding windows to patchify an image and a larger kernel to increase its global receptive field. ConvNeXt also makes several layer design choices to be more memory-efficient and improve performance, so it competes favorably with Transformers! ### Encoder[[cv-encoder]]\nThe [Vision Transformer (ViT)](model\\_doc/vit) opened the door to computer vision tasks without convolutions. ViT uses a standard Transformer encoder, but its main breakthrough was how it treated an image. It splits an image into fixed-size patches and uses them to create an embedding, just like how a sentence is split into tokens. ViT capitalized on the Transformers' efficient architecture to demonstrate competitive results with the CNNs at the time while requiring fewer resources to train. ViT was soon followed by other vision models that could also handle dense vision tasks like segmentation as well as detection. One of these models is the [Swin](model\\_doc/swin) Transformer. It builds hierarchical feature maps (like a CNN ðŸ‘€ and unlike ViT) from smaller-sized patches and merges them with neighboring patches in deeper layers. Attention is only computed within a local window, and the window is shifted between attention layers to create connections to help the model learn better. Since the Swin Transformer can produce hierarchical feature maps, it is a good candidate for dense prediction tasks like segmentation and detection. The [SegFormer](model\\_doc/segformer) also uses a Transformer encoder to build hierarchical feature maps, but it adds a simple multilayer perceptron (MLP) decoder on top to combine all the feature maps and make a prediction. Other vision models, like BeIT and ViTMAE, drew inspiration from BERT's pretraining objective. [BeIT](model\\_doc/beit) is pretrained by \\*masked image modeling (MIM)\\*; the image patches are randomly masked, and the image is also tokenized into visual tokens. BeIT is trained to predict the visual tokens corresponding to the masked patches. [ViTMAE](model\\_doc/vitmae) has a similar pretraining objective, except it must predict the pixels instead of visual tokens."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Hybrid Vision Transformer (ViT Hybrid)\n## Overview\nThe hybrid Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\nat Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, Neil Houlsby. It's the first paper that successfully trains a Transformer encoder on ImageNet, attaining\nvery good results compared to familiar convolutional architectures. ViT hybrid is a slight variant of the [plain Vision Transformer](vit),\nby leveraging a convolutional backbone (specifically, [BiT](bit)) whose features are used as initial \""tokens\"" for the Transformer."", ""Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. 1. \\*\\*[SwiftFormer](https://huggingface.co/docs/transformers/model\\_doc/swiftformer)\\*\\* (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan. 1. \\*\\*[Swin Transformer](https://huggingface.co/docs/transformers/model\\_doc/swin)\\*\\* (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. 1. \\*\\*[Swin Transformer V2](https://huggingface.co/docs/transformers/model\\_doc/swinv2)\\*\\* (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo. 1. \\*\\*[Swin2SR](https://huggingface.co/docs/transformers/model\\_doc/swin2sr)\\*\\* (from University of WÃ¼rzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V."", ""Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. 1. \\*\\*[SwiftFormer](https://huggingface.co/docs/transformers/model\\_doc/swiftformer)\\*\\* (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan. 1. \\*\\*[Swin Transformer](https://huggingface.co/docs/transformers/model\\_doc/swin)\\*\\* (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. 1. \\*\\*[Swin Transformer V2](https://huggingface.co/docs/transformers/model\\_doc/swinv2)\\*\\* (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo. 1. \\*\\*[Swin2SR](https://huggingface.co/docs/transformers/model\\_doc/swin2sr)\\*\\* (from University of WÃ¼rzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V."", ""Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. 1. \\*\\*[SwiftFormer](https://huggingface.co/docs/transformers/model\\_doc/swiftformer)\\*\\* (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan. 1. \\*\\*[Swin Transformer](https://huggingface.co/docs/transformers/model\\_doc/swin)\\*\\* (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. 1. \\*\\*[Swin Transformer V2](https://huggingface.co/docs/transformers/model\\_doc/swinv2)\\*\\* (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo. 1. \\*\\*[Swin2SR](https://huggingface.co/docs/transformers/model\\_doc/swin2sr)\\*\\* (from University of WÃ¼rzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V."", ""Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. 1. \\*\\*[SwiftFormer](https://huggingface.co/docs/transformers/model\\_doc/swiftformer)\\*\\* (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan. 1. \\*\\*[Swin Transformer](https://huggingface.co/docs/transformers/model\\_doc/swin)\\*\\* (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. 1. \\*\\*[Swin Transformer V2](https://huggingface.co/docs/transformers/model\\_doc/swinv2)\\*\\* (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo. 1. \\*\\*[Swin2SR](https://huggingface.co/docs/transformers/model\\_doc/swin2sr)\\*\\* (from University of WÃ¼rzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V."", ""Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. 1. \\*\\*[SwiftFormer](https://huggingface.co/docs/transformers/model\\_doc/swiftformer)\\*\\* (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan. 1. \\*\\*[Swin Transformer](https://huggingface.co/docs/transformers/model\\_doc/swin)\\*\\* (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. 1. \\*\\*[Swin Transformer V2](https://huggingface.co/docs/transformers/model\\_doc/swinv2)\\*\\* (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo. 1. \\*\\*[Swin2SR](https://huggingface.co/docs/transformers/model\\_doc/swin2sr)\\*\\* (from University of WÃ¼rzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V.""]",åœ¨æ’°å†™æ—¶ï¼Œæœ€å—æ¬¢è¿Žçš„è§†è§‰å˜æ¢å™¨æ¨¡åž‹æ˜¯ `google/vit-base-patch16-224`ï¼Œè¯¥æ¨¡åž‹åœ¨åˆ†è¾¨çŽ‡ä¸º 224x224 åƒç´ çš„ ImageNet å›¾åƒä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚,google/vit-base-patch16-224
"What is the command to upload an ESPnet model to a Hugging Face repository?
","[""Using ESPnet at Hugging Face\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks. ## Exploring ESPnet in the Hub\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). All models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more. 2. Metadata tags that help for discoverability and contain information such as license, language and datasets. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png)\n\n## Using existing models\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\_model\\_zoo)). If you're interested in doing inference, different classes for different tasks have a `from\\_pretrained` method that allows loading models from the Hub."", ""n this video, we're going to understand how to manage a model repository on the HuggingFace model hub. In order to handle a repository, you should first have a Hugging Face account. A link to create a new account is available in the description. Once you are logged in, you can create a new repository by clicking on the \""New model\"" option. You should be facing a similar modal to the following. In the \""Owner\"" input, you can put either your own namespace or any of your organisations namespaces. The model name is the model identifier that will then be used to identify your model on your chosen namespace. The final choice is between public and private. Public models are accessible by anyone. This is the recommended, free option, as this makes your model easily accessible and shareable. The owners of your namespace are the only ones who can update and change your model. A more advanced option is the private option. In this case, only the owners of your namespace will have visibility over your model. Other users won't know it exists and will not be able to use it. Let's create a dummy model to play with. Once your model is created, comes the management of that model! Three tabs are available to you. You're facing the first one, which is the model card page; this is the page used to showcase your model to the world."", ""- Option1: Pushing the model to Hugging Face Hub\n```python\nmodel.push\\_to\\_hub(\nf\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\""/\"", \""\\_\""),\ntoken = \""hf\\_...\""\n)\n```\ntoken (`bool` or `str`, \\*optional\\*):\n`token` is to be used for HTTP Bearer authorization when accessing remote files. If `True`, will use the token generated\nwhen running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo\\_url`\nis not specified. Or you can get your token from https://huggingface.co/settings/token\n```\n- Or save model locally\n```python\npeft\\_model\\_id = f\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\""/\"", \""\\_\"")\nmodel.save\\_pretrained(peft\\_model\\_id)\n```\n```python\n# saving model\npeft\\_model\\_id = f\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\n\""/\"", \""\\_\""\n)\nmodel.save\\_pretrained(peft\\_model\\_id)\n```\n```python\nckpt = f\""{peft\\_model\\_id}/adapter\\_model.bin\""\n!du -h $ckpt\n```\n```python\nfrom peft import PeftModel, PeftConfig\npeft\\_model\\_id = f\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\n\""/\"", \""\\_\""\n)\nconfig = PeftConfig.from\\_pretrained(peft\\_model\\_id)\nmodel = AutoModelForCausalLM.from\\_pretrained(config.base\\_model\\_name\\_or\\_path)\nmodel = PeftModel.from\\_pretrained(model, peft\\_model\\_id)\n```\n```python\nmodel.to(device)\nmodel.eval()\ni = 4\ninputs = tokenizer(f'{text\\_column} : {dataset[\""test\""][i][\""Tweet text\""]} Label : ', return\\_tensors=\""pt\"")\nprint(dataset[\""test\""][i][\""Tweet text\""])\nprint(inputs)\nwith torch.no\\_grad():\ninputs = {k: v.to(device) for k, v in inputs.items()}\noutputs = model.generate(\ninput\\_ids=inputs[\""input\\_ids\""], attention\\_mask=inputs[\""attention\\_mask\""], max\\_new\\_tokens=10, eos\\_token\\_id=3\n)\nprint(outputs)\nprint(tokenizer.batch\\_decode(outputs.detach().cpu().numpy(), skip\\_special\\_tokens=True))\n```"", ""4. As soon as your Space is built, Hugging Face will detect that it is associated with the model. A \""Linked Models\"" button should appear in the top right corner of the Space, as shown here:\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/linked-models.png)\n\\*Note\\*: You can also add linked models manually by explicitly updating them in the [README metadata for the Space, as described here](https://huggingface.co/docs/hub/spaces-config-reference). Your Space should appear in the Demo tab next to the paper on ArXiv in a few minutes ðŸ¤—"", ""Philipp: Sure you can use the new Hugging Face Deep Learning Container. Jeff: ok. Jeff: and how can I get started? Jeff: where can I find documentation? Philipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face\n'''\nnlp(conversation)\n## Results\n| key | value |\n| --- | ----- |\n{eval\\_table}\n{test\\_table}\n\""\""\""\n# Generate model card (todo: add more data from Trainer)\nmodel\\_card = MODEL\\_CARD\\_TEMPLATE.format(\nmodel\\_name=f\""{hyperparameters['model\\_name\\_or\\_path'].split('/')[1]}-{hyperparameters['dataset\\_name']}\"",\nhyperparameters=json.dumps(hyperparameters, indent=4, sort\\_keys=True),\neval\\_table=\""\\n\"".join(f\""| {k} | {v} |\"" for k, v in eval\\_results.items()),\ntest\\_table=\""\\n\"".join(f\""| {k} | {v} |\"" for k, v in test\\_results.items()),\n)\nwith open(f\""{local\\_path}/README.md\"", \""w\"") as f:\nf.write(model\\_card)\n```\nAfter we have our unzipped model and model card located in `my\\_bart\\_model` we can use the either `huggingface\\_hub` SDK to create a repository and upload it to [huggingface.co](https://huggingface.co) â€“ or just to https://huggingface.co/new an create a new repository and upload it. ```python\nfrom getpass import getpass\nfrom huggingface\\_hub import HfApi, Repository\nhf\\_username = \""philschmid\"" # your username on huggingface.co\nhf\\_email = \""philipp@huggingface.co\"" # email used for commit\nrepository\\_name = f\""{hyperparameters['model\\_name\\_or\\_path'].split('/')[1]}-{hyperparameters['dataset\\_name']}\"" # repository name on huggingface.co\npassword = getpass(\""Enter your password:\"") # creates a prompt for entering password\n# get hf token\ntoken = HfApi().login(username=hf\\_username, password=password)\n# create repository\nrepo\\_url = HfApi().create\\_repo(token=token, name=repository\\_name, exist\\_ok=True)\n# create a Repository instance\nmodel\\_repo = Repository(use\\_auth\\_token=token,\nclone\\_from=repo\\_url,\nlocal\\_dir=local\\_path,\ngit\\_user=hf\\_username,\ngit\\_email=hf\\_email)\n# push model to the hub\nmodel\\_repo.push\\_to\\_hub()\n```\n---\n## Test inference\nAfter we uploaded our model we can access it at `https://huggingface.co/{hf\\_username}/{repository\\_name}`\n```python\nprint(f\""https://huggingface.co/{hf\\_username}/{repository\\_name}\"")\n```\nAnd use the \""Hosted Inference API\"" widget to test it. [https://huggingface.co/philschmid/bart-large-cnn-samsum](https://huggingface.co/philschmid/bart-large-cnn-samsum)\n![inference](assets/19\\_sagemaker\\_distributed\\_training\\_seq2seq/inference-test.png)"", ""FrameworkSwitchCourse {fw} />\n# Sharing pretrained models[[sharing-pretrained-models]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nIn the steps below, we'll take a look at the easiest ways to share pretrained models to the ðŸ¤— Hub. There are tools and utilities available that make it simple to share and update models directly on the Hub, which we will explore below. We encourage all users that train models to contribute by sharing them with the community â€” sharing models, even when trained on very specific datasets, will help others, saving them time and compute resources and providing access to useful trained artifacts. In turn, you can benefit from the work that others have done! There are three ways to go about creating new model repositories:\n- Using the `push\\_to\\_hub` API\n- Using the `huggingface\\_hub` Python library\n- Using the web interface\nOnce you've created a repository, you can upload files to it via git and git-lfs. We'll walk you through creating model repositories and uploading files to them in the following sections. ## Using the `push\\_to\\_hub` API[[using-the-pushtohub-api]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nThe simplest way to upload files to the Hub is by leveraging the `push\\_to\\_hub` API."", ""![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/code_snippet.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/code_snippet-dark.png)\n\nFirst, register your library by editing [model-libaries.ts](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries.ts). Then, add a code snippet by updating the [library-ui-elements](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/library-ui-elements.ts) file with instructions for your model. For example, the [Asteroid](https://huggingface.co/asteroid-team) integration includes a brief code snippet for how to load and use an Asteroid model:\n```typescript\nconst asteroid = (model: ModelData) =>\n`from asteroid.models import BaseModel\nmodel = BaseModel.from\\_pretrained(\""${model.id}\"")`;\n```\nDoing so will also add a tag to your model so users can quickly identify models from your library. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-tags.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-tags-dark.png)\n\n## Upload files to the Hub\nYou might also want to provide a method for creating model repositories and uploading files to the Hub directly from your library. The `huggingface\\_hub` library offers two ways to assist you with creating repositories and uploading files:\n- `create\\_repo` creates a repository on the Hub. - `upload\\_file` directly uploads files to a repository on the Hub. ### `create\\_repo`\nThe `create\\_repo` method creates a repository on the Hub. Use the `name` parameter to provide a name for your repository:\n```python\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(repo\\_id=\""test-model\"")\n'https://huggingface.co/lysandre/test-model'\n```\nWhen you check your Hugging Face account, you should now see a `test-model` repository under your namespace."", ""Hugging Face Hub documentation\nThe Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. The Hub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine Learning. Are you ready to join the path towards open source Machine Learning? ðŸ¤—\n\nRepositories\n\n[Introduction](./repositories)\n[Getting Started](./repositories-getting-started)\n[Repository Settings](./repositories-settings)\n[Pull requests and Discussions](./repositories-pull-requests-discussions)\n[Notifications](./notifications)\n[Collections](./collections)\n[Webhooks](./webhooks)\n[Next Steps](./repositories-next-steps)\n[Licenses](./repositories-licenses)\n\nModels\n\n[Introduction](./models)\n[The Model Hub](./models-the-hub)\n[Model Cards](./model-cards)\n[Gated Models](./models-gated)\n[Uploading Models](./models-uploading)\n[Downloading Models](./models-downloading)\n[Libraries](./models-libraries)\n[Tasks](./models-tasks)\n[Widgets](./models-widgets)\n[Inference API](./models-inference)\n[Download Stats](./models-download-stats)\n\nDatasets\n\n[Introduction](./datasets)\n[Datasets Overview](./datasets-overview)\n[Dataset Cards](./datasets-cards)\n[Gated Datasets](./datasets-gated)\n[Uploading Datasets](./datasets-adding)\n[Downloading Datasets](./datasets-downloading)\n[Libraries](./datasets-libraries)\n[Dataset Viewer](./datasets-viewer)\n[Download Stats](./datasets-download-stats)\n[Data files Configuration](./datasets-data-files-configuration)\n\nSpaces\n\n[Introduction](./spaces)\n[Spaces Overview](./spaces-overview)\n[Gradio Spaces](./spaces-sdks-gradio)\n[Streamlit Spaces](./spaces-sdks-streamlit)\n[Static HTML Spaces](./spaces-sdks-static)\n[Docker Spaces](./spaces-sdks-docker)\n[Embed your Space](./spaces-embed)\n[Run with Docker](./spaces-run-with-docker)\n[Reference](./spaces-config-reference)\n[Changelog](./spaces-changelog)\n[Advanced Topics](./spaces-advanced)\n[Sign in with HF](./spaces-oauth)\n\nOther\n\n[Organizations](./organizations)\n[Enterprise Hub](./enterprise-hub)\n[Billing](./billing)\n[Security](./security)\n[Moderation](./moderation)\n[Paper Pages](./paper-pages)\n[Search](./search)\n[Digital Object Identifier (DOI)](./doi)\n[Hub API Endpoints](./api)\n[Sign in with HF](./oauth)\n[Contributor Code of Conduct](https://huggingface.co/code-of-conduct)\n[Content Guidelines](https://huggingface.co/content-guidelines)\n\n## What's the Hugging Face Hub?"", ""The file name here should be `README.md` since it is what Hugging Face Hub expects. ```python\nmodel\\_card.save(Path(local\\_repo) / \""README.md\"")\n```\nWe can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub\\_utils`. Hugging Face Hub requires tokens for authentication, therefore you need to pass your token in either `notebook\\_login` if you're logging in from a notebook, or `huggingface-cli login` if you're logging in from the CLI. ```python\n# if the repository doesn't exist remotely on the Hugging Face Hub, it will be created when we set create\\_remote to True\nrepo\\_id = \""skops-user/my-awesome-model\""\nhub\\_utils.push(\nrepo\\_id=repo\\_id,\nsource=local\\_repo,\ntoken=token,\ncommit\\_message=\""pushing files to the repo from the example!\"",\ncreate\\_remote=True,\n)\n```\nOnce we push the model to the Hub, anyone can use it unless the repository is private. You can download the models using `download`. Apart from the model file, the repository contains the model configuration and the environment requirements. ```python\ndownload\\_repo = \""downloaded-model\""\nhub\\_utils.download(repo\\_id=repo\\_id, dst=download\\_repo)\n```\nThe inference widget is enabled to make predictions in the repository. ![Hosted Inference Widget](assets/94\\_skops/skops\\_widget.png)\nIf the requirements of your project have changed, you can use `update\\_env` to update the environment. ```python\nhub\\_utils.update\\_env(path=local\\_repo, requirements=[\""scikit-learn\""])\n```\nYou can see the example repository pushed with above code [here](https://huggingface.co/scikit-learn/skops-blog-example)."", ""To share a model with the community, you need an account on [huggingface.co](https://huggingface.co/join). You can also join an existing organization or create a new one. ## Repository features\nEach repository on the Model Hub behaves like a typical GitHub repository. Our repositories offer versioning, commit history, and the ability to visualize differences. The Model Hub's built-in versioning is based on git and [git-lfs](https://git-lfs.github.com/). In other words, you can treat one model as one repository, enabling greater access control and scalability. Version control allows \\*revisions\\*, a method for pinning a specific version of a model with a commit hash, tag or branch. As a result, you can load a specific model version with the `revision` parameter:\n```py\n>>> model = AutoModel.from\\_pretrained(\n... \""julien-c/EsperBERTo-small\"", revision=\""v2.0.1\"" # tag name, or branch name, or commit hash\n... )\n```\nFiles are also easily edited in a repository, and you can view the commit history as well as the difference:\n![vis\\_diff](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vis\\_diff.png)\n## Setup\nBefore sharing a model to the Hub, you will need your Hugging Face credentials. If you have access to a terminal, run the following command in the virtual environment where ðŸ¤— Transformers is installed. This will store your access token in your Hugging Face cache folder (`~/.cache/` by default):\n```bash\nhuggingface-cli login\n```\nIf you are using a notebook like Jupyter or Colaboratory, make sure you have the [`huggingface\\_hub`](https://huggingface.co/docs/hub/adding-a-library) library installed. This library allows you to programmatically interact with the Hub. ```bash\npip install huggingface\\_hub\n```\nThen use `notebook\\_login` to sign-in to the Hub, and follow the link [here](https://huggingface.co/settings/token) to generate a token to login with:\n```py\n>>> from huggingface\\_hub import notebook\\_login\n>>> notebook\\_login()\n```\n## Convert a model for all frameworks\nTo ensure your model can be used by someone working with a different framework, we recommend you convert and upload your model with both PyTorch and TensorFlow checkpoints."", ""Uploading models\nTo upload models to the Hub, you'll need to create an account at [Hugging Face](https://huggingface.co/join). Models on the Hub are [Git-based repositories](./repositories), which give you versioning, branches, discoverability and sharing features, integration with over a dozen libraries, and more! You have control over what you want to upload to your repository, which could include checkpoints, configs, and any other files. You can link repositories with an individual, such as [osanseviero/fashion\\_brands\\_patterns](https://huggingface.co/osanseviero/fashion\\_brands\\_patterns), or with an organization, such as [facebook/bart-large-xsum](https://huggingface.co/facebook/bart-large-xsum). Organizations can collect models related to a company, community, or library! If you choose an organization, the model will be featured on the organizationâ€™s page, and every member of the organization will have the ability to contribute to the repository. You can create a new organization [here](https://huggingface.co/organizations/new). There are several ways to upload models to the Hub, described below. We suggest adding a [Model Card](./model-cards) to your repo to document your model. ## Using the web interface\nTo create a brand new model repository, visit [huggingface.co/new](http://huggingface.co/new)."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Share a model\nThe last two tutorials showed how you can fine-tune a model with PyTorch, Keras, and ðŸ¤— Accelerate for distributed setups. The next step is to share your model with the community! At Hugging Face, we believe in openly sharing knowledge and resources to democratize artificial intelligence for everyone. We encourage you to consider sharing your model with the community to help others save time and resources. In this tutorial, you will learn two methods for sharing a trained or fine-tuned model on the [Model Hub](https://huggingface.co/models):\n- Programmatically push your files to the Hub. - Drag-and-drop your files to the Hub with the web interface."", ""For example:\n\\* `Speech2Text` for Automatic Speech Recognition. \\* `Text2Speech` for Text to Speech. \\* `SeparateSpeech` for Audio Source Separation. Here is an inference example:\n```py\nimport soundfile\nfrom espnet2.bin.tts\\_inference import Text2Speech\ntext2speech = Text2Speech.from\\_pretrained(\""model\\_name\"")\nspeech = text2speech(\""foobar\"")[\""wav\""]\nsoundfile.write(\""out.wav\"", speech.numpy(), text2speech.fs, \""PCM\\_16\"")\n```\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png)\n\n## Sharing your models\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\_model\\_zoo#register-your-model)). The `run.sh` script allows to upload a given model to a Hugging Face repository. ```bash\n./run.sh --stage 15 --skip\\_upload\\_hf false --hf\\_repo username/model\\_repo\n```\n## Additional resources\n\\* ESPnet [docs](https://espnet.github.io/espnet/index.html). \\* ESPnet model zoo [repository](https://github.com/espnet/espnet\\_model\\_zoo)."", ""The Model Hub\n## What is the Model Hub? The Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing. Download pre-trained models with the [`huggingface\\_hub` client library](https://huggingface.co/docs/huggingface\\_hub/index), with ðŸ¤— [`Transformers`](https://huggingface.co/docs/transformers/index) for fine-tuning and other usages or with any of the over [15 integrated libraries](./models-libraries). You can even leverage the [Inference API](./models-inference) to use models in production settings. You can refer to the following video for a guide on navigating the Model Hub:\nTo learn how to upload models to the Hub, you can refer to the [Repositories Getting Started Guide](./repositories-getting-started)."", ""Models\nThe Hugging Face Hub hosts many models for a [variety of machine learning tasks](https://huggingface.co/tasks). Models are stored in repositories, so they benefit from [all the features](./repositories) possessed by every repo on the Hugging Face Hub. Additionally, model repos have attributes that make exploring and using models as easy as possible. These docs will take you through everything you'll need to know to find models on the Hub, upload your models, and make the most of everything the Model Hub offers! ## Contents\n- [The Model Hub](./models-the-hub)\n- [Model Cards](./model-cards)\n- [CO2 emissions](./model-cards-co2)\n- [Gated models](./models-gated)\n- [Libraries](./models-libraries)\n- [Uploading Models](./models-uploading)\n- [Downloading Models](./models-downloading)\n- [Widgets](./models-widgets)\n- [Widget Examples](./models-widgets-examples)\n- [Inference API](./models-inference)\n- [Frequently Asked Questions](./models-faq)\n- [Advanced Topics](./models-advanced)\n- [Integrating libraries with the Hub](./models-adding-libraries)\n- [Tasks](./models-tasks)"", ""The Hugging Face Hub[[the-hugging-face-hub]]\nThe [Hugging Face Hub](https://huggingface.co/) â€“- our main website â€“- is a central platform that enables anyone to discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with more than 10,000 publicly available. We'll focus on the models in this chapter, and take a look at the datasets in Chapter 5. The models in the Hub are not limited to ðŸ¤— Transformers or even NLP. There are models from [Flair](https://github.com/flairNLP/flair) and [AllenNLP](https://github.com/allenai/allennlp) for NLP, [Asteroid](https://github.com/asteroid-team/asteroid) and [pyannote](https://github.com/pyannote/pyannote-audio) for speech, and [timm](https://github.com/rwightman/pytorch-image-models) for vision, to name a few. Each of these models is hosted as a Git repository, which allows versioning and reproducibility. Sharing a model on the Hub means opening it up to the community and making it accessible to anyone looking to easily use it, in turn eliminating their need to train a model on their own and simplifying sharing and usage. Additionally, sharing a model on the Hub automatically deploys a hosted Inference API for that model. Anyone in the community is free to test it out directly on the model's page, with custom inputs and appropriate widgets. The best part is that sharing and using any public model on the Hub is completely free! [Paid plans](https://huggingface.co/pricing) also exist if you wish to share models privately. The video below shows how to navigate the Hub. Having a huggingface.co account is required to follow along this part, as we'll be creating and managing repositories on the Hugging Face Hub: [create an account](https://huggingface.co/join)""]","To upload an ESPnet model to a Hugging Face repository, you can use the following command in the terminal:

```bash
./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo
```

Make sure to replace `username/model_repo` with your actual Hugging Face username and the desired model repository name.",./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo
"What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?
","[""Add custom Dependencies\nInference Endpointsâ€™ base image includes all required libraries to run inference on ðŸ¤— Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\\* [customize your inference pipeline](/docs/inference-endpoints/guides/custom\\_handler) and need additional Python dependencies\n\\* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)). To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within. ```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\\* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n\\* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom\\_container) for inference."", ""### 6. Deploy your Custom Handler as an Inference Endpoint\nThe last step is to deploy your Custom Handler as an Inference Endpoint. You can deploy your Custom Handler like you would a regular Inference Endpoint. Add your repository, select your cloud and region, your instance and security setting, and deploy. When creating your Endpoint, the Inference Endpoint Service will check for an available and valid `handler.py`, and will use it for serving requests no matter which â€œTaskâ€ you select. \\_Note: In your [Inference Endpoints dashboard](https://ui.endpoints.huggingface.co/), the Task for this Endpoint should now be set to Custom\\_"", ""The file name here should be `README.md` since it is what Hugging Face Hub expects. ```python\nmodel\\_card.save(Path(local\\_repo) / \""README.md\"")\n```\nWe can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub\\_utils`. Hugging Face Hub requires tokens for authentication, therefore you need to pass your token in either `notebook\\_login` if you're logging in from a notebook, or `huggingface-cli login` if you're logging in from the CLI. ```python\n# if the repository doesn't exist remotely on the Hugging Face Hub, it will be created when we set create\\_remote to True\nrepo\\_id = \""skops-user/my-awesome-model\""\nhub\\_utils.push(\nrepo\\_id=repo\\_id,\nsource=local\\_repo,\ntoken=token,\ncommit\\_message=\""pushing files to the repo from the example!\"",\ncreate\\_remote=True,\n)\n```\nOnce we push the model to the Hub, anyone can use it unless the repository is private. You can download the models using `download`. Apart from the model file, the repository contains the model configuration and the environment requirements. ```python\ndownload\\_repo = \""downloaded-model\""\nhub\\_utils.download(repo\\_id=repo\\_id, dst=download\\_repo)\n```\nThe inference widget is enabled to make predictions in the repository. ![Hosted Inference Widget](assets/94\\_skops/skops\\_widget.png)\nIf the requirements of your project have changed, you can use `update\\_env` to update the environment. ```python\nhub\\_utils.update\\_env(path=local\\_repo, requirements=[\""scikit-learn\""])\n```\nYou can see the example repository pushed with above code [here](https://huggingface.co/scikit-learn/skops-blog-example)."", ""In both the cases the `model.generate` method produces the audio and follows the same principles as text generation. You can read more about it in our [how to generate](https://huggingface.co/blog/how-to-generate) blog post. Alright! With the basic usage outlined above, let's deploy MusicGen for fun and profit! First, we'll define a custom handler in `handler.py`. We can use the [Inference Endpoints template](https://huggingface.co/docs/inference-endpoints/guides/custom\\_handler#3-customize-endpointhandler) and override the `\\_\\_init\\_\\_` and `\\_\\_call\\_\\_` methods with our custom inference code. `\\_\\_init\\_\\_` will initialize the model and the processor, and `\\_\\_call\\_\\_` will take the data and return the generated music. You can find the modified `EndpointHandler` class below. ðŸ‘‡\n```python\nfrom typing import Dict, List, Any\nfrom transformers import AutoProcessor, MusicgenForConditionalGeneration\nimport torch\nclass EndpointHandler:\ndef \\_\\_init\\_\\_(self, path=\""\""):\n# load model and processor from path\nself.processor = AutoProcessor.from\\_pretrained(path)\nself.model = MusicgenForConditionalGeneration.from\\_pretrained(path, torch\\_dtype=torch.float16).to(\""cuda\"")\ndef \\_\\_call\\_\\_(self, data: Dict[str, Any]) -> Dict[str, str]:\n\""\""\""\nArgs:\ndata (:dict:):\nThe payload with the text prompt and generation parameters. \""\""\""\n# process input\ninputs = data.pop(\""inputs\"", data)\nparameters = data.pop(\""parameters\"", None)\n# preprocess\ninputs = self.processor(\ntext=[inputs],\npadding=True,\nreturn\\_tensors=\""pt\"",).to(\""cuda\"")\n# pass inputs with all kwargs in data\nif parameters is not None:\nwith torch.autocast(\""cuda\""):\noutputs = self.model.generate(\\*\\*inputs, \\*\\*parameters)\nelse:\nwith torch.autocast(\""cuda\""):\noutputs = self.model.generate(\\*\\*inputs,)\n# postprocess the prediction\nprediction = outputs[0].cpu().numpy().tolist()\nreturn [{\""generated\\_audio\"": prediction}]\n```\nTo keep things simple, in this example we are only generating audio from text, and not conditioning it with a melody. Next, we will create a `requirements.txt` file containing all the dependencies we need to run our inference code:\n```\ntransformers==4.31.0\naccelerate>=0.20.3\n```\nUploading these two files to our repository will suffice to serve the model. ![inference-files](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ie\\_musicgen/files.png)\nWe can now create the Inference Endpoint. Head to the [Inference Endpoints](https://huggingface.co/inference-endpoints) page and click `Deploy your first model`. In the \""Model repository\"" field, enter the identifier of your duplicated repository. Then select the hardware you want and create the endpoint. Any instance with a minimum of 16 GB RAM should work for `musicgen-large`."", ""This can be done by using the `huggingface-cli`. \\_Note: Make sure to configure git config as well.\\_\n```\n# setup cli with token\nhuggingface-cli login\ngit config --global credential.helper store\n```\n### 2. Create EndpointHandler (CP)\nAfter we have set up our environment, we can start creating your custom handler. The custom handler is a Python class (`EndpointHandler`) inside a `handler.py` file in our repository. The `EndpointHandler` needs to implement an `\\_\\_init\\_\\_` and a `\\_\\_call\\_\\_` method. - The `\\_\\_init\\_\\_` method will be called when starting the Endpoint and will receive 1 argument, a string with the path to your model weights. This allows you to load your model correctly. - The `\\_\\_call\\_\\_` method will be called on every request and receive a dictionary with your request body as a python dictionary. It will always contain the `inputs` key. The first step is to create our `handler.py` in the local clone of our repository. ```\n!cd distilbert-base-uncased-emotion && touch handler.py\n```\nIn there, you define your `EndpointHandler` class with the `\\_\\_init\\_\\_` and `\\_\\_call\\_\\_ `method. ```python\nfrom typing import Dict, List, Any\nclass EndpointHandler():\ndef \\_\\_init\\_\\_(self, path=\""\""):\n# Preload all the elements you are going to need at inference. # pseudo:\n# self.model= load\\_model(path)\ndef \\_\\_call\\_\\_(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:\n\""\""\""\ndata args:\ninputs (:obj: `str` | `PIL.Image` | `np.array`)\nkwargs\nReturn:\nA :obj:`list` | `dict`: will be serialized and returned\n\""\""\""\n# pseudo\n# self.model(input)\n```\n### 3."", ""Please wait for the Inference Endpoint to be deployed using `endpoint.wait()` and try again. ```\nFor more details about how to use the [`InferenceClient`], check out the [Inference guide](../guides/inference). ## Manage lifecycle\nNow that we saw how to create an Inference Endpoint and run inference on it, let's see how to manage its lifecycle. In this section, we will see methods like [`~InferenceEndpoint.pause`], [`~InferenceEndpoint.resume`], [`~InferenceEndpoint.scale\\_to\\_zero`], [`~InferenceEndpoint.update`] and [`~InferenceEndpoint.delete`]. All of those methods are aliases added to [`InferenceEndpoint`] for convenience. If you prefer, you can also use the generic methods defined in `HfApi`: [`pause\\_inference\\_endpoint`], [`resume\\_inference\\_endpoint`], [`scale\\_to\\_zero\\_inference\\_endpoint`], [`update\\_inference\\_endpoint`], and [`delete\\_inference\\_endpoint`]. ### Pause or scale to zero\nTo reduce costs when your Inference Endpoint is not in use, you can choose to either pause it using [`~InferenceEndpoint.pause`] or scale it to zero using [`~InferenceEndpoint.scale\\_to\\_zero`]. An Inference Endpoint that is \\*paused\\* or \\*scaled to zero\\* doesn't cost anything. The difference between those two is that a \\*paused\\* endpoint needs to be explicitly \\*resumed\\* using [`~InferenceEndpoint.resume`]. On the contrary, a \\*scaled to zero\\* endpoint will automatically start if an inference call is made to it, with an additional cold start delay. An Inference Endpoint can also be configured to scale to zero automatically after a certain period of inactivity. ```py\n# Pause and resume endpoint\n>>> endpoint.pause()\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='paused', url=None)\n>>> endpoint.resume()\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='pending', url=None)\n>>> endpoint.wait().client.text\\_generation(...)\n... # Scale to zero\n>>> endpoint.scale\\_to\\_zero()\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2', status='scaledToZero', url='https://jpj7k2q4j805b727.us-east-1.aws.endpoints.huggingface.cloud')\n# Endpoint is not 'running' but still has a URL and will restart on first call. ```\n### Update model or hardware requirements\nIn some cases, you might also want to update your Inference Endpoint without creating a new one. You can either update the hosted model or the hardware requirements to run the model. You can do this using [`~InferenceEndpoint.update`]:\n```py\n# Change target model\n>>> endpoint.update(repository=\""gpt2-large\"")\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2-large', status='pending', url=None)\n# Update number of replicas\n>>> endpoint.update(min\\_replica=2, max\\_replica=6)\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2-large', status='pending', url=None)\n# Update to larger instance\n>>> endpoint.update(accelerator=\""cpu\"", instance\\_size=\""large\"", instance\\_type=\""c6i\"")\nInferenceEndpoint(name='my-endpoint-name', namespace='Wauplin', repository='gpt2-large', status='pending', url=None)\n```\n### Delete the endpoint\nFinally if you won't use the Inference Endpoint anymore, you can simply call [`~InferenceEndpoint.delete()`]."", ""You can access important attributes such as `name`, `repository`, `status`, `task`, `created\\_at`, `updated\\_at`, etc. If you need it, you can also access the raw response from the server with `endpoint.raw`. Once your Inference Endpoint is created, you can find it on your [personal dashboard](https://ui.endpoints.huggingface.co/). ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/huggingface\\_hub/inference\\_endpoints\\_created.png)\n#### Using a custom image\nBy default the Inference Endpoint is built from a docker image provided by Hugging Face. However, it is possible to specify any docker image using the `custom\\_image` parameter. A common use case is to run LLMs using the [text-generation-inference](https://github.com/huggingface/text-generation-inference) framework. This can be done like this:\n```python\n# Start an Inference Endpoint running Zephyr-7b-beta on TGI\n>>> from huggingface\\_hub import create\\_inference\\_endpoint\n>>> endpoint = create\\_inference\\_endpoint(\n..."", ""## User defined code and modules\nThe Hugging Face Inference Toolkit allows the user to override the default methods of the `HuggingFaceHandlerService`. You will need to create a folder named `code/` with an `inference.py` file in it. See [here](#create-a-model-artifact-for-deployment) for more details on how to archive your model artifacts. For example:\n```bash\nmodel.tar.gz/\n|- pytorch\\_model.bin\n|- .... |- code/\n|- inference.py\n|- requirements.txt\n```\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:\n\\* `model\\_fn(model\\_dir)` overrides the default method for loading a model. The return value `model` will be used in `predict` for predictions. `predict` receives argument the `model\\_dir`, the path to your unzipped `model.tar.gz`. \\* `transform\\_fn(model, data, content\\_type, accept\\_type)` overrides the default transform function with your custom implementation. You will need to implement your own `preprocess`, `predict` and `postprocess` steps in the `transform\\_fn`. This method can't be combined with `input\\_fn`, `predict\\_fn` or `output\\_fn` mentioned below. \\* `input\\_fn(input\\_data, content\\_type)` overrides the default method for preprocessing. The return value `data` will be used in `predict` for predictions. The inputs are:\n- `input\\_data` is the raw body of your request. - `content\\_type` is the content type from the request header. \\* `predict\\_fn(processed\\_data, model)` overrides the default method for predictions. The return value `predictions` will be used in `postprocess`. The input is `processed\\_data`, the result from `preprocess`. \\* `output\\_fn(prediction, accept)` overrides the default method for postprocessing. The return value `result` will be the response of your request (e.g.`JSON`). The inputs are:\n- `predictions` is the result from `predict`. - `accept` is the return accept type from the HTTP Request, e.g. `application/json`. Here is an example of a custom inference module with `model\\_fn`, `input\\_fn`, `predict\\_fn`, and `output\\_fn`:\n```python\nfrom sagemaker\\_huggingface\\_inference\\_toolkit import decoder\\_encoder\ndef model\\_fn(model\\_dir):\n# implement custom code to load the model\nloaded\\_model = ... return loaded\\_model\ndef input\\_fn(input\\_data, content\\_type):\n# decode the input data (e.g. JSON string -> dict)\ndata = decoder\\_encoder.decode(input\\_data, content\\_type)\nreturn data\ndef predict\\_fn(data, model):\n# call your custom model with the data\noutputs = model(data , ... )\nreturn predictions\ndef output\\_fn(prediction, accept):\n# convert the model output to the desired output format (e.g. dict -> JSON string)\nresponse = decoder\\_encoder.encode(prediction, accept)\nreturn response\n```\nCustomize your inference module with only `model\\_fn` and `transform\\_fn`:\n```python\nfrom sagemaker\\_huggingface\\_inference\\_toolkit import decoder\\_encoder\ndef model\\_fn(model\\_dir):\n# implement custom code to load the model\nloaded\\_model = ..."", ""This will ensure the inference API is registered for tasks supported by your model. This file is automatically generated as part of a [GitHub Action](https://github.com/huggingface/api-inference-community/actions/workflows/python-api-export-tasks.yaml) in the [\napi-inference-community repository](https://github.com/huggingface/api-inference-community) repository. You can see an example of this [here](https://github.com/huggingface/api-inference-community/actions/runs/5126874210/jobs/9221890853#step:5:8). With these simple but powerful methods, you brought the full functionality of the Hub into your library. Users can download files stored on the Hub from your library with `hf\\_hub\\_download`, create repositories with `create\\_repo`, and upload files with `upload\\_file`. You also set up Inference API with your library, allowing users to interact with your models on the Hub from inside a browser. ## Document your library\nFinally, you can add your library to the Hub's documentation. Check for example the [Setfit PR](https://github.com/huggingface/hub-docs/pull/1150) that added [SetFit](./setfit) to the documentation."", ""Inference Endpoints Version\nHugging Face Inference Endpoints comes with a default serving container which is used for all [supported Transformers and Sentence-Transformers tasks](/docs/inference-endpoints/supported\\_tasks) and for [custom inference handler](/docs/inference-endpoints/guides/custom\\_handler) and implement batching. Below you will find information about the installed packages and versions used. You can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository. Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom\\_dependencies). ## Installed packages & version\nThe installed packages are split into `general`, `CPU` & `GPU` packages. The `general` packages are installed in all containers, the `CPU` and `GPU` packages are only installed in the corresponding containers. The Hugging Face Inference Runtime has separate versions for `PyTorch` and `TensorFlow` for `CPU` and `GPU`, which are used based on the selected `framework` when creating an Inference Endpoint. The `TensorFlow` and `PyTorch` flavors are grouped together in the list below. ### General\n- `Python`: `3.9.13`\n### CPU\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\n- `diffusers`: `0.14.0`\n- `accelerate`: `0.17.1`\n- `sentence\\_transformers`: `latest`\n- `pandas`: `latest`\n- `pytorch`: `1.13.1`\n- `torchvision`: `0.14.1`\n- `tensorflow`: `2.9.1`\n### GPU\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\n- `diffusers`: `0.14.0`\n- `accelerate`: `0.17.1`\n- `sentence\\_transformers`: `latest`\n- `pandas`: `latest`\n- `pytorch`: `1.13.1=py3.9\\_cuda11.8\\*`\n- `torchvision`: `0.14.1`\n- `tensorflow`: `2.9.1=\\*cuda112\\*py39\\*`"", ""Use a custom Container Image\nInference Endpoints not only allows you to [customize your inference handler](/docs/inference-endpoints/guides/custom\\_handler), but it also allows you to provide a custom container image. Those can be public images like `tensorflow/serving:2.7.3` or private Images hosted on [Docker Hub](https://hub.docker.com/), [AWS ECR](https://aws.amazon.com/ecr/?nc1=h\\_ls), [Azure ACR](https://azure.microsoft.com/de-de/services/container-registry/), or [Google GCR](https://cloud.google.com/container-registry?hl=de). ![custom container config](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/custom_container.png)\nThe [creation flow](/docs/inference-endpoints/guides/create\\_endpoint) of your Image artifacts from a custom image is the same as the base image. This means Inference Endpoints will create a unique image artifact derived from your provided image, including all Model Artifacts. The Model Artifacts (weights) are stored under `/repository`. For example, if you use` tensorflow/serving` as your custom image, then you have to set `model\\_base\\_path=\""/repository\"":\n```\ntensorflow\\_model\\_server \\\n--rest\\_api\\_port=5000 \\\n--model\\_name=my\\_model \\\n--model\\_base\\_path=\""/repository\""\n```"", ""Inference Endpoints\nInference Endpoints provides a secure production solution to easily deploy any `transformers`, `sentence-transformers`, and `diffusers` models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the [Hub](https://huggingface.co/models). In this guide, we will learn how to programmatically manage Inference Endpoints with `huggingface\\_hub`. For more information about the Inference Endpoints product itself, check out its [official documentation](https://huggingface.co/docs/inference-endpoints/index). This guide assumes `huggingface\\_hub` is correctly installed and that your machine is logged in. Check out the [Quick Start guide](https://huggingface.co/docs/huggingface\\_hub/quick-start#quickstart) if that's not the case yet. The minimal version supporting Inference Endpoints API is `v0.19.0`. ## Create an Inference Endpoint\nThe first step is to create an Inference Endpoint using [`create\\_inference\\_endpoint`]:\n```py\n>>> from huggingface\\_hub import create\\_inference\\_endpoint\n>>> endpoint = create\\_inference\\_endpoint(\n..."", ""Create custom Inference Handler\nHugging Face Endpoints supports all of the Transformers and Sentence-Transformers tasks and can support custom tasks, including custom pre- & post-processing. The customization can be done through a [handler.py](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) file in your model repository on the Hugging Face Hub. The [handler.py](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) needs to implement the [EndpointHandler](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) class with a `\\_\\_init\\_\\_` and a `\\_\\_call\\_\\_` method. If you want to use custom dependencies, e.g. [optimum](https://raw.githubusercontent.com/huggingface/optimum), the dependencies must be listed in a `requirements.txt` as described above in â€œadd custom dependencies.â€\n## Custom Handler Examples\nThere are already several public examples on the [Hugging Face Hub](https://huggingface.co/models?other=endpoints-template) where you can take insipiration or directly use them. The repositories are tagged with `endpoints-template` and can be found under this [link](https://huggingface.co/models?other=endpoints-template). Included examples are for:\n\\* [Optimum and ONNX Runtime](https://huggingface.co/philschmid/distilbert-onnx-banking77)\n\\* [Image Embeddings with BLIP](https://huggingface.co/florentgbelidji/blip\\_image\\_embeddings)\n\\* [TrOCR for OCR Detection](https://huggingface.co/philschmid/trocr-base-printed)\n\\* [Optimized Sentence Transformers with Optimum](https://huggingface.co/philschmid/all-MiniLM-L6-v2-optimum-embeddings)\n\\* [Pyannote Speaker diarization](https://huggingface.co/philschmid/pyannote-speaker-diarization-endpoint)\n\\* [LayoutLM](https://huggingface.co/philschmid/layoutlm-funsd)\n\\* [Flair NER](https://huggingface.co/philschmid/flair-ner-english-ontonotes-large)\n\\* [GPT-J 6B Single GPU](https://huggingface.co/philschmid/gpt-j-6B-fp16-sharded)\n\\* [Donut Document understanding](https://huggingface.co/philschmid/donut-base-finetuned-cord-v2)\n\\* [SetFit classifier](https://huggingface.co/philschmid/setfit-ag-news-endpoint)\n## Tutorial\nBefore creating a Custom Handler, you need a Hugging Face Model repository with your model weights and an Access Token with \\_WRITE\\_ access to the repository."", ""To find, create and manage Access Tokens, click [here](https://huggingface.co/settings/tokens). If you want to write a Custom Handler for an existing model from the community, you can use the [repo\\_duplicator](https://huggingface.co/spaces/osanseviero/repo\\_duplicator) to create a repository fork. The code can also be found in this [Notebook](https://colab.research.google.com/drive/1hANJeRa1PK1gZaUorobnQGu4bFj4\\_4Rf?usp=sharing). You can also search for already existing Custom Handlers here: [https://huggingface.co/models?other=endpoints-template](https://huggingface.co/models?other=endpoints-template)\n### 1. Set up Development Environment\nThe easiest way to develop our custom handler is to set up a local development environment, to implement, test, and iterate there, and then deploy it as an Inference Endpoint. The first step is to install all required development dependencies. \\_needed to create the custom handler, not needed for inference\\_\n```\n# install git-lfs to interact with the repository\nsudo apt-get update\nsudo apt-get install git-lfs\n# install transformers (not needed since it is installed by default in the container)\npip install transformers[sklearn,sentencepiece,audio,vision]\n```\nAfter we have installed our libraries we will clone our repository to our development environment. We will use [philschmid/distilbert-base-uncased-emotion](https://huggingface.co/philschmid/distilbert-base-uncased-emotion) during the tutorial. ```\ngit lfs install\ngit clone https://huggingface.co/philschmid/distilbert-base-uncased-emotion\n```\nTo be able to push our CP later you need to login into our HF account."", ""ðŸ¤— Inference Endpoints\nðŸ¤— Inference Endpoints offers a secure production solution to easily deploy any ðŸ¤— Transformers, Sentence-Transformers and Diffusion models from the Hub on dedicated and autoscaling infrastructure managed by Hugging Face. A Hugging Face Endpoint is built from a [Hugging Face Model Repository](https://huggingface.co/models). When an Endpoint is created, the service creates image artifacts that are either built from the model you select or a custom-provided container image. The image artifacts are completely decoupled from the Hugging Face Hub source repositories to ensure the highest security and reliability levels. ðŸ¤— Inference Endpoints support all of the [ðŸ¤— Transformers, Sentence-Transformers and Diffusion tasks](/docs/inference-endpoints/supported\\_tasks) as well as [custom tasks](/docs/inference-endpoints/guides/custom\\_handler) not supported by ðŸ¤— Transformers yet like speaker diarization and diffusion. In addition, ðŸ¤— Inference Endpoints gives you the option to use a custom container image managed on an external service, for instance, [Docker Hub](https://hub.docker.com/), [AWS ECR](https://aws.amazon.com/ecr/?nc1=h\\_ls), [Azure ACR](https://azure.microsoft.com/de-de/services/container-registry/), or [Google GCR](https://cloud.google.com/container-registry?hl=de). ![creation-flow](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/creation\\_flow.png)\n## Documentation and Examples\n\\* [Security & Compliance](/docs/inference-endpoints/security)\n\\* [Supported Transformers Task](/docs/inference-endpoints/supported\\_tasks)\n\\* [API Reference](/docs/inference-endpoints/api\\_reference)\n\\* [Autoscaling](/docs/inference-endpoints/autoscaling)\n\\* [FAQ](/docs/inference-endpoints/faq)\n\\* [Help & Support](/docs/inference-endpoints/support)\n### Guides\n\\* [Access the solution (UI)](/docs/inference-endpoints/guides/access)\n\\* [Create your first Endpoint](/docs/inference-endpoints/guides/create\\_endpoint)\n\\* [Send Requests to Endpoints](/docs/inference-endpoints/guides/test\\_endpoint)\n\\* [Update your Endpoint](/docs/inference-endpoints/guides/update\\_endpoint)\n\\* [Advanced Setup (Instance Types, Auto Scaling, Versioning)](/docs/inference-endpoints/guides/advanced)\n\\* [Create a Private Endpoint with AWS PrivateLink](/docs/inference-endpoints/guides/private\\_link)\n\\* [Add custom Dependencies](/docs/inference-endpoints/guides/custom\\_dependencies)\n\\* [Create custom Inference Handler](/docs/inference-endpoints/guides/custom\\_handler)\n\\* [Use a custom Container Image](/docs/inference-endpoints/guides/custom\\_container)\n\\* [Access and read Logs](/docs/inference-endpoints/guides/logs)\n\\* [Access and view Metrics](/docs/inference-endpoints/guides/metrics)\n\\* [Change Organization or Account](/docs/inference-endpoints/guides/change\\_organization)\n### Others\n\\* [Inference Endpoints Versions](/docs/inference-endpoints/others/runtime)\n\\* [Serialization & Deserialization for Requests](/docs/inference-endpoints/others/serialization)"", ""Inference Endpoints\nInference Endpoints provides a secure production solution to easily deploy models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the [Hub](https://huggingface.co/models). This page is a reference for `huggingface\\_hub`'s integration with Inference Endpoints. For more information about the Inference Endpoints product, check out its [official documentation](https://huggingface.co/docs/inference-endpoints/index). Check out the [related guide](../guides/inference\\_endpoints) to learn how to use `huggingface\\_hub` to manage your Inference Endpoints programmatically. Inference Endpoints can be fully managed via API. The endpoints are documented with [Swagger](https://api.endpoints.huggingface.cloud/). The [`InferenceEndpoint`] class is a simple wrapper built on top on this API. ## Methods\nA subset of the Inference Endpoint features are implemented in [`HfApi`]:\n- [`get\\_inference\\_endpoint`] and [`list\\_inference\\_endpoints`] to get information about your Inference Endpoints\n- [`create\\_inference\\_endpoint`], [`update\\_inference\\_endpoint`] and [`delete\\_inference\\_endpoint`] to deploy and manage Inference Endpoints\n- [`pause\\_inference\\_endpoint`] and [`resume\\_inference\\_endpoint`] to pause and resume an Inference Endpoint\n- [`scale\\_to\\_zero\\_inference\\_endpoint`] to manually scale an Endpoint to 0 replicas\n## InferenceEndpoint\nThe main dataclass is [`InferenceEndpoint`].""]","To install custom Python dependencies for Inference Endpoints, you should add a `requirements.txt` file to your model repository on the Hugging Face Hub. This file should list the Python dependencies you want to install.",requirements.txt
"How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Textual inversion\n[[open-in-colab]]\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer). This guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text\\_inversion) training guide. Import the necessary libraries:\n```py\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.utils import make\\_image\\_grid\n```\n## Stable Diffusion 1 and 2\nPick a Stable Diffusion checkpoint and a pre-learned concept from the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer):\n```py\npretrained\\_model\\_name\\_or\\_path = \""runwayml/stable-diffusion-v1-5\""\nrepo\\_id\\_embeds = \""sd-concepts-library/cat-toy\""\n```\nNow you can load a pipeline, and pass the pre-learned concept to it:\n```py\npipeline = StableDiffusionPipeline.from\\_pretrained(\npretrained\\_model\\_name\\_or\\_path, torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\npipeline.load\\_textual\\_inversion(repo\\_id\\_embeds)\n```\nCreate a prompt with the pre-learned concept by using the special placeholder token ``, and choose the number of samples and rows of images you'd like to generate:\n```py\nprompt = \""a grafitti in a favela wall with a  on it\""\nnum\\_samples\\_per\\_row = 2\nnum\\_rows = 2\n```\nThen run the pipeline (feel free to adjust the parameters like `num\\_inference\\_steps` and `guidance\\_scale` to see how they affect image quality), save the generated images and visualize them with the helper function you created at the beginning:\n```py\nall\\_images = []\nfor \\_ in range(num\\_rows):\nimages = pipeline(prompt, num\\_images\\_per\\_prompt=num\\_samples\\_per\\_row, num\\_inference\\_steps=50, guidance\\_scale=7.5).images\nall\\_images.extend(images)\ngrid = make\\_image\\_grid(all\\_images, num\\_rows, num\\_samples\\_per\\_row)\ngrid\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/textual_inversion_inference.png)\n\n## Stable Diffusion XL\nStable Diffusion XL (SDXL) can also use textual inversion vectors for inference. In contrast to Stable Diffusion 1 and 2, SDXL has two text encoders so you'll need two textual inversion embeddings - one for each text encoder model. Let's download the SDXL textual inversion embeddings and have a closer look at it's structure:\n```py\nfrom huggingface\\_hub import hf\\_hub\\_download\nfrom safetensors.torch import load\\_file\nfile = hf\\_hub\\_download(\""dn118/unaestheticXL\"", filename=\""unaestheticXLv31.safetensors\"")\nstate\\_dict = load\\_file(file)\nstate\\_dict\n```\n```\n{'clip\\_g': tensor([[ 0.0077, -0.0112, 0.0065, ..., 0.0195, 0.0159, 0.0275],\n...,\n[-0.0170, 0.0213, 0.0143, ..., -0.0302, -0.0240, -0.0362]],\n'clip\\_l': tensor([[ 0.0023, 0.0192, 0.0213, ..., -0.0385, 0.0048, -0.0011],\n...,\n[ 0.0475, -0.0508, -0.0145, ..., 0.0070, -0.0089, -0.0163]],\n```\nThere are two tensors, `\""clip\\_g\""` and `\""clip\\_l\""`. `\""clip\\_g\""` corresponds to the bigger text encoder in SDXL and refers to\n`pipe.text\\_encoder\\_2` and `\""clip\\_l\""` refers to `pipe.text\\_encoder`. Now you can load each tensor separately by passing them along with the correct text encoder and tokenizer\nto [`~loaders.TextualInversionLoaderMixin.load\\_textual\\_inversion`]:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipe = AutoPipelineForText2Image.from\\_pretrained(\""stabilityai/stable-diffusion-xl-base-1.0\"", variant=\""fp16\"", torch\\_dtype=torch.float16)\npipe.to(\""cuda\"")\npipe.load\\_textual\\_inversion(state\\_dict[\""clip\\_g\""], token=\""unaestheticXLv31\"", text\\_encoder=pipe.text\\_encoder\\_2, tokenizer=pipe.tokenizer\\_2)\npipe.load\\_textual\\_inversion(state\\_dict[\""clip\\_l\""], token=\""unaestheticXLv31\"", text\\_encoder=pipe.text\\_encoder, tokenizer=pipe.tokenizer)\n# the embedding should be used as a negative embedding, so we pass it as a negative prompt\ngenerator = torch.Generator().manual\\_seed(33)\nimage = pipe(\""a woman standing in front of a mountain\"", negative\\_prompt=\""unaestheticXLv31\"", generator=generator).images[0]\nimage\n```"", ""Distillation for quantization on Textual Inversion models to personalize text2image\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images.\\_By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images\\_\nThe `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. We have enabled distillation for quantization in `textual\\_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method. ## Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n```bash\npip install -r requirements.txt\n```\n## Prepare Datasets\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n[![](https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg)](https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg)\n## Get a FP32 Textual Inversion model\nUse the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model. ```bash\nexport MODEL\\_NAME=\""CompVis/stable-diffusion-v1-4\""\nexport DATA\\_DIR=\""./dicoo\""\naccelerate launch textual\\_inversion.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--train\\_data\\_dir=$DATA\\_DIR \\\n--learnable\\_property=\""object\"" \\\n--placeholder\\_token=\""\"" --initializer\\_token=\""toy\"" \\\n--resolution=512 \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=4 \\\n--max\\_train\\_steps=3000 \\\n--learning\\_rate=5.0e-04 --scale\\_lr \\\n--lr\\_scheduler=\""constant\"" \\\n--lr\\_warmup\\_steps=0 \\\n--output\\_dir=\""dicoo\\_model\""\n```\n## Do distillation for quantization\nDistillation for quantization is a method that combines [intermediate layer knowledge distillation](https://github.com/intel/neural-compressor/blob/master/docs/source/distillation.md#intermediate-layer-knowledge-distillation) and [quantization aware training](https://github.com/intel/neural-compressor/blob/master/docs/source/quantization.md#quantization-aware-training) in the same training process to improve the performance of the quantized model."", ""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Training with Intel Extension for PyTorch\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \""textual\\_inversion.py\"". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual\\_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script. The example supports both single node and multi-node distributed training:\n### Single node training\n```bash\nexport MODEL\\_NAME=\""CompVis/stable-diffusion-v1-4\""\nexport DATA\\_DIR=\""path-to-dir-containing-dicoo-images\""\npython textual\\_inversion.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--train\\_data\\_dir=$DATA\\_DIR \\\n--learnable\\_property=\""object\"" \\\n--placeholder\\_token=\""\"" --initializer\\_token=\""toy\"" \\\n--seed=7 \\\n--resolution=512 \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=1 \\\n--max\\_train\\_steps=3000 \\\n--learning\\_rate=2.5e-03 --scale\\_lr \\\n--output\\_dir=\""textual\\_inversion\\_dicoo\""\n```\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. You may not get performance speedup without Bfloat16 support. ### Multi-node distributed training\nBefore running the scripts, make sure to install the library's training dependencies successfully:\n```bash\npython -m pip install oneccl\\_bind\\_pt==1.13 -f https://developer.intel.com/ipex-whl-stable-cpu\n```\n```bash\nexport MODEL\\_NAME=\""CompVis/stable-diffusion-v1-4\""\nexport DATA\\_DIR=\""path-to-dir-containing-dicoo-images\""\noneccl\\_bindings\\_for\\_pytorch\\_path=$(python -c \""from oneccl\\_bindings\\_for\\_pytorch import cwd; print(cwd)\"")\nsource $oneccl\\_bindings\\_for\\_pytorch\\_path/env/setvars.sh\npython -m intel\\_extension\\_for\\_pytorch.cpu.launch --distributed \\\n--hostfile hostfile --nnodes 2 --nproc\\_per\\_node 2 textual\\_inversion.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--train\\_data\\_dir=$DATA\\_DIR \\\n--learnable\\_property=\""object\"" \\\n--placeholder\\_token=\""\"" --initializer\\_token=\""toy\"" \\\n--seed=7 \\\n--resolution=512 \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=1 \\\n--max\\_train\\_steps=750 \\\n--learning\\_rate=2.5e-03 --scale\\_lr \\\n--output\\_dir=\""textual\\_inversion\\_dicoo\""\n```\nThe above is a simple distributed training usage on 2 nodes with 2 processes on each node."", ""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install . ```\nThen cd in the example folder and run:\n```bash\npip install -r requirements.txt\n```\nAnd initialize an [ðŸ¤— Accelerate](https://github.com/huggingface/accelerate/) environment with:\n```bash\naccelerate config\n```\n### Cat toy example\nFirst, let's login so that we can upload the checkpoint to the Hub during training:\n```bash\nhuggingface-cli login\n```\nNow let's get our dataset. For this example we will use some cat images: https://huggingface.co/datasets/diffusers/cat\\_toy\\_example . Let's first download it locally:\n```py\nfrom huggingface\\_hub import snapshot\\_download\nlocal\\_dir = \""./cat\""\nsnapshot\\_download(\""diffusers/cat\\_toy\\_example\"", local\\_dir=local\\_dir, repo\\_type=\""dataset\"", ignore\\_patterns=\"".gitattributes\"")\n```\nThis will be our training data. Now we can launch the training using:\n\\*\\*\\_\\_\\_Note: Change the `resolution` to 768 if you are using the [stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2) 768x768 model.\\_\\_\\_\\*\\*\n```bash\nexport MODEL\\_NAME=\""runwayml/stable-diffusion-v1-5\""\nexport DATA\\_DIR=\""./cat\""\naccelerate launch textual\\_inversion.py \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--train\\_data\\_dir=$DATA\\_DIR \\\n--learnable\\_property=\""object\"" \\\n--placeholder\\_token=\""\"" \\\n--initializer\\_token=\""toy\"" \\\n--resolution=512 \\\n--train\\_batch\\_size=1 \\\n--gradient\\_accumulation\\_steps=4 \\\n--max\\_train\\_steps=3000 \\\n--learning\\_rate=5.0e-04 \\\n--scale\\_lr \\\n--lr\\_scheduler=\""constant\"" \\\n--lr\\_warmup\\_steps=0 \\\n--push\\_to\\_hub \\\n--output\\_dir=\""textual\\_inversion\\_cat\""\n```\nA full training run takes ~1 hour on one V100 GPU.""]","To teach new concepts to Stable Diffusion using Textual Inversion, you need just 3-5 images.",3-5 images
"What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?
","[""In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future. ## Sharded checkpoints\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save\\_pretrained(save\\_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in. You can control the maximum size before sharding with the `max\\_shard\\_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model. ```py\nfrom transformers import AutoModel\nmodel = AutoModel.from\\_pretrained(\""bert-base-cased\"")\n```\nIf you save it using [`~PreTrainedModel.save\\_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n```py\n>>> import os\n>>> import tempfile\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir)\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model.bin']\n```\nNow let's use a maximum shard size of 200MB:\n```py\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model-00001-of-00003.bin', 'pytorch\\_model-00002-of-00003.bin', 'pytorch\\_model-00003-of-00003.bin', 'pytorch\\_model.bin.index.json']\n```\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index."", ""A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from\\_pretrained`] method:\n```py\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... new\\_model = AutoModel.from\\_pretrained(tmp\\_dir)\n```\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard. Behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n```py\n>>> import json\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... with open(os.path.join(tmp\\_dir, \""pytorch\\_model.bin.index.json\""), \""r\"") as f:\n... index = json.load(f)\n>>> print(index.keys())\ndict\\_keys(['metadata', 'weight\\_map'])\n```\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n```py\n>>> index[\""metadata\""]\n{'total\\_size': 433245184}\n```\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state\\_dict`) to the file it's stored in:\n```py\n>>> index[\""weight\\_map\""]\n{'embeddings.LayerNorm.bias': 'pytorch\\_model-00001-of-00003.bin',\n'embeddings.LayerNorm.weight': 'pytorch\\_model-00001-of-00003.bin',\n... ```\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from\\_pretrained`] (like you would do `model.load\\_state\\_dict()` for a full checkpoint) you should use [`~modeling\\_utils.load\\_sharded\\_checkpoint`]:\n```py\n>>> from transformers.modeling\\_utils import load\\_sharded\\_checkpoint\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... load\\_sharded\\_checkpoint(model, tmp\\_dir)\n```\n## Low memory loading\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library. Please read the following guide for more information: [Large model loading using Accelerate](./main\\_classes/model#large-model-loading)""]","In Transformers version 4.18.0, model checkpoints that take more than 10GB of space are automatically sharded into smaller pieces.",10GB
"What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?
","[""Similar to the dataset curation/selection step, documenting and measuring bias-related phenomena in models can help both ML developers who are selecting a model to use as-is or to finetune and ML developers who want to train their own models. For the latter, measures of bias-related phenomena in the model can help them learn from what has worked or what hasnâ€™t for other models and serve as a signal to guide their own development choices. Model cards were originally proposed by [(Mitchell et al., 2019)](https://dl.acm.org/doi/10.1145/3287560.3287596) and provide a framework for model reporting that showcases information relevant to bias risks, including broad ethical considerations, disaggregated evaluation, and use case recommendation. The Hugging Face Hub provides even more tools for model documentation, with a [model card guidebook](https://huggingface.co/docs/hub/model-cards) in the Hub documentation, and an [app that lets you create extensive model cards](https://huggingface.co/spaces/huggingface/Model\\_Cards\\_Writing\\_Tool) easily for your new model. ![Model Card writing tool by Ezi, Marissa, and Meg](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img6.png)\n*[Model Card writing tool](https://huggingface.co/spaces/huggingface/Model_Cards_Writing_Tool) by [Ezi](https://huggingface.co/Ezi), [Marissa](https://huggingface.co/Marissa), and [Meg](https://huggingface.co/meg)*\n\nDocumentation is a great first step for sharing general insights about a modelâ€™s behavior, but it is usually static and presents the same information to all users. In many cases, especially for generative models that can generate outputs to approximate the distribution of their training data, we can gain a more contextual understanding of bias-related phenomena and \\*\\*negative stereotypes\\*\\* by visualizing and contrasting model outputs. Access to model generations can help users bring [intersectional issues in the model behavior](https://www.technologyreview.com/2022/12/12/1064751/the-viral-ai-avatar-app-lensa-undressed-me-without-my-consent/) corresponding to their lived experience, and evaluate to what extent a model reproduces [gendered stereotypes for different adjectives](https://www.vice.com/en/article/bvm35w/this-tool-lets-anyone-see-the-bias-in-ai-image-generators). To facilitate this process, we built a tool that lets you compare generations not just across a set of adjectives and professions, but also across different models! [Go try it out](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to get a sense of which model might carry the least bias risks in your use case. ![Visualize Adjective and Occupation Biases in Image Generation by Sasha](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img7.png)\n\n*[Visualize Adjective and Occupation Biases in Image Generation](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) by [Sasha](https://huggingface.co/sasha)*\n\nVisualization of model outputs isnâ€™t just for generative models though! For classification models, we also want to look out for bias-related harms caused by a modelâ€™s \\*\\*disparate performance\\*\\* on different demographics. If you know what protected classes are most at risk of discrimination and have those annotated in an evaluation set, then you can report disaggregated performance over the different categories in [your model card](https://dl.acm.org/doi/10.1145/3287560.3287596) as mentioned above, so users can make informed decisions. If however, you are worried that you havenâ€™t identified all populations at risk of bias-related harms, or if you do not have access to annotated test examples to measure the biases you suspect, thatâ€™s where interactive visualizations of where and how the model fails come in handy! To help you with this, the [SEAL app](https://huggingface.co/spaces/nazneen/seal) groups similar mistakes by your model and shows you some common features in each cluster. If you want to go further, you can even combine it with the [disaggregators library](https://github.com/huggingface/disaggregators) we introduced in the datasets section to find clusters that are indicative of bias-related failure modes! ![Systematic Error Analysis and Labeling (SEAL) by Nazneen](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img8.png)\n*[Systematic Error Analysis and Labeling (SEAL)](https://huggingface.co/spaces/nazneen/seal) by [Nazneen](https://huggingface.co/nazneen)*\n\nFinally, a few benchmarks exist that can measure bias-related phenomena in models. For language models, benchmarks such as [BOLD](https://github.com/amazon-science/bold), [HONEST](https://aclanthology.org/2021.naacl-main.191.pdf), or [WinoBias](https://aclanthology.org/N18-2003/) provide quantitative evaluations of targeted behaviors that are indicative of biases in the models. While the benchmarks have their [limitations](https://aclanthology.org/2021.acl-long.81/), they do provide a limited view into some pre-identified bias risks that can help describe how the models function or choose between different models. You can find these evaluations pre-computed on a range of common language models [in this exploration Space](https://huggingface.co/spaces/sasha/BiasDetection) to get a first sense of how they compare! ![Language Model Bias Detection by Sasha](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img9.png)\n*[Language Model Bias Detection](https://huggingface.co/spaces/sasha/BiasDetection) by [Sasha](https://huggingface.co/sasha)*\n\nEven with access to a benchmark for the models you are considering, you might find that running evaluations of the larger language models you are considering can be prohibitively expensive or otherwise technically impossible with your own computing resources. The [Evaluation on the Hub](https://huggingface.co/spaces/autoevaluate/model-evaluator) tool we released this year can help with that: not only will it run the evaluations for you, but it will also help connect them to the model documentation so the results are available once and for all â€” so everyone can see, for example, that size [measurably increases bias risks in models like OPT](https://huggingface.co/blog/zero-shot-eval-on-the-hub)! ![Large model WinoBias scores computed with Evaluation on the Hub by Helen, Tristan, Abhishek, Lewis, and Douwe](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img_winobias.png)\n*[[Large model WinoBias scores computed with Evaluation on the Hub](https://huggingface.co/blog/zero-shot-eval-on-the-hub) by [Helen](https://huggingface.co/mathemakitten), [Tristan](https://huggingface.co/Tristan), [Abhishek](https://huggingface.co/abhishek), [Lewis](https://huggingface.co/lewtun), and [Douwe](https://huggingface.co/douwekiela)](https://huggingface.co/spaces/sasha/BiasDetection)*\n\n#### Model selection/development: recommendations\nFor models just as for datasets, different tools for documentation and evaluation will provide different views of bias risks in a model which all have a part to play in helping developers choose, develop, or understand ML systems. \\* Visualize\n\\* Generative model: visualize how the modelâ€™s outputs may reflect stereotypes\n\\* Classification model: visualize model errors to identify failure modes that could lead to disparate performance\n\\* Evaluate\n\\* When possible, evaluate models on relevant benchmarks\n\\* Document\n\\* Share your learnings from visualization and qualitative evaluation\n\\* Report your modelâ€™s disaggregated performance and results on applicable fairness benchmarks\n## Conclusion and Overview of Bias Analysis and Documentation Tools from ðŸ¤—\nAs we learn to leverage ML systems in more and more applications, reaping their benefits equitably will depend on our ability to actively mitigate the risks of bias-related harms associated with the technology."", ""If youâ€™re choosing between two different datasets for a given task or choosing between two ML models trained on different datasets, knowing which one better represents the demographic makeup of your ML systemâ€™s user base can help you make an informed decision to minimize bias-related risks. If youâ€™re curating a dataset iteratively by filtering data points from a source or selecting new sources of data to add, measuring how these choices affect the diversity and biases present in your overall dataset can make it safer to use in general. Weâ€™ve recently released two tools you can leverage to measure your data through a bias-informed lens. The [disaggregatorsðŸ¤— library](https://github.com/huggingface/disaggregators) provides utilities to quantify the composition of your dataset, using either metadata or leveraging models to infer properties of data points. This can be particularly useful to minimize risks of bias-related \\*\\*[representation harms](https://aclanthology.org/P16-2096/)\\*\\* or \\*\\*disparate performances\\*\\* of trained models. Look at the [demo](https://huggingface.co/spaces/society-ethics/disaggregators) to see it applied to the LAION, MedMCQA, and The Stack datasets! ![Disaggregators tool by Nima](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img4.png)\n*[Disaggregator tool](https://huggingface.co/spaces/society-ethics/disaggregators) by [Nima](https://huggingface.co/NimaBoscarino)*\n\nOnce you have some helpful statistics about the composition of your dataset, youâ€™ll also want to look at associations between features in your data items, particularly at associations that may encode derogatory or otherwise negative stereotypes. The Data Measurements Tool we [originally introduced](https://huggingface.co/blog/data-measurements-tool#comparison-statistics) last year allows you to do this by looking at the [normalized Pointwise Mutual Information (nPMI)](https://dl.acm.org/doi/10.1145/3461702.3462557) between terms in your text-based dataset; particularly associations between gendered pronouns that may denote gendered stereotypes. [Run it yourself](https://github.com/huggingface/data-measurements-tool) or [try it here](https://huggingface.co/spaces/huggingface/data-measurements-tool) on a few pre-computed datasets! ![Data Measurements tool by Meg, Sasha, Bibi, and the Gradio team](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img5.png)\n*[Data Measurements tool](https://huggingface.co/spaces/huggingface/data-measurements-tool) by [Meg](https://huggingface.co/meg), [Sasha](https://huggingface.co/sasha), [Bibi](https://huggingface.co/Bibss), and the [Gradio team](https://gradio.app/)*\n\n#### Dataset selection/curation: recommendations\nThese tools arenâ€™t full solutions by themselves, rather, they are designed to support critical examination and improvement of datasets through several lenses, including the lens of bias and bias-related risks. In general, we encourage you to keep the following steps in mind when leveraging these and other tools to mitigate bias risks at the dataset curation/selection stage:\n\\* Identify:\n\\* Aspects of the dataset creation that may exacerbate specific biases\n\\* Demographic categories and social variables that are particularly important to the datasetâ€™s task and domain\n\\* Measure:\n\\* The demographic distribution in your dataset\n\\* Pre-identified negative stereotypes represented\n\\* Document:\n\\* Share what youâ€™ve Identified and Measured in your Dataset Card so it can benefit other users, developers, and otherwise affected people\n\\* Adapt:\n\\* By choosing the dataset least likely to cause bias-related harms\n\\* By iteratively improving your dataset in ways that reduce bias risks\n### I am training/selecting a model for my ML system, how can I address bias?""]","I'm unable to provide information on Weights and Biases (W&B) as it is not related to HuggingFace or gradio-app. If you have questions specifically about HuggingFace or gradio-app, feel free to ask!","To track their machine learning experiments at every stage, from training to production."
"What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?
","[""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\n\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\n\n#### **English** | [ç®€ä½“ä¸­æ–‡](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ç¹é«”ä¸­æ–‡](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [í•œêµ­ì–´](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [EspaÃ±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [æ—¥æœ¬èªž](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [à¤¹à¤¿à¤¨à¥à¤¦à¥€](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [Ð ortuguÃªs](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [à°¤à±†à°²à±à°—à±](https://github.com/huggingface/transformers//blob/main/README_te.md) |\n\n### State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n\n### \n\nðŸ¤— Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio."", ""Please let us know if you have any issue or feature you would like the library to support. For all questions, the [forums](https://discuss.huggingface.co/c/accelerate) is the place to check! For more complex examples in situation, you can look at the official [Transformers examples](https://github.com/huggingface/transformers/tree/master/examples). Each folder contains a `run\\_task\\_no\\_trainer.py` that leverages the Accelerate library!"", ""--\ntitle: \""Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\""\nthumbnail: /blog/assets/80\\_intel/01.png\nauthors:\n- user: juliensimon\n---\n# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\n![image](assets/80\\_intel/01.png)\nThe mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption. Today, we're excited to announce that Intel has officially joined our [Hardware Partner Program](https://huggingface.co/hardware). Thanks to the [Optimum](https://github.com/huggingface/optimum-intel) open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers. Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage. Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms. â€œ\\*Weâ€™re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.\\*â€, says Wei Li, Intel Vice President & General Manager, AI and Analytics. In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs. The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Letâ€™s take a deeper look at it. ## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\nðŸ¤— Optimum provides an API called BetterTransformer, a fast path of standard PyTorch Transformer APIs to benefit from interesting speedups on CPU & GPU through sparsity and fused kernels as Flash Attention. For now, BetterTransformer supports the fastpath from the native [`nn.TransformerEncoderLayer`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) as well as Flash Attention and Memory-Efficient Attention from [`torch.nn.functional.scaled\\_dot\\_product\\_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled\\_dot\\_product\\_attention.html)."", ""This collaboration set the seeds for Leandro and I to eventually join Hugging Face. And I've been here now for around nine months. ### That is incredible. How does it feel to have a copy of your book in your hands? \\*\\*Lewis:\\*\\* I have to say, I just became a parent about a year and a half ago and it feels kind of similar to my son being born. You're holding this thing that you created. It's quite an exciting feeling and so different to actually hold it (compared to reading a PDF). Confirms that itâ€™s actually real and I didn't just dream about it. ### Exactly. Congratulations! Want to briefly read one endorsement that I love about this book;\nâ€œ\\_Complexity made simple. This is a rare and precious book about NLP, transformers, and the growing ecosystem around them, Hugging Face. Whether these are still buzzwords to you or you already have a solid grasp of it all, the authors will navigate you with humor, scientific rigor, and plenty of code examples into the deepest secrets of the coolest technology around. From â€œoff-the-shelf pre-trainedâ€ to â€œfrom-scratch customâ€ models, and from performance to missing labels issues, the authors address practically every real-life struggle of an ML engineer and provide state-of-the-art solutions, making this book destined to dictate the standards in the field for years to come.\\_â€\nâ€”Luca Perrozi Ph.D., Data Science and Machine Learning Associate Manager at Accenture. Checkout [Natural Language Processing with Transformers](https://transformersbook.com/). ### Can you talk about the work you've done with the transformers library? \\*\\*Lewis:\\*\\* One of the things that I experienced in my previous jobs before Hugging Face was there's this challenge in the industry when deploying these models into production; these models are really large in terms of the number of parameters and this adds a lot of complexity to the requirements you might have. So for example, if you're trying to build a chatbot you need this model to be very fast and responsive. And most of the time these models are a bit too slow if you just take an off-the-shelf model, train it, and then try to integrate it into your application. So what I've been working on for the last few months on the transformers library is providing the functionality to export these models into a format that lets you run them much more efficiently using tools that we have at Hugging Face, but also just general tools in the open-source ecosystem. In a way, the philosophy of the transformers library is like writing lots of code so that the users don't have to write that code. In this particular example, what we're talking about is something called the ONNX format. It's a special format that is used in industry where you can basically have a model that's written in PyTorch but you can then convert it to TensorFlow or you can run it on some very dedicated hardware. And if you actually look at what's needed to make this conversion happen in the transformers library, it's fairly gnarly. But we make it so that you only really have to run one line of code and the library will take care of you. So the idea is that this particular feature lets machine learning engineers or even data scientists take their model, convert it to this format, and then optimize it to get faster latency and higher throughput."", ""Port of Hugging Face's Transformers library, using the tch-rs crate and pre-processing from rust-tokenizers. Supports multi-threaded tokenization and GPU inference. This repository exposes the model base architecture, task-specific heads and ready-to-use pipelines. Keywords: Rust, BERT, Inference\n## [EasyNLP](https://github.com/alibaba/EasyNLP)\n[EasyNLP](https://github.com/alibaba/EasyNLP) is an easy-to-use NLP development and application toolkit in PyTorch, first released inside Alibaba in 2021. It is built with scalable distributed training strategies and supports a comprehensive suite of NLP algorithms for various NLP applications. [EasyNLP](https://github.com/alibaba/EasyNLP) integrates knowledge distillation and few-shot learning for landing large pre-trained models, together with various popular multi-modality pre-trained models. It provides a unified framework of model training, inference, and deployment for real-world applications. Keywords: NLP, Knowledge distillation, Few-shot learning, Multi-modality, Training, Inference, Deployment\n## [TurboTransformers](https://github.com/Tencent/TurboTransformers)\nA fast and user-friendly runtime for transformer inference (Bert, Albert, GPT2, Decoders, etc) on CPU and GPU."", ""## Additional resources\n\\* Transformers.js [repository](https://github.com/xenova/transformers.js)\n\\* Transformers.js [docs](https://huggingface.co/docs/transformers.js)\n\\* Transformers.js [demo](https://xenova.github.io/transformers.js/)"", ""--\ntitle: \""How we sped up transformer inference 100x for ðŸ¤— API customers\""\nthumbnail: /blog/assets/09\\_accelerated\\_inference/thumbnail.png\n---\n# How we sped up transformer inference 100x for ðŸ¤— API customers\nðŸ¤— Transformers has become the default library for data scientists all around the world to explore state of the art NLP models and build new NLP features. With over 5,000 pre-trained and fine-tuned models available, in over 250 languages, it is a rich playground, easily accessible whichever framework you are working in. While experimenting with models in ðŸ¤— Transformers is easy, deploying these large models into production with maximum performance, and managing them into an architecture that scales with usage is a \\*\\*hard engineering challenge\\*\\* for any Machine Learning Engineer. This 100x performance gain and built-in scalability is why subscribers of our hosted [Accelerated Inference API](https://huggingface.co/pricing) chose to build their NLP features on top of it. To get to the \\*\\*last 10x of performance\\*\\* boost, the optimizations need to be low-level, specific to the model, and to the target hardware. This post shares some of our approaches squeezing every drop of compute juice for our customers. ðŸ‹\n## Getting to the first 10x speedup\nThe first leg of the optimization journey is the most accessible, all about using the best combination of techniques offered by the [Hugging Face libraries](https://github.com/huggingface/), independent of the target hardware. We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main\\_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:\n-| Naive version | Optimized version |\n-|:---------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------:|\n-|![](/blog/assets/09\\_accelerated\\_inference/unoptimized\\_graph.png)|![](/blog/assets/09\\_accelerated\\_inference/optimized\\_graph.png)|\nTokenization is often a bottleneck for efficiency during inference. We use the most efficient methods from the [ðŸ¤— Tokenizers](https://github.com/huggingface/tokenizers/) library, leveraging the Rust implementation of the model tokenizer in combination with smart caching to get up to 10x speedup for the overall latency. Leveraging the latest features of the Hugging Face libraries, we achieve a reliable 10x speed up compared to an out-of-box deployment for a given model/hardware pair. As new releases of Transformers and Tokenizers typically ship every month, our API customers do not need to constantly adapt to new optimization opportunities, their models just keep running faster."", ""You can read more about it in the [blog announcement](https://huggingface.co/blog/sentence-transformers-in-the-hub): you can find over 200 [models](https://huggingface.co/models?library=sentence-transformers) in the Hub, easily share your models with the rest of the community and reuse models from the community. But that's not all! You can now find over 100 Adapter Transformers in the Hub and try out Speechbrain models with widgets directly in the browser for different tasks such as audio classification. If you're interested in our collaborations to integrate new ML libraries to the Hub, you can read more about them [here](https://huggingface.co/docs/hub/libraries). ![Filter of all libraries](assets/27\\_summer\\_at\\_huggingface/filters.png)\n## Solutions\n### \\*\\*Coming soon: Infinity\\*\\*\nTransformers latency down to 1ms? ðŸ¤¯ðŸ¤¯ðŸ¤¯\nWe have been working on a really sleek solution to achieve unmatched efficiency for state-of-the-art Transformer models, for companies to deploy in their own infrastructure. - Infinity comes as a single-container and can be deployed in any production environment. - It can achieve 1ms latency for BERT-like models on GPU and 4-10ms on CPU ðŸ¤¯ðŸ¤¯ðŸ¤¯\n- Infinity meets the highest security requirements and can be integrated into your system without the need for internet access. You have control over all incoming and outgoing traffic. âš ï¸ Join us for a [live announcement and demo on Sep 28](https://app.livestorm.co/hugging-face/hugging-face-infinity-launch?type=detailed), where we will be showcasing Infinity for the first time in public! ### \\*\\*NEW: Hardware Acceleration\\*\\*\nHugging Face is [partnering with leading AI hardware accelerators](http://hf.co/hardware) such as Intel, Qualcomm and GraphCore to make state-of-the-art production performance accessible and extend training capabilities on SOTA hardware."", ""--\ntitle: \""Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\""\nthumbnail: /blog/assets/60\\_habana/habana.png\nauthors:\n- user: susanlansing\nguest: true\n---\n# Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\n\\*Santa Clara and San Francisco, CA, April 12th, 2022\\*\nPowered by deep learning, transformer models deliver state-of-the-art performance on a wide range of machine learning tasks, such as natural language processing, computer vision, speech, and more. However, training them at scale often requires a large amount of computing power, making the whole process unnecessarily long, complex, and costly. Today, [HabanaÂ® Labs](https://habana.ai/), a pioneer in high-efficiency, purpose-built deep learning processors, and Hugging Face, the home of [Transformer](https://github.com/huggingface/transformers) models, are happy to announce that theyâ€™re joining forces to make it easier and quicker to train high-quality transformer models. Thanks to the integration of Habanaâ€™s [SynapseAI software suite](https://habana.ai/training-software/) with the Hugging Face [Optimum open-source library](https://github.com/huggingface/optimum), data scientists and machine learning engineers can now accelerate their Transformer training jobs on Habana processors with just a few lines of code and enjoy greater productivity as well as lower training cost. [Habana Gaudi](https://habana.ai/training/) training solutions, which power Amazonâ€™s EC2 DL1 instances and Supermicroâ€™s X12 Gaudi AI Training Server, deliver price/performance up to 40% lower than comparable training solutions and enable customers to train more while spending less. The integration of ten 100 Gigabit Ethernet ports onto every Gaudi processor enables system scaling from 1 to thousands of Gaudis with ease and cost-efficiency. Habanaâ€™s SynapseAIÂ® is optimizedâ€”at inceptionâ€”to enable Gaudi performance and usability, supports TensorFlow and PyTorch frameworks, with a focus on computer vision and natural language processing applications. With 60,000+ stars on Github, 30,000+ models, and millions of monthly visits, Hugging Face is one of the fastest-growing projects in open source software history, and the go-to place for the machine learning community. With its [Hardware Partner Program](https://huggingface.co/hardware), Hugging Face provides Gaudiâ€™s advanced deep learning hardware with the ultimate Transformer toolset. This partnership will enable rapid expansion of the Habana Gaudi training transformer model library, bringing Gaudi efficiency and ease of use to a wide array of customer use cases like natural language processing, computer vision, speech, and more. â€œ\\*Weâ€™re excited to partner with Hugging Face and its many open-source developers to address the growing demand for transformer models that benefit from the efficiency, usability, and scalability of the Gaudi training platform\\*â€, said Sree Ganesan, head of software product management, Habana Labs. â€œHabana Gaudi brings a new level of efficiency to deep learning model training, and weâ€™re super excited to make this performance easily accessible to Transformer users with minimal code changes through Optimumâ€, said Jeff Boudier, product director at Hugging Face. To learn how to get started training with Habana Gaudi, please visit [https://developer.habana.ai](https://developer.habana.ai)."", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Philosophy\nðŸ¤— Transformers is an opinionated library built for:\n- machine learning researchers and educators seeking to use, study or extend large-scale Transformers models. - hands-on practitioners who want to fine-tune those models or serve them in production, or both. - engineers who just want to download a pretrained model and use it to solve a given machine learning task. The library was designed with two strong goals in mind:\n1. Be as easy and fast to use as possible:\n- We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions,\njust three standard classes required to use each model: [configuration](main\\_classes/configuration),\n[models](main\\_classes/model), and a preprocessing class ([tokenizer](main\\_classes/tokenizer) for NLP, [image processor](main\\_classes/image\\_processor) for vision, [feature extractor](main\\_classes/feature\\_extractor) for audio, and [processor](main\\_classes/processors) for multimodal inputs). - All of these classes can be initialized in a simple and unified way from pretrained instances by using a common\n`from\\_pretrained()` method which downloads (if needed), caches and\nloads the related class instance and associated data (configurations' hyperparameters, tokenizers' vocabulary,\nand models' weights) from a pretrained checkpoint provided on [Hugging Face Hub](https://huggingface.co/models) or your own saved checkpoint. - On top of those three base classes, the library provides two APIs: [`pipeline`] for quickly\nusing a model for inference on a given task and [`Trainer`] to quickly train or fine-tune a PyTorch model (all TensorFlow models are compatible with `Keras.fit`). - As a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to\nextend or build upon the library, just use regular Python, PyTorch, TensorFlow, Keras modules and inherit from the base\nclasses of the library to reuse functionalities like model loading and saving. If you'd like to learn more about our coding philosophy for models, check out our [Repeat Yourself](https://huggingface.co/blog/transformers-design-philosophy) blog post. 2. Provide state-of-the-art models with performances as close as possible to the original models:\n- We provide at least one example for each architecture which reproduces a result provided by the official authors\nof said architecture. - The code is usually as close to the original code base as possible which means some PyTorch code may be not as\n\\*pytorchic\\* as it could be as a result of being converted TensorFlow code and vice versa."", ""## Additional resources\n\\* Transformers [library](https://github.com/huggingface/transformers). \\* Transformers [docs](https://huggingface.co/docs/transformers/index). \\* Share a model [guide](https://huggingface.co/docs/transformers/model\\_sharing)."", ""# ðŸ¤— Transformers\nState-of-the-art Machine Learning for [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), and [JAX](https://jax.readthedocs.io/en/latest/). ðŸ¤— Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:\nðŸ“ \\*\\*Natural Language Processing\\*\\*: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation. ðŸ–¼ï¸ \\*\\*Computer Vision\\*\\*: image classification, object detection, and segmentation. ðŸ—£ï¸ \\*\\*Audio\\*\\*: automatic speech recognition and audio classification. ðŸ™ \\*\\*Multimodal\\*\\*: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering. ðŸ¤— Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model's life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments. Join the growing community on the [Hub](https://huggingface.co/models), [forum](https://discuss.huggingface.co/), or [Discord](https://discord.com/invite/JfAtkvEtRb) today! ## If you are looking for custom support from the Hugging Face team\n[![HuggingFace Expert Acceleration Program](https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png)](https://huggingface.co/support)\n## Contents\nThe documentation is organized into five sections:\n- \\*\\*GET STARTED\\*\\* provides a quick tour of the library and installation instructions to get up and running."", ""!---\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\n\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\n\n#### **English** | [ç®€ä½“ä¸­æ–‡](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ç¹é«”ä¸­æ–‡](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [í•œêµ­ì–´](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [EspaÃ±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [æ—¥æœ¬èªž](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [à¤¹à¤¿à¤¨à¥à¤¦à¥€](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [Ð ortuguÃªs](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [à°¤à±†à°²à±à°—à±](https://github.com/huggingface/transformers//blob/main/README_te.md) |\n\n### Aprendizado de mÃ¡quina de Ãºltima geraÃ§Ã£o para JAX, PyTorch e TensorFlow\n\n### \n\nA biblioteca ðŸ¤— Transformers oferece milhares de modelos prÃ©-treinados para executar tarefas em diferentes modalidades, como texto, visÃ£o e Ã¡udio."", ""!---\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\n\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\n\n#### [English](https://github.com/huggingface/transformers/blob/main/README.md) | [ç®€ä½“ä¸­æ–‡](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ç¹é«”ä¸­æ–‡](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [í•œêµ­ì–´](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [EspaÃ±ol](https://github.com/huggingface/transformers/blob/main/README_es.md) | [æ—¥æœ¬èªž](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [à¤¹à¤¿à¤¨à¥à¤¦à¥€](https://github.com/huggingface/transformers/blob/main/README_hd.md) | **Ð ÑƒÑÑÐºÐ¸Ð¹** [à°¤à±†à°²à±à°—à±](https://github.com/huggingface/transformers//blob/main/README_te.md) |\n\n### Ð¡Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ðµ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ JAX, PyTorch Ð¸ TensorFlow\n\n### \n\nðŸ¤— Transformers Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ñ‚Ñ‹ÑÑÑ‡Ð¸ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡, Ñ‚Ð°ÐºÐ¸Ñ… ÐºÐ°Ðº Ñ‚ÐµÐºÑÑ‚, Ð·Ñ€ÐµÐ½Ð¸Ðµ Ð¸ Ð°ÑƒÐ´Ð¸Ð¾."", ""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_logo_name.png)\n\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\n\n#### [English](https://github.com/huggingface/transformers/) | [ç®€ä½“ä¸­æ–‡](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [ç¹é«”ä¸­æ–‡](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [í•œêµ­ì–´](https://github.com/huggingface/transformers/blob/main/README_ko.md) | **EspaÃ±ol** | [æ—¥æœ¬èªž](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [à¤¹à¤¿à¤¨à¥à¤¦à¥€](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [à°¤à±†à°²à±à°—à±](https://github.com/huggingface/transformers//blob/main/README_te.md) |\n\n### Lo Ãºltimo de Machine Learning para JAX, PyTorch y TensorFlow\n\n### \n\nðŸ¤— Transformers aporta miles de modelos preentrenados Para realizar tareas en diferentes modalidades como texto, vision, y audio.""]",The name of the open-source library created by Hugging Face to simplify Transformer acceleration is **Optimum**.,Optimum
"What parameter is used to ensure that elements in a row have the same height in Gradio?
","[""The Row will wrap if there isn't sufficient space to satisfy all `min\\_width` values. Learn more about Rows in the [docs](https://gradio.app/docs/#row). ## Columns and Nesting\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n$code\\_rows\\_and\\_columns\n$demo\\_rows\\_and\\_columns\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. Learn more about Columns in the [docs](https://gradio.app/docs/#column). # Dimensions\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases. Below is an example illustrating the use of viewport width (vw):\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\nim = gr.ImageEditor(\nwidth=\""50vw\"",\n)\ndemo.launch()\n```\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n```python\nimport gradio as gr\ncss = \""\""\""\n.container {\nheight: 100vh;\n}\n\""\""\""\nwith gr.Blocks(css=css) as demo:\nwith gr.Column(elem\\_classes=[\""container\""]):\nname = gr.Chatbot(value=[[\""1\"", \""2\""]], height=\""70%\"")\ndemo.launch()\n```\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height."", ""Gradio Demo: blocks\\_group\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ndef greet(name):\nreturn \""Hello \"" + name + \""!\""\nwith gr.Blocks() as demo:\ngr.Markdown(\""### This is a couple of elements without any gr.Group. Form elements naturally group together anyway.\"")\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\ngr.Image()\ngr.Slider()\ngr.Markdown(\""### This is the same set put in a gr.Group.\"")\nwith gr.Group():\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\ngr.Image()\ngr.Slider()\ngr.Markdown(\""### Now in a Row, no group.\"")\nwith gr.Row():\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\ngr.Image()\ngr.Slider()\ngr.Markdown(\""### Now in a Row in a group.\"")\nwith gr.Group():\nwith gr.Row():\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\ngr.Image()\ngr.Slider()\ngr.Markdown(\""### Several rows grouped together.\"")\nwith gr.Group():\nwith gr.Row():\ngr.Textbox(\""A\"")\ngr.Number(3)\ngr.Button()\nwith gr.Row():\ngr.Image()\ngr.Audio()\ngr.Markdown(\""### Several columns grouped together. If columns are uneven, there is a gray group background.\"")\nwith gr.Group():\nwith gr.Row():\nwith gr.Column():\nname = gr.Textbox(label=\""Name\"")\nbtn = gr.Button(\""Hello\"")\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True)\ngr.Number()\ngr.Textbox()\nwith gr.Column():\ngr.Image()\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True)\nwith gr.Row():\ngr.Number(scale=2)\ngr.Textbox()\ngr.Markdown(\""### container=False removes label, padding, and block border, placing elements 'directly' on background.\"")\ngr.Radio([1,2,3], container=False)\ngr.Textbox(container=False)\ngr.Image(\""https://picsum.photos/id/237/200/300\"", container=False, height=200)\ngr.Markdown(\""### Textbox, Dropdown, and Number input boxes takes up full space when within a group without a container.\"")\nwith gr.Group():\nname = gr.Textbox(label=\""Name\"")\noutput = gr.Textbox(show\\_label=False, container=False)\ngreet\\_btn = gr.Button(\""Greet\"")\nwith gr.Row():\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True, container=False)\ngr.Textbox(container=False)\ngr.Number(container=False)\ngr.Image(height=100)\ngreet\\_btn.click(fn=greet, inputs=name, outputs=output, api\\_name=\""greet\"")\ngr.Markdown(\""### More examples\"")\nwith gr.Group():\ngr.Chatbot()\nwith gr.Row():\nname = gr.Textbox(label=\""Prompot\"", container=False)\ngo = gr.Button(\""go\"", scale=0)\nwith gr.Column():\ngr.Radio([1,2,3], container=False)\ngr.Slider(0, 20, container=False)\nwith gr.Group():\nwith gr.Row():\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True, container=False, elem\\_id=\""here2\"")\ngr.Number(container=False)\ngr.Textbox(container=False)\nwith gr.Row():\nwith gr.Column():\ngr.Dropdown([\""a\"", \""b\"", \""c\""], interactive=True, container=False, elem\\_id=\""here2\"")\nwith gr.Column():\ngr.Number(container=False)\nwith gr.Column():\ngr.Textbox(container=False)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Controlling Layout\nBy default, Components in Blocks are arranged vertically. Let's take a look at how we can rearrange Components. Under the hood, this layout structure uses the [flexbox model of web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS\\_Flexible\\_Box\\_Layout/Basic\\_Concepts\\_of\\_Flexbox). ## Rows\nElements within a `with gr.Row` clause will all be displayed horizontally. For example, to display two Buttons side by side:\n```python\nwith gr.Blocks() as demo:\nwith gr.Row():\nbtn1 = gr.Button(\""Button 1\"")\nbtn2 = gr.Button(\""Button 2\"")\n```\nTo make every element in a Row have the same height, use the `equal\\_height` argument of the `style` method. ```python\nwith gr.Blocks() as demo:\nwith gr.Row(equal\\_height=True):\ntextbox = gr.Textbox()\nbtn2 = gr.Button(\""Button 2\"")\n```\nThe widths of elements in a Row can be controlled via a combination of `scale` and `min\\_width` arguments that are present in every Component. - `scale` is an integer that defines how an element will take up space in a Row. If scale is set to `0`, and element will not expand to take up space. If scale is set to `1` or greater, the element well expand. Multiple elements in a row will expand proportional to their scale. Below, `btn1` will expand twice as much as `btn2`, while `btn0` will not expand at all:\n```python\nwith gr.Blocks() as demo:\nwith gr.Row():\nbtn0 = gr.Button(\""Button 0\"", scale=0)\nbtn1 = gr.Button(\""Button 1\"", scale=1)\nbtn2 = gr.Button(\""Button 2\"", scale=2)\n```\n- `min\\_width` will set the minimum width the element will take."", ""`@gradio/gallery`\n```html\n```\nBaseGallery\n```javascript\nexport let show\\_label = true;\nexport let label: string;\nexport let root = \""\"";\nexport let root\\_url: null | string = null;\nexport let value: { image: FileData; caption: string | null }[] | null = null;\nexport let columns: number | number[] | undefined = [2];\nexport let rows: number | number[] | undefined = undefined;\nexport let height: number | \""auto\"" = \""auto\"";\nexport let preview: boolean;\nexport let allow\\_preview = true;\nexport let object\\_fit: \""contain\"" | \""cover\"" | \""fill\"" | \""none\"" | \""scale-down\"" =\n\""cover\"";\nexport let show\\_share\\_button = false;\nexport let show\\_download\\_button = false;\nexport let i18n: I18nFormatter;\nexport let selected\\_index: number | null = null;\n```"", ""## 3.4.1\n### New Features:\n###### 1. See Past and Upcoming Changes in the Release History ðŸ‘€\nYou can now see gradio's release history directly on the website, and also keep track of upcoming changes. Just go [here](https://gradio.app/changelog/). ![release-history](https://user-images.githubusercontent.com/9021060/193145458-3de699f7-7620-45de-aa73-a1c1b9b96257.gif)\n### Bug Fixes:\n1. Fix typo in guide image path by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2357](https://github.com/gradio-app/gradio/pull/2357)\n2. Raise error if Blocks has duplicate component with same IDs by [@abidlabs](https://github.com/abidlabs) in [PR 2359](https://github.com/gradio-app/gradio/pull/2359)\n3. Catch the permission exception on the audio component by [@Ian-GL](https://github.com/Ian-GL) in [PR 2330](https://github.com/gradio-app/gradio/pull/2330)\n4. Fix image_classifier_interface_load demo by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2365](https://github.com/gradio-app/gradio/pull/2365)\n5. Fix combining adjacent components without gaps by introducing `gr.Row(variant=\""compact\"")` by [@aliabid94](https://github.com/aliabid94) in [PR 2291](https://github.com/gradio-app/gradio/pull/2291) This comes with deprecation of the following arguments for `Component.style`: `round`, `margin`, `border`."", ""Gradio Demo: rows\\_and\\_columns\n```\n!pip install -q gradio\n```\n```\n# Downloading files from the demo repo\nimport os\nos.mkdir('images')\n!wget -q -O images/cheetah.jpg https://github.com/gradio-app/gradio/raw/main/demo/rows\\_and\\_columns/images/cheetah.jpg\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\nwith gr.Row():\ntext1 = gr.Textbox(label=\""t1\"")\nslider2 = gr.Textbox(label=\""s2\"")\ndrop3 = gr.Dropdown([\""a\"", \""b\"", \""c\""], label=\""d3\"")\nwith gr.Row():\nwith gr.Column(scale=1, min\\_width=600):\ntext1 = gr.Textbox(label=\""prompt 1\"")\ntext2 = gr.Textbox(label=\""prompt 2\"")\ninbtw = gr.Button(\""Between\"")\ntext4 = gr.Textbox(label=\""prompt 1\"")\ntext5 = gr.Textbox(label=\""prompt 2\"")\nwith gr.Column(scale=2, min\\_width=600):\nimg1 = gr.Image(\""images/cheetah.jpg\"")\nbtn = gr.Button(\""Go\"")\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""## 3.4.1\n### New Features:\n###### 1. See Past and Upcoming Changes in the Release History ðŸ‘€\nYou can now see gradio's release history directly on the website, and also keep track of upcoming changes. Just go [here](https://gradio.app/changelog/). ![release-history](https://user-images.githubusercontent.com/9021060/193145458-3de699f7-7620-45de-aa73-a1c1b9b96257.gif)\n### Bug Fixes:\n1. Fix typo in guide image path by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2357](https://github.com/gradio-app/gradio/pull/2357)\n2. Raise error if Blocks has duplicate component with same IDs by [@abidlabs](https://github.com/abidlabs) in [PR 2359](https://github.com/gradio-app/gradio/pull/2359)\n3. Catch the permission exception on the audio component by [@Ian-GL](https://github.com/Ian-GL) in [PR 2330](https://github.com/gradio-app/gradio/pull/2330)\n4. Fix image_classifier_interface_load demo by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2365](https://github.com/gradio-app/gradio/pull/2365)\n5. Fix combining adjacent components without gaps by introducing `gr.Row(variant=\""compact\"")` by [@aliabid94](https://github.com/aliabid94) in [PR 2291](https://github.com/gradio-app/gradio/pull/2291) This comes with deprecation of the following arguments for `Component.style`: `round`, `margin`, `border`.""]",The parameter used to ensure that elements in a row have the same height in Gradio is the `equal_height` argument of the `style` method.,equal_height
"What is the command to install the latest version of Optimum with OpenVINO support?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Installation\nðŸ¤— Optimum can be installed using `pip` as follows:\n```bash\npython -m pip install optimum\n```\nIf you'd like to use the accelerator-specific features of ðŸ¤— Optimum, you can install the required dependencies according to the table below:\n| Accelerator | Installation |\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\n| [ONNX runtime](https://onnxruntime.ai/docs/) | `pip install --upgrade-strategy eager install optimum[onnxruntime]`|\n| [Intel Neural Compressor (INC)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) | `pip install --upgrade-strategy eager optimum[neural-compressor]` |\n| [Intel OpenVINO](https://docs.openvino.ai/latest/index.html) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |\n| [AMD Instinct GPUs and Ryzen AI NPU](https://www.amd.com/en/graphics/instinct-server-accelerators) | `pip install --upgrade-strategy eager optimum[amd]` |\n| [Habana Gaudi Processor (HPU)](https://habana.ai/training/) | `pip install --upgrade-strategy eager optimum[habana]` |\n| [FuriosaAI](https://www.furiosa.ai/) | `pip install --upgrade-strategy eager optimum[furiosa]` |\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version. If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you can install the base library from source as follows:\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\nFor the accelerator-specific features, you can install them by appending `optimum[accelerator\\_type]` to the `pip` command, e.g. ```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```"", ""You can find more examples in the ðŸ¤— Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion-xl), and running SDXL in OpenVINO is supported for text-to-image and image-to-image."", ""--\ntitle: \""Accelerate your models with ðŸ¤— Optimum Intel and OpenVINO\""\nthumbnail: /blog/assets/113\\_openvino/thumbnail.png\nauthors:\n- user: echarlaix\n- user: juliensimon\n---\n# Accelerate your models with ðŸ¤— Optimum Intel and OpenVINO\n![image](assets/113\\_openvino/thumbnail.png)\nLast July, we [announced](https://huggingface.co/blog/intel) that Intel and Hugging Face would collaborate on building state-of-the-art yet simple hardware acceleration tools for Transformer models. â€‹\nToday, we are very happy to announce that we added Intel [OpenVINO](https://docs.openvino.ai/latest/index.html) to [Optimum Intel](https://github.com/huggingface/optimum-intel). You can now easily perform inference with OpenVINO Runtime on a variety of Intel processors ([see](https://docs.openvino.ai/latest/openvino\\_docs\\_OV\\_UG\\_supported\\_plugins\\_Supported\\_Devices.html) the full list of supported devices) using Transformers models which can be hosted either on the Hugging Face hub or locally. You can also quantize your model with the OpenVINO Neural Network Compression Framework ([NNCF](https://github.com/openvinotoolkit/nncf)), and reduce its size and prediction latency in near minutes. â€‹\nThis first release is based on OpenVINO 2022.2 and enables inference for a large quantity of PyTorch models using our [`OVModels`](https://huggingface.co/docs/optimum/intel/inference). Post-training static quantization and quantization aware training can be applied on many encoder models (BERT, DistilBERT, etc.). More encoder models will be supported in the upcoming OpenVINO release. Currently the quantization of Encoder Decoder models is not enabled, however this restriction should be lifted with our integration of the next OpenVINO release. â€‹Let us show you how to get started in minutes!â€‹\n## Quantizing a Vision Transformer with Optimum Intel and OpenVINO\nâ€‹\nIn this example, we will run post-training static quantization on a Vision Transformer (ViT) [model](https://huggingface.co/juliensimon/autotrain-food101-1471154050) fine-tuned for image classification on the [food101](https://huggingface.co/datasets/food101) dataset."", ""```\n\\* If you want to install the dependencies beyond in a local Python environment. You can pip install them once you have [ROCM 5.7](https://rocmdocs.amd.com/en/latest/deploy/linux/quick\\_start.html) well installed. ```bash\npip install onnx ninja\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm5.7\npip install pip install --pre onnxruntime-training -f https://download.onnxruntime.ai/onnxruntime\\_nightly\\_rocm57.html\npip install torch-ort\npip install --upgrade protobuf==3.20.2\n```\nAnd run post-installation configuration:\n```bash\npython -m torch\\_ort.configure\n```\n### Install Optimum\nYou can install Optimum via pypi:\n```bash\npip install optimum\n```\nOr install from source:\n```bash\npip install git+https://github.com/huggingface/optimum.git\n```\nThis command installs the current main dev version of Optimum, which could include latest developments(new features, bug fixes). However, the\nmain version might not be very stable. If you run into any problem, please open an [issue](https://github.com/huggingface/optimum/issues) so\nthat we can fix it as soon as possible. ## ORTTrainer\nThe [`ORTTrainer`] class inherits the [`Trainer`](https://huggingface.co/docs/transformers/main/en/main\\_classes/trainer#trainer)\nof Transformers. You can easily adapt the codes by replacing `Trainer` of transformers with `ORTTrainer` to take advantage of the acceleration\nempowered by ONNX Runtime. Here is an example of how to use `ORTTrainer` compared with `Trainer`:\n```diff\n-from transformers import Trainer, TrainingArguments\n+from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments\n# Step 1: Define training arguments\n-training\\_args = TrainingArguments(\n+training\\_args = ORTTrainingArguments(\noutput\\_dir=\""path/to/save/folder/\"",\n- optim = \""adamw\\_hf\"",\n+ optim=\""adamw\\_ort\\_fused\"",\n... )\n# Step 2: Create your ONNX Runtime Trainer\n-trainer = Trainer(\n+trainer = ORTTrainer(\nmodel=model,\nargs=training\\_args,\ntrain\\_dataset=train\\_dataset,\n+ feature=\""text-classification\"",\n... )\n# Step 3: Use ONNX Runtime for training!ðŸ¤—\ntrainer.train()\n```\nCheck out more detailed [example scripts](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training) in the optimum repository. ## ORTSeq2SeqTrainer\nThe [`ORTSeq2SeqTrainer`] class is similar to the [`Seq2SeqTrainer`](https://huggingface.co/docs/transformers/main/en/main\\_classes/trainer#transformers.Seq2SeqTrainer)\nof Transformers. You can easily adapt the codes by replacing `Seq2SeqTrainer` of transformers with `ORTSeq2SeqTrainer` to take advantage of the acceleration\nempowered by ONNX Runtime. Here is an example of how to use `ORTSeq2SeqTrainer` compared with `Seq2SeqTrainer`:\n```diff\n-from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n+from optimum.onnxruntime import ORTSeq2SeqTrainer, ORTSeq2SeqTrainingArguments\n# Step 1: Define training arguments\n-training\\_args = Seq2SeqTrainingArguments(\n+training\\_args = ORTSeq2SeqTrainingArguments(\noutput\\_dir=\""path/to/save/folder/\"",\n- optim = \""adamw\\_hf\"",\n+ optim=\""adamw\\_ort\\_fused\"",\n... )\n# Step 2: Create your ONNX Runtime Seq2SeqTrainer\n-trainer = Seq2SeqTrainer(\n+trainer = ORTSeq2SeqTrainer(\nmodel=model,\nargs=training\\_args,\ntrain\\_dataset=train\\_dataset,\n+ feature=\""text2text-generation\"",\n... )\n# Step 3: Use ONNX Runtime for training!ðŸ¤—\ntrainer.train()\n```\nCheck out more detailed [example scripts](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training) in the optimum repository."", ""In this blog post, you'll learn:\n- [1. What is Optimum? An ELI5](#1-what-is-optimum-an-eli5)\n- [2. New Optimum inference and pipeline features](#2-new-optimum-inference-and-pipeline-features)\n- [3. End-to-End tutorial on accelerating RoBERTa for Question-Answering including quantization and optimization](#3-end-to-end-tutorial-on-accelerating-roberta-for-question-answering-including-quantization-and-optimization)\n- [4. Current Limitations](#4-current-limitations)\n- [5. Optimum Inference FAQ](#5-optimum-inference-faq)\n- [6. Whatâ€™s next?](#6-whats-next)\nLet's get started! ðŸš€\n## 1. What is Optimum? An ELI5\n[Hugging Face Optimum](https://github.com/huggingface/optimum) is an open-source library and an extension of [Hugging Face Transformers](https://github.com/huggingface/transformers), that provides a unified API of performance optimization tools to achieve maximum efficiency to train and run models on accelerated hardware, including toolkits for optimized performance on [Graphcore IPU](https://github.com/huggingface/optimum-graphcore) and [Habana Gaudi](https://github.com/huggingface/optimum-habana)."", ""As demonstrated by this [Intel Space](https://huggingface.co/spaces/Intel/Stable-Diffusion-Side-by-Side), the same code runs on a previous generation Intel Xeon (code name Ice Lake) in about 45 seconds. Out of the box, we can see that Sapphire Rapids CPUs are quite faster without any code change! Now, let's accelerate! ## Optimum Intel and OpenVINO\n[Optimum Intel](https://huggingface.co/docs/optimum/intel/index) accelerates end-to-end pipelines on Intel architectures. Its API is extremely similar to the vanilla [Diffusers](https://huggingface.co/docs/diffusers/index) API, making it trivial to adapt existing code. Optimum Intel supports [OpenVINO](https://docs.openvino.ai/latest/index.html), an Intel open-source toolkit for high-performance inference. Optimum Intel and OpenVINO can be installed as follows:\n```\npip install optimum[openvino]\n```\nStarting from the code above, we only need to replace `StableDiffusionPipeline` with `OVStableDiffusionPipeline`. To load a PyTorch model and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model. ```python\nfrom optimum.intel.openvino import OVStableDiffusionPipeline\n... ov\\_pipe = OVStableDiffusionPipeline.from\\_pretrained(model\\_id, export=True)\nlatency = elapsed\\_time(ov\\_pipe, prompt)\nprint(latency)\n# Don't forget to save the exported model\nov\\_pipe.save\\_pretrained(\""./openvino\"")\n```\nOpenVINO automatically optimizes the model for the `bfloat16` format. Thanks to this, the average latency is now \\*\\*16.7 seconds\\*\\*, a sweet 2x speedup. The pipeline above support dynamic input shapes, with no restriction on the number of images or their resolution. With Stable Diffusion, your application is usually restricted to one (or a few) different output resolutions, such as 512x512, or 256x256."", ""Thanks to built-in optimization techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep tweaking for maximum performance. [Optimum Intel](https://github.com/huggingface/optimum-intel) is part of Optimum and builds on top of the [Intel Neural Compressor](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) (INC). INC is an [open-source library](https://github.com/intel/neural-compressor) that delivers unified interfaces across multiple deep learning frameworks for popular network compression technologies, such as quantization, pruning, and knowledge distillation. This tool supports automatic accuracy-driven tuning strategies to help users quickly build the best quantized model. With Optimum Intel, you can apply state-of-the-art optimization techniques to your Transformers with minimal effort. Letâ€™s look at a complete example. ## Case study: Quantizing DistilBERT with Optimum Intel\nIn this example, we will run post-training quantization on a DistilBERT model fine-tuned for classification. Quantization is a process that shrinks memory and compute requirements by reducing the bit width of model parameters."", ""![ONNX Runtime](https://github.com/huggingface/optimum/actions/workflows/test\\_onnxruntime.yml/badge.svg)](https://github.com/huggingface/optimum/actions/workflows/test\\_onnxruntime.yml)\n# Hugging Face Optimum\nðŸ¤— Optimum is an extension of ðŸ¤— Transformers and Diffusers, providing a set of optimization tools enabling maximum efficiency to train and run models on targeted hardware, while keeping things easy to use. ## Installation\nðŸ¤— Optimum can be installed using `pip` as follows:\n```bash\npython -m pip install optimum\n```\nIf you'd like to use the accelerator-specific features of ðŸ¤— Optimum, you can install the required dependencies according to the table below:\n| Accelerator | Installation |\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\n| [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/overview) | `pip install --upgrade-strategy eager optimum[onnxruntime]` |\n| [Intel Neural Compressor](https://huggingface.co/docs/optimum/intel/index) | `pip install --upgrade-strategy eager optimum[neural-compressor]`|\n| [OpenVINO](https://huggingface.co/docs/optimum/intel/index) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |\n| [AMD Instinct GPUs and Ryzen AI NPU](https://huggingface.co/docs/optimum/amd/index) | `pip install --upgrade-strategy eager optimum[amd]` |\n| [Habana Gaudi Processor (HPU)](https://huggingface.co/docs/optimum/habana/index) | `pip install --upgrade-strategy eager optimum[habana]` |\n| [FuriosaAI](https://huggingface.co/docs/optimum/furiosa/index) | `pip install --upgrade-strategy eager optimum[furiosa]` |\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version. To install from source:\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\nFor the accelerator-specific features, append `optimum[accelerator\\_type]` to the above command:\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```\n## Accelerated Inference\nðŸ¤— Optimum provides multiple tools to export and run optimized models on various ecosystems:\n- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage\\_guides/export\\_a\\_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage\\_guides/models)\n- TensorFlow Lite\n- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)\n- Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage\\_guides/accelerate\\_inference)\nThe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line. ### Features summary\n| Features | [ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview)| [Neural Compressor](https://huggingface.co/docs/optimum/main/en/intel/optimization\\_inc)| [OpenVINO](https://huggingface.co/docs/optimum/main/en/intel/inference)| [TensorFlow Lite](https://huggingface.co/docs/optimum/main/en/exporters/tflite/overview)|\n|:----------------------------------:|:------------------:|:------------------:|:------------------:|:------------------:|\n| Graph optimization | :heavy\\_check\\_mark: | N/A | :heavy\\_check\\_mark: | N/A |\n| Post-training dynamic quantization | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | N/A | :heavy\\_check\\_mark: |\n| Post-training static quantization | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: |\n| Quantization Aware Training (QAT) | N/A | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | N/A |\n| FP16 (half precision) | :heavy\\_check\\_mark: | N/A | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: |\n| Pruning | N/A | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | N/A |\n| Knowledge Distillation | N/A | :heavy\\_check\\_mark: | :heavy\\_check\\_mark: | N/A |\n### OpenVINO\nBefore you begin, make sure you have all the necessary libraries installed :\n```bash\npip install --upgrade-strategy eager optimum[openvino,nncf]\n```\nIt is possible to export ðŸ¤— Transformers and Diffusers models to the OpenVINO format easily:\n```bash\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert\\_sst2\\_ov\n```\nIf you add `--int8`, the weights will be quantized to INT8."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Quick tour\nThis quick tour is intended for developers who are ready to dive into the code and see examples of how to integrate ðŸ¤— Optimum into their model training and inference workflows. ## Accelerated inference\n#### OpenVINO\nTo load a model and run inference with OpenVINO Runtime, you can just replace your `AutoModelForXxx` class with the corresponding `OVModelForXxx` class. If you want to load a PyTorch checkpoint, set `export=True` to convert your model to the OpenVINO IR (Intermediate Representation). ```diff\n- from transformers import AutoModelForSequenceClassification\n+ from optimum.intel.openvino import OVModelForSequenceClassification\nfrom transformers import AutoTokenizer, pipeline\n# Download a tokenizer and model from the Hub and convert to OpenVINO format\ntokenizer = AutoTokenizer.from\\_pretrained(model\\_id)\nmodel\\_id = \""distilbert-base-uncased-finetuned-sst-2-english\""\n- model = AutoModelForSequenceClassification.from\\_pretrained(model\\_id)\n+ model = OVModelForSequenceClassification.from\\_pretrained(model\\_id, export=True)\n# Run inference! classifier = pipeline(\""text-classification\"", model=model, tokenizer=tokenizer)\nresults = classifier(\""He's a dreadful magician.\"")\n```\nYou can find more examples in the [documentation](https://huggingface.co/docs/optimum/intel/inference) and in the [examples](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino)."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# ðŸ¤— Optimum\nðŸ¤— Optimum is an extension of [Transformers](https://huggingface.co/docs/transformers) that provides a set of performance optimization tools to train and run models on targeted hardware with maximum efficiency. The AI ecosystem evolves quickly, and more and more specialized hardware along with their own optimizations are emerging every day. As such, Optimum enables developers to efficiently use any of these platforms with the same ease inherent to Transformers. ðŸ¤— Optimum is distributed as a collection of packages - check out the links below for an in-depth look at each one. [Habana\n\nMaximize training throughput and efficiency with Habana's Gaudi processor](./habana/index)\n[Intel\n\nOptimize your model to speedup inference with OpenVINO and Neural Compressor](./intel/index)\n[AWS Trainium/Inferentia\n\nAccelerate your training and inference workflows with AWS Trainium and AWS Inferentia](https://huggingface.co/docs/optimum-neuron/index)\n[NVIDIA\n\nAccelerate inference with NVIDIA TensorRT-LLM on the NVIDIA platform](https://github.com/huggingface/optimum-nvidia)\n[AMD\n\nEnable performance optimizations for AMD Instinct GPUs and AMD Ryzen AI NPUs](./amd/index)\n[FuriosaAI\n\nFast and efficient inference on FuriosaAI WARBOY](./furiosa/index)\n[ONNX Runtime\n\nApply quantization and graph optimization to accelerate Transformers models training and inference with ONNX Runtime](./onnxruntime/overview)\n[BetterTransformer\n\nA one-liner integration to use PyTorch's BetterTransformer with Transformers models](./bettertransformer/overview)"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# OpenVINO\nðŸ¤— [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion pipelines compatible with OpenVINO to perform inference on a variety of Intel processors (see the [full list](https://docs.openvino.ai/latest/openvino\\_docs\\_OV\\_UG\\_supported\\_plugins\\_Supported\\_Devices.html) of supported devices). You'll need to install ðŸ¤— Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\n```bash\npip install --upgrade-strategy eager optimum[\""openvino\""]\n```\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO. ## Stable Diffusion\nTo load and run inference, use the [`~optimum.intel.OVStableDiffusionPipeline`]. If you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, set `export=True`:\n```python\nfrom optimum.intel import OVStableDiffusionPipeline\nmodel\\_id = \""runwayml/stable-diffusion-v1-5\""\npipeline = OVStableDiffusionPipeline.from\\_pretrained(model\\_id, export=True)\nprompt = \""sailing ship in storm by Rembrandt\""\nimage = pipeline(prompt).images[0]\n# Don't forget to save the exported model\npipeline.save\\_pretrained(\""openvino-sd-v1-5\"")\n```\nTo further speed-up inference, statically reshape the model. If you change any parameters such as the outputs height or width, youâ€™ll need to statically reshape your model again. ```python\n# Define the shapes related to the inputs and desired outputs\nbatch\\_size, num\\_images, height, width = 1, 1, 512, 512\n# Statically reshape the model\npipeline.reshape(batch\\_size, height, width, num\\_images)\n# Compile the model before inference\npipeline.compile()\nimage = pipeline(\nprompt,\nheight=height,\nwidth=width,\nnum\\_images\\_per\\_prompt=num\\_images,\n).images[0]\n```\n\n![](https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/stable_diffusion_v1_5_sail_boat_rembrandt.png)\n\nYou can find more examples in the ðŸ¤— Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion), and Stable Diffusion is supported for text-to-image, image-to-image, and inpainting. ## Stable Diffusion XL\nTo load and run inference with SDXL, use the [`~optimum.intel.OVStableDiffusionXLPipeline`]:\n```python\nfrom optimum.intel import OVStableDiffusionXLPipeline\nmodel\\_id = \""stabilityai/stable-diffusion-xl-base-1.0\""\npipeline = OVStableDiffusionXLPipeline.from\\_pretrained(model\\_id)\nprompt = \""sailing ship in storm by Rembrandt\""\nimage = pipeline(prompt).images[0]\n```\nTo further speed-up inference, [statically reshape](#stable-diffusion) the model as shown in the Stable Diffusion section."", ""- \\*\\*No cache:\\*\\* Currently when loading an optimized model (\\*.onnx), it will not be cached locally. ## 5. Optimum Inference FAQ\n\\*\\*Which tasks are supported?\\*\\*\nYou can find a list of all supported tasks in the [documentation](https://huggingface.co/docs/optimum/main/en/pipelines). Currently support pipelines tasks are `feature-extraction`, `text-classification`, `token-classification`, `question-answering`, `zero-shot-classification`, `text-generation`\n\\*\\*Which models are supported?\\*\\*\nAny model that can be exported with [transformers.onnx](https://huggingface.co/docs/transformers/serialization) and has a supported task can be used, this includes among others BERT, ALBERT, GPT2, RoBERTa, XLM-RoBERTa, DistilBERT .... \\*\\*Which runtimes are supported?\\*\\*\nCurrently, ONNX Runtime is supported. We are working on adding more in the future. [Let us know](https://discuss.huggingface.co/c/optimum/59) if you are interested in a specific runtime. \\*\\*How can I use Optimum with Transformers?\\*\\*\nYou can find an example and instructions in our [documentation](https://huggingface.co/docs/optimum/main/en/pipelines#transformers-pipeline-usage). \\*\\*How can I use GPUs?\\*\\*\nTo be able to use GPUs you simply need to install `optimum[onnxruntine-gpu]` which will install the required GPU providers and use them by default. \\*\\*How can I use a quantized and optimized model with pipelines?\\*\\*\nYou can load the optimized or quantized model using the new [ORTModelForXXX](https://huggingface.co/docs/optimum/main/en/onnxruntime/modeling\\_ort) classes using the [from\\_pretrained](https://huggingface.co/docs/optimum/main/en/onnxruntime/modeling\\_ort#optimum.onnxruntime.ORTModelForQuestionAnswering.forward.example) method."", ""The Optimizer used for the baseline PyTorch runs is the AdamW optimizer and the ORT Training runs use the Fused Adam Optimizer. The runs were performed on a single Nvidia A100 node with 8 GPUs. ![Optimum-onnxruntime Training Benchmark](assets/optimum_onnxruntime-training/onnxruntime-training-benchmark.png)\nAdditional details on configuration settings to turn on Optimum for training acceleration can be found [here](https://huggingface.co/docs/optimum/onnxruntime/usage\\_guides/trainer). The version information used for these runs is as follows:\n```\nPyTorch: 1.14.0.dev20221103+cu116; ORT: 1.14.0.dev20221103001+cu116; DeepSpeed: 0.6.6; HuggingFace: 4.24.0.dev0; Optimum: 1.4.1.dev0; Cuda: 11.6.2\n```\n## Optimum Library\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive. To target these needs, Hugging Face built two open-sourced libraries: \\_\\_Accelerate\\_\\_ and \\_\\_Optimum\\_\\_. While [ðŸ¤— Accelerate](https://huggingface.co/docs/accelerate/index) focuses on out-of-the-box distributed training, [ðŸ¤— Optimum](https://huggingface.co/docs/optimum/index), as an extension of transformers, accelerates model training and inference by leveraging the maximum efficiency of usersâ€™ targeted hardware. Optimum integrated machine learning accelerators like ONNX Runtime and specialized hardware like [Intel's Habana Gaudi](https://huggingface.co/blog/habana-gaudi-2-benchmark), so users can benefit from considerable speedup in both training and inference. Besides, Optimum seamlessly integrates other Hugging Faceâ€™s tools while inheriting the same ease of use as Transformers. Developers can easily adapt their work to achieve lower latency with less computing power. ## ONNX Runtime Training\n[ONNX Runtime](https://onnxruntime.ai/) accelerates [large model training](https://onnxruntime.ai/docs/get-started/training-pytorch.html) to speed up throughput by up to 40% standalone, and 130% when composed with [DeepSpeed](https://www.deepspeed.ai/tutorials/zero/) for popular HuggingFace transformer based models. ONNX Runtime is already integrated as part of Optimum and enables faster training through Hugging Faceâ€™s Optimum training framework. ONNX Runtime Training achieves such throughput improvements via several memory and compute optimizations. The memory optimizations enable ONNX Runtime to maximize the batch size and utilize the available memory efficiently whereas the compute optimizations speed up the training time. These optimizations include, but are not limited to, efficient memory planning, kernel optimizations, multi tensor apply for Adam Optimizer (which batches the elementwise updates applied to all the modelâ€™s parameters into one or a few kernel launches), FP16 optimizer (which eliminates a lot of device to host memory copies), mixed precision training and graph optimizations like node fusions and node eliminations. ONNX Runtime Training supports both [NVIDIA](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/accelerate-pytorch-transformer-model-training-with-onnx-runtime/ba-p/2540471) and [AMD GPUs](https://cloudblogs.microsoft.com/opensource/2021/07/13/onnx-runtime-release-1-8-1-previews-support-for-accelerated-training-on-amd-gpus-with-the-amd-rocm-open-software-platform/), and offers extensibility with custom operators. In short, it empowers AI developers to take full advantage of the ecosystem they are familiar with, like PyTorch and Hugging Face, and use acceleration from ONNX Runtime on the target device of their choice to save both time and resources. ## ONNX Runtime Training in Optimum\nOptimum provides an `ORTTrainer` API that extends the `Trainer` in Transformers to use ONNX Runtime as the backend for acceleration. `ORTTrainer` is an easy-to-use API containing feature-complete training loop and evaluation loop. It supports features like hyperparameter search, mixed-precision training and distributed training with multiple GPUs. `ORTTrainer` enables AI developers to compose ONNX Runtime and other third-party acceleration techniques when training Transformersâ€™ models, which helps accelerate the training further and gets the best out of the hardware. For example, developers can combine ONNX Runtime Training with distributed data parallel and mixed-precision training integrated in Transformersâ€™ Trainer. Besides, `ORTTrainer` makes it easy to compose ONNX Runtime Training with DeepSpeed ZeRO-1, which saves memory by partitioning the optimizer states. After the pre-training or the fine-tuning is done, developers can either save the trained PyTorch model or convert it to the ONNX format with APIs that Optimum implemented for ONNX Runtime to ease the deployment for Inference. And just like `Trainer`, `ORTTrainer` has full integration with Hugging Face Hub: after the training, users can upload their model checkpoints to their Hugging Face Hub account. So concretely, what should users do with Optimum to take advantage of the ONNX Runtime acceleration for training?"", ""Optimum can be used for accelerated training, quantization, graph optimization, and now inference as well with support for [transformers pipelines](https://huggingface.co/docs/transformers/main/en/main\\_classes/pipelines#pipelines). ## 2. New Optimum inference and pipeline features\nWith [release](https://github.com/huggingface/optimum/releases/tag/v1.2.0) of Optimum 1.2, we are adding support for [inference](https://huggingface.co/docs/optimum/main/en/onnxruntime/modeling\\_ort) and [transformers pipelines](https://huggingface.co/docs/transformers/main/en/main\\_classes/pipelines#pipelines). This allows Optimum users to leverage the same API they are used to from transformers with the power of accelerated runtimes, like [ONNX Runtime](https://onnxruntime.ai/). \\*\\*Switching from Transformers to Optimum Inference\\*\\*\nThe [Optimum Inference models](https://huggingface.co/docs/optimum/main/en/onnxruntime/modeling\\_ort) are API compatible with Hugging Face Transformers models. This means you can just replace your `AutoModelForXxx` class with the corresponding `ORTModelForXxx` class in Optimum. For example, this is how you can use a question answering model in Optimum:\n```diff\nfrom transformers import AutoTokenizer, pipeline\n-from transformers import AutoModelForQuestionAnswering\n+from optimum.onnxruntime import ORTModelForQuestionAnswering\n-model = AutoModelForQuestionAnswering.from\\_pretrained(\""deepset/roberta-base-squad2\"") # pytorch checkpoint\n+model = ORTModelForQuestionAnswering.from\\_pretrained(\""optimum/roberta-base-squad2\"") # onnx checkpoint\ntokenizer = AutoTokenizer.from\\_pretrained(\""deepset/roberta-base-squad2\"")\noptimum\\_qa = pipeline(\""question-answering\"", model=model, tokenizer=tokenizer)\nquestion = \""What's my name?\""\ncontext = \""My name is Philipp and I live in Nuremberg.\""\npred = optimum\\_qa(question, context)\n```\nIn the first release, we added [support for ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/modeling\\_ort) but there is more to come! These new `ORTModelForXX` can now be used with the [transformers pipelines](https://huggingface.co/docs/transformers/main/en/main\\_classes/pipelines#pipelines).""]","To install the latest version of Optimum with OpenVINO support, you can use the following command:

```bash
pip install --upgrade-strategy eager optimum[openvino]
```","pip install --upgrade-strategy eager optimum[""openvino""]"

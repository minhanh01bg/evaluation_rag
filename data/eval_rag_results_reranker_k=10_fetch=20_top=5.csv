user_input,retrieved_contexts,response,reference
Who proposed the hybrid Vision Transformer model?,"[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Hybrid Vision Transformer (ViT Hybrid)\n## Overview\nThe hybrid Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\nat Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, Neil Houlsby. It's the first paper that successfully trains a Transformer encoder on ImageNet, attaining\nvery good results compared to familiar convolutional architectures. ViT hybrid is a slight variant of the [plain Vision Transformer](vit),\nby leveraging a convolutional backbone (specifically, [BiT](bit)) whose features are used as initial \""tokens\"" for the Transformer."", ""With so many Transformer variants available, it can be easy to miss the bigger picture. What all these models have in common is they're based on the original Transformer architecture. Some models only use the encoder or decoder, while others use both. This provides a useful taxonomy to categorize and examine the high-level differences within models in the Transformer family, and it'll help you understand Transformers you haven't encountered before. If you aren't familiar with the original Transformer model or need a refresher, check out the [How do Transformers work](https://huggingface.co/course/chapter1/4?fw=pt) chapter from the Hugging Face course.## Computer vision\n### Convolutional network\nFor a long time, convolutional networks (CNNs) were the dominant paradigm for computer vision tasks until the [Vision Transformer](https://arxiv.org/abs/2010.11929) demonstrated its scalability and efficiency. Even then, some of a CNN's best qualities, like translation invariance, are so powerful (especially for certain tasks) that some Transformers incorporate convolutions in their architecture. [ConvNeXt](model\\_doc/convnext) flipped this exchange around and incorporated design choices from Transformers to modernize a CNN. For example, ConvNeXt uses non-overlapping sliding windows to patchify an image and a larger kernel to increase its global receptive field. ConvNeXt also makes several layer design choices to be more memory-efficient and improve performance, so it competes favorably with Transformers! ### Encoder[[cv-encoder]]\nThe [Vision Transformer (ViT)](model\\_doc/vit) opened the door to computer vision tasks without convolutions. ViT uses a standard Transformer encoder, but its main breakthrough was how it treated an image. It splits an image into fixed-size patches and uses them to create an embedding, just like how a sentence is split into tokens. ViT capitalized on the Transformers' efficient architecture to demonstrate competitive results with the CNNs at the time while requiring fewer resources to train. ViT was soon followed by other vision models that could also handle dense vision tasks like segmentation as well as detection. One of these models is the [Swin](model\\_doc/swin) Transformer. It builds hierarchical feature maps (like a CNN 👀 and unlike ViT) from smaller-sized patches and merges them with neighboring patches in deeper layers. Attention is only computed within a local window, and the window is shifted between attention layers to create connections to help the model learn better. Since the Swin Transformer can produce hierarchical feature maps, it is a good candidate for dense prediction tasks like segmentation and detection. The [SegFormer](model\\_doc/segformer) also uses a Transformer encoder to build hierarchical feature maps, but it adds a simple multilayer perceptron (MLP) decoder on top to combine all the feature maps and make a prediction. Other vision models, like BeIT and ViTMAE, drew inspiration from BERT's pretraining objective. [BeIT](model\\_doc/beit) is pretrained by \\*masked image modeling (MIM)\\*; the image patches are randomly masked, and the image is also tokenized into visual tokens. BeIT is trained to predict the visual tokens corresponding to the masked patches. [ViTMAE](model\\_doc/vitmae) has a similar pretraining objective, except it must predict the pixels instead of visual tokens."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Video Vision Transformer (ViViT)\n## Overview\nThe Vivit model was proposed in [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691) by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid. The paper proposes one of the first successful pure-transformer based set of models for video understanding. The abstract from the paper is the following:\n\\*We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks.\\*\nThis model was contributed by [jegormeister](https://huggingface.co/jegormeister). The original code (written in JAX) can be found [here](https://github.com/google-research/scenic/tree/main/scenic/projects/vivit). ## VivitConfig\n[[autodoc]] VivitConfig\n## VivitImageProcessor\n[[autodoc]] VivitImageProcessor\n- preprocess\n## VivitModel\n[[autodoc]] VivitModel\n- forward\n## VivitForVideoClassification\n[[autodoc]] transformers.VivitForVideoClassification\n- forward"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# TVLT\n## Overview\nThe TVLT model was proposed in [TVLT: Textless Vision-Language Transformer](https://arxiv.org/abs/2209.14156)\nby Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal (the first three authors contributed equally). The Textless Vision-Language Transformer (TVLT) is a model that uses raw visual and audio inputs for vision-and-language representation learning, without using text-specific modules such as tokenization or automatic speech recognition (ASR). It can perform various audiovisual and vision-language tasks like retrieval, question answering, etc. The abstract from the paper is the following:\n\\*In this work, we present the Textless Vision-Language Transformer (TVLT), where homogeneous transformer blocks take raw visual and audio inputs for vision-and-language representation learning with minimal modality-specific design, and do not use text-specific modules such as tokenization or automatic speech recognition (ASR). TVLT is trained by reconstructing masked patches of continuous video frames and audio spectrograms (masked autoencoding) and contrastive modeling to align video and audio. TVLT attains performance comparable to its text-based counterpart on various multimodal tasks, such as visual question answering, image retrieval, video retrieval, and multimodal sentiment analysis, with 28x faster inference speed and only 1/3 of the parameters. Our findings suggest the possibility of learning compact and efficient visual-linguistic representations from low-level visual and audio signals without assuming the prior existence of text.\\*\n\n![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/tvlt_architecture.png)\n\n TVLT architecture."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Transformer2D\nA Transformer model for image-like data from [CompVis](https://huggingface.co/CompVis) that is based on the [Vision Transformer](https://huggingface.co/papers/2010.11929) introduced by Dosovitskiy et al. The [`Transformer2DModel`] accepts discrete (classes of vector embeddings) or continuous (actual embeddings) inputs. When the input is \\*\\*continuous\\*\\*:\n1. Project the input and reshape it to `(batch\\_size, sequence\\_length, feature\\_dimension)`. 2. Apply the Transformer blocks in the standard way. 3. Reshape to image. When the input is \\*\\*discrete\\*\\*:\nIt is assumed one of the input classes is the masked latent pixel. The predicted classes of the unnoised image don't contain a prediction for the masked pixel because the unnoised image cannot be masked.""]","The hybrid Vision Transformer (ViT) model was proposed by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.","Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby."
Who are the authors of the Longformer paper?,"[""Peters, Arman Cohan. The abstract from the paper is the following:\n\\*Transformer-based models are unable to process long sequences due to their self-attention operation, which scales\nquadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or\nlonger. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we\nevaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In\ncontrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our\npretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on\nWikiHop and TriviaQA.\\*\nThis model was contributed by [beltagy](https://huggingface.co/beltagy)."", ""The Authors' code can be found [here](https://github.com/allenai/longformer). ## Usage tips\n- Since the Longformer is based on RoBERTa, it doesn't have `token\\_type\\_ids`. You don't need to indicate which\ntoken belongs to which segment. Just separate your segments with the separation token `tokenizer.sep\\_token` (or\n``). - A transformer model replacing the attention matrices by sparse matrices to go faster. Often, the local context (e.g., what are the two tokens left and right?) is enough to take action for a given token. Some preselected input tokens are still given global attention, but the attention matrix has way less parameters, resulting in a speed-up. See the local attention section for more information. ## Longformer Self Attention\nLongformer self attention employs self attention on both a \""local\"" context and a \""global\"" context. Most tokens only\nattend \""locally\"" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\n`config.attention\\_window`. Note that `config.attention\\_window` can be of type `List` to define a\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \""globally\"" to all other tokens, as it is\nconventionally done for all tokens in `BertSelfAttention`. Note that \""locally\"" and \""globally\"" attending tokens are projected by different query, key and value matrices. Also note\nthat every \""locally\"" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \""globally\""\nattending tokens so that global attention is \\*symmetric\\*. The user can define which tokens attend \""locally\"" and which tokens attend \""globally\"" by setting the tensor\n`global\\_attention\\_mask` at run-time appropriately. All Longformer models employ the following logic for\n`global\\_attention\\_mask`:\n- 0: the token attends \""locally\"",\n- 1: the token attends \""globally\"". For more information please also refer to [`~LongformerModel.forward`] method. Using Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n\\_s \\times n\\_s)\\\\) to\n\\\\(\\mathcal{O}(n\\_s \\times w)\\\\), with \\\\(n\\_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize."", ""The question of how these inductive biases compare to each other for varying quantities of training data remains. All these works highlight the importance of long-range inputs modeling in natural language. In the industry, it is common to encounter use-cases such as document translation, document classification or document summarization which require modeling very long sequences in an efficient and robust way. Recently, zero-shot examples priming (a la GPT3) has also emerged as a promising alternative to standard fine-tuning, and increasing the number of priming examples (and thus the context size) steadily increases the performance and robustness. Finally, it is common in other modalities such as speech or protein modeling to encounter long sequences beyond the standard 512 time steps. Modeling long inputs is not antithetical to modeling short inputs but instead should be thought from the perspective of a continuum from shorter to longer sequences. [Shortformer](https://arxiv.org/abs/2012.15832), Longformer and BERT provide evidence that training the model on short sequences and gradually increasing sequence lengths lead to an accelerated training and stronger downstream performance. This observation is coherent with the intuition that the long-range dependencies acquired when little data is available can rely on spurious correlations instead of robust language understanding. This echoes some experiments Teven Le Scao has run on language modeling: LSTMs are stronger learners in the low data regime compared to transformers and give better perplexities on small-scale language modeling benchmarks such as Penn Treebank. From a practical point of view, the question of positional embeddings is also a crucial methodological aspect with computational efficiency trade-offs. Relative positional embeddings (introduced in Transformer-XL and used in Compressive Transformers) are appealing because they can easily be extended to yet-unseen sequence lengths, but at the same time, relative positional embeddings are computationally expensive. On the other side, absolute positional embeddings (used in Longformer and Linformer) are less flexible for sequences longer than the ones seen during training, but are computationally more efficient. Interestingly, [Shortformer](https://arxiv.org/abs/2012.15832) introduces a simple alternative by adding the positional information to the queries and keys of the self-attention mechanism instead of adding it to the token embeddings. The method is called position-infused attention and is shown to be very efficient while producing strong results. ## @Hugging Face 🤗: Long-range modeling\nThe Longformer implementation and the associated open-source checkpoints are available through the Transformers library and the [model hub](https://huggingface.co/models?search=longformer). Performer and Big Bird, which is a long-range model based on sparse attention, are currently in the works as part of our [call for models](https://twitter.com/huggingface/status/1359903233976762368), an effort involving the community in order to promote open-source contributions. We would be pumped to hear from you if you’ve wondered how to contribute to `transformers` but did not know where to start! For further reading, we recommend checking Patrick Platen’s blog on [Reformer](https://arxiv.org/abs/2001.04451), Teven Le Scao’s post on [Johnson-Lindenstrauss approximation](https://tevenlescao.github.io/blog/fastpages/jupyter/2020/06/18/JL-Lemma-+-Linformer.html), [Efficient Transfomers: A Survey](https://arxiv.org/abs/2009.06732), and [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/abs/2011.04006)."", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# LED\n## Overview\nThe LED model was proposed in [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz\nBeltagy, Matthew E. Peters, Arman Cohan. The abstract from the paper is the following:\n\\*Transformer-based models are unable to process long sequences due to their self-attention operation, which scales\nquadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or\nlonger. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we\nevaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8."", ""\\* The FAVOR+ algorithm is the combination of multiple components. It is not clear which of these components have the most empirical impact on the performance, especially in view of the variety of modalities considered in this work. ## Reading group discussion\nThe developments in pre-trained transformer-based language models for natural language understanding and generation are impressive. Making these systems efficient for production purposes has become a very active research area. This emphasizes that we still have much to learn and build both on the methodological and practical sides to enable efficient and general deep learning based systems, in particular for applications that require modeling long-range inputs. The four papers above offer different ways to deal with the quadratic memory complexity of the self-attention mechanism, usually by reducing it to linear complexity. Linformer and Longformer both rely on the observation that the self-attention matrix does not contain \\\\(n × n\\\\) worth of information (the attention matrix is low-rank and sparse). Performer gives a principled method to approximate the softmax-attention kernel (and any kernelizable attention mechanisms beyond softmax). Compressive Transformer offers an orthogonal approach to model long range dependencies based on recurrence. These different inductive biases have implications in terms of computational speed and generalization beyond the training setup. In particular, Linformer and Longformer lead to different trade-offs: Longformer explicitly designs the sparse attention patterns of the self-attention (fixed patterns) while Linformer learns the low-rank matrix factorization of the self-attention matrix. In our experiments, Longformer is less efficient than Linformer, and is currently highly dependent on implementation details. On the other hand, Linformer’s decomposition only works for fixed context length (fixed at training) and cannot generalize to longer sequences without specific adaptation. Moreover, it cannot cache previous activations which can be extremely useful in the generative setup. Interestingly, Performer is conceptually different: it learns to approximate the softmax attention kernel without relying on any sparsity or low-rank assumption.""]","The authors of the Longformer paper are Iz Beltagy, Matthew E. Peters, and Arman Cohan.","Iz Beltagy, Matthew E. Peters, Arman Cohan"
How many layers does the distilbert-base-uncased model have?,"[""The model has 6 layers, 768 dimension and 12 heads, totalizing 66M parameters. - `distilbert-base-uncased-distilled-squad`: A finetuned version of `distilbert-base-uncased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 86.9 on the dev set (for comparison, Bert `bert-base-uncased` version reaches a 88.5 F1 score). - `distilbert-base-cased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-cased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 65M parameters. - `distilbert-base-cased-distilled-squad`: A finetuned version of `distilbert-base-cased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 87.1 on the dev set (for comparison, Bert `bert-base-cased` version reaches a 88.7 F1 score). - `distilbert-base-german-cased`: DistilBERT German language model pretrained on 1/2 of the data used to pretrain Bert using distillation with the supervision of the `bert-base-german-dbmdz-cased` version of German DBMDZ Bert. For NER tasks the model reaches a F1 score of 83.49 on the CoNLL-2003 test set (for comparison, `bert-base-german-dbmdz-cased` reaches a 84.52 F1 score), and a F1 score of 85.23 on the GermEval 2014 test set (`bert-base-german-dbmdz-cased` reaches a 86.89 F1 score). - `distilgpt2`: DistilGPT2 English language model pretrained with the supervision of `gpt2` (the smallest version of GPT2) on [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), a reproduction of OpenAI's WebText dataset. The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 124M parameters for GPT2). On average, DistilGPT2 is two times faster than GPT2. - `distilroberta-base`: DistilRoBERTa English language model pretrained with the supervision of `roberta-base` solely on [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), a reproduction of OpenAI's WebText dataset (it is ~4 times less training data than the teacher RoBERTa). The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base). On average DistilRoBERTa is twice as fast as Roberta-base. - `distilbert-base-multilingual-cased`: DistilmBERT multilingual model pretrained with the supervision of `bert-base-multilingual-cased` on the concatenation of Wikipedia in 104 different languages. The model supports the 104 languages listed [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages). The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters (compared to 177M parameters for mBERT-base). On average DistilmBERT is twice as fast as mBERT-base. Using DistilBERT is very similar to using BERT. DistilBERT share the same tokenizer as BERT's `bert-base-uncased` even though we provide a link to this tokenizer under the `DistilBertTokenizer` name to have a consistent naming between the library models. ```python\ntokenizer = DistilBertTokenizer.from\\_pretrained('distilbert-base-cased')\nmodel = DistilBertModel.from\\_pretrained('distilbert-base-cased')\ninput\\_ids = torch.tensor(tokenizer.encode(\""Hello, my dog is cute\"")).unsqueeze(0)\noutputs = model(input\\_ids)\nlast\\_hidden\\_states = outputs[0] # The last hidden-state is the first element of the output tuple\n```\nSimilarly, using the other Distil\\* models simply consists in calling the base classes with a different pretrained checkpoint:\n- DistilBERT uncased: `model = DistilBertModel.from\\_pretrained('distilbert-base-uncased')`\n- DistilGPT2: `model = GPT2Model.from\\_pretrained('distilgpt2')`\n- DistilRoBERTa: `model = RobertaModel.from\\_pretrained('distilroberta-base')`\n- DistilmBERT: `model = DistilBertModel.from\\_pretrained('distilbert-base-multilingual-cased')`\n## How to train Distil\\*\nIn the following, we will explain how you can train DistilBERT."", ""This is\nalso a seeting where we only have `K=4` possible labels. The higher the number of classes for a given task, the more\ndrastic the speedup will be, since the zero-shot teacher's complexity scales linearly with the number of classes. Since we secretly have access to ground truth labels for AG's news, we can evaluate the accuracy of each model. The\noriginal zero-shot model `roberta-large-mnli` gets an accuracy of 69.3% on the held-out test set. After training a\nstudent on the unlabeled training set, the distilled model gets a similar score of 70.4%. Lastly, you can share the distilled model with the community and/or use it with our inference API by [uploading it\nto the 🤗 Hub](https://huggingface.co/transformers/model\\_sharing.html). We've uploaded the distilled model from this\nexample at\n[joeddav/distilbert-base-uncased-agnews-student](https://huggingface.co/joeddav/distilbert-base-uncased-agnews-student)."", ""The 6.55 and 4.50 versions are close to the 16-bit version in this aspect. - The same seed produces an equivalent composition, but will not preserve the same details. Dog breeds may be different, for example. - Adherence to the prompt may degrade as we increase compression. In this example, the aggressive 3.41 version loses the board. PSNR only compares how much pixels differ overall, but does not care about the subjects in the images. You need to examine results and assess them for your use case. This technique is great for Stable Diffusion XL because we can keep about the same UNet size even though the number of parameters tripled with respect to the previous version. But it's not exclusive to it! You can apply the method to any Stable Diffusion Core ML model. ## How are Mixed-Bit Recipes Created? The following plot shows the signal strength (PSNR in dB) versus model size reduction (% of float16 size) for `stabilityai/stable-diffusion-xl-base-1.0`. The `{1,2,4,6,8}`-bit curves are generated by progressively palettizing more layers using a palette with a fixed number of bits. The layers were ordered in ascending order of their isolated impact to end-to-end signal strength, so the cumulative compression's impact is delayed as much as possible. The mixed-bit curve is based on falling back to a higher number of bits as soon as a layer's isolated impact to end-to-end signal integrity drops below a threshold. Note that all curves based on palettization outperform linear 8-bit quantization at the same model size except for 1-bit. ![PSNR-vs-size-reduction-plot](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/stable-diffusion-xl-coreml/stabilityai\\_stable-diffusion-xl-base-1.0\\_psnr\\_vs\\_size.png)\nMixed-bit palettization runs in two phases: \\_analysis\\_ and \\_application\\_. The goal of the analysis phase is to find points in the mixed-bit curve (the brown one above all the others in the figure) so we can choose our desired quality-vs-size tradeoff. As mentioned in the previous section, we iterate through the layers and select the lowest bit depths that yield results above a given PSNR threshold. We repeat the process for various thresholds to get different quantization strategies. The result of the process is thus a set of quantization recipes, where each recipe is just a JSON dictionary detailing the number of bits to use for each layer in the model."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# DeepFloyd IF\n## Overview\nDeepFloyd IF is a novel state-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding. The model is a modular composed of a frozen text encoder and three cascaded pixel diffusion modules:\n- Stage 1: a base model that generates 64x64 px image based on text prompt,\n- Stage 2: a 64x64 px => 256x256 px super-resolution model, and\n- Stage 3: a 256x256 px => 1024x1024 px super-resolution model\nStage 1 and Stage 2 utilize a frozen text encoder based on the T5 transformer to extract text embeddings, which are then fed into a UNet architecture enhanced with cross-attention and attention pooling. Stage 3 is [Stability AI's x4 Upscaling model](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler). The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID score of 6.66 on the COCO dataset. Our work underscores the potential of larger UNet architectures in the first stage of cascaded diffusion models and depicts a promising future for text-to-image synthesis."", ""at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless\""\n```\nAs expected from our `group\\_texts()` function above, this looks identical to the decoded `input\\_ids` -- but then how can our model possibly learn anything? We're missing a key step: inserting `[MASK]` tokens at random positions in the inputs! Let's see how we can do this on the fly during fine-tuning using a special data collator. ## Fine-tuning DistilBERT with the `Trainer` API[[fine-tuning-distilbert-with-the-trainer-api]]\nFine-tuning a masked language model is almost identical to fine-tuning a sequence classification model, like we did in [Chapter 3](/course/chapter3).""]",The `distilbert-base-uncased` model has 6 layers.,6
What is the style of TPU access when using Google Colab?,"[""### What kinds of TPU are available? New users are often very confused by the range of TPUs, and the different ways to access them. The first key distinction to understand is the difference between \\*\\*TPU Nodes\\*\\* and \\*\\*TPU VMs.\\*\\*\nWhen you use a \\*\\*TPU Node\\*\\*, you are effectively indirectly accessing a remote TPU. You will need a separate VM, which will initialize your network and data pipeline and then forward them to the remote node. When you use a TPU on Google Colab, you are accessing it in the \\*\\*TPU Node\\*\\* style. Using TPU Nodes can have some quite unexpected behaviour for people who aren’t used to them! In particular, because the TPU is located on a physically different system to the machine you’re running your Python code on, your data cannot be local to your machine - any data pipeline that loads from your machine’s internal storage will totally fail! Instead, data must be stored in Google Cloud Storage where your data pipeline can still access it, even when the pipeline is running on the remote TPU node. If you can fit all your data in memory as `np.ndarray` or `tf.Tensor`, then you can `fit()` on that data even when using Colab or a TPU Node, without needing to upload it to Google Cloud Storage. \\*\\*🤗Specific Hugging Face Tip🤗:\\*\\* The methods `Dataset.to\\_tf\\_dataset()` and its higher-level wrapper `model.prepare\\_tf\\_dataset()` , which you will see throughout our TF code examples, will both fail on a TPU Node. The reason for this is that even though they create a `tf.data.Dataset` it is not a “pure” `tf.data` pipeline and uses `tf.numpy\\_function` or `Dataset.from\\_generator()` to stream data from the underlying HuggingFace `Dataset`. This HuggingFace `Dataset` is backed by data that is on a local disc and which the remote TPU Node will not be able to read. The second way to access a TPU is via a \\*\\*TPU VM.\\*\\* When using a TPU VM, you connect directly to the machine that the TPU is attached to, much like training on a GPU VM. TPU VMs are generally easier to work with, particularly when it comes to your data pipeline. All of the above warnings do not apply to TPU VMs! This is an opinionated document, so here’s our opinion: \\*\\*Avoid using TPU Node if possible.\\*\\* It is more confusing and more difficult to debug than TPU VMs. It is also likely to be unsupported in future - Google’s latest TPU, TPUv4, can only be accessed as a TPU VM, which suggests that TPU Nodes are increasingly going to become a “legacy” access method. However, we understand that the only free TPU access is on Colab and Kaggle Kernels, which uses TPU Node - so we’ll try to explain how to handle it if you have to! Check the [TPU example notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu\\_training-tf.ipynb) for code samples that explain this in more detail. ### What sizes of TPU are available? A single TPU (a v2-8/v3-8/v4-8) runs 8 replicas. TPUs exist in \\*\\*pods\\*\\* that can run hundreds or thousands of replicas simultaneously. When you use more than a single TPU but less than a whole pod (for example, a v3-32), your TPU fleet is referred to as a \\*\\*pod slice.\\*\\*\nWhen you access a free TPU via Colab, you generally get a single v2-8 TPU."", ""Here is the demo for running T5-11B. The T5-11B model checkpoint is in FP32 which uses 42GB of memory and does not fit on Google Colab. With our 8-bit modules it only uses 11GB and fits easily:\n[![Open In Colab: T5-11b demo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing)\nOr this demo for BLOOM-3B:\n[![Open In Colab: BLOOM-3b demo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/HuggingFace\\_int8\\_demo.ipynb)\n## Scope of improvements\nThis approach, in our opinion, greatly improves access to very large models. With no performance degradation, it enables users with less compute to access models that were previously inaccessible. We've found several areas for improvement that can be worked on in the future to make this method even better for large models! ### Faster inference speed for smaller models\nAs we have seen in the [the benchmarking section](#is-it-faster-than-native-models), we could improve the runtime speed for small model (<=6B parameters) by a factor of almost 2x. However, while the inference speed is robust for large models like BLOOM-176B there are still improvements to be had for small models."", ""We strongly recommend students \\*\\*to use Google Colab for the hands-on exercises instead of running them on their personal computers\\*\\*. By using Google Colab, \\*\\*you can focus on learning and experimenting without worrying about the technical aspects of setting up your environments\\*\\*. To validate this hands-on for the certification process, you need to push your trained model to the Hub and \\*\\*get a result of >= 200\\*\\*. To find your result, go to the leaderboard and find your model, \\*\\*the result = mean\\_reward - std of reward\\*\\*\nFor more information about the certification process, check this section 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process\n## Set the GPU 💪\n- To \\*\\*accelerate the agent's training, we'll use a GPU\\*\\*. To do that, go to `Runtime > Change Runtime type`\n![GPU Step 1](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg)\n- `Hardware Accelerator > GPU`\n![GPU Step 2](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg)\n# Install RL-Baselines3 Zoo and its dependencies 📚\nIf you see `ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.` \\*\\*this is normal and it's not a critical error\\*\\* there's a conflict of version."", ""Be careful to not remove punctuation marks that can change the meaning of the sentence. \\*E.g.\\* you should not remove the single quotation mark `'` in English, as it would change the words `\""it's\""` to `\""its\""` which is a different word and has thus a different meaning. For more tips on data processing see [\""How to effectively preprocess the data\""](#how-to-effectively-preprocess-the-data\"")\n- Hyperparameter Tuning. Depending on the size of the data you should probably change the hyperparameters of the google colab. You can change any parameter you like. For more tips and tricks see [\""How to do hyperparameter tuning for my language\""](#how-to-do-hyperparameter-tuning-for-my-language)\nWhen running the google colab make sure that you uncomment the cell corresponding to mounting your google drive to the colab. This cell looks as follows:\n```python\n# from google.colab import drive\n# drive.mount('/content/gdrive/')\n```\nUncomment it, run it, and follow the instructions to mount your google drive. This way you can be sure that the model parameters and created tokenizer & feature extractor files are saved in \\*\\*your\\*\\* google drive. Also, make sure that you uncomment the cells corresponding to save the preprocessing files and trained model weights to your drive. Otherwise, you might lose a trained model if you google crashes. You should change the name of your model from `wav2vec2-large-xlsr-turkish-demo` to `wav2vec2-large-xlsr-{your\\_favorite\\_name}`. Those cells correspond to:\n```python\n# processor.save\\_pretrained(\""/content/gdrive/MyDrive/wav2vec2-large-xlsr-turkish-demo\"")\n```\nand the line:\n```python\noutput\\_dir=\""/content/gdrive/MyDrive/wav2vec2-large-xlsr-turkish-demo\"",\n```\nfurther below (which should already be uncommented). Having finished the training you should find the following files/folders under the folder `wav2vec2-large-xlsr-{your\\_favorite\\_name}` in your google drive:\n- `preprocessor\\_config.json` - the parameters of the feature extractor\n- `special\\_tokens\\_map.json` - the special token map of the tokenizer\n- `tokenizer\\_config.json` - the parameters of the tokenizer\n- `vocab.json` - the vocabulary of the tokenizer\n- `checkpoint-{...}/` - the saved checkpoints saved during training."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# JAX/Flax\n[[open-in-colab]]\n🤗 Diffusers supports Flax for super fast inference on Google TPUs, such as those available in Colab, Kaggle or Google Cloud Platform. This guide shows you how to run inference with Stable Diffusion using JAX/Flax. Before you begin, make sure you have the necessary libraries installed:\n```py\n# uncomment to install the necessary libraries in Colab\n#!pip install -q jax==0.3.25 jaxlib==0.3.25 flax transformers ftfy\n#!pip install -q diffusers\n```\nYou should also make sure you're using a TPU backend. While JAX does not run exclusively on TPUs, you'll get the best performance on a TPU because each server has 8 TPU accelerators working in parallel. If you are running this guide in Colab, select \\*Runtime\\* in the menu above, select the option \\*Change runtime type\\*, and then select \\*TPU\\* under the \\*Hardware accelerator\\* setting. Import JAX and quickly check whether you're using a TPU:\n```python\nimport jax\nimport jax.tools.colab\\_tpu\njax.tools.colab\\_tpu.setup\\_tpu()\nnum\\_devices = jax.device\\_count()\ndevice\\_type = jax.devices()[0].device\\_kind\nprint(f\""Found {num\\_devices} JAX devices of type {device\\_type}.\"")\nassert (\n\""TPU\"" in device\\_type,\n\""Available device is not a TPU, please select TPU from Runtime > Change runtime type > Hardware accelerator\""\n)\n# Found 8 JAX devices of type Cloud TPU. ```\nGreat, now you can import the rest of the dependencies you'll need:\n```python\nimport jax.numpy as jnp\nfrom jax import pmap\nfrom flax.jax\\_utils import replicate\nfrom flax.training.common\\_utils import shard\nfrom diffusers import FlaxStableDiffusionPipeline\n```\n## Load a model\nFlax is a functional framework, so models are stateless and parameters are stored outside of them. Loading a pretrained Flax pipeline returns \\*both\\* the pipeline and the model weights (or parameters). In this guide, you'll use `bfloat16`, a more efficient half-float type that is supported by TPUs (you can also use `float32` for full precision if you want). ```python\ndtype = jnp.bfloat16\npipeline, params = FlaxStableDiffusionPipeline.from\\_pretrained(\n\""CompVis/stable-diffusion-v1-4\"",\nrevision=\""bf16\"",\ndtype=dtype,\n)\n```\n## Inference\nTPUs usually have 8 devices working in parallel, so let's use the same prompt for each device.""]","When using Google Colab, you are accessing a TPU in the **TPU Node** style. This means you are indirectly accessing a remote TPU, and you will need a separate VM to initialize your network and data pipeline, which then forwards them to the remote node.",TPU Node style.
What is the default label inferred by Gradio for the input parameter in the GUI?,"[""By default, the Gradio use UTF-8 encoding for scripts. \\*\\*For reload mode\\*\\*, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n1. Configure encoding declaration of python script, for example: `# -\\*- coding: cp1252 -\\*-`\n2. Confirm that your code editor has identified that encoding format. 3. Run like this: `gradio run.py --encoding cp1252`\n🔥 If your application accepts command line arguments, you can pass them in as well. Here's an example:\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# Greetings {args.name}!\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nWhich you could run like this: `gradio run.py --name Gretel`\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code."", ""his sentiment analaysis demo takes in input text and returns its classification for either positive, negative or neutral using Gradio's Label output."", ""Key Features\nLet's go through some of the key features of Gradio. This guide is intended to be a high-level overview of various things that you should be aware of as you build your demo. Where appropriate, we link to more detailed guides on specific topics. 1. [Components](#components)\n2. [Queuing](#queuing)\n3. [Streaming outputs](#streaming-outputs)\n4. [Streaming inputs](#streaming-inputs)\n5. [Alert modals](#alert-modals)\n6. [Styling](#styling)\n7. [Progress bars](#progress-bars)\n8. [Batch functions](#batch-functions)\n## Components\nGradio includes more than 30 pre-built components (as well as many user-built \\_custom components\\_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on. Each component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[\""webcam\"", \""upload\""]`). \\*\\*Static and Interactive Components\\*\\*\nEvery component has a \\_static\\_ version that is designed to \\*display\\* data, and most components also have an \\_interactive\\_ version designed to let users input or modify the data. Typically, you don't need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports. \\*\\*Preprocessing and Postprocessing\\*\\*\nWhen a component is used as an input, Gradio automatically handles the \\_preprocessing\\_ needed to convert the data from a type sent by the user's browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array). Similarly, when a component is used as an output, Gradio automatically handles the \\_postprocessing\\_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (a gallery of images). Consider an example demo with three input components (`gr.Textbox`, `gr.Number`, and `gr.Image`) and two outputs (`gr.Number` and `gr.Gallery`) that serve as a UI for your image-to-image generation model. Below is a diagram of what our preprocessing will send to the model and what our postprocessing will require from it. ![](https://github.com/gradio-app/gradio/blob/main/guides/assets/dataflow.svg?raw=true)\nIn this image, the following preprocessing steps happen to send the data from the browser to your function:\n\\* The text in the textbox is converted to a Python `str` (essentially no preprocessing)\n\\* The number in the number input in converted to a Python `float` (essentially no preprocessing)\n\\* Most importantly, ihe image supplied by the user is converted to a `numpy.array` representation of the RGB values in the image\nImages are converted to NumPy arrays because they are a common format for machine learning workflows. You can control the \\_preprocessing\\_ using the component's parameters when constructing the component. For example, if you instantiate the `Image` component with the following parameters, it will preprocess the image to the `PIL` format instead:\n```py\nimg = gr.Image(type=\""pil\"")\n```\nPostprocessing is even simpler! Gradio automatically recognizes the format of the returned data (e.g. does the user's function return a `numpy` array or a `str` filepath for the `gr.Image` component?) and postprocesses it appropriately into a format that can be displayed by the browser. So in the image above, the following postprocessing steps happen to send the data returned from a user's function to the browser:\n\\* The `float` is displayed as a number and displayed directly to the user\n\\* The list of string filepaths (`list[str]`) is interpreted as a list of image filepaths and displayed as a gallery in the browser\nTake a look at the [Docs](https://gradio.app/docs) to see all the parameters for each Gradio component."", ""imple image classification in Pytorch with Gradio's Image input and Label output."", ""he simplest possible Gradio demo.""]","The default label inferred by Gradio for the input parameter in the GUI is ""Input"".",name
What does the 'beta_start' parameter value mean in DDPMScheduler?,"[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# DDPMScheduler\n[Denoising Diffusion Probabilistic Models](https://huggingface.co/papers/2006.11239) (DDPM) by Jonathan Ho, Ajay Jain and Pieter Abbeel proposes a diffusion based model of the same name. In the context of the 🤗 Diffusers library, DDPM refers to the discrete denoising scheduler from the paper as well as the pipeline. The abstract from the paper is:\n\\*We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at [this https URL](https://github.com/hojonathanho/diffusion).\\*\n## DDPMScheduler\n[[autodoc]] DDPMScheduler\n## DDPMSchedulerOutput\n[[autodoc]] schedulers.scheduling\\_ddpm.DDPMSchedulerOutput"", ""This happens according to a \\*\\*variance schedule\\*\\*. The original DDPM authors employed a linear schedule:\n> We set the forward process variances to constants\nincreasing linearly from \\\\(\\beta\\_1 = 10^{−4}\\\\)\nto \\\\(\\beta\\_T = 0.02\\\\). However, it was shown in ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)) that better results can be achieved when employing a cosine schedule. Below, we define various schedules for the \\\\(T\\\\) timesteps (we'll choose one later on). ```python\ndef cosine\\_beta\\_schedule(timesteps, s=0.008):\n\""\""\""\ncosine schedule as proposed in https://arxiv.org/abs/2102.09672\n\""\""\""\nsteps = timesteps + 1\nx = torch.linspace(0, timesteps, steps)\nalphas\\_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) \\* torch.pi \\* 0.5) \\*\\* 2\nalphas\\_cumprod = alphas\\_cumprod / alphas\\_cumprod[0]\nbetas = 1 - (alphas\\_cumprod[1:] / alphas\\_cumprod[:-1])\nreturn torch.clip(betas, 0.0001, 0.9999)\ndef linear\\_beta\\_schedule(timesteps):\nbeta\\_start = 0.0001\nbeta\\_end = 0.02\nreturn torch.linspace(beta\\_start, beta\\_end, timesteps)\ndef quadratic\\_beta\\_schedule(timesteps):\nbeta\\_start = 0.0001\nbeta\\_end = 0.02\nreturn torch.linspace(beta\\_start\\*\\*0.5, beta\\_end\\*\\*0.5, timesteps) \\*\\* 2\ndef sigmoid\\_beta\\_schedule(timesteps):\nbeta\\_start = 0.0001\nbeta\\_end = 0.02\nbetas = torch.linspace(-6, 6, timesteps)\nreturn torch.sigmoid(betas) \\* (beta\\_end - beta\\_start) + beta\\_start\n```\nTo start with, let's use the linear schedule for \\\\(T=300\\\\) time steps and define the various variables from the \\\\(\\beta\\_t\\\\) which we will need, such as the cumulative product of the variances \\\\(\\bar{\\alpha}\\_t\\\\)."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# DDIMInverseScheduler\n`DDIMInverseScheduler` is the inverted scheduler from [Denoising Diffusion Implicit Models](https://huggingface.co/papers/2010.02502) (DDIM) by Jiaming Song, Chenlin Meng and Stefano Ermon. The implementation is mostly based on the DDIM inversion definition from [Null-text Inversion for Editing Real Images using Guided Diffusion Models](https://huggingface.co/papers/2211.09794). ## DDIMInverseScheduler\n[[autodoc]] DDIMInverseScheduler"", ""## Tips\nDynamic thresholding from [Imagen](https://huggingface.co/papers/2205.11487) is supported, and for pixel-space\ndiffusion models, you can set both `algorithm\\_type=\""dpmsolver++\""` and `thresholding=True` to use the dynamic\nthresholding. This thresholding method is unsuitable for latent-space diffusion models such as\nStable Diffusion. ## DPMSolverMultistepInverseScheduler\n[[autodoc]] DPMSolverMultistepInverseScheduler\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling\\_utils.SchedulerOutput"", ""It often really depends on the specific use case which scheduler to choose. A good approach is always to run multiple different\nschedulers to compare results. ## Changing the Scheduler in Flax\nIf you are a JAX/Flax user, you can also change the default pipeline scheduler. This is a complete example of how to run inference using the Flax Stable Diffusion pipeline and the super-fast [DPM-Solver++ scheduler](../api/schedulers/multistep\\_dpm\\_solver):\n```Python\nimport jax\nimport numpy as np\nfrom flax.jax\\_utils import replicate\nfrom flax.training.common\\_utils import shard\nfrom diffusers import FlaxStableDiffusionPipeline, FlaxDPMSolverMultistepScheduler\nmodel\\_id = \""runwayml/stable-diffusion-v1-5\""\nscheduler, scheduler\\_state = FlaxDPMSolverMultistepScheduler.from\\_pretrained(\nmodel\\_id,\nsubfolder=\""scheduler\""\n)\npipeline, params = FlaxStableDiffusionPipeline.from\\_pretrained(\nmodel\\_id,\nscheduler=scheduler,\nrevision=\""bf16\"",\ndtype=jax.numpy.bfloat16,\n)\nparams[\""scheduler\""] = scheduler\\_state\n# Generate 1 image per parallel device (8 on TPUv2-8 or TPUv3-8)\nprompt = \""a photo of an astronaut riding a horse on mars\""\nnum\\_samples = jax.device\\_count()\nprompt\\_ids = pipeline.prepare\\_inputs([prompt] \\* num\\_samples)\nprng\\_seed = jax.random.PRNGKey(0)\nnum\\_inference\\_steps = 25\n# shard inputs and rng\nparams = replicate(params)\nprng\\_seed = jax.random.split(prng\\_seed, jax.device\\_count())\nprompt\\_ids = shard(prompt\\_ids)\nimages = pipeline(prompt\\_ids, params, prng\\_seed, num\\_inference\\_steps, jit=True).images\nimages = pipeline.numpy\\_to\\_pil(np.asarray(images.reshape((num\\_samples,) + images.shape[-3:])))\n```\nThe following Flax schedulers are \\_not yet compatible\\_ with the Flax Stable Diffusion Pipeline:\n- `FlaxLMSDiscreteScheduler`\n- `FlaxDDPMScheduler`""]","In the context of the DDPMScheduler, the 'beta_start' parameter value refers to the initial variance used in the forward process of the diffusion model. Specifically, it is set to a small value, typically \(10^{-4}\), which represents the starting point of the variance schedule for the diffusion process. This value is crucial as it influences the noise added to the data during the diffusion process, affecting the quality of the generated images. The 'beta_start' is part of a linear schedule that increases the variance up to a specified 'beta_end' value, which in this case is \(0.02\).",0.0001
How do you register a custom Resnet model to the auto classes in Transformers?,"[""## Registering a model with custom code to the auto classes\nIf you are writing a library that extends 🤗 Transformers, you may want to extend the auto classes to include your own\nmodel. This is different from pushing the code to the Hub in the sense that users will need to import your library to\nget the custom models (contrarily to automatically downloading the model code from the Hub). As long as your config has a `model\\_type` attribute that is different from existing model types, and that your model\nclasses have the right `config\\_class` attributes, you can just add them to the auto classes like this:\n```py\nfrom transformers import AutoConfig, AutoModel, AutoModelForImageClassification\nAutoConfig.register(\""resnet\"", ResnetConfig)\nAutoModel.register(ResnetConfig, ResnetModel)\nAutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification)\n```\nNote that the first argument used when registering your custom config to [`AutoConfig`] needs to match the `model\\_type`\nof your custom config, and the first argument used when registering your custom models to any auto model class needs\nto match the `config\\_class` of those models."", ""You can check the result\nin this [model repo](https://huggingface.co/sgugger/custom-resnet50d). See the [sharing tutorial](model\\_sharing) for more information on the push to Hub method. ## Using a model with custom code\nYou can use any configuration, model or tokenizer with custom code files in its repository with the auto-classes and\nthe `from\\_pretrained` method. All files and code uploaded to the Hub are scanned for malware (refer to the [Hub security](https://huggingface.co/docs/hub/security#malware-scanning) documentation for more information), but you should still\nreview the model code and author to avoid executing malicious code on your machine. Set `trust\\_remote\\_code=True` to use\na model with custom code:\n```py\nfrom transformers import AutoModelForImageClassification\nmodel = AutoModelForImageClassification.from\\_pretrained(\""sgugger/custom-resnet50d\"", trust\\_remote\\_code=True)\n```\nIt is also strongly encouraged to pass a commit hash as a `revision` to make sure the author of the models did not\nupdate the code with some malicious new lines (unless you fully trust the authors of the models). ```py\ncommit\\_hash = \""ed94a7c6247d8aedce4647f00f20de6875b5b292\""\nmodel = AutoModelForImageClassification.from\\_pretrained(\n\""sgugger/custom-resnet50d\"", trust\\_remote\\_code=True, revision=commit\\_hash\n)\n```\nNote that when browsing the commit history of the model repo on the Hub, there is a button to easily copy the commit\nhash of any commit."", ""The line that sets the `config\\_class` is not mandatory, unless\nyou want to register your model with the auto classes (see last section). If your model is very similar to a model inside the library, you can re-use the same configuration as this model. You can have your model return anything you want, but returning a dictionary like we did for\n`ResnetModelForImageClassification`, with the loss included when labels are passed, will make your model directly\nusable inside the [`Trainer`] class. Using another output format is fine as long as you are planning on using your own\ntraining loop or another library for training. Now that we have our model class, let's create one:\n```py\nresnet50d = ResnetModelForImageClassification(resnet50d\\_config)\n```\nAgain, you can use any of the methods of [`PreTrainedModel`], like [`~PreTrainedModel.save\\_pretrained`] or\n[`~PreTrainedModel.push\\_to\\_hub`]. We will use the second in the next section, and see how to push the model weights\nwith the code of our model. But first, let's load some pretrained weights inside our model. In your own use case, you will probably be training your custom model on your own data. To go fast for this tutorial,\nwe will use the pretrained version of the resnet50d. Since our model is just a wrapper around it, it's going to be\neasy to transfer those weights:\n```py\nimport timm\npretrained\\_model = timm.create\\_model(\""resnet50d\"", pretrained=True)\nresnet50d.model.load\\_state\\_dict(pretrained\\_model.state\\_dict())\n```\nNow let's see how to make sure that when we do [`~PreTrainedModel.save\\_pretrained`] or [`~PreTrainedModel.push\\_to\\_hub`], the\ncode of the model is saved."", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Building custom models\nThe 🤗 Transformers library is designed to be easily extensible. Every model is fully coded in a given subfolder\nof the repository with no abstraction, so you can easily copy a modeling file and tweak it to your needs. If you are writing a brand new model, it might be easier to start from scratch. In this tutorial, we will show you\nhow to write a custom model and its configuration so it can be used inside Transformers, and how you can share it\nwith the community (with the code it relies on) so that anyone can use it, even if it's not present in the 🤗\nTransformers library. We'll see how to build upon transformers and extend the framework with your hooks and\ncustom code. We will illustrate all of this on a ResNet model, by wrapping the ResNet class of the\n[timm library](https://github.com/rwightman/pytorch-image-models) into a [`PreTrainedModel`]."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ResNet\n## Overview\nThe ResNet model was proposed in [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun. Our implementation follows the small changes made by [Nvidia](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet\\_50\\_v1\\_5\\_for\\_pytorch), we apply the `stride=2` for downsampling in bottleneck's `3x3` conv and not in the first `1x1`.""]","To register a custom ResNet model to the auto classes in Transformers, you can follow these steps:

1. Ensure that your config has a `model_type` attribute that is different from existing model types.
2. Make sure your model classes have the correct `config_class` attributes.
3. Use the following code to register your custom config and models:

```py
from transformers import AutoConfig, AutoModel, AutoModelForImageClassification

AutoConfig.register(""resnet"", ResnetConfig)
AutoModel.register(ResnetConfig, ResnetModel)
AutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification)
```

In this code:
- The first argument used when registering your custom config to `AutoConfig` needs to match the `model_type` of your custom config.
- The first argument used when registering your custom models to any auto model class needs to match the `config_class` of those models.","Use AutoConfig.register(""resnet"", ResnetConfig), AutoModel.register(ResnetConfig, ResnetModel), and AutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification) to register the custom Resnet model to the auto classes."
Who is the author of the paper 'Learning Transferable Visual Models From Natural Language Supervision'?,"[""The model would have separate or fused encoders for vision and language to achieve this understanding. But these inputs and outputs can take several forms. Below we give some examples:\n- Image retrieval from natural language text. - Phrase grounding, i.e., performing object detection from an input image and natural language phrase (example: A \\*\\*young person\\*\\* swings a \\*\\*bat\\*\\*). - Visual question answering, i.e., finding answers from an input image and a question in natural language. - Generate a caption for a given image. This can also take the form of conditional text generation, where you'd start with a natural language prompt and an image. - Detection of hate speech from social media content involving both images and text modalities. ## Learning Strategies\nA vision-language model typically consists of 3 key elements: an image encoder, a text encoder, and a strategy to fuse information from the two encoders. These key elements are tightly coupled together as the loss functions are designed around both the model architecture and the learning strategy. While vision-language model research is hardly a new research area, the design of such models has changed tremendously over the years. Whereas earlier research adopted hand-crafted image descriptors and pre-trained word vectors or the frequency-based TF-IDF features, the latest research predominantly adopts image and text encoders with [transformer](https://arxiv.org/abs/1706.03762) architectures to separately or jointly learn image and text features. These models are pre-trained with strategic pre-training objectives that enable various downstream tasks. In this section, we'll discuss some of the typical pre-training objectives and strategies for vision-language models that have been shown to perform well regarding their transfer performance. We'll also touch upon additional interesting things that are either specific to these objectives or can be used as general components for pre-training. We’ll cover the following themes in the pre-training objectives:\n- \\*\\*Contrastive Learning:\\*\\* Aligning images and texts to a joint feature space in a contrastive manner\n- \\*\\*PrefixLM:\\*\\* Jointly learning image and text embeddings by using images as a prefix to a language model\n- \\*\\*Multi-modal Fusing with Cross Attention:\\*\\* Fusing visual information into layers of a language model with a cross-attention mechanism\n- \\*\\*MLM / ITM:\\*\\* Aligning parts of images with text with masked-language modeling and image-text matching objectives\n- \\*\\*No Training:\\*\\* Using stand-alone vision and language models via iterative optimization\nNote that this section is a non-exhaustive list, and there are various other approaches, as well as hybrid strategies such as [Unified-IO](https://arxiv.org/abs/2206.08916). For a more comprehensive review of multi-modal models, refer to [this work.](https://arxiv.org/abs/2210.09263)\n### 1) Contrastive Learning\n\n![Contrastive Learning](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/contrastive_learning.png)  \n*Contrastive pre-training and zero-shot image classification as shown [here](https://openai.com/blog/clip).*\n\nContrastive learning is a commonly used pre-training objective for vision models and has proven to be a highly effective pre-training objective for vision-language models as well."", ""It can be\ninstructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing\nfor the task, similarly to the zero-shot capabilities of GPT-2 and 3. The abstract from the paper is the following:\n\\*State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This\nrestricted form of supervision limits their generality and usability since additional labeled data is needed to specify\nany other visual concept. Learning directly from raw text about images is a promising alternative which leverages a\nmuch broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes\nwith which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400\nmillion (image, text) pairs collected from the internet. After pre-training, natural language is used to reference\nlearned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study\nthe performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks\nsuch as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need\nfor any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot\nwithout needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained\nmodel weights at this https URL.\\*\nThis model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/openai/CLIP). ## Usage tips and example\nCLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image\nclassification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text\nfeatures."", ""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# CLIP\n## Overview\nThe CLIP model was proposed in [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. CLIP\n(Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs."", ""Vision-language models enable a plethora of useful and interesting use cases that go beyond just VQA and zero-shot segmentation. We encourage you to try out the different use cases supported by the models mentioned in this section. For sample code, refer to the respective documentation of the models. ## Emerging Areas of Research\nWith the massive advances in vision-language models, we see the emergence of new downstream tasks and application areas, such as medicine and robotics. For example, vision-language models are increasingly getting adopted for medical use cases, resulting in works such as [Clinical-BERT](https://ojs.aaai.org/index.php/AAAI/article/view/20204) for medical diagnosis and report generation from radiographs and [MedFuseNet](https://www.nature.com/articles/s41598-021-98390-1) for visual question answering in the medical domain. We also see a massive surge of works that leverage joint vision-language representations for image manipulation (e.g., [StyleCLIP](https://arxiv.org/abs/2103.17249), [StyleMC](https://arxiv.org/abs/2112.08493), [DiffusionCLIP](https://arxiv.org/abs/2110.02711)), text-based video retrieval (e.g., [X-CLIP](https://arxiv.org/abs/2207.07285)) and manipulation (e.g., [Text2Live](https://arxiv.org/abs/2204.02491)) and 3D shape and texture manipulation (e.g., [AvatarCLIP](https://arxiv.org/abs/2205.08535), [CLIP-NeRF](https://arxiv.org/abs/2112.05139), [Latent3D](https://arxiv.org/abs/2202.06079), [CLIPFace](https://arxiv.org/abs/2212.01406), [Text2Mesh](https://arxiv.org/abs/2112.03221)). In a similar line of work, [MVT](https://arxiv.org/abs/2204.02174) proposes a joint 3D scene-text representation model, which can be used for various downstream tasks such as 3D scene completion. While robotics research hasn’t leveraged vision-language models on a wide scale yet, we see works such as [CLIPort](https://arxiv.org/abs/2109.12098) leveraging joint vision-language representations for end-to-end imitation learning and reporting large improvements over previous SOTA. We also see that large language models are increasingly getting adopted in robotics tasks such as common sense reasoning, navigation, and task planning. For example, [ProgPrompt](https://arxiv.org/abs/2209.11302) proposes a framework to generate situated robot task plans using large language models (LLMs). Similarly, [SayCan](https://say-can.github.io/assets/palm\\_saycan.pdf) uses LLMs to select the most plausible actions given a visual description of the environment and available objects. While these advances are impressive, robotics research is still confined to limited sets of environments and objects due to the limitation of object detection datasets. With the emergence of open-vocabulary object detection models such as [OWL-ViT](https://arxiv.org/abs/2205.06230) and [GLIP](https://arxiv.org/abs/2112.03857), we can expect a tighter integration of multi-modal models with robotic navigation, reasoning, manipulation, and task-planning frameworks. ## Conclusion\nThere have been incredible advances in multi-modal models in recent years, with vision-language models making the most significant leap in performance and the variety of use cases and applications. In this blog, we talked about the latest advancements in vision-language models, as well as what multi-modal datasets are available and which pre-training strategies we can use to train and fine-tune such models. We also showed how these models are integrated into 🤗 Transformers and how you can use them to perform various tasks with a few lines of code. We are continuing to integrate the most impactful computer vision and multi-modal models and would love to hear back from you. To stay up to date with the latest news in multi-modal research, you can follow us on Twitter: [@adirik](https://twitter.com/https://twitter.com/alaradirik), [@NielsRogge](https://twitter.com/NielsRogge), [@apsdehal](https://twitter.com/apsdehal), [@a\\_e\\_roberts](https://twitter.com/a\\_e\\_roberts), [@RisingSayak](https://mobile.twitter.com/a\\_e\\_roberts), and [@huggingface](https://twitter.com/huggingface). \\*Acknowledgements: We thank Amanpreet Singh and Amy Roberts for their rigorous reviews."", ""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# VisualBERT\n## Overview\nThe VisualBERT model was proposed in [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang. VisualBERT is a neural network trained on a variety of (image, text) pairs. The abstract from the paper is the following:\n\\*We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks. VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an\nassociated input image with self-attention. We further propose two visually-grounded language model objectives for\npre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2,\nand Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly\nsimpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any\nexplicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between\nverbs and image regions corresponding to their arguments.\\*\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani).""]","The authors of the paper 'Learning Transferable Visual Models From Natural Language Supervision' are Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.","Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever."
What is the purpose of local attention in Longformer?,"[""The Authors' code can be found [here](https://github.com/allenai/longformer). ## Usage tips\n- Since the Longformer is based on RoBERTa, it doesn't have `token\\_type\\_ids`. You don't need to indicate which\ntoken belongs to which segment. Just separate your segments with the separation token `tokenizer.sep\\_token` (or\n``). - A transformer model replacing the attention matrices by sparse matrices to go faster. Often, the local context (e.g., what are the two tokens left and right?) is enough to take action for a given token. Some preselected input tokens are still given global attention, but the attention matrix has way less parameters, resulting in a speed-up. See the local attention section for more information. ## Longformer Self Attention\nLongformer self attention employs self attention on both a \""local\"" context and a \""global\"" context. Most tokens only\nattend \""locally\"" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\n`config.attention\\_window`. Note that `config.attention\\_window` can be of type `List` to define a\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \""globally\"" to all other tokens, as it is\nconventionally done for all tokens in `BertSelfAttention`. Note that \""locally\"" and \""globally\"" attending tokens are projected by different query, key and value matrices. Also note\nthat every \""locally\"" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \""globally\""\nattending tokens so that global attention is \\*symmetric\\*. The user can define which tokens attend \""locally\"" and which tokens attend \""globally\"" by setting the tensor\n`global\\_attention\\_mask` at run-time appropriately. All Longformer models employ the following logic for\n`global\\_attention\\_mask`:\n- 0: the token attends \""locally\"",\n- 1: the token attends \""globally\"". For more information please also refer to [`~LongformerModel.forward`] method. Using Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n\\_s \\times n\\_s)\\\\) to\n\\\\(\\mathcal{O}(n\\_s \\times w)\\\\), with \\\\(n\\_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize."", ""\\* Standard pre-trained models can be adapted to long-range inputs by simply replacing the standard self-attention with the long-range self-attention proposed in this paper and then fine-tuning on the downstream task. This avoids costly pre-training specific to long-range inputs. #### Follow-up questions\n\\* The increasing size (throughout the layers) of the dilated windowed self-attention echoes findings in computer vision on increasing the receptive field of stacked CNN. How do these two findings relate? What are the transposable learnings? \\* Longformer’s Encoder-Decoder architecture works well for tasks that do not require a long target length (e.g. summarization). However, how would it work for long-range seq2seq tasks which require a long target length (e.g. document translation, speech recognition, etc.) especially considering the cross-attention layer of encoder-decoder’s models? \\* In practice, the sliding window self-attention relies on many indexing operations to ensure a symmetric query-key weights matrix. Those operations are very slow on TPUs which highlights the question of the applicability of such patterns on other hardware. ### [Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)\nJack W."", ""After you feel like you have gotten a good overview of the architecture\nof the model, you might want to write to Patrick with any\nquestions you might have. This might include questions regarding the\nmodel's architecture, its attention layer, etc. We will be more than\nhappy to help you. #### Additional resources\nBefore diving into the code, here are some additional resources that might be worth taking a look at:\n- [Yannic Kilcher's paper summary](https://www.youtube.com/watch?v=WVPE62Gk3EM&ab\\_channel=YannicKilcher)\n- [Yannic Kilcher's summary of Longformer](https://www.youtube.com/watch?v=\\_8KNb5iqblE&ab\\_channel=YannicKilcher) - Longformer and BigBird are \\*\\*very\\*\\* similar models. Since Longformer has already been ported to 🤗 Transformers, it is useful to understand the differences between the two models\n- [Blog post](https://medium.com/dsc-msit/is-google-bigbird-gonna-be-the-new-leader-in-nlp-domain-8c95cecc30f8) - A relatively superficial blog post about BigBird. Might be a good starting point to understand BigBird\n#### Make sure you've understood the fundamental aspects of BigBird\nAlright, now you should be ready to take a closer look into the actual code of BigBird. You should have understood the following aspects of BigBird by now:\n- BigBird provides a new attention layer for long-range sequence modelling that can be used\nas a drop-in replacement for already existing architectures. This means that every transformer-based model architecture can replace its [Self-attention layer](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a) with BigBird's self-attention layer. - BigBird's self-attention layer is composed of three mechanisms: block sparse (local) self-attention, global self-attention, random self-attention\n- BigBird's block sparse (local) self-attention is different from Longformer's local self-attention."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Nyströmformer\n## Overview\nThe Nyströmformer model was proposed in [\\*Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention\\*](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn\nFung, Yin Li, and Vikas Singh."", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# LED\n## Overview\nThe LED model was proposed in [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz\nBeltagy, Matthew E. Peters, Arman Cohan. The abstract from the paper is the following:\n\\*Transformer-based models are unable to process long sequences due to their self-attention operation, which scales\nquadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or\nlonger. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we\nevaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8.""]","The purpose of local attention in Longformer is to allow each token to attend to a limited context of tokens around it, specifically to its \(\frac{1}{2} w\) previous tokens and \(\frac{1}{2} w\) succeeding tokens, where \(w\) is the window length defined in `config.attention_window`. This mechanism helps to reduce the memory and time complexity of the self-attention operation, making it more efficient for processing long sequences. By focusing on local contexts, Longformer can effectively handle long documents while maintaining performance.","To take action for a given token using the local context, such as the two tokens to the left and right, and to build a representation of the whole sentence by stacking attention layers with a small window."
What library is used to import the text encoder?,"[""The audio signal is converted to a log-mel spectrogram encoded by the encoder. The decoder generates the transcript autoregressively from the encoder's hidden states and the previous tokens. ## Multimodal\n### Encoder[[mm-encoder]]\n[VisualBERT](model\\_doc/visual\\_bert) is a multimodal model for vision-language tasks released shortly after BERT. It combines BERT and a pretrained object detection system to extract image features into visual embeddings, passed alongside text embeddings to BERT. VisualBERT predicts the masked text based on the unmasked text and the visual embeddings, and it also has to predict whether the text is aligned with the image. When ViT was released, [ViLT](model\\_doc/vilt) adopted ViT in its architecture because it was easier to get the image embeddings this way. The image embeddings are jointly processed with the text embeddings. From there, ViLT is pretrained by image text matching, masked language modeling, and whole word masking. [CLIP](model\\_doc/clip) takes a different approach and makes a pair prediction of (`image`, `text`) . An image encoder (ViT) and a text encoder (Transformer) are jointly trained on a 400 million (`image`, `text`) pair dataset to maximize the similarity between the image and text embeddings of the (`image`, `text`) pairs. After pretraining, you can use natural language to instruct CLIP to predict the text given an image or vice versa. [OWL-ViT](model\\_doc/owlvit) builds on top of CLIP by using it as its backbone for zero-shot object detection. After pretraining, an object detection head is added to make a set prediction over the (`class`, `bounding box`) pairs. ### Encoder-decoder[[mm-encoder-decoder]]\nOptical character recognition (OCR) is a long-standing text recognition task that typically involves several components to understand the image and generate the text. [TrOCR](model\\_doc/trocr) simplifies the process using an end-to-end Transformer."", ""Encoder models[[encoder-models]]\n\nEncoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having \""bi-directional\"" attention, and are often called \\*auto-encoding models\\*. The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence. Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering. Representatives of this family of models include:\n- [ALBERT](https://huggingface.co/docs/transformers/model\\_doc/albert)\n- [BERT](https://huggingface.co/docs/transformers/model\\_doc/bert)\n- [DistilBERT](https://huggingface.co/docs/transformers/model\\_doc/distilbert)\n- [ELECTRA](https://huggingface.co/docs/transformers/model\\_doc/electra)\n- [RoBERTa](https://huggingface.co/docs/transformers/model\\_doc/roberta)"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# EnCodec\n## Overview\nThe EnCodec neural codec model was proposed in [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438) by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi. The abstract from the paper is the following:\n\\*We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio.\\*\nThis model was contributed by [Matthijs](https://huggingface.co/Matthijs), [Patrick Von Platen](https://huggingface.co/patrickvonplaten) and [Arthur Zucker](https://huggingface.co/ArthurZ). The original code can be found [here](https://github.com/facebookresearch/encodec). ## Usage example\nHere is a quick example of how to encode and decode an audio using this model:\n```python\n>>> from datasets import load\\_dataset, Audio\n>>> from transformers import EncodecModel, AutoProcessor\n>>> librispeech\\_dummy = load\\_dataset(\""hf-internal-testing/librispeech\\_asr\\_dummy\"", \""clean\"", split=\""validation\"")\n>>> model = EncodecModel.from\\_pretrained(\""facebook/encodec\\_24khz\"")\n>>> processor = AutoProcessor.from\\_pretrained(\""facebook/encodec\\_24khz\"")\n>>> librispeech\\_dummy = librispeech\\_dummy.cast\\_column(\""audio\"", Audio(sampling\\_rate=processor.sampling\\_rate))\n>>> audio\\_sample = librispeech\\_dummy[-1][\""audio\""][\""array\""]\n>>> inputs = processor(raw\\_audio=audio\\_sample, sampling\\_rate=processor.sampling\\_rate, return\\_tensors=\""pt\"")\n>>> encoder\\_outputs = model.encode(inputs[\""input\\_values\""], inputs[\""padding\\_mask\""])\n>>> audio\\_values = model.decode(encoder\\_outputs.audio\\_codes, encoder\\_outputs.audio\\_scales, inputs[\""padding\\_mask\""])[0]\n>>> # or the equivalent with a forward pass\n>>> audio\\_values = model(inputs[\""input\\_values\""], inputs[\""padding\\_mask\""]).audio\\_values\n```\n## EncodecConfig\n[[autodoc]] EncodecConfig\n## EncodecFeatureExtractor\n[[autodoc]] EncodecFeatureExtractor\n- \\_\\_call\\_\\_\n## EncodecModel\n[[autodoc]] EncodecModel\n- decode\n- encode\n- forward"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Textual Inversion\nTextual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder. [`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings. To learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading\\_adapters#textual-inversion) loading guide. ## TextualInversionLoaderMixin\n[[autodoc]] loaders.textual\\_inversion.TextualInversionLoaderMixin"", ""Le. 1. \\*\\*[ELECTRA](https://huggingface.co/docs/transformers/model\\_doc/electra)\\*\\* (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning. 1. \\*\\*[EnCodec](https://huggingface.co/docs/transformers/model\\_doc/encodec)\\*\\* (from Meta AI) released with the paper [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438) by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi. 1. \\*\\*[EncoderDecoder](https://huggingface.co/docs/transformers/model\\_doc/encoder-decoder)\\*\\* (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. 1. \\*\\*[ERNIE](https://huggingface.co/docs/transformers/model\\_doc/ernie)\\*\\* (from Baidu) released with the paper [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223) by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu. 1. \\*\\*[ErnieM](https://huggingface.co/docs/transformers/model\\_doc/ernie\\_m)\\*\\* (from Baidu) released with the paper [ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora](https://arxiv.org/abs/2012.15674) by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang.""]",The text encoder can be imported using the `TextualInversionLoaderMixin` from the `loaders.textual_inversion` module. This mixin provides a function for loading Textual Inversion embeddings into the text encoder.,Transformers
What license is used for TheBloke/Chronohermes-Grad-L2-13B-GPTQ?,"[""‼️ Commercial use:\nFalcon 180b can be commercially used but under very restrictive conditions, excluding any \""hosting use\"". We recommend to check the [license](https://huggingface.co/spaces/tiiuae/falcon-180b-license/blob/main/LICENSE.txt) and consult your legal team if you are interested in using it for commercial purposes. ## How good is Falcon 180B? Falcon 180B is the best openly released LLM today, outperforming Llama 2 70B and OpenAI’s GPT-3.5 on MMLU, and is on par with Google's PaLM 2-Large on HellaSwag, LAMBADA, WebQuestions, Winogrande, PIQA, ARC, BoolQ, CB, COPA, RTE, WiC, WSC, ReCoRD. Falcon 180B typically sits somewhere between GPT 3.5 and GPT4 depending on the evaluation benchmark and further finetuning from the community will be very interesting to follow now that it's openly released. ![Palm 2 comparison](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/162\\_falcon\\_180b/palm2\\_480.jpg)\nWith 68.74 on the Hugging Face Leaderboard, Falcon 180B is the highest-scoring openly released pre-trained LLM, surpassing Meta’s LLaMA 2 (67.35). | Model | Size | Leaderboard score | Commercial use or license | Pretraining length |\n| ------- | ---- | ----------------- | ------------------------- | ------------------ |\n| Falcon | 180B | 68.74 | 🟠 | 3,500B |\n| Llama 2 | 70B | 67.35 | 🟠 | 2,000B |\n| LLaMA | 65B | 64.23 | 🔴 | 1,400B |\n| Falcon | 40B | 61.48 | 🟢 | 1,000B |\n| MPT | 30B | 56.15 | 🟢 | 1,000B |\n![open\\_llm\\_leaderboard.png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/162\\_falcon\\_180b/open\\_llm\\_leaderboard.jpg)\nThe quantized Falcon models preserve similar metrics across benchmarks."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# GPTBigCode\n## Overview\nThe GPTBigCode model was proposed in [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) by BigCode. The listed authors are: Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra. The abstract from the paper is the following:\n\\*The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at [this https URL.](https://huggingface.co/bigcode)\\*\nThe model is an optimized [GPT2 model](https://huggingface.co/docs/transformers/model\\_doc/gpt2) with support for Multi-Query Attention."", ""Each generation is repeated for 4 batches, and metrics are averaged over the number of batches and generation length. Additional benchmarks could be done in the act-order case. From the bencharmk, it appears that Exllama kernel is the best-in-class for GPTQ, although it is rather slow for larger batch sizes. The memory savings are not exactly of x4 although weights are in int4. This can be explained by the possible static buffers used by the kernels, the CUDA context (taken into account in the measurements), and the KV cache that is still in fp16. Bitsandbytes uses the fp4 scheme, with the compute in fp16. ### Batch size = 1\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|None|None |None|None |None |26.0 |36.958 |27.058 |29152.98 |\n| gptq | False | 4 | 128 | exllamav2 | 36.07 | 32.25 | 31.01 | 11313.75 |\n|gptq |False |4 |128 |exllama|36.2 |33.711 |29.663 |10484.34 |\n|gptq |False |4 |128 |autogptq-cuda-old|36.2 |46.44 |21.53 |10344.62 |\n|bitsandbytes|None |None|None |None |37.64 |52.00 |19.23 |11018.36 |\n### Batch size = 2\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|None|None |None|None |None |26.0 |37.35 |53.53 |30831.09 |\n| gptq | False | 4 | 128 | exllamav2 | 36.07 | 35.81 | 55.85 | 12112.42 |\n|gptq |False |4 |128 |exllama|36.2 |37.25 |53.68 |12162.43 |\n|gptq |False |4 |128 |autogptq-cuda-old|36.2 |47.41 |42.18 |12020.34 |\n|bitsandbytes|None |None|None |None |37.64 |74.62 |26.80 |12834.84 |\n### Batch size = 4\n|quantization |act\\_order|bits|group\\_size|kernel |Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|-----------------|-------------|----------------------|------------------|----------------|\n|None|None |None|None |None |26.0 |37.89 |105.55 |34187.22 |\n| gptq | False | 4 | 128 | exllamav2 | 36.07 | 36.04 | 110.98 | 16387.19 |\n|gptq |False |4 |128 |exllama |36.2 |54.14 |73.87 |15518.55 |\n|gptq |False |4 |128 |autogptq-cuda-old|36.2 |60.98 |65.59 |15374.67 |\n|bitsandbytes|None |None|None |None |37.64 |80.24 |49.85 |16187.69 |\n### Batch size = 8\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|None|None |None|None |None |26.0 |47.37 |168.86 |40327.62 |\n| gptq | False | 4 | 128 | exllamav2 | 36.07 | 47.31 | 169.11 | 22463.02 |\n|gptq |False |4 |128 |exllama|36.2 |73.57 |108.73 |21864.56 |\n|gptq |False |4 |128 |autogptq-cuda-old|36.2 |104.44 |76.59 |20987.68 |\n|bitsandbytes|None |None|None |None |37.64 |91.29 |87.63 |22894.02 |\n### Batch size = 16\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|None|None |None|None |None |26.0 |69.94 |228.76 |53986.51 |\n| gptq | False | 4 | 128 | exllamav2 | 36.07 | 83.09 | 192.55 | 35740.95 |\n|gptq |False |4 |128 |exllama|36.2 |95.41 |167.68 |34777.04 |\n|gptq |False |4 |128 |autogptq-cuda-old|36.2 |192.48 |83.12 |35497.62 |\n|bitsandbytes|None |None|None |None |37.64 |113.98 |140.38 |35532.37 |\n## Prefill-only benchmark results\nRun\n```shell\n# pytorch fp16\nCUDA\\_VISIBLE\\_DEVICES=0 python benchmark\\_gptq.py --model meta-llama/Llama-2-13b-chat-hf --sweep --num-batches 10 --task text-generation --prefill --generate\n# GPTQ with exllamav2 kernel (int4/fp16)\nCUDA\\_VISIBLE\\_DEVICES=0 python benchmark\\_gptq.py --model TheBloke/Llama-2-13B-chat-GPTQ --sweep --num-batches 10 --gptq --task text-generation --prefill --use-exllama --exllama-version 2 --generate\n# GPTQ with exllamav kernel (int4/fp16)\nCUDA\\_VISIBLE\\_DEVICES=0 python benchmark\\_gptq.py --model TheBloke/Llama-2-13B-chat-GPTQ --sweep --num-batches 10 --gptq --task text-generation --prefill --use-exllama --generate\n# GPTQ without exllama kernel (int4/fp16)\nCUDA\\_VISIBLE\\_DEVICES=0 python benchmark\\_gptq.py --model TheBloke/Llama-2-13B-chat-GPTQ --sweep --num-batches 10 --gptq --task text-generation --prefill --generate\n# using bitsandbytes fp4/fp16 scheme\nCUDA\\_VISIBLE\\_DEVICES=0 python benchmark\\_gptq.py --model meta-llama/Llama-2-13b-chat-hf --sweep --num-batches 10 --task text-generation --prefill --bitsandbytes --generate\n```\nThe benchmark below is for a prompt length of 512, measuring only the prefill step on a single NVIDIA A100-SXM4-80GB GPU. The forward is repeated 10 times. This benchmark typically corresponds to the forward during training (to the difference that here `generate` is called, which has some overhead). ### Batch size = 1\n|quantization |act\\_order|bits|group\\_size|kernel |prompt\\_length|new\\_tokens|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Max memory (MB)|\n|-----|---------|----|----------|-----------------|-------------|----------|-------------|----------------------|------------------|---------------|\n|None|None |None|None |None |512 |1 |27.22 |96.38 |10.38 |27999.54 |\n| gptq | False | 4 | 128 | exllamav2 | 512 | 1 | 6.63 | 116.07 | 8.62 | 10260.35 |\n|gptq |False |4 |128 |exllama |512 |1 |38.35 |112.54 |8.89 |9330.89 |\n|gptq |False |4 |128 |autogptq-cuda-old|512 |1 |43.94 |368.13 |2.72 |9474.19 |\n|bitsandbytes|None|None|None|None|512|1 |37.46|139.17 |7.19 |9952.65 |\n### Batch size = 2\n|quantization |act\\_order|bits|group\\_size|kernel |prompt\\_length|new\\_tokens|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Max memory (MB)|\n|-----|---------|----|----------|-----------------|-------------|----------|-------------|----------------------|------------------|---------------|\n|None|None |None|None |None |512 |1 |27.22 |169.95 |11.77 |28524.37 |\n| gptq | False | 4 | 128 | exllamav2 | 512 | 1 | 6.63 | 212.07 | 9.43 | 10783.60 |\n|gptq |False |4 |128 |exllama |512 |1 |38.35 |190.44 |10.50 |9855.71 |\n|gptq |False |4 |128 |autogptq-cuda-old|512 |1 |43.94 |443.80 |4.51 |9928.23 |\n|bitsandbytes|None|None|None|None|512|1 |37.46|212.76 |9.40 |10421.89|\n### Batch size = 4\n|quantization |act\\_order|bits|group\\_size|kernel |prompt\\_length|new\\_tokens|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Max memory (MB)|\n|-----|---------|----|----------|-----------------|-------------|----------|-------------|----------------------|------------------|---------------|\n|None|None |None|None |None |512 |1 |27.22 |305.99 |13.07 |29574.01 |\n| gptq | False | 4 | 128 | exllamav2 | 512 | 1 | 6.63 | 385.58 | 10.37 | 11829.59 |\n|gptq |False |4 |128 |exllama |512 |1 |38.35 |345.54 |11.58 |10905.35 |\n|gptq |False |4 |128 |autogptq-cuda-old|512 |1 |43.94 |597.24 |6.70 |10838.42 |\n|bitsandbytes|None|None|None|None|512|1 |37.46|349.18 |11.46|11440.08|\n### Batch size = 8\n|quantization |act\\_order|bits|group\\_size|kernel |prompt\\_length|new\\_tokens|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Max memory (MB)|\n|-----|---------|----|----------|-----------------|-------------|----------|-------------|----------------------|------------------|---------------|\n|None|None |None|None |None |512 |1 |27.22 |600.47 |13.32 |31673.30 |\n| gptq | False | 4 | 128 | exllamav2 | 512 | 1 | 6.63 | 753.06 | 10.62 | 13920.50 |\n|gptq |False |4 |128 |exllama |512 |1 |38.35 |659.61 |12.13 |13004.64 |\n|gptq |False |4 |128 |autogptq-cuda-old|512 |1 |43.94 |909.09 |8.80 |12862.18 |\n|bitsandbytes|None|None|None|None|512|1 |37.46|643.42 |12.43|13539.37|\n### Batch size = 16\n|quantization |act\\_order|bits|group\\_size|kernel |prompt\\_length|new\\_tokens|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Max memory (MB)|\n|-----|---------|----|-----------|----------|-------------|----------|-------------|----------------------|------------------|---------------|\n|None|None |None|None |None |512 |1 |27.22 |1209.07 |13.23 |35871.88 |\n| gptq | False | 4 | 128 | exllamav2 | 512 | 1 | 6.63 | 1467.36 | 10.90 | 18104.44 |\n|gptq |False |4 |128 |exllama |512 |1 |38.35 |1280.25 |12.50 |17203.22 |\n|gptq |False |4 |128 |autogptq-cuda-old |512 |1 |43.94 |1533.54 |10.43 |17060.76 |\n|bitsandbytes|None|None|None|None|512|1 |37.46|1256.88|12.73|17737.95|\n## Perplexity benchmark results\nRun\n```shell\n# pytorch fp16\nCUDA\\_VISIBLE\\_DEVICES=0 python benchmark\\_gptq.py --model meta-llama/Llama-2-13b-chat-hf --task text-generation --ppl\n# GPTQ with exllamav2 kernel (int4/fp16)\nCUDA\\_VISIBLE\\_DEVICES=0 python benchmark\\_gptq.py --model TheBloke/Llama-2-13B-chat-GPTQ --revision gptq-4bit-128g-actorder\\_True --gptq --task text-generation --use-exllama --exllama-version 2 --ppl\n# GPTQ with exllama kernel (int4/fp16)\nCUDA\\_VISIBLE\\_DEVICES=0 python benchmark\\_gptq.py --model TheBloke/Llama-2-13B-chat-GPTQ --revision gptq-4bit-128g-actorder\\_True --gptq --task text-generation --use-exllama --ppl\n# GPTQ without exllama kernel (int4/fp16)\nCUDA\\_VISIBLE\\_DEVICES=0 python benchmark\\_gptq.py --model TheBloke/Llama-2-13B-chat-GPTQ --revision gptq-4bit-128g-actorder\\_True --gptq --task text-generation --ppl\n# using bitsandbytes fp4/fp16 scheme\nCUDA\\_VISIBLE\\_DEVICES=0 python benchmark\\_gptq.py --model meta-llama/Llama-2-13b-chat-hf ---task text-generation --bitsandbytes --ppl\n```\n| quantization | act\\_order | bits | group\\_size | kernel | perplexity |\n|--------------|-----------|------|------------|------------------|------------|\n| None | None | None | None | None | 6.61 |\n| gptq | True | 4 | 128 | exllamav2 | 6.77 |\n| gptq | True | 4 | 128 | exllama | 6.77 |\n| gptq | True | 4 | 128 | autogptq-cuda-old| 6.77 |\n| bitsandbytes | None | 4 | None | None | 6.78 |"", ""There are 2 ways of defining a license tag. Either the license is one of the officially-supported licenses. In this case, simply defining it as a string in the metadata is enough:\n```yaml\n# Example from https://huggingface.co/codellama/CodeLlama-34b-hf\n---\nlicense: llama2\n---\n```\nOtherwise, the license is considered as `other`. In that case, we can set a custom name and a URL to the said license. Here is an example of how it looks like:\n```yaml\n# Example from https://huggingface.co/coqui/XTTS-v1\n---\nlicense: other\nlicense\\_name: coqui-public-model-license\nlicense\\_link: https://coqui.ai/cpml\n---\n```\nThis challenge aims to improve the completeness of this metadata on the Hub, which will ultimately benefit all users. ## How to contribute? How to do it in practice? That's simple! We have listed models below that have no `license` in their metadata but still contain a LICENSE file in the repo. Those models require an update in their metadata to describe which license is attached. We only selected the models that have at least 1 download and 1 like to filter out unrelevant models. For each model, the workflow looks like this:\n1. Choose a model in the list below. We suggest focusing on the most downloaded or liked models first. 2. \\*\\*Check that the model doesn't already have a PR to update the license\\*\\*. Maybe another contributor already checked it!"", ""## Acknowledgements\nWe would like to thank [William](https://github.com/PanQiWei) for his support and his work on the amazing AutoGPTQ library and for his help in the integration. We would also like to thank [TheBloke](https://huggingface.co/TheBloke) for his work on quantizing many models with AutoGPTQ and sharing them on the Hub and for his help with the integration. We would also like to aknowledge [qwopqwop200](https://github.com/qwopqwop200) for his continuous contributions on AutoGPTQ library and his work on extending the library for CPU that is going to be released in the next versions of AutoGPTQ. Finally, we would like to thank [Pedro Cuenca](https://github.com/pcuenca) for his help with the writing of this blogpost.""]",The license for TheBloke/Chronohermes-Grad-L2-13B-GPTQ is not explicitly mentioned in the provided context. You may need to check the model's repository on Hugging Face for the specific license details.,llama-2-community-license
What is a potential legal issue with using outputs from language models?,"[""The prompts are passed through the initial language model to generate new text. Human annotators are used to rank the generated text outputs from the LM. One may initially think that humans should apply a scalar score directly to each piece of text in order to generate a reward model, but this is difficult to do in practice. The differing values of humans cause these scores to be uncalibrated and noisy. Instead, rankings are used to compare the outputs of multiple models and create a much better regularized dataset. There are multiple methods for ranking the text. One method that has been successful is to have users compare generated text from two language models conditioned on the same prompt. By comparing model outputs in head-to-head matchups, an [Elo](https://en.wikipedia.org/wiki/Elo\\_rating\\_system) system can be used to generate a ranking of the models and outputs relative to each-other. These different methods of ranking are normalized into a scalar reward signal for training. An interesting artifact of this process is that the successful RLHF systems to date have used reward language models with varying sizes relative to the text generation (e.g. OpenAI 175B LM, 6B reward model, Anthropic used LM and reward models from 10B to 52B, DeepMind uses 70B Chinchilla models for both LM and reward)."", ""4. \\*\\*Refine the content.\\*\\* I continue to refine the generated content to better fit the game. ![ChatGPT for Story #4](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/chatgpt4.png)\n\nI'm happy with this result. So, should I use it directly? Maybe. Since this is a free game being developed for an AI tutorial, probably. However, it may not be straightforward for commercial products, having potential unintended legal, ethical, and commercial ramifications. > ⚠️ \\*\\*Limitation:\\*\\* Using outputs from language models directly may have unintended legal, ethical, and commercial ramifications. Some potential unintended ramifications of using outputs directly are as follows:\n- Legal: The legal landscape surrounding Generative AI is currently very unclear, with several ongoing lawsuits. - Ethical: Language models can produce plagiarized or biased outputs. For more information, check out the [Ethics and Society Newsletter](https://huggingface.co/blog/ethics-soc-2). - Commercial: [Some](https://www.searchenginejournal.com/google-says-ai-generated-content-is-against-guidelines/444916/) sources have stated that AI-generated content may be deprioritized by search engines. This [may not](https://seo.ai/blog/google-is-not-against-ai-content) be the case for most non-spam content, but is worth considering. Tools such as [AI Content Detector](https://writer.com/ai-content-detector/) can be used to check whether content may be detected as AI-generated. There is ongoing research on language model [watermarking](https://arxiv.org/abs/2301.10226) which may mark text as AI-generated. Given these limitations, the safest approach may be to use language models like ChatGPT for brainstorming but write the final content by hand."", ""You can use any dataset with plain text and tokenize the text to prepare the data. Causal language models can be used to generate code. For more information about the Causal Language Modeling task, check out the Hugging Face course. Masked Language Modeling video\nWelcome to the Hugging Face tasks series! In this video we’ll take a look at Masked Language Modeling. Masked language modeling is the task of predicting which words should fill in the blanks of a sentence. These models take a masked text as the input and output the possible values for that mask. Masked language modeling is handy before fine-tuning your model for your task. For example, if you need to use a model in a specific domain, say, biomedical documents, models like BERT will treat your domain-specific words as rare tokens. If you train a masked language model using your biomedical corpus and then fine tune your model on a downstream task, you will have a better performance. Classification metrics can’t be used as there’s no single correct answer to mask values."", ""- `exporters` no longer applies a softmax to outputs when converting text generation tasks. We realized this was necessary for some generation algorithms. - `exporters` now defaults to using fixed sequence lengths for text models. Core ML has a way to specify \""flexible shapes\"", such that your input sequence may have any length between 1 and, say, 4096 tokens. We discovered that flexible inputs only run on CPU, but not on GPU or the Neural Engine. More investigation coming soon! We'll keep adding best practices to our tools so you don't have to discover the same issues again. ## Optimization\nThere's no point in converting models if they don't run fast on your target hardware and respect system resources. The models mentioned in this post are pretty big for local use, and we are consciously using them to stretch the limits of what's possible with current technology and understand where the bottlenecks are. There are a few key optimization areas we've identified. They are a very important topic for us and the subject of current and upcoming work. Some of them include:\n- Cache attention keys and values from previous generations, just like the transformers models do in the PyTorch implementation. The computation of attention scores needs to run on the whole sequence generated so far, but all the past key-value pairs were already computed in previous runs."", ""It’s an important distinction to make between these models. On the Hugging Face Hub, you can find both causal language models and causal language models fine-tuned on instructions (which we’ll give links to later in this blog post). Llama is one of the first open-source LLMs to have outperformed/matched closed-source ones. A research group led by Together has created a reproduction of Llama's dataset, called Red Pajama, and trained LLMs and instruction fine-tuned models on it. You can read more about it [here](https://www.together.xyz/blog/redpajama) and find [the model checkpoints on Hugging Face Hub](https://huggingface.co/models?sort=trending&search=togethercomputer%2Fredpajama). By the time this blog post is written, three of the largest causal language models with open-source licenses are [MPT-30B by MosaicML](https://huggingface.co/mosaicml/mpt-30b), [XGen by Salesforce](https://huggingface.co/Salesforce/xgen-7b-8k-base) and [Falcon by TII UAE](https://huggingface.co/tiiuae/falcon-40b), available completely open on Hugging Face Hub. Recently, Meta released [Llama 2](https://ai.meta.com/Llama/), an open-access model with a license that allows commercial use. As of now, Llama 2 outperforms all of the other open-source large language models on different benchmarks. [Llama 2 checkpoints on Hugging Face Hub](https://huggingface.co/meta-Llama) are compatible with transformers, and the largest checkpoint is available for everyone to try at [HuggingChat](https://huggingface.co/chat/).""]","A potential legal issue with using outputs from language models is that the legal landscape surrounding Generative AI is currently very unclear, with several ongoing lawsuits. This uncertainty may lead to legal ramifications when using AI-generated content directly.","The legal landscape surrounding Generative AI is currently very unclear, with several ongoing lawsuits."
What is the filename used when downloading the model from the Hugging Face Hub?,"[""The Model Hub\n## What is the Model Hub? The Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing. Download pre-trained models with the [`huggingface\\_hub` client library](https://huggingface.co/docs/huggingface\\_hub/index), with 🤗 [`Transformers`](https://huggingface.co/docs/transformers/index) for fine-tuning and other usages or with any of the over [15 integrated libraries](./models-libraries). You can even leverage the [Inference API](./models-inference) to use models in production settings. You can refer to the following video for a guide on navigating the Model Hub:\nTo learn how to upload models to the Hub, you can refer to the [Repositories Getting Started Guide](./repositories-getting-started)."", ""Models Download Stats\n## How are download stats generated for models? Counting the number of downloads for models is not a trivial task as a single model repository might contain multiple files, including multiple model weight files (e.g., with sharded models), and different formats depending on the library. To avoid double counting downloads (e.g., counting a single download of a model as multiple downloads), the Hub uses a set of query files that are employed for download counting. No information is sent from the user, and no additional calls are made for this. The count is done server-side as we serve files for downloads. Every HTTP request to these files, including `GET` and `HEAD` will be counted as a download. By default, when no library is specified, the Hub uses `config.json` as the default query file. Otherwise, the query file depends on each library, and the Hub might examine files such as `pytorch\\_model.bin` and `adapter\\_config.json`. ## Which are the query files for different libraries? By default, the Hub looks at `config.json`, `config.yaml`, `hyperparams.yaml`, and `meta.yaml`. For the following set of libraries, there are specific query files\n```json\n{\n\""adapter-transformers\"": {\nfilter: [\n{\nterm: { path: \""adapter\\_config.json\"" },\n},\n],\n},\n\""asteroid\"": {\nfilter: [\n{\nterm: { path: \""pytorch\\_model.bin\"" },\n},\n],\n},\n\""flair\"": {\nfilter: [\n{\nterm: { path: \""pytorch\\_model.bin\"" },\n},\n],\n},\n\""keras\"": {\nfilter: [\n{\nterm: { path: \""saved\\_model.pb\"" },\n},\n],\n},\n\""ml-agents\"": {\nfilter: [\n{\nwildcard: { path: \""\\*.onnx\"" },\n},\n],\n},\n\""nemo\"": {\nfilter: [\n{\nwildcard: { path: \""\\*.nemo\"" },\n},\n],\n},\n\""open\\_clip\"": {\nfilter: [\n{\nwildcard: { path: \""\\*pytorch\\_model.bin\"" },\n},\n],\n},\n\""sample-factory\"": {\nfilter: [\n{\nterm: { path: \""cfg.json\"" },\n},\n],\n},\n\""paddlenlp\"": {\nfilter: [\n{\nterm: { path: \""model\\_config.json\"" },\n},\n],\n},\n\""speechbrain\"": {\nfilter: [\n{\nterm: { path: \""hyperparams.yaml\"" },\n},\n],\n},\n\""sklearn\"": {\nfilter: [\n{\nterm: { path: \""sklearn\\_model.joblib\"" },\n},\n],\n},\n\""spacy\"": {\nfilter: [\n{\nwildcard: { path: \""\\*.whl\"" },\n},\n],\n},\n\""stanza\"": {\nfilter: [\n{\nterm: { path: \""models/default.zip\"" },\n},\n],\n},\n\""stable-baselines3\"": {\nfilter: [\n{\nwildcard: { path: \""\\*.zip\"" },\n},\n],\n},\n\""timm\"": {\nfilter: [\n{\nterms: { path: [\""pytorch\\_model.bin\"", \""model.safetensors\""] },\n},\n],\n},\n\""diffusers\"": {\n/// Filter out nested safetensors and pickle weights to avoid double counting downloads from the diffusers lib\nmust\\_not: [\n{\nwildcard: { path: \""\\*/\\*.safetensors\"" },\n},\n{\nwildcard: { path: \""\\*/\\*.bin\"" },\n},\n],\n/// Include documents that match at least one of the following rules\nshould: [\n/// Downloaded from diffusers lib\n{\nterm: { path: \""model\\_index.json\"" },\n},\n/// Direct downloads (LoRa, Auto1111 and others)\n{\nwildcard: { path: \""\\*.safetensors\"" },\n},\n{\nwildcard: { path: \""\\*.ckpt\"" },\n},\n{\nwildcard: { path: \""\\*.bin\"" },\n},\n],\nminimum\\_should\\_match: 1,\n},\n\""peft\"": {\nfilter: [\n{\nterm: { path: \""adapter\\_config.json\"" },\n},\n],\n}\n}\n```"", ""Downloading models\n## Integrated libraries\nIf a model on the Hub is tied to a [supported library](./models-libraries), loading the model can be done in just a few lines. For information on accessing the model, you can click on the \""Use in \\_Library\\_\"" button on the model page to see how to do so. For example, `distilgpt2` shows how to do so with 🤗 Transformers below. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/models-usage.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/models-usage-dark.png)\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/models-usage-modal.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/models-usage-modal-dark.png)\n\n## Using the Hugging Face Client Library\nYou can use the [`huggingface\\_hub`](https://github.com/huggingface/huggingface\\_hub) library to create, delete, update and retrieve information from repos. You can also download files from repos or integrate them into your library! For example, you can quickly load a Scikit-learn model with a few lines. ```py\nfrom huggingface\\_hub import hf\\_hub\\_download\nimport joblib\nREPO\\_ID = \""YOUR\\_REPO\\_ID\""\nFILENAME = \""sklearn\\_model.joblib\""\nmodel = joblib.load(\nhf\\_hub\\_download(repo\\_id=REPO\\_ID, filename=FILENAME)\n)\n```\n## Using Git\nSince all models on the Model Hub are Git repositories, you can clone the models locally by running:\n```bash\ngit lfs install\ngit clone git@hf.co: # example: git clone git@hf.co:bigscience/bloom\n```\nIf you have write-access to the particular model repo, you'll also have the ability to commit and push revisions to the model. Add your SSH public key to [your user settings](https://huggingface.co/settings/keys) to push changes and/or access private repos."", ""This allows you to create a place to share your organization's artefacts. - \\*\\*Minting DOIs\\*\\*: A [DOI](https://www.doi.org/) (Digital Object Identifier) is a persistent digital identifier for an object. DOIs have become essential for creating persistent identifiers for publications, datasets and software. A persistent identifier is often required by journals, conferences or researcher funders when referencing academic outputs. The Hugging Face Hub supports issuing DOIs for models, datasets, and demos shared on the Hub. - \\*\\*Usage tracking\\*\\*: you can view download stats for datasets and models hosted in the Hub monthly or see the total number of downloads over all time. These stats can be a valuable way for institutions to demonstrate their impact. - \\*\\*Script-based dataset sharing\\*\\*: if you already have dataset hosted somewhere, you can still provide access to them via the Hugging Face hub using a [dataset loading script](https://huggingface.co/docs/datasets/dataset\\_script). - \\*\\*Model and dataset gating\\*\\*: there are circumstances where you want more control over who is accessing models and datasets. The Hugging Face hub supports model and dataset gating, allowing you to add access controls. ## How can I get help using the Hub? The Hub [docs](https://huggingface.co/docs/hub/index) go into more detail about the various features of the Hugging Face Hub. You can also find more information about [sharing datasets on the Hub](https://huggingface.co/docs/datasets/upload\\_dataset) and information about [sharing Transformers models to the Hub](https://huggingface.co/docs/transformers/model\\_sharing)."", ""Check out the [cache limitations](../guides/manage-cache#limitations) section for more details. ## Download from the CLI\nYou can use the `huggingface-cli download` command from the terminal to directly download files from the Hub. Internally, it uses the same [`hf\\_hub\\_download`] and [`snapshot\\_download`] helpers described above and prints the\nreturned path to the terminal. ```bash\n>>> huggingface-cli download gpt2 config.json\n/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n```\nYou can download multiple files at once which displays a progress bar and returns the snapshot path in which the files\nare located:\n```bash\n>>> huggingface-cli download gpt2 config.json model.safetensors\nFetching 2 files: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 23831.27it/s]\n/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10\n```\nFor more details about the CLI download command, please refer to the [CLI guide](./cli#huggingface-cli-download).""]","The filename used when downloading a model from the Hugging Face Hub depends on the specific model and the library being used. By default, the Hub looks at `config.json`, `config.yaml`, `hyperparams.yaml`, and `meta.yaml`. For different libraries, there are specific query files that may be used, such as `pytorch_model.bin` for PyTorch models or `saved_model.pb` for Keras models. You can refer to the documentation for the specific model or library to find the exact filename used for downloading.",q-learning.pkl
What method is used by ViTHybridImageProcessor?,"[""The abstract from the paper is the following:\n\\*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its\napplications to computer vision remain limited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional networks while keeping their overall\nstructure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),\nVision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring\nsubstantially fewer computational resources to train.\\*\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code (written in JAX) can be\nfound [here](https://github.com/google-research/vision\\_transformer). ## Resources\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ViT Hybrid. - [`ViTHybridForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image\\_classification.ipynb). - See also: [Image classification task guide](../tasks/image\\_classification)\nIf you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. ## ViTHybridConfig\n[[autodoc]] ViTHybridConfig\n## ViTHybridImageProcessor\n[[autodoc]] ViTHybridImageProcessor\n- preprocess\n## ViTHybridModel\n[[autodoc]] ViTHybridModel\n- forward\n## ViTHybridForImageClassification\n[[autodoc]] ViTHybridForImageClassification\n- forward"", ""We will use the same model checkpoint for this. Let's initialize the model and the processor. ```python\nfrom transformers import Swin2SRForImageSuperResolution, Swin2SRImageProcessor\nmodel = Swin2SRForImageSuperResolution.from\\_pretrained(\""caidas/swin2SR-lightweight-x2-64\"").to(device)\nprocessor = Swin2SRImageProcessor(\""caidas/swin2SR-lightweight-x2-64\"")\n```\n`pipeline` abstracts away the preprocessing and postprocessing steps that we have to do ourselves, so let's preprocess the image. We will pass the image to the processor and then move the pixel values to GPU. ```python\npixel\\_values = processor(image, return\\_tensors=\""pt\"").pixel\\_values\nprint(pixel\\_values.shape)\npixel\\_values = pixel\\_values.to(device)\n```\nWe can now infer the image by passing pixel values to the model. ```python\nimport torch\nwith torch.no\\_grad():\noutputs = model(pixel\\_values)\n```\nOutput is an object of type `ImageSuperResolutionOutput` that looks like below 👇\n```\n(loss=None, reconstruction=tensor([[[[0.8270, 0.8269, 0.8275, ..., 0.7463, 0.7446, 0.7453],\n[0.8287, 0.8278, 0.8283, ..., 0.7451, 0.7448, 0.7457],\n[0.8280, 0.8273, 0.8269, ..., 0.7447, 0.7446, 0.7452],\n...,\n[0.5923, 0.5933, 0.5924, ..., 0.0697, 0.0695, 0.0706],\n[0.5926, 0.5932, 0.5926, ..., 0.0673, 0.0687, 0.0705],\n[0.5927, 0.5914, 0.5922, ..., 0.0664, 0.0694, 0.0718]]]],\ndevice='cuda:0'), hidden\\_states=None, attentions=None)\n```\nWe need to get the `reconstruction` and post-process it for visualization."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# VAE Image Processor\nThe [`VaeImageProcessor`] provides a unified API for [`StableDiffusionPipeline`]s to prepare image inputs for VAE encoding and post-processing outputs once they're decoded. This includes transformations such as resizing, normalization, and conversion between PIL Image, PyTorch, and NumPy arrays. All pipelines with [`VaeImageProcessor`] accept PIL Image, PyTorch tensor, or NumPy arrays as image inputs and return outputs based on the `output\\_type` argument by the user. You can pass encoded image latents directly to the pipeline and return latents from the pipeline as a specific output with the `output\\_type` argument (for example `output\\_type=\""latent\""`). This allows you to take the generated latents from one pipeline and pass it to another pipeline as input without leaving the latent space. It also makes it much easier to use multiple pipelines together by passing PyTorch tensors directly between different pipelines. ## VaeImageProcessor\n[[autodoc]] image\\_processor.VaeImageProcessor\n## VaeImageProcessorLDM3D\nThe [`VaeImageProcessorLDM3D`] accepts RGB and depth inputs and returns RGB and depth outputs."", ""At a high level, this involves:\n1. Loading the [\""Quick, Draw!\"" dataset](https://huggingface.co/datasets/Xenova/quickdraw-small). 2. Transforming the dataset using a [`MobileViTImageProcessor`](https://huggingface.co/docs/transformers/model\\_doc/mobilevit#transformers.MobileViTImageProcessor). 3. Defining our [collate function](https://huggingface.co/docs/transformers/main\\_classes/data\\_collator) and [evaluation metric](https://huggingface.co/docs/evaluate/types\\_of\\_evaluations#metrics). 4. Loading the [pre-trained MobileVIT model](https://huggingface.co/apple/mobilevit-small) using [`MobileViTForImageClassification.from\\_pretrained`](https://huggingface.co/docs/transformers/model\\_doc/mobilevit#transformers.MobileViTForImageClassification). 5. Training the model using the [`Trainer`](https://huggingface.co/docs/transformers/main\\_classes/trainer) and [`TrainingArguments`](https://huggingface.co/docs/transformers/main\\_classes/trainer#transformers.TrainingArguments) helper classes. 6. Evaluating the model using [🤗 Evaluate](https://huggingface.co/docs/evaluate). \\*NOTE:\\* You can find our finetuned model [here](https://huggingface.co/Xenova/quickdraw-mobilevit-small) on the Hugging Face Hub."", ""[`AutoImageProcessor`] takes care of processing image data to create `pixel\\_values`, `pixel\\_mask`, and\n`labels` that a DETR model can train with. The image processor has some attributes that you won't have to worry about:\n- `image\\_mean = [0.485, 0.456, 0.406 ]`\n- `image\\_std = [0.229, 0.224, 0.225]`\nThese are the mean and standard deviation used to normalize images during the model pre-training. These values are crucial\nto replicate when doing inference or finetuning a pre-trained image model. Instantiate the image processor from the same checkpoint as the model you want to finetune. ```py\n>>> from transformers import AutoImageProcessor\n>>> checkpoint = \""facebook/detr-resnet-50\""\n>>> image\\_processor = AutoImageProcessor.from\\_pretrained(checkpoint)\n```\nBefore passing the images to the `image\\_processor`, apply two preprocessing transformations to the dataset:\n- Augmenting images\n- Reformatting annotations to meet DETR expectations\nFirst, to make sure the model does not overfit on the training data, you can apply image augmentation with any data augmentation library.""]","The `ViTHybridImageProcessor` provides a method called `preprocess`, which is used for preparing image inputs for the Vision Transformer (ViT) model. This method handles the necessary transformations to convert images into a format suitable for the model.",preprocess
Who contributed the code component for syntax highlighting in Gradio?,"[""Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling. ##### Various performance improvements\nThese improvements will be particularly beneficial to large applications. - Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast. - Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%). - Corrected an issue that was causing markdown to re-render infinitely. - Ensured that the `gr.3DModel` does re-render prematurely. Thanks [@pngwn](https://github.com/pngwn)! ### Features\n- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app/gradio/commit/fbdad78af4c47454cbb570f88cc14bf4479bbceb) - Lazy load interactive or static variants of a component individually, rather than loading both variants regardless."", ""Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling. ##### Various performance improvements\nThese improvements will be particularly beneficial to large applications. - Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast. - Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%). - Corrected an issue that was causing markdown to re-render infinitely. - Ensured that the `gr.3DModel` does re-render prematurely. Thanks [@pngwn](https://github.com/pngwn)! ### Features\n- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app/gradio/commit/fbdad78af4c47454cbb570f88cc14bf4479bbceb) - Lazy load interactive or static variants of a component individually, rather than loading both variants regardless."", ""Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling. ##### Various performance improvements\nThese improvements will be particularly beneficial to large applications. - Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast. - Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%). - Corrected an issue that was causing markdown to re-render infinitely. - Ensured that the `gr.3DModel` does re-render prematurely. Thanks [@pngwn](https://github.com/pngwn)! ### Features\n- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app/gradio/commit/fbdad78af4c47454cbb570f88cc14bf4479bbceb) - Lazy load interactive or static variants of a component individually, rather than loading both variants regardless."", ""Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling. ##### Various performance improvements\nThese improvements will be particularly beneficial to large applications. - Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast. - Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%). - Corrected an issue that was causing markdown to re-render infinitely. - Ensured that the `gr.3DModel` does re-render prematurely. Thanks [@pngwn](https://github.com/pngwn)! ### Features\n- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app/gradio/commit/fbdad78af4c47454cbb570f88cc14bf4479bbceb) - Lazy load interactive or static variants of a component individually, rather than loading both variants regardless."", ""Including syntax highlighting and Github Flavoured Markdown. We also have more consistent markdown behaviour and styling. ##### Various performance improvements\nThese improvements will be particularly beneficial to large applications. - Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast. - Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%). - Corrected an issue that was causing markdown to re-render infinitely. - Ensured that the `gr.3DModel` does re-render prematurely. Thanks [@pngwn](https://github.com/pngwn)! ### Features\n- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app/gradio/commit/fbdad78af4c47454cbb570f88cc14bf4479bbceb) - Lazy load interactive or static variants of a component individually, rather than loading both variants regardless.""]",The code component for syntax highlighting in Gradio was contributed by [@pngwn](https://github.com/pngwn).,@pngwn
"Where can you find the Apache License, Version 2.0?","[""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Legacy examples\nThis folder contains examples which are not actively maintained (mostly contributed by the community). Using these examples together with a recent version of the library usually requires to make small (sometimes big) adaptations to get the scripts working."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Scenes\nUnder construction 🚧."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Optimization\n## ORTOptimizer\n[[autodoc]] onnxruntime.optimization.ORTOptimizer\n- all"", ""3. Find the corresponding license. URLs to the LICENSE file(s) in the repo is provided to ease the search. 1. Note: the table suggests the name/URL for Llama models. It doesn't mean that the suggestion is accurate. It is to be double-checked correctly. 4. Check which license applies to the model. Attributes are:\n1. Either `license`: `mit`, `apache-2`, `llama2`, etc. A complete list can be found on the left side [on this page](https://huggingface.co/models?license=license:apache-2.0&sort=trending). Otherwise, you can also fetch this list as JSON [at this URL](https://huggingface.co/api/models-tags-by-type?type=license). 2. Either `license\\_name` (short, lowercase, without spaces - e.g. `\""coqui-public-model-license\""`) + `license\\_link` (a URL). If possible, a URL owned by the author is better (e.g. `\""https://coqui.ai/cpml\""`) but linking to the LICENSE file in the repo is also fine. 5. Open a PR on the Hub, describing the intent and suggesting a modification to the repo author. Here are 2 example for [XTTS-v1](https://huggingface.co/coqui/XTTS-v1/discussions/10/files) and [stable-diffusion-xl-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9/discussions/58) models."", ""Licenses\nYou are able to add a license to any repo that you create on the Hugging Face Hub to let other users know about the permissions that you want to attribute to your code or data. The license can be specified in your repository's `README.md` file, known as a \\*card\\* on the Hub, in the card's metadata section. Remember to seek out and respect a project's license if you're considering using their code or data. A full list of the available licenses is available here:\n\nFullname | License identifier (to use in model card)\n--- | ---\nApache license 2.0 | `apache-2.0`\nMIT | `mit`\nOpenRAIL license family | `openrail`\nBigScience OpenRAIL-M | `bigscience-openrail-m`\nCreativeML OpenRAIL-M | `creativeml-openrail-m`\nBigScience BLOOM RAIL 1.0 | `bigscience-bloom-rail-1.0`\nBigCode Open RAIL-M v1 | `bigcode-openrail-m`\nAcademic Free License v3.0 | `afl-3.0`\nArtistic license 2.0 | `artistic-2.0`\nBoost Software License 1.0 | `bsl-1.0`\nBSD license family | `bsd`\nBSD 2-clause \""Simplified\"" license | `bsd-2-clause`\nBSD 3-clause \""New\"" or \""Revised\"" license | `bsd-3-clause`\nBSD 3-clause Clear license | `bsd-3-clause-clear`\nComputational Use of Data Agreement | `c-uda`\nCreative Commons license family | `cc`\nCreative Commons Zero v1.0 Universal | `cc0-1.0`\nCreative Commons Attribution 2.0 | `cc-by-2.0`\nCreative Commons Attribution 2.5 | `cc-by-2.5`\nCreative Commons Attribution 3.0 | `cc-by-3.0`\nCreative Commons Attribution 4.0 | `cc-by-4.0`\nCreative Commons Attribution Share Alike 3.0 | `cc-by-sa-3.0`\nCreative Commons Attribution Share Alike 4.0 | `cc-by-sa-4.0`\nCreative Commons Attribution Non Commercial 2.0 | `cc-by-nc-2.0`\nCreative Commons Attribution Non Commercial 3.0 | `cc-by-nc-3.0`\nCreative Commons Attribution Non Commercial 4.0 | `cc-by-nc-4.0`\nCreative Commons Attribution No Derivatives 4.0 | `cc-by-nd-4.0`\nCreative Commons Attribution Non Commercial No Derivatives 3.0 | `cc-by-nc-nd-3.0`\nCreative Commons Attribution Non Commercial No Derivatives 4.0 | `cc-by-nc-nd-4.0`\nCreative Commons Attribution Non Commercial Share Alike 2.0 | `cc-by-nc-sa-2.0`\nCreative Commons Attribution Non Commercial Share Alike 3.0 | `cc-by-nc-sa-3.0`\nCreative Commons Attribution Non Commercial Share Alike 4.0 | `cc-by-nc-sa-4.0`\nCommunity Data License Agreement – Sharing, Version 1.0 | `cdla-sharing-1.0`\nCommunity Data License Agreement – Permissive, Version 1.0 | `cdla-permissive-1.0`\nCommunity Data License Agreement – Permissive, Version 2.0 | `cdla-permissive-2.0`\nDo What The F\\*ck You Want To Public License | `wtfpl`\nEducational Community License v2.0 | `ecl-2.0`\nEclipse Public License 1.0 | `epl-1.0`\nEclipse Public License 2.0 | `epl-2.0`\nEtalab Open License 2.0 | `etalab-2.0`\nEuropean Union Public License 1.1 | `eupl-1.1`\nGNU Affero General Public License v3.0 | `agpl-3.0`\nGNU Free Documentation License family | `gfdl`\nGNU General Public License family | `gpl`\nGNU General Public License v2.0 | `gpl-2.0`\nGNU General Public License v3.0 | `gpl-3.0`\nGNU Lesser General Public License family | `lgpl`\nGNU Lesser General Public License v2.1 | `lgpl-2.1`\nGNU Lesser General Public License v3.0 | `lgpl-3.0`\nISC | `isc`\nLaTeX Project Public License v1.3c | `lppl-1.3c`\nMicrosoft Public License | `ms-pl`\nMozilla Public License 2.0 | `mpl-2.0`\nOpen Data Commons License Attribution family | `odc-by`\nOpen Database License family | `odbl`\nOpen Rail++-M License | `openrail++`\nOpen Software License 3.0 | `osl-3.0`\nPostgreSQL License | `postgresql`\nSIL Open Font License 1.1 | `ofl-1.1`\nUniversity of Illinois/NCSA Open Source License | `ncsa`\nThe Unlicense | `unlicense`\nzLib License | `zlib`\nOpen Data Commons Public Domain Dedication and License | `pddl`\nLesser General Public License For Linguistic Resources | `lgpl-lr`\nDeepFloyd IF Research License Agreement | `deepfloyd-if-license`\nLlama 2 Community License Agreement | `llama2`\nUnknown | `unknown`\nOther | `other`\nIn case of `license: other` please add the license's text to a `LICENSE` file inside your repo (or contact us to add the license you use to this list), and set a name for it in `license\\_name`.""]","You can find the Apache License, Version 2.0 at the following URL: [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0).",http://www.apache.org/licenses/LICENSE-2.0
What tool can be used for sentiment analysis in NLP?,"[""Most times, this is caused is an ongoing situation that needs to be addressed asap (e.g. an app not working because of server outages or a really bad experience with a customer support representative). Now that we covered what is sentiment analysis and why it's useful, let's get our hands dirty and actually do sentiment analysis of tweets!💥\n## How to do Twitter sentiment analysis with code? Nowadays, getting started with sentiment analysis on Twitter is quite easy and straightforward 🙌\nWith a few lines of code, you can automatically get tweets, run sentiment analysis and visualize the results. And you can learn how to do all these things in just a few minutes! In this section, we'll show you how to do it with a cool little project: we'll do sentiment analysis of tweets mentioning [Notion](https://twitter.com/notionhq)! First, you'll use [Tweepy](https://www.tweepy.org/), an open source Python library to get tweets mentioning @NotionHQ using the [Twitter API](https://developer.twitter.com/en/docs/twitter-api)."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Text classification\n[[open-in-colab]]\nText classification is a common NLP task that assigns a label or class to text. Some of the largest companies run text classification in production for a wide range of practical applications. One of the most popular forms of text classification is sentiment analysis, which assigns a label like 🙂 positive, 🙁 negative, or 😐 neutral to a sequence of text. This guide will show you how to:\n1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [IMDb](https://huggingface.co/datasets/imdb) dataset to determine whether a movie review is positive or negative."", ""--\ntitle: \""Sentiment Analysis on Encrypted Data with Homomorphic Encryption\""\nthumbnail: /blog/assets/sentiment-analysis-fhe/thumbnail.png\nauthors:\n- user: jfrery-zama\nguest: true\n---\n# Sentiment Analysis on Encrypted Data with Homomorphic Encryption\nIt is well-known that a sentiment analysis model determines whether a text is positive, negative, or neutral. However, this process typically requires access to unencrypted text, which can pose privacy concerns. Homomorphic encryption is a type of encryption that allows for computation on encrypted data without needing to decrypt it first. This makes it well-suited for applications where user's personal and potentially sensitive data is at risk (e.g. sentiment analysis of private messages). This blog post uses the [Concrete-ML library](https://github.com/zama-ai/concrete-ml), allowing data scientists to use machine learning models in fully homomorphic encryption (FHE) settings without any prior knowledge of cryptography. We provide a practical tutorial on how to use the library to build a sentiment analysis model on encrypted data. The post covers:\n- transformers\n- how to use transformers with XGBoost to perform sentiment analysis\n- how to do the training\n- how to use Concrete-ML to turn predictions into predictions over encrypted data\n- how to [deploy to the cloud](https://docs.zama.ai/concrete-ml/getting-started/cloud) using a client/server protocol\nLast but not least, we’ll finish with a complete demo over [Hugging Face Spaces](https://huggingface.co/spaces) to show this functionality in action. ## Setup the environment\nFirst make sure your pip and setuptools are up to date by running:\n```\npip install -U pip setuptools\n```\nNow we can install all the required libraries for the this blog with the following command. ```\npip install concrete-ml transformers datasets\n```\n## Using a public dataset\nThe dataset we use in this notebook can be found [here](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment). To represent the text for sentiment analysis, we chose to use a transformer hidden representation as it yields high accuracy for the final model in a very efficient way. For a comparison of this representation set against a more common procedure like the TF-IDF approach, please see this [full notebook](https://github.com/zama-ai/concrete-ml/blob/release/0.4.x/use\\_case\\_examples/encrypted\\_sentiment\\_analysis/SentimentClassification.ipynb). We can start by opening the dataset and visualize some statistics. ```python\nfrom datasets import load\\_datasets\ntrain = load\\_dataset(\""osanseviero/twitter-airline-sentiment\"")[\""train\""].to\\_pandas()\ntext\\_X = train['text']\ny = train['airline\\_sentiment']\ny = y.replace(['negative', 'neutral', 'positive'], [0, 1, 2])\npos\\_ratio = y.value\\_counts()[2] / y.value\\_counts().sum()\nneg\\_ratio = y.value\\_counts()[0] / y.value\\_counts().sum()\nneutral\\_ratio = y.value\\_counts()[1] / y.value\\_counts().sum()\nprint(f'Proportion of positive examples: {round(pos\\_ratio \\* 100, 2)}%')\nprint(f'Proportion of negative examples: {round(neg\\_ratio \\* 100, 2)}%')\nprint(f'Proportion of neutral examples: {round(neutral\\_ratio \\* 100, 2)}%')\n```\nThe output, then, looks like this:\n```\nProportion of positive examples: 16.14%\nProportion of negative examples: 62.69%\nProportion of neutral examples: 21.17%\n```\nThe ratio of positive and neutral examples is rather similar, though we have significantly more negative examples."", ""```\n!pip install -q gradio nltk\n```\n```\nimport gradio as gr\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nnltk.download(\""vader\\_lexicon\"")\nsid = SentimentIntensityAnalyzer()\ndef sentiment\\_analysis(text):\nscores = sid.polarity\\_scores(text)\ndel scores[\""compound\""]\nreturn scores\ndemo = gr.Interface(\nfn=sentiment\\_analysis,\ninputs=gr.Textbox(placeholder=\""Enter a positive or negative sentence here...\""),\noutputs=\""label\"",\nexamples=[[\""This is wonderful!\""]])\ndemo.launch()\n```"", ""Explore the results of sentiment analysis\nHow are people talking about NFTs on Twitter? Are they talking mostly positively or negatively? Let's explore the results of the sentiment analysis to find out! First, let's load the results on a dataframe and see examples of tweets that were labeled for each sentiment:\n```python\nimport pandas as pd\n# Load the data in a dataframe\ndf = pd.DataFrame(tweets)\npd.set\\_option('display.max\\_colwidth', None)\n# Show a tweet for each sentiment\ndisplay(df[df[\""sentiment\""] == 'POS'].head(1))\ndisplay(df[df[\""sentiment\""] == 'NEU'].head(1))\ndisplay(df[df[\""sentiment\""] == 'NEG'].head(1))\n```\nOutput:\n```\nTweet: @NFTGalIery Warm, exquisite and elegant palette of charming beauty Its price is 2401 ETH. \\nhttps://t.co/Ej3BfVOAqc\\n#NFTs #NFTartists #art #Bitcoin #Crypto #OpenSeaNFT #Ethereum #BTC Sentiment: POS\nTweet: How much our followers made on #Crypto in December:\\n#DAPPRadar airdrop — $200\\nFree #VPAD tokens — $800\\n#GasDAO airdrop — up to $1000\\nStarSharks\\_SSS IDO — $3500\\nCeloLaunch IDO — $3000\\n12 Binance XMas #NFTs — $360 \\nTOTAL PROFIT: $8500+\\n\\nJoin and earn with us https://t.co/fS30uj6SYx Sentiment: NEU\nTweet: Stupid guy #2\\nhttps://t.co/8yKzYjCYIl\\n\\n#NFT #NFTs #nftcollector #rarible https://t.co/O4V19gMmVk Sentiment: NEG\n```\nThen, let's see how many tweets you got for each sentiment and visualize these results:\n```python\n# Let's count the number of tweets by sentiments\nsentiment\\_counts = df.groupby(['sentiment']).size()\nprint(sentiment\\_counts)\n# Let's visualize the sentiments\nfig = plt.figure(figsize=(6,6), dpi=100)\nax = plt.subplot(111)\nsentiment\\_counts.plot.pie(ax=ax, autopct='%1.1f%%', startangle=270, fontsize=12, label=\""\"")\n```\nInterestingly, most of the tweets about NFTs are positive (56.1%) and almost none are negative\n(2.0%):\n\nSentiment analysis result of NFTs tweets\n\nFinally, let's see what words stand out for each sentiment by creating a word cloud:\n```python\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n# Wordcloud with positive tweets\npositive\\_tweets = df['tweet'][df[\""sentiment\""] == 'POS']\nstop\\_words = [\""https\"", \""co\"", \""RT\""] + list(STOPWORDS)\npositive\\_wordcloud = WordCloud(max\\_font\\_size=50, max\\_words=100, background\\_color=\""white\"", stopwords = stop\\_words).generate(str(positive\\_tweets))\nplt.figure()\nplt.title(\""Positive Tweets - Wordcloud\"")\nplt.imshow(positive\\_wordcloud, interpolation=\""bilinear\"")\nplt.axis(\""off\"")\nplt.show()\n# Wordcloud with negative tweets\nnegative\\_tweets = df['tweet'][df[\""sentiment\""] == 'NEG']\nstop\\_words = [\""https\"", \""co\"", \""RT\""] + list(STOPWORDS)\nnegative\\_wordcloud = WordCloud(max\\_font\\_size=50, max\\_words=100, background\\_color=\""white\"", stopwords = stop\\_words).generate(str(negative\\_tweets))\nplt.figure()\nplt.title(\""Negative Tweets - Wordcloud\"")\nplt.imshow(negative\\_wordcloud, interpolation=\""bilinear\"")\nplt.axis(\""off\"")\nplt.show()\n```\nSome of the words associated with positive tweets include Discord, Ethereum, Join, Mars4 and Shroom:\n\nWord cloud for positive tweets\n\nIn contrast, words associated with negative tweets include: cookies chaos, Solana, and OpenseaNFT:\n\nWord cloud for negative tweets\n\nAnd that is it!""]","One popular tool for sentiment analysis in NLP is the SentimentIntensityAnalyzer from the NLTK library. It provides a simple way to analyze the sentiment of text by assigning scores for positive, negative, and neutral sentiments. Additionally, you can use transformer models like DistilBERT for more advanced sentiment analysis tasks.",transformers pipeline
Who are the authors of the paper associated with Graphormer?,"[""!--Copyright 2022 The HuggingFace Team and Microsoft. All rights reserved. Licensed under the MIT License; you may not use this file except in compliance with\nthe License. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Graphormer\n## Overview\nThe Graphormer model was proposed in [Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234) by\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen and Tie-Yan Liu. It is a Graph Transformer model, modified to allow computations on graphs instead of text sequences by generating embeddings and features of interest during preprocessing and collation, then using a modified attention. The abstract from the paper is the following:\n\\*The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.\\*\nThis model was contributed by [clefourrier](https://huggingface.co/clefourrier). The original code can be found [here](https://github.com/microsoft/Graphormer). ## Usage tips\nThis model will not work well on large graphs (more than 100 nodes/edges), as it will make the memory explode. You can reduce the batch size, increase your RAM, or decrease the `UNREACHABLE\\_NODE\\_DISTANCE` parameter in algos\\_graphormer.pyx, but it will be hard to go above 700 nodes/edges. This model does not use a tokenizer, but instead a special collator during training."", ""Paper Pages\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png)\n\n## Linking a Paper to a model, dataset or Space\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository's tags. Clicking on the arxiv tag will let you:\n\\* Visit the Paper page. \\* Filter for other models or datasets on the Hub that cite the same paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\n\n## Claiming authorship to a Paper\nThe Hub will attempt to automatically match paper to users based on their email. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png)\n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \""claim authorship\""."", ""1. \\*\\*[GPTBigCode](https://huggingface.co/docs/transformers/model\\_doc/gpt\\_bigcode)\\*\\* (BigCode 에서 제공)은 Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.의 [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988)논문과 함께 발표했습니다. 1. \\*\\*[GPTSAN-japanese](https://huggingface.co/docs/transformers/model\\_doc/gptsan-japanese)\\*\\* released in the repository [tanreinama/GPTSAN](https://github.com/tanreinama/GPTSAN/blob/main/report/model.md) by Toshiyuki Sakamoto(tanreinama). 1. \\*\\*[Graphormer](https://huggingface.co/docs/transformers/model\\_doc/graphormer)\\*\\* (from Microsoft) Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu 의 [Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234) 논문과 함께 발표했습니다. 1. \\*\\*[GroupViT](https://huggingface.co/docs/transformers/model\\_doc/groupvit)\\*\\* (UCSD, NVIDIA 에서) Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang 의 [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) 논문과 함께 발표했습니다. 1. \\*\\*[HerBERT](https://huggingface.co/docs/transformers/model\\_doc/herbert)\\*\\* (Allegro.pl, AGH University of Science and Technology 에서 제공)은 Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.의 [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf)논문과 함께 발표했습니다. 1. \\*\\*[Hubert](https://huggingface.co/docs/transformers/model\\_doc/hubert)\\*\\* (Facebook 에서) Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed 의 [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) 논문과 함께 발표했습니다. 1. \\*\\*[I-BERT](https://huggingface.co/docs/transformers/model\\_doc/ibert)\\*\\* (Berkeley 에서) Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer 의 [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) 논문과 함께 발표했습니다."", ""## GraphormerConfig\n[[autodoc]] GraphormerConfig\n## GraphormerModel\n[[autodoc]] GraphormerModel\n- forward\n## GraphormerForGraphClassification\n[[autodoc]] GraphormerForGraphClassification\n- forward"", ""1. \\*\\*[LED](https://huggingface.co/docs/transformers/model\\_doc/led)\\*\\* (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan. 1. \\*\\*[LeViT](https://huggingface.co/docs/transformers/model\\_doc/levit)\\*\\* (from Meta AI) released with the paper [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://arxiv.org/abs/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze. 1. \\*\\*[LiLT](https://huggingface.co/docs/transformers/model\\_doc/lilt)\\*\\* (from South China University of Technology) released with the paper [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding. 1. \\*\\*[LLaMA](https://huggingface.co/docs/transformers/model\\_doc/llama)\\*\\* (from The FAIR team of Meta AI) released with the paper [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. 1. \\*\\*[Llama2](https://huggingface.co/docs/transformers/model\\_doc/llama2)\\*\\* (from The FAIR team of Meta AI) released with the paper [Llama2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.""]","The authors of the paper associated with Graphormer are Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu.","Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu."
What function does Accelerate provide to determine a device map?,"[""This took a while to pinpoint and either we went for 100% compliance and the model\nwas much slower, or we would take a small difference in generation\nbut was much faster to run and simpler code. We opted for a configurable flag. # First inference (PP + Accelerate)\n```\nNote: Pipeline Parallelism (PP) means in this context that each GPU will own\nsome layers so each GPU will work on a given chunk of data before handing\nit off to the next GPU. ```\nNow we have a workable `transformers` clean version of the start\nworking on running this. Bloom is a 352GB (176B parameters in bf16) model, we need at least that much\nGPU RAM to make it fit. We briefly explored offloading to CPU on smaller machines\nbut the inference speed was orders of magnitude slower so we discarded it. Then we wanted to basically use the [pipeline](https://huggingface.co/docs/transformers/v4.22.2/en/pipeline\\_tutorial#pipeline-usage). So it's dogfooding and this is what the API uses under the hood all the time. However `pipelines` are not distributed aware (it's not their goal). After briefly\ndiscussing options, we ended up using [accelerate](https://github.com/huggingface/accelerate/) newly\ncreated `device\\_map=\""auto\""` to manage the sharding of the model. We had to iron\nout a few bugs, and fix the `transformers` code a bit to help `accelerate` do the right job."", ""Now let's talk about Accelerate, a library aimed to make this process more seameless and also help with a few best practices\n## 🤗 Accelerate\n[Accelerate](https://huggingface.co/docs/accelerate) is a library designed to allow you to perform what we just did above, without needing to modify your code greatly. On top of this, the data pipeline innate to Accelerate can also improve performance to your code as well. First, let's wrap all of the above code we just performed into a single function, to help us visualize the difference:\n```python\ndef train\\_ddp(rank, world\\_size):\nsetup(rank, world\\_size)\n# Build DataLoaders\ntransform = transforms.Compose([\ntransforms.ToTensor(),\ntransforms.Normalize((0.1307), (0.3081))\n])\ntrain\\_dset = datasets.MNIST('data', train=True, download=True, transform=transform)\ntest\\_dset = datasets.MNIST('data', train=False, transform=transform)\ntrain\\_loader = torch.utils.data.DataLoader(train\\_dset, shuffle=True, batch\\_size=64)\ntest\\_loader = torch.utils.data.DataLoader(test\\_dset, shuffle=False, batch\\_size=64)\n# Build model\nmodel = model.to(rank)\nddp\\_model = DDP(model, device\\_ids=[rank])\n# Build optimizer\noptimizer = optim.AdamW(ddp\\_model.parameters(), lr=1e-3)\n# Train for a single epoch\nmodel.train()\nfor batch\\_idx, (data, target) in enumerate(train\\_loader):\ndata, target = data.to(device), target.to(device)\noutput = model(data)\nloss = F.nll\\_loss(output, target)\nloss.backward()\noptimizer.step()\noptimizer.zero\\_grad()\n# Evaluate\nmodel.eval()\ncorrect = 0\nwith torch.no\\_grad():\nfor data, target in test\\_loader:\ndata, target = data.to(device), target.to(device)\noutput = model(data)\npred = output.argmax(dim=1, keepdim=True)\ncorrect += pred.eq(target.view\\_as(pred)).sum().item()\nprint(f'Accuracy: {100."", ""upercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on which you can run your training: it could be on CPU, GPUs, TPUs. Distributed on one machine with several devices, or several machines (often called nodes) each with multiple devices. On top of that there are new tweaks to make your training faster or more memory efficient, like mixed precision and DeepSpeed. Each of those setups or training tweaks, requires you to change the code of your training loop in one way or another and to learn a new API. All those setups are handled by the Trainer API, and there are several third-party libraries that can also help you with that. The problem with those is that they can feel like a black box and that it might not be easy to implement the tweak to the training loop you need. Accelerate has been designed specifically to let you retain full control over your training loop and be as non-intrusive as possible. With just four lines to add to your training loop (here shown on the code of the training loop from the \""Raw training loop\"" video), Accelerate will handle all the setups and training tweaks mentioned on the first slide."", ""The attribute `has\\_fp16\\_weights` has to be set to `False` in order to directly load the weights in `int8` together with the quantization statistics. We also discard the replacement for some modules (here the `lm\\_head`) since we want to keep the latest in their native precision for more precise and stable results. But it isn't over yet! The function above is executed under the `init\\_empty\\_weights` context manager which means that the new model will be still in the `meta` device. For models that are initialized under this context manager, `accelerate` will manually load the parameters of each module and move them to the correct devices. In `bitsandbytes`, setting a `Linear8bitLt` module's device is a crucial step (if you are curious, you can check the code snippet [here](https://github.com/TimDettmers/bitsandbytes/blob/bd515328d70f344f935075f359c5aefc616878d5/bitsandbytes/nn/modules.py#L94)) as we have seen in our toy script. Here the quantization step fails when calling it twice. We had to come up with an implementation of `accelerate`'s `set\\_module\\_tensor\\_to\\_device` function (termed as `set\\_module\\_8bit\\_tensor\\_to\\_device`) to make sure we don't call it twice. Let's discuss this in detail in the section below! ### Be very careful on how to set devices with `accelerate`\nHere we played a very delicate balancing act with the `accelerate` library! Once you load your model and set it on the correct devices, sometimes you still need to call `set\\_module\\_tensor\\_to\\_device` to dispatch the model with hooks on all devices."", ""'model.decoder.layers.9': 0,\n'model.decoder.layers.10.self\\_attn': 0,\n'model.decoder.layers.10.activation\\_fn': 0,\n'model.decoder.layers.10.self\\_attn\\_layer\\_norm': 0,\n'model.decoder.layers.10.fc1': 'cpu',\n'model.decoder.layers.10.fc2': 'cpu',\n'model.decoder.layers.10.final\\_layer\\_norm': 'cpu',\n'model.decoder.layers.11': 'cpu',\n... 'model.decoder.layers.17': 'cpu',\n'model.decoder.layers.18.self\\_attn': 'cpu',\n'model.decoder.layers.18.activation\\_fn': 'cpu',\n'model.decoder.layers.18.self\\_attn\\_layer\\_norm': 'cpu',\n'model.decoder.layers.18.fc1': 'disk',\n'model.decoder.layers.18.fc2': 'disk',\n'model.decoder.layers.18.final\\_layer\\_norm': 'disk',\n'model.decoder.layers.19': 'disk',\n... 'model.decoder.layers.39': 'disk',\n'lm\\_head': 'disk'}\n```\nAccelerate evaluated that the embeddings and the decoder up until the 9th block could all fit on the GPU (device 0), then part of the 10th block needs to be on the CPU, as well as the following weights until the 17th layer. Then the 18th layer is split between the CPU and the disk and the following layers must all be offloaded to disk\nActually using this device map later on won't work, because the layers composing this model have residual connections (where the input of the block is added to the output of the block) so all of a given layer should be on the same device. We can indicate this to Accelerate by passing a list of module names that shouldn't be split with the `no\\_split\\_module\\_classes` keyword argument:\n```python\ndevice\\_map = infer\\_auto\\_device\\_map(model, no\\_split\\_module\\_classes=[\""OPTDecoderLayer\""])\n```\nThis will then return\n```python out\n'model.decoder.embed\\_tokens': 0,\n'model.decoder.embed\\_positions': 0,\n'model.decoder.final\\_layer\\_norm': 0,\n'model.decoder.layers.0': 0,\n'model.decoder.layers.1': 0,\n...""]",Accelerate provides the function `infer_auto_device_map` to determine a device map. This function can be used to specify which modules should not be split across devices by using the `no_split_module_classes` keyword argument.,infer_auto_device_map
Why is the __call__ method decorated with torch.no_grad?,"[""The behavior is fully analogous to [PyTorch's `to` method](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to). - [`\\_\\_call\\_\\_`] method to use the pipeline in inference. `\\_\\_call\\_\\_` defines inference logic of the pipeline and should ideally encompass all aspects of it, from pre-processing to forwarding tensors to the different models and schedulers, as well as post-processing. The API of the `\\_\\_call\\_\\_` method can strongly vary from pipeline to pipeline. \\*E.g.\\* a text-to-image pipeline, such as [`StableDiffusionPipeline`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\_diffusion/pipeline\\_stable\\_diffusion.py) should accept among other things the text prompt to generate the image. A pure image generation pipeline, such as [DDPMPipeline](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/ddpm) on the other hand can be run without providing any inputs. To better understand what inputs can be adapted for\neach pipeline, one should look directly into the respective pipeline. \\*\\*Note\\*\\*: All pipelines have PyTorch's autograd disabled by decorating the `\\_\\_call\\_\\_` method with a [`torch.no\\_grad`](https://pytorch.org/docs/stable/generated/torch.no\\_grad.html) decorator because pipelines should\nnot be used for training. If you want to store the gradients during the forward pass, we recommend writing your own pipeline, see also our [community-examples](https://github.com/huggingface/diffusers/tree/main/examples/community)\n## Contribution\nWe are more than happy about any contribution to the officially supported pipelines 🤗."", ""This is done very simply using hooks. > [hooks](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register\\_forward\\_hook) are a PyTorch API that adds functions executed just before each forward called\nWe couldn't use this directly since they only support models with regular arguments and no keyword arguments in their forward pass, but we took the same idea. Once the model is loaded, the `dispatch\\_model` function will add hooks to every module and submodule that are executed before and after each forward pass. They will:\n- make sure all the inputs of the module are on the same device as the weights;\n- if the weights have been offloaded to the CPU, move them to GPU 0 before the forward pass and back to the CPU just after;\n- if the weights have been offloaded to disk, load them in RAM then on the GPU 0 before the forward pass and free this memory just after. The whole process is summarized in the following video:\nThis way, your model can be loaded and run even if you don't have enough GPU RAM and CPU RAM. The only thing you need is disk space (and lots of patience!) While this solution is pretty naive if you have multiple GPUs (there is no clever pipeline parallelism involved, just using the GPUs sequentially) it still yields [pretty decent results for BLOOM](https://huggingface.co/blog/bloom-inference-pytorch-scripts). And it allows you to run the model on smaller setups (albeit more slowly)."", ""We had the version working but something was off in the generations compared to its\nPython counterpart. During the investigation of the issues, we figured... \\*\\*that we had forgotten to remove the profiler in the Pytorch measurements\\*\\*... That was the epic fail because removing it gave us back the 25% and then both\ncodes ran just as fast. This is what we initially expected, that python mustn't\nbe a performance hit, since it's mostly running torch cpp's code. In the end,\n3 days is not the end of the world, and it might become useful sometime in the\nfuture but still pretty bad. This is quite common when doing optimizations to do wrong or misrepresentative\nmeasurements which end up being disappointing or even detrimental to the overall\nproduct. This is why doing it in small steps and having expectations about the\noutcome as soon as possible helps contain that risk. Another place where we had to be extra careful, was the initial forward pass (without\npast) and the later forward passes (with past). If you optimize the first one,\nyou're most certainly going to be slowing down the later ones which are much more\nimportant and account for most of the runtime. Another pretty common culprit is measuring times which are CPU times, and not\nactual CUDA times, so you need to `torch.cuda.synchronize()` when doing\nruns to be sure that the kernels complete. ## Custom kernel\nSo far, we had achieved close to DeepSpeed performance without any custom code\noutside of PyTorch! Pretty neat. We also didn't have to make any compromise\non the flexibility of the run time batch size! But given the DeepSpeed experience, we wanted to try and write a custom kernel\nto fuse a few operations in the hot path where `torch.jit.script` wasn't able to\ndo it for us. Essentially the following two lines:\n```python\nattn\\_weights = attention\\_scores.masked\\_fill\\_(attention\\_mask, torch.finfo(attention\\_scores.dtype).min)\nattention\\_probs = F.softmax(attn\\_weights, dim=-1, dtype=torch.float32).to(input\\_dtype)\n```\nThe first masked fill is creating a new tensor, which is here only to\nsay to the softmax operator to ignore those values."", ""That's the second most annoying thing about CUDA errors: they irremediably break your kernel. The most annoying thing about them is the fact that they are hard to debug. Why is that? It has to do with the way GPUs work. They are extremely efficient at executing a lot of operations in parallel, but the drawback is that when one of those instructions results in an error, you don't know it instantly. It's only when the program calls a synchronization of the multiple processes on the GPU that it will realize something went wrong, so the error is actually raised at a place that has nothing to do with what created it. For instance, if we look at our previous traceback, the error was raised during the backward pass, but we will see in a minute that it actually stems from something in the forward pass. So how do we debug those errors? The answer is easy: we don't. Unless your CUDA error is an out-of-memory error (which means there is not enough memory in your GPU), you should always go back to the CPU to debug it. To do this in our case, we just have to put the model back on the CPU and call it on our batch -- the batch returned by the `DataLoader` has not been moved to the GPU yet:\n```python\noutputs = trainer.model.cpu()(\\*\\*batch)\n```\n```python out\n~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll\\_loss(input, target, weight, size\\_average, ignore\\_index, reduce, reduction)\n2386 )\n2387 if dim == 2:\n-> 2388 ret = torch.\\_C.\\_nn.nll\\_loss(input, target, weight, \\_Reduction.get\\_enum(reduction), ignore\\_index)\n2389 elif dim == 4:\n2390 ret = torch.\\_C.\\_nn.nll\\_loss2d(input, target, weight, \\_Reduction.get\\_enum(reduction), ignore\\_index)\nIndexError: Target 2 is out of bounds."", ""Torch API\n[[autodoc]] safetensors.torch.load\\_file\n[[autodoc]] safetensors.torch.load\n[[autodoc]] safetensors.torch.save\\_file\n[[autodoc]] safetensors.torch.save\n[[autodoc]] safetensors.torch.load\\_model\n[[autodoc]] safetensors.torch.save\\_model""]","The `__call__` method is decorated with `torch.no_grad` because pipelines should not be used for training. This decorator disables PyTorch's autograd, which is responsible for tracking gradients. By using `torch.no_grad`, it ensures that the operations performed during inference do not track gradients, thus saving memory and improving performance. This is particularly important for inference tasks where gradient computation is unnecessary.",Because pipelines should not be used for training.
What is the latency in milliseconds with fp16 optimization?,"[""The impact of `fp16` on latency is less marked with `batch\\_size = 1`, but here it is of enormous interest as it can reduce latency by almost half, and almost double throughput! # Concluding remarks\nThis blog post showcased a few simple optimization tricks bundled in the 🤗 ecosystem. Using anyone of these techniques, or a combination of all three, can greatly improve Bark inference speed and memory footprint. \\* You can use the large version of Bark without any performance degradation and a footprint of just 2GB instead of 5GB, 15% faster, \\*\\*using 🤗 Better Transformer and CPU offload\\*\\*. \\* Do you prefer high throughput? \\*\\*Batch by 8 with 🤗 Better Transformer and half-precision\\*\\*. \\* You can get the best of both worlds by using \\*\\*fp16, 🤗 Better Transformer and CPU offload\\*\\*!"", ""Also, it is important to note that tuning OpenMP library will only work within software that uses the OpenMP API internally. More specially, now only PyTorch and TorchScript really make usage of OpenMP and thus benefit from OpenMP backend tuning. This also explains why we reported latencies only for these two frameworks. ## Automatic Performances Tuning: Bayesian Optimization with Intel SigOpt\nAs mentioned above, many knobs can be tweaked to improve latency and throughput on Intel CPUs, but because there are many, tuning all of them to get optimal performance can be cumbersome. For instance, in our experiments, the following knobs were tuned:\n- The number of cores: although using as many cores as you have is often a good idea, it does not always provide the best performance because it also means more communication between the different threads. On top of that, having better performance with fewer cores can be very useful as it allows to run multiple instances at the same time, resulting in both better latency and throughput. - The memory allocator: which memory allocator out of the default malloc, Google's tcmalloc and Facebook's jemalloc provides the best performance? - The parallelism library: which parallelism library out of GNU OpenMP and Intel OpenMP provides the best performance? - Transparent Huge Pages: does enabling Transparent Huge Pages (THP) on the system provide better performance? - KMP block time parameter: sets the time, in milliseconds, that a thread should wait, after completing the execution of a parallel region, before sleeping. Of course, the brute force approach, consisting of trying out all the possibilities will provide the best knob values to use to get optimal performance but,\nthe size of the search space being `N x 3 x 2 x 2 x 2 = 24N`, it can take a lot of time: on a machine with 80 physical cores, this means trying out at most `24 x 80 = 1920` different setups! 😱\nFortunately, Intel's [SigOpt](https://sigopt.com/), through Bayesian optimization, allows us to make these tuning experiments both faster and more convenient to analyse, while providing similar performance than the brute force approach. When we analyse the relative difference between the absolute best latency and what SigOpt provides, we observe that although it is often not as good as brute force (except for sequence length = 512 in that specific case),\nit gives very close performance, with \\*\\*8.6%\\*\\* being the biggest gap on this figure. |  |  |\n| --- | --- |\n| Figure 23. Absolute best latency found by SigOpt automatic tuning vs brute force | Figure 24. Relative best latency found by SigOpt automatic tuning vs brute force |\n\nSigOpt is also very useful for analysis: it provides a lot of figures and valuable information."", ""By default, this is set to `True` which is used to train in mixed Int8/FP16 precision. However, we are interested in memory efficient inference for which we need to use `has\\_fp16\\_weights=False`. 5. Now time to load your model in 8-bit! ```py\nint8\\_model.load\\_state\\_dict(torch.load(\""model.pt\""))\nint8\\_model = int8\\_model.to(0) # Quantization happens here\n```\nNote that the quantization step is done in the second line once the model is set on the GPU. If you print `int8\\_model[0].weight` before calling the `.to` function you get:\n```\nint8\\_model[0].weight\nParameter containing:\ntensor([[ 0.0031, -0.0438, 0.0494, ..., -0.0046, -0.0410, 0.0436],\n[-0.1013, 0.0394, 0.0787, ..., 0.0986, 0.0595, 0.0162],\n[-0.0859, -0.1227, -0.1209, ..., 0.1158, 0.0186, -0.0530],\n...,\n[ 0.0804, 0.0725, 0.0638, ..., -0.0487, -0.0524, -0.1076],\n[-0.0200, -0.0406, 0.0663, ..., 0.0123, 0.0551, -0.0121],\n[-0.0041, 0.0865, -0.0013, ..., -0.0427, -0.0764, 0.1189]],\ndtype=torch.float16)\n```\nWhereas if you print it after the second line's call you get:\n```\nint8\\_model[0].weight\nParameter containing:\ntensor([[ 3, -47, 54, ..., -5, -44, 47],\n[-104, 40, 81, ..., 101, 61, 17],\n[ -89, -127, -125, ..., 120, 19, -55],\n...,\n[ 82, 74, 65, ..., -49, -53, -109],\n[ -21, -42, 68, ..., 13, 57, -12],\n[ -4, 88, -1, ..., -43, -78, 121]],\ndevice='cuda:0', dtype=torch.int8, requires\\_grad=True)\n```\nThe weights values are \""truncated\"" as we have seen when explaining quantization in the previous sections. Also, the values seem to be distributed between [-127, 127]. You might also wonder how to retrieve the FP16 weights in order to perform the outlier MatMul in fp16? You can simply do:\n```py\n(int8\\_model[0].weight.CB \\* int8\\_model[0].weight.SCB) / 127\n```\nAnd you will get:\n```\ntensor([[ 0.0028, -0.0459, 0.0522, ..., -0.0049, -0.0428, 0.0462],\n[-0.0960, 0.0391, 0.0782, ..., 0.0994, 0.0593, 0.0167],\n[-0.0822, -0.1240, -0.1207, ..., 0.1181, 0.0185, -0.0541],\n...,\n[ 0.0757, 0.0723, 0.0628, ..., -0.0482, -0.0516, -0.1072],\n[-0.0194, -0.0410, 0.0657, ..., 0.0128, 0.0554, -0.0118],\n[-0.0037, 0.0859, -0.0010, ..., -0.0423, -0.0759, 0.1190]],\ndevice='cuda:0')\n```\nWhich is close enough to the original FP16 values (2 print outs up)!"", ""It's extremely important to avoid the hug of death. On this benchmark the initial performance was (on 16xA100 40Go on GCP which is the machine used throughout):\nRequests/s : 0.3 (throughput)\nLatency: 350ms/token (latency)\nThose numbers are not that great. Before getting to work let's estimate\nthe best we can imagine achieving. The formula for amount of operations is `24Bsh^2 + 4𝐵s^2h24Bsh^2 + 4𝐵s^2h` where `B` is\nthe batch size, `s` the sequence length, and `h` the hidden dimension. Let's do the math and we are getting `17 TFlop` for a single forward pass. Looking at the [specs](https://www.nvidia.com/en-us/data-center/a100/) of A100 it claims `312 TFLOPS` for a single card. That means a single GPU could potentially run at `17 / 312 = 54ms/token`. We're using 16 of those so `3ms/token` on the overall\nmachine. Take all these numbers with a big grain of salt, it's never possible to reach those numbers,\nand real-life performance rarely matches the specs. Also if computation is not your limiting\nfactor then this is not the lowest you can get. It's just good practice to know how far you are from\nyour target. In this case, we're 2 orders of magnitude so pretty far. Also, this estimate puts\nall the flops at the service of latency which means only a single request can go at a time (it's ok since you're maximizing your machine\nso there's not much else to be done, but we can have higher latency and get throughput back through batching much more easily). # Exploring many routes\n```\nNote: Tensor Parallelism (TP) means in this context that each GPU will own\npart of the weights, so ALL gpus are active all the time and do less work."", ""The figure below shows an overall optimization workflow. ![overview](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/train-optimize-sd-intel/overview.png)\nThe resultant model is highly beneficial when running inference on devices with limited computational resources, such as client or edge CPUs. As it was mentioned, stacking Token Merging with quantization leads to an additional reduction in the inference latency. ![Image 1](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/train-optimize-sd-intel/image_torch.png)\n\nPyTorch FP32, Inference Speed: 230.5 seconds, Memory Footprint: 3.44 GB\n\n![Image 2](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/train-optimize-sd-intel/image_fp32.png)\n\nOpenVINO FP32, Inference Speed: 120 seconds (**1.9x**), Memory Footprint: 3.44 GB\n\n![Image 3](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/train-optimize-sd-intel/image_quantized.png)\n\nOpenVINO 8-bit, Inference Speed: 59 seconds (**3.9x**), Memory Footprint: 0.86 GB (**0.25x**)\n\n![Image 4](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/train-optimize-sd-intel/image_tome_quantized.png)\n\nToMe + OpenVINO 8-bit, Inference Speed: 44.6 seconds (**5.1x**), Memory Footprint: 0.86 GB (**0.25x**)\n\nResults of image generation [demo](https://huggingface.co/spaces/helenai/stable\\_diffusion) using different optimized models.""]","The blog post does not provide a specific latency value in milliseconds for fp16 optimization. However, it mentions that using fp16 can significantly reduce latency, potentially by almost half, and almost double throughput. For more detailed performance metrics, you may need to refer to specific benchmarks or experiments conducted with the models in question.",10.32
How can you share a Gradio demo publicly?,"[""Sharing demos with others[[sharing-demos-with-others]]\nNow that you've built a demo, you'll probably want to share it with others. Gradio demos\ncan be shared in two ways: using a \\*\\*\\*temporary share link\\*\\*\\* or \\*\\*\\*permanent hosting on Spaces\\*\\*\\*. We'll cover both of these approaches shortly. But before you share your demo, you may want to polish it up 💅. ### Polishing your Gradio demo:[[polishing-your-gradio-demo]]\n\n![Overview of a gradio interface](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter9/gradio-demo-overview.png)\n![Overview of a gradio interface](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter9/gradio-demo-overview-dark.png)\n\nTo add additional content to your demo, the `Interface` class supports some optional parameters:\n- `title`: you can give a title to your demo, which appears \\_above\\_ the input and output components. - `description`: you can give a description (in text, Markdown, or HTML) for the interface, which appears above the input and output components and below the title."", ""分享您的应用\n如何分享您的 Gradio 应用：\n1. [使用 share 参数分享演示](#sharing-demos)\n2. [在 HF Spaces 上托管](#hosting-on-hf-spaces)\n3. [嵌入托管的空间](#embedding-hosted-spaces)\n4. [使用 Web 组件嵌入](#embedding-with-web-components)\n5. [使用 API 页面](#api-page)\n6. [在页面上添加身份验证](#authentication)\n7. [访问网络请求](#accessing-the-network-request-directly)\n8. [在 FastAPI 中挂载](#mounting-within-another-fastapi-app)\n9. [安全性](#security-and-file-access)\n## 分享演示\n通过在 `launch()` 方法中设置 `share=True`，可以轻松公开分享 Gradio 演示。就像这样：\n```python\ndemo.launch(share=True)\n```\n这将生成一个公开的可分享链接，您可以将其发送给任何人！当您发送此链接时，对方用户可以在其浏览器中尝试模型。因为处理过程发生在您的设备上（只要您的设备保持开启！），您不必担心任何打包依赖项的问题。一个分享链接通常看起来像这样：\\*\\*XXXXX.gradio.app\\*\\*。尽管链接是通过 Gradio URL 提供的，但我们只是您本地服务器的代理，并不会存储通过您的应用发送的任何数据。\n但请记住，这些链接可以被公开访问，这意味着任何人都可以使用您的模型进行预测！因此，请确保不要通过您编写的函数公开任何敏感信息，也不要允许在您的设备上进行任何关键更改。如果您设置 `share=False`（默认值，在 colab 笔记本中除外），则只创建一个本地链接，可以通过[端口转发](https://www.ssh.com/ssh/tunneling/example)与特定用户共享。\n![](/assets/guides/sharing.svg)\n分享链接在 72 小时后过期。\n## 在 HF Spaces 上托管\n如果您想在互联网上获得您的 Gradio 演示的永久链接，请使用 Hugging Face Spaces。 [Hugging Face Spaces](http://huggingface.co/spaces/) 提供了免费托管您的机器学习模型的基础设施！\n在您创建了一个免费的 Hugging Face 账户后，有三种方法可以将您的 Gradio 应用部署到 Hugging Face Spaces：\n1."", ""These Gradio demos are hosted on [Hugging Face Spaces](https://huggingface.co/spaces) and are embedded using the standard \\ tag. ## How to Contribute a Guide\n1. Clone or fork this `gradio` repo\n2. Add a new markdown document with a descriptive title to the `/guides` folder\n3. Write your Guide in standard markdown! Embed Gradio demos wherever helpful\n4. Add a list of `related\\_spaces` at the top of the markdown document (see the previously linked Guides for how to do this)\n5. Add 3 `tags` at the top of the markdown document to help users find your guide (again, see the previously linked Guides for how to do this)\n6. Open a PR to have your guide reviewed\nThat's it! We're looking forward to reading your Guide 🥳"", ""--\ntitle: \""Showcase Your Projects in Spaces using Gradio\""\nthumbnail: /blog/assets/28\\_gradio-spaces/thumbnail.png\nauthors:\n- user: merve\n---\n# Showcase Your Projects in Spaces using Gradio\nIt's so easy to demonstrate a Machine Learning project thanks to [Gradio](https://gradio.app/). In this blog post, we'll walk you through:\n- the recent Gradio integration that helps you demo models from the Hub seamlessly with few lines of code leveraging the [Inference API](https://huggingface.co/inference-api). - how to use Hugging Face Spaces to host demos of your own models. ## Hugging Face Hub Integration in Gradio\nYou can demonstrate your models in the Hub easily. You only need to define the [Interface](https://gradio.app/docs#interface) that includes:\n- The repository ID of the model you want to infer with\n- A description and title\n- Example inputs to guide your audience\nAfter defining your Interface, just call `.launch()` and your demo will start running. You can do this in Colab, but if you want to share it with the community a great option is to use Spaces! Spaces are a simple, free way to host your ML demo apps in Python. To do so, you can create a repository at https://huggingface.co/new-space and select Gradio as the SDK. Once done, you can create a file called `app.py`, copy the code below, and your app will be up and running in a few seconds! ```python\nimport gradio as gr\ndescription = \""Story generation with GPT-2\""\ntitle = \""Generate your own story\""\nexamples = [[\""Adventurer is approached by a mysterious stranger in the tavern for a new quest.\""]]\ninterface = gr.Interface.load(\""huggingface/pranavpsv/gpt2-genre-story-generator\"",\ndescription=description,\nexamples=examples\n)\ninterface.launch()\n```\nYou can play with the Story Generation model [here](https://huggingface.co/spaces/merve/GPT-2-story-gen)\n![story-gen](assets/28\\_gradio-spaces/story-gen.png)\nUnder the hood, Gradio calls the Inference API which supports Transformers as well as other popular ML frameworks such as spaCy, SpeechBrain and Asteroid."", ""### What's Next? Keep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features). Or, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/). ## Questions? If you'd like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help. If you like Gradio, please leave us a ⭐ on GitHub! ## Open Source Stack\nGradio is built on top of many wonderful open-source libraries! [![huggingface](readme_files/huggingface_mini.svg)](https://huggingface.co)\n[![python](readme_files/python.svg)](https://www.python.org)\n[![fastapi](readme_files/fastapi.svg)](https://fastapi.tiangolo.com)\n[![encode](readme_files/encode.svg)](https://www.encode.io)\n[![svelte](readme_files/svelte.svg)](https://svelte.dev)\n[![vite](readme_files/vite.svg)](https://vitejs.dev)\n[![pnpm](readme_files/pnpm.svg)](https://pnpm.io)\n[![tailwind](readme_files/tailwind.svg)](https://tailwindcss.com)\n[![storybook](readme_files/storybook.svg)](https://storybook.js.org/)\n[![chromatic](readme_files/chromatic.svg)](https://www.chromatic.com/)\n## License\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository. ## Citation\nAlso check out the paper \\_[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019\\_, and please cite it if you use Gradio in your work.""]","您可以通过在 `launch()` 方法中设置 `share=True` 来轻松公开分享 Gradio 演示。示例如下：

```python
demo.launch(share=True)
```

这将生成一个公开的可分享链接，您可以将其发送给任何人！请注意，这些链接可以被公开访问，意味着任何人都可以使用您的模型进行预测。分享链接在 72 小时后过期。",Set `share=True` in the `launch()` method of the Gradio demo.
What is the Top 1 Accuracy of swsl_resnet18 on ImageNet?,"[""In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.\\*\nThis model was contributed by [adirik](https://huggingface.co/adirik). The original code can be found [here](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet). ## EfficientNetConfig\n[[autodoc]] EfficientNetConfig\n## EfficientNetImageProcessor\n[[autodoc]] EfficientNetImageProcessor\n- preprocess\n## EfficientNetModel\n[[autodoc]] EfficientNetModel\n- forward\n## EfficientNetForImageClassification\n[[autodoc]] EfficientNetForImageClassification\n- forward"", ""Validation and Benchmark Results\nThis folder contains validation and benchmark results for the models in this collection. Validation scores are currently only run for models with pretrained weights and ImageNet-1k heads, benchmark numbers are run for all. ## Datasets\nThere are currently results for the ImageNet validation set and 5 additional test / label sets. The test set results include rank and top-1/top-5 differences from clean validation. For the \""Real Labels\"", ImageNetV2, and Sketch test sets, the differences were calculated against the full 1000 class ImageNet-1k validation set. For both the Adversarial and Rendition sets, the differences were calculated against 'clean' runs on the ImageNet-1k validation set with the same 200 classes used in each test set respectively. ### ImageNet Validation - [`results-imagenet.csv`](results-imagenet.csv)\nThe standard 50,000 image ImageNet-1k validation set. Model selection during training utilizes this validation set, so it is not a true test set. Question: Does anyone have the official ImageNet-1k test set classification labels now that challenges are done? \\* Source: http://image-net.org/challenges/LSVRC/2012/index\n\\* Paper: \""ImageNet Large Scale Visual Recognition Challenge\"" - https://arxiv.org/abs/1409.0575\n### ImageNet-\""Real Labels\"" - [`results-imagenet-real.csv`](results-imagenet-real.csv)\nThe usual ImageNet-1k validation set with a fresh new set of labels intended to improve on mistakes in the original annotation process. \\* Source: https://github.com/google-research/reassessed-imagenet\n\\* Paper: \""Are we done with ImageNet?\"" - https://arxiv.org/abs/2006.07159\n### ImageNetV2 Matched Frequency - [`results-imagenetv2-matched-frequency.csv`](results-imagenetv2-matched-frequency.csv)\nAn ImageNet test set of 10,000 images sampled from new images roughly 10 years after the original. Care was taken to replicate the original ImageNet curation/sampling process. \\* Source: https://github.com/modestyachts/ImageNetV2\n\\* Paper: \""Do ImageNet Classifiers Generalize to ImageNet?\"" - https://arxiv.org/abs/1902.10811\n### ImageNet-Sketch - [`results-sketch.csv`](results-sketch.csv)\n50,000 non photographic (or photos of such) images (sketches, doodles, mostly monochromatic) covering all 1000 ImageNet classes. \\* Source: https://github.com/HaohanWang/ImageNet-Sketch\n\\* Paper: \""Learning Robust Global Representations by Penalizing Local Predictive Power\"" - https://arxiv.org/abs/1905.13549\n### ImageNet-Adversarial - [`results-imagenet-a.csv`](results-imagenet-a.csv)\nA collection of 7500 images covering 200 of the 1000 ImageNet classes. Images are naturally occurring adversarial examples that confuse typical ImageNet classifiers. This is a challenging dataset, your typical ResNet-50 will score 0% top-1. For clean validation with same 200 classes, see [`results-imagenet-a-clean.csv`](results-imagenet-a-clean.csv)\n\\* Source: https://github.com/hendrycks/natural-adv-examples\n\\* Paper: \""Natural Adversarial Examples\"" - https://arxiv.org/abs/1907.07174\n### ImageNet-Rendition - [`results-imagenet-r.csv`](results-imagenet-r.csv)\nRenditions of 200 ImageNet classes resulting in 30,000 images for testing robustness. For clean validation with same 200 classes, see [`results-imagenet-r-clean.csv`](results-imagenet-r-clean.csv)\n\\* Source: https://github.com/hendrycks/imagenet-r\n\\* Paper: \""The Many Faces of Robustness\"" - https://arxiv.org/abs/2006.16241\n### TODO\n\\* Explore adding a reduced version of ImageNet-C (Corruptions) and ImageNet-P (Perturbations) from https://github.com/hendrycks/robustness."", ""SWSL ResNet\n\\*\\*Residual Networks\\*\\*, or \\*\\*ResNets\\*\\*, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack [residual blocks](https://paperswithcode.com/method/residual-block) ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. The models in this collection utilise semi-weakly supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only. ## How do I use this model on an image? To load a pretrained model:\n```python\nimport timm\nmodel = timm.create\\_model('swsl\\_resnet18', pretrained=True)\nmodel.eval()\n```\nTo load and preprocess the image:\n```python\nimport urllib\nfrom PIL import Image\nfrom timm.data import resolve\\_data\\_config\nfrom timm.data.transforms\\_factory import create\\_transform\nconfig = resolve\\_data\\_config({}, model=model)\ntransform = create\\_transform(\\*\\*config)\nurl, filename = (\""https://github.com/pytorch/hub/raw/master/images/dog.jpg\"", \""dog.jpg\"")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\nTo get the model predictions:\n```python\nimport torch\nwith torch.no\\_grad():\nout = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n```\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename = (\""https://raw.githubusercontent.com/pytorch/hub/master/imagenet\\_classes.txt\"", \""imagenet\\_classes.txt\"")\nurllib.request.urlretrieve(url, filename)\nwith open(\""imagenet\\_classes.txt\"", \""r\"") as f:\ncategories = [s.strip() for s in f.readlines()]\n# Print top categories per image\ntop5\\_prob, top5\\_catid = torch.topk(probabilities, 5)\nfor i in range(top5\\_prob.size(0)):\nprint(categories[top5\\_catid[i]], top5\\_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\nReplace the model name with the variant you want to use, e.g. `swsl\\_resnet18`. You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature\\_extraction/), just change the name of the model you want to use. ## How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). ```python\nmodel = timm.create\\_model('swsl\\_resnet18', pretrained=True, num\\_classes=NUM\\_FINETUNE\\_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset."", ""This is generally known as \""ResNet v1.5\"". ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision. The abstract from the paper is the following:\n\\*Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\\*\nThe figure below illustrates the architecture of ResNet."", ""ie approx 180-200 for ResNe(X)t50, and 220+ for larger. Increase batch size and LR proportionally for better GPUs or with AMP enabled. These params were for 2 1080Ti cards:\n`./distributed\\_train.sh 2 /imagenet/ --model seresnext26t\\_32x4d --lr 0.1 --warmup-epochs 5 --epochs 160 --weight-decay 1e-4 --sched cosine --reprob 0.4 --remode pixel -b 112`\n## EfficientNet-B3 with RandAugment - 81.5 top-1, 95.7 top-5\nThe training of this model started with the same command line as EfficientNet-B2 w/ RA above. After almost three weeks of training the process crashed. The results weren't looking amazing so I resumed the training several times with tweaks to a few params (increase RE prob, decrease rand-aug, increase ema-decay). Nothing looked great. I ended up averaging the best checkpoints from all restarts. The result is mediocre at default res/crop but oddly performs much better with a full image test crop of 1.0. ## EfficientNet-B0 with RandAugment - 77.7 top-1, 95.3 top-5\n[Michael Klachko](https://github.com/michaelklachko) achieved these results with the command line for B2 adapted for larger batch size, with the recommended B0 dropout rate of 0.2. `./distributed\\_train.sh 2 /imagenet/ --model efficientnet\\_b0 -b 384 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-path 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .048`\n## ResNet50 with JSD loss and RandAugment (clean + 2x RA augs) - 79.04 top-1, 94.39 top-5\nTrained on two older 1080Ti cards, this took a while.""]","The Top 1 Accuracy of swsl_resnet18 on ImageNet is not explicitly mentioned in the provided context. However, it is noted that the models in this collection utilize semi-weakly supervised learning to improve performance. For specific accuracy metrics, you may need to refer to the model's documentation or benchmark results directly.",73.28%
What is the license for the weights of the SWSL ResNet models?,"[""SWSL ResNet\n\\*\\*Residual Networks\\*\\*, or \\*\\*ResNets\\*\\*, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack [residual blocks](https://paperswithcode.com/method/residual-block) ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. The models in this collection utilise semi-weakly supervised learning to improve the performance of the model. The approach brings important gains to standard architectures for image, video and fine-grained classification. Please note the CC-BY-NC 4.0 license on theses weights, non-commercial use only. ## How do I use this model on an image? To load a pretrained model:\n```python\nimport timm\nmodel = timm.create\\_model('swsl\\_resnet18', pretrained=True)\nmodel.eval()\n```\nTo load and preprocess the image:\n```python\nimport urllib\nfrom PIL import Image\nfrom timm.data import resolve\\_data\\_config\nfrom timm.data.transforms\\_factory import create\\_transform\nconfig = resolve\\_data\\_config({}, model=model)\ntransform = create\\_transform(\\*\\*config)\nurl, filename = (\""https://github.com/pytorch/hub/raw/master/images/dog.jpg\"", \""dog.jpg\"")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\nTo get the model predictions:\n```python\nimport torch\nwith torch.no\\_grad():\nout = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n```\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename = (\""https://raw.githubusercontent.com/pytorch/hub/master/imagenet\\_classes.txt\"", \""imagenet\\_classes.txt\"")\nurllib.request.urlretrieve(url, filename)\nwith open(\""imagenet\\_classes.txt\"", \""r\"") as f:\ncategories = [s.strip() for s in f.readlines()]\n# Print top categories per image\ntop5\\_prob, top5\\_catid = torch.topk(probabilities, 5)\nfor i in range(top5\\_prob.size(0)):\nprint(categories[top5\\_catid[i]], top5\\_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\nReplace the model name with the variant you want to use, e.g. `swsl\\_resnet18`. You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature\\_extraction/), just change the name of the model you want to use. ## How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). ```python\nmodel = timm.create\\_model('swsl\\_resnet18', pretrained=True, num\\_classes=NUM\\_FINETUNE\\_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset."", ""I've taken care to make sure any third party code included or adapted has compatible (permissive) licenses such as MIT, BSD, etc. I've made an effort to avoid any GPL / LGPL conflicts. That said, it is your responsibility to ensure you comply with licenses here and conditions of any dependent licenses. Where applicable, I've linked the sources/references for various components in docstrings. If you think I've missed anything please create an issue. ### Pretrained Weights\nSo far all of the pretrained weights available here are pretrained on ImageNet with a select few that have some additional pretraining (see extra note below). ImageNet was released for non-commercial research purposes only (https://image-net.org/download). It's not clear what the implications of that are for the use of pretrained weights from that dataset. Any models I have trained with ImageNet are done for research purposes and one should assume that the original dataset license applies to the weights. It's best to seek legal advice if you intend to use the pretrained weights in a commercial product. #### Pretrained on more than ImageNet\nSeveral weights included or references here were pretrained with proprietary datasets that I do not have access to. These include the Facebook WSL, SSL, SWSL ResNe(Xt) and the Google Noisy Student EfficientNet models. The Facebook models have an explicit non-commercial license (CC-BY-NC 4.0, https://github.com/facebookresearch/semi-supervised-ImageNet1K-models, https://github.com/facebookresearch/WSL-Images). The Google models do not appear to have any restriction beyond the Apache 2.0 license (and ImageNet concerns). In either case, you should contact Facebook or Google with any questions. ## Citing\n### BibTeX\n```bibtex\n@misc{rw2019timm,\nauthor = {Ross Wightman},\ntitle = {PyTorch Image Models},\nyear = {2019},\npublisher = {GitHub},\njournal = {GitHub repository},\ndoi = {10.5281/zenodo.4414861},\nhowpublished = {\\url{https://github.com/rwightman/pytorch-image-models}}\n}\n```\n### Latest DOI\n[![DOI](https://zenodo.org/badge/168799526.svg)](https://zenodo.org/badge/latestdoi/168799526)"", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ResNet\n## Overview\nThe ResNet model was proposed in [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun. Our implementation follows the small changes made by [Nvidia](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet\\_50\\_v1\\_5\\_for\\_pytorch), we apply the `stride=2` for downsampling in bottleneck's `3x3` conv and not in the first `1x1`."", ""No-license models challenge\n## Context\nThe Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open\\_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable information, including a `license` tag. In this challenge, we will focus on models that have no license defined but that have a `LICENSE` file in the repo. These are models for which the author actually cares about the model license but didn't make it searchable by authors."", ""There are model checkpoints in different sizes and open-source or [open RAIL](https://huggingface.co/blog/open\\_rail) licenses for both, except for [Codegen fine-tuned on instruction](https://huggingface.co/Salesforce/codegen25-7b-instruct). The Hugging Face Hub also hosts various models fine-tuned for instruction or chat use. They come in various styles and sizes depending on your needs. - [MPT-30B-Chat](https://huggingface.co/mosaicml/mpt-30b-chat), by Mosaic ML, uses the CC-BY-NC-SA license, which does not allow commercial use. However, [MPT-30B-Instruct](https://huggingface.co/mosaicml/mpt-30b-instruct) uses CC-BY-SA 3.0, which can be used commercially. - [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) and [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) both use the Apache 2.0 license, so commercial use is also permitted. - Another popular family of models is OpenAssistant, some of which are built on Meta's Llama model using a custom instruction-tuning dataset. Since the original Llama model can only be used for research, the OpenAssistant checkpoints built on Llama don’t have full open-source licenses. However, there are OpenAssistant models built on open-source models like [Falcon](https://huggingface.co/models?search=openassistant/falcon) or [pythia](https://huggingface.co/models?search=openassistant/pythia) that use permissive licenses. - [StarChat Beta](https://huggingface.co/HuggingFaceH4/starchat-beta) is the instruction fine-tuned version of StarCoder, and has BigCode Open RAIL-M v1 license, which allows commercial use.""]","The weights of the SWSL ResNet models are under the CC-BY-NC 4.0 license, which allows for non-commercial use only.",CC-BY-NC 4.0 license
Who proposed Consistency Models?,"[""Latent Consistency Distillation Example:\n[Latent Consistency Models (LCMs)](https://arxiv.org/abs/2310.04378) is a method to distill a latent diffusion model to enable swift inference with minimal steps. This example demonstrates how to use latent consistency distillation to distill stable-diffusion-v1.5 for inference with few timesteps. ## Full model distillation\n### Running locally with PyTorch\n#### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install -e . ```\nThen cd in the example folder and run\n```bash\npip install -r requirements.txt\n```\nAnd initialize an [🤗 Accelerate](https://github.com/huggingface/accelerate/) environment with:\n```bash\naccelerate config\n```\nOr for a default accelerate configuration without answering questions about your environment\n```bash\naccelerate config default\n```\nOr if your environment doesn't support an interactive shell e.g. a notebook\n```python\nfrom accelerate.utils import write\\_basic\\_config\nwrite\\_basic\\_config()\n```\nWhen running `accelerate config`, if we specify torch compile mode to True there can be dramatic speedups."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Consistency Models\nConsistency Models were proposed in [Consistency Models](https://huggingface.co/papers/2303.01469) by Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. The abstract from the paper is:\n\\*Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.\\*\nThe original codebase can be found at [openai/consistency\\_models](https://github.com/openai/consistency\\_models), and additional checkpoints are available at [openai](https://huggingface.co/openai)."", ""Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference. Project Page: [this https URL](https://latent-consistency-models.github.io/).\\*\nA demo for the [SimianLuo/LCM\\_Dreamshaper\\_v7](https://huggingface.co/SimianLuo/LCM\\_Dreamshaper\\_v7) checkpoint can be found [here](https://huggingface.co/spaces/SimianLuo/Latent\\_Consistency\\_Model). The pipelines were contributed by [luosiallen](https://luosiallen.github.io/), [nagolinc](https://github.com/nagolinc), and [dg845](https://github.com/dg845). ## LatentConsistencyModelPipeline\n[[autodoc]] LatentConsistencyModelPipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_freeu\n- disable\\_freeu\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n## LatentConsistencyModelImg2ImgPipeline\n[[autodoc]] LatentConsistencyModelImg2ImgPipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_freeu\n- disable\\_freeu\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Latent Consistency Model Multistep Scheduler\n## Overview\nMultistep and onestep scheduler (Algorithm 3) introduced alongside latent consistency models in the paper [Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference](https://arxiv.org/abs/2310.04378) by Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. This scheduler should be able to generate good samples from [`LatentConsistencyModelPipeline`] in 1-8 steps. ## LCMScheduler\n[[autodoc]] LCMScheduler"", ""Consistency Decoder\nConsistency decoder can be used to decode the latents from the denoising UNet in the [`StableDiffusionPipeline`]. This decoder was introduced in the [DALL-E 3 technical report](https://openai.com/dall-e-3). The original codebase can be found at [openai/consistencydecoder](https://github.com/openai/consistencydecoder). Inference is only supported for 2 iterations as of now. The pipeline could not have been contributed without the help of [madebyollin](https://github.com/madebyollin) and [mrsteyk](https://github.com/mrsteyk) from [this issue](https://github.com/openai/consistencydecoder/issues/1). ## ConsistencyDecoderVAE\n[[autodoc]] ConsistencyDecoderVAE\n- all\n- decode""]","Consistency Models were proposed by Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.","Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever"
What is the Top 1 Accuracy of tv_resnet152 on ImageNet?,"[""Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.\\*\n## UNet1DModel\n[[autodoc]] UNet1DModel\n## UNet1DOutput\n[[autodoc]] models.unet\\_1d.UNet1DOutput"", ""Validation and Benchmark Results\nThis folder contains validation and benchmark results for the models in this collection. Validation scores are currently only run for models with pretrained weights and ImageNet-1k heads, benchmark numbers are run for all. ## Datasets\nThere are currently results for the ImageNet validation set and 5 additional test / label sets. The test set results include rank and top-1/top-5 differences from clean validation. For the \""Real Labels\"", ImageNetV2, and Sketch test sets, the differences were calculated against the full 1000 class ImageNet-1k validation set. For both the Adversarial and Rendition sets, the differences were calculated against 'clean' runs on the ImageNet-1k validation set with the same 200 classes used in each test set respectively. ### ImageNet Validation - [`results-imagenet.csv`](results-imagenet.csv)\nThe standard 50,000 image ImageNet-1k validation set. Model selection during training utilizes this validation set, so it is not a true test set. Question: Does anyone have the official ImageNet-1k test set classification labels now that challenges are done? \\* Source: http://image-net.org/challenges/LSVRC/2012/index\n\\* Paper: \""ImageNet Large Scale Visual Recognition Challenge\"" - https://arxiv.org/abs/1409.0575\n### ImageNet-\""Real Labels\"" - [`results-imagenet-real.csv`](results-imagenet-real.csv)\nThe usual ImageNet-1k validation set with a fresh new set of labels intended to improve on mistakes in the original annotation process. \\* Source: https://github.com/google-research/reassessed-imagenet\n\\* Paper: \""Are we done with ImageNet?\"" - https://arxiv.org/abs/2006.07159\n### ImageNetV2 Matched Frequency - [`results-imagenetv2-matched-frequency.csv`](results-imagenetv2-matched-frequency.csv)\nAn ImageNet test set of 10,000 images sampled from new images roughly 10 years after the original. Care was taken to replicate the original ImageNet curation/sampling process. \\* Source: https://github.com/modestyachts/ImageNetV2\n\\* Paper: \""Do ImageNet Classifiers Generalize to ImageNet?\"" - https://arxiv.org/abs/1902.10811\n### ImageNet-Sketch - [`results-sketch.csv`](results-sketch.csv)\n50,000 non photographic (or photos of such) images (sketches, doodles, mostly monochromatic) covering all 1000 ImageNet classes. \\* Source: https://github.com/HaohanWang/ImageNet-Sketch\n\\* Paper: \""Learning Robust Global Representations by Penalizing Local Predictive Power\"" - https://arxiv.org/abs/1905.13549\n### ImageNet-Adversarial - [`results-imagenet-a.csv`](results-imagenet-a.csv)\nA collection of 7500 images covering 200 of the 1000 ImageNet classes. Images are naturally occurring adversarial examples that confuse typical ImageNet classifiers. This is a challenging dataset, your typical ResNet-50 will score 0% top-1. For clean validation with same 200 classes, see [`results-imagenet-a-clean.csv`](results-imagenet-a-clean.csv)\n\\* Source: https://github.com/hendrycks/natural-adv-examples\n\\* Paper: \""Natural Adversarial Examples\"" - https://arxiv.org/abs/1907.07174\n### ImageNet-Rendition - [`results-imagenet-r.csv`](results-imagenet-r.csv)\nRenditions of 200 ImageNet classes resulting in 30,000 images for testing robustness. For clean validation with same 200 classes, see [`results-imagenet-r-clean.csv`](results-imagenet-r-clean.csv)\n\\* Source: https://github.com/hendrycks/imagenet-r\n\\* Paper: \""The Many Faces of Robustness\"" - https://arxiv.org/abs/2006.16241\n### TODO\n\\* Explore adding a reduced version of ImageNet-C (Corruptions) and ImageNet-P (Perturbations) from https://github.com/hendrycks/robustness."", ""In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.\\*\nThis model was contributed by [adirik](https://huggingface.co/adirik). The original code can be found [here](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet). ## EfficientNetConfig\n[[autodoc]] EfficientNetConfig\n## EfficientNetImageProcessor\n[[autodoc]] EfficientNetImageProcessor\n- preprocess\n## EfficientNetModel\n[[autodoc]] EfficientNetModel\n- forward\n## EfficientNetForImageClassification\n[[autodoc]] EfficientNetForImageClassification\n- forward"", ""This is generally known as \""ResNet v1.5\"". ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision. The abstract from the paper is the following:\n\\*Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\\*\nThe figure below illustrates the architecture of ResNet."", ""It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU. To learn more read the [model card](https://github.com/onnx/models/tree/main/vision/classification/efficientnet-lite4)\nHere we walk through setting up a example demo for EfficientNet-Lite4 using Gradio\nFirst we import our dependencies and download and load the efficientnet-lite4 model from the onnx model zoo. Then load the labels from the labels\\_map.txt file. We then setup our preprocessing functions, load the model for inference, and setup the inference function. Finally, the inference function is wrapped into a gradio interface for a user to interact with. See the full code below. ```python\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport cv2\nimport json\nimport gradio as gr\nfrom huggingface\\_hub import hf\\_hub\\_download\nfrom onnx import hub\nimport onnxruntime as ort\n# loads ONNX model from ONNX Model Zoo\nmodel = hub.load(\""efficientnet-lite4\"")\n# loads the labels text file\nlabels = json.load(open(\""labels\\_map.txt\"", \""r\""))\n# sets image file dimensions to 224x224 by resizing and cropping image from center\ndef pre\\_process\\_edgetpu(img, dims):\noutput\\_height, output\\_width, \\_ = dims\nimg = resize\\_with\\_aspectratio(img, output\\_height, output\\_width, inter\\_pol=cv2.INTER\\_LINEAR)\nimg = center\\_crop(img, output\\_height, output\\_width)\nimg = np.asarray(img, dtype='float32')\n# converts jpg pixel value from [0 - 255] to float array [-1.0 - 1.0]\nimg -= [127.0, 127.0, 127.0]\nimg /= [128.0, 128.0, 128.0]\nreturn img\n# resizes the image with a proportional scale\ndef resize\\_with\\_aspectratio(img, out\\_height, out\\_width, scale=87.5, inter\\_pol=cv2.INTER\\_LINEAR):\nheight, width, \\_ = img.shape\nnew\\_height = int(100. \\* out\\_height / scale)\nnew\\_width = int(100. \\* out\\_width / scale)\nif height > width:\nw = new\\_width\nh = int(new\\_height \\* height / width)\nelse:\nh = new\\_height\nw = int(new\\_width \\* width / height)\nimg = cv2.resize(img, (w, h), interpolation=inter\\_pol)\nreturn img\n# crops the image around the center based on given height and width\ndef center\\_crop(img, out\\_height, out\\_width):\nheight, width, \\_ = img.shape\nleft = int((width - out\\_width) / 2)\nright = int((width + out\\_width) / 2)\ntop = int((height - out\\_height) / 2)\nbottom = int((height + out\\_height) / 2)\nimg = img[top:bottom, left:right]\nreturn img\nsess = ort.InferenceSession(model)\ndef inference(img):\nimg = cv2.imread(img)\nimg = cv2.cvtColor(img, cv2.COLOR\\_BGR2RGB)\nimg = pre\\_process\\_edgetpu(img, (224, 224, 3))\nimg\\_batch = np.expand\\_dims(img, axis=0)\nresults = sess.run([\""Softmax:0\""], {\""images:0\"": img\\_batch})[0]\nresult = reversed(results[0].argsort()[-5:])\nresultdic = {}\nfor r in result:\nresultdic[labels[str(r)]] = float(results[0][r])\nreturn resultdic\ntitle = \""EfficientNet-Lite4\""\ndescription = \""EfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite model.""]","The Top 1 Accuracy of tv_resnet152 on ImageNet is 3.57% error, which corresponds to a Top 1 accuracy of 96.43%. This result was achieved by an ensemble of residual networks and won the 1st place on the ILSVRC 2015 classification task.",78.32%
Under which license is the HuggingFace Transformers library released?,"[""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Research projects\nThis folder contains various research projects using 🤗 Transformers. They are not maintained and require a specific\nversion of 🤗 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice. If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder."", ""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# How To Request Support\nThis is an Open Source Project so please be mindful that like in any other project of this kind there is no obligation to answer all requests for help. However, we want to encourage you to ask for help whenever you think it's needed! We are happy about every question we get because it allows us to better understand your needs, possible misunderstandings, and most importantly a way for you to help us make this library better. That being said, this document's main purpose is to provide guidelines at how you can formulate your requests to increase your chances to be understood and to get support. There are two main venues to receive support: [the forums](https://discuss.huggingface.co/) and [the GitHub issues](https://github.com/huggingface/transformers/issues). ## The Forums\n[The user forums](https://discuss.huggingface.co/) are supported by the wide community of the library users and backed up by developers when needed. If you have a difficulty with deploying this library or some questions, or you'd like to discuss a new feature, please first consider discussing those things at the forums. Only when you feel your subject matter has been crystalized and you still need support from the library developers do proceed to file an [issue](https://github.com/huggingface/transformers/issues). In particular all \""Please explain\"" questions or objectively very user-specific feature requests belong to the forums. Here are some example of such questions:\n\\* \""I would like to use a BertModel within a RL-Agent for a customer support service. How can I use a BertForMaskedLM in my ChatBotModel?\""\n\\* \""Could you please explain why T5 has no positional embedding matrix under T5Model?\""\n\\* \""How should I set my generation parameters for translation?\""\n\\* \""How to train T5 on De->En translation?\""\n## The GitHub Issues\nEverything which hints at a bug should be opened as an [issue](https://github.com/huggingface/transformers/issues)."", ""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\n\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\n\n#### **English** | [简体中文](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [繁體中文](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [한국어](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Español](https://github.com/huggingface/transformers/blob/main/README_es.md) | [日本語](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [हिन्दी](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [Русский](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [Рortuguês](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [తెలుగు](https://github.com/huggingface/transformers//blob/main/README_te.md) |\n\n### State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n\n### \n\n🤗 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction"", ""The Hugging Face Hub[[the-hugging-face-hub]]\nThe [Hugging Face Hub](https://huggingface.co/) –- our main website –- is a central platform that enables anyone to discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with more than 10,000 publicly available. We'll focus on the models in this chapter, and take a look at the datasets in Chapter 5. The models in the Hub are not limited to 🤗 Transformers or even NLP. There are models from [Flair](https://github.com/flairNLP/flair) and [AllenNLP](https://github.com/allenai/allennlp) for NLP, [Asteroid](https://github.com/asteroid-team/asteroid) and [pyannote](https://github.com/pyannote/pyannote-audio) for speech, and [timm](https://github.com/rwightman/pytorch-image-models) for vision, to name a few. Each of these models is hosted as a Git repository, which allows versioning and reproducibility. Sharing a model on the Hub means opening it up to the community and making it accessible to anyone looking to easily use it, in turn eliminating their need to train a model on their own and simplifying sharing and usage. Additionally, sharing a model on the Hub automatically deploys a hosted Inference API for that model. Anyone in the community is free to test it out directly on the model's page, with custom inputs and appropriate widgets. The best part is that sharing and using any public model on the Hub is completely free! [Paid plans](https://huggingface.co/pricing) also exist if you wish to share models privately. The video below shows how to navigate the Hub. Having a huggingface.co account is required to follow along this part, as we'll be creating and managing repositories on the Hugging Face Hub: [create an account](https://huggingface.co/join)""]","The HuggingFace Transformers library is released under the Apache License, Version 2.0.","Apache License, Version 2.0"
What is the model structure of GPTSAN?,"[""It has a model structure of Prefix-LM. It works as a shifted Masked Language Model for Prefix Input tokens. Un-prefixed inputs behave like normal generative models. The Spout vector is a GPTSAN specific input. Spout is pre-trained with random inputs, but you can specify a class of text or an arbitrary vector during fine-tuning. This allows you to indicate the tendency of the generated text. GPTSAN has a sparse Feed Forward based on Switch-Transformer. You can also add other layers and train them partially. See the original GPTSAN repository for details. ### Prefix-LM Model\nGPTSAN has the structure of the model named Prefix-LM in the `T5` paper. (The original GPTSAN repository calls it `hybrid`)\nIn GPTSAN, the `Prefix` part of Prefix-LM, that is, the input position that can be referenced by both tokens, can be specified with any length. Arbitrary lengths can also be specified differently for each batch. This length applies to the text entered in `prefix\\_text` for the tokenizer. The tokenizer returns the mask of the `Prefix` part of Prefix-LM as `token\\_type\\_ids`. The model treats the part where `token\\_type\\_ids` is 1 as a `Prefix` part, that is, the input can refer to both tokens before and after. ## Usage tips\nSpecifying the Prefix part is done with a mask passed to self-attention. When token\\_type\\_ids=None or all zero, it is equivalent to regular causal mask\nfor example:\n>>> x\\_token = tokenizer(\""ｱｲｳｴ\"")\ninput\\_ids: | SOT | SEG | ｱ | ｲ | ｳ | ｴ |\ntoken\\_type\\_ids: | 1 | 0 | 0 | 0 | 0 | 0 |\nprefix\\_lm\\_mask:\nSOT | 1 0 0 0 0 0 |\nSEG | 1 1 0 0 0 0 |\nｱ | 1 1 1 0 0 0 |\nｲ | 1 1 1 1 0 0 |\nｳ | 1 1 1 1 1 0 |\nｴ | 1 1 1 1 1 1 |\n>>> x\\_token = tokenizer(\""\"", prefix\\_text=\""ｱｲｳｴ\"")\ninput\\_ids: | SOT | ｱ | ｲ | ｳ | ｴ | SEG |\ntoken\\_type\\_ids: | 1 | 1 | 1 | 1 | 1 | 0 |\nprefix\\_lm\\_mask:\nSOT | 1 1 1 1 1 0 |\nｱ | 1 1 1 1 1 0 |\nｲ | 1 1 1 1 1 0 |\nｳ | 1 1 1 1 1 0 |\nｴ | 1 1 1 1 1 0 |\nSEG | 1 1 1 1 1 1 |\n>>> x\\_token = tokenizer(\""ｳｴ\"", prefix\\_text=\""ｱｲ\"")\ninput\\_ids: | SOT | ｱ | ｲ | SEG | ｳ | ｴ |\ntoken\\_type\\_ids: | 1 | 1 | 1 | 0 | 0 | 0 |\nprefix\\_lm\\_mask:\nSOT | 1 1 1 0 0 0 |\nｱ | 1 1 1 0 0 0 |\nｲ | 1 1 1 0 0 0 |\nSEG | 1 1 1 1 0 0 |\nｳ | 1 1 1 1 1 0 |\nｴ | 1 1 1 1 1 1 |\n### Spout Vector\nA Spout Vector is a special vector for controlling text generation."", ""GPT is one of them. This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/openai/finetune-transformer-lm). ## Usage tips\n- GPT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\nthe left. - GPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next\ntoken in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be\nobserved in the \\*run\\_generation.py\\* example script. Note:\nIf you want to reproduce the original tokenization process of the \\*OpenAI GPT\\* paper, you will need to install `ftfy`\nand `SpaCy`:\n```bash\npip install spacy ftfy==4.4.3\npython -m spacy download en\n```\nIf you don't install `ftfy` and `SpaCy`, the [`OpenAIGPTTokenizer`] will default to tokenize\nusing BERT's `BasicTokenizer` followed by Byte-Pair Encoding (which should be fine for most usage, don't worry)."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# GPTSAN-japanese\n## Overview\nThe GPTSAN-japanese model was released in the repository by Toshiyuki Sakamoto (tanreinama). GPTSAN is a Japanese language model using Switch Transformer. It has the same structure as the model introduced as Prefix LM\nin the T5 paper, and support both Text Generation and Masked Language Modeling tasks. These basic tasks similarly can\nfine-tune for translation or summarization. ### Usage example\nThe `generate()` method can be used to generate text using GPTSAN-Japanese model. ```python\n>>> from transformers import AutoModel, AutoTokenizer\n>>> import torch\n>>> tokenizer = AutoTokenizer.from\\_pretrained(\""Tanrei/GPTSAN-japanese\"")\n>>> model = AutoModel.from\\_pretrained(\""Tanrei/GPTSAN-japanese\"").cuda()\n>>> x\\_tok = tokenizer(\""は、\"", prefix\\_text=\""織田信長\"", return\\_tensors=\""pt\"")\n>>> torch.manual\\_seed(0)\n>>> gen\\_tok = model.generate(x\\_tok.input\\_ids.cuda(), token\\_type\\_ids=x\\_tok.token\\_type\\_ids.cuda(), max\\_new\\_tokens=20)\n>>> tokenizer.decode(gen\\_tok[0])\n'織田信長は、2004年に『戦国BASARA』のために、豊臣秀吉'\n```\n## GPTSAN Features\nGPTSAN has some unique features."", ""It is possible to achieve the goal of protecting the model owner’s IP while still maintaining the privacy of the user's data. This demo shows that an LLM model implemented in FHE maintains the quality of the original model’s predictions. To do this, it’s necessary to adapt the [GPT2](https://huggingface.co/gpt2) implementation from the Hugging Face [transformers library](https://github.com/huggingface/transformers), reworking sections of the inference using Concrete-Python, which enables the conversion of Python functions into their FHE equivalents. ![Figure 1. GPT2 Architecture. Source: https://en.wikipedia.org/wiki/GPT-2](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/encrypted-llm/gpt2\\_architecture.png)\nFigure 1 shows the GPT2 architecture which has a repeating structure: a series of multi-head attention (MHA) layers applied successively. Each MHA layer projects the inputs using the model weights, computes the attention mechanism, and re-projects the output of the attention into a new tensor."", ""\""EleutherAI/gpt-j-6B\"",\n... revision=\""float16\"",\n... torch\\_dtype=torch.float16,\n... ).to(device)\n```\n- The model should fit on 16GB GPU for inference. For training/fine-tuning it would take much more GPU RAM. Adam\noptimizer for example makes four copies of the model: model, gradients, average and squared average of the gradients. So it would need at least 4x model size GPU memory, even with mixed precision as gradient updates are in fp32. This\nis not including the activations and data batches, which would again require some more GPU RAM. So one should explore\nsolutions such as DeepSpeed, to train/fine-tune the model. Another option is to use the original codebase to\ntrain/fine-tune the model on TPU and then convert the model to Transformers format for inference. Instructions for\nthat could be found [here](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto\\_finetune.md)\n- Although the embedding matrix has a size of 50400, only 50257 entries are used by the GPT-2 tokenizer. These extra\ntokens are added for the sake of efficiency on TPUs. To avoid the mismatch between embedding matrix size and vocab\nsize, the tokenizer for [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) contains 143 extra tokens\n`<|extratoken\\_1|>... <|extratoken\\_143|>`, so the `vocab\\_size` of tokenizer also becomes 50400. ## Usage examples\nThe [`~generation.GenerationMixin.generate`] method can be used to generate text using GPT-J\nmodel.""]","The model structure of GPTSAN is based on the Prefix-LM model introduced in the T5 paper. It functions as a shifted Masked Language Model for Prefix Input tokens, where un-prefixed inputs behave like normal generative models. The Prefix part of Prefix-LM can be specified with any length, and arbitrary lengths can be set differently for each batch. The model uses a sparse Feed Forward based on Switch-Transformer and allows for the addition of other layers that can be trained partially.",Prefix-LM
Which LCM LoRA model is derived from the SDXL 1.0 base?,"[""3. Use the LoRA with any SDXL diffusion model and the LCM scheduler; bingo! You get high-quality inference in just a few steps. For more details on the process, please [download our paper](https://huggingface.co/latent-consistency/lcm-lora-sdxl/resolve/main/LCM-LoRA-Technical-Report.pdf). ## Why does this matter? Fast inference of Stable Diffusion and SDXL enables new use-cases and workflows. To name a few:\n- \\*\\*Accessibility\\*\\*: generative tools can be used effectively by more people, even if they don’t have access to the latest hardware. - \\*\\*Faster iteration\\*\\*: get more images and multiple variants in a fraction of the time! This is great for artists and researchers; whether for personal or commercial use. - Production workloads may be possible on different accelerators, including CPUs. - Cheaper image generation services. To gauge the speed difference we are talking about, generating a single 1024x1024 image on an M1 Mac with SDXL (base) takes about a minute. Using the LCM LoRA, we get great results in just ~6s (4 steps)."", ""The details in the final image are amazing, but it took 50 steps to get there. ### LCM LoRAs with other models\nThis technique also works for any other fine-tuned SDXL or Stable Diffusion model. To demonstrate, let's see how to run inference on [`collage-diffusion`](https://huggingface.co/wavymulder/collage-diffusion), a model fine-tuned from [Stable Diffusion v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5) using Dreambooth. The code is similar to the one we saw in the previous examples. We load the fine-tuned model, and then the LCM LoRA suitable for Stable Diffusion v1.5. ```py\nfrom diffusers import DiffusionPipeline, LCMScheduler\nimport torch\nmodel\\_id = \""wavymulder/collage-diffusion\""\nlcm\\_lora\\_id = \""latent-consistency/lcm-lora-sdv1-5\""\npipe = DiffusionPipeline.from\\_pretrained(model\\_id, variant=\""fp16\"")\npipe.scheduler = LCMScheduler.from\\_config(pipe.scheduler.config)\npipe.load\\_lora\\_weights(lcm\\_lora\\_id)\npipe.to(device=\""cuda\"", dtype=torch.float16)\nprompt = \""collage style kid sits looking at the night sky, full of stars\""\ngenerator = torch.Generator(device=pipe.device).manual\\_seed(1337)\nimages = pipe(\nprompt=prompt,\ngenerator=generator,\nnegative\\_prompt=negative\\_prompt,\nnum\\_inference\\_steps=4,\nguidance\\_scale=1,\n).images[0]\nimages\n```\n\n![LCM LoRA technique with a Dreambooth Stable Diffusion v1.5 model, allowing 4-step inference.](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/lcm-lora/collage.png?download=true)  \n*LCM LoRA technique with a Dreambooth Stable Diffusion v1.5 model, allowing 4-step inference.*\n\n### Full Diffusers Integration\nThe integration of LCM in `diffusers` makes it possible to take advantage of many features and workflows that are part of the diffusers toolbox."", ""## Benefits\nWe have approximately \\*\\*2500\\*\\* distinct public LoRAs on the Hub. The vast majority (\\*\\*~92%\\*\\*) of them are LoRAs based on the [Stable Diffusion XL Base 1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) model. Before this mutualization, this would have meant deploying a dedicated service for all of them (eg. for all the yellow merged matrices in the diagram above); releasing + reserving at least one new GPU. The time to spawn the service and have it ready to serve requests for a specific model is approximately \\*\\*25s\\*\\*, then on top of this you have the inference time (\\*\\*~10s\\*\\* for a 1024x1024 SDXL inference diffusion with 25 inference steps on an A10G). If an adapter is only occasionally requested, its service gets stopped to free resources preempted by others. If you were requesting a LoRA that was not so popular, even if it was based on the SDXL model like the vast majority of adapters found on the Hub so far, it would have required \\*\\*35s\\*\\* to warm it up and get an answer on the first request (the following ones would have taken the inference time, eg. \\*\\*10s\\*\\*). Now: request time has decreased from 35s to 13s since adapters will use only a few distinct \""blue\"" base models (like 2 significant ones for Diffusion). Even if your adapter is not so popular, there is a good chance that its \""blue\"" service is already warmed up."", ""The core idea with LCM LoRA is to train just a small number of adapters, [known as LoRA layers](https://huggingface.co/docs/peft/conceptual\\_guides/lora), instead of the full model. The resulting LoRAs can then be applied to any fine-tuned version of the model without having to distil them separately. If you are itching to see how this looks in practice, just jump to the [next section](#fast-inference-with-sdxl-lcm-loras) to play with the inference code. If you want to train your own LoRAs, this is the process you’d use:\n1. Select an available teacher model from the Hub. For example, you can use [SDXL (base)](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0), or any fine-tuned or dreamboothed version you like. 2. [Train a LCM LoRA](#how-to-train-lcm-models-and-loras) on the model. LoRA is a type of performance-efficient fine-tuning, or PEFT, that is much cheaper to accomplish than full model fine-tuning. For additional details on PEFT, please check [this blog post](https://huggingface.co/blog/peft) or [the diffusers LoRA documentation](https://huggingface.co/docs/diffusers/training/lora)."", ""- Applies the LCM LoRA. - Changes the scheduler to the LCMScheduler, which is the one used in latent consistency models. - That’s it! This would result in the following full-resolution image:\n\n![SDXL in 4 steps with LCM LoRA](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/lcm-lora/lcm-1.jpg?download=true)  \n*Image generated with SDXL in 4 steps using an LCM LoRA.*\n\n### Quality Comparison\nLet’s see how the number of steps impacts generation quality. The following code will generate images with 1 to 8 total inference steps:\n```py\nimages = []\nfor steps in range(8):\ngenerator = torch.Generator(device=pipe.device).manual\\_seed(1337)\nimage = pipe(\nprompt=prompt,\nnum\\_inference\\_steps=steps+1,\nguidance\\_scale=1,\ngenerator=generator,\n).images[0]\nimages.append(image)\n```\nThese are the 8 images displayed in a grid:\n\n![LCM LoRA generations with 1 to 8 steps](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/lcm-lora/lcm-grid.jpg?download=true)  \n*LCM LoRA generations with 1 to 8 steps.*\n\nAs expected, using just \\*\\*1\\*\\* step produces an approximate shape without discernible features and lacking texture.""]",The LCM LoRA model derived from the SDXL 1.0 base is the one available at `latent-consistency/lcm-lora-sdxl`.,latent-consistency/lcm-lora-sdxl
What is the Top 1 Accuracy for tf_efficientnet_b6 on ImageNet?,"[""In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.\\*\nThis model was contributed by [adirik](https://huggingface.co/adirik). The original code can be found [here](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet). ## EfficientNetConfig\n[[autodoc]] EfficientNetConfig\n## EfficientNetImageProcessor\n[[autodoc]] EfficientNetImageProcessor\n- preprocess\n## EfficientNetModel\n[[autodoc]] EfficientNetModel\n- forward\n## EfficientNetForImageClassification\n[[autodoc]] EfficientNetForImageClassification\n- forward"", ""AdvProp (EfficientNet)\n\\*\\*AdvProp\\*\\* is an adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to the method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. The weights from this model were ported from [Tensorflow/TPU](https://github.com/tensorflow/tpu). ## How do I use this model on an image? To load a pretrained model:\n```python\nimport timm\nmodel = timm.create\\_model('tf\\_efficientnet\\_b0\\_ap', pretrained=True)\nmodel.eval()\n```\nTo load and preprocess the image:\n```python\nimport urllib\nfrom PIL import Image\nfrom timm.data import resolve\\_data\\_config\nfrom timm.data.transforms\\_factory import create\\_transform\nconfig = resolve\\_data\\_config({}, model=model)\ntransform = create\\_transform(\\*\\*config)\nurl, filename = (\""https://github.com/pytorch/hub/raw/master/images/dog.jpg\"", \""dog.jpg\"")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\nTo get the model predictions:\n```python\nimport torch\nwith torch.no\\_grad():\nout = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n```\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename = (\""https://raw.githubusercontent.com/pytorch/hub/master/imagenet\\_classes.txt\"", \""imagenet\\_classes.txt\"")\nurllib.request.urlretrieve(url, filename)\nwith open(\""imagenet\\_classes.txt\"", \""r\"") as f:\ncategories = [s.strip() for s in f.readlines()]\n# Print top categories per image\ntop5\\_prob, top5\\_catid = torch.topk(probabilities, 5)\nfor i in range(top5\\_prob.size(0)):\nprint(categories[top5\\_catid[i]], top5\\_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\nReplace the model name with the variant you want to use, e.g. `tf\\_efficientnet\\_b0\\_ap`. You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature\\_extraction/), just change the name of the model you want to use. ## How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). ```python\nmodel = timm.create\\_model('tf\\_efficientnet\\_b0\\_ap', pretrained=True, num\\_classes=NUM\\_FINETUNE\\_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset. ## How do I train this model? You can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh. ## Citation\n```BibTeX\n@misc{xie2020adversarial,\ntitle={Adversarial Examples Improve Image Recognition},\nauthor={Cihang Xie and Mingxing Tan and Boqing Gong and Jiang Wang and Alan Yuille and Quoc V."", ""(Tensorflow) EfficientNet CondConv\n\\*\\*EfficientNet\\*\\* is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a \\*compound coefficient\\*. Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use $2^N$ times more computational resources, then we can simply increase the network depth by $\\alpha ^ N$, width by $\\beta ^ N$, and image size by $\\gamma ^ N$, where $\\alpha, \\beta, \\gamma$ are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient $\\phi$ to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of [MobileNetV2](https://paperswithcode.com/method/mobilenetv2), in addition to squeeze-and-excitation blocks. This collection of models amends EfficientNet by adding [CondConv](https://paperswithcode.com/method/condconv) convolutions. The weights from this model were ported from [Tensorflow/TPU](https://github.com/tensorflow/tpu). ## How do I use this model on an image? To load a pretrained model:\n```python\nimport timm\nmodel = timm.create\\_model('tf\\_efficientnet\\_cc\\_b0\\_4e', pretrained=True)\nmodel.eval()\n```\nTo load and preprocess the image:\n```python\nimport urllib\nfrom PIL import Image\nfrom timm.data import resolve\\_data\\_config\nfrom timm.data.transforms\\_factory import create\\_transform\nconfig = resolve\\_data\\_config({}, model=model)\ntransform = create\\_transform(\\*\\*config)\nurl, filename = (\""https://github.com/pytorch/hub/raw/master/images/dog.jpg\"", \""dog.jpg\"")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\nTo get the model predictions:\n```python\nimport torch\nwith torch.no\\_grad():\nout = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n```\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename = (\""https://raw.githubusercontent.com/pytorch/hub/master/imagenet\\_classes.txt\"", \""imagenet\\_classes.txt\"")\nurllib.request.urlretrieve(url, filename)\nwith open(\""imagenet\\_classes.txt\"", \""r\"") as f:\ncategories = [s.strip() for s in f.readlines()]\n# Print top categories per image\ntop5\\_prob, top5\\_catid = torch.topk(probabilities, 5)\nfor i in range(top5\\_prob.size(0)):\nprint(categories[top5\\_catid[i]], top5\\_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\nReplace the model name with the variant you want to use, e.g. `tf\\_efficientnet\\_cc\\_b0\\_4e`. You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature\\_extraction/), just change the name of the model you want to use. ## How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). ```python\nmodel = timm.create\\_model('tf\\_efficientnet\\_cc\\_b0\\_4e', pretrained=True, num\\_classes=NUM\\_FINETUNE\\_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset."", ""EfficientNet (Knapsack Pruned)\n\\*\\*EfficientNet\\*\\* is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a \\*compound coefficient\\*. Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use $2^N$ times more computational resources, then we can simply increase the network depth by $\\alpha ^ N$, width by $\\beta ^ N$, and image size by $\\gamma ^ N$, where $\\alpha, \\beta, \\gamma$ are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient $\\phi$ to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of [MobileNetV2](https://paperswithcode.com/method/mobilenetv2), in addition to [squeeze-and-excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block). This collection consists of pruned EfficientNet models. ## How do I use this model on an image? To load a pretrained model:\n```python\nimport timm\nmodel = timm.create\\_model('efficientnet\\_b1\\_pruned', pretrained=True)\nmodel.eval()\n```\nTo load and preprocess the image:\n```python\nimport urllib\nfrom PIL import Image\nfrom timm.data import resolve\\_data\\_config\nfrom timm.data.transforms\\_factory import create\\_transform\nconfig = resolve\\_data\\_config({}, model=model)\ntransform = create\\_transform(\\*\\*config)\nurl, filename = (\""https://github.com/pytorch/hub/raw/master/images/dog.jpg\"", \""dog.jpg\"")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\nTo get the model predictions:\n```python\nimport torch\nwith torch.no\\_grad():\nout = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n```\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename = (\""https://raw.githubusercontent.com/pytorch/hub/master/imagenet\\_classes.txt\"", \""imagenet\\_classes.txt\"")\nurllib.request.urlretrieve(url, filename)\nwith open(\""imagenet\\_classes.txt\"", \""r\"") as f:\ncategories = [s.strip() for s in f.readlines()]\n# Print top categories per image\ntop5\\_prob, top5\\_catid = torch.topk(probabilities, 5)\nfor i in range(top5\\_prob.size(0)):\nprint(categories[top5\\_catid[i]], top5\\_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\nReplace the model name with the variant you want to use, e.g."", ""`tf\\_efficientnet\\_b0`. You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature\\_extraction/), just change the name of the model you want to use. ## How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). ```python\nmodel = timm.create\\_model('tf\\_efficientnet\\_b0', pretrained=True, num\\_classes=NUM\\_FINETUNE\\_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset. ## How do I train this model? You can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh. ## Citation\n```BibTeX\n@misc{tan2020efficientnet,\ntitle={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},\nauthor={Mingxing Tan and Quoc V.""]",The Top-1 accuracy for EfficientNet-B6 on ImageNet is 87.2%.,84.11%
What utility allows starting multi-gpu training in a Jupyter Notebook?,"[""Finally, if we want our training to take a couple of minutes instead of a few hours, we will need to use a GPU. The first step is to get one, for instance by using a colab notebook. Then you need to actually send your model and training data on it by using a torch device. Double-check the following lines print a CUDA device for you! We can now put everything together! First we put our model in training mode (which will activate the training behavior for some layers like Dropout) then go through the number of epochs we picked and all the data in our training dataloader. Then we go through all the steps we have seen already: send the data to the GPU, compute the model outputs, and in particular the loss. Use the loss to compute gradients, then make a training step with the optimizer. Update the learning rate in our scheduler for the next iteration and zero the gradients of the optimizer. Once this is finished, we can evaluate our model very easily with a metric from the Datasets library. First we put our model in evaluation mode, then go through all the data in the evaluation data loader. As we have seen in the Trainer video, the model outputs logits and we need to apply the argmax function to convert them into predictions. The metric object then has an add\\_batch method we can use to send it those intermediate predictions."", ""Launching a training inside a notebook will then display a progress bar and complete the table you see here as you pass every epoch."", ""The most common case is where\nyou have a single GPU. The methods that you can apply to improve training efficiency on a single GPU extend to other setups\nsuch as multiple GPU. However, there are also techniques that are specific to multi-GPU or CPU training. We cover them in\nseparate sections. \\* [Methods and tools for efficient training on a single GPU](perf\\_train\\_gpu\\_one): start here to learn common approaches that can help optimize GPU memory utilization, speed up the training, or both. \\* [Multi-GPU training section](perf\\_train\\_gpu\\_many): explore this section to learn about further optimization methods that apply to a multi-GPU settings, such as data, tensor, and pipeline parallelism. \\* [CPU training section](perf\\_train\\_cpu): learn about mixed precision training on CPU. \\* [Efficient Training on Multiple CPUs](perf\\_train\\_cpu\\_many): learn about distributed CPU training. \\* [Training on TPU with TensorFlow](perf\\_train\\_tpu\\_tf): if you are new to TPUs, refer to this section for an opinionated introduction to training on TPUs and using XLA. \\* [Custom hardware for training](perf\\_hardware): find tips and tricks when building your own deep learning rig. \\* [Hyperparameter Search using Trainer API](hpo\\_train)\n## Inference\nEfficient inference with large models in a production environment can be as challenging as training them. In the following\nsections we go through the steps to run inference on CPU and single/multi-GPU setups. \\* [Inference on a single CPU](perf\\_infer\\_cpu)\n\\* [Inference on a single GPU](perf\\_infer\\_gpu\\_one)\n\\* [Multi-GPU inference](perf\\_infer\\_gpu\\_one)\n\\* [XLA Integration for TensorFlow Models](tf\\_xla)\n## Training and inference\nHere you'll find techniques, tips and tricks that apply whether you are training a model, or running inference with it. \\* [Instantiating a big model](big\\_models)\n\\* [Troubleshooting performance issues](debugging)\n## Contribute\nThis document is far from being complete and a lot more needs to be added, so if you have additions or corrections to\nmake please don't hesitate to open a PR or if you aren't sure start an Issue and we can discuss the details there."", ""!---\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Model training anatomy\nTo understand performance optimization techniques that one can apply to improve efficiency of model training\nspeed and memory utilization, it's helpful to get familiar with how GPU is utilized during training, and how compute\nintensity varies depending on an operation performed. Let's start by exploring a motivating example of GPU utilization and the training run of a model. For the demonstration,\nwe'll need to install a few libraries:\n```bash\npip install transformers datasets accelerate nvidia-ml-py3\n```\nThe `nvidia-ml-py3` library allows us to monitor the memory usage of the models from within Python. You might be familiar\nwith the `nvidia-smi` command in the terminal - this library allows to access the same information in Python directly. Then, we create some dummy data: random token IDs between 100 and 30000 and binary labels for a classifier. In total, we get 512 sequences each with length 512 and store them in a [`~datasets.Dataset`] with PyTorch format. ```py\n>>> import numpy as np\n>>> from datasets import Dataset\n>>> seq\\_len, dataset\\_size = 512, 512\n>>> dummy\\_data = {\n..."", ""!---\nCopyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Multiple-choice training (e.g. SWAG)\nThis folder contains the `run\\_swag.py` script, showing an examples of \\*multiple-choice answering\\* with the\n🤗 Transformers library. For straightforward use-cases you may be able to use these scripts without modification,\nalthough we have also included comments in the code to indicate areas that you may need to adapt to your own projects. ### Multi-GPU and TPU usage\nBy default, the script uses a `MirroredStrategy` and will use multiple GPUs effectively if they are available. TPUs\ncan also be used by passing the name of the TPU resource with the `--tpu` argument. ### Memory usage and data loading\nOne thing to note is that all data is loaded into memory in this script. Most multiple-choice datasets are small\nenough that this is not an issue, but if you have a very large dataset you will need to modify the script to handle\ndata streaming. This is particularly challenging for TPUs, given the stricter requirements and the sheer volume of data\nrequired to keep them fed. A full explanation of all the possible pitfalls is a bit beyond this example script and\nREADME, but for more information you can see the 'Input Datasets' section of\n[this document](https://www.tensorflow.org/guide/tpu). ### Example command\n```bash\npython run\\_swag.py \\\n--model\\_name\\_or\\_path distilbert-base-cased \\\n--output\\_dir output \\\n--do\\_eval \\\n--do\\_train\n```""]",The utility that allows starting multi-GPU training in a Jupyter Notebook is the `Trainer` API from the Hugging Face Transformers library. It provides a straightforward way to manage training across multiple GPUs effectively.,notebook_launcher
Who are the authors of 'Contrastive Search Is What You Need For Neural Text Generation'?,"[""--\ntitle: \""Generating Human-level Text with Contrastive Search in Transformers 🤗\""\nthumbnail: /blog/assets/115\\_introducing\\_contrastive\\_search/thumbnail.png\nauthors:\n- user: GMFTBY\n---\n# Generating Human-level Text with Contrastive Search in Transformers 🤗\n\\*\\*\\*\\*\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/115_introducing_contrastive_search.ipynb)\n### 1. Introduction:\nNatural language generation (i.e. text generation) is one of the core tasks in natural language processing (NLP). In this blog, we introduce the current state-of-the-art decoding method, \\_\\_\\_Contrastive Search\\_\\_\\_, for neural text generation. Contrastive search is originally proposed in \\_\""A Contrastive Framework for Neural Text Generation\""\\_ [[1]](#references) ([[Paper]](https://arxiv.org/abs/2202.06417)[[Official Implementation]](https://github.com/yxuansu/SimCTG)) at NeurIPS 2022. Moreover, in this follow-up work, \\_\""Contrastive Search Is What You Need For Neural Text Generation\""\\_ [[2]](#references) ([[Paper]](https://arxiv.org/abs/2210.14140) [[Official Implementation]](https://github.com/yxuansu/Contrastive\\_Search\\_Is\\_What\\_You\\_Need)), the authors further demonstrate that contrastive search can generate human-level text using \\*\\*off-the-shelf\\*\\* language models across \\*\\*16\\*\\* languages."", ""--\ntitle: \""Assisted Generation: a new direction toward low-latency text generation\""\nthumbnail: /blog/assets/assisted-generation/thumbnail.png\nauthors:\n- user: joaogante\n---\n# Assisted Generation: a new direction toward low-latency text generation\nLarge language models are all the rage these days, with many companies investing significant resources to scale them up and unlock new capabilities. However, as humans with ever-decreasing attention spans, we also dislike their slow response times. Latency is critical for a good user experience, and smaller models are often used despite their lower quality (e.g. in [code completion](https://ai.googleblog.com/2022/07/ml-enhanced-code-completion-improves.html)). Why is text generation so slow? What’s preventing you from deploying low-latency large language models without going bankrupt? In this blog post, we will revisit the bottlenecks for autoregressive text generation and introduce a new decoding method to tackle the latency problem. You’ll see that by using our new method, assisted generation, you can reduce latency up to 10x in commodity hardware! ## Understanding text generation latency\nThe core of modern text generation is straightforward to understand. Let’s look at the central piece, the ML model. Its input contains a text sequence, which includes the text generated so far, and potentially other model-specific components (for instance, Whisper also has an audio input). The model takes the input and runs a forward pass: the input is fed to the model and passed sequentially along its layers until the unnormalized log probabilities for the next token are predicted (also known as logits). A token may consist of entire words, sub-words, or even individual characters, depending on the model. The [illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/) is a great reference if you’d like to dive deeper into this part of text generation. [](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov)\nA model forward pass gets you the logits for the next token, which you can freely manipulate (e.g."", ""When I finally looked at this for a few moments, I immediately thought, \""\n1: I enjoy walking with my cute dog. The only time I felt like walking was when I was working, so it was awesome for me. I didn't want to walk for days. I am really curious how she can walk with me\n2: I enjoy walking with my cute dog (Chama-I-I-I-I-I), and I really enjoy running. I play in a little game I play with my brother in which I take pictures of our houses. ```\nCool, now you should have all the tools to let your model write your\nstories with `transformers`! ## Conclusion\nAs \\*ad-hoc\\* decoding methods, \\*top-p\\* and \\*top-K\\* sampling seem to\nproduce more fluent text than traditional \\*greedy\\* - and \\*beam\\* search\non open-ended language generation. There is\nevidence that the apparent flaws of \\*greedy\\* and \\*beam\\* search -\nmainly generating repetitive word sequences - are caused by the model\n(especially the way the model is trained), rather than the decoding\nmethod, \\*cf.\\* [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf). Also, as demonstrated in\n[Welleck et al. (2020)](https://arxiv.org/abs/2002.02492), it looks as\n\\*top-K\\* and \\*top-p\\* sampling also suffer from generating repetitive word\nsequences. In [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf), the\nauthors show that according to human evaluations, \\*beam\\* search can\ngenerate more fluent text than \\*Top-p\\* sampling, when adapting the\nmodel's training objective. Open-ended language generation is a rapidly evolving field of research\nand as it is often the case there is no one-size-fits-all method here,\nso one has to see what works best in one's specific use case. Fortunately, \\*you\\* can try out all the different decoding methods in\n`transfomers` 🤗 -- you can have an overview of the available methods\n[here](https://huggingface.co/docs/transformers/generation\\_strategies#decoding-strategies). Thanks to everybody, who has contributed to the blog post: Alexander Rush, Julien Chaumand, Thomas Wolf, Victor Sanh, Sam Shleifer, Clément Delangue, Yacine Jernite, Oliver Åstrand and John de Wasseige. ## Appendix\n`generate` has evolved into a highly composable method, with flags to manipulate the resulting text in many\ndirections that were not covered in this blog post. Here are a few helpful pages to guide you:\n- [How to parameterize `generate`](https://huggingface.co/docs/transformers/generation\\_strategies#default-text-generation-configuration)\n- [How to stream the output](https://huggingface.co/docs/transformers/generation\\_strategies#streaming)\n- [Full list of decoding options](https://huggingface.co/docs/transformers/en/main\\_classes/text\\_generation#transformers.GenerationConfig)\n- [`generate` API reference](https://huggingface.co/docs/transformers/en/main\\_classes/text\\_generation#transformers.GenerationMixin.generate)\n- [LLM score leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open\\_llm\\_leaderboard)\nIf you find that navigating our docs is challenging and you can't easily find what you're looking for, drop us a message in [this GitHub issue](https://github.com/huggingface/transformers/issues/24575)."", ""If you are new to this concept, we recommend reading [this blog post that illustrates how common decoding strategies work](https://huggingface.co/blog/how-to-generate). Here, we'll show some of the parameters that control the decoding strategies and illustrate how you can use them. ### Greedy Search\n[`generate`] uses greedy search decoding by default so you don't have to pass any parameters to enable it. This means the parameters `num\\_beams` is set to 1 and `do\\_sample=False`. ```python\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n>>> prompt = \""I look forward to\""\n>>> checkpoint = \""distilgpt2\""\n>>> tokenizer = AutoTokenizer.from\\_pretrained(checkpoint)\n>>> inputs = tokenizer(prompt, return\\_tensors=\""pt\"")\n>>> model = AutoModelForCausalLM.from\\_pretrained(checkpoint)\n>>> outputs = model.generate(\\*\\*inputs)\n>>> tokenizer.batch\\_decode(outputs, skip\\_special\\_tokens=True)\n['I look forward to seeing you all again!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']\n```\n### Contrastive search\nThe contrastive search decoding strategy was proposed in the 2022 paper [A Contrastive Framework for Neural Text Generation](https://arxiv.org/abs/2202.06417). It demonstrates superior results for generating non-repetitive yet coherent long outputs. To learn how contrastive search\nworks, check out [this blog post](https://huggingface.co/blog/introducing-csearch). The two main parameters that enable and control the behavior of contrastive search are `penalty\\_alpha` and `top\\_k`:\n```python\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM\n>>> checkpoint = \""gpt2-large\""\n>>> tokenizer = AutoTokenizer.from\\_pretrained(checkpoint)\n>>> model = AutoModelForCausalLM.from\\_pretrained(checkpoint)\n>>> prompt = \""Hugging Face Company is\""\n>>> inputs = tokenizer(prompt, return\\_tensors=\""pt\"")\n>>> outputs = model.generate(\\*\\*inputs, penalty\\_alpha=0.6, top\\_k=4, max\\_new\\_tokens=100)\n>>> tokenizer.batch\\_decode(outputs, skip\\_special\\_tokens=True)\n['Hugging Face Company is a family owned and operated business."", ""## \\*\\*Conclusion\\*\\*\nConstrained beam search gives us a flexible means to inject external knowledge and requirements into text generation. Previously, there was no easy way to tell the model to 1. include a list of sequences where 2. some of which are optional and some are not, such that 3. they're generated \\*somewhere\\* in the sequence at respective reasonable positions. Now, we can have full control over our generation with a mix of different subclasses of `Constraint` objects! This new feature is based mainly on the following papers:\n- [Guided Open Vocabulary Image Captioning with Constrained Beam Search](https://arxiv.org/pdf/1612.00576.pdf)\n- [Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation](https://arxiv.org/abs/1804.06609)\n- [Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting](https://aclanthology.org/N19-1090/)\n- [Guided Generation of Cause and Effect](https://arxiv.org/pdf/2107.09846.pdf)\nLike the ones above, many new research papers are exploring ways of using external knowledge (e.g., KGs, KBs) to guide the outputs of large deep learning models.""]","The authors of ""Contrastive Search Is What You Need For Neural Text Generation"" are not explicitly listed in the provided context. However, it is mentioned that the paper is a follow-up work related to contrastive search in neural text generation. For more detailed information, you may refer to the paper directly.",Yixuan Su and Nigel Collier
Where can you find BERT's conversion script for porting from TensorFlow to PyTorch?,"[""Here's how to do it:\n1. Make sure you are logged into your Hugging Face account in your terminal. You can log in using the command\n`huggingface-cli login` (you can find your access tokens [here](https://huggingface.co/settings/tokens))\n2. Run `transformers-cli pt-to-tf --model-name foo/bar`, where `foo/bar` is the name of the model repository\ncontaining the PyTorch weights you want to convert\n3. Tag `@joaogante` and `@Rocketknight1` in the 🤗 Hub PR the command above has just created\nThat's it! 🎉\n## Debugging mismatches across ML frameworks 🐛\nAt some point, when adding a new architecture or when creating TensorFlow weights for an existing architecture, you\nmight come across errors complaining about mismatches between PyTorch and TensorFlow. You might even decide to open the\nmodel architecture code for the two frameworks, and find that they look identical. What's going on? 🤔\nFirst of all, let's talk about why understanding these mismatches matters. Many community members will use 🤗\nTransformers models out of the box, and trust that our models behave as expected. When there is a large mismatch\nbetween the two frameworks, it implies that the model is not following the reference implementation for at least one\nof the frameworks. This might lead to silent failures, in which the model runs but has poor performance. This is\narguably worse than a model that fails to run at all! To that end, we aim at having a framework mismatch smaller than\n`1e-5` at all stages of the model."", ""By setting it to\n`True` for `module.project\\_q` and `module.project\\_hid`, we make sure the custom initialization we did is not overridden later on,\nthe `\\_init\\_weights` function won't be applied to them. \\*\\*6. Write a conversion script\\*\\*\nNext, you should write a conversion script that lets you convert the checkpoint you used to debug \\*brand\\_new\\_bert\\* in\nthe original repository to a checkpoint compatible with your just created 🤗 Transformers implementation of\n\\*brand\\_new\\_bert\\*. It is not advised to write the conversion script from scratch, but rather to look through already\nexisting conversion scripts in 🤗 Transformers for one that has been used to convert a similar model that was written in\nthe same framework as \\*brand\\_new\\_bert\\*. Usually, it is enough to copy an already existing conversion script and\nslightly adapt it for your use case. Don't hesitate to ask the Hugging Face team to point you to a similar already\nexisting conversion script for your model. - If you are porting a model from TensorFlow to PyTorch, a good starting point might be BERT's conversion script [here](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling\\_bert.py#L91)\n- If you are porting a model from PyTorch to PyTorch, a good starting point might be BART's conversion script [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert\\_bart\\_original\\_pytorch\\_checkpoint\\_to\\_pytorch.py)\nIn the following, we'll quickly explain how PyTorch models store layer weights and define layer names."", ""While users are still able to load your model from a different framework if you skip this step, it will be slower because 🤗 Transformers will need to convert the checkpoint on-the-fly. Converting a checkpoint for another framework is easy. Make sure you have PyTorch and TensorFlow installed (see [here](installation) for installation instructions), and then find the specific model for your task in the other framework. Specify `from\\_tf=True` to convert a checkpoint from TensorFlow to PyTorch:\n```py\n>>> pt\\_model = DistilBertForSequenceClassification.from\\_pretrained(\""path/to/awesome-name-you-picked\"", from\\_tf=True)\n>>> pt\\_model.save\\_pretrained(\""path/to/awesome-name-you-picked\"")\n```\n\nSpecify `from\\_pt=True` to convert a checkpoint from PyTorch to TensorFlow:\n```py\n>>> tf\\_model = TFDistilBertForSequenceClassification.from\\_pretrained(\""path/to/awesome-name-you-picked\"", from\\_pt=True)\n```\nThen you can save your new TensorFlow model with its new checkpoint:\n```py\n>>> tf\\_model.save\\_pretrained(\""path/to/awesome-name-you-picked\"")\n```\n\nIf a model is available in Flax, you can also convert a checkpoint from PyTorch to Flax:\n```py\n>>> flax\\_model = FlaxDistilBertForSequenceClassification.from\\_pretrained(\n..."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\n🤗 Optimum handles the export of TensorFlow models to TFLite in the `exporters.tflite` module. In addition, models hosted on the Hugging Face Hub with PyTorch weights but having a TensorFlow implementation will also be supported in the export thanks to Transformers' [TFPreTrainedModel.from\\_pretrained()](https://huggingface.co/docs/transformers/main/en/main\\_classes/model#transformers.TFPreTrainedModel.from\\_pretrained) auto-conversion to TensorFlow. The TFLite export support provides classes, functions and a command line interface to export a model easily. Supported architectures:\n- Albert\n- BERT\n- Camembert\n- ConvBert\n- Deberta\n- Deberta V2\n- DistilBert\n- Electra\n- Flaubert\n- MobileBert\n- MPNet\n- ResNet\n- Roberta\n- RoFormer\n- XLM\n- XLMRoberta"", ""Note that for BigBird you have to change the attention layer. BigBird's attention\nlayer is quite complex as you can see [here](https://github.com/google-research/bigbird/blob/103a3345f94bf6364749b51189ed93024ca5ef26/bigbird/core/attention.py#L560). Don't\nfeel discouraged by this! In a first step you should simply make sure that\nthe layer `BigBirdAttention` has the correct weights as can be found in the\npretrained checkpoints. This means that you have to make sure that in the\n`\\_\\_init\\_\\_(self, ...)` function of `BigBirdAttention`, all submodules include all\nnecessary `nn.Module` layers. Only at a later stage do we need to fully rewrite\nthe complex attention function. \\*\\*6. Write a conversion script\\*\\*\nNext, you should write a conversion script that lets you convert the\ncheckpoint you used to debug \\*BigBird\\* in the original\nrepository to a checkpoint compatible with your just created 🤗\nTransformers implementation of \\*BigBird\\*. It is not advised to\nwrite the conversion script from scratch, but rather to look through\nalready existing conversion scripts in 🤗 Transformers for one that has\nbeen used to convert a similar model that was written in the same\nframework as \\*BigBird\\*.""]",You can find BERT's conversion script for porting from TensorFlow to PyTorch [here](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91).,https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91
What are the advantages of streaming mode over downloading a dataset?,"[""aving and reloading a dataset. In this video we'll take a look saving a dataset in various formats, and explore the ways to reload the saved data. When you download a dataset, the processing scripts and data are stored locally on your computer. The cache allows the Datasets library to avoid re-downloading or processing the entire dataset every time you use it. The data is stored in the form of Arrow tables whose location can be found by accessing the dataset's cache\\_files attribute. In this example, we've downloaded the allocine dataset from the Hugging Face Hub and you can see there are three Arrow files stored in the cache, one for each split. But in many cases, you'll want to save your dataset in a different location or format. As shown in the table, the Datasets library provides four main functions to achieve this. You're probably familiar with the CSV and JSON formats, both of which are great if you want to save small to medium-sized datasets. But if your dataset is huge, you'll want to save it in either the Arrow or Parquet formats. Arrow files are great if you plan to reload or process the data in the near future."", ""In total, this process required 13 lines of\nPython code, right from loading the dataset to the final filtering step. Keeping the notebook as general as possible, we only performed the fundamental data preparation steps. However, there\nis no restriction to the functions you can apply to your audio dataset. You can extend the function `prepare\\_dataset`\nto perform much more involved operations, such as data augmentation, voice activity detection or noise reduction. With\n🤗 Datasets, if you can write it in a Python function, you can apply it to your dataset! ## Streaming Mode: The Silver Bullet\nOne of the biggest challenges faced with audio datasets is their sheer size. The `xs` configuration of GigaSpeech contained just 10\nhours of training data, but amassed over 13GB of storage space for download and preparation. So what happens when we\nwant to train on a larger split? The full `xl` configuration contains 10,000 hours of training data, requiring over 1TB of\nstorage space. For most speech researchers, this well exceeds the specifications of a typical hard drive disk. Do we need to fork out and buy additional storage? Or is there a way we can train on these datasets with \\*\\*no disk space\nconstraints\\*\\*? 🤗 Datasets allow us to do just this. It is made possible through the use of [\\_streaming\\_](https://huggingface.co/docs/datasets/stream)\nmode, depicted graphically in Figure 1. Streaming allows us to load the data progressively as we iterate over the dataset. Rather than downloading the whole dataset at once, we load the dataset sample by sample. We iterate over the dataset,\nloading and preparing samples \\_on the fly\\_ when they are needed. This way, we only ever load the samples that we're using,\nand not the ones that we're not! Once we're done with a sample, we continue iterating over the dataset and load the next one. This is analogous to \\_downloading\\_ a TV show versus \\_streaming\\_ it. When we download a TV show, we download the entire video\noffline and save it to our disk. We have to wait for the entire video to download before we can watch it and require as\nmuch disk space as size of the video file. Compare this to streaming a TV show. Here, we don’t download any part of the\nvideo to disk, but rather iterate over the remote video file and load each part in real-time as required. We don't have\nto wait for the full video to buffer before we can start watching, we can start as soon as the first portion of the video\nis ready! This is the same \\_streaming\\_ principle that we apply to loading datasets. ![Trulli](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming.gif)\n\n**Figure 1:** Streaming mode. The dataset is loaded progressively as we iterate over the dataset. Streaming mode has three primary advantages over downloading the entire dataset at once:\n1. \\*\\*Disk space:\\*\\* samples are loaded to memory one-by-one as we iterate over the dataset. Since the data is not downloaded locally, there are no disk space requirements, so you can use datasets of arbitrary size. 2. \\*\\*Download and processing time:\\*\\* audio datasets are large and need a significant amount of time to download and process. With streaming, loading and processing is done on the fly, meaning you can start using the dataset as soon as the first sample is ready."", ""PyTorch offers parallelized data loading, retrieving batches of indices instead of individually, and streaming to iterate over the dataset without downloading it on disk. #### Use multiple Workers\nYou can parallelize data loading with the `num\\_workers` argument of a PyTorch `DataLoader` and get a higher throughput. Under the hood, the `DataLoader` starts `num\\_workers` processes. Each process reloads the dataset passed to the `DataLoader` and is used to query examples. Reloading the dataset inside a worker doesn't fill up your RAM, since it simply memory-maps the dataset again from your disk. ```py\n>>> import numpy as np\n>>> from datasets import Dataset, load\\_from\\_disk\n>>> from torch.utils.data import DataLoader\n>>> data = np.random.rand(10\\_000)\n>>> Dataset.from\\_dict({\""data\"": data}).save\\_to\\_disk(\""my\\_dataset\"")\n>>> ds = load\\_from\\_disk(\""my\\_dataset\"").with\\_format(\""torch\"")\n>>> dataloader = DataLoader(ds, batch\\_size=32, num\\_workers=4)\n```\n### Stream data\nStream a dataset by loading it as an [`IterableDataset`]. This allows you to progressively iterate over a remote dataset without downloading it on disk and or over local data files. Learn more about which type of dataset is best for your use case in the [choosing between a regular dataset or an iterable dataset](./about\\_mapstyle\\_vs\\_iterable) guide. An iterable dataset from `datasets` inherits from `torch.utils.data.IterableDataset` so you can pass it to a `torch.utils.data.DataLoader`:\n```py\n>>> import numpy as np\n>>> from datasets import Dataset, load\\_dataset\n>>> from torch.utils.data import DataLoader\n>>> data = np.random.rand(10\\_000)\n>>> Dataset.from\\_dict({\""data\"": data}).push\\_to\\_hub(\""/my\\_dataset\"") # Upload to the Hugging Face Hub\n>>> my\\_iterable\\_dataset = load\\_dataset(\""/my\\_dataset\"", streaming=True, split=\""train\"")\n>>> dataloader = DataLoader(my\\_iterable\\_dataset, batch\\_size=32)\n```\nIf the dataset is split in several shards (i.e. if the dataset consists of multiple data files), then you can stream in parallel using `num\\_workers`:\n```py\n>>> my\\_iterable\\_dataset = load\\_dataset(\""deepmind/code\\_contests\"", streaming=True, split=\""train\"")\n>>> my\\_iterable\\_dataset.n\\_shards\n39\n>>> dataloader = DataLoader(my\\_iterable\\_dataset, batch\\_size=32, num\\_workers=4)\n```\nIn this case each worker is given a subset of the list of shards to stream from."", ""The first feature is that it treat every dataset as a memory-mapped file. Memory mapping is a mechanism that maps a portion of a file or an entire file on disk to a chunk of virtual memory. This allows applications to access can access segments in an extremely large file without having to read the entire file into memory first. Another cool feature of Arrow's memory mapping capability is that it allows multiple processes to work with the same large dataset without moving it or copying it in any way. This \""zero-copy\"" feature of Arrow makes it extremely fast for iterating over a dataset. In this example you can see that we iterate over 15 million rows in about a minute using a standard laptop - that's not too bad at all! Let's now take a look at how we can stream a large dataset. The only change you need to make is to set the streaming=True argument in the load\\_dataset() function. This will return a special IterableDataset object, which is a bit different to the Dataset objects we've seen in other videos. This object is an iterable, which means we can't index it to access elements, but instead iterate on it using the iter and next methods. This will download and access a single example from the dataset, which means you can progressively iterate through a huge dataset without having to download it first. Tokenizing text with the map() method also works in a similar way. We first stream the dataset and then apply the map() method with the tokenizer. To get the first tokenized example we apply iter and next. The main difference with an IterableDataset is that instead of using the select() method to return example, we use the take() and skip() methods because we can't index into the dataset. The take() method returns the first N examples in the dataset, while skip() skips the first N and returns the rest."", ""Differences between Dataset and IterableDataset\nThere are two types of dataset objects, a [`Dataset`] and an [`IterableDataset`]. Whichever type of dataset you choose to use or create depends on the size of the dataset. In general, an [`IterableDataset`] is ideal for big datasets (think hundreds of GBs!) due to its lazy behavior and speed advantages, while a [`Dataset`] is great for everything else. This page will compare the differences between a [`Dataset`] and an [`IterableDataset`] to help you pick the right dataset object for you. ## Downloading and streaming\nWhen you have a regular [`Dataset`], you can access it using `my\\_dataset[0]`. This provides random access to the rows. Such datasets are also called \""map-style\"" datasets. For example you can download ImageNet-1k like this and access any row:\n```python\nfrom datasets import load\\_dataset\nimagenet = load\\_dataset(\""imagenet-1k\"", split=\""train\"") # downloads the full dataset\nprint(imagenet[0])\n```\nBut one caveat is that you must have the entire dataset stored on your disk or in memory, which blocks you from accessing datasets bigger than the disk.""]","Streaming mode has three primary advantages over downloading the entire dataset at once:

1. **Disk space:** Samples are loaded to memory one-by-one as you iterate over the dataset. Since the data is not downloaded locally, there are no disk space requirements, allowing you to use datasets of arbitrary size.

2. **Download and processing time:** Audio datasets are large and require significant time to download and process. With streaming, loading and processing are done on the fly, meaning you can start using the dataset as soon as the first sample is ready.

3. **Efficiency:** Streaming allows you to load the data progressively, which means you only load the samples that you're using, rather than downloading the entire dataset at once. This is analogous to streaming a TV show, where you can start watching as soon as the first portion is ready without waiting for the entire video to download.",1. Disk space: No local disk space is required as data is loaded to memory one-by-one. 2. Download and processing time: Allows immediate use of the dataset without waiting for full download. 3. Easy experimentation: Enables testing on a few samples without downloading the entire dataset.
What teacher model is used with frugalscore_tiny_bert-base_mover-score?,"[""The inputs it takes are:\n`predictions`: a list of strings representing the predictions to score. `references`: a list of string representing the references for each prediction. Its optional arguments are:\n`batch\\_size`: the batch size for predictions (default value is `32`). `max\\_length`: the maximum sequence length (default value is `128`). `device`: either \""gpu\"" or \""cpu\"" (default value is `None`). ```python\n>>> results = frugalscore.compute(predictions=['hello there', 'huggingface'], references=['hello world', 'hugging face'], batch\\_size=16, max\\_length=64, device=\""gpu\"")\n```\n## Output values\nThe output of FrugalScore is a dictionary with the list of scores for each prediction-reference pair:\n```python\n{'scores': [0.6307541, 0.6449357]}\n```\n### Values from popular papers\nThe [original FrugalScore paper](https://arxiv.org/abs/2110.08559) reported that FrugalScore-Tiny retains 97.7/94.7% of the original performance compared to [BertScore](https://huggingface.co/metrics/bertscore) while running 54 times faster and having 84 times less parameters. ## Examples\nMaximal values (exact match between `references` and `predictions`):\n```python\n>>> frugalscore = evaluate.load(\""frugalscore\"")\n>>> results = frugalscore.compute(predictions=['hello world'], references=['hello world'])\n>>> print(results)\n{'scores': [0.9891098]}\n```\nPartial values:\n```python\n>>> frugalscore = evaluate.load(\""frugalscore\"")\n>>> results = frugalscore.compute(predictions=['hello world'], references=['hugging face'])\n>>> print(results)\n{'scores': [0.42482382]}\n```\n## Limitations and bias\nFrugalScore is based on [BertScore](https://huggingface.co/metrics/bertscore) and [MoverScore](https://arxiv.org/abs/1909.02622), and the models used are based on the original models used for these scores. The full list of available models for FrugalScore is:\n| FrugalScore | Student | Teacher | Method |\n|----------------------------------------------------|-------------|----------------|------------|\n| [moussaKam/frugalscore\\_tiny\\_bert-base\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_tiny\\_bert-base\\_bert-score) | BERT-tiny | BERT-Base | BERTScore |\n| [moussaKam/frugalscore\\_small\\_bert-base\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_small\\_bert-base\\_bert-score) | BERT-small | BERT-Base | BERTScore |\n| [moussaKam/frugalscore\\_medium\\_bert-base\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_medium\\_bert-base\\_bert-score) | BERT-medium | BERT-Base | BERTScore |\n| [moussaKam/frugalscore\\_tiny\\_roberta\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_tiny\\_roberta\\_bert-score) | BERT-tiny | RoBERTa-Large | BERTScore |\n| [moussaKam/frugalscore\\_small\\_roberta\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_small\\_roberta\\_bert-score) | BERT-small | RoBERTa-Large | BERTScore |\n| [moussaKam/frugalscore\\_medium\\_roberta\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_medium\\_roberta\\_bert-score) | BERT-medium | RoBERTa-Large | BERTScore |\n| [moussaKam/frugalscore\\_tiny\\_deberta\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_tiny\\_deberta\\_bert-score) | BERT-tiny | DeBERTa-XLarge | BERTScore |\n| [moussaKam/frugalscore\\_small\\_deberta\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_small\\_deberta\\_bert-score) | BERT-small | DeBERTa-XLarge | BERTScore |\n| [moussaKam/frugalscore\\_medium\\_deberta\\_bert-score](https://huggingface.co/moussaKam/frugalscore\\_medium\\_deberta\\_bert-score) | BERT-medium | DeBERTa-XLarge | BERTScore |\n| [moussaKam/frugalscore\\_tiny\\_bert-base\\_mover-score](https://huggingface.co/moussaKam/frugalscore\\_tiny\\_bert-base\\_mover-score) | BERT-tiny | BERT-Base | MoverScore |\n| [moussaKam/frugalscore\\_small\\_bert-base\\_mover-score](https://huggingface.co/moussaKam/frugalscore\\_small\\_bert-base\\_mover-score) | BERT-small | BERT-Base | MoverScore |\n| [moussaKam/frugalscore\\_medium\\_bert-base\\_mover-score](https://huggingface.co/moussaKam/frugalscore\\_medium\\_bert-base\\_mover-score) | BERT-medium | BERT-Base | MoverScore |\nDepending on the size of the model picked, the loading time will vary: the `tiny` models will load very quickly, whereas the `medium` ones can take several minutes, depending on your Internet connection. ## Citation\n```bibtex\n@article{eddine2021frugalscore,\ntitle={FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metrics for Automatic Text Generation},\nauthor={Eddine, Moussa Kamal and Shang, Guokan and Tixier, Antoine J-P and Vazirgiannis, Michalis},\njournal={arXiv preprint arXiv:2110.08559},\nyear={2021}\n}\n```\n## Further References\n- [Original FrugalScore code](https://github.com/moussaKam/FrugalScore)\n- [FrugalScore paper](https://arxiv.org/abs/2110.08559)"", ""Metric Card for BERT Score\n## Metric description\nBERTScore is an automatic evaluation metric for text generation that computes a similarity score for each token in the candidate sentence with each token in the reference sentence. It leverages the pre-trained contextual embeddings from [BERT](https://huggingface.co/bert-base-uncased) models and matches words in candidate and reference sentences by cosine similarity. Moreover, BERTScore computes precision, recall, and F1 measure, which can be useful for evaluating different language generation tasks. ## How to use\nBERTScore takes 3 mandatory arguments : `predictions` (a list of string of candidate sentences), `references` (a list of strings or list of list of strings of reference sentences) and either `lang` (a string of two letters indicating the language of the sentences, in [ISO 639-1 format](https://en.wikipedia.org/wiki/List\\_of\\_ISO\\_639-1\\_codes)) or `model\\_type` (a string specififying which model to use, according to the BERT specification). The default behavior of the metric is to use the suggested model for the target language when one is specified, otherwise to use the `model\\_type` indicated. ```python\nfrom datasets import load\\_metric\nbertscore = load\\_metric(\""bertscore\"")\npredictions = [\""hello there\"", \""general kenobi\""]\nreferences = [\""hello there\"", \""general kenobi\""]\nresults = bertscore.compute(predictions=predictions, references=references, lang=\""en\"")\n```\nBERTScore also accepts multiple optional arguments:\n`num\\_layers` (int): The layer of representation to use."", ""This is\nalso a seeting where we only have `K=4` possible labels. The higher the number of classes for a given task, the more\ndrastic the speedup will be, since the zero-shot teacher's complexity scales linearly with the number of classes. Since we secretly have access to ground truth labels for AG's news, we can evaluate the accuracy of each model. The\noriginal zero-shot model `roberta-large-mnli` gets an accuracy of 69.3% on the held-out test set. After training a\nstudent on the unlabeled training set, the distilled model gets a similar score of 70.4%. Lastly, you can share the distilled model with the community and/or use it with our inference API by [uploading it\nto the 🤗 Hub](https://huggingface.co/transformers/model\\_sharing.html). We've uploaded the distilled model from this\nexample at\n[joeddav/distilbert-base-uncased-agnews-student](https://huggingface.co/joeddav/distilbert-base-uncased-agnews-student)."", ""the contradiction score. This is sometimes called \""multi-class multi-label\"" classification. - `--temperature` (default: `1.0`): The temperature applied to the softmax of the teacher model predictions. A\nhigher temperature results in a student with smoother (lower confidence) predictions than the teacher while a value\n`<1` resultings in a higher-confidence, peaked distribution. The default `1.0` is equivalent to no smoothing. - `--teacher\\_batch\\_size` (default: `32`): The batch size used for generating a single set of teacher predictions. Does not affect training. Use `--per\\_device\\_train\\_batch\\_size` to change the training batch size. Any of the arguments in the 🤗 Trainer's\n[`TrainingArguments`](https://huggingface.co/transformers/main\\_classes/trainer.html?#trainingarguments) can also be\nmodified, such as `--learning\\_rate`, `--fp16`, `--no\\_cuda`, `--warmup\\_steps`, etc. Run `python distill\\_classifier.py\n-h` for a full list of available arguments or consult the [Trainer\ndocumentation](https://huggingface.co/transformers/main\\_classes/trainer.html#trainingarguments)."", ""Fine-tune a SegFormer model\n## Load the model to fine-tune\nThe SegFormer authors define 5 models with increasing sizes: B0 to B5. The following chart (taken from the original paper) shows the performance of these different models on the ADE20K dataset, compared to other models. [Source](https://arxiv.org/abs/2105.15203)\n\nHere, we'll load the smallest SegFormer model (B0), pre-trained on ImageNet-1k. It's only about 14MB in size! Using a small model will make sure that our model can run smoothly on our pizza delivery robot. ```python\nfrom transformers import SegformerForSemanticSegmentation\npretrained\\_model\\_name = \""nvidia/mit-b0\""\nmodel = SegformerForSemanticSegmentation.from\\_pretrained(\npretrained\\_model\\_name,\nid2label=id2label,\nlabel2id=label2id\n)\n```\n## Set up the Trainer\nTo fine-tune the model on our data, we'll use Hugging Face's [Trainer API](https://huggingface.co/docs/transformers/main\\_classes/trainer). We need to set up the training configuration and an evalutation metric to use a Trainer. First, we'll set up the [`TrainingArguments`](https://huggingface.co/docs/transformers/main\\_classes/trainer#transformers.TrainingArguments). This defines all training hyperparameters, such as learning rate and the number of epochs, frequency to save the model and so on. We also specify to push the model to the hub after training (`push\\_to\\_hub=True`) and specify a model name (`hub\\_model\\_id`). ```python\nfrom transformers import TrainingArguments\nepochs = 50\nlr = 0.00006\nbatch\\_size = 2\nhub\\_model\\_id = \""segformer-b0-finetuned-segments-sidewalk-2\""\ntraining\\_args = TrainingArguments(\n\""segformer-b0-finetuned-segments-sidewalk-outputs\"",\nlearning\\_rate=lr,\nnum\\_train\\_epochs=epochs,\nper\\_device\\_train\\_batch\\_size=batch\\_size,\nper\\_device\\_eval\\_batch\\_size=batch\\_size,\nsave\\_total\\_limit=3,\nevaluation\\_strategy=\""steps\"",\nsave\\_strategy=\""steps\"",\nsave\\_steps=20,\neval\\_steps=20,\nlogging\\_steps=1,\neval\\_accumulation\\_steps=5,\nload\\_best\\_model\\_at\\_end=True,\npush\\_to\\_hub=True,\nhub\\_model\\_id=hub\\_model\\_id,\nhub\\_strategy=\""end\"",\n)\n```\nNext, we'll define a function that computes the evaluation metric we want to work with.""]",The teacher model used with `frugalscore_tiny_bert-base_mover-score` is BERT-Base.,BERT-Base
What class is used to compile the model to run on IPUs?,"[""Here, we show the transformed images. Our dataset is now ready to be used. ## Preparing the model\n\nTo train a model on the IPU we need to import it from Hugging Face Hub and define a trainer using the IPUTrainer class. The IPUTrainer class takes the same arguments as the original [Transformer Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) and works in tandem with the IPUConfig object which specifies the behaviour for compilation and execution on the IPU. Now we import the ViT model from Hugging Face. To use this model on the IPU we need to load the IPU configuration, `IPUConfig`, which gives control to all the parameters specific to Graphcore IPUs (existing IPU configs [can be found here](https://huggingface.co/Graphcore)). We are going to use `Graphcore/vit-base-ipu`. Let's set our training hyperparameters using `IPUTrainingArguments`. This subclasses the Hugging Face `TrainingArguments` class, adding parameters specific to the IPU and its execution characteristics. ## Implementing a custom performance metric for evaluation\n\nThe performance of multi-label classification models can be assessed using the area under the ROC (receiver operating characteristic) curve (AUC\\_ROC)."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Optimize inference using torch.compile()\nThis guide aims to provide a benchmark on the inference speed-ups introduced with [`torch.compile()`](https://pytorch.org/tutorials/intermediate/torch\\_compile\\_tutorial.html) for [computer vision models in 🤗 Transformers](https://huggingface.co/models?pipeline\\_tag=image-classification&library=transformers&sort=trending). ## Benefits of torch.compile\nDepending on the model and the GPU, `torch.compile()` yields up to 30% speed-up during inference. To use `torch.compile()`, simply install any version of `torch` above 2.0. Compiling a model takes time, so it's useful if you are compiling the model only once instead of every time you infer. To compile any computer vision model of your choice, call `torch.compile()` on the model as shown below:\n```diff\nfrom transformers import AutoModelForImageClassification\nmodel = AutoModelForImageClassification.from\\_pretrained(MODEL\\_ID).to(\""cuda\"")\n+ model = torch.compile(model)\n```\n`compile()` comes with multiple modes for compiling, which essentially differ in compilation time and inference overhead."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Efficient Training on CPU\nThis guide focuses on training large models efficiently on CPU. ## Mixed precision with IPEX\nIPEX is optimized for CPUs with AVX-512 or above, and functionally works for CPUs with only AVX2. So, it is expected to bring performance benefit for Intel CPU generations with AVX-512 or above while CPUs with only AVX2 (e.g., AMD CPUs or older Intel CPUs) might result in a better performance under IPEX, but not guaranteed. IPEX provides performance optimizations for CPU training with both Float32 and BFloat16."", ""This adapter works by decoupling the cross-attention layers of the image and text features. All the other model components are frozen and only the embedded image features in the UNet are trained. As a result, IP-Adapter files are typically only ~100MBs. IP-Adapter works with most of our pipelines, including Stable Diffusion, Stable Diffusion XL (SDXL), ControlNet, T2I-Adapter, AnimateDiff. And you can use any custom models finetuned from the same base models. It also works with LCM-Lora out of box. You can find official IP-Adapter checkpoints in [h94/IP-Adapter](https://huggingface.co/h94/IP-Adapter). IP-Adapter was contributed by [okotaku](https://github.com/okotaku). Let's first create a Stable Diffusion Pipeline. ```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\nfrom diffusers.utils import load\\_image\npipeline = AutoPipelineForText2Image.from\\_pretrained(\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16).to(\""cuda\"")\n```\nNow load the [h94/IP-Adapter](https://huggingface.co/h94/IP-Adapter) weights with the [`~loaders.IPAdapterMixin.load\\_ip\\_adapter`] method. ```py\npipeline.load\\_ip\\_adapter(\""h94/IP-Adapter\"", subfolder=\""models\"", weight\\_name=\""ip-adapter\\_sd15.bin\"")\n```\nIP-Adapter relies on an image encoder to generate the image features, if your IP-Adapter weights folder contains a \""image\\_encoder\"" subfolder, the image encoder will be automatically loaded and registered to the pipeline. Otherwise you can so load a [`~transformers.CLIPVisionModelWithProjection`] model and pass it to a Stable Diffusion pipeline when you create it. ```py\nfrom diffusers import AutoPipelineForText2Image, CLIPVisionModelWithProjection\nimport torch\nimage\\_encoder = CLIPVisionModelWithProjection.from\\_pretrained(\n\""h94/IP-Adapter\"",\nsubfolder=\""models/image\\_encoder\"",\ntorch\\_dtype=torch.float16,\n).to(\""cuda\"")\npipeline = AutoPipelineForText2Image.from\\_pretrained(\""runwayml/stable-diffusion-v1-5\"", image\\_encoder=image\\_encoder, torch\\_dtype=torch.float16).to(\""cuda\"")\n```\nIP-Adapter allows you to use both image and text to condition the image generation process. For example, let's use the bear image from the [Textual Inversion](#textual-inversion) section as the image prompt (`ip\\_adapter\\_image`) along with a text prompt to add \""sunglasses\"". 😎\n```py\npipeline.set\\_ip\\_adapter\\_scale(0.6)\nimage = load\\_image(\""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load\\_neg\\_embed.png\"")\ngenerator = torch.Generator(device=\""cpu\"").manual\\_seed(33)\nimages = pipeline(\n    prompt='best quality, high quality, wearing sunglasses',\n    ip\\_adapter\\_image=image,\n    negative\\_prompt=\""monochrome, lowres, bad anatomy, worst quality, low quality\"",\n    num\\_inference\\_steps=50,\n    generator=generator,\n).images\nimages[0]\n```\n\n![](https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/ip-bear.png)\n\nYou can use the [`~loaders.IPAdapterMixin.set\\_ip\\_adapter\\_scale`] method to adjust the text prompt and image prompt condition ratio. If you're only using the image prompt, you should set the scale to `1.0`. You can lower the scale to get more generation diversity, but it'll be less aligned with the prompt. `scale=0.5` can achieve good results in most cases when you use both text and image prompts."", ""This is the case for our [BERT](https://huggingface.co/bert-large-uncased) model, and you should pass its name as the input argument to ```--model\\_name\\_or\\_path```. In order to use the IPU, Optimum will look for the ```ipu\\_config.json``` file from the path passed to the argument ```--ipu\\_config\\_name```. ```\n$ python3 run\\_qa.py \\\n--ipu\\_config\\_name=./ \\\n--model\\_name\\_or\\_path bert-base-uncased \\\n--dataset\\_name squad \\\n--do\\_train \\\n--do\\_eval \\\n--output\\_dir output \\\n--overwrite\\_output\\_dir \\\n--per\\_device\\_train\\_batch\\_size 2 \\\n--per\\_device\\_eval\\_batch\\_size 2 \\\n--learning\\_rate 6e-5 \\\n--num\\_train\\_epochs 3 \\\n--max\\_seq\\_length 384 \\\n--doc\\_stride 128 \\\n--seed 1984 \\\n--lr\\_scheduler\\_type linear \\\n--loss\\_scaling 64 \\\n--weight\\_decay 0.01 \\\n--warmup\\_ratio 0.1 \\\n--output\\_dir /tmp/debug\\_squad/\n```\n### A closer look at Optimum-Graphcore\n#### Getting the data\nA very simple way to get datasets is to use the Hugging Face [Datasets library](https://github.com/huggingface/datasets), which makes it easy for developers to download and share datasets on the Hugging Face hub.""]",The class used to compile the model to run on IPUs is the `IPUTrainer` class.,IPUTrainer
What is the default activation function used in DistilBertConfig?,"[""All attributes are customizable, creating space for experimentation. For example, you can customize a default model to:\n- Try a different activation function with the `activation` parameter. - Use a higher dropout ratio for the attention probabilities with the `attention\\_dropout` parameter. ```py\n>>> my\\_config = DistilBertConfig(activation=\""relu\"", attention\\_dropout=0.4)\n>>> print(my\\_config)\nDistilBertConfig {\n\""activation\"": \""relu\"",\n\""attention\\_dropout\"": 0.4,\n\""dim\"": 768,\n\""dropout\"": 0.1,\n\""hidden\\_dim\"": 3072,\n\""initializer\\_range\"": 0.02,\n\""max\\_position\\_embeddings\"": 512,\n\""model\\_type\"": \""distilbert\"",\n\""n\\_heads\"": 12,\n\""n\\_layers\"": 6,\n\""pad\\_token\\_id\"": 0,\n\""qa\\_dropout\"": 0.1,\n\""seq\\_classif\\_dropout\"": 0.2,\n\""sinusoidal\\_pos\\_embds\"": false,\n\""transformers\\_version\"": \""4.16.2\"",\n\""vocab\\_size\"": 30522\n}\n```\nPretrained model attributes can be modified in the [`~PretrainedConfig.from\\_pretrained`] function:\n```py\n>>> my\\_config = DistilBertConfig.from\\_pretrained(\""distilbert-base-uncased\"", activation=\""relu\"", attention\\_dropout=0.4)\n```\nOnce you are satisfied with your model configuration, you can save it with [`~PretrainedConfig.save\\_pretrained`]. Your configuration file is stored as a JSON file in the specified save directory:\n```py\n>>> my\\_config.save\\_pretrained(save\\_directory=\""./your\\_model\\_save\\_path\"")\n```\nTo reuse the configuration file, load it with [`~PretrainedConfig.from\\_pretrained`]:\n```py\n>>> my\\_config = DistilBertConfig.from\\_pretrained(\""./your\\_model\\_save\\_path/config.json\"")\n```\nYou can also save your configuration file as a dictionary or even just the difference between your custom configuration attributes and the default configuration attributes!"", ""We'll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task). {/if}\nIn the code sample above we didn't use `BertConfig`, and instead loaded a pretrained model via the `bert-base-cased` identifier. This is a model checkpoint that was trained by the authors of BERT themselves; you can find more details about it in its [model card](https://huggingface.co/bert-base-cased). This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results. The weights have been downloaded and cached (so future calls to the `from\\_pretrained()` method won't re-download them) in the cache folder, which defaults to \\*~/.cache/huggingface/transformers\\*. You can customize your cache folder by setting the `HF\\_HOME` environment variable."", ""## Combining DistilBERT and Flash Attention 2\nFirst, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature. ```bash\npip install -U flash-attn --no-build-isolation\n```\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of flash-attn repository. Make also sure to load your model in half-precision (e.g. `torch.float16`)\nTo load and run a model using Flash Attention 2, refer to the snippet below:\n```python\n>>> import torch\n>>> from transformers import AutoTokenizer, AutoModel\n>>> device = \""cuda\"" # the device to load the model onto\n>>> tokenizer = AutoTokenizer.from\\_pretrained('distilbert-base-uncased')\n>>> model = AutoModel.from\\_pretrained(\""distilbert-base-uncased\"", torch\\_dtype=torch.float16, attn\\_implementation=\""flash\\_attention\\_2\"")\n>>> text = \""Replace me by any text you'd like.\""\n>>> encoded\\_input = tokenizer(text, return\\_tensors='pt').to(device)\n>>> model.to(device)\n>>> output = model(\\*\\*encoded\\_input)\n```\n## DistilBertConfig\n[[autodoc]] DistilBertConfig\n## DistilBertTokenizer\n[[autodoc]] DistilBertTokenizer\n## DistilBertTokenizerFast\n[[autodoc]] DistilBertTokenizerFast\n\n## DistilBertModel\n[[autodoc]] DistilBertModel\n- forward\n## DistilBertForMaskedLM\n[[autodoc]] DistilBertForMaskedLM\n- forward\n## DistilBertForSequenceClassification\n[[autodoc]] DistilBertForSequenceClassification\n- forward\n## DistilBertForMultipleChoice\n[[autodoc]] DistilBertForMultipleChoice\n- forward\n## DistilBertForTokenClassification\n[[autodoc]] DistilBertForTokenClassification\n- forward\n## DistilBertForQuestionAnswering\n[[autodoc]] DistilBertForQuestionAnswering\n- forward\n\n## TFDistilBertModel\n[[autodoc]] TFDistilBertModel\n- call\n## TFDistilBertForMaskedLM\n[[autodoc]] TFDistilBertForMaskedLM\n- call\n## TFDistilBertForSequenceClassification\n[[autodoc]] TFDistilBertForSequenceClassification\n- call\n## TFDistilBertForMultipleChoice\n[[autodoc]] TFDistilBertForMultipleChoice\n- call\n## TFDistilBertForTokenClassification\n[[autodoc]] TFDistilBertForTokenClassification\n- call\n## TFDistilBertForQuestionAnswering\n[[autodoc]] TFDistilBertForQuestionAnswering\n- call\n\n## FlaxDistilBertModel\n[[autodoc]] FlaxDistilBertModel\n- \\_\\_call\\_\\_\n## FlaxDistilBertForMaskedLM\n[[autodoc]] FlaxDistilBertForMaskedLM\n- \\_\\_call\\_\\_\n## FlaxDistilBertForSequenceClassification\n[[autodoc]] FlaxDistilBertForSequenceClassification\n- \\_\\_call\\_\\_\n## FlaxDistilBertForMultipleChoice\n[[autodoc]] FlaxDistilBertForMultipleChoice\n- \\_\\_call\\_\\_\n## FlaxDistilBertForTokenClassification\n[[autodoc]] FlaxDistilBertForTokenClassification\n- \\_\\_call\\_\\_\n## FlaxDistilBertForQuestionAnswering\n[[autodoc]] FlaxDistilBertForQuestionAnswering\n- \\_\\_call\\_\\_"", ""Distil\\*\nAuthor: @VictorSanh\nThis folder contains the original code used to train Distil\\* as well as examples showcasing how to use DistilBERT, DistilRoBERTa and DistilGPT2. \\*\\*January 20, 2020 - Bug fixing\\*\\* We have recently discovered and fixed [a bug](https://github.com/huggingface/transformers/commit/48cbf267c988b56c71a2380f748a3e6092ccaed3) in the evaluation of our `run\\_\\*.py` scripts that caused the reported metrics to be over-estimated on average. We have updated all the metrics with the latest runs. \\*\\*December 6, 2019 - Update\\*\\* We release \\*\\*DistilmBERT\\*\\*: 92% of `bert-base-multilingual-cased` on XNLI. The model supports 104 different languages listed [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages). \\*\\*November 19, 2019 - Update\\*\\* We release German \\*\\*DistilBERT\\*\\*: 98.8% of `bert-base-german-dbmdz-cased` on NER tasks. \\*\\*October 23, 2019 - Update\\*\\* We release \\*\\*DistilRoBERTa\\*\\*: 95% of `RoBERTa-base`'s performance on GLUE, twice as fast as RoBERTa while being 35% smaller. \\*\\*October 3, 2019 - Update\\*\\* We release our [NeurIPS workshop paper](https://arxiv.org/abs/1910.01108) explaining our approach on \\*\\*DistilBERT\\*\\*."", ""Model type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BloomConfig, ... ```""]","The default activation function used in `DistilBertConfig` is ""gelu"".",gelu
What is the top 1 accuracy of mobilenetv3_large_100 on ImageNet?,"[""`mobilenetv3\\_large\\_100`. You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the [timm feature extraction examples](../feature\\_extraction), just change the name of the model you want to use. ## How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). ```py\n>>> model = timm.create\\_model('mobilenetv3\\_large\\_100', pretrained=True, num\\_classes=NUM\\_FINETUNE\\_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset. ## How do I train this model? You can follow the [timm recipe scripts](../scripts) for training a new model afresh. ## Citation\n```BibTeX\n@article{DBLP:journals/corr/abs-1905-02244,\nauthor = {Andrew Howard and\nMark Sandler and\nGrace Chu and\nLiang{-}Chieh Chen and\nBo Chen and\nMingxing Tan and\nWeijun Wang and\nYukun Zhu and\nRuoming Pang and\nVijay Vasudevan and\nQuoc V. Le and\nHartwig Adam},\ntitle = {Searching for MobileNetV3},\njournal = {CoRR},\nvolume = {abs/1905.02244},\nyear = {2019},\nurl = {http://arxiv.org/abs/1905.02244},\narchivePrefix = {arXiv},\neprint = {1905.02244},\ntimestamp = {Tue, 12 Jan 2021 15:30:06 +0100},\nbiburl = {https://dblp.org/rec/journals/corr/abs-1905-02244.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```"", ""MobileNet v2\n\\*\\*MobileNetV2\\*\\* is a convolutional neural network architecture that seeks to perform well on mobile devices. It is based on an [inverted residual structure](https://paperswithcode.com/method/inverted-residual-block) where the residual connections are between the bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. As a whole, the architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters, followed by 19 residual bottleneck layers. ## How do I use this model on an image? To load a pretrained model:\n```py\n>>> import timm\n>>> model = timm.create\\_model('mobilenetv2\\_100', pretrained=True)\n>>> model.eval()\n```\nTo load and preprocess the image:\n```py\n>>> import urllib\n>>> from PIL import Image\n>>> from timm.data import resolve\\_data\\_config\n>>> from timm.data.transforms\\_factory import create\\_transform\n>>> config = resolve\\_data\\_config({}, model=model)\n>>> transform = create\\_transform(\\*\\*config)\n>>> url, filename = (\""https://github.com/pytorch/hub/raw/master/images/dog.jpg\"", \""dog.jpg\"")\n>>> urllib.request.urlretrieve(url, filename)\n>>> img = Image.open(filename).convert('RGB')\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\nTo get the model predictions:\n```py\n>>> import torch\n>>> with torch.no\\_grad():\n... out = model(tensor)\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\n>>> print(probabilities.shape)\n>>> # prints: torch.Size([1000])\n```\nTo get the top-5 predictions class names:\n```py\n>>> # Get imagenet class mappings\n>>> url, filename = (\""https://raw.githubusercontent.com/pytorch/hub/master/imagenet\\_classes.txt\"", \""imagenet\\_classes.txt\"")\n>>> urllib.request.urlretrieve(url, filename)\n>>> with open(\""imagenet\\_classes.txt\"", \""r\"") as f:\n... categories = [s.strip() for s in f.readlines()]\n>>> # Print top categories per image\n>>> top5\\_prob, top5\\_catid = torch.topk(probabilities, 5)\n>>> for i in range(top5\\_prob.size(0)):\n... print(categories[top5\\_catid[i]], top5\\_prob[i].item())\n>>> # prints class names and probabilities like:\n>>> # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\nReplace the model name with the variant you want to use, e.g."", ""MobileNet v3\n\\*\\*MobileNetV3\\*\\* is a convolutional neural network that is designed for mobile phone CPUs. The network design includes the use of a [hard swish activation](https://paperswithcode.com/method/hard-swish) and [squeeze-and-excitation](https://paperswithcode.com/method/squeeze-and-excitation-block) modules in the [MBConv blocks](https://paperswithcode.com/method/inverted-residual-block). ## How do I use this model on an image? To load a pretrained model:\n```python\nimport timm\nmodel = timm.create\\_model('mobilenetv3\\_large\\_100', pretrained=True)\nmodel.eval()\n```\nTo load and preprocess the image:\n```python\nimport urllib\nfrom PIL import Image\nfrom timm.data import resolve\\_data\\_config\nfrom timm.data.transforms\\_factory import create\\_transform\nconfig = resolve\\_data\\_config({}, model=model)\ntransform = create\\_transform(\\*\\*config)\nurl, filename = (\""https://github.com/pytorch/hub/raw/master/images/dog.jpg\"", \""dog.jpg\"")\nurllib.request.urlretrieve(url, filename)\nimg = Image.open(filename).convert('RGB')\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\n```\nTo get the model predictions:\n```python\nimport torch\nwith torch.no\\_grad():\nout = model(tensor)\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\nprint(probabilities.shape)\n# prints: torch.Size([1000])\n```\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename = (\""https://raw.githubusercontent.com/pytorch/hub/master/imagenet\\_classes.txt\"", \""imagenet\\_classes.txt\"")\nurllib.request.urlretrieve(url, filename)\nwith open(\""imagenet\\_classes.txt\"", \""r\"") as f:\ncategories = [s.strip() for s in f.readlines()]\n# Print top categories per image\ntop5\\_prob, top5\\_catid = torch.topk(probabilities, 5)\nfor i in range(top5\\_prob.size(0)):\nprint(categories[top5\\_catid[i]], top5\\_prob[i].item())\n# prints class names and probabilities like:\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\n```\nReplace the model name with the variant you want to use, e.g. `mobilenetv3\\_large\\_100`. You can find the IDs in the model summaries at the top of this page. To extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature\\_extraction/), just change the name of the model you want to use. ## How do I finetune this model? You can finetune any of the pre-trained models just by changing the classifier (the last layer). ```python\nmodel = timm.create\\_model('mobilenetv3\\_large\\_100', pretrained=True, num\\_classes=NUM\\_FINETUNE\\_CLASSES)\n```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset. ## How do I train this model? You can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh. ## Citation\n```BibTeX\n@article{DBLP:journals/corr/abs-1905-02244,\nauthor = {Andrew Howard and\nMark Sandler and\nGrace Chu and\nLiang{-}Chieh Chen and\nBo Chen and\nMingxing Tan and\nWeijun Wang and\nYukun Zhu and\nRuoming Pang and\nVijay Vasudevan and\nQuoc V."", ""Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters.\\*\nThis model was contributed by [matthijs](https://huggingface.co/Matthijs). The original code and weights can be found [here for the main model](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet) and [here for DeepLabV3+](https://github.com/tensorflow/models/tree/master/research/deeplab). ## Usage tips\n- The checkpoints are named \\*\\*mobilenet\\\\_v2\\\\_\\*depth\\*\\\\_\\*size\\*\\*\\*, for example \\*\\*mobilenet\\\\_v2\\\\_1.0\\\\_224\\*\\*, where \\*\\*1.0\\*\\* is the depth multiplier (sometimes also referred to as \""alpha\"" or the width multiplier) and \\*\\*224\\*\\* is the resolution of the input images the model was trained on. - Even though the checkpoint is trained on images of specific size, the model will work on images of any size. The smallest supported image size is 32x32. - One can use [`MobileNetV2ImageProcessor`] to prepare images for the model. - The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes). However, the model predicts 1001 classes: the 1000 classes from ImageNet plus an extra “background” class (index 0). - The segmentation model uses a [DeepLabV3+](https://arxiv.org/abs/1802.02611) head. The available semantic segmentation checkpoints are pre-trained on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/). - The original TensorFlow checkpoints use different padding rules than PyTorch, requiring the model to determine the padding amount at inference time, since this depends on the input image size. To use native PyTorch padding behavior, create a [`MobileNetV2Config`] with `tf\\_padding = False`. Unsupported features:\n- The [`MobileNetV2Model`] outputs a globally pooled version of the last hidden state."", ""We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.\\*\nThis model was contributed by [matthijs](https://huggingface.co/Matthijs). The original code and weights can be found [here](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet\\_v1.md). ## Usage tips\n- The checkpoints are named \\*\\*mobilenet\\\\_v1\\\\_\\*depth\\*\\\\_\\*size\\*\\*\\*, for example \\*\\*mobilenet\\\\_v1\\\\_1.0\\\\_224\\*\\*, where \\*\\*1.0\\*\\* is the depth multiplier (sometimes also referred to as \""alpha\"" or the width multiplier) and \\*\\*224\\*\\* is the resolution of the input images the model was trained on.""]",The top-1 accuracy of `mobilenetv3_large_100` on ImageNet is 75.2%.,75.77%
What version of Python is required to contribute to 🤗 Transformers?,"[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# How to convert a 🤗 Transformers model to TensorFlow? Having multiple frameworks available to use with 🤗 Transformers gives you flexibility to play their strengths when\ndesigning your application, but it implies that compatibility must be added on a per-model basis. The good news is that\nadding TensorFlow compatibility to an existing model is simpler than [adding a new model from scratch](add\\_new\\_model)! Whether you wish to have a deeper understanding of large TensorFlow models, make a major open-source contribution, or\nenable TensorFlow for your model of choice, this guide is for you. This guide empowers you, a member of our community, to contribute TensorFlow model weights and/or\narchitectures to be used in 🤗 Transformers, with minimal supervision from the Hugging Face team. Writing a new model\nis no small feat, but hopefully this guide will make it less of a rollercoaster 🎢 and more of a walk in the park 🚶. Harnessing our collective experiences is absolutely critical to make this process increasingly easier, and thus we\nhighly encourage that you suggest improvements to this guide! Before you dive deeper, it is recommended that you check the following resources if you're new to 🤗 Transformers:\n- [General overview of 🤗 Transformers](add\\_new\\_model#general-overview-of-transformers)\n- [Hugging Face's TensorFlow Philosophy](https://huggingface.co/blog/tensorflow-philosophy)\nIn the remainder of this guide, you will learn what's needed to add a new TensorFlow model architecture, the\nprocedure to convert PyTorch into TensorFlow model weights, and how to efficiently debug mismatches across ML\nframeworks."", ""Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh. 1. Want to contribute a new model? We have added a \\*\\*detailed guide and templates\\*\\* to guide you in the process of adding a new model. You can find them in the [`templates`](./templates) folder of the repository. Be sure to check the [contributing guidelines](./CONTRIBUTING.md) and contact the maintainers or open an issue to collect feedbacks before starting your PR. Чтобы проверить, есть ли у каждой модели реализация на Flax, PyTorch или TensorFlow, или связанный с ней токенизатор, поддерживаемый библиотекой 🤗 Tokenizers, обратитесь к [этой таблице](https://huggingface.co/docs/transformers/index#supported-frameworks). Эти реализации были протестированы на нескольких наборах данных (см. примеры скриптов) и должны соответствовать производительности оригинальных реализаций. Более подробную информацию о производительности можно найти в разделе \""Примеры\"" [документации](https://github.com/huggingface/transformers/tree/main/examples). ## Изучи больше\n| Секция | Описание |\n|-|-|\n| [Документация](https://huggingface.co/docs/transformers/) | Полная документация по API и гайды |\n| [Краткие описания задач](https://huggingface.co/docs/transformers/task\\_summary) | Задачи поддерживаются 🤗 Transformers |\n| [Пособие по предварительной обработке](https://huggingface.co/docs/transformers/preprocessing) | Использование класса `Tokenizer` для подготовки данных для моделей |\n| [Обучение и доработка](https://huggingface.co/docs/transformers/training) | Использование моделей, предоставляемых 🤗 Transformers, в цикле обучения PyTorch/TensorFlow и API `Trainer`. |\n| [Быстрый тур: Тонкая настройка/скрипты использования](https://github.com/huggingface/transformers/tree/main/examples) | Примеры скриптов для тонкой настройки моделей на широком спектре задач |\n| [Совместное использование и загрузка моделей](https://huggingface.co/docs/transformers/model\\_sharing) | Загружайте и делитесь с сообществом своими доработанными моделями |\n## Цитирование\nТеперь у нас есть [статья](https://www.aclweb.org/anthology/2020.emnlp-demos.6/), которую можно цитировать для библиотеки 🤗 Transformers:\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\ntitle = \""Transformers: State-of-the-Art Natural Language Processing\"",\nauthor = \""Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\"",\nbooktitle = \""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\"",\nmonth = oct,\nyear = \""2020\"",\naddress = \""Online\"",\npublisher = \""Association for Computational Linguistics\"",\nurl = \""https://www.aclweb.org/anthology/2020.emnlp-demos.6\"",\npages = \""38--45\""\n}\n```"", ""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Contribute to 🤗 Transformers\nEveryone is welcome to contribute, and we value everybody's contribution. Code\ncontributions are not the only way to help the community. Answering questions, helping\nothers, and improving the documentation are also immensely valuable. It also helps us if you spread the word! Reference the library in blog posts\nabout the awesome projects it made possible, shout out on Twitter every time it has\nhelped you, or simply ⭐️ the repository to say thank you. However you choose to contribute, please be mindful and respect our\n[code of conduct](https://github.com/huggingface/transformers/blob/main/CODE\\_OF\\_CONDUCT.md). \\*\\*This guide was heavily inspired by the awesome [scikit-learn guide to contributing](https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md).\\*\\*\n## Ways to contribute\nThere are several ways you can contribute to 🤗 Transformers:\n\\* Fix outstanding issues with the existing code."", ""ow to add BigBird to 🤗 Transformers? =====================================\nMentor: [Patrick](https://github.com/patrickvonplaten)\nBegin: 12.02.2020\nEstimated End: 19.03.2020\nContributor: [Vasudev](https://github.com/thevasudevgupta)\nAdding a new model is often difficult and requires an in-depth knowledge\nof the 🤗 Transformers library and ideally also of the model's original\nrepository. At Hugging Face, we are trying to empower the community more\nand more to add models independently. The following sections explain in detail how to add BigBird\nto Transformers. You will work closely with Patrick to\nintegrate BigBird into Transformers. By doing so, you will both gain a\ntheoretical and deep practical understanding of BigBird. But more importantly, you will have made a major\nopen-source contribution to Transformers. Along the way, you will:\n- get insights into open-source best practices\n- understand the design principles of one of the most popular NLP\nlibraries\n- learn how to do efficiently test large NLP models\n- learn how to integrate Python utilities like `black`, `ruff`,\n`make fix-copies` into a library to always ensure clean and readable\ncode\nTo start, let's try to get a general overview of the Transformers\nlibrary."", ""This was an\nincredible effort led by many different people. To maximize the GPU during training, several solutions were explored\nand in the end, [Megatron-Deepspeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed) was chosen to train the end model. This meant that the code as-is wasn't necessarily compatible with the `transformers`\nlibrary. # Porting to transformers\nBecause of the original training code, we set out to do something which we regularly\ndo: port an existing model to `transformers`. The goal was to extract from the\ntraining code the relevant parts and implement it within `transformers`. This effort was tackled by [Younes](/ybelkada). This is by no means a small effort as it took almost a month and [200 commits](https://github.com/huggingface/transformers/pull/17474/commits) to get there. There are several things to note that will come back later:\nWe needed to have smaller models [bigscience/bigscience-small-testing](https://huggingface.co/bigscience/bigscience-small-testing) and [bigscience/bloom-560m](https://huggingface.co/bigscience/bloom-560m).""]","To contribute to 🤗 Transformers, Python version 3.6 or higher is required.",Python 3.8 or above
What is the default value of HF_INFERENCE_ENDPOINT?,"[""### DO\\_NOT\\_TRACK\nBoolean value. Equivalent to `HF\\_HUB\\_DISABLE\\_TELEMETRY`. When set to true, telemetry is globally disabled in the Hugging Face Python ecosystem (`transformers`, `diffusers`, `gradio`, etc.). See https://consoledonottrack.com/ for more details. ### NO\\_COLOR\nBoolean value. When set, `huggingface-cli` tool will not print any ANSI color. See [no-color.org](https://no-color.org/). ### XDG\\_CACHE\\_HOME\nUsed only when `HF\\_HOME` is not set! This is the default way to configure where [user-specific non-essential (cached) data should be written](https://wiki.archlinux.org/title/XDG\\_Base\\_Directory)\non linux machines. If `HF\\_HOME` is not set, the default home will be `\""$XDG\\_CACHE\\_HOME/huggingface\""` instead\nof `\""~/.cache/huggingface\""`."", ""If the request times out, `huggingface\\_hub` will default to the locally cached files. Setting a lower value speeds up the workflow for machines with a slow connection that have already cached files. A higher value guarantees the metadata call to succeed in more cases. Default to 10s. ### HF\\_HUB\\_DOWNLOAD\\_TIMEOUT\nInteger value to define the number of seconds to wait for server response when downloading a file. If the request times out, a TimeoutError is raised. Setting a higher value is beneficial on machine with a slow connection. A smaller value makes the process fail quicker in case of complete network outage. Default to 10s. ## Boolean values\nThe following environment variables expect a boolean value. The variable will be considered\nas `True` if its value is one of `{\""1\"", \""ON\"", \""YES\"", \""TRUE\""}` (case-insensitive). Any other value\n(or undefined) will be considered as `False`. ### HF\\_HUB\\_OFFLINE\nIf set, no HTTP calls will me made to the Hugging Face Hub. If you try to download files, only the cached files will be accessed. If no cache file is detected, an error is raised This is useful in case your network is slow and you don't care about having the latest version of a file. If `HF\\_HUB\\_OFFLINE=1` is set as environment variable and you call any method of [`HfApi`], an [`~huggingface\\_hub.utils.OfflineModeIsEnabled`] exception will be raised."", ""Pause and Resume your Endpoint\nYou can `pause` & `resume` endpoints to save cost and configurations. Please note that if your endpoint is in a `failed` state, you will need to create a new endpoint. To `pause`/`resume` your endpoint, navigate to the \""overview\"" tab and click the button at top right corner, which will show \""Pause endpoint\"" to pause, or \""Resume endpoint\"" to reactivate the paused endpoint. When pausing an endpoint the min & max number of replicas will be set to 0. When resuming an endpoint the min & max number of replicas will be set to 1. This allows you to programmatically pause and resume your endpoint by updating the \""min\\_replicas\"" and \""max\\_replicas\"" fields in the API. Paused inference endpoints will have the following status: `PAUSED`. Paused endpoints will be NOT be billed until resumed. Pausing & Resuming an endpoint is a great way to save costs when you don't need your endpoint to be running. For example, you can easily pause your endpoint during the night or weekends. You should pause your endpoint when you don't need it for the time being. The url of your endpoint will remain the same, even if you pause and resume it. This means that you can pause your endpoint and resume it later without having to update your code. ## Pause an Inference Endpoint\nTo pause an endpoint, navigate to the \""overview\"" tab and click the button at top right corner, which says \""Pause endpoint\"". ![Pause an Inference Endpoint](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/pause_endpoint.png)\nAfter clicking the button, you will be asked to confirm the action. Click \""Pause {ENDPOINT-NAME}\"" to confirm. ![Pause modal confirm Inference Endpoint](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/pause_endpoint_confirm.png)\nAfter that your replicas will be set to 0 and your endpoint will be paused."", ""Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes. ![overview](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png)\n## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉! ![run inference](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png)"", ""By default, `huggingface\\_hub` uses the Python-based `requests.get` and `requests.post` functions. Although these are reliable and versatile, they may not be the most efficient choice for machines with high bandwidth. [`hf\\_transfer`](https://github.com/huggingface/hf\\_transfer) is a Rust-based package developed to maximize the bandwidth used by dividing large files into smaller parts and transferring them simultaneously using multiple threads. This approach can potentially double the transfer speed. To use `hf\\_transfer`, you need to install it separately [from PyPI](https://pypi.org/project/hf-transfer/) and set `HF\\_HUB\\_ENABLE\\_HF\\_TRANSFER=1` as an environment variable. Please note that using `hf\\_transfer` comes with certain limitations. Since it is not purely Python-based, debugging errors may be challenging. Additionally, `hf\\_transfer` lacks several user-friendly features such as resumable downloads and proxies. These omissions are intentional to maintain the simplicity and speed of the Rust logic. Consequently, `hf\\_transfer` is not enabled by default in `huggingface\\_hub`. ## Deprecated environment variables\nIn order to standardize all environment variables within the Hugging Face ecosystem, some variables have been marked as deprecated. Although they remain functional, they no longer take precedence over their replacements. The following table outlines the deprecated variables and their corresponding alternatives:\n| Deprecated Variable | Replacement |\n| --- | --- |\n| `HUGGINGFACE\\_HUB\\_CACHE` | `HF\\_HUB\\_CACHE` |\n| `HUGGINGFACE\\_ASSETS\\_CACHE` | `HF\\_ASSETS\\_CACHE` |\n| `HUGGING\\_FACE\\_HUB\\_TOKEN` | `HF\\_TOKEN` |\n| `HUGGINGFACE\\_HUB\\_VERBOSITY` | `HF\\_HUB\\_VERBOSITY` |\n## From external tools\nSome environment variables are not specific to `huggingface\\_hub` but are still taken into account when they are set.""]","The default value of `HF_INFERENCE_ENDPOINT` is not explicitly mentioned in the provided context. If you need specific details about this environment variable, please refer to the official Hugging Face documentation or resources.","""https://api-inference.huggingface.com"""
What is the name of the ResNet variant that uses squeeze-and-excitation blocks?,"[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# UNet1DModel\nThe [UNet](https://huggingface.co/papers/1505.04597) model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in 🤗 Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in 🤗 Diffusers, depending on it's number of dimensions and whether it is a conditional model or not. This is a 1D UNet model. The abstract from the paper is:\n\\*There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ConvNeXT\n## Overview\nThe ConvNeXT model was proposed in [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The abstract from the paper is the following:\n\\*The \""Roaring 20s\"" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers\n(e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide\nvariety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive\nbiases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually \""modernize\"" a standard ResNet toward the design\nof a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models\ndubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy\nand outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.\\*\n![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnext_architecture.jpg)\n ConvNeXT architecture."", ""EfficientNet (Knapsack Pruned)\n\\*\\*EfficientNet\\*\\* is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a \\*compound coefficient\\*. Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use \\\\( 2^N \\\\) times more computational resources, then we can simply increase the network depth by \\\\( \\alpha ^ N \\\\), width by \\\\( \\beta ^ N \\\\), and image size by \\\\( \\gamma ^ N \\\\), where \\\\( \\alpha, \\beta, \\gamma \\\\) are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\\\( \\phi \\\\) to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of [MobileNetV2](https://paperswithcode.com/method/mobilenetv2), in addition to [squeeze-and-excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block)."", ""24 block variant, 79.2 top-1. \\* Add ResNet51-Q model w/ pretrained weights at 82.36 top-1. \\* NFNet inspired block layout with quad layer stem and no maxpool\n\\* Same param count (35.7M) and throughput as ResNetRS-50 but +1.5 top-1 @ 224x224 and +2.5 top-1 at 288x288\n### May 25, 2021\n\\* Add LeViT, Visformer, Convit (PR by Aman Arora), Twins (PR by paper authors) transformer models\n\\* Cleanup input\\_size/img\\_size override handling and testing for all vision transformer models\n\\* Add `efficientnetv2\\_rw\\_m` model and weights (started training before official code). 84.8 top-1, 53M params. ### May 14, 2021\n\\* Add EfficientNet-V2 official model defs w/ ported weights from official [Tensorflow/Keras](https://github.com/google/automl/tree/master/efficientnetv2) impl. \\* 1k trained variants: `tf\\_efficientnetv2\\_s/m/l`\n\\* 21k trained variants: `tf\\_efficientnetv2\\_s/m/l\\_in21k`\n\\* 21k pretrained -> 1k fine-tuned: `tf\\_efficientnetv2\\_s/m/l\\_in21ft1k`\n\\* v2 models w/ v1 scaling: `tf\\_efficientnetv2\\_b0` through `b3`\n\\* Rename my prev V2 guess `efficientnet\\_v2s` -> `efficientnetv2\\_rw\\_s`\n\\* Some blank `efficientnetv2\\_\\*` models in-place for future native PyTorch training\n### May 5, 2021\n\\* Add MLP-Mixer models and port pretrained weights from [Google JAX impl](https://github.com/google-research/vision\\_transformer/tree/linen)\n\\* Add CaiT models and pretrained weights from [FB](https://github.com/facebookresearch/deit)\n\\* Add ResNet-RS models and weights from [TF](https://github.com/tensorflow/tpu/tree/master/models/official/resnet/resnet\\_rs)."", ""## How do I train this model? You can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh. ## Citation\n```BibTeX\n@misc{hu2019squeezeandexcitation,\ntitle={Squeeze-and-Excitation Networks},\nauthor={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},\nyear={2019},\neprint={1709.01507},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n```""]",The ResNet variant that uses squeeze-and-excitation blocks is known as Squeeze-and-Excitation Networks.,SE ResNet
What Python package needs to be installed for the Gradio demo?,"[""Gradio Demo: white\\_noise\\_vid\\_not\\_playable\n```\n!pip install -q gradio opencv-python\n```\n```\nimport cv2\nimport gradio as gr\nimport numpy as np\ndef gif\\_maker():\nimg\\_array = []\nheight, width = 50, 50\nfor i in range(30):\nimg\\_array.append(np.random.randint(0, 255, size=(height, width, 3)).astype(np.uint8))\noutput\\_file = \""test.mp4\""\nout = cv2.VideoWriter(output\\_file, cv2.VideoWriter\\_fourcc(\\*'mp4v'), 15, (height, width))\nfor i in range(len(img\\_array)):\nout.write(img\\_array[i])\nout.release()\nreturn output\\_file, output\\_file\ndemo = gr.Interface(gif\\_maker, inputs=None, outputs=[gr.Video(), gr.File()])\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: asr\n```\n!pip install -q gradio torch torchaudio transformers\n```\n```\nimport gradio as gr\nfrom transformers import pipeline\nimport numpy as np\ntranscriber = pipeline(\""automatic-speech-recognition\"", model=\""openai/whisper-base.en\"")\ndef transcribe(audio):\nsr, y = audio\ny = y.astype(np.float32)\ny /= np.max(np.abs(y))\nreturn transcriber({\""sampling\\_rate\"": sr, \""raw\"": y})[\""text\""]\ndemo = gr.Interface(\ntranscribe,\ngr.Audio(sources=[\""microphone\""]),\n\""text\"",\n)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: gpt2\\_xl\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\ntitle = \""gpt2-xl\""\nexamples = [\n[\""The tower is 324 metres (1,063 ft) tall,\""],\n[\""The Moon's orbit around Earth has\""],\n[\""The smooth Borealis basin in the Northern Hemisphere covers 40%\""],\n]\ndemo = gr.load(\n\""huggingface/gpt2-xl\"",\ninputs=gr.Textbox(lines=5, max\\_lines=6, label=\""Input Text\""),\ntitle=title,\nexamples=examples,\n)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""Gradio Demo: colorpicker\\_component\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.ColorPicker()\ndemo.launch()\n```"", ""Gradio Demo: hello\\_blocks\\_decorator\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\nname = gr.Textbox(label=\""Name\"")\noutput = gr.Textbox(label=\""Output Box\"")\ngreet\\_btn = gr.Button(\""Greet\"")\n@greet\\_btn.click(inputs=name, outputs=output)\ndef greet(name):\nreturn \""Hello \"" + name + \""!\""\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```""]","For the Gradio demos, you need to install the `gradio` package. Depending on the specific demo, additional packages may also be required, such as `opencv-python`, `torch`, `torchaudio`, and `transformers`. Here are the installation commands for the demos:

1. For the white noise video demo:
   ```
   !pip install -q gradio opencv-python
   ```

2. For the automatic speech recognition (ASR) demo:
   ```
   !pip install -q gradio torch torchaudio transformers
   ```

3. For the GPT-2 XL demo:
   ```
   !pip install -q gradio
   ```

4. For the color picker component demo:
   ```
   !pip install -q gradio
   ```

5. For the hello blocks decorator demo:
   ```
   !pip install -q gradio
   ```",gradio
What does ZeRO-powered data parallelism store on each GPU?,"[""Distributed training is the key to enable training such large ML models. There have been major recent advances in the field of \\*\\*Distributed Training at Scale\\*\\*. Few the most notable advances are given below:\n1. Data Parallelism using ZeRO - Zero Redundancy Optimizer [2]\n1. Stage 1: Shards optimizer states across data parallel workers/GPUs\n2. Stage 2: Shards optimizer states + gradients across data parallel workers/GPUs\n3. Stage 3: Shards optimizer states + gradients + model parameters across data parallel workers/GPUs\n4. CPU Offload: Offloads the gradients + optimizer states to CPU building on top of ZERO Stage 2 [3]\n2. Tensor Parallelism [4]: Form of model parallelism wherein sharding parameters of individual layers with huge number of parameters across accelerators/GPUs is done in a clever manner to achieve parallel computation while avoiding expensive communication synchronization overheads. 3. Pipeline Parallelism [5]: Form of model parallelism wherein different layers of the model are put across different accelerators/GPUs and pipelining is employed to keep all the accelerators running simultaneously. Here, for instance, the second accelerator/GPU computes on the first micro-batch while the first accelerator/GPU computes on the second micro-batch."", ""You can see two factors at play leading to better performance here. 1. The throughput here was improved by using Tensor Parallelism (TP) instead of the Pipeline Parallelism (PP) of Accelerate. Because Accelerate is meant to be very generic it is also unfortunately hard to maximize the GPU usage. All computations are done first on GPU 0, then on GPU 1, etc. until GPU 8, which means 7 GPUs are idle all the time. DeepSpeed-Inference on the other hand uses TP, meaning it will send tensors to all GPUs, compute part of the generation on each GPU and then all GPUs communicate to each other the results, then move on to the next layer. That means all GPUs are active at once but they need to communicate much more. 2. DeepSpeed-Inference also uses custom CUDA kernels to avoid allocating too much memory and doing tensor copying to and from GPUs. The effect of this is lesser memory requirements and fewer kernel starts which improves the throughput and allows for bigger batch sizes leading to higher overall throughput. If you are interested in more examples you can take a look at [Accelerate GPT-J inference with DeepSpeed-Inference on GPUs](https://www.philschmid.de/gptj-deepspeed-inference) or [Accelerate BERT inference with DeepSpeed-Inference on GPUs](https://www.philschmid.de/bert-deepspeed-inference). ## Deepspeed ZeRO-Inference\n[Deepspeed ZeRO](https://www.deepspeed.ai/tutorials/zero/) uses a magical sharding approach which can take almost any model and scale it across a few or hundreds of GPUs and the do training or inference on it. ### Setup\n```\npip install deepspeed\n```\n### Run\nNote that the script currently runs the same inputs on all GPUs, but you can run a different stream on each GPU, and get `n\\_gpu` times faster throughput. You can't do that with Deepspeed-Inference. ```\ndeepspeed --num\\_gpus 8 bloom-inference-scripts/bloom-ds-zero-inference.py --name bigscience/bloom --batch\\_size 1 --benchmark\n```\nPlease remember that with ZeRO the user can generate multiple unique streams at the same time - and thus the overall performance should be throughput in secs/token divided by number of participating GPUs - so 8x to 16x faster depending on whether 8 or 16 GPUs were used! You can also try the offloading solutions with just one smallish GPU, which will take a long time to run, but if you don't have 8 huge GPUs this is as good as it gets. CPU-Offload (1x GPUs):\n```\ndeepspeed --num\\_gpus 1 bloom-inference-scripts/bloom-ds-zero-inference.py --name bigscience/bloom --batch\\_size 8 --cpu\\_offload --benchmark\n```\nNVMe-Offload (1x GPUs):\n```\ndeepspeed --num\\_gpus 1 bloom-inference-scripts/bloom-ds-zero-inference.py --name bigscience/bloom --batch\\_size 8 --nvme\\_offload\\_path=/path/to/nvme\\_offload --benchmark\n```\nmake sure to adjust `/path/to/nvme\\_offload` to somewhere you have ~400GB of free memory on a fast NVMe drive."", ""Additionally DeepSpeed is currently developing a related product called Deepspeed-Inference which has no relationship\nto the ZeRO technology, but instead uses tensor parallelism to scale models that can't fit onto a single GPU. This is a\nwork in progress and we will provide the integration once that product is complete. ### Memory Requirements\nSince Deepspeed ZeRO can offload memory to CPU (and NVMe) the framework provides utils that allow one to tell how much CPU and GPU memory will be needed depending on the number of GPUs being used. Let's estimate how much memory is needed to finetune \""bigscience/T0\\_3B\"" on a single GPU:\n```bash\n$ python -c 'from transformers import AutoModel; \\\nfrom deepspeed.runtime.zero.stage3 import estimate\\_zero3\\_model\\_states\\_mem\\_needs\\_all\\_live; \\\nmodel = AutoModel.from\\_pretrained(\""bigscience/T0\\_3B\""); \\\nestimate\\_zero3\\_model\\_states\\_mem\\_needs\\_all\\_live(model, num\\_gpus\\_per\\_node=1, num\\_nodes=1)'\n[...]\nEstimated memory needed for params, optim states and gradients for a:\nHW: Setup with 1 node, 1 GPU per node."", ""Why would you want to use DeepSpeed with just one GPU? 1. It has a ZeRO-offload feature which can delegate some computations and memory to the host's CPU and RAM, and thus\nleave more GPU resources for model's needs - e.g. larger batch size, or enabling a fitting of a very big model which\nnormally won't fit. 2. It provides a smart GPU memory management system, that minimizes memory fragmentation, which again allows you to fit\nbigger models and data batches. While we are going to discuss the configuration in details next, the key to getting a huge improvement on a single GPU\nwith DeepSpeed is to have at least the following configuration in the configuration file:\n```json\n{\n\""zero\\_optimization\"": {\n\""stage\"": 2,\n\""offload\\_optimizer\"": {\n\""device\"": \""cpu\"",\n\""pin\\_memory\"": true\n},\n\""allgather\\_partitions\"": true,\n\""allgather\\_bucket\\_size\"": 2e8,\n\""reduce\\_scatter\"": true,\n\""reduce\\_bucket\\_size\"": 2e8,\n\""overlap\\_comm\"": true,\n\""contiguous\\_gradients\"": true\n}\n}\n```\nwhich enables optimizer offload and some other important features."", ""It has already been\ndiscussed in [ZeRO Data Parallelism](#zero-data-parallelism). Normally it's a standalone feature that doesn't require PP or TP. But it can be combined with PP and TP. When ZeRO-DP is combined with PP (and optionally TP) it typically enables only ZeRO stage 1 (optimizer sharding). While it's theoretically possible to use ZeRO stage 2 (gradient sharding) with Pipeline Parallelism, it will have negative\nperformance impacts. There would need to be an additional reduce-scatter collective for every micro-batch to aggregate\nthe gradients before sharding, which adds a potentially significant communication overhead. By nature of Pipeline Parallelism,\nsmall micro-batches are used and instead the focus is on trying to balance arithmetic intensity (micro-batch size) with\nminimizing the Pipeline bubble (number of micro-batches). Therefore those communication costs are going to impact the performance. In addition, there are already fewer layers than normal due to PP and so the memory savings won't be huge. PP already\nreduces gradient size by ``1/PP``, and so gradient sharding savings on top of that are less significant than pure DP.""]","ZeRO-powered data parallelism stores different components on each GPU depending on the stage of ZeRO being used:

1. **Stage 1**: Shards optimizer states across data parallel workers/GPUs.
2. **Stage 2**: Shards optimizer states and gradients across data parallel workers/GPUs.
3. **Stage 3**: Shards optimizer states, gradients, and model parameters across data parallel workers/GPUs.

This approach allows for efficient memory usage and enables the training of large models across multiple GPUs.","A slice of the model parameters, gradients, and optimizer states."
What is the license URL for TheBloke/Llama2-22B-Daydreamer-v3-GGUF?,"[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Open-Llama\nThis model is in maintenance mode only, we don't accept any new PRs changing its code. If you run into any issues running this model, please reinstall the last version that supported this model: v4.31.0. You can do so by running the following command: `pip install -U transformers==4.31.0`. This model differs from the [OpenLLaMA models](https://huggingface.co/models?search=openllama) on the Hugging Face Hub, which primarily use the [LLaMA](llama) architecture. ## Overview\nThe Open-Llama model was proposed in the open source Open-Llama project by community developer s-JoL. The model is mainly based on LLaMA with some modifications, incorporating memory-efficient attention from Xformers, stable embedding from Bloom, and shared input-output embedding from PaLM. And the model is pre-trained on both Chinese and English, which gives it better performance on Chinese language tasks."", ""Licenses\nYou are able to add a license to any repo that you create on the Hugging Face Hub to let other users know about the permissions that you want to attribute to your code or data. The license can be specified in your repository's `README.md` file, known as a \\*card\\* on the Hub, in the card's metadata section. Remember to seek out and respect a project's license if you're considering using their code or data. A full list of the available licenses is available here:\n\nFullname | License identifier (to use in model card)\n--- | ---\nApache license 2.0 | `apache-2.0`\nMIT | `mit`\nOpenRAIL license family | `openrail`\nBigScience OpenRAIL-M | `bigscience-openrail-m`\nCreativeML OpenRAIL-M | `creativeml-openrail-m`\nBigScience BLOOM RAIL 1.0 | `bigscience-bloom-rail-1.0`\nBigCode Open RAIL-M v1 | `bigcode-openrail-m`\nAcademic Free License v3.0 | `afl-3.0`\nArtistic license 2.0 | `artistic-2.0`\nBoost Software License 1.0 | `bsl-1.0`\nBSD license family | `bsd`\nBSD 2-clause \""Simplified\"" license | `bsd-2-clause`\nBSD 3-clause \""New\"" or \""Revised\"" license | `bsd-3-clause`\nBSD 3-clause Clear license | `bsd-3-clause-clear`\nComputational Use of Data Agreement | `c-uda`\nCreative Commons license family | `cc`\nCreative Commons Zero v1.0 Universal | `cc0-1.0`\nCreative Commons Attribution 2.0 | `cc-by-2.0`\nCreative Commons Attribution 2.5 | `cc-by-2.5`\nCreative Commons Attribution 3.0 | `cc-by-3.0`\nCreative Commons Attribution 4.0 | `cc-by-4.0`\nCreative Commons Attribution Share Alike 3.0 | `cc-by-sa-3.0`\nCreative Commons Attribution Share Alike 4.0 | `cc-by-sa-4.0`\nCreative Commons Attribution Non Commercial 2.0 | `cc-by-nc-2.0`\nCreative Commons Attribution Non Commercial 3.0 | `cc-by-nc-3.0`\nCreative Commons Attribution Non Commercial 4.0 | `cc-by-nc-4.0`\nCreative Commons Attribution No Derivatives 4.0 | `cc-by-nd-4.0`\nCreative Commons Attribution Non Commercial No Derivatives 3.0 | `cc-by-nc-nd-3.0`\nCreative Commons Attribution Non Commercial No Derivatives 4.0 | `cc-by-nc-nd-4.0`\nCreative Commons Attribution Non Commercial Share Alike 2.0 | `cc-by-nc-sa-2.0`\nCreative Commons Attribution Non Commercial Share Alike 3.0 | `cc-by-nc-sa-3.0`\nCreative Commons Attribution Non Commercial Share Alike 4.0 | `cc-by-nc-sa-4.0`\nCommunity Data License Agreement – Sharing, Version 1.0 | `cdla-sharing-1.0`\nCommunity Data License Agreement – Permissive, Version 1.0 | `cdla-permissive-1.0`\nCommunity Data License Agreement – Permissive, Version 2.0 | `cdla-permissive-2.0`\nDo What The F\\*ck You Want To Public License | `wtfpl`\nEducational Community License v2.0 | `ecl-2.0`\nEclipse Public License 1.0 | `epl-1.0`\nEclipse Public License 2.0 | `epl-2.0`\nEtalab Open License 2.0 | `etalab-2.0`\nEuropean Union Public License 1.1 | `eupl-1.1`\nGNU Affero General Public License v3.0 | `agpl-3.0`\nGNU Free Documentation License family | `gfdl`\nGNU General Public License family | `gpl`\nGNU General Public License v2.0 | `gpl-2.0`\nGNU General Public License v3.0 | `gpl-3.0`\nGNU Lesser General Public License family | `lgpl`\nGNU Lesser General Public License v2.1 | `lgpl-2.1`\nGNU Lesser General Public License v3.0 | `lgpl-3.0`\nISC | `isc`\nLaTeX Project Public License v1.3c | `lppl-1.3c`\nMicrosoft Public License | `ms-pl`\nMozilla Public License 2.0 | `mpl-2.0`\nOpen Data Commons License Attribution family | `odc-by`\nOpen Database License family | `odbl`\nOpen Rail++-M License | `openrail++`\nOpen Software License 3.0 | `osl-3.0`\nPostgreSQL License | `postgresql`\nSIL Open Font License 1.1 | `ofl-1.1`\nUniversity of Illinois/NCSA Open Source License | `ncsa`\nThe Unlicense | `unlicense`\nzLib License | `zlib`\nOpen Data Commons Public Domain Dedication and License | `pddl`\nLesser General Public License For Linguistic Resources | `lgpl-lr`\nDeepFloyd IF Research License Agreement | `deepfloyd-if-license`\nLlama 2 Community License Agreement | `llama2`\nUnknown | `unknown`\nOther | `other`\nIn case of `license: other` please add the license's text to a `LICENSE` file inside your repo (or contact us to add the license you use to this list), and set a name for it in `license\\_name`."", ""Refer to the documentation of Llama2 which can be found [here](llama2). ## Resources\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with LLaMA. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource. - A [notebook](https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb#scrollTo=f04ba4d2) on how to use prompt tuning to adapt the LLaMA model for text classification task. 🌎\n- [StackLLaMA: A hands-on guide to train LLaMA with RLHF](https://huggingface.co/blog/stackllama#stackllama-a-hands-on-guide-to-train-llama-with-rlhf), a blog post about how to train LLaMA to answer questions on [Stack Exchange](https://stackexchange.com/) with RLHF. ⚗️ Optimization\n- A [notebook](https://colab.research.google.com/drive/1SQUXq1AMZPSLD4mk3A3swUIc6Y2dclme?usp=sharing) on how to fine-tune LLaMA model using xturing library on GPU which has limited memory. 🌎\n⚡️ Inference\n- A [notebook](https://colab.research.google.com/github/DominguesM/alpaca-lora-ptbr-7b/blob/main/notebooks/02%20-%20Evaluate.ipynb) on how to run the LLaMA Model using PeftModel from the 🤗 PEFT library. 🌎\n- A [notebook](https://colab.research.google.com/drive/1l2GiSSPbajVyp2Nk3CFT4t3uH6-5TiBe?usp=sharing) on how to load a PEFT adapter LLaMA model with LangChain. 🌎\n🚀 Deploy\n- A [notebook](https://colab.research.google.com/github/lxe/simple-llama-finetuner/blob/master/Simple\\_LLaMA\\_FineTuner.ipynb#scrollTo=3PM\\_DilAZD8T) on how to fine-tune LLaMA model using LoRA method via the 🤗 PEFT library with intuitive UI. 🌎\n- A [notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction\\_to\\_amazon\\_algorithms/jumpstart-foundation-models/text-generation-open-llama.ipynb) on how to deploy Open-LLaMA model for text generation on Amazon SageMaker. 🌎\n## LlamaConfig\n[[autodoc]] LlamaConfig\n## LlamaTokenizer\n[[autodoc]] LlamaTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## LlamaTokenizerFast\n[[autodoc]] LlamaTokenizerFast\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- update\\_post\\_processor\n- save\\_vocabulary\n## LlamaModel\n[[autodoc]] LlamaModel\n- forward\n## LlamaForCausalLM\n[[autodoc]] LlamaForCausalLM\n- forward\n## LlamaForSequenceClassification\n[[autodoc]] LlamaForSequenceClassification\n- forward\n## FlaxLlamaModel\n[[autodoc]] FlaxLlamaModel\n- \\_\\_call\\_\\_\n## FlaxLlamaForCausalLM\n[[autodoc]] FlaxLlamaForCausalLM\n- \\_\\_call\\_\\_"", ""--\ntitle: \""Code Llama: Llama 2 learns to code\""\nthumbnail: /blog/assets/160\\_codellama/thumbnail.jpg\nauthors:\n- user: philschmid\n- user: osanseviero\n- user: pcuenq\n- user: lewtun\n- user: lvwerra\n- user: loubnabnl\n- user: ArthurZ\n- user: joaogante\n---\n# Code Llama: Llama 2 learns to code\n## Introduction\nCode Llama is a family of state-of-the-art, open-access versions of [Llama 2](https://huggingface.co/blog/llama2) specialized on code tasks, and we’re excited to release integration in the Hugging Face ecosystem! Code Llama has been released with the same permissive community license as Llama 2 and is available for commercial use. Today, we’re excited to release:\n- Models on the Hub with their model cards and license\n- Transformers integration\n- Integration with Text Generation Inference for fast and efficient production-ready inference\n- Integration with Inference Endpoints\n- Integration with VS Code extension\n- Code benchmarks\nCode LLMs are an exciting development for software engineers because they can boost productivity through code completion in IDEs, take care of repetitive or annoying tasks like writing docstrings, or create unit tests. ## Table of Contents\n- [Introduction](#introduction)\n- [Table of Contents](#table-of-contents)\n- [What’s Code Llama?](#whats-code-llama)\n- [How to use Code Llama?](#how-to-use-code-llama)\n- [Demo](#demo)\n- [Transformers](#transformers)\n- [A Note on dtypes](#a-note-on-dtypes)\n- [Code Completion](#code-completion)\n- [Code Infilling](#code-infilling)\n- [Conversational Instructions](#conversational-instructions)\n- [4-bit Loading](#4-bit-loading)\n- [Using text-generation-inference and Inference Endpoints](#using-text-generation-inference-and-inference-endpoints)\n- [Using VS Code extension](#using-vs-code-extension)\n- [Evaluation](#evaluation)\n- [Additional Resources](#additional-resources)\n## What’s Code Llama? The Code Llama release introduces a family of models of 7, 13, and 34 billion parameters. The base models are initialized from Llama 2 and then trained on 500 billion tokens of code data. Meta fine-tuned those base models for two different flavors: a Python specialist (100 billion additional tokens) and an instruction fine-tuned version, which can understand natural language instructions. The models show state-of-the-art performance in Python, C++, Java, PHP, C#, TypeScript, and Bash. The 7B and 13B base and instruct variants support infilling based on surrounding content, making them ideal for use as code assistants. Code Llama was trained on a 16k context window. In addition, the three model variants had additional long-context fine-tuning, allowing them to manage a context window of up to 100,000 tokens. Increasing Llama 2’s 4k context window to Code Llama’s 16k (that can extrapolate up to 100k) was possible due to recent developments in RoPE scaling. The community found that Llama’s position embeddings can be interpolated linearly or in the frequency domain, which eases the transition to a larger context window through fine-tuning. In the case of Code Llama, the frequency domain scaling is done with a slack: the fine-tuning length is a fraction of the scaled pretrained length, giving the model powerful extrapolation capabilities. ![Training Process](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/160\\_codellama/training-process.jpg \""Training Process\"")\nAll models were initially trained with 500 billion tokens on a near-deduplicated dataset of publicly available code."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Scenes\nUnder construction 🚧.""]",I cannot provide information about specific models or their licenses outside of the context provided. Please refer to the Hugging Face Hub for details on model licenses.,https://huggingface.co/TheBloke/Llama2-22B-Daydreamer-v3-GGUF/blob/main/LICENSE.txt
What is the recommended scheduler for the IP-Adapter face model?,"[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Latent Consistency Model Multistep Scheduler\n## Overview\nMultistep and onestep scheduler (Algorithm 3) introduced alongside latent consistency models in the paper [Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference](https://arxiv.org/abs/2310.04378) by Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. This scheduler should be able to generate good samples from [`LatentConsistencyModelPipeline`] in 1-8 steps. ## LCMScheduler\n[[autodoc]] LCMScheduler"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# IP-Adapter\n[IP-Adapter](https://hf.co/papers/2308.06721) is a lightweight adapter that enables prompting a diffusion model with an image. This method decouples the cross-attention layers of the image and text features. The image features are generated from an image encoder. Files generated from IP-Adapter are only ~100MBs. Learn how to load an IP-Adapter checkpoint and image in the [IP-Adapter](../../using-diffusers/loading\\_adapters#ip-adapter) loading guide. ## IPAdapterMixin\n[[autodoc]] loaders.ip\\_adapter.IPAdapterMixin"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# IPNDMScheduler\n`IPNDMScheduler` is a fourth-order Improved Pseudo Linear Multistep scheduler. The original implementation can be found at [crowsonkb/v-diffusion-pytorch](https://github.com/crowsonkb/v-diffusion-pytorch/blob/987f8985e38208345c1959b0ea767a625831cc9b/diffusion/sampling.py#L296). ## IPNDMScheduler\n[[autodoc]] IPNDMScheduler\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling\\_utils.SchedulerOutput"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# DPMSolverSinglestepScheduler\n`DPMSolverSinglestepScheduler` is a single step scheduler from [DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps](https://huggingface.co/papers/2206.00927) and [DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models](https://huggingface.co/papers/2211.01095) by Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPMSolver (and the improved version DPMSolver++) is a fast dedicated high-order solver for diffusion ODEs with convergence order guarantee."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Schedulers\n[[open-in-colab]]\nDiffusion pipelines are inherently a collection of diffusion models and schedulers that are partly independent from each other. This means that one is able to switch out parts of the pipeline to better customize\na pipeline to one's use case. The best example of this is the [Schedulers](../api/schedulers/overview). Whereas diffusion models usually simply define the forward pass from noise to a less noisy sample,\nschedulers define the whole denoising process, \\*i.e.\\*:\n- How many denoising steps?""]","The recommended scheduler for the IP-Adapter is the `IPNDMScheduler`, which is a fourth-order Improved Pseudo Linear Multistep scheduler. This scheduler is designed to work effectively with diffusion models, including those utilizing the IP-Adapter.",DDIMScheduler and EulerDiscreteScheduler
What is one advantage of using a generator of lists of texts in `train_new_from_iterator()`?,"[""You can use anything working as an iterator here, be it a\n`List`{.interpreted-text role=\""obj\""}, `Tuple`{.interpreted-text\nrole=\""obj\""}, or a `np.Array`{.interpreted-text role=\""obj\""}. Anything\nworks as long as it provides strings. ## Using the 🤗 Datasets library\nAn awesome way to access one of the many datasets that exist out there\nis by using the 🤗 Datasets library. For more information about it, you\nshould check [the official documentation\nhere](https://huggingface.co/docs/datasets/). Let's start by loading our dataset:\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_tutorial\\_train\\_from\\_iterators.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START load\\_dataset\"",\n\""end-before\"": \""END load\\_dataset\"",\n\""dedent\"": 8}\nThe next step is to build an iterator over this dataset. The easiest way\nto do this is probably by using a generator:\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_tutorial\\_train\\_from\\_iterators.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START def\\_batch\\_iterator\"",\n\""end-before\"": \""END def\\_batch\\_iterator\"",\n\""dedent\"": 8}\nAs you can see here, for improved efficiency we can actually provide a\nbatch of examples used to train, instead of iterating over them one by\none. By doing so, we can expect performances very similar to those we\ngot while training directly from files. With our iterator ready, we just need to launch the training. In order\nto improve the look of our progress bars, we can specify the total\nlength of the dataset:\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_tutorial\\_train\\_from\\_iterators.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START train\\_datasets\"",\n\""end-before\"": \""END train\\_datasets\"",\n\""dedent\"": 8}\nAnd that's it! ## Using gzip files\nSince gzip files in Python can be used as iterators, it is extremely\nsimple to train on such files:\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_tutorial\\_train\\_from\\_iterators.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START single\\_gzip\"",\n\""end-before\"": \""END single\\_gzip\"",\n\""dedent\"": 8}\nNow if we wanted to train from multiple gzip files, it wouldn't be much\nharder:\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_tutorial\\_train\\_from\\_iterators.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START multi\\_gzip\"",\n\""end-before\"": \""END multi\\_gzip\"",\n\""dedent\"": 8}\nAnd voilà!"", ""FrameworkSwitchCourse {fw} />\n# Training a causal language model from scratch[[training-a-causal-language-model-from-scratch]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nUp until now, we've mostly been using pretrained models and fine-tuning them for new use cases by reusing the weights from pretraining. As we saw in [Chapter 1](/course/chapter1), this is commonly referred to as \\_transfer learning\\_, and it's a very successful strategy for applying Transformer models to most real-world use cases where labeled data is sparse. In this chapter, we'll take a different approach and train a completely new model from scratch. This is a good approach to take if you have a lot of data and it is very different from the pretraining data used for the available models. However, it also requires considerably more compute resources to pretrain a language model than just to fine-tune an existing one. Examples where it can make sense to train a new model include for datasets consisting of musical notes, molecular sequences such as DNA, or programming languages. The latter have recently gained traction thanks to tools such as TabNine and GitHub's Copilot, powered by OpenAI's Codex model, that can generate long sequences of code. This task of text generation is best addressed with auto-regressive or causal language models such as GPT-2. In this section we will build a scaled-down version of a code generation model: we'll focus on one-line completions instead of full functions or classes, using a subset of Python code. When working with data in Python you are in frequent contact with the Python data science stack, consisting of the `matplotlib`, `seaborn`, `pandas`, and `scikit-learn` libraries. When using those frameworks it's common to need to look up specific commands, so it would be nice if we could use a model to complete these calls for us. In [Chapter 6](/course/chapter6) we created an efficient tokenizer to process Python source code, but what we still need is a large-scale dataset to pretrain a model on. Here, we'll apply our tokenizer to a corpus of Python code derived from GitHub repositories."", ""This is the simplest method in our case to have a tokenizer adapted to python code. Remember, the first step is to gather a training corpus. We will use a subpart of the CodeSearchNet dataset containing only python functions from open source libraries on Github. It's good timing, this dataset is known by the datasets library and we can load it in two lines of code. Then, as the \""train\\_new\\_from\\_iterator\"" method expects a iterator of lists of texts we create the \""get\\_training\\_corpus\"" function which will return an iterator. Now that we have our iterator on our python functions corpus, we can load the gpt-2 tokenizer architecture. Here \""old\\_tokenizer\"" is not adapted to our corpus but we only need one more line to train it on our new corpus. An argument that is common to most of the tokenization algorithms used at the moment is the size of the vocabulary, we choose here the value 52 thousand. Finally, once the training is finished, we just have to save our new tokenizer locally or send it to the hub to be able to reuse it very easily afterwards. Finally, let's see together on an example if it was useful to re-train a tokenizer similar to gpt2 one. With the original tokenizer of GPT-2 we see that all spaces are isolated and the method name \""randn\"" relatively common in python code is split in 2. With our new tokenizer, single and double indentations have been learned and the method \""randn\"" is tokenized into 1 token."", ""Here is how it can be done with code, with one loop to concatenate all the texts and another one to chunk it. Notice how it reduces the number of samples in our dataset here, there must have been quite a few short entries! Once this is done, the masking is the easy part. There is a data collator designed specifically for this in the Transformers library. You can use it directly in the Trainer, or when converting your datasets to tensorflow datasets before doing Keras.fit, with the to\\_tf\\_dataset method."", ""prompt,\n... max\\_new\\_tokens=20,\n... do\\_sample=True,\n... top\\_k=10,\n... return\\_full\\_text = False,\n... )\n>>> for seq in sequences:\n... print(f\""{seq['generated\\_text']}\"")\nA volte, ho creduto a sei impossibili cose prima di colazione. ```\nHere we've added a `do\\_sample=True` and `top\\_k=10` to allow the model to be a bit more flexible when generating output. #### Text summarization\nSimilar to the translation, text summarization is another generative task where the output \\*\\*heavily\\*\\* relies on the input,\nand encoder-decoder models can be a better choice. However, decoder-style models can be used for this task as well. Previously, we have placed the instructions at the very beginning of the prompt. However, the very end of the prompt can\nalso be a suitable location for instructions. Typically, it's better to place the instruction on one of the extreme ends. ```python\n>>> torch.manual\\_seed(3) # doctest: +IGNORE\\_RESULT\n>>> prompt = \""\""\""Permaculture is a design process mimicking the diversity, functionality and resilience of natural ecosystems.""]","One advantage of using a generator of lists of texts in `train_new_from_iterator()` is that it allows for improved efficiency by providing a batch of examples used for training, rather than iterating over them one by one. This can lead to performance that is very similar to training directly from files.",You will avoid loading the whole dataset into memory at once.
What does the stride argument control in the tokenizer?,"[""he tokenization pipeline involves several steps that convert raw text into numbers. In this video, we will see what happens during the pre-tokenization step. The pre-tokenization operation is the operation performed after the normalization of the text and before the application of the tokenization algorithm. This step consists in applying rules that do not need to be learned to perform a first division of the text. Let's look at how several tokenizers pre\\_tokenize this example. The gpt 2 pretokenization divides the text on spaces and some punctuation - but the apostrophe is not a division criterion for example. We also notice that spaces have been replaced by a capital G with a dot above. Albert's pre-tokenization divides the text at the level of spaces, adds a space at the beginning of the sentence and replaces spaces with a special underscore. Finally, Bert's pre-tokenization divides the text at the level of punctuation and spaces. Unlike the previous tokenizers, spaces are not transformed and integrated to the tokens produced with this pre-tokenizer. Through these 3 examples, we could observe the two main types of operations brought by the pre-tokenization: some changes on the text and the division of the string into tokens that can be associated to words. Finally, the \""backend\\_tokenizer\"" of the fast tokenizers also allows to test the pre-tokenization operation very easily thanks to its \""pre\\_tokenize\\_str\"" method. We notice that the output of this operation is composed of both tokens and offsets which allow to link the token to its position in the text given in input of the method."", ""Some tokenizers just split on spaces, so they will consider this as one word. Others use punctuation on top of spaces, so will consider it two words. ✏️ \\*\\*Try it out!\\*\\* Create a tokenizer from the `bert-base-cased` and `roberta-base` checkpoints and tokenize \""81s\"" with them. What do you observe? What are the word IDs? Similarly, there is a `sentence\\_ids()` method that we can use to map a token to the sentence it came from (though in this case, the `token\\_type\\_ids` returned by the tokenizer can give us the same information). Lastly, we can map any word or token to characters in the original text, and vice versa, via the `word\\_to\\_chars()` or `token\\_to\\_chars()` and `char\\_to\\_word()` or `char\\_to\\_token()` methods. For instance, the `word\\_ids()` method told us that `##yl` is part of the word at index 3, but which word is it in the sentence? We can find out like this:\n```py\nstart, end = encoding.word\\_to\\_chars(3)\nexample[start:end]\n```\n```python out\nSylvain\n```\nAs we mentioned previously, this is all powered by the fact the fast tokenizer keeps track of the span of text each token comes from in a list of \\*offsets\\*."", ""We attack here the big part: the design of our tokenizer with the tokenizers library. We start by initializing a tokenizer instance with a WordPiece model because it is the model used by BERT. Then we can define our normalizer. We will define it as a succession of 2 normalizations used to clean up characters not visible in the text, 1 lowercasing normalization and 2 normalizations used to remove accents. For the pre-tokenization, we will chain two pre\\_tokenizer. The first one separating the text at the level of spaces and the second one isolating the punctuation marks. Now, we can define the trainer that will allow us to train the WordPiece model chosen at the beginning. To carry out the training, we will have to choose a vocabulary size, here we choose twenty-five thousand and also announce the special tokens that we absolutely want to add to our vocabulary. In one line of code, we can train our WordPiece model using the iterator we defined earlier. Once the model has been trained, we can retrieve the ids of the special class and separation tokens because we will need them to post-process our sequence. Thanks to the TemplateProcessing class, we can add the CLS token at the beginning of each sequence and the SEP token at the end of the sequence and between two sentences if we tokenize a text pair."", ""Finally, we just have to define our decoder which will allow us to remove the hashtags at the beginning of the tokens that must be reattached to the previous token. And there it ist, you have all the necessary lines of code to define your own tokenizer. Now that we have a brand new tokenizer with the tokenizers library we just have to load it into a fast tokenizer from the transformers library. Here again we have several possibilities. We can load it in the generic class \""PreTrainedTokenizerFast\"" or in the BertTokenizerFast class since we have built a bert type tokenizer here. I hope this video has helped you understand how you can create your own tokenizer and that you are ready to navigate the tokenizers library documentation to choose the components for your brand-new tokenizer!"", ""`tokenizers-linux-arm64-gnu`\nThis is the \\*\\*aarch64-unknown-linux-gnu\\*\\* binary for `tokenizers`""]","The stride argument in the tokenizer controls the overlap between the chunks of text when tokenizing long sequences. It allows you to specify how many tokens to skip when moving to the next chunk. This is particularly useful when you want to ensure that important context is not lost between chunks, as it allows for some overlap between consecutive tokenized segments. By adjusting the stride, you can manage the trade-off between computational efficiency and the preservation of context in the tokenization process.",The stride argument controls the number of overlapping tokens.
What is the role of machine learning in predictive healthcare networks?,"[""However, companies can create the required foundation for successful machine learning projects with a thorough review of accurate, comprehensive, and constant training data. #### \\*\\*4. Where do you see ML having the biggest impact in the next 5-10 years?\\*\\*\nIn the next 5-10 years, ML will have the biggest impact on transforming the healthcare sector. \\*\\*Networked hospitals and connected care:\\*\\*\nWith predictive care, command centers are all set to analyze clinical and location data to monitor supply and demand across healthcare networks in real-time. With ML, healthcare professionals will be able to spot high-risk patients more quickly and efficiently, thus removing bottlenecks in the system. You can check the spread of contractible diseases faster, take better measures to manage epidemics, identify at-risk patients more accurately, especially for genetic diseases, and more. \\*\\*Better staff and patient experiences:\\*\\*\nPredictive healthcare networks are expected to reduce wait times, improve staff workflows, and take on the ever-growing administrative burden. By learning from every patient, diagnosis, and procedure, ML is expected to create experiences that adapt to hospital staff as well as the patient. This improves health outcomes and reduces clinician shortages and burnout while enabling the system to be financially sustainable. ---\n🤗 Thank you for joining us in this second installment of ML Director Insights. Stay tuned for more insights from ML Directors in Finance, Healthcare and e-Commerce. Big thanks to Omar Rahman, Cao (Danica) Xiao, Raphael Cohen, and Martin Ostrovsky for their brilliant insights and participation in this piece. We look forward to watching each of your continued successes and will be cheering you on each step of the way. 🎉\nIf you or your team are interested in accelerating your ML roadmap with Hugging Face Experts please visit [hf.co/support](https://huggingface.co/support?utm\\_source=article&utm\\_medium=blog&utm\\_campaign=ml\\_director\\_insights\\_2) to learn more."", ""Although federated learning is a relatively new field and has only taken hold in the last decade, it has already enabled machine learning research to progress in ways previously unimaginable. For example, 10 competing biopharma companies that would traditionally never share data with each other set up a collaboration in the [MELLODDY](https://www.melloddy.eu/) project by sharing the world’s largest collection of small molecules with known biochemical or cellular activity. This ultimately enabled all of the companies involved to build more accurate predictive models for drug discovery, a huge milestone in medical research. ## Substra x HF\nResearch on the capabilities of federated learning is growing rapidly but the majority of recent work has been limited to simulated environments. Real world examples and implementations still remain limited due to the difficulty of deploying and architecting federated networks. As a leading open-source platform for federated learning deployment, Substra has been battle tested in many complex security environments and IT infrastructures, and has enabled [medical breakthroughs in breast cancer research](https://www.nature.com/articles/s41591-022-02155-w). ![Substra diagram](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/139\\_substra/diagram.jpg)\nHugging Face collaborated with the folks managing Substra to create this space, which is meant to give you an idea of the real world challenges that researchers and scientists face - mainly, a lack of centralized, high quality data that is ‘ready for AI’."", ""Bridging those types of data is non-trivial. In addition, third-party data spatial/gis and weather are extremely important. Through the lens of machine learning, we are able to find and explore features and outcomes that have a real impact. #### \\*\\*2. What are the biggest ML challenges within Utilities?\\*\\*\nThere is a demystification that needs to happen. What machine learning can do and where it needs to be monitored or could fall short. The utility industry has established ways of operating, machine learning can be perceived as a disruptor. Because of this, departments can be slow to adopt any new technology or paradigm. However, if the practitioner is able to prove results, then results create traction and a larger appetite to adopt. Additional challenges are on-premise data and access to the cloud and infrastructure. It’s a gradual process and has a learning curve that requires patience. #### \\*\\*3. What’s a common mistake you see people make trying to integrate ML into Utilities?\\*\\*\nNot unique to utilizes, but moving too fast and neglecting good data quality and simple quality checks."", ""Whether and to what extent bias in the system concretely affects people ultimately depends on what the system is used for. As such, the first place developers can work to mitigate bias is when deciding how ML fits in their system, e.g., by deciding what optimization objective it will use. For example, let’s go back to one of the first highly-publicized cases of a Machine Learning system used in production for algorithmic content recommendation. From 2006 to 2009, Netflix ran the [Netflix Prize](https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/The-Netflix-Prize-Bennett.pdf), a competition with a 1M$ cash prize challenging teams around the world to develop ML systems to accurately predict a user’s rating for a new movie based on their past ratings. The [winning submission](https://www.asc.ohio-state.edu/statistics/dmsl/GrandPrize2009\\_BPC\\_BigChaos.pdf) improved the RMSE (Root-mean-square-error) of predictions on unseen user-movie pairs by over 10% over Netflix’s own CineMatch algorithm, meaning it got much better at predicting how users would rate a new movie based on their history."", ""### Models\n[Transfer learning](https://www.youtube.com/watch?v=BqqfQnyjmgg&ab\\_channel=HuggingFace) has changed the way companies approach machine learning problems. Traditionally, companies needed to train models from scratch, which requires a lot of time, data, and resources. Now machine learning teams can use a pre-trained model and [fine-tune it for their own use case](https://huggingface.co/course/chapter3/1?fw=pt) in a fast and cost-effective way. This dramatically accelerates the process of getting accurate and performant models. On the Hub, you can find 60,000+ state-of-the-art open source pre-trained models for NLP, computer vision, speech, time-series, biology, reinforcement learning, chemistry and more. You can use the search bar or filter by tasks, libraries, licenses and other tags to find the right model for your particular use case:\n\n60,000+ models available on the Hub\n\nThese models span 180 languages and support up to 25 ML libraries (including Transformers, Keras, spaCy, Timm and others), so there is a lot of flexibility in terms of the type of models, languages and libraries.""]","Machine learning plays a crucial role in predictive healthcare networks by enabling healthcare professionals to analyze clinical and location data in real-time. This allows for the quick identification of high-risk patients, which helps to remove bottlenecks in the healthcare system. Additionally, machine learning facilitates faster tracking of the spread of contractible diseases, better management of epidemics, and more accurate identification of at-risk patients, particularly for genetic diseases.

Moreover, predictive healthcare networks are expected to reduce wait times, improve staff workflows, and alleviate the administrative burden on healthcare providers. By learning from every patient, diagnosis, and procedure, machine learning can create experiences that adapt to both hospital staff and patients, ultimately improving health outcomes and supporting the financial sustainability of healthcare systems.","Machine learning helps spot high-risk patients more quickly and efficiently, checks the spread of contractible diseases faster, manages epidemics better, identifies at-risk patients more accurately, and creates experiences that adapt to both hospital staff and patients."
What is the size of the Parquet file for the train split of the rotten_tomatoes dataset?,"[""Now that we have a dataset, we need to prepare the texts so they're in a format suitable for pretraining. ## Preparing the dataset[[preparing-the-dataset]]\nThe first step will be to tokenize the data, so we can use it for training. Since our goal is to mainly autocomplete short function calls, we can keep the context size relatively small. This has the benefit that we can train the model much faster and it requires significantly less memory. If it is important for your application to have more context (for example, if you want the model to write unit tests based on a file with the function definition), make sure you increase that number, but also keep in mind that this comes with a greater GPU memory footprint. For now, let's fix the context size at 128 tokens, as opposed to the 1,024 or 2,048 used in GPT-2 or GPT-3, respectively. Most documents contain many more than 128 tokens, so simply truncating the inputs to the maximum length would eliminate a large fraction of our dataset. Instead, we'll use the `return\\_overflowing\\_tokens` option to tokenize the whole input and split it into several chunks, as we did in [Chapter 6](/course/chapter6/4). We'll also use the `return\\_length` option to return the length of each created chunk automatically. Often the last chunk will be smaller than the context size, and we'll get rid of these pieces to avoid padding issues; we don't really need them as we have plenty of data anyway. ![Chunking a large texts in several pieces.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg)\n![Chunking a large texts in several pieces.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg)\n\nLet's see exactly how this works by looking at the first two examples:\n```py\nfrom transformers import AutoTokenizer\ncontext\\_length = 128\ntokenizer = AutoTokenizer.from\\_pretrained(\""huggingface-course/code-search-net-tokenizer\"")\noutputs = tokenizer(\nraw\\_datasets[\""train\""][:2][\""content\""],\ntruncation=True,\nmax\\_length=context\\_length,\nreturn\\_overflowing\\_tokens=True,\nreturn\\_length=True,\n)\nprint(f\""Input IDs length: {len(outputs['input\\_ids'])}\"")\nprint(f\""Input chunk lengths: {(outputs['length'])}\"")\nprint(f\""Chunk mapping: {outputs['overflow\\_to\\_sample\\_mapping']}\"")\n```\n```python out\nInput IDs length: 34\nInput chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\nChunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n```\nWe can see that we get 34 segments in total from those two examples."", ""The final training corpus has a size of 3 GB, which is still small – for your model, you will get better results the more data you can get to pretrain on. ## 2. Train a tokenizer\nWe choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let’s arbitrarily pick its size to be 52,000. We recommend training a byte-level BPE (rather than let’s say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `` tokens!). ```python\n#! pip install tokenizers\nfrom pathlib import Path\nfrom tokenizers import ByteLevelBPETokenizer\npaths = [str(x) for x in Path(\""./eo\\_data/\"").glob(\""\\*\\*/\\*.txt\"")]\n# Initialize a tokenizer\ntokenizer = ByteLevelBPETokenizer()\n# Customize training\ntokenizer.train(files=paths, vocab\\_size=52\\_000, min\\_frequency=2, special\\_tokens=[\n\""~~\"",\n\""\"",\n\""~~\"",\n\""\"",\n\""\"",\n])\n# Save files to disk\ntokenizer.save\\_model(\"".\"", \""esperberto\"")\n```\nAnd here’s a slightly accelerated capture of the output:\n![tokenizers](assets/01\\_how-to-train/tokenizers-fast.gif)\nOn our dataset, training took about ~5 minutes."", ""This is useful if you have a large number of images, if you want to embed multiple image columns, or if you want to store additional information about the images in the same file. Parquet is also useful for storing data such as raw bytes, which is not supported by JSON/CSV. ```\nmy\\_dataset\\_repository/\n└── train.parquet\n```\nNote that for the user convenience, every dataset hosted in the Hub is automatically converted to Parquet format. Read more about it in the [Parquet format](./datasets-viewer#access-the-parquet-files) documentation."", ""### Download and extract the data. It consists of two compressed folders: one with images, and the other—with associated image captions. Note that the compressed images folder is 13GB in size. ```bash\nwget http://images.cocodataset.org/annotations/annotations\\_trainval2014.zip\nwget http://images.cocodataset.org/zips/train2014.zip\nunzip annotations\\_trainval2014.zip\nunzip train2014.zip\nmkdir coco\\_dataset\nmv train2014 coco\\_dataset/\nmv annotations coco\\_dataset/\n```\n### Prepare dataset files and split the dataset. ```python\nimport json\nimport collections\nimages\\_dir = \""coco\\_dataset/train2014\""\nannotation\\_file = \""coco\\_dataset/annotations/captions\\_train2014.json\""\nwith open(annotation\\_file, \""r\"") as f:\nannotations = json.load(f)[\""annotations\""]\nimage\\_path\\_to\\_caption = collections.defaultdict(list)\nfor element in annotations:\ncaption = f\""{element['caption'].lower().rstrip('.')}\""\nimage\\_path = images\\_dir + \""/COCO\\_train2014\\_\"" + \""%012d.jpg\"" % (element[\""image\\_id\""])\nimage\\_path\\_to\\_caption[image\\_path].append(caption)\nlines = []\nfor image\\_path, captions in image\\_path\\_to\\_caption.items():\nlines.append(json.dumps({\""image\\_path\"": image\\_path, \""captions\"": captions}))\ntrain\\_lines = lines[:-8000]\nvalid\\_line = lines[-8000:]\nwith open(\""coco\\_dataset/train\\_dataset.json\"", \""w\"") as f:\nf.write(\""\\n\"".join(train\\_lines))\nwith open(\""coco\\_dataset/valid\\_dataset.json\"", \""w\"") as f:\nf.write(\""\\n\"".join(valid\\_line))\n```\n> Note: The data loading and processing part of this script can still be improved for maximum performance."", ""Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package\\_reference/main\\_classes#datasets.DatasetInfo) object of the `datasets` library. ```json\n{\n\""dataset\\_info\"": {\n\""description\"": \""DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\n\"",\n\""citation\"": \""@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = {2018}\\n}\\n\"",\n\""homepage\"": \""https://duorc.github.io/\"",\n\""license\"": \""https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\"",\n\""features\"": {\n\""plot\\_id\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""plot\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""title\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""question\\_id\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""question\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""answers\"": {\n\""feature\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""\\_type\"": \""Sequence\""\n},\n\""no\\_answer\"": {\n\""dtype\"": \""bool\"",\n\""\\_type\"": \""Value\""\n}\n},\n\""builder\\_name\"": \""duorc\"",\n\""config\\_name\"": \""SelfRC\"",\n\""version\"": {\n\""version\\_str\"": \""1.0.0\"",\n\""major\"": 1,\n\""minor\"": 0,\n\""patch\"": 0\n},\n\""splits\"": {\n\""train\"": {\n\""name\"": \""train\"",\n\""num\\_bytes\"": 239852729,\n\""num\\_examples\"": 60721,\n\""dataset\\_name\"": \""duorc\""\n},\n\""validation\"": {\n\""name\"": \""validation\"",\n\""num\\_bytes\"": 51662519,\n\""num\\_examples\"": 12961,\n\""dataset\\_name\"": \""duorc\""\n},\n\""test\"": {\n\""name\"": \""test\"",\n\""num\\_bytes\"": 49142710,\n\""num\\_examples\"": 12559,\n\""dataset\\_name\"": \""duorc\""\n}\n},\n\""download\\_checksums\"": {\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_train.json\"": {\n\""num\\_bytes\"": 24388192,\n\""checksum\"": null\n},\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_dev.json\"": {\n\""num\\_bytes\"": 5051240,\n\""checksum\"": null\n},\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_test.json\"": {\n\""num\\_bytes\"": 5023228,\n\""checksum\"": null\n}\n},\n\""download\\_size\"": 34462660,\n\""dataset\\_size\"": 340657958,\n\""size\\_in\\_bytes\"": 375120618\n}\n}\n```""]",The size of the Parquet file for the train split of the rotten_tomatoes dataset is not specified in the provided context. Please refer to the dataset documentation on HuggingFace for detailed information regarding the sizes of specific dataset splits.,698845
What are the two types of guides for contributing to Gradio?,"[""How to add support for more languages\nWe would love to support more languages for Gradio 🌎\nTo add your language, do the following steps:\n1. Create a new json file in this directory\n2. Name the file after the language code (Here's a list: http://4umi.com/web/html/languagecodes.php)\n3. Please provide clear and complete translations. Take a look at the [`en.json`](https://github.com/gradio-app/gradio/blob/master/js/app/public/lang/en.json) file for the corresponding English text. That's it!"", ""- @MohamedAliRashad for making their first contribution to gradio! ## 3.15.0\n### New Features:\nGradio's newest plotting component `gr.LinePlot`! 📈\nWith this component you can easily create time series visualizations with customizable\nappearance for your demos and dashboards ... all without having to know an external plotting library. For an example of the api see below:\n```python\ngr.LinePlot(stocks,\nx=\""date\"",\ny=\""price\"",\ncolor=\""symbol\"",\ncolor_legend_position=\""bottom\"",\nwidth=600, height=400, title=\""Stock Prices\"")\n```\n![image](https://user-images.githubusercontent.com/41651716/208711646-81ae3745-149b-46a3-babd-0569aecdd409.png)\nBy [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2807](https://github.com/gradio-app/gradio/pull/2807)\n### Bug Fixes:\n- Fixed bug where the `examples_per_page` parameter of the `Examples` component was not passed to the internal `Dataset` component by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2861](https://github.com/gradio-app/gradio/pull/2861)\n- Fixes loading Spaces that have components with default values by [@abidlabs](https://github.com/abidlabs) in [PR 2855](https://github.com/gradio-app/gradio/pull/2855)\n- Fixes flagging when `allow_flagging=\""auto\""` in `gr.Interface()` by [@abidlabs](https://github.com/abidlabs) in [PR 2695](https://github.com/gradio-app/gradio/pull/2695)\n- Fixed bug where passing a non-list value to `gr.CheckboxGroup` would crash the entire app by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2866](https://github.com/gradio-app/gradio/pull/2866)\n### Documentation Changes:\n- Added a Guide on using BigQuery with Gradio's `DataFrame` and `ScatterPlot` component,\nby [@abidlabs](https://github.com/abidlabs) in [PR 2794](https://github.com/gradio-app/gradio/pull/2794)\n### Testing and Infrastructure Changes:\nNo changes to highlight."", ""You can create a new story with a `\\*.stories.svelte` file. ## 🕸️ Gradio Website\nWe also welcome any contributions to our [website](https://www.gradio.app). First, build the website:\n```\npnpm build:cdn-local\n```\nthen serve the website build:\n```\npnpm preview:cdn-local\n```\nThis will serve a build of `gradio.js` on port `4321`. You can then navigate to `js/\\_website/src/routes/+layout.svelte` and replace the source of the website build from:\n```\n```\nto\n```\n```\nYou should now be able to view a local version of the website at `http://localhost:4321`. ## 📚 Component Storybook\nIf you would like to fix an issue or contribute to our Storybook, you can get it running locally with:\n```\npnpm storybook\n```\n## 📮 Submitting PRs\nAll PRs should be against `main`, and ideally should address an open issue, unless the change is small. Direct commits to main are blocked, and PRs require an approving review to merge into main. By convention, the Gradio maintainers will review PRs when:\n- An initial review has been requested\n- A clear, descriptive title has been assigned to the PR\n- A maintainer (@abidlabs, @aliabid94, @aliabd, @AK391, @dawoodkhan82, @pngwn, @freddyaboulton, @hannahblair) is tagged in the PR comments and asked to complete a review\n🧹 We ask that you make sure initial CI checks are passing before requesting a review. One of the Gradio maintainers will merge the PR when all the checks are passing. You can safely ignore the Vercel and Spaces checks, which only run under maintainers' pull requests. Don't forget the format your code before pushing:\n```\nbash scripts/format\\_backend.sh\n```\n```\nbash scripts/format\\_frontend.sh\n```\nThank you for taking the time to contribute to our project!"", ""## What methods are mandatory for implementing a custom component in Gradio? You must implement the `preprocess`, `postprocess`, `as\\_example`, `api\\_info`, `example\\_inputs`, `flag`, and `read\\_from\\_flag` methods. Read more in the [backend guide](./backend). ## What is the purpose of a `data\\_model` in Gradio custom components? A `data\\_model` defines the expected data format for your component, simplifying the component development process and self-documenting your code. It streamlines API usage and example caching. ## Why is it important to use `FileData` for components dealing with file uploads? Utilizing `FileData` is crucial for components that expect file uploads. It ensures secure file handling, automatic caching, and streamlined client library functionality. ## How can I add event triggers to my custom Gradio component? You can define event triggers in the `EVENTS` class attribute by listing the desired event names, which automatically adds corresponding methods to your component. ## Can I implement a custom Gradio component without defining a `data\\_model`? Yes, it is possible to create custom components without a `data\\_model`, but you are going to have to manually implement `api\\_info`, `example\\_inputs`, `flag`, and `read\\_from\\_flag` methods. ## Are there sample custom components I can learn from? We have prepared this [collection](https://huggingface.co/collections/gradio/custom-components-65497a761c5192d981710b12) of custom components on the HuggingFace Hub that you can use to get started! ## How can I find custom components created by the Gradio community? We're working on creating a gallery to make it really easy to discover new custom components. In the meantime, you can search for HuggingFace Spaces that are tagged as a `gradio-custom-component` [here](https://huggingface.co/search/full-text?q=gradio-custom-component&type=space)"", ""Contributing a Guide\nWant to help teach Gradio? Consider contributing a Guide! 🤗\nBroadly speaking, there are two types of guides:\n- \\*\\*Use cases\\*\\*: guides that cover step-by-step how to build a particular type of machine learning demo or app using Gradio. Here's an example: [\\_Creating a Chatbot\\_](https://github.com/gradio-app/gradio/blob/master/guides/creating\\_a\\_chatbot.md)\n- \\*\\*Feature explanation\\*\\*: guides that describe in detail a particular feature of Gradio. Here's an example: [\\_Using Flagging\\_](https://github.com/gradio-app/gradio/blob/master/guides/using\\_flagging.md)\nWe encourage you to submit either type of Guide! (Looking for ideas? We may also have open [issues](https://github.com/gradio-app/gradio/issues?q=is%3Aopen+is%3Aissue+label%3Aguides) where users have asked for guides on particular topics)\n## Guide Structure\nAs you can see with the previous examples, Guides are standard markdown documents. They usually:\n- start with an Introduction section describing the topic\n- include subheadings to make articles easy to navigate\n- include real code snippets that make it easy to follow along and implement the Guide\n- include embedded Gradio demos to make them more interactive and provide immediate demonstrations of the topic being discussed.""]","There are two types of guides for contributing to Gradio:

1. **Use cases**: Guides that cover step-by-step how to build a particular type of machine learning demo or app using Gradio.
2. **Feature explanation**: Guides that describe in detail a particular feature of Gradio.",Use cases and Feature explanation.
What method is called to compute gradients in the training pipeline?,"[""This is because there are other components that also require memory storage. Saving all activations from the forward pass in order to compute the gradients during the backward pass can result in\nsignificant memory overhead. The alternative approach of discarding the activations and recalculating them when needed\nduring the backward pass, would introduce a considerable computational overhead and slow down the training process. \\*\\*Gradient checkpointing\\*\\* offers a compromise between these two approaches and saves strategically selected activations\nthroughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. For\nan in-depth explanation of gradient checkpointing, refer to [this great article](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9). To enable gradient checkpointing in the [`Trainer`], pass the corresponding a flag to [`TrainingArguments`]:\n```py\ntraining\\_args = TrainingArguments(\nper\\_device\\_train\\_batch\\_size=1, gradient\\_accumulation\\_steps=4, gradient\\_checkpointing=True, \\*\\*default\\_args\n)\n```\nAlternatively, use 🤗 Accelerate - find the 🤗 Accelerate example [further in this guide](#using-accelerate)."", ""This effort saved us more than one month of training time. These notes were written primarily for the training mode, but they should mostly apply for inference as well. For example, during inference Gradient Checkpointing is a no-op since it is only useful during training. Additionally, we found out that if you are doing a multi-GPU inference and not using [DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/), [Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts) should provide a superior performance. Other quick related performance notes:\n- if you are training something from scratch always try to have tensors with shapes that are divisible by 16 (e.g. hidden size). For batch size try divisible by 2 at least. There are [wave and tile quanitization](https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/) divisibility that is hardware-specific if you want to squeeze even higher performance from your GPUs. ### Activation Checkpointing or Gradient Checkpointing\nActivation checkpointing and gradient checkpointing are two distinct terms that refer to the same methodology."", ""For instance, for fp16 data type a multiple of 8 is recommended, unless\nit's an A100 GPU, in which case use multiples of 64. For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup. ## Gradient Accumulation\nThe \\*\\*gradient accumulation\\*\\* method aims to calculate gradients in smaller increments instead of computing them for the\nentire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward\nand backward passes through the model and accumulating the gradients during the process. Once a sufficient number of\ngradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\nbecomes possible to increase the \\*\\*effective batch size\\*\\* beyond the limitations imposed by the GPU's memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can\nslow down the training process. You can enable gradient accumulation by adding the `gradient\\_accumulation\\_steps` argument to [`TrainingArguments`]:\n```py\ntraining\\_args = TrainingArguments(per\\_device\\_train\\_batch\\_size=1, gradient\\_accumulation\\_steps=4, \\*\\*default\\_args)\n```\nIn the above example, your effective batch size becomes 4."", ""## Intended uses & limitations\n#### How to use\n```python\n# TODO: add an example code snippet for running this diffusion pipeline\n```\n#### Limitations and bias\n[TODO: provide examples of latent issues and potential remediations]\n## Training data\n[TODO: describe the data used to train the model]\n### Training hyperparameters\nThe following hyperparameters were used during training:\n- learning\\_rate: {{ learning\\_rate }}\n- train\\_batch\\_size: {{ train\\_batch\\_size }}\n- eval\\_batch\\_size: {{ eval\\_batch\\_size }}\n- gradient\\_accumulation\\_steps: {{ gradient\\_accumulation\\_steps }}\n- optimizer: AdamW with betas=({{ adam\\_beta1 }}, {{ adam\\_beta2 }}), weight\\_decay={{ adam\\_weight\\_decay }} and epsilon={{ adam\\_epsilon }}\n- lr\\_scheduler: {{ lr\\_scheduler }}\n- lr\\_warmup\\_steps: {{ lr\\_warmup\\_steps }}\n- ema\\_inv\\_gamma: {{ ema\\_inv\\_gamma }}\n- ema\\_inv\\_gamma: {{ ema\\_power }}\n- ema\\_inv\\_gamma: {{ ema\\_max\\_decay }}\n- mixed\\_precision: {{ mixed\\_precision }}\n### Training results\n📈 [TensorBoard logs](https://huggingface.co/{{ repo\\_name }}/tensorboard?#scalars)"", ""The input prompts are converted to text embeddings by the pipeline's text\\_encoder and the interpolation is done on the resulting text\\_embeddings over the number of steps specified. Defaults to 5 steps. ```python\nimport torch\nfrom diffusers import DiffusionPipeline\ndevice = torch.device(\""cpu\"" if not torch.cuda.is\\_available() else \""cuda\"")\npipe = DiffusionPipeline.from\\_pretrained(\n\""kakaobrain/karlo-v1-alpha\"",\ntorch\\_dtype=torch.float16,\ncustom\\_pipeline=\""unclip\\_text\\_interpolation\""\n)\npipe.to(device)\nstart\\_prompt = \""A photograph of an adult lion\""\nend\\_prompt = \""A photograph of a lion cub\""\n#For best results keep the prompts close in length to each other. Of course, feel free to try out with differing lengths. generator = torch.Generator(device=device).manual\\_seed(42)\noutput = pipe(start\\_prompt, end\\_prompt, steps = 6, generator = generator, enable\\_sequential\\_cpu\\_offload=False)\nfor i,image in enumerate(output.images):\nimg.save('result%s.jpg' % i)\n```\nThe resulting images in order:-\n![result\\_0](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPTextInterpolationSamples/resolve/main/lion\\_to\\_cub\\_0.png)\n![result\\_1](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPTextInterpolationSamples/resolve/main/lion\\_to\\_cub\\_1.png)\n![result\\_2](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPTextInterpolationSamples/resolve/main/lion\\_to\\_cub\\_2.png)\n![result\\_3](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPTextInterpolationSamples/resolve/main/lion\\_to\\_cub\\_3.png)\n![result\\_4](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPTextInterpolationSamples/resolve/main/lion\\_to\\_cub\\_4.png)\n![result\\_5](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPTextInterpolationSamples/resolve/main/lion\\_to\\_cub\\_5.png)\n### UnCLIP Image Interpolation Pipeline\nThis Diffusion Pipeline takes two images or an image\\_embeddings tensor of size 2 and interpolates between their embeddings using spherical interpolation ( slerp ). The input images/image\\_embeddings are converted to image embeddings by the pipeline's image\\_encoder and the interpolation is done on the resulting image\\_embeddings over the number of steps specified. Defaults to 5 steps. ```python\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom PIL import Image\ndevice = torch.device(\""cpu\"" if not torch.cuda.is\\_available() else \""cuda\"")\ndtype = torch.float16 if torch.cuda.is\\_available() else torch.bfloat16\npipe = DiffusionPipeline.from\\_pretrained(\n\""kakaobrain/karlo-v1-alpha-image-variations\"",\ntorch\\_dtype=dtype,\ncustom\\_pipeline=\""unclip\\_image\\_interpolation\""\n)\npipe.to(device)\nimages = [Image.open('./starry\\_night.jpg'), Image.open('./flowers.jpg')]\n#For best results keep the prompts close in length to each other. Of course, feel free to try out with differing lengths. generator = torch.Generator(device=device).manual\\_seed(42)\noutput = pipe(image = images ,steps = 6, generator = generator)\nfor i,image in enumerate(output.images):\nimage.save('starry\\_to\\_flowers\\_%s.jpg' % i)\n```\nThe original images:-\n![starry](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPImageInterpolationSamples/resolve/main/starry\\_night.jpg)\n![flowers](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPImageInterpolationSamples/resolve/main/flowers.jpg)\nThe resulting images in order:-\n![result0](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPImageInterpolationSamples/resolve/main/starry\\_to\\_flowers\\_0.png)\n![result1](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPImageInterpolationSamples/resolve/main/starry\\_to\\_flowers\\_1.png)\n![result2](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPImageInterpolationSamples/resolve/main/starry\\_to\\_flowers\\_2.png)\n![result3](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPImageInterpolationSamples/resolve/main/starry\\_to\\_flowers\\_3.png)\n![result4](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPImageInterpolationSamples/resolve/main/starry\\_to\\_flowers\\_4.png)\n![result5](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPImageInterpolationSamples/resolve/main/starry\\_to\\_flowers\\_5.png)\n### DDIM Noise Comparative Analysis Pipeline\n#### \\*\\*Research question: What visual concepts do the diffusion models learn from each noise level during training?\\*\\*\nThe [P2 weighting (CVPR 2022)](https://arxiv.org/abs/2204.00227) paper proposed an approach to answer the above question, which is their second contribution.""]","The method called to compute gradients in the training pipeline is **gradient accumulation**. This method aims to calculate gradients in smaller increments instead of computing them for the entire batch at once. It involves iteratively calculating gradients in smaller batches by performing forward and backward passes through the model and accumulating the gradients during the process. Once a sufficient number of gradients have been accumulated, the model's optimization step is executed. You can enable gradient accumulation by adding the `gradient_accumulation_steps` argument to `TrainingArguments`.",backward()
Who created the Falcon language models?,"[""They are made available under the Apache 2.0 license. Falcon's architecture is modern and optimized for inference, with multi-query attention and support for efficient\nattention variants like `FlashAttention`. Both 'base' models trained only as causal language models as well as\n'instruct' models that have received further fine-tuning are available. Falcon models are (as of 2023) some of the largest and most powerful open-source language models,\nand consistently rank highly in the [OpenLLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open\\_llm\\_leaderboard). ## Converting custom checkpoints\nFalcon models were initially added to the Hugging Face Hub as custom code checkpoints. However, Falcon is now fully\nsupported in the Transformers library. If you fine-tuned a model from a custom code checkpoint, we recommend converting\nyour checkpoint to the new in-library format, as this should give significant improvements to stability and\nperformance, especially for generation, as well as removing the need to use `trust\\_remote\\_code=True`! You can convert custom code checkpoints to full Transformers checkpoints using the `convert\\_custom\\_code\\_checkpoint.py`\nscript located in the\n[Falcon model directory](https://github.com/huggingface/transformers/tree/main/src/transformers/models/falcon)\nof the Transformers library. To use this script, simply call it with\n`python convert\\_custom\\_code\\_checkpoint.py --checkpoint\\_dir my\\_model`. This will convert your checkpoint in-place, and\nyou can immediately load it from the directory afterwards with e.g. `from\\_pretrained()`. If your model hasn't been\nuploaded to the Hub, we recommend making a backup before attempting the conversion, just in case! ## FalconConfig\n[[autodoc]] FalconConfig\n- all\n## FalconModel\n[[autodoc]] FalconModel\n- forward\n## FalconForCausalLM\n[[autodoc]] FalconForCausalLM\n- forward\n## FalconForSequenceClassification\n[[autodoc]] FalconForSequenceClassification\n- forward\n## FalconForTokenClassification\n[[autodoc]] FalconForTokenClassification\n- forward\n## FalconForQuestionAnswering\n[[autodoc]] FalconForQuestionAnswering\n- forward"", ""You can read more about how to fine-tune, deploy and prompt with Llama 2 in [this blog post](https://huggingface.co/blog/llama2). The second type of text generation model is commonly referred to as the text-to-text generation model. These models are trained on text pairs, which can be questions and answers or instructions and responses. The most popular ones are T5 and BART (which, as of now, aren’t state-of-the-art). Google has recently released the FLAN-T5 series of models. FLAN is a recent technique developed for instruction fine-tuning, and FLAN-T5 is essentially T5 fine-tuned using FLAN. As of now, the FLAN-T5 series of models are state-of-the-art and open-source, available on the [Hugging Face Hub](https://huggingface.co/models?search=google/flan). Note that these are different from instruction-tuned causal language models, although the input-output format might seem similar. Below you can see an illustration of how these models work. ![FLAN-T5 Illustration](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/os\\_llms/flan\\_t5.png)\nHaving more variation of open-source text generation models enables companies to keep their data private, to adapt models to their domains faster, and to cut costs for inference instead of relying on closed paid APIs. All open-source causal language models on Hugging Face Hub can be found [here](https://huggingface.co/models?pipeline\\_tag=text-generation), and text-to-text generation models can be found [here](https://huggingface.co/models?pipeline\\_tag=text2text-generation&sort=trending). ### Models created with love by Hugging Face with BigScience and BigCode 💗\nHugging Face has co-led two science initiatives, BigScience and BigCode. As a result of them, two large language models were created, [BLOOM](https://huggingface.co/bigscience/bloom) 🌸 and [StarCoder](https://huggingface.co/bigcode/starcoder) 🌟. BLOOM is a causal language model trained on 46 languages and 13 programming languages. It is the first open-source model to have more parameters than GPT-3. You can find all the available checkpoints in the [BLOOM documentation](https://huggingface.co/docs/transformers/model\\_doc/bloom). StarCoder is a language model trained on permissive code from GitHub (with 80+ programming languages 🤯) with a Fill-in-the-Middle objective. It’s not fine-tuned on instructions, and thus, it serves more as a coding assistant to complete a given code, e.g., translate Python to C++, explain concepts (what’s recursion), or act as a terminal. You can try all of the StarCoder checkpoints [in this application](https://huggingface.co/spaces/bigcode/bigcode-playground). It also comes with a [VSCode extension](https://marketplace.visualstudio.com/items?itemName=HuggingFace.huggingface-vscode). Snippets to use all models mentioned in this blog post are given in either the model repository or the documentation page of that model type in Hugging Face. ## Licensing\nMany text generation models are either closed-source or the license limits commercial use. Fortunately, open-source alternatives are starting to appear and being embraced by the community as building blocks for further development, fine-tuning, or integration with other projects. Below you can find a list of some of the large causal language models with fully open-source licenses:\n- [Falcon 40B](https://huggingface.co/tiiuae/falcon-40b)\n- [XGen](https://huggingface.co/tiiuae/falcon-40b)\n- [MPT-30B](https://huggingface.co/mosaicml/mpt-30b)\n- [Pythia-12B](https://huggingface.co/EleutherAI/pythia-12b)\n- [RedPajama-INCITE-7B](https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base)\n- [OpenAssistant (Falcon variant)](https://huggingface.co/OpenAssistant/falcon-40b-sft-mix-1226)\nThere are two code generation models, [StarCoder by BigCode](https://huggingface.co/models?sort=trending&search=bigcode%2Fstarcoder) and [Codegen by Salesforce](https://huggingface.co/models?sort=trending&search=salesforce%2Fcodegen)."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Falcon\n## Overview\nFalcon is a class of causal decoder-only models built by [TII](https://www.tii.ae/). The largest Falcon checkpoints\nhave been trained on >=1T tokens of text, with a particular emphasis on the [RefinedWeb](https://arxiv.org/abs/2306.01116)\ncorpus."", ""Let's now take a look at how you can fine-tune your very own Falcon models - perhaps one of yours will end up on top of the leaderboard 🤗. ## Fine-tuning with PEFT\nTraining 10B+ sized models can be technically and computationally challenging. In this section we look at the tools available in the Hugging Face ecosystem to efficiently train extremely large models on simple hardware and show how to fine-tune the Falcon-7b on a single NVIDIA T4 (16GB - Google Colab). Let's see how we can train Falcon on the [Guanaco dataset](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) a high-quality subset of the [Open Assistant dataset](https://huggingface.co/datasets/OpenAssistant/oasst1) consisting of around 10,000 dialogues. With the [PEFT library](https://github.com/huggingface/peft) we can use the recent [QLoRA](https://arxiv.org/abs/2305.14314) approach to fine-tune adapters that are placed on top of the frozen 4-bit model. You can learn more about the integration of 4-bit quantized models [in this blog post](https://huggingface.co/blog/4bit-transformers-bitsandbytes). Because just a tiny fraction of the model is trainable when using Low Rank Adapters (LoRA), both the number of learned parameters and the size of the trained artifact are dramatically reduced. As shown in the screenshot below, the saved model has only 65MB for the 7B parameters model (15GB in float16). | ![repo-screenshot.png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/147\\_falcon/adapter-screenshot.png) |\n|:--:|\n| **The final repository has only 65MB of weights - compared to the original model that has approximately 15GB in half precision** |\nMore specifically, after selecting the target modules to adapt (in practice the query / key layers of the attention module), small trainable linear layers are attached close to these modules as illustrated below). The hidden states produced by the adapters are then added to the original states to get the final hidden state. | ![lora-gif](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/133\\_trl\\_peft/lora-animated.gif) |\n|:--:|\n| **The output activations original (frozen) pretrained weights (left) are augmented by a low rank adapter comprised of weight matrices A and B (right).** |\nOnce trained, there is no need to save the entire model as the base model was kept frozen. In addition, it is possible to keep the model in any arbitrary dtype (int8, fp4, fp16, etc.) as long as the output hidden states from these modules are casted to the same dtype as the ones from the adapters - this is the case for bitsandbytes modules (`Linear8bitLt` and `Linear4bit` ) that return hidden states with the same dtype as the original unquantized module."", ""It's the same technology that powers [HuggingChat](https://huggingface.co/chat/). We've also built a Core ML version of the 7B instruct model, and this is how it runs on an M1 MacBook Pro:\n[\n\nVideo: Falcon 7B Instruct running on an M1 MacBook Pro with Core ML. ](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/147_falcon/falcon-7b.mp4)\nThe video shows a lightweight app that leverages a Swift library for the heavy lifting: model loading, tokenization, input preparation, generation, and decoding. We are busy building this library to empower developers to integrate powerful LLMs in all types of applications without having to reinvent the wheel. It's still a bit rough, but we can't wait to share it with you. Meanwhile, you can download the [Core ML weights](https://huggingface.co/tiiuae/falcon-7b-instruct/tree/main/coreml/text-generation) from the repo and explore them yourself! # Inference\nYou can use the familiar transformers APIs to run the models on your own hardware, but you need to pay attention to a couple of details:\n- The models were trained using the `bfloat16` datatype, so we recommend you use the same. This requires a recent version of CUDA and works best on modern cards. You may also try to run inference using `float16`, but keep in mind that the models were evaluated using `bfloat16`. - You need to allow remote code execution. This is because the models use a new architecture that is not part of `transformers` yet - instead, the code necessary is provided by the model authors in the repo. Specifically, these are the files whose code will be used if you allow remote execution (using `falcon-7b-instruct` as an example): [configuration\\_RW.py](https://huggingface.co/tiiuae/falcon-7b-instruct/blob/main/configuration\\_RW.py), [modelling\\_RW.py](https://huggingface.co/tiiuae/falcon-7b-instruct/blob/main/modelling\\_RW.py). With these considerations, you can use the transformers `pipeline` API to load the 7B instruction model like this:\n```python\nfrom transformers import AutoTokenizer\nimport transformers\nimport torch\nmodel = \""tiiuae/falcon-7b-instruct\""\ntokenizer = AutoTokenizer.from\\_pretrained(model)\npipeline = transformers.pipeline(\n\""text-generation\"",\nmodel=model,\ntokenizer=tokenizer,\ntorch\\_dtype=torch.bfloat16,\ntrust\\_remote\\_code=True,\ndevice\\_map=\""auto\"",\n)\n```\nAnd then, you'd run text generation using code like the following:\n```python\nsequences = pipeline(\n\""Write a poem about Valencia.\"",\nmax\\_length=200,\ndo\\_sample=True,\ntop\\_k=10,\nnum\\_return\\_sequences=1,\neos\\_token\\_id=tokenizer.eos\\_token\\_id,\n)\nfor seq in sequences:\nprint(f\""Result: {seq['generated\\_text']}\"")\n```\nAnd you may get something like the following:\n```bash\nValencia, city of the sun\nThe city that glitters like a star\nA city of a thousand colors\nWhere the night is illuminated by stars\nValencia, the city of my heart\nWhere the past is kept in a golden chest\n```\n### Inference of Falcon 40B\nRunning the 40B model is challenging because of its size: it doesn't fit in a single A100 with 80 GB of RAM.""]",The Falcon language models were created by TII (Technology Innovation Institute).,Technology Innovation Institute

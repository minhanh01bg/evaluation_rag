user_input,retrieved_contexts,response,reference
"What architecture is the `tokenizers-linux-x64-musl` binary designed for?
","[""`tokenizers-linux-x64-musl`\nThis is the \\*\\*x86\\_64-unknown-linux-musl\\*\\* binary for `tokenizers`"", ""Most of the tokenizers used by the current state of the art language models need to be trained on a corpus that is similar to the one used to pre-train the language model. This training consists in learning rules to divide the text into tokens and the way to learn these rules and use them depends on the chosen tokenizer model. Thus, to train a new tokenizer it is first necessary to build a training corpus composed of raw texts. Then, you have to choose an architecture for your tokenizer. Here there are two options: the simplest is to reuse the same architecture as the one of a tokenizer used by another model already trained,otherwise it is also possible to completely design your tokenizer but it requires more experience and attention. Once the architecture is chosen, one can thus train this tokenizer on your constituted corpus. Finally, the last thing that you need to do is to save the learned rules to be able to use this tokenizer which is now ready to be used. Let's take an example: let's say you want to train a GPT-2 model on Python code. Even if the python code is in English this type of text is very specific and deserves a tokenizer trained on it - to convince you of this we will see at the end the difference produced on an example. For that we are going to use the method \""train\\_new\\_from\\_iterator\"" that all the fast tokenizers of the library have and thus in particular GPT2TokenizerFast."", ""!-- DISABLE-FRONTMATTER-SECTIONS -->\n# Tokenizers\nFast State-of-the-art tokenizers, optimized for both research and\nproduction\n[🤗 Tokenizers](https://github.com/huggingface/tokenizers) provides an\nimplementation of today's most used tokenizers, with a focus on\nperformance and versatility. These tokenizers are also used in [🤗 Transformers](https://github.com/huggingface/transformers). # Main features:\n- Train new vocabularies and tokenize, using today's most used tokenizers. - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server's CPU. - Easy to use, but also extremely versatile. - Designed for both research and production. - Full alignment tracking. Even with destructive normalization, it's always possible to get the part of the original sentence that corresponds to any token. - Does all the pre-processing: Truncation, Padding, add the special tokens your model needs.""]",The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.,x86_64-unknown-linux-musl
"What is the purpose of the BLIP-Diffusion model?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# BLIP-Diffusion\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. The abstract from the paper is:\n\\*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).\\*\nThe original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization. `BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## BlipDiffusionPipeline\n[[autodoc]] BlipDiffusionPipeline\n- all\n- \\_\\_call\\_\\_\n## BlipDiffusionControlNetPipeline\n[[autodoc]] BlipDiffusionControlNetPipeline\n- all\n- \\_\\_call\\_\\_"", ""The Stable Diffusion model is distributed under the CreateML OpenRail-M license. It's an open license that claims no rights on the outputs you generate and prohibits you from deliberately producing illegal or harmful content. The [model card](https://huggingface.co/CompVis/stable-diffusion-v1-4) provides more details, so take a moment to read them and consider carefully whether you accept the license. If you do, you need to be a registered user in the Hub and use an access token for the code to work. You have two options to provide your access token:\n- Use the `huggingface-cli login` command-line tool in your terminal and paste your token when prompted. It will be saved in a file in your computer. - Or use `notebook\\_login()` in a notebook, which does the same thing. The following cell will present a login interface unless you've already authenticated before in this computer. You'll need to paste your access token. ``` python\nif not (Path.home()/'.huggingface'/'token').exists(): notebook\\_login()\n```\nTPU devices support `bfloat16`, an efficient half-float type. We'll use it for our tests, but you can also use `float32` to use full precision instead. ``` python\ndtype = jnp.bfloat16\n```\nFlax is a functional framework, so models are stateless and parameters are stored outside them. Loading the pre-trained Flax pipeline will return both the pipeline itself and the model weights (or parameters). We are using a `bf16` version of the weights, which leads to type warnings that you can safely ignore. ``` python\npipeline, params = FlaxStableDiffusionPipeline.from\\_pretrained(\n\""CompVis/stable-diffusion-v1-4\"",\nrevision=\""bf16\"",\ndtype=dtype,\n)\n```\n## Inference\nSince TPUs usually have 8 devices working in parallel, we'll replicate our prompt as many times as devices we have."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# BLIP\n## Overview\nThe BLIP model was proposed in [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. BLIP is a model that is able to perform various multi-modal tasks including:\n- Visual Question Answering\n- Image-Text retrieval (Image-text matching)\n- Image Captioning\nThe abstract from the paper is the following:\n\\*Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks.""]","The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation in text-to-image generation tasks. It creates novel renditions of an input subject based on text prompts while overcoming limitations such as lengthy fine-tuning and difficulties in preserving subject fidelity. BLIP-Diffusion introduces a new multimodal encoder pre-trained to provide subject representation, allowing for efficient fine-tuning and flexible combinations with existing techniques for novel subject-driven generation and editing applications.",The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.
"How can a user claim authorship of a paper on the Hugging Face Hub?
","[""The Hugging Face Blog Repository 🤗\nThis is the official repository of the [Hugging Face Blog](https://hf.co/blog). \\*\\*If you are an external contributor\\*\\*: If your blog post is not a collaboration post with Hugging Face, please consider creating a [community blog](https://huggingface.co/blog-explorers) instead. Community blog posts appear on our blogs main page just like the blogs in this repository. ## How to write an article? 📝\n1️⃣ Create a branch `YourName/Title`\n2️⃣ Create a md (markdown) file, \\*\\*use a short file name\\*\\*. For instance, if your title is \""Introduction to Deep Reinforcement Learning\"", the md file name could be `intro-rl.md`. This is important because the \\*\\*file name will be the blogpost's URL\\*\\*. 3️⃣ Create a new folder in `assets`. Use the same name as the name of the md file. Optionally you may add a numerical prefix to that folder, using the number that hasn't been used yet. But this is no longer required. i.e. the asset folder in this example could be `123\\_intro-rl` or `intro-rl`. This folder will contain \\*\\*your thumbnail only\\*\\*. The folder number is mostly for (rough) ordering purposes, so it's no big deal if two concurrent articles use the same number. For the rest of your files, create a mirrored folder in the HuggingFace Documentation Images [repo](https://huggingface.co/datasets/huggingface/documentation-images/tree/main/blog). This is to reduce bloat in the GitHub base repo when cloning and pulling. 🖼️: In terms of images, \\*\\*try to have small files\\*\\* to avoid having a slow loading user experience:\n- Use compressed images, you can use this website: https://tinypng.com or https://www.iloveimg.com/compress-image\n4️⃣ Copy and paste this to your md file and change the elements\n- title\n- thumbnail\n- authors\n```\n---\ntitle: \""PUT YOUR TITLE HERE\""\nthumbnail: /blog/assets/101\\_decision-transformers-train/thumbnail.gif\nauthors:\n- user: your\\_hf\\_user\n- user: your\\_coauthor\n---\n# Train your first Decision Transformer\nYour content here [...]\n```\nWhen published, the Hub will insert the following UI elements right after the blogpost's main header (i.e."", ""Once you are satisfied, go to the webpage of your fork on GitHub. Click on \""Pull request\"". Make sure to add the GitHub handle of Patrick\nas one reviewer, so that the Hugging Face team gets notified for future changes. 6. Change the PR into a draft by clicking on \""Convert to draft\"" on the\nright of the GitHub pull request web page. In the following, whenever you have done some progress, don't forget to\ncommit your work and push it to your account so that it shows in the\npull request. Additionally, you should make sure to update your work\nwith the current main from time to time by doing:\ngit fetch upstream\ngit merge upstream/main\nIn general, all questions you might have regarding the model or your\nimplementation should be asked in your PR and discussed/solved in the\nPR. This way, Patrick will always be notified when you are\ncommitting new code or if you have a question. It is often very helpful\nto point Patrick to your added code so that the Hugging\nFace team can efficiently understand your problem or question. To do so, you can go to the \""Files changed\"" tab where you see all of\nyour changes, go to a line regarding which you want to ask a question,\nand click on the \""+\"" symbol to add a comment. Whenever a question or\nproblem has been solved, you can click on the \""Resolve\"" button of the\ncreated comment. In the same way, Patrick will open comments when reviewing\nyour code. We recommend asking most questions on GitHub on your PR."", ""We focus on fostering collaborative resolutions for any potential conflicts between users and repository authors, intervening only when necessary. To promote transparency, we encourage open discussions to occur within our Community tab. Our approach is a reflection of our ongoing efforts to adapt and progress, which is made possible by the invaluable input of our users who actively collaborate and share their feedback. We are committed to being receptive to comments and constantly striving for improvement. We encourage you to reach out to [feedback@huggingface.co](mailto:feedback@huggingface.co) with any questions or concerns. Let's join forces to build a friendly and supportive community that encourages open AI and ML collaboration! Together, we can make great strides forward in fostering a welcoming environment for everyone.""]","To claim authorship of a paper on the Hugging Face Hub, a user should follow these steps:

1. Ensure that they have a Hugging Face account.
2. Navigate to the specific paper or model they wish to claim authorship for.
3. Use the appropriate channels provided by Hugging Face, such as reaching out through the community tab or contacting support, to express their claim of authorship.
4. Provide any necessary information or documentation that supports their claim.

For more detailed instructions or specific processes, users may refer to the Hugging Face documentation or community guidelines.","By clicking their name on the corresponding Paper page and clicking ""claim authorship"", then confirming the request in paper settings for admin team validation."
"What is the purpose of the /healthcheck endpoint in the Datasets server API?
","[""Datasets server SSE API\n> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache. ## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub\\_cache` field is null for deleted entries, or when the response is an error. The `num\\_rows` value is `0` if it could not be determined. ```typescript\n{\ndataset: string;\nhub\\_cache: null | {\npreview: boolean;\nviewer: boolean;\npartial: boolean;\nnum\\_rows: int;\n};\n}\n```"", ""Datasets server API - rows endpoint\n> /rows endpoint\n## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### API service\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split"", ""Check dataset validity\nBefore you download a dataset from the Hub, it is helpful to know if a specific dataset you're interested in is available. Datasets Server provides the `/is-valid` endpoint to check if a specific dataset works without any errors. The API endpoint will return an error for datasets that cannot be loaded with the [🤗 Datasets](https://github.com/huggingface/datasets) library, for example, because the data hasn't been uploaded or the format is not supported. The largest datasets are partially supported by Datasets Server. If they are{\"" \""}\n[streamable](https://huggingface.co/docs/datasets/stream), Datasets\nServer can extract the first 100 rows without downloading the whole dataset. This is especially useful for previewing large datasets where downloading the\nwhole dataset may take hours! See the `preview` field in the\nresponse of `/is-valid` to check if a dataset is partially\nsupported. This guide shows you how to check dataset validity programmatically, but free to try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-17b761d0-b2b8-4638-a4f7-73be9049c324), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/isValidDataset). ## Check if a dataset is valid\n`/is-valid` checks whether a specific dataset loads without any error. This endpoint's query parameter requires you to specify the name of the dataset:\n\n```python\nimport requests\nheaders = {\""Authorization\"": f\""Bearer {API\\_TOKEN}\""}\nAPI\\_URL = \""https://datasets-server.huggingface.co/is-valid?dataset=rotten\\_tomatoes\""\ndef query():\nresponse = requests.get(API\\_URL, headers=headers)\nreturn response.json()\ndata = query()\n```\n\n```js\nimport fetch from \""node-fetch\"";\nasync function query(data) {\nconst response = await fetch(\n\""https://datasets-server.huggingface.co/is-valid?dataset=rotten\\_tomatoes\"",\n{\nheaders: { Authorization: `Bearer ${API\\_TOKEN}` },\nmethod: \""GET\""\n}\n);\nconst result = await response.json();\nreturn result;\n}\nquery().then((response) => {\nconsole.log(JSON.stringify(response));\n});\n```\n\n```curl\ncurl https://datasets-server.huggingface.co/is-valid?dataset=rotten\\_tomatoes \\\n-X GET \\\n-H \""Authorization: Bearer ${API\\_TOKEN}\""\n```\nThe response looks like this if a dataset is valid:\n```json\n{\n\""viewer\"": true,\n\""preview\"": true\n}\n```\nIf only the first rows of a dataset are available, then the response looks like:\n```json\n{\n\""viewer\"": false,\n\""preview\"": true\n}\n```\nFinally, if the dataset is not valid at all, then the response is:\n```json\n{\n\""viewer\"": false,\n\""preview\"": false\n}\n```\nSome cases where a dataset is not valid are:\n- the dataset viewer is disabled\n- the dataset is gated but the access is not granted: no token is passed or the passed token is not authorized\n- the dataset is private\n- the dataset contains no data or the data format is not supported\nRemember if a dataset is [gated](./quick_start#gated-datasets),\nyou'll need to provide your user token to submit a successful query!""]",The purpose of the `/healthcheck` endpoint in the Datasets server API is to ensure that the app is running.,Ensure the app is running
"What is the default context window size for Local Attention in the LongT5 model?
","[""\\n Assistant: Roughly 75 million people live in France \\n User: And how many are in Germany?\""`. Two things should be noted here:\n1. Keeping all the context is crucial for LLMs deployed in chat so that the LLM understands all the previous context of the conversation. E.g. for the example above the LLM needs to understand that the user refers to the population when asking `\""And how many are in Germany\""`. 2. The key-value cache is extremely useful for chat as it allows us to continuously grow the encoded chat history instead of having to re-encode the chat history again from scratch (as e.g. would be the case when using an encoder-decoder architecture). There is however one catch. While the required peak memory for the \\\\( \\mathbf{QK}^T \\\\) matrix is significantly reduced, holding the key-value cache in memory can become very memory expensive for long input sequences or multi-turn chat. Remember that the key-value cache needs to store the key-value vectors for all previous input vectors \\\\( \\mathbf{x}\\_i \\text{, for } i \\in \\{1, \\ldots, c - 1\\} \\\\) for all self-attention layers and for all attention heads. Let's compute the number of float values that need to be stored in the key-value cache for the LLM `bigcode/octocoder` that we used before. The number of float values amounts to two times the sequence length times the number of attention heads times the attention head dimension and times the number of layers. Computing this for our LLM at a hypothetical input sequence length of 16000 gives:\n```python\nconfig = model.config\n2 \\* 16\\_000 \\* config.n\\_layer \\* config.n\\_head \\* config.n\\_embd // config.n\\_head\n```\n\\*\\*Output\\*\\*:\n```\n7864320000\n```\nRoughly 8 billion float values! Storing 8 billion float values in `float16` precision requires around 15 GB of RAM which is circa half as much as the model weights themselves! Researchers have proposed two methods that allow to significantly reduce the memory cost of storing the key-value cache:\n1. [Multi-Query-Attention (MQA)](https://arxiv.org/abs/1911.02150)\nMulti-Query-Attention was proposed in Noam Shazeer's \\*Fast Transformer Decoding: One Write-Head is All You Need\\* paper. As the title says, Noam found out that instead of using `n\\_head` key-value projections weights, one can use a single head-value projection weight pair that is shared across all attention heads without that the model's performance significantly degrades. > By using a single head-value projection weight pair, the key value vectors \\\\( \\mathbf{k}\\_i, \\mathbf{v}\\_i \\\\) have to be identical across all attention heads which in turn means that we only need to store 1 key-value projection pair in the cache instead of `n\\_head` ones. As most LLMs use between 20 and 100 attention heads, MQA significantly reduces the memory consumption of the key-value cache. For the LLM used in this notebook we could therefore reduce the required memory consumption from 15 GB to less than 400 MB at an input sequence length of 16000. In addition to memory savings, MQA also leads to improved computational efficiency as explained in the following. In auto-regressive decoding, large key-value vectors need to be reloaded, concatenated with the current key-value vector pair to be then fed into the \\\\( \\mathbf{q}\\_c\\mathbf{K}^T \\\\) computation at every step. For auto-regressive decoding, the required memory bandwidth for the constant reloading can become a serious time bottleneck. By reducing the size of the key-value vectors less memory needs to be accessed, thus reducing the memory bandwidth bottleneck. For more detail, please have a look at [Noam's paper](https://arxiv.org/abs/1911.02150). The important part to understand here is that reducing the number of key-value attention heads to 1 only makes sense if a key-value cache is used. The peak memory consumption of the model for a single forward pass without key-value cache stays unchanged as every attention head still has a unique query vector so that each attention head still has a different \\\\( \\mathbf{QK}^T \\\\) matrix. MQA has seen wide adoption by the community and is now used by many of the most popular LLMs:\n- [\\*\\*Falcon\\*\\*](https://huggingface.co/tiiuae/falcon-40b)\n- [\\*\\*PaLM\\*\\*](https://arxiv.org/abs/2204.02311)\n- [\\*\\*MPT\\*\\*](https://huggingface.co/mosaicml/mpt-30b)\n- [\\*\\*BLOOM\\*\\*](https://huggingface.co/bigscience/bloom)\nAlso, the checkpoint used in this notebook - `bigcode/octocoder` - makes use of MQA."", ""Long story short, the default self-attention algorithm quickly becomes prohibitively memory-expensive for large input contexts. As LLMs improve in text comprehension and generation, they are applied to increasingly complex tasks. While models once handled the translation or summarization of a few sentences, they now manage entire pages, demanding the capability to process extensive input lengths. How can we get rid of the exorbitant memory requirements for large input lengths? We need a new way to compute the self-attention mechanism that gets rid of the \\\\( QK^T \\\\) matrix. [Tri Dao et al.](https://arxiv.org/abs/2205.14135) developed exactly such a new algorithm and called it \\*\\*Flash Attention\\*\\*. In a nutshell, Flash Attention breaks the \\\\(\\mathbf{V} \\times \\text{Softmax}(\\mathbf{QK}^T\\\\)) computation apart and instead computes smaller chunks of the output by iterating over multiple softmax computation steps:\n$$ \\textbf{O}\\_i \\leftarrow s^a\\_{ij} \\* \\textbf{O}\\_i + s^b\\_{ij} \\* \\mathbf{V}\\_{j} \\times \\text{Softmax}(\\mathbf{QK}^T\\_{i,j}) \\text{ for multiple } i, j \\text{ iterations} $$\nwith \\\\( s^a\\_{ij} \\\\) and \\\\( s^b\\_{ij} \\\\) being some softmax normalization statistics that need to be recomputed for every \\\\( i \\\\) and \\\\( j \\\\) . Please note that the whole Flash Attention is a bit more complex and is greatly simplified here as going in too much depth is out of scope for this guide. The reader is invited to take a look at the well-written [Flash Attention paper](https://arxiv.org/abs/2205.14135) for more details. The main takeaway here is:\n> By keeping track of softmax normalization statistics and by using some smart mathematics, Flash Attention gives \\*\\*numerical identical\\*\\* outputs compared to the default self-attention layer at a memory cost that only increases linearly with \\\\( N \\\\) . Looking at the formula, one would intuitively say that Flash Attention must be much slower compared to the default self-attention formula as more computation needs to be done. Indeed Flash Attention requires more FLOPs compared to normal attention as the softmax normalization statistics have to constantly be recomputed (see [paper](https://arxiv.org/abs/2205.14135) for more details if interested)\n> However, Flash Attention is much faster in inference compared to default attention which comes from its ability to significantly reduce the demands on the slower, high-bandwidth memory of the GPU (VRAM), focusing instead on the faster on-chip memory (SRAM). Essentially, Flash Attention makes sure that all intermediate write and read operations can be done using the fast \\*on-chip\\* SRAM memory instead of having to access the slower VRAM memory to compute the output vector \\\\( \\mathbf{O} \\\\) . In practice, there is currently absolutely no reason to \\*\\*not\\*\\* use Flash Attention if available."", ""This can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion, especially with a small number of layers. Growing the receptive field has been shown to improve quality significantly (Salimans et al.). Doing so, however, comes at a significant cost in number of parameters and consequently computational performance and can make training such models more challenging. >\n> ... self-attention can achieve a better balance in the trade-off between the virtually unlimited receptive field of the necessarily sequential PixelRNN and the limited receptive field of the much more parallelizable PixelCNN and its various extensions. [Image Transformer](https://arxiv.org/abs/1802.05751) uses transformers by restricting self attention over local neighborhoods of pixels. [Taming Transformers](https://arxiv.org/abs/2012.09841) and [DALL-E 1](https://arxiv.org/abs/2102.12092) combine convolutions and transformers. Both train a VQ-VAE to learn a discrete latent space, and then a transformer is trained in the compressed latent space. The transformer context is global but masked, because attention is provided over all previously predicted latent pixels, but the model is still AR so attention cannot be provided over not yet predicted pixels. [ImageBART](https://arxiv.org/abs/2108.08827) combines convolutions, transformers, and diffusion processes.""]",The default context window size for Local Attention in the LongT5 model is 512 tokens.,127 tokens
"What method is used to load a checkpoint for a task using `AutoPipeline`?
","[""The `AutoPipeline` class is designed to simplify the variety of pipelines in 🤗 Diffusers. It is a generic, \\*task-first\\* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name. Take a look at the [AutoPipeline](../api/pipelines/auto\\_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting. This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights. ## Choose an AutoPipeline for your task\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nprompt = \""peasant and dragon combat, wood cutting style, viking era, bevel with rune\""\nimage = pipeline(prompt, num\\_inference\\_steps=25).images[0]\nimage\n```\n\n![generated image of peasant fighting dragon in wood cutting style](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png)\n\nUnder the hood, [`AutoPipelineForText2Image`]:\n1. automatically detects a `\""stable-diffusion\""` class from the [`model\\_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model\\_index.json) file\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\""stable-diffusion\""` class name\nLikewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\""stable-diffusion\""` checkpoint from the `model\\_index.json` file and it'll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image:\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\npipeline = AutoPipelineForImage2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"",\ntorch\\_dtype=torch.float16,\nuse\\_safetensors=True,\n).to(\""cuda\"")\nprompt = \""a portrait of a dog wearing a pearl earring\""\nurl = \""https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665\\_Girl\\_with\\_a\\_Pearl\\_Earring.jpg/800px-1665\\_Girl\\_with\\_a\\_Pearl\\_Earring.jpg\""\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\""RGB\"")\nimage.thumbnail((768, 768))\nimage = pipeline(prompt, image, num\\_inference\\_steps=200, strength=0.75, guidance\\_scale=10.5).images[0]\nimage\n```\n\n![generated image of a vermeer portrait of a dog wearing a pearl earring](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png)\n\nAnd if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way:\n```py\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image\nimport torch\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""stabilityai/stable-diffusion-xl-base-1.0\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nimg\\_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\_examples/overture-creations-5sI6fQgYIuo.png\""\nmask\\_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\_examples/overture-creations-5sI6fQgYIuo\\_mask.png\""\ninit\\_image = load\\_image(img\\_url).convert(\""RGB\"")\nmask\\_image = load\\_image(mask\\_url).convert(\""RGB\"")\nprompt = \""A majestic tiger sitting on a bench\""\nimage = pipeline(prompt, image=init\\_image, mask\\_image=mask\\_image, num\\_inference\\_steps=50, strength=0.80).images[0]\nimage\n```\n\n![generated image of a tiger sitting on a bench](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png)\n\nIf you try to load an unsupported checkpoint, it'll throw an error:\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\npipeline = AutoPipelineForImage2Image.from\\_pretrained(\n\""openai/shap-e-img2img\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n)\n\""ValueError: AutoPipeline can't find a pipeline linked to ShapEImg2ImgPipeline for None\""\n```\n## Use multiple pipelines\nFor some workflows or if you're loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory."", ""If you want to use one of the official checkpoints for a task, explore the [gligen](https://huggingface.co/gligen) Hub organizations! [`StableDiffusionGLIGENPipeline`] was contributed by [Nikhil Gajendrakumar](https://github.com/nikhil-masterful) and [`StableDiffusionGLIGENTextImagePipeline`] was contributed by [Nguyễn Công Tú Anh](https://github.com/tuanh123789). ## StableDiffusionGLIGENPipeline\n[[autodoc]] StableDiffusionGLIGENPipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n- enable\\_model\\_cpu\\_offload\n- prepare\\_latents\n- enable\\_fuser\n## StableDiffusionGLIGENTextImagePipeline\n[[autodoc]] StableDiffusionGLIGENTextImagePipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n- enable\\_model\\_cpu\\_offload\n- prepare\\_latents\n- enable\\_fuser\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput"", ""This is a custom pipeline that merges upto 3 pretrained model checkpoints as long as they are in the HuggingFace model\\_index.json format. The checkpoint merging is currently memory intensive as it modifies the weights of a DiffusionPipeline object in place. Expect atleast 13GB RAM Usage on Kaggle GPU kernels and\non colab you might run out of the 12GB memory even while merging two checkpoints. Usage:-\n```python\nfrom diffusers import DiffusionPipeline\n#Return a CheckpointMergerPipeline class that allows you to merge checkpoints. #The checkpoint passed here is ignored. But still pass one of the checkpoints you plan to\n#merge for convenience\npipe = DiffusionPipeline.from\\_pretrained(\""CompVis/stable-diffusion-v1-4\"", custom\\_pipeline=\""checkpoint\\_merger\"")\n#There are multiple possible scenarios:\n#The pipeline with the merged checkpoints is returned in all the scenarios\n#Compatible checkpoints a.k.a matched model\\_index.json files. Ignores the meta attributes in model\\_index.json during comparison.( attrs with \\_ as prefix )\nmerged\\_pipe = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""CompVis/stable-diffusion-v1-2\""], interp = \""sigmoid\"", alpha = 0.4)\n#Incompatible checkpoints in model\\_index.json but merge might be possible. Use force = True to ignore model\\_index.json compatibility\nmerged\\_pipe\\_1 = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""hakurei/waifu-diffusion\""], force = True, interp = \""sigmoid\"", alpha = 0.4)\n#Three checkpoint merging. Only \""add\\_difference\"" method actually works on all three checkpoints. Using any other options will ignore the 3rd checkpoint. merged\\_pipe\\_2 = pipe.merge([\""CompVis/stable-diffusion-v1-4\"",\""hakurei/waifu-diffusion\"",\""prompthero/openjourney\""], force = True, interp = \""add\\_difference\"", alpha = 0.4)\nprompt = \""An astronaut riding a horse on Mars\""\nimage = merged\\_pipe(prompt).images[0]\n```\nSome examples along with the merge details:\n1. \""CompVis/stable-diffusion-v1-4\"" + \""hakurei/waifu-diffusion\"" ; Sigmoid interpolation; alpha = 0.8\n![Stable plus Waifu Sigmoid 0.8](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/stability\\_v1\\_4\\_waifu\\_sig\\_0.8.png)\n2. \""hakurei/waifu-diffusion\"" + \""prompthero/openjourney\"" ; Inverse Sigmoid interpolation; alpha = 0.8\n![Stable plus Waifu Sigmoid 0.8](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/waifu\\_openjourney\\_inv\\_sig\\_0.8.png)\n3. \""CompVis/stable-diffusion-v1-4\"" + \""hakurei/waifu-diffusion\"" + \""prompthero/openjourney\""; Add Difference interpolation; alpha = 0.5\n![Stable plus Waifu plus openjourney add\\_diff 0.5](https://huggingface.co/datasets/NagaSaiAbhinay/CheckpointMergerSamples/resolve/main/stable\\_waifu\\_openjourney\\_add\\_diff\\_0.5.png)\n### Stable Diffusion Comparisons\nThis Community Pipeline enables the comparison between the 4 checkpoints that exist for Stable Diffusion.""]","The `AutoPipeline` class automatically detects the correct pipeline class to use for a specific task, which simplifies the process of loading a checkpoint. You can use methods like `AutoPipelineForText2Image`, `AutoPipelineForImage2Image`, or `AutoPipelineForInpainting` to load the corresponding pipeline for text-to-image, image-to-image, or inpainting tasks, respectively. This allows you to focus on the task without needing to know the specific pipeline class name.",from_pretrained()
"What is the purpose of Diffusers library?
","[""Research projects\nThis folder contains various research projects using 🧨 Diffusers. They are not really maintained by the core maintainers of this library and often require a specific version of Diffusers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice. If you need help with any of those, please open an issue where you directly ping the author(s), as indicated at the top of the README of each folder."", ""This design philosophy is reflected in multiple parts of the library:\n- We follow PyTorch's API with methods like [`DiffusionPipeline.to`](https://huggingface.co/docs/diffusers/main/en/api/diffusion\\_pipeline#diffusers.DiffusionPipeline.to) to let the user handle device management. - Raising concise error messages is preferred to silently correct erroneous input. Diffusers aims at teaching the user, rather than making the library as easy to use as possible. - Complex model vs. scheduler logic is exposed instead of magically handled inside. Schedulers/Samplers are separated from diffusion models with minimal dependencies on each other. This forces the user to write the unrolled denoising loop. However, the separation allows for easier debugging and gives the user more control over adapting the denoising process or switching out diffusion models or schedulers. - Separately trained components of the diffusion pipeline, \\*e.g.\\* the text encoder, the UNet, and the variational autoencoder, each has their own model class."", ""--\ntitle: \""Ethical Guidelines for developing the Diffusers library\""\nthumbnail: /blog/assets/ethics-diffusers/thumbnail.png\nauthors:\n- user: giadap\n---\n# Ethical guidelines for developing the Diffusers library\nWe are on a journey to make our libraries more responsible, one commit at a time! As part of the [Diffusers library documentation](https://huggingface.co/docs/diffusers/main/en/index), we are proud to announce the publication of an [ethical framework](https://huggingface.co/docs/diffusers/main/en/conceptual/ethical\\_guidelines). Given diffusion models' real case applications in the world and potential negative impacts on society, this initiative aims to guide the technical decisions of the Diffusers library maintainers about community contributions. We wish to be transparent in how we make decisions, and above all, we aim to clarify what values guide those decisions. We see ethics as a process that leverages guiding values, concrete actions, and continuous adaptation. For this reason, we are committed to adjusting our guidelines over time, following the evolution of the Diffusers project and the valuable feedback from the community that keeps it alive. # Ethical guidelines\n\\* \\*\\*Transparency\\*\\*: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions. \\* \\*\\*Consistency\\*\\*: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent. \\* \\*\\*Simplicity\\*\\*: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project’s goals lean and coherent. \\* \\*\\*Accessibility\\*\\*: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community. \\* \\*\\*Reproducibility\\*\\*: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.""]","The Diffusers library is designed to facilitate the use of diffusion models, which are powerful tools for generating data. It aims to provide a user-friendly interface while maintaining control over the underlying processes. The library follows PyTorch's API for device management and emphasizes transparency, consistency, simplicity, accessibility, and reproducibility in its development and usage. This approach allows users to adapt the denoising process, switch out diffusion models or schedulers, and engage with the community effectively.",To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.
"What method does the EulerAncestralDiscreteScheduler use for sampling?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# HeunDiscreteScheduler\nThe Heun scheduler (Algorithm 1) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. The scheduler is ported from the [k-diffusion](https://github.com/crowsonkb/k-diffusion) library and created by [Katherine Crowson](https://github.com/crowsonkb/). ## HeunDiscreteScheduler\n[[autodoc]] HeunDiscreteScheduler\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling\\_utils.SchedulerOutput"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# EulerAncestralDiscreteScheduler\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling\\_euler\\_ancestral\\_discrete.EulerAncestralDiscreteSchedulerOutput"", ""The proposed method can be applied to any DMs and can generate high-fidelity samples in as few as 10 steps. In our experiments, it takes about 3 minutes on one A6000 GPU to generate 50k images from CIFAR10. Moreover, by directly using pre-trained DMs, we achieve the state-of-art sampling performance when the number of score function evaluation~(NFE) is limited, e.g., 4.17 FID with 10 NFEs, 3.37 FID, and 9.74 IS with only 15 NFEs on CIFAR10. Code is available at [this https URL](https://github.com/qsh-zh/deis).\\*\n## Tips\nIt is recommended to set `solver\\_order` to 2 or 3, while `solver\\_order=1` is equivalent to [`DDIMScheduler`]. Dynamic thresholding from [Imagen](https://huggingface.co/papers/2205.11487) is supported, and for pixel-space\ndiffusion models, you can set `thresholding=True` to use the dynamic thresholding. ## DEISMultistepScheduler\n[[autodoc]] DEISMultistepScheduler\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling\\_utils.SchedulerOutput""]",The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling.,Ancestral sampling with Euler method steps.
"What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?
","[""The abstract from the paper is the following:\n\\*While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a \""red bounding box\"" or \""pointed arrow\"". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available.\\*\nTips:\n- The architecture is similar than llava architecture except that the multi-modal projector takes a set of concatenated vision hidden states and has an additional layernorm layer on that module."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Image tasks with IDEFICS\n[[open-in-colab]]\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach\nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. For instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can\nsolve image-text tasks with a large multimodal model called IDEFICS. [IDEFICS](../model\\_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198),\na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image\nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content,\ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b)\nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the 🤗 Hub."", ""The largest model has 1.6 trillion parameters. - [NLLB MoE (Meta)](https://huggingface.co/facebook/nllb-moe-54b): A MoE variant of the NLLB translation model. - [OpenMoE](https://huggingface.co/fuzhao): A community effort that has released Llama-based MoEs. - [Mixtral 8x7B (Mistral)](https://huggingface.co/mistralai): A high-quality MoE that outperforms Llama 2 70B and has much faster inference. A instruct-tuned model is also released. Read more about it in [the announcement blog post](https://mistral.ai/news/mixtral-of-experts/). ## Exciting directions of work\nFurther experiments on \\*\\*distilling\\*\\* a sparse MoE back to a dense model with less parameters but similar number of parameters. Another area will be quantization of MoEs. [QMoE](https://arxiv.org/abs/2310.16795) (Oct. 2023) is a good step in this direction by quantizing the MoEs to less than 1 bit per parameter, hence compressing the 1.6T Switch Transformer which uses 3.2TB accelerator to just 160GB. So, TL;DR, some interesting areas to explore:\n\\* Distilling Mixtral into a dense model\n\\* Explore model merging techniques of the experts and their impact in inference time\n\\* Perform extreme quantization techniques of Mixtral\n## Some resources\n- [Adaptive Mixture of Local Experts (1991)](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)\n- [Learning Factored Representations in a Deep Mixture of Experts (2013)](https://arxiv.org/abs/1312.4314)\n- [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017)](https://arxiv.org/abs/1701.06538)\n- [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Jun 2020)](https://arxiv.org/abs/2006.16668)\n- [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts (Dec 2021)](https://arxiv.org/abs/2112.06905)\n- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (Jan 2022)](https://arxiv.org/abs/2101.03961)\n- [ST-MoE: Designing Stable and Transferable Sparse Expert Models (Feb 2022)](https://arxiv.org/abs/2202.08906)\n- [FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models(April 2022)](https://dl.acm.org/doi/10.1145/3503221.3508418)\n- [MegaBlocks: Efficient Sparse Training with Mixture-of-Experts (Nov 2022)](https://arxiv.org/abs/2211.15841)\n- [Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models (May 2023)](https://arxiv.org/abs/2305.14705)\n- [Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1), [Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1). ## Citation\n```bibtex\n@misc {sanseviero2023moe,\nauthor = { Omar Sanseviero and\nLewis Tunstall and\nPhilipp Schmid and\nSourab Mangrulkar and\nYounes Belkada and\nPedro Cuenca\n},\ntitle = { Mixture of Experts Explained },\nyear = 2023,\nurl = { https://huggingface.co/blog/moe },\npublisher = { Hugging Face Blog }\n}\n```\n```\nSanseviero, et al., \""Mixture of Experts Explained\"", Hugging Face Blog, 2023.""]",The large multimodal model that can solve image-text tasks and is based on Flamingo is called IDEFICS.,IDEFICS
"What is the purpose of the `gradio.Blocks` API?
","[""\\*\\*Prerequisites\\*\\*: This Guide uses [Gradio Blocks](/guides/quickstart/#blocks-more-flexibility-and-control), so make your are familiar with the Blocks class. ## Setting up your BigQuery Credentials\nTo use Gradio with BigQuery, you will need to obtain your BigQuery credentials and use them with the [BigQuery Python client](https://pypi.org/project/google-cloud-bigquery/). If you already have BigQuery credentials (as a `.json` file), you can skip this section. If not, you can do this for free in just a couple of minutes. 1. First, log in to your Google Cloud account and go to the Google Cloud Console (https://console.cloud.google.com/)\n2. In the Cloud Console, click on the hamburger menu in the top-left corner and select \""APIs & Services\"" from the menu. If you do not have an existing project, you will need to create one. 3. Then, click the \""+ Enabled APIs & services\"" button, which allows you to enable specific services for your project. Search for \""BigQuery API\"", click on it, and click the \""Enable\"" button. If you see the \""Manage\"" button, then the BigQuery is already enabled, and you're all set."", ""使用 Gradio 块像函数一样\nTags: TRANSLATION, HUB, SPACES\n\\*\\*先决条件\\*\\*: 本指南是在块介绍的基础上构建的。请确保[先阅读该指南](https://gradio.app/quickstart/#blocks-more-flexibility-and-control)。\n## 介绍\n你知道吗，除了作为一个全栈机器学习演示，Gradio 块应用其实也是一个普通的 Python 函数！？\n这意味着如果你有一个名为 `demo` 的 Gradio 块（或界面）应用，你可以像使用任何 Python 函数一样使用 `demo`。\n所以，像 `output = demo(\""Hello\"", \""friend\"")` 这样的操作会在输入为 \""Hello\"" 和 \""friend\"" 的情况下运行 `demo` 中定义的第一个事件，并将其存储在变量 `output` 中。\n如果以上内容让你打瞌睡 🥱，请忍耐一下！通过将应用程序像函数一样使用，你可以轻松地组合 Gradio 应用。\n接下来的部分将展示如何实现。\n## 将块视为函数\n假设我们有一个将英文文本翻译为德文文本的演示块。\n$code\\_english\\_translator\n我已经将它托管在 Hugging Face Spaces 上的 [gradio/english\\_translator](https://huggingface.co/spaces/gradio/english\\_translator)。\n你也可以在下面看到演示：\n$demo\\_english\\_translator\n现在，假设你有一个生成英文文本的应用程序，但你还想额外生成德文文本。\n你可以选择：\n1. 将我的英德翻译的源代码复制粘贴到你的应用程序中。\n2. 在你的应用程序中加载我的英德翻译，并将其当作普通的 Python 函数处理。\n选项 1 从技术上讲总是有效的，但它经常引入不必要的复杂性。\n选项 2 允许你借用所需的功能，而不会过于紧密地耦合我们的应用程序。\n你只需要在源文件中调用 `Blocks.load` 类方法即可。\n之后，你就可以像使用普通的 Python 函数一样使用我的翻译应用程序了！\n下面的代码片段和演示展示了如何使用 `Blocks.load`。\n请注意，变量 `english\\_translator` 是我的英德翻译应用程序，但它在 `generate\\_text` 中像普通函数一样使用。\n$code\\_generate\\_english\\_german\n$demo\\_generate\\_english\\_german\n## 如何控制使用应用程序中的哪个函数\n如果你正在加载的应用程序定义了多个函数，你可以使用 `fn\\_index` 和 `api\\_name` 参数指定要使用的函数。\n在英德演示的代码中，你会看到以下代码行：\ntranslate\\_btn.click(translate, inputs=english, outputs=german, api\\_name=\""translate-to-german\"")\n这个 `api\\_name` 在我们的应用程序中给这个函数一个唯一的名称。你可以使用这个名称告诉 Gradio 你想使用\n上游空间中的哪个函数：\nenglish\\_generator(text, api\\_name=\""translate-to-german\"")[0][\""generated\\_text\""]\n你也可以使用 `fn\\_index` 参数。\n假设我的应用程序还定义了一个英语到西班牙语的翻译函数。\n为了在我们的文本生成应用程序中使用它，我们将使用以下代码：\nenglish\\_generator(text, fn\\_index=1)[0][\""generated\\_text\""]\nGradio 空间中的函数是从零开始索引的，所以西班牙语翻译器将是我的空间中的第二个函数，\n因此你会使用索引 1。\n## 结语\n我们展示了将块应用视为普通 Python 函数的方法，这有助于在不同的应用程序之间组合功能。\n任何块应用程序都可以被视为一个函数，但一个强大的模式是在将其视为函数之前，\n在[自己的应用程序中加载](https://huggingface.co/spaces)托管在[Hugging Face Spaces](https://huggingface.co/spaces)上的应用程序。\n您也可以加载托管在[Hugging Face Model Hub](https://huggingface.co/models)上的模型——有关示例，请参阅[使用 Hugging Face 集成](/using\\_hugging\\_face\\_integrations)指南。\n### 开始构建！⚒️\n## Parting Remarks\n我们展示了如何将 Blocks 应用程序视为常规 Python 函数，以便在不同的应用程序之间组合功能。\n任何 Blocks 应用程序都可以被视为函数，但是一种有效的模式是在将其视为自己应用程序的函数之前，先`加载`托管在[Hugging Face Spaces](https://huggingface.co/spaces)上的应用程序。\n您还可以加载托管在[Hugging Face Model Hub](https://huggingface.co/models)上的模型-请参见[使用 Hugging Face 集成指南](/using\\_hugging\\_face\\_integrations)中的示例。\n### Happy building!"", ""Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our \\*\\*newly developed\\*\\* low-level API called `gradio.Blocks`. Now, what's the difference between `Interface` and `Blocks`? - ⚡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs. - 🧱 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \""building blocks\""). ### Why Blocks 🧱?[[why-blocks-]]\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\n- Group together related demos as multiple tabs in one web application\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a dropdown) or its visibility based on user input\nWe will explore all of these concepts below. ### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook. ```py\nimport gradio as gr\ndef flip\\_text(x):\nreturn x[::-1]\ndemo = gr.Blocks()\nwith demo:\ngr.Markdown(\n\""\""\""\n# Flip Text! Start typing below to see the output. \""\""\""\n)\ninput = gr.Textbox(placeholder=\""Flip this text\"")\noutput = gr.Textbox()\ninput.change(fn=flip\\_text, inputs=input, outputs=output)\ndemo.launch()\n```\nThis simple example above introduces 4 concepts that underlie Blocks:\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.""]","The `gradio.Blocks` API is a low-level API that allows you to have full control over the data flows and layout of your application. It provides flexibility to build complex, multi-step applications, enabling you to:

- Group related demos as multiple tabs in one web application.
- Change the layout of your demo, specifying where the inputs and outputs are located.
- Create multi-step interfaces, where the output of one model becomes the input to the next model, or have more flexible data flows in general.
- Modify a component's properties (e.g., choices in a dropdown) or its visibility based on user input.

Overall, `gradio.Blocks` offers more customization and control compared to the higher-level `Interface` API.","The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications."
"What is the purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents""?
","[""We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art\nperformances on a bunch of tasks including ImageNet-CN, Flicker30k- CN, and COCO-CN. Further, we obtain very close performances with\nCLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding.\\*\nThis model was contributed by [jongjyh](https://huggingface.co/jongjyh). ## Usage tips and example\nThe usage of AltCLIP is very similar to the CLIP. the difference between CLIP is the text encoder. Note that we use bidirectional attention instead of casual attention\nand we take the [CLS] token in XLM-R to represent text embedding. AltCLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image\nclassification. AltCLIP uses a ViT like transformer to get visual features and a bidirectional language model to get the text\nfeatures. Both the text and visual features are then projected to a latent space with identical dimension. The dot\nproduct between the projected image and text features is then used as a similar score. To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-overlapping patches,\nwhich are then linearly embedded."", ""It enables highly efficient computation of modern NLP and CV models such as BERT, GPT, Transformer, etc. It is therefore best useful for machine translation, text generation, image classification, and other sequence related tasks. Keywords: Training, Inference, Sequence Processing, Sequence Generation\n## [LaTeX-OCR](https://github.com/lukas-blecher/LaTeX-OCR)\nThe goal of this project is to create a learning based system that takes an image of a math formula and returns corresponding LaTeX code. Keywords: OCR, LaTeX, Math formula\n## [open\\_clip](https://github.com/mlfoundations/open\\_clip)\nOpenCLIP is an open source implementation of OpenAI's CLIP. The goal of this repository is to enable training models with contrastive image-text supervision, and to investigate their properties such as robustness to distribution shift. The starting point is an implementation of CLIP that matches the accuracy of the original CLIP models when trained on the same dataset. Specifically, a ResNet-50 model trained with this codebase on OpenAI's 15 million image subset of YFCC achieves 32.7% top-1 accuracy on ImageNet. Keywords: CLIP, Open-source, Contrastive, Image-text\n## [dalle-playground](https://github.com/saharmor/dalle-playground)\nA playground to generate images from any text prompt using Stable Diffusion and Dall-E mini."", ""Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference. Project Page: [this https URL](https://latent-consistency-models.github.io/).\\*\nA demo for the [SimianLuo/LCM\\_Dreamshaper\\_v7](https://huggingface.co/SimianLuo/LCM\\_Dreamshaper\\_v7) checkpoint can be found [here](https://huggingface.co/spaces/SimianLuo/Latent\\_Consistency\\_Model). The pipelines were contributed by [luosiallen](https://luosiallen.github.io/), [nagolinc](https://github.com/nagolinc), and [dg845](https://github.com/dg845). ## LatentConsistencyModelPipeline\n[[autodoc]] LatentConsistencyModelPipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_freeu\n- disable\\_freeu\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n## LatentConsistencyModelImg2ImgPipeline\n[[autodoc]] LatentConsistencyModelImg2ImgPipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_freeu\n- disable\\_freeu\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput""]","I'm sorry, but I cannot provide information about the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents"" as it is not part of the resources available from HuggingFace and gradio-app. Please refer to the relevant documentation or resources for details on that specific paper.",The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.
"What command is used to install the requirements for a research project using 🤗 Transformers?
","[""For instance, if the review was negative, I could create a feature which would contact the customer, apologize for the poor quality of service, and inform him/her that our support team will contact him/her as soon as possible to assist him and hopefully fix the problem. Since I don't plan to get more than 2,000 requests per month, I didn't impose any performance constraints regarding the time and the scalability. ## The Transformers library\nI was a bit confused at the beginning when I downloaded the .h5 file. I thought it would be compatible with `tensorflow.keras.models.load\\_model`, but this wasn't the case. After a few minutes of research I was able to figure out that the file was a weights checkpoint rather than a Keras model. After that, I tried out the API that Hugging Face offers and read a bit more about the pipeline feature they offer. Since the results of the API & the pipeline were great, I decided that I could serve the model through the pipeline on my own server. Below is the [official example](https://github.com/huggingface/transformers#quick-tour) from the Transformers GitHub page. ```python\nfrom transformers import pipeline\n# Allocate a pipeline for sentiment-analysis\nclassifier = pipeline('sentiment-analysis')\nclassifier('We are very happy to include pipeline into the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9978193640708923}]\n```\n## Deploy transformers to Google Cloud\n> GCP is chosen as it is the cloud environment I am using in my personal organization."", ""Most parts of the code you will add for\nthe new 🤗 Transformers model already exist somewhere in 🤗\nTransformers. Take some time to find similar, already existing\nmodels and tokenizers you can copy from. [grep](https://www.gnu.org/software/grep/) and\n[rg](https://github.com/BurntSushi/ripgrep) are your friends. Note\nthat it might very well happen that your model's tokenizer is based\non one model implementation, and your model's modeling code on\nanother one. \\*E.g.\\*, FSMT's modeling code is based on BART, while\nFSMT's tokenizer code is based on XLM. - It's more of an engineering challenge than a scientific challenge. You should spend more time on creating an efficient debugging\nenvironment than trying to understand all theoretical aspects of the\nmodel in the paper. - Ask for help when you're stuck! Models are the core component of 🤗\nTransformers so we, at Hugging Face, are more than happy to help\nyou at every step to add your model. Don't hesitate to ask if you\nnotice you are not making progress. In the following, we try to give you a general recipe that we found most\nuseful when porting a model to 🤗 Transformers. The following list is a summary of everything that has to be done to add\na model and can be used by you as a To-Do List:\n1."", ""Note that unlike the main examples these are not actively maintained, and may require specific older versions of dependencies in order to run. While we strive to present as many use cases as possible, the example scripts are just that - examples. It is expected that they won't work out-of-the-box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs. To help you with that, most of the examples fully expose the preprocessing of the data, allowing you to tweak and edit them as required. Please discuss on the [forum](https://discuss.huggingface.co/) or in an [issue](https://github.com/huggingface/transformers/issues) a feature you would like to implement in an example before submitting a PR; we welcome bug fixes, but since we want to keep the examples as simple as possible it's unlikely that we will merge a pull request adding more functionality at the cost of readability. ## Important note\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, you have to \\*\\*install the library from source\\*\\* and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\n```bash\ngit clone https://github.com/huggingface/transformers\ncd transformers\npip install . ```\nThen cd in the example folder of your choice and run\n```bash\npip install -r requirements.txt\n```\nTo browse the examples corresponding to released versions of 🤗 Transformers, click on the line below and then on your desired version of the library:\nExamples for older versions of 🤗 Transformers\n\n* [v4.21.0](https://github.com/huggingface/transformers/tree/v4.21.0/examples)\n* [v4.20.1](https://github.com/huggingface/transformers/tree/v4.20.1/examples)\n* [v4.19.4](https://github.com/huggingface/transformers/tree/v4.19.4/examples)\n* [v4.18.0](https://github.com/huggingface/transformers/tree/v4.18.0/examples)\n* [v4.17.0](https://github.com/huggingface/transformers/tree/v4.17.0/examples)\n* [v4.16.2](https://github.com/huggingface/transformers/tree/v4.16.2/examples)\n* [v4.15.0](https://github.com/huggingface/transformers/tree/v4.15.0/examples)\n* [v4.14.1](https://github.com/huggingface/transformers/tree/v4.14.1/examples)\n* [v4.13.0](https://github.com/huggingface/transformers/tree/v4.13.0/examples)\n* [v4.12.5](https://github.com/huggingface/transformers/tree/v4.12.5/examples)\n* [v4.11.3](https://github.com/huggingface/transformers/tree/v4.11.3/examples)\n* [v4.10.3](https://github.com/huggingface/transformers/tree/v4.10.3/examples)\n* [v4.9.2](https://github.com/huggingface/transformers/tree/v4.9.2/examples)\n* [v4.8.2](https://github.com/huggingface/transformers/tree/v4.8.2/examples)\n* [v4.7.0](https://github.com/huggingface/transformers/tree/v4.7.0/examples)\n* [v4.6.1](https://github.com/huggingface/transformers/tree/v4.6.1/examples)\n* [v4.5.1](https://github.com/huggingface/transformers/tree/v4.5.1/examples)\n* [v4.4.2](https://github.com/huggingface/transformers/tree/v4.4.2/examples)\n* [v4.3.3](https://github.com/huggingface/transformers/tree/v4.3.3/examples)\n* [v4.2.2](https://github.com/huggingface/transformers/tree/v4.2.2/examples)\n* [v4.1.1](https://github.com/huggingface/transformers/tree/v4.1.1/examples)\n* [v4.0.1](https://github.com/huggingface/transformers/tree/v4.0.1/examples)\n* [v3.5.1](https://github.com/huggingface/transformers/tree/v3.5.1/examples)\n* [v3.4.0](https://github.com/huggingface/transformers/tree/v3.4.0/examples)\n* [v3.3.1](https://github.com/huggingface/transformers/tree/v3.3.1/examples)\n* [v3.2.0](https://github.com/huggingface/transformers/tree/v3.2.0/examples)\n* [v3.1.0](https://github.com/huggingface/transformers/tree/v3.1.0/examples)\n* [v3.0.2](https://github.com/huggingface/transformers/tree/v3.0.2/examples)\n* [v2.11.0](https://github.com/huggingface/transformers/tree/v2.11.0/examples)\n* [v2.10.0](https://github.com/huggingface/transformers/tree/v2.10.0/examples)\n* [v2.9.1](https://github.com/huggingface/transformers/tree/v2.9.1/examples)\n* [v2.8.0](https://github.com/huggingface/transformers/tree/v2.8.0/examples)\n* [v2.7.0](https://github.com/huggingface/transformers/tree/v2.7.0/examples)\n* [v2.6.0](https://github.com/huggingface/transformers/tree/v2.6.0/examples)\n* [v2.5.1](https://github.com/huggingface/transformers/tree/v2.5.1/examples)\n* [v2.4.0](https://github.com/huggingface/transformers/tree/v2.4.0/examples)\n* [v2.3.0](https://github.com/huggingface/transformers/tree/v2.3.0/examples)\n* [v2.2.0](https://github.com/huggingface/transformers/tree/v2.2.0/examples)\n* [v2.1.1](https://github.com/huggingface/transformers/tree/v2.1.0/examples)\n* [v2.0.0](https://github.com/huggingface/transformers/tree/v2.0.0/examples)\n* [v1.2.0](https://github.com/huggingface/transformers/tree/v1.2.0/examples)\n* [v1.1.0](https://github.com/huggingface/transformers/tree/v1.1.0/examples)\n* [v1.0.0](https://github.com/huggingface/transformers/tree/v1.0.0/examples)\nAlternatively, you can switch your cloned 🤗 Transformers to a specific version (for instance with v3.5.1) with\n```bash\ngit checkout tags/v3.5.1\n```\nand run the example command as usual afterward. ## Running the Examples on Remote Hardware with Auto-Setup\n[run\\_on\\_remote.py](./run\\_on\\_remote.py) is a script that launches any example on remote self-hosted hardware,\nwith automatic hardware and environment setup.""]","To install the requirements for a research project using 🤗 Transformers, you should navigate to the example folder of your choice and run the following command:

```bash
pip install -r requirements.txt
```",pip install -r requirements.txt
"What task does the `roberta-large-mnli` checkpoint perform?
","[""!-- DISABLE-FRONTMATTER-SECTIONS -->\n# End-of-chapter quiz[[end-of-chapter-quiz]]\nThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood. First, though, let's test what you learned in this chapter! ### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform? roberta-large-mnli page.\""\n},\n{\ntext: \""Text classification\"",\nexplain: \""More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) — a task also called *natural language inference*.\"",\ncorrect: true\n},\n{\ntext: \""Text generation\"",\nexplain: \""Look again on the [roberta-large-mnli page](\\\""https://huggingface.co/roberta-large-mnli\\\"").\""\n}\n]}\n/>\n### 2. What will the following code return? ```py\nfrom transformers import pipeline\nner = pipeline(\""ner\"", grouped\\_entities=True)\nner(\""My name is Sylvain and I work at Hugging Face in Brooklyn.\"")\n```\nsentiment-analysis pipeline.\""\n},\n{\ntext: \""It will return a generated text completing this sentence.\"",\nexplain: \""This is incorrect — it would be a `text-generation` pipeline.\"",\n},\n{\ntext: \""It will return the words representing persons, organizations or locations.\"",\nexplain: \""Furthermore, with `grouped_entities=True`, it will group together the words belonging to the same entity, like \\\""Hugging Face\\\"".\"",\ncorrect: true\n}\n]}\n/>\n### 3."", ""This is\nalso a seeting where we only have `K=4` possible labels. The higher the number of classes for a given task, the more\ndrastic the speedup will be, since the zero-shot teacher's complexity scales linearly with the number of classes. Since we secretly have access to ground truth labels for AG's news, we can evaluate the accuracy of each model. The\noriginal zero-shot model `roberta-large-mnli` gets an accuracy of 69.3% on the held-out test set. After training a\nstudent on the unlabeled training set, the distilled model gets a similar score of 70.4%. Lastly, you can share the distilled model with the community and/or use it with our inference API by [uploading it\nto the 🤗 Hub](https://huggingface.co/transformers/model\\_sharing.html). We've uploaded the distilled model from this\nexample at\n[joeddav/distilbert-base-uncased-agnews-student](https://huggingface.co/joeddav/distilbert-base-uncased-agnews-student)."", ""Second, an enhanced mask decoder is used to replace the output softmax layer to\npredict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency\nof model pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of\nthe training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9%\n(90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and\npre-trained models will be made publicly available at https://github.com/microsoft/DeBERTa.\\*\nThis model was contributed by [DeBERTa](https://huggingface.co/DeBERTa). This model TF 2.0 implementation was\ncontributed by [kamalkraj](https://huggingface.co/kamalkraj) . The original code can be found [here](https://github.com/microsoft/DeBERTa). ## Resources\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with DeBERTa.""]","The `roberta-large-mnli` checkpoint performs text classification, specifically classifying if two sentences are logically linked across three labels: contradiction, neutral, and entailment. This task is also known as natural language inference.",Text classification
"What service is replacing the Paid tier of the Inference API at Hugging Face?
","[""The Hugging Face Hub[[the-hugging-face-hub]]\nThe [Hugging Face Hub](https://huggingface.co/) –- our main website –- is a central platform that enables anyone to discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with more than 10,000 publicly available. We'll focus on the models in this chapter, and take a look at the datasets in Chapter 5. The models in the Hub are not limited to 🤗 Transformers or even NLP. There are models from [Flair](https://github.com/flairNLP/flair) and [AllenNLP](https://github.com/allenai/allennlp) for NLP, [Asteroid](https://github.com/asteroid-team/asteroid) and [pyannote](https://github.com/pyannote/pyannote-audio) for speech, and [timm](https://github.com/rwightman/pytorch-image-models) for vision, to name a few. Each of these models is hosted as a Git repository, which allows versioning and reproducibility. Sharing a model on the Hub means opening it up to the community and making it accessible to anyone looking to easily use it, in turn eliminating their need to train a model on their own and simplifying sharing and usage. Additionally, sharing a model on the Hub automatically deploys a hosted Inference API for that model. Anyone in the community is free to test it out directly on the model's page, with custom inputs and appropriate widgets. The best part is that sharing and using any public model on the Hub is completely free! [Paid plans](https://huggingface.co/pricing) also exist if you wish to share models privately. The video below shows how to navigate the Hub. Having a huggingface.co account is required to follow along this part, as we'll be creating and managing repositories on the Hugging Face Hub: [create an account](https://huggingface.co/join)"", ""The base foundation model can then be fine-tuned on the updated labels and evaluated once again, with this iterative “detect and correct” process continuing until the adapted foundation model is sufficiently high quality to deploy. Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One – or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) – specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment. ## How does Hugging Face help? Snorkel AI’s partnership with Hugging Face supercharges Snorkel Flow’s foundation model capabilities. Initially we only made a small number of foundation models available. Each one required a dedicated service, making it prohibitively expensive and difficult for us to offer enterprises the flexibility to capitalize on the rapidly growing variety of models available. Adopting Hugging Face’s Inference Endpoint service enabled us to expand the number of foundation models our users could tap into while keeping costs manageable. Hugging Face’s service allows users to create a model API in a few clicks and begin using it immediately. Crucially, the new service has “pause and resume” capabilities that allow us to activate a model API when a client needs it, and put it to sleep when they don’t. \""We were pleasantly surprised to see how straightforward Hugging Face Inference Endpoint service was to set up.. All the configuration options were pretty self-explanatory, but we also had access to all the options we needed in terms of what cloud to run on, what security level we needed, etc.\""\n– Snorkel CTO and Co-founder Braden Hancock\n## How does this help Snorkel customers?"", ""We add new labels on a daily basis and encounter the fun challenges of model lifecycle management, including unique things like company rebranding and new companies and products emerging after Rocket Companies acquired Truebill in late 2021. We constantly examine whether we have the right model topology for our problem. While LLMs have recently been in the news, we’ve struggled to find an implementation that can outperform our specialized transformer classifiers at this time in both speed and cost. We see promise in the early results of using them in the long tail of services (i.e. mom-and-pop shops) - keep an eye out for that in a future version of Rocket Money! The uptime and confidence we have in the HuggingFace Inference API has allowed us to focus our energy on the value generated by the models and less on the plumbing and day-to-day operation. With the help of Hugging Face, we have taken on more scale and complexity within our model and the types of value it generates. Their customer service and support have exceeded our expectations and they’re genuinely a great partner in our journey. \\_If you want to learn how Hugging Face can manage your ML inference workloads, contact the Hugging Face team [here](https://huggingface.co/support#form/).\\_""]","The service replacing the Paid tier of the Inference API at Hugging Face is the Inference Endpoint service. This service allows users to create a model API in a few clicks and begin using it immediately, with features such as ""pause and resume"" capabilities for managing model API activation based on client needs.",Inference Endpoints
"What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?
","[""In the original model it is possible to use an average pooling layer with a fixed 7x7 window and stride 1 instead of global pooling. For inputs that are larger than the recommended image size, this gives a pooled output that is larger than 1x1. The Hugging Face implementation does not support this. - The original TensorFlow checkpoints include quantized models. We do not support these models as they include additional \""FakeQuantization\"" operations to unquantize the weights. - It's common to extract the output from the expansion layers at indices 10 and 13, as well as the output from the final 1x1 convolution layer, for downstream purposes. Using `output\\_hidden\\_states=True` returns the output from all intermediate layers. There is currently no way to limit this to specific layers. - The DeepLabV3+ segmentation head does not use the final convolution layer from the backbone, but this layer gets computed anyway. There is currently no way to tell [`MobileNetV2Model`] up to which layer it should run."", ""However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model\\_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model\\_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model\\_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix. ### Decoder[[nlp-decoder]]\n[GPT-2](model\\_doc/gpt2) is a decoder-only Transformer that predicts the next word in the sequence."", ""The abstract from the paper is the following:\n\\*Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by\ntaking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of\nquantization parameters and evaluate their choices on a wide range of neural network models for different application\ndomains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration\nby processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is\nable to maintain accuracy within 1% of the floating-point baseline on all networks studied, including models that are\nmore difficult to quantize, such as MobileNets and BERT-large.\\*\nThis model was contributed by [shangz](https://huggingface.co/shangz). ## Usage tips\n- QDQBERT model adds fake quantization operations (pair of QuantizeLinear/DequantizeLinear ops) to (i) linear layer\ninputs and weights, (ii) matmul inputs, (iii) residual add inputs, in BERT model. - QDQBERT requires the dependency of [Pytorch Quantization Toolkit](https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization). To install `pip install pytorch-quantization --extra-index-url https://pypi.ngc.nvidia.com`\n- QDQBERT model can be loaded from any checkpoint of HuggingFace BERT model (for example \\*bert-base-uncased\\*), and\nperform Quantization Aware Training/Post Training Quantization. - A complete example of using QDQBERT model to perform Quatization Aware Training and Post Training Quantization for\nSQUAD task can be found at [transformers/examples/research\\_projects/quantization-qdqbert/](examples/research\\_projects/quantization-qdqbert/). ### Set default quantizers\nQDQBERT model adds fake quantization operations (pair of QuantizeLinear/DequantizeLinear ops) to BERT by\n`TensorQuantizer` in [Pytorch Quantization Toolkit](https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization). `TensorQuantizer` is the module\nfor quantizing tensors, with `QuantDescriptor` defining how the tensor should be quantized. Refer to [Pytorch\nQuantization Toolkit userguide](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/userguide.html) for more details. Before creating QDQBERT model, one has to set the default `QuantDescriptor` defining default tensor quantizers. Example:\n```python\n>>> import pytorch\\_quantization.nn as quant\\_nn\n>>> from pytorch\\_quantization.tensor\\_quant import QuantDescriptor\n>>> # The default tensor quantizer is set to use Max calibration method\n>>> input\\_desc = QuantDescriptor(num\\_bits=8, calib\\_method=\""max\"")\n>>> # The default tensor quantizer is set to be per-channel quantization for weights\n>>> weight\\_desc = QuantDescriptor(num\\_bits=8, axis=((0,)))\n>>> quant\\_nn.QuantLinear.set\\_default\\_quant\\_desc\\_input(input\\_desc)\n>>> quant\\_nn.QuantLinear.set\\_default\\_quant\\_desc\\_weight(weight\\_desc)\n```\n### Calibration\nCalibration is the terminology of passing data samples to the quantizer and deciding the best scaling factors for\ntensors.""]","SqueezeBERT uses depthwise separable convolutions instead of fully-connected layers for the Q (query), K (key), V (value), and feed-forward network (FFN) layers. This architectural feature helps to reduce the number of parameters and improve efficiency.",Grouped convolutions
"What type of license is the HuggingFace Team's software distributed under?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction"", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Rigid Bodies\n[[autodoc]] RigidBodyComponent"", ""Please give it a try and let us know what you think. We'd love to read your feedback on the Hugging Face [forum](https://discuss.huggingface.co/). Thank you for reading!""]","The HuggingFace Team's software is distributed under the Apache License, Version 2.0.","Apache License, Version 2.0"
"What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?
","[""It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\n\\*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\*\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A."", ""--\ntitle: Hyperparameter Search with Transformers and Ray Tune\nthumbnail: /blog/assets/06\\_ray\\_tune/ray-hf.jpg\nauthors:\n- user: ray-project\nguest: true\n---\n# Hyperparameter Search with Transformers and Ray Tune\n##### A guest blog post by Richard Liaw from the Anyscale team\nWith cutting edge research implementations, thousands of trained models easily accessible, the Hugging Face [transformers](https://github.com/huggingface/transformers) library has become critical to the success and growth of natural language processing today. For any machine learning model to achieve good performance, users often need to implement some form of parameter tuning. Yet, nearly everyone ([1](https://medium.com/@prakashakshay90/fine-tuning-bert-model-using-pytorch-f34148d58a37), [2](https://mccormickml.com/2019/07/22/BERT-fine-tuning/#advantages-of-fine-tuning)) either ends up disregarding hyperparameter tuning or opting to do a simplistic grid search with a small search space. However, simple experiments are able to show the benefit of using an advanced tuning technique. Below is [a recent experiment run on a BERT](https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b) model from [Hugging Face transformers](https://github.com/huggingface/transformers) on the [RTE dataset](https://aclweb.org/aclwiki/Textual\\_Entailment\\_Resource\\_Pool). Genetic optimization techniques like [PBT](https://docs.ray.io/en/latest/tune/api\\_docs/schedulers.html#population-based-training-tune-schedulers-populationbasedtraining) can provide large performance improvements compared to standard hyperparameter optimization techniques. |  |  |  |  |  |\n| --- | --- | --- | --- | --- |\n| **Algorithm** | **Best Val Acc.** | **Best Test Acc.** | **Total GPU min** | **Total $ cost** |\n| Grid Search | 74% | 65.4% | 45 min | $2.30 |\n| Bayesian Optimization +Early Stop | 77% | 66.9% | 104 min | $5.30 |\n| Population-based Training | 78% | 70.5% | 48 min | $2.45 |\n\nIf you’re leveraging [Transformers](https://github.com/huggingface/transformers), you’ll want to have a way to easily access powerful hyperparameter tuning solutions without giving up the customizability of the Transformers framework."", ""An intuition would be that these preference models need to have similar capacity to understand the text given to them as a model would need in order to generate said text. At this point in the RLHF system, we have an initial language model that can be used to generate text and a preference model that takes in any text and assigns it a score of how well humans perceive it. Next, we use \\*\\*reinforcement learning (RL)\\*\\* to optimize the original language model with respect to the reward model. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/reward-model.png)\n\n### Fine-tuning with RL\nTraining a language model with reinforcement learning was, for a long time, something that people would have thought as impossible both for engineering and algorithmic reasons. What multiple organizations seem to have gotten to work is fine-tuning some or all of the parameters of a \\*\\*copy of the initial LM\\*\\* with a policy-gradient RL algorithm, Proximal Policy Optimization (PPO). Some parameters of the LM are frozen because fine-tuning an entire 10B or 100B+ parameter model is prohibitively expensive (for more, see Low-Rank Adaptation ([LoRA](https://arxiv.org/abs/2106.09685)) for LMs or the [Sparrow](https://arxiv.org/abs/2209.14375) LM from DeepMind) -- depending on the scale of the model and infrastructure being used. The exact dynamics of how many parameters to freeze, or not, is considered an open research problem. PPO has been around for a relatively long time – there are [tons](https://spinningup.openai.com/en/latest/algorithms/ppo.html) of [guides](https://huggingface.co/blog/deep-rl-ppo) on how it works.""]","The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:

1. Splitting the embedding matrix into two smaller matrices.
2. Using repeating layers split among groups.",Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.
"What are the three main steps for fine-tuning a model with the 🤗 Datasets library?
","[""Considerations for model evaluation\nDeveloping an ML model is rarely a one-shot deal: it often involves multiple stages of defining the model architecture and tuning hyper-parameters before converging on a final set. Responsible model evaluation is a key part of this process, and 🤗 Evaluate is here to help! Here are some things to keep in mind when evaluating your model using the 🤗 Evaluate library:\n## Properly splitting your data\nGood evaluation generally requires three splits of your dataset:\n- \\*\\*train\\*\\*: this is used for training your model. - \\*\\*validation\\*\\*: this is used for validating the model hyperparameters. - \\*\\*test\\*\\*: this is used for evaluating your model. Many of the datasets on the 🤗 Hub are separated into 2 splits: `train` and `validation`; others are split into 3 splits (`train`, `validation` and `test`) -- make sure to use the right split for the right purpose! Some datasets on the 🤗 Hub are already separated into these three splits. However, there are also many that only have a train/validation or only train split. If the dataset you're using doesn't have a predefined train-test split, it is up to you to define which part of the dataset you want to use for training your model and which you want to use for hyperparameter tuning or final evaluation. Training and evaluating on the same split can misrepresent your results! If you overfit on your training data the evaluation results on that split will look great but the model will perform poorly on new data. Depending on the size of the dataset, you can keep anywhere from 10-30% for evaluation and the rest for training, while aiming to set up the test set to reflect the production data as close as possible. Check out [this thread](https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090) for a more in-depth discussion of dataset splitting! ## The impact of class imbalance\nWhile many academic datasets, such as the [IMDb dataset](https://huggingface.co/datasets/imdb) of movie reviews, are perfectly balanced, most real-world datasets are not. In machine learning a \\*balanced dataset\\* corresponds to a datasets where all labels are represented equally. In the case of the IMDb dataset this means that there are as many positive as negative reviews in the dataset. In an imbalanced dataset this is not the case: in fraud detection for example there are usually many more non-fraud cases than fraud cases in the dataset. Having an imbalanced dataset can skew the results of your metrics. Imagine a dataset with 99 \""non-fraud\"" cases and 1 \""fraud\"" case. A simple model that always predicts \""non-fraud\"" cases would give yield a 99% accuracy which might sound good at first until you realize that you will never catch a fraud case."", ""Introduction[[introduction]]\nIn [Chapter 3](/course/chapter3) you got your first taste of the 🤗 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics. But this is just scratching the surface of what 🤗 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\\* What do you do when your dataset is not on the Hub? \\* How can you slice and dice a dataset? (And what if you \\_really\\_ need to use Pandas?)\n\\* What do you do when your dataset is huge and will melt your laptop's RAM? \\* What the heck are \""memory mapping\"" and Apache Arrow? \\* How can you create your own dataset and push it to the Hub? The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!"", ""ow to slice and dice a dataset. Most of the time, the data you work with won’t be perfectly prepared for training models. In this video we’ll explore various features that Datasets provides to clean up your datasets. The Datasets library provides several built-in methods that allow you to wrangle your data. In this video we'll see how you can shuffle and split your data, select the rows you're interested in, tweak the columns, and apply processing functions with the map() method. Let's start with shuffling. It is generally a good idea to apply shuffling to the training set so that your model doesn't learn any artificial ordering in the data. If you want to shuffle the whole dataset, you can apply the appropriately named shuffle() method to your dataset. You can see an example of this method in action here, where we've downloaded the training split of the SQUAD dataset and shuffled all the rows randomly.Another way to shuffle the data is to create random train and test splits. This can be useful if you have to create your own test splits from raw data. To do this, you just apply the train\\_test\\_split method and specify how large the test split should be. In this example, we've specified that the test set should be 10% of the total dataset size. You can see that the output of train\\_test\\_split is a DatasetDict object, whose keys correspond to the new splits.""]","The three main steps for fine-tuning a model with the 🤗 Datasets library are:

1. Load a dataset from the Hugging Face Hub.
2. Preprocess the data with `Dataset.map()`.
3. Load and compute metrics.",1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.
"What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?
","[""It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark. Figure 4. Latency (Batch=1, Physical Cores=2)\n\n---\n## Conclusion\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput. Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens. The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial)\n## Resources\n\\* [Hugging Face Infinity](https://huggingface.co/infinity)\n\\* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\n\\* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i)\n\\* [DistilBERT](https://huggingface.co/docs/transformers/model\\_doc/distilbert)\n\\* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\\* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\n\\* [🤗 Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)"", ""It is important to highlight results provided here reflects out-of-the-box framework setup hence, they might not provide the absolute best performances. We decided to not include optimizations as part of this blog post to focus on hardware and efficiency. Optimizations will be discussed in the second part! 🚀\nThen, we covered and detailed the impact, and the importance of setting the thread affinity along with the trade-off between the target problem size, and the number of cores required for achieving the task. Also, it is important to define \\*\\*which criteria\\*\\* \\_(i.e. latency vs throughput)\\_ to use when optimizing your deployment as the resulting setups might be totally different. On a more general note, small problem sizes (\\_short sequences and/or small batches\\_) might require much fewer cores to achieve the best possible latency than big problems (\\_very long sequences and/or big batches\\_). It is interesting to cover all these aspects when thinking about the final deployment platform as it might cut the cost of the infrastructure drastically. For instance, our 48 cores machine charges \\*\\*4.848\\$/h\\*\\* whereas a smaller instances with only 8 cores lowers the cost to \\*\\*0.808\\$/h\\*\\*, leading to a \\*\\*6x cost reduction\\*\\*. Last but not least, many of the knobs discussed along this blog post can be automatically tuned through a [launcher script](https://github.com/huggingface/tune/blob/main/launcher.py)\nhighly inspired from the original script made by Intel and available [here](https://github.com/intel/intel-extension-for-pytorch/blob/master/intel\\_pytorch\\_extension\\_py/launch.py). The launcher script is able to automatically starts your python process(es) with the correct thread affinity, effectively\nsplitting resources across instances along with many other performances tips! We will detail many of this tips in the second part 🧐. In the follow-up blog post, more advanced settings and tuning techniques to decrease model latency even further will be involved, such as:\n- Launcher script walk-through\n- Tuning the memory allocation library\n- Using Linux's Transparent Huge Pages mechanisms\n- Using vendor-specific Math/Parallel libraries\nStay tuned! 🤗\n## Acknowledgments\n- [Omry Yadan](https://github.com/omry) (Facebook FAIR) - Author of [OmegaConf](https://github.com/omry/omegaconf) & [Hydra](https://github.com/facebookresearch/hydra) for all the tips setting up Hydra correctly. - All Intel & Intel Labs' NLP colleagues - For the ongoing optimizations and research efforts they are putting into transformers and more generally in the NLP field. - Hugging Face colleagues - For all the comments and improvements in the reviewing process. ## References\n1. [Benchmarking Transformers: PyTorch and TensorFlow](https://medium.com/huggingface/benchmarking-transformers-pytorch-and-tensorflow-e2917fb891c2)\n2. [HuggingFace's Transformers: State-of-the-art Natural Language Processing](https://arxiv.org/abs/1910.03771v2)\n3. [HuggingFace's Model Hub](https://huggingface.co/models)\n4. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin & al."", ""This took a while to pinpoint and either we went for 100% compliance and the model\nwas much slower, or we would take a small difference in generation\nbut was much faster to run and simpler code. We opted for a configurable flag. # First inference (PP + Accelerate)\n```\nNote: Pipeline Parallelism (PP) means in this context that each GPU will own\nsome layers so each GPU will work on a given chunk of data before handing\nit off to the next GPU. ```\nNow we have a workable `transformers` clean version of the start\nworking on running this. Bloom is a 352GB (176B parameters in bf16) model, we need at least that much\nGPU RAM to make it fit. We briefly explored offloading to CPU on smaller machines\nbut the inference speed was orders of magnitude slower so we discarded it. Then we wanted to basically use the [pipeline](https://huggingface.co/docs/transformers/v4.22.2/en/pipeline\\_tutorial#pipeline-usage). So it's dogfooding and this is what the API uses under the hood all the time. However `pipelines` are not distributed aware (it's not their goal). After briefly\ndiscussing options, we ended up using [accelerate](https://github.com/huggingface/accelerate/) newly\ncreated `device\\_map=\""auto\""` to manage the sharding of the model. We had to iron\nout a few bugs, and fix the `transformers` code a bit to help `accelerate` do the right job.""]",Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.,+800%
"What is the command to upload a spaCy pipeline to the Hugging Face Hub?
","[""Hugging Face Spaces are Git repositories, meaning that you can work on your Space incrementally (and collaboratively) by pushing commits. Take a look at the [Getting Started with Repositories](./repositories-getting-started) guide to learn about how you can create and edit files before continuing. ## Add the dependencies\nFor the \\*\\*Hot Dog Classifier\\*\\* we'll be using a [🤗 Transformers pipeline](https://huggingface.co/docs/transformers/pipeline\\_tutorial) to use the model, so we need to start by installing a few dependencies. This can be done by creating a \\*\\*requirements.txt\\*\\* file in our repository, and adding the following dependencies to it:\n```\ntransformers\ntorch\n```\nThe Spaces runtime will handle installing the dependencies! ## Create the Streamlit app\nTo create the Streamlit app, make a new file in the repository called \\*\\*app.py\\*\\*, and add the following code:\n```python\nimport streamlit as st\nfrom transformers import pipeline\nfrom PIL import Image\npipeline = pipeline(task=\""image-classification\"", model=\""julien-c/hotdog-not-hotdog\"")\nst.title(\""Hot Dog? Or Not?\"")\nfile\\_name = st.file\\_uploader(\""Upload a hot dog candidate image\"")\nif file\\_name is not None:\ncol1, col2 = st.columns(2)\nimage = Image.open(file\\_name)\ncol1.image(image, use\\_column\\_width=True)\npredictions = pipeline(image)\ncol2.header(\""Probabilities\"")\nfor p in predictions:\ncol2.subheader(f\""{ p['label'] }: { round(p['score'] \\* 100, 1)}%\"")\n```\nThis Python script uses a [🤗 Transformers pipeline](https://huggingface.co/docs/transformers/pipeline\\_tutorial) to load the [julien-c/hotdog-not-hotdog](https://huggingface.co/julien-c/hotdog-not-hotdog) model, which is used by the Streamlit interface."", ""![snippet](assets/23\\_spacy/snippet.png)\n![snippet](assets/23\\_spacy/snippet2.png)\nYou can even make HTTP requests to call the models from the Inference API, which is useful in production settings. Here is an example of a simple request:\n```bash\ncurl -X POST --data '{\""inputs\"": \""Hello, this is Omar\""}' https://api-inference.huggingface.co/models/spacy/en\\_core\\_web\\_sm\n>>> [{\""entity\\_group\"":\""PERSON\"",\""word\"":\""Omar\"",\""start\"":15,\""end\"":19,\""score\"":1.0}]\n```\nAnd for larger-scale use cases, you can click \""Deploy > Accelerated Inference\"" and see how to do this with Python. ### Sharing your models\nBut probably the coolest feature is that now you can very easily share your models with the `spacy-huggingface-hub` [library](https://github.com/explosion/spacy-huggingface-hub), which extends the `spaCy` CLI with a new command, `huggingface-hub push`. ```bash\nhuggingface-cli login\npython -m spacy package ./en\\_ner\\_fashion ./output --build wheel\ncd ./output/en\\_ner\\_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en\\_ner\\_fashion-0.0.0-py3-none-any.whl\n```\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community."", ""A few notes on this integration:\n- you will need to be logged in to the Hugging Face website locally for it to work, the easiest way to achieve this is to run `huggingface-cli login` and then type your username and password when prompted. You can also pass along your authentication token with the `--hub\\_token` argument. - the `output\\_dir` you pick will either need to be a new folder or a local clone of the distant repository you are using.""]","The command to upload a spaCy pipeline to the Hugging Face Hub is:

```bash
python -m spacy huggingface-hub push <your_model_package>.whl
```

Make sure to replace `<your_model_package>.whl` with the actual name of your packaged model file.",python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
"What is the time and memory complexity of the Nyströmformer's approximation of self-attention?
","[""--\ntitle: \""Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\""\nthumbnail: /blog/assets/86\\_nystromformer/thumbnail.png\nauthors:\n- user: asi\nguest: true\n---\n# Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\n## Introduction\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. The [Nyströmformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nyströmformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nyström method and how it can be adapted to approximate self-attention. ## Nyström method for matrix approximation\nAt the heart of Nyströmformer is the Nyström method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nyström method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:\n\nRepresenting P as a block matrix\n\nWe now have four submatrices: \\\\(A\\_P, B\\_P, F\\_P,\\\\) and \\\\(C\\_P\\\\), with sizes \\\\(m \\times m, m \\times (n - m), (n - m) \\times m\\\\) and\n\\\\((n - m) \\times (n - m)\\\\) respectively. The \\\\(m\\\\) sampled columns are contained in \\\\(A\\_P\\\\) and \\\\(F\\_P\\\\), whereas the \\\\(m\\\\) sampled rows are contained in \\\\(A\\_P\\\\) and \\\\(B\\_P\\\\). So, the entries of \\\\(A\\_P, B\\_P,\\\\) and \\\\(F\\_P\\\\) are known to us, and we will estimate \\\\(C\\_P\\\\). According to the Nyström method, \\\\(C\\_P\\\\) is given by:\n$$C\\_P = F\\_P A\\_P^+ B\\_P$$\nHere, \\\\(+\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). Thus, the Nyström approximation of \\\\(P, \\hat{P}\\\\) can be written as:\n\nNyström approximation of P\n\nAs shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Nyströmformer\n## Overview\nThe Nyströmformer model was proposed in [\\*Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention\\*](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn\nFung, Yin Li, and Vikas Singh."", ""Using Local self attention, the memory and time complexity of the query-key matmul operation can be reduced from\n\\\\(\\mathcal{O}(n\\_s \\times n\\_s)\\\\) to \\\\(\\mathcal{O}(n\\_s \\times \\log(n\\_s))\\\\), which usually represents the memory\nand time bottleneck in a transformer model, with \\\\(n\\_s\\\\) being the sequence length. ### Training\nDuring training, we must ensure that the sequence length is set to a value that can be divided by the least common\nmultiple of `config.lsh\\_chunk\\_length` and `config.local\\_chunk\\_length` and that the parameters of the Axial\nPositional Encodings are correctly set as described above. Reformer is very memory efficient so that the model can\neasily be trained on sequences as long as 64000 tokens. For training, the [`ReformerModelWithLMHead`] should be used as follows:\n```python\ninput\\_ids = tokenizer.encode(\""This is a sentence from the training data\"", return\\_tensors=\""pt\"")\nloss = model(input\\_ids, labels=input\\_ids)[0]\n```\n## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Causal language modeling task guide](../tasks/language\\_modeling)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n## ReformerConfig\n[[autodoc]] ReformerConfig\n## ReformerTokenizer\n[[autodoc]] ReformerTokenizer\n- save\\_vocabulary\n## ReformerTokenizerFast\n[[autodoc]] ReformerTokenizerFast\n## ReformerModel\n[[autodoc]] ReformerModel\n- forward\n## ReformerModelWithLMHead\n[[autodoc]] ReformerModelWithLMHead\n- forward\n## ReformerForMaskedLM\n[[autodoc]] ReformerForMaskedLM\n- forward\n## ReformerForSequenceClassification\n[[autodoc]] ReformerForSequenceClassification\n- forward\n## ReformerForQuestionAnswering\n[[autodoc]] ReformerForQuestionAnswering\n- forward""]","The Nyströmformer's approximation of self-attention has a time and memory complexity of \\(O(n)\\), which is an improvement over the standard self-attention mechanism that has a complexity of \\(O(n^2)\\).",O(n)
"What is the goal of the Named Entity Recognition task in token classification?
","[""Then we calculate it for every class and take the overall average to evaluate our model. An example dataset used for this task is ConLL2003. Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels. You can extract important information from invoices using named entity recognition models, such as date, organization name or address. For more information about the Token classification task, check out the Hugging Face course. Question Answering video\nWelcome to the Hugging Face tasks series. In this video, we will take a look at the Question Answering task. Question answering is the task of extracting an answer in a given document. Question answering models take a context, which is the document you want to search in, and a question and return an answer. Note that the answer is not generated, but extracted from the context. This type of question answering is called extractive. The task is evaluated on two metrics, exact match and F1-Score. As the name implies, exact match looks for an exact match between the predicted answer and the correct answer. A common metric used is the F1-Score, which is calculated over tokens that are predicted correctly and incorrectly."", ""et's study how to preprocess a dataset for token classification! Token classification regroups any task that can be framed as labelling each word (or token) in a sentence, like identifying the persons, organizations and locations for instance. For our example, we will use the Conll dataset, in which we remove columns we won't use and rename the other ones to get to a dataset with just two columns: words and labels. If you have your own dataset for token classification, just make sure you clean your data to get to the same point, with one column containing words (as list of strings) and another containing labels (as integers spanning from to to your number of labels -1).() Make sure you have your label names stored somewhere - here we get them from the dataset features - so you are able to map the integers to some real labels when inspecting your data! Here we are doing named entity recognitions, so ours labels are either O for words that do not belong to any entity, LOC, for location, PER, for person, ORG for organization and MISC for miscellaneous. Each label has two versions: the B- labels indicate a word that begins an entity while the I- labels indicate a word that is inside an entity."", ""The goal of this task is to classify each token in the input text into one of the following categories: person, organization, location, or none of them. - News Category Classification (NCC) on the Soham articles dataset from [IndicGLUE](https://aclanthology.org/2020.findings-emnlp.445/). The goal of this task is to predict the category to which belong the input text. We evaluated it during training on the NER task to check that everything was going well; as you can see on the following plot, this was indeed the case! Evaluation metrics of fine-tuned models on the NER task from different checkpoints of pre-trained models. At the end of training, we compared sahajBERT with three other pretrained language models: [XLM-R Large](https://arxiv.org/abs/1911.02116), [IndicBert](https://aclanthology.org/2020.findings-emnlp.445/), and [bnRoBERTa](https://huggingface.co/neuralspace-reverie/indic-transformers-bn-roberta). In the table below, you can see that our model has results comparable to the best Bengali language models available on HF Hub, even though our model has only ~18M trained parameters, while, for instance, XLM-R (a strong multilingual baseline), has ~559M parameters and was trained on several hundred V100 GPUs.""]","The goal of the Named Entity Recognition (NER) task in token classification is to classify each token in the input text into one of the following categories: person, organization, location, or none of them. This involves identifying and labeling words that correspond to specific named entities within the text.","The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization."
"What is the resolution of images used by the CLIPSeg model?
","[""For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm\\_source=hf&utm\\_medium=blog&utm\\_campaign=clipseg). Before we describe how to do that, let’s first take a look at how CLIPSeg works. ## CLIP: the magic model behind CLIPSeg\n[CLIP](https://huggingface.co/docs/transformers/main/en/model\\_doc/clip), which stands for \\*\\*C\\*\\*ontrastive \\*\\*L\\*\\*anguage–\\*\\*I\\*\\*mage \\*\\*P\\*\\*re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract \\*representation\\* of your input. This abstract representation, also called an \\*embedding\\*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together)."", ""Clark, Dan Garrette, Iulia Turc, John Wieting. 1. \\*\\*[Chinese-CLIP](https://huggingface.co/docs/transformers/model\\_doc/chinese\\_clip)\\*\\* (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou. 1. \\*\\*[CLAP](https://huggingface.co/docs/transformers/model\\_doc/clap)\\*\\* (from LAION-AI) released with the paper [Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation](https://arxiv.org/abs/2211.06687) by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov. 1. \\*\\*[CLIP](https://huggingface.co/docs/transformers/model\\_doc/clip)\\*\\* (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. 1. \\*\\*[CLIPSeg](https://huggingface.co/docs/transformers/model\\_doc/clipseg)\\*\\* (from University of Göttingen) released with the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke and Alexander Ecker."", ""For this model, the acceptable height\nand width are set to 224, and the number of channels is 3. You can verify\nthis by inspecting the config argument of the model (`model.config`). The model yields a 1000-d vector of `logits`. # Model Surgery\nUsually, every ML model has certain preprocessing and postprocessing\nsteps. The ViT model is no exception to this. The major preprocessing\nsteps include:\n- Scaling the image pixel values to [0, 1] range. - Normalizing the scaled pixel values to [-1, 1]. - Resizing the image so that it has a spatial resolution of (224, 224). You can confirm these by investigating the image processor associated\nwith the model:\n```py\nfrom transformers import AutoImageProcessor\nprocessor = AutoImageProcessor.from\\_pretrained(ckpt)\nprint(processor)\n```\nThis should print:\n```bash\nViTImageProcessor {\n\""do\\_normalize\"": true,\n\""do\\_resize\"": true,\n\""image\\_mean\"": [\n0.5,\n0.5,\n0.5\n],\n\""image\\_std\"": [\n0.5,\n0.5,\n0.5\n],\n\""resample\"": 2,\n\""size\"": 224\n}\n```\nSince this is an image classification model pre-trained on the\n[ImageNet-1k dataset](https://huggingface.co/datasets/imagenet-1k), the model\noutputs need to be mapped to the ImageNet-1k classes as the\npost-processing step.""]",The CLIPSeg model uses images with a resolution of 352 x 352 pixels.,352 x 352 pixels
"What can you use Gradio for?
","[""The Gradio Client is mostly used with apps hosted on [Hugging Face Spaces](https://hf.space), but your app can be hosted anywhere, such as your own server. \\*\\*Prequisites\\*\\*: To use the Gradio client, you do \\_not\\_ need to know the `gradio` library in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components. ## Installation\nThe lightweight `@gradio/client` package can be installed from the npm registry with a package manager of your choice and support node version 18 and above:\n```bash\nnpm i @gradio/client\n```\n## Connecting to a running Gradio App\nStart by connecting instantiating a `client` instance and connecting it to a Gradio app that is running on Hugging Face Spaces or generally anywhere on the web. ## Connecting to a Hugging Face Space\n```js\nimport { client } from \""@gradio/client\"";\nconst app = client(\""abidlabs/en2fr\""); // a Space that translates from English to French\n```\nYou can also connect to private Spaces by passing in your HF token with the `hf\\_token` property of the options parameter. You can get your HF token here: https://huggingface.co/settings/tokens\n```js\nimport { client } from \""@gradio/client\"";\nconst app = client(\""abidlabs/my-private-space\"", { hf\\_token=\""hf\\_...\"" })\n```\n## Duplicating a Space for private use\nWhile you can use any public Space as an API, you may get rate limited by Hugging Face if you make too many requests. For unlimited usage of a Space, simply duplicate the Space to create a private Space, and then use it to make as many requests as you'd like! The `@gradio/client` exports another function, `duplicate`, to make this process simple (you'll need to pass in your [Hugging Face token](https://huggingface.co/settings/tokens)). `duplicate` is almost identical to `client`, the only difference is under the hood:\n```js\nimport { client } from \""@gradio/client\"";\nconst response = await fetch(\n\""https://audio-samples.github.io/samples/mp3/blizzard\\_unconditional/sample-0.mp3\""\n);\nconst audio\\_file = await response.blob();\nconst app = await duplicate(\""abidlabs/whisper\"", { hf\\_token: \""hf\\_...\"" });\nconst transcription = app.predict(\""/predict\"", [audio\\_file]);\n```\nIf you have previously duplicated a Space, re-running `duplicate` will \\_not\\_ create a new Space."", ""Here's an example of how to use these attributes to create a Gradio app that does not lazy load and has an initial height of 0px. ```html\n```\nHere's another example of how to use the `render` event. An event listener is used to capture the `render` event and will call the `handleLoadComplete()` function once rendering is complete. ```html\n```\n\\_Note: While Gradio's CSS will never impact the embedding page, the embedding page can affect the style of the embedded Gradio app. Make sure that any CSS in the parent page isn't so general that it could also apply to the embedded Gradio app and cause the styling to break. Element selectors such as `header { ... }` and `footer { ... }` will be the most likely to cause issues.\\_\n### Embedding with IFrames\nTo embed with IFrames instead (if you cannot add javascript to your website, for example), add this element:\n```html\n```\nAgain, you can find the `src=` attribute to your Space's embed URL, which you can find in the \""Embed this Space\"" button. Note: if you use IFrames, you'll probably want to add a fixed `height` attribute and set `style=\""border:0;\""` to remove the boreder. In addition, if your app requires permissions such as access to the webcam or the microphone, you'll need to provide that as well using the `allow` attribute. ## API Page\nYou can use almost any Gradio app as an API! In the footer of a Gradio app [like this one](https://huggingface.co/spaces/gradio/hello\\_world), you'll see a \""Use via API\"" link. ![Use via API](https://github.com/gradio-app/gradio/blob/main/guides/assets/use\\_via\\_api.png?raw=true)\nThis is a page that lists the endpoints that can be used to query the Gradio app, via our supported clients: either [the Python client](https://gradio.app/guides/getting-started-with-the-python-client/), or [the JavaScript client](https://gradio.app/guides/getting-started-with-the-js-client/)."", ""Using Gradio for Tabular Data Science Workflows\nRelated spaces: https://huggingface.co/spaces/scikit-learn/gradio-skops-integration, https://huggingface.co/spaces/scikit-learn/tabular-playground, https://huggingface.co/spaces/merve/gradio-analysis-dashboard\n## Introduction\nTabular data science is the most widely used domain of machine learning, with problems ranging from customer segmentation to churn prediction. Throughout various stages of the tabular data science workflow, communicating your work to stakeholders or clients can be cumbersome; which prevents data scientists from focusing on what matters, such as data analysis and model building. Data scientists can end up spending hours building a dashboard that takes in dataframe and returning plots, or returning a prediction or plot of clusters in a dataset. In this guide, we'll go through how to use `gradio` to improve your data science workflows. We will also talk about how to use `gradio` and [skops](https://skops.readthedocs.io/en/stable/) to build interfaces with only one line of code! ### Prerequisites\nMake sure you have the `gradio` Python package already [installed](/getting\\_started). ## Let's Create a Simple Interface! We will take a look at how we can create a simple UI that predicts failures based on product information. ```python\nimport gradio as gr\nimport pandas as pd\nimport joblib\nimport datasets\ninputs = [gr.Dataframe(row\\_count = (2, \""dynamic\""), col\\_count=(4,\""dynamic\""), label=\""Input Data\"", interactive=1)]\noutputs = [gr.Dataframe(row\\_count = (2, \""dynamic\""), col\\_count=(1, \""fixed\""), label=\""Predictions\"", headers=[\""Failures\""])]\nmodel = joblib.load(\""model.pkl\"")\n# we will give our dataframe as example\ndf = datasets.load\\_dataset(\""merve/supersoaker-failures\"")\ndf = df[\""train\""].to\\_pandas()\ndef infer(input\\_dataframe):\nreturn pd.DataFrame(model.predict(input\\_dataframe))\ngr.Interface(fn = infer, inputs = inputs, outputs = outputs, examples = [[df.head(2)]]).launch()\n```\nLet's break down above code.""]","Gradio can be used for a variety of applications, particularly in the domain of machine learning and data science. Here are some key uses:

1. **Creating User Interfaces**: Gradio allows you to create simple user interfaces for machine learning models, enabling users to interact with models through web applications.

2. **Tabular Data Science Workflows**: It can improve workflows by allowing data scientists to build interfaces that take in dataframes and return predictions or visualizations with minimal code.

3. **Embedding Applications**: Gradio apps can be embedded in other web pages using IFrames, making it easy to integrate machine learning functionalities into existing websites.

4. **API Usage**: Almost any Gradio app can be used as an API, allowing developers to query the app programmatically.

5. **Rapid Prototyping**: Gradio facilitates quick prototyping of machine learning models, allowing for faster iteration and testing of ideas.

Overall, Gradio streamlines the process of building and sharing machine learning applications, making it accessible for both developers and non-developers.","Create a demo for your machine learning model, share your machine learning model with others, and debug your model."
"What TensorFlow API function is used to load a saved tensor file?
","[""### What is a SavedModel? A SavedModel contains a standalone TensorFlow model, including its weights and its architecture. It does not require the original source of the model to be run, which makes it useful for sharing or deploying with any backend that supports reading a SavedModel such as Java, Go, C++ or JavaScript among others. The internal structure of a SavedModel is represented as such:\n```\nsavedmodel\n/assets\n-> here the needed assets by the model (if any)\n/variables\n-> here the model checkpoints that contains the weights\nsaved\\_model.pb -> protobuf file representing the model graph\n```\n### How to install TensorFlow Serving? There are three ways to install and use TensorFlow Serving:\n- through a Docker container,\n- through an apt package,\n- or using [pip](https://pypi.org/project/pip/). To make things easier and compliant with all the existing OS, we will use Docker in this tutorial. ### How to create a SavedModel? SavedModel is the format expected by TensorFlow Serving. Since Transformers v4.2.0, creating a SavedModel has three additional features:\n1."", ""On CPU, if the file is already in cache, then it can\ntruly be zero-copy, whereas on GPU there is not such disk cache, so a copy is always required\nbut you can bypass allocating all the tensors on CPU at any given point. SafeTensors is not zero-copy for the header. The choice of JSON is pretty arbitrary, but since deserialization is <<< of the time required to load the actual tensor data and is readable I went that way, (also space is <<< to the tensor data). - Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the\nmoment. - Order: 'C' or row-major. This seems to have won. We can add that information later if needed. - Stride: No striding, all tensors need to be packed before being serialized. I have yet to see a case where it seems useful to have a strided tensor stored in serialized format. ### Benefits\nSince we can invent a new format we can propose additional benefits:\n- Prevent DOS attacks: We can craft the format in such a way that it's almost\nimpossible to use malicious files to DOS attack a user. Currently, there's a limit\non the size of the header of 100MB to prevent parsing extremely large JSON. Also when reading the file, there's a guarantee that addresses in the file\ndo not overlap in any way, meaning when you're loading a file you should never\nexceed the size of the file in memory\n- Faster load: PyTorch seems to be the fastest file to load out in the major\nML formats. However, it does seem to have an extra copy on CPU, which we\ncan bypass in this lib by using `torch.UntypedStorage.from\\_file`. Currently, CPU loading times are extremely fast with this lib compared to pickle. GPU loading times are as fast or faster than PyTorch equivalent. Loading first on CPU with memmapping with torch, and then moving all tensors to GPU seems\nto be faster too somehow (similar behavior in torch pickle)\n- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to\nload only part of the tensors on the various models. For\n[BLOOM](https://huggingface.co/bigscience/bloom) using this format enabled\nto load the model on 8 GPUs from 10mn with regular PyTorch weights down to 45s."", ""Before checkpointing tensors in your model, you want to ensure that all of your tensors are on CPU devices instead of XLA devices. This way, when you load back the tensors, you’ll load them through CPU devices and then have the opportunity to place them on whatever XLA devices you desire. We provide the `xm.save()` API for this, which already takes care of only writing to storage location from only one process on each host (or one globally if using a shared file system across hosts). ```python\nclass PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin):\n…\ndef save\\_pretrained(self, save\\_directory):\n... if getattr(self.config, \""xla\\_device\"", False):\nimport torch\\_xla.core.xla\\_model as xm\nif xm.is\\_master\\_ordinal():\n# Save configuration file\nmodel\\_to\\_save.config.save\\_pretrained(save\\_directory)\n# xm.save takes care of saving only from master\nxm.save(state\\_dict, output\\_model\\_file)\n```\n```python\nclass Trainer:\n…\ndef train(self, \\*args, \\*\\*kwargs):\n... if is\\_torch\\_tpu\\_available():\nxm.rendezvous(\""saving\\_optimizer\\_states\"")\nxm.save(self.optimizer.state\\_dict(),\nos.path.join(output\\_dir, \""optimizer.pt\""))\nxm.save(self.lr\\_scheduler.state\\_dict(),\nos.path.join(output\\_dir, \""scheduler.pt\""))\n```\n## PyTorch / XLA Library\nPyTorch / XLA is a Python package that uses the XLA linear algebra compiler to connect the PyTorch deep learning framework with XLA devices, which includes CPU, GPU, and Cloud TPUs.""]","The context does not provide specific information about a TensorFlow API function used to load a saved tensor file. It primarily discusses the SavedModel format and related features. For detailed information on TensorFlow API functions, please refer to the official TensorFlow documentation.",safetensors.tensorflow.load_file
"Where can you access the logs of your Endpoints in Hugging Face Endpoints?
","[""Access and view Metrics\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the “Analytics” tab of your Endpoints. ![metric dashboard](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png)\n## Access Metrics via API\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api\\_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints."", ""Hugging Face Inference Endpoints documentation\n## Setup\n```bash\npip install hf-doc-builder==0.4.0 watchdog --upgrade\n```\n## Local Development\n```bash\ndoc-builder preview endpoints docs/source/ --not\\_python\\_module\n```\n## Build Docs\n```bash\ndoc-builder build endpoints docs/source/ --build\\_dir build/ --not\\_python\\_module\n```\n## Add assets/Images\nAdding images/assets is only possible through `https://` links meaning you need to use `https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/` prefix. example\n```bash\n![Login](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/0_login.png)\n```\n## Generate API Reference\n1. Copy openapi spec from `https://api.endpoints.huggingface.cloud/api-doc/openapi.json`\n2. create markdown `widdershins --environment env.json openapi.json -o myOutput.md`\n3. copy into `api\\_reference.mdx`"", ""- \\*\\*Protected\\*\\*: A Protected Endpoint is available from the internet, secured with TLS/SSL, and requires a valid Hugging Face token for authentication. - \\*\\*Private\\*\\* A Private Endpoint is only available through an intra-region secured AWS or Azure PrivateLink connection. Private Endpoints are not accessible from the internet. Public and Protected Endpoints do not require any additional configuration. For Private Endpoints, you need to provide the AWS account ID of the account which also should have access to 🤗 Inference Endpoints. ![endpoint types](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/endpoint_types.png)""]",You can access the logs of your Endpoints in Hugging Face Endpoints through the UI on the detailed overview in the “Logs” tab of your Endpoints.,"In the ""Logs"" tab of your Endpoint through the UI."
"What is the latest task added to Hugging Face AutoTrain for Computer Vision?
","[""--\ntitle: Image Classification with AutoTrain\nthumbnail: /blog/assets/105\\_autotrain-image-classification/thumbnail.png\nauthors:\n- user: nimaboscarino\n---\n# Image Classification with AutoTrain\nSo you’ve heard all about the cool things that are happening in the machine learning world, and you want to join in. There’s just one problem – you don’t know how to code! 😱 Or maybe you’re a seasoned software engineer who wants to add some ML to your side-project, but you don’t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. That’s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature we’ve just added, we’re making “no-code” machine learning better than ever. Best of all, you can create your first project for ✨ free! ✨\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with \\*\\*zero\\*\\* configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer 🤯 We’ve been expanding the number of tasks that we support, and we’re proud to announce that \\*\\*you can now use AutoTrain for Computer Vision\\*\\*!"", ""Then it would be easier to analyze a specific feature. We used HuggingFace’s powerful APIs and AutoTrain along with Kili’s easy-to-use interface in this example. The modeling with AutoTrain just took 30 minutes, it chose the models and trained them for our use. AutoTrain is definitely much more efficient since I spent more time as I develop the model by myself. All the code, datasets, and scripts can be found in [github](https://github.com/alperiox/review-classification-kili-hf-automl). You can also try the [AutoTrain model](https://huggingface.co/alperiox/autonlp-user-review-classification-536415182). While we can consider this as a valid starting point, we should collect more data and try to build better pipelines. Better pipelines would result in more efficient improvements."", ""Other datasets try to overcome this issue by focusing on specific tasks or domains. For example, the [Howto100M](https://www.di.ens.fr/willow/research/howto100m/) dataset consists of 136M video clips with captions that describe how to perform complex tasks such as cooking, handcrafting, gardening, and fitness step-by-step. Similarly, the [QuerYD](https://www.robots.ox.ac.uk/~vgg/data/queryd/) dataset focuses on the event localization task such that the captions of videos describe the relative location of objects and actions in detail. [CelebV-Text](https://celebv-text.github.io/) is a large-scale facial text-video dataset of over 70K videos to generate videos with realistic faces, emotions, and gestures. ## Text-to-Video at Hugging Face\nUsing Hugging Face Diffusers, you can easily download, run and fine-tune various pretrained text-to-video models, including Text2Video-Zero and ModelScope by [Alibaba / DAMO Vision Intelligence Lab](https://huggingface.co/damo-vilab). We are currently working on integrating other exciting works into Diffusers and 🤗 Transformers.""]","The latest task added to Hugging Face AutoTrain for Computer Vision is the ability to train models with zero configuration needed. Users can now utilize AutoTrain for various computer vision tasks, making it easier to create machine learning models without extensive coding knowledge.",Image Classification
"What is the default repository type created by the `create_repo` function on Hugging Face Hub?
","[""Firstly, there are a few methods to manage repository creation, deletion, and others:\n```python no-format\nfrom huggingface\\_hub import (\n# User management\nlogin,\nlogout,\nwhoami,\n# Repository creation and management\ncreate\\_repo,\ndelete\\_repo,\nupdate\\_repo\\_visibility,\n# And some methods to retrieve/change information about the content\nlist\\_models,\nlist\\_datasets,\nlist\\_metrics,\nlist\\_repo\\_files,\nupload\\_file,\ndelete\\_file,\n)\n```\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them. The `create\\_repo` method can be used to create a new repository on the hub:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"")\n```\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"", organization=\""huggingface\"")\n```\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization."", ""The file name here should be `README.md` since it is what Hugging Face Hub expects. ```python\nmodel\\_card.save(Path(local\\_repo) / \""README.md\"")\n```\nWe can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub\\_utils`. Hugging Face Hub requires tokens for authentication, therefore you need to pass your token in either `notebook\\_login` if you're logging in from a notebook, or `huggingface-cli login` if you're logging in from the CLI. ```python\n# if the repository doesn't exist remotely on the Hugging Face Hub, it will be created when we set create\\_remote to True\nrepo\\_id = \""skops-user/my-awesome-model\""\nhub\\_utils.push(\nrepo\\_id=repo\\_id,\nsource=local\\_repo,\ntoken=token,\ncommit\\_message=\""pushing files to the repo from the example!\"",\ncreate\\_remote=True,\n)\n```\nOnce we push the model to the Hub, anyone can use it unless the repository is private. You can download the models using `download`. Apart from the model file, the repository contains the model configuration and the environment requirements. ```python\ndownload\\_repo = \""downloaded-model\""\nhub\\_utils.download(repo\\_id=repo\\_id, dst=download\\_repo)\n```\nThe inference widget is enabled to make predictions in the repository. ![Hosted Inference Widget](assets/94\\_skops/skops\\_widget.png)\nIf the requirements of your project have changed, you can use `update\\_env` to update the environment. ```python\nhub\\_utils.update\\_env(path=local\\_repo, requirements=[\""scikit-learn\""])\n```\nYou can see the example repository pushed with above code [here](https://huggingface.co/scikit-learn/skops-blog-example)."", ""Make sure you are logged into Hugging Face before you begin:\n```bash\nhuggingface-cli login\n```\nThen add the `push\\_to\\_hub` argument to the script. This argument will create a repository with your Hugging Face username and the folder name specified in `output\\_dir`. To give your repository a specific name, use the `push\\_to\\_hub\\_model\\_id` argument to add it. The repository will be automatically listed under your namespace. The following example shows how to upload a model with a specific repository name:\n```bash\npython examples/pytorch/summarization/run\\_summarization.py\n--model\\_name\\_or\\_path t5-small \\\n--do\\_train \\\n--do\\_eval \\\n--dataset\\_name cnn\\_dailymail \\\n--dataset\\_config \""3.0.0\"" \\\n--source\\_prefix \""summarize: \"" \\\n--push\\_to\\_hub \\\n--push\\_to\\_hub\\_model\\_id finetuned-t5-cnn\\_dailymail \\\n--output\\_dir /tmp/tst-summarization \\\n--per\\_device\\_train\\_batch\\_size=4 \\\n--per\\_device\\_eval\\_batch\\_size=4 \\\n--overwrite\\_output\\_dir \\\n--predict\\_with\\_generate\n```""]",The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository.,model
"How many splits does the ""duorc"" dataset have?
","[""It provides the number of rows, the number of colums (where applicable) and the size in bytes for the different forms of the data: original files, size in memory (RAM) and auto-converted parquet files. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has 187.213 rows along all its configurations and splits, for a total of 97MB. ```json\n{\n\""size\"": {\n\""dataset\"": {\n\""dataset\"": \""duorc\"",\n\""num\\_bytes\\_original\\_files\"": 97383710,\n\""num\\_bytes\\_parquet\\_files\"": 58710973,\n\""num\\_bytes\\_memory\"": 1059067116,\n\""num\\_rows\"": 187213\n},\n\""configs\"": [\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""num\\_bytes\\_original\\_files\"": 62921050,\n\""num\\_bytes\\_parquet\\_files\"": 37709127,\n\""num\\_bytes\\_memory\"": 718409158,\n\""num\\_rows\"": 100972,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""num\\_bytes\\_original\\_files\"": 34462660,\n\""num\\_bytes\\_parquet\\_files\"": 21001846,\n\""num\\_bytes\\_memory\"": 340657958,\n\""num\\_rows\"": 86241,\n\""num\\_columns\"": 7\n}\n],\n\""splits\"": [\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""train\"",\n\""num\\_bytes\\_parquet\\_files\"": 26005668,\n\""num\\_bytes\\_memory\"": 496682909,\n\""num\\_rows\"": 69524,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""validation\"",\n\""num\\_bytes\\_parquet\\_files\"": 5566868,\n\""num\\_bytes\\_memory\"": 106510489,\n\""num\\_rows\"": 15591,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""ParaphraseRC\"",\n\""split\"": \""test\"",\n\""num\\_bytes\\_parquet\\_files\"": 6136591,\n\""num\\_bytes\\_memory\"": 115215760,\n\""num\\_rows\"": 15857,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""train\"",\n\""num\\_bytes\\_parquet\\_files\"": 14851720,\n\""num\\_bytes\\_memory\"": 239852729,\n\""num\\_rows\"": 60721,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""validation\"",\n\""num\\_bytes\\_parquet\\_files\"": 3114390,\n\""num\\_bytes\\_memory\"": 51662519,\n\""num\\_rows\"": 12961,\n\""num\\_columns\"": 7\n},\n{\n\""dataset\"": \""duorc\"",\n\""config\"": \""SelfRC\"",\n\""split\"": \""test\"",\n\""num\\_bytes\\_parquet\\_files\"": 3035736,\n\""num\\_bytes\\_memory\"": 49142710,\n\""num\\_rows\"": 12559,\n\""num\\_columns\"": 7\n}\n]\n},\n\""pending\"": [],\n\""failed\"": [],\n\""partial\"": false\n}\n```\nIf the size has `partial: true` it means that the actual size of the dataset couldn't been determined because it's too big. In that case the number of rows and bytes can be inferior to the actual numbers."", ""Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package\\_reference/main\\_classes#datasets.DatasetInfo) object of the `datasets` library. ```json\n{\n\""dataset\\_info\"": {\n\""description\"": \""DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\n\"",\n\""citation\"": \""@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = {2018}\\n}\\n\"",\n\""homepage\"": \""https://duorc.github.io/\"",\n\""license\"": \""https://raw.githubusercontent.com/duorc/duorc/master/LICENSE\"",\n\""features\"": {\n\""plot\\_id\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""plot\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""title\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""question\\_id\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""question\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""answers\"": {\n\""feature\"": {\n\""dtype\"": \""string\"",\n\""\\_type\"": \""Value\""\n},\n\""\\_type\"": \""Sequence\""\n},\n\""no\\_answer\"": {\n\""dtype\"": \""bool\"",\n\""\\_type\"": \""Value\""\n}\n},\n\""builder\\_name\"": \""duorc\"",\n\""config\\_name\"": \""SelfRC\"",\n\""version\"": {\n\""version\\_str\"": \""1.0.0\"",\n\""major\"": 1,\n\""minor\"": 0,\n\""patch\"": 0\n},\n\""splits\"": {\n\""train\"": {\n\""name\"": \""train\"",\n\""num\\_bytes\"": 239852729,\n\""num\\_examples\"": 60721,\n\""dataset\\_name\"": \""duorc\""\n},\n\""validation\"": {\n\""name\"": \""validation\"",\n\""num\\_bytes\"": 51662519,\n\""num\\_examples\"": 12961,\n\""dataset\\_name\"": \""duorc\""\n},\n\""test\"": {\n\""name\"": \""test\"",\n\""num\\_bytes\"": 49142710,\n\""num\\_examples\"": 12559,\n\""dataset\\_name\"": \""duorc\""\n}\n},\n\""download\\_checksums\"": {\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_train.json\"": {\n\""num\\_bytes\"": 24388192,\n\""checksum\"": null\n},\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_dev.json\"": {\n\""num\\_bytes\"": 5051240,\n\""checksum\"": null\n},\n\""https://raw.githubusercontent.com/duorc/duorc/master/dataset/SelfRC\\_test.json\"": {\n\""num\\_bytes\"": 5023228,\n\""checksum\"": null\n}\n},\n\""download\\_size\"": 34462660,\n\""dataset\\_size\"": 340657958,\n\""size\\_in\\_bytes\"": 375120618\n}\n}\n```"", ""atasets and DataFrames equals love. Although the processing functions of Datasets will cover most the cases needed to train a model, there are times when you’ll need to switch to a library like Pandas to access more powerful features or high-level APIs for visualisation. Fortunately, Datasets is designed to be interoperable with libraries like Pandas, as well as NumPy, PyTorch, TensorFlow, and JAX. In this video, we'll take a look at how we can quickly switch our data to Pandas DataFrames and back. As an example, let's suppose we're analysing Supreme Court cases from Switzerland. As usual we download our dataset from the Hub using the load\\_dataset() function, and you can see that the first element of the training set is an ordinary Python dictionary with various fields of interest. Now suppose that before we train any models, we'd like to explore the data a bit. For example we might be interested in knowing which legal area is most common or we might want to know how the languages are distributed across regions. Answering these questions with the native Arrow format isn't easy, but we can easily switch to Pandas to get our answers!""]","The ""duorc"" dataset has three splits: train, validation, and test.",Six
"What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?
","[""--\ntitle: \""Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\""\nthumbnail: /blog/assets/62\\_pytorch\\_fsdp/fsdp-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\nIn this post we will look at how we can leverage \\*\\*[Accelerate](https://github.com/huggingface/accelerate)\\*\\* Library for training large models which enables users to leverage the latest features of \\*\\*[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)\\*\\*. # Motivation 🤗\n\\*\\*With the ever increasing scale, size and parameters of the Machine Learning (ML) models, ML practitioners are finding it difficult to train or even load such large models on their hardware.\\*\\* On one hand, it has been found that large models learn quickly (data and compute efficient) and are significantly more performant when compared to smaller models [1]; on the other hand, it becomes prohibitive to train such models on most of the available hardware."", ""This would result in the CPU RAM getting out of memory leading to processes being terminated. 2. Saving entire intermediate checkpoints using `FULL\\_STATE\\_DICT` with CPU offloading on rank 0 takes a lot of time and often results in NCCL Timeout errors due to indefinite hanging during broadcasting. However, at the end of training, we want the whole model state dict instead of the sharded state dict which is only compatible with FSDP. 3. We need to improve the speed and reduce the VRAM usage to train faster and save compute costs. Let’s look at how to solve the above challenges and fine-tune a 70B model! Before we get started, here's all the required resources to reproduce our results:\n1. Codebase:\nhttps://github.com/pacman100/DHS-LLM-Workshop/tree/main/chat\\_assistant/training with flash-attn V2 monkey patch\n2. FSDP config: https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat\\_assistant/training/configs/fsdp\\_config.yaml\n3. SLURM script `launch.slurm`: https://gist.github.com/pacman100/1cb1f17b2f1b3139a63b764263e70b25\n4. Model: `meta-llama/Llama-2-70b-chat-hf`\n5. Dataset: [smangrul/code-chat-assistant-v1](https://huggingface.co/datasets/smangrul/code-chat-assistant-v1) (mix of LIMA+GUANACO with proper formatting in a ready-to-train format)\n### Pre-requisites\nFirst follow these steps to install Flash Attention V2: Dao-AILab/flash-attention: Fast and memory-efficient exact attention (github.com)."", ""Megatron-DeepSpeed implements 3D Parallelism to allow huge models to train in a very efficient way. Let’s briefly discuss the 3D components. 1. \\*\\*DataParallel (DP)\\*\\* - the same setup is replicated multiple times, and each being fed a slice of the data. The processing is done in parallel and all setups are synchronized at the end of each training step. 2. \\*\\*TensorParallel (TP)\\*\\* - each tensor is split up into multiple chunks, so instead of having the whole tensor reside on a single GPU, each shard of the tensor resides on its designated GPU. During processing each shard gets processed separately and in parallel on different GPUs and the results are synced at the end of the step. This is what one may call horizontal parallelism, as the splitting happens on a horizontal level. 3. \\*\\*PipelineParallel (PP)\\*\\* - the model is split up vertically (layer-level) across multiple GPUs, so that only one or several layers of the model are placed on a single GPU. Each GPU processes in parallel different stages of the pipeline and works on a small chunk of the batch. 4. \\*\\*Zero Redundancy Optimizer (ZeRO)\\*\\* - also performs sharding of the tensors somewhat similar to TP, except the whole tensor gets reconstructed in time for a forward or backward computation, therefore the model doesn't need to be modified.""]","Fully Sharded Data Parallel (FSDP) is designed to address the challenges of training large machine learning models by efficiently distributing the model's parameters across multiple GPUs. Its purpose includes:

1. **Memory Efficiency**: FSDP shards the model's parameters, which allows each GPU to only hold a portion of the model, significantly reducing the memory footprint on each device.

2. **Improved Training Speed**: By distributing the model's parameters and computations, FSDP can accelerate the training process, enabling practitioners to train larger models more quickly.

3. **Scalability**: FSDP allows for scaling up the training of large models across multiple GPUs, making it feasible to work with models that would otherwise be too large to fit into the memory of a single GPU.

4. **Reduced VRAM Usage**: It helps in lowering the VRAM usage, which is crucial for training large models, thus saving compute costs.

Overall, FSDP enhances the ability to train large models efficiently and effectively in a distributed environment.","FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes."
"What file format is used to save and store PyTorch model weights more securely than `.bin` files?
","[""Convert weights to safetensors\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`. The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository. For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights. Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space."", ""Please help or contribute if you can 🔥. ### Improve `torch.load/save`\nThere's an open discussion in progress at PyTorch on having a [Safe way of loading only weights from \\*.pt file by default](https://github.com/pytorch/pytorch/issues/52181) – please chime in there! ### Hub’s Security Scanner\n#### What we have now\nWe have created a security scanner that scans every file pushed to the Hub and runs security checks. At the time of writing, it runs two types of scans:\n- ClamAV scans\n- Pickle Import scans\nFor ClamAV scans, files are run through the open-source antivirus [ClamAV](https://www.clamav.net). While this covers a good amount of dangerous files, it doesn’t cover pickle exploits. We have implemented a Pickle Import scan, which extracts the list of imports referenced in a pickle file. Every time you upload a `pytorch\\_model.bin` or any other pickled file, this scan is run. On the hub the list of imports will be displayed next to each file containing imports. If any import looks suspicious, it will be highlighted. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-pickle-imports.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-pickle-imports-dark.png)\n\nWe get this data thanks to [`pickletools.genops`](https://docs.python.org/3/library/pickletools.html#pickletools.genops) which allows us to read the file without executing potentially dangerous code."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Export to TorchScript\nThis is the very beginning of our experiments with TorchScript and we are still\nexploring its capabilities with variable-input-size models. It is a focus of interest to\nus and we will deepen our analysis in upcoming releases, with more code examples, a more\nflexible implementation, and benchmarks comparing Python-based codes with compiled\nTorchScript. According to the [TorchScript documentation](https://pytorch.org/docs/stable/jit.html):\n> TorchScript is a way to create serializable and optimizable models from PyTorch code. There are two PyTorch modules, [JIT and\nTRACE](https://pytorch.org/docs/stable/jit.html), that allow developers to export their\nmodels to be reused in other programs like efficiency-oriented C++ programs. We provide an interface that allows you to export 🤗 Transformers models to TorchScript\nso they can be reused in a different environment than PyTorch-based Python programs. Here, we explain how to export and use our models using TorchScript. Exporting a model requires two things:\n- model instantiation with the `torchscript` flag\n- a forward pass with dummy inputs\nThese necessities imply several things developers should be careful about as detailed\nbelow. ## TorchScript flag and tied weights\nThe `torchscript` flag is necessary because most of the 🤗 Transformers language models\nhave tied weights between their `Embedding` layer and their `Decoding` layer. TorchScript does not allow you to export models that have tied weights, so it is\nnecessary to untie and clone the weights beforehand. Models instantiated with the `torchscript` flag have their `Embedding` layer and\n`Decoding` layer separated, which means that they should not be trained down the line. Training would desynchronize the two layers, leading to unexpected results. This is not the case for models that do not have a language model head, as those do not\nhave tied weights. These models can be safely exported without the `torchscript` flag. ## Dummy inputs and standard lengths\nThe dummy inputs are used for a models forward pass.""]",The more secure file format used to save and store PyTorch model weights is `.safetensors`.,`.safetensors`
"What type of security certification does Hugging Face have?
","[""Security\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)"", ""Hugging Face Inference Endpoints documentation\n## Setup\n```bash\npip install hf-doc-builder==0.4.0 watchdog --upgrade\n```\n## Local Development\n```bash\ndoc-builder preview endpoints docs/source/ --not\\_python\\_module\n```\n## Build Docs\n```bash\ndoc-builder build endpoints docs/source/ --build\\_dir build/ --not\\_python\\_module\n```\n## Add assets/Images\nAdding images/assets is only possible through `https://` links meaning you need to use `https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/` prefix. example\n```bash\n![Login](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/0_login.png)\n```\n## Generate API Reference\n1. Copy openapi spec from `https://api.endpoints.huggingface.cloud/api-doc/openapi.json`\n2. create markdown `widdershins --environment env.json openapi.json -o myOutput.md`\n3. copy into `api\\_reference.mdx`"", ""The certification process\nThe certification process is \\*\\*completely free\\*\\*:\n- To get a \\*certificate of completion\\*: you need \\*\\*to pass 80% of the assignments\\*\\*. - To get a \\*certificate of excellence\\*: you need \\*\\*to pass 100% of the assignments\\*\\*. There's \\*\\*no deadlines, the course is self-paced\\*\\*. ![Course certification](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit0/certification.jpg)\nWhen we say pass, \\*\\*we mean that your model must be pushed to the Hub and get a result equal or above the minimal requirement\\*\\*. To check your progression and which unit you passed/not passed: https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course\nNow that you're ready for the certification process, you need to:\n1. Go here: https://huggingface.co/spaces/huggingface-projects/Deep-RL-Course-Certification/\n2. Type your \\*hugging face username\\*, your \\*first name\\*, \\*last name\\*\n3. Click on \""Generate my certificate\"". - If you passed 80% of the assignments, \\*\\*congratulations\\*\\* you've just got the certificate of completion. - If you passed 100% of the assignments, \\*\\*congratulations\\*\\* you've just got the excellence certificate. - If you are below 80%, don't be discouraged!""]","Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses.",SOC2 Type 2 certified
"What do RAG models combine to generate outputs?
","[""The prompts are passed through the initial language model to generate new text. Human annotators are used to rank the generated text outputs from the LM. One may initially think that humans should apply a scalar score directly to each piece of text in order to generate a reward model, but this is difficult to do in practice. The differing values of humans cause these scores to be uncalibrated and noisy. Instead, rankings are used to compare the outputs of multiple models and create a much better regularized dataset. There are multiple methods for ranking the text. One method that has been successful is to have users compare generated text from two language models conditioned on the same prompt. By comparing model outputs in head-to-head matchups, an [Elo](https://en.wikipedia.org/wiki/Elo\\_rating\\_system) system can be used to generate a ranking of the models and outputs relative to each-other. These different methods of ranking are normalized into a scalar reward signal for training. An interesting artifact of this process is that the successful RLHF systems to date have used reward language models with varying sizes relative to the text generation (e.g. OpenAI 175B LM, 6B reward model, Anthropic used LM and reward models from 10B to 52B, DeepMind uses 70B Chinchilla models for both LM and reward)."", ""To get those metrics, we'll need to generate outputs from the model and convert them to strings. Let's build some lists of labels and predictions for the ROUGE metric to compare (note that if you get import errors for this section, you may need to`!pip install tqdm`). We're also going to use a trick that dramatically increases performance - compiling our generation code with [XLA](https://www.tensorflow.org/xla), TensorFlow's accelerated linear algebra compiler. XLA applies various optimizations to the model's computation graph, and results in significant improvements to speed and memory usage. As described in the Hugging Face [blog](https://huggingface.co/blog/tf-xla-generate), XLA works best when our input shapes don't vary too much. To handle this, we'll pad our inputs to multiples of 128, and make a new dataset with the padding collator, and then we'll apply the `@tf.function(jit\\_compile=True)` decorator to our generation function, which marks the whole function for compilation with XLA. ```python\nfrom tqdm import tqdm\nimport numpy as np\ngeneration\\_data\\_collator = DataCollatorForSeq2Seq(\ntokenizer, model=model, return\\_tensors=\""tf\"", pad\\_to\\_multiple\\_of=320\n)\ntf\\_generate\\_dataset = model.prepare\\_tf\\_dataset(\ntokenized\\_datasets[\""validation\""],\ncollate\\_fn=generation\\_data\\_collator,\nshuffle=False,\nbatch\\_size=8,\ndrop\\_remainder=True,\n)\n@tf.function(jit\\_compile=True)\ndef generate\\_with\\_xla(batch):\nreturn model.generate(\ninput\\_ids=batch[\""input\\_ids\""],\nattention\\_mask=batch[\""attention\\_mask\""],\nmax\\_new\\_tokens=32,\n)\nall\\_preds = []\nall\\_labels = []\nfor batch, labels in tqdm(tf\\_generate\\_dataset):\npredictions = generate\\_with\\_xla(batch)\ndecoded\\_preds = tokenizer.batch\\_decode(predictions, skip\\_special\\_tokens=True)\nlabels = labels.numpy()\nlabels = np.where(labels != -100, labels, tokenizer.pad\\_token\\_id)\ndecoded\\_labels = tokenizer.batch\\_decode(labels, skip\\_special\\_tokens=True)\ndecoded\\_preds = [\""\\n\"".join(sent\\_tokenize(pred.strip())) for pred in decoded\\_preds]\ndecoded\\_labels = [\""\\n\"".join(sent\\_tokenize(label.strip())) for label in decoded\\_labels]\nall\\_preds.extend(decoded\\_preds)\nall\\_labels.extend(decoded\\_labels)\n```\nOnce we have our lists of label and prediction strings, computing the ROUGE score is easy:\n```python\nresult = rouge\\_score.compute(\npredictions=decoded\\_preds, references=decoded\\_labels, use\\_stemmer=True\n)\nresult = {key: value.mid.fmeasure \\* 100 for key, value in result.items()}\n{k: round(v, 4) for k, v in result.items()}\n```\n```\n{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}\n```\n{/if}\n{#if fw === 'pt'}\n## Fine-tuning mT5 with 🤗 Accelerate[[fine-tuning-mt5-with-accelerate]]\nFine-tuning our model with 🤗 Accelerate is very similar to the text classification example we encountered in [Chapter 3](/course/chapter3)."", ""These outputs are then passed to the decoder. The decoder uses the encoder's output alongside other inputs, in order to generate a prediction. It then predicts an output, which it will re-use in future iterations, hence the term \""auto-regressive\"". Finally, to get an understanding of the encoder-decoders as a whole, we recommend you check out the video on encoder-decoders.""]","RAG (Retrieval-Augmented Generation) models combine two main components to generate outputs: a retriever and a generator. The retriever fetches relevant documents or pieces of information from a large corpus based on the input query, while the generator uses this retrieved information to produce coherent and contextually relevant text outputs. This combination allows RAG models to leverage external knowledge effectively, enhancing the quality and relevance of the generated responses.",Pretrained dense retrieval (DPR) and sequence-to-sequence models.
"What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Feature Extractor\nA feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction from sequences, e.g., pre-processing audio files to generate Log-Mel Spectrogram features, feature extraction from images, e.g., cropping image files, but also padding, normalization, and conversion to NumPy, PyTorch, and TensorFlow tensors. ## FeatureExtractionMixin\n[[autodoc]] feature\\_extraction\\_utils.FeatureExtractionMixin\n- from\\_pretrained\n- save\\_pretrained\n## SequenceFeatureExtractor\n[[autodoc]] SequenceFeatureExtractor\n- pad\n## BatchFeature\n[[autodoc]] BatchFeature\n## ImageFeatureExtractionMixin\n[[autodoc]] image\\_utils.ImageFeatureExtractionMixin"", ""In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs). \\*\\*Use case 1: web page classification (training, inference) + token classification (inference), parse\\_html = True\\*\\*\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML. ```python\n>>> from transformers import MarkupLMProcessor\n>>> processor = MarkupLMProcessor.from\\_pretrained(\""microsoft/markuplm-base\"")\n>>> html\\_string = \""\""\""\n... ... ... ... Hello world\n... ... ... # Welcome\n\n... Here is my website. ... ... \""\""\""\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\n>>> encoding = processor(html\\_string, return\\_tensors=\""pt\"")\n>>> print(encoding.keys())\ndict\\_keys(['input\\_ids', 'token\\_type\\_ids', 'attention\\_mask', 'xpath\\_tags\\_seq', 'xpath\\_subs\\_seq'])\n```\n\\*\\*Use case 2: web page classification (training, inference) + token classification (inference), parse\\_html=False\\*\\*\nIn case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse\\_html` to `False`. ```python\n>>> from transformers import MarkupLMProcessor\n>>> processor = MarkupLMProcessor.from\\_pretrained(\""microsoft/markuplm-base\"")\n>>> processor.parse\\_html = False\n>>> nodes = [\""hello\"", \""world\"", \""how\"", \""are\""]\n>>> xpaths = [\""/html/body/div/li[1]/div/span\"", \""/html/body/div/li[1]/div/span\"", \""html/body\"", \""html/body/div\""]\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return\\_tensors=\""pt\"")\n>>> print(encoding.keys())\ndict\\_keys(['input\\_ids', 'token\\_type\\_ids', 'attention\\_mask', 'xpath\\_tags\\_seq', 'xpath\\_subs\\_seq'])\n```\n\\*\\*Use case 3: token classification (training), parse\\_html=False\\*\\*\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\ncorresponding node labels in order to train a model."", ""It is based on Facebook's\nRoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl\ndata. The abstract from the paper is the following:\n\\*This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a\nwide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred\nlanguages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly\noutperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on\nXNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on\nlow-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We\nalso present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the\ntrade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource\nlanguages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing\nper-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We\nwill make XLM-R code, data, and models publicly available.\\*\nThis model was contributed by [stefan-it](https://huggingface.co/stefan-it). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/xlmr). ## Usage tips\n- XLM-RoBERTa is a multilingual model trained on 100 different languages. Unlike some XLM multilingual models, it does\nnot require `lang` tensors to understand which language is used, and should be able to determine the correct\nlanguage from the input ids.""]",The MarkupLMFeatureExtractor uses the `beautifulsoup4` library to extract data from HTML and XML files.,Beautiful Soup
"What is the file size limit for syncing to HF Spaces without using Git-LFS?
","[""There are a few reasons for this:\n- Uploading and downloading smaller files is much easier both for you and the other users. Connection issues can always\nhappen when streaming data and smaller files avoid resuming from the beginning in case of errors. - Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\nleading to a slower download speed. In all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size. - \\*\\*Number of commits\\*\\*: There is no hard limit for the total number of commits on your repo history. However, from\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a\nfresh start using `huggingface\\_hub`'s [`super\\_squash\\_history`](https://huggingface.co/docs/huggingface\\_hub/main/en/package\\_reference/hf\\_api#huggingface\\_hub.HfApi.super\\_squash\\_history)."", ""This optimizes the startup time by having the files ready when your application starts. This is particularly useful for Spaces that rely on large models or datasets that would otherwise need to be downloaded at runtime. The format for each item is `\""repository\\_name\""` to download all files from a repository, or `\""repository\\_name file1,file2\""` for downloading specific files within that repository. You can also specify a specific commit to download using the format `\""repository\\_name file1,file2 commit\\_sha256\""`. Example usage:\n```yaml\npreload\\_from\\_hub:\n- warp-ai/wuerstchen-prior text\\_encoder/model.safetensors,prior/diffusion\\_pytorch\\_model.safetensors\n- coqui/XTTS-v1\n- gpt2 config.json 11c5a3d5811f50298f278a704980280950aedb10\n```\nIn this example, the Space will preload specific .safetensors files from `warp-ai/wuerstchen-prior`, the complete `coqui/XTTS-v1` repository, and a specific revision of the `config.json` file in the `gpt2` repository from the Hugging Face Hub during build time."", ""{/if}\n✏️ When creating the repository from the web interface, the \\*.gitattributes\\* file is automatically set up to consider files with certain extensions, such as \\*.bin\\* and \\*.h5\\*, as large files, and git-lfs will track them with no necessary setup on your side. We can now go ahead and proceed like we would usually do with traditional Git repositories. We can add all the files to Git's staging environment using the `git add` command:\n```bash\ngit add . ```\nWe can then have a look at the files that are currently staged:\n```bash\ngit status\n```\n{#if fw === 'pt'}\n```bash\nOn branch main\nYour branch is up to date with 'origin/main'. Changes to be committed:\n(use \""git restore --staged ...\"" to unstage)\nmodified: .gitattributes\nnew file: config.json\nnew file: pytorch\\_model.bin\nnew file: sentencepiece.bpe.model\nnew file: special\\_tokens\\_map.json\nnew file: tokenizer.json\nnew file: tokenizer\\_config.json\n```\n{:else}\n```bash\nOn branch main\nYour branch is up to date with 'origin/main'. Changes to be committed:\n(use \""git restore --staged ...\"" to unstage)\nmodified: .gitattributes\nnew file: config.json\nnew file: sentencepiece.bpe.model\nnew file: special\\_tokens\\_map.json\nnew file: tf\\_model.h5\nnew file: tokenizer.json\nnew file: tokenizer\\_config.json\n```\n{/if}\nSimilarly, we can make sure that git-lfs is tracking the correct files by using its `status` command:\n```bash\ngit lfs status\n```\n{#if fw === 'pt'}\n```bash\nOn branch main\nObjects to be pushed to origin/main:\nObjects to be committed:\nconfig.json (Git: bc20ff2)\npytorch\\_model.bin (LFS: 35686c2)\nsentencepiece.bpe.model (LFS: 988bc5a)\nspecial\\_tokens\\_map.json (Git: cb23931)\ntokenizer.json (Git: 851ff3e)\ntokenizer\\_config.json (Git: f0f7783)\nObjects not staged for commit:\n```\nWe can see that all files have `Git` as a handler, except \\*pytorch\\_model.bin\\* and \\*sentencepiece.bpe.model\\*, which have `LFS`. Great!""]",The file size limit for syncing to HF Spaces without using Git-LFS is 1GB.,10MB
"What is the title of the paper introducing the ByT5 model?
","[""## Conclusion\nSpeechT5 is an interesting model because — unlike most other models — it allows you to perform multiple tasks with the same architecture. Only the pre-nets and post-nets change. By pre-training the model on these combined tasks, it becomes more capable at doing each of the individual tasks when fine-tuned. We have only included checkpoints for the speech recognition (ASR), speech synthesis (TTS), and voice conversion tasks but the paper also mentions the model was successfully used for speech translation, speech enhancement, and speaker identification. It’s very versatile!"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# SpeechT5\n## Overview\nThe SpeechT5 model was proposed in [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei. The abstract from the paper is the following:\n\\*Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder. Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder. Extensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\\*\nThis model was contributed by [Matthijs](https://huggingface.co/Matthijs). The original code can be found [here](https://github.com/microsoft/SpeechT5). ## SpeechT5Config\n[[autodoc]] SpeechT5Config\n## SpeechT5HifiGanConfig\n[[autodoc]] SpeechT5HifiGanConfig\n## SpeechT5Tokenizer\n[[autodoc]] SpeechT5Tokenizer\n- \\_\\_call\\_\\_\n- save\\_vocabulary\n- decode\n- batch\\_decode\n## SpeechT5FeatureExtractor\n[[autodoc]] SpeechT5FeatureExtractor\n- \\_\\_call\\_\\_\n## SpeechT5Processor\n[[autodoc]] SpeechT5Processor\n- \\_\\_call\\_\\_\n- pad\n- from\\_pretrained\n- save\\_pretrained\n- batch\\_decode\n- decode\n## SpeechT5Model\n[[autodoc]] SpeechT5Model\n- forward\n## SpeechT5ForSpeechToText\n[[autodoc]] SpeechT5ForSpeechToText\n- forward\n## SpeechT5ForTextToSpeech\n[[autodoc]] SpeechT5ForTextToSpeech\n- forward\n- generate\n## SpeechT5ForSpeechToSpeech\n[[autodoc]] SpeechT5ForSpeechToSpeech\n- forward\n- generate\\_speech\n## SpeechT5HifiGan\n[[autodoc]] SpeechT5HifiGan\n- forward"", ""This model was contributed by [agemagician](https://huggingface.co/agemagician) and [stefan-it](https://huggingface.co/stefan-it). The original code can be\nfound [here](https://github.com/google-research/t5x). ## Usage tips\n- UMT5 was only pre-trained on [mC4](https://huggingface.co/datasets/mc4) excluding any supervised training. Therefore, this model has to be fine-tuned before it is usable on a downstream task, unlike the original T5 model. - Since umT5 was pre-trained in an unsupervise manner, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix. ## Differences with mT5? `UmT5` is based on mT5, with a non-shared relative positional bias that is computed for each layer. This means that the model set `has\\_relative\\_bias` for each layer. The conversion script is also different because the model was saved in t5x's latest checkpointing format. # Sample usage\n```python\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n>>> model = AutoModelForSeq2SeqLM.from\\_pretrained(\""google/umt5-small\"")\n>>> tokenizer = AutoTokenizer.from\\_pretrained(\""google/umt5-small\"")\n>>> inputs = tokenizer(\n... \""A  walks into a bar and orders a  with  pinch of .\"",\n... return\\_tensors=\""pt\"",\n... )\n>>> outputs = model.generate(\\*\\*inputs)\n>>> print(tokenizer.batch\\_decode(outputs))\n['nyone who drink a alcohol A A. This I']\n```\nRefer to [T5's documentation page](t5) for more tips, code examples and notebooks.""]","I'm sorry, but I can only provide information related to HuggingFace and gradio-app resources.",ByT5: Towards a token-free future with pre-trained byte-to-byte models
"What is the dimension of the feature vector for the base BERT model?
","[""## L\n### labels\nThe labels are an optional argument which can be passed in order for the model to compute the loss itself. These labels\nshould be the expected prediction of the model: it will use the standard loss in order to compute the loss between its\npredictions and the expected value (the label). These labels are different according to the model head, for example:\n- For sequence classification models, ([`BertForSequenceClassification`]), the model expects a tensor of dimension\n`(batch\\_size)` with each value of the batch corresponding to the expected label of the entire sequence. - For token classification models, ([`BertForTokenClassification`]), the model expects a tensor of dimension\n`(batch\\_size, seq\\_length)` with each value corresponding to the expected label of each individual token. - For masked language modeling, ([`BertForMaskedLM`]), the model expects a tensor of dimension `(batch\\_size,\nseq\\_length)` with each value corresponding to the expected label of each individual token: the labels being the token\nID for the masked token, and values to be ignored for the rest (usually -100). - For sequence to sequence tasks, ([`BartForConditionalGeneration`], [`MBartForConditionalGeneration`]), the model\nexpects a tensor of dimension `(batch\\_size, tgt\\_seq\\_length)` with each value corresponding to the target sequences\nassociated with each input sequence."", ""This way, we only have a 32\\*32 = 1024-long\nsequence, but now of integers in the range 0..511. So we are shrinking the sequence length at the cost of a bigger\nembedding matrix. In other words, the vocabulary size of ImageGPT is 512, + 1 for a special \""start of sentence\"" (SOS)\ntoken, used at the beginning of every sequence. One can use [`ImageGPTImageProcessor`] to prepare\nimages for the model. - Despite being pre-trained entirely unsupervised (i.e. without the use of any labels), ImageGPT produces fairly\nperformant image features useful for downstream tasks, such as image classification. The authors showed that the\nfeatures in the middle of the network are the most performant, and can be used as-is to train a linear model (such as\na sklearn logistic regression model for example). This is also referred to as \""linear probing\"". Features can be\neasily obtained by first forwarding the image through the model, then specifying `output\\_hidden\\_states=True`, and\nthen average-pool the hidden states at whatever layer you like. - Alternatively, one can further fine-tune the entire model on a downstream dataset, similar to BERT. For this, you can\nuse [`ImageGPTForImageClassification`]. - ImageGPT comes in different sizes: there's ImageGPT-small, ImageGPT-medium and ImageGPT-large. The authors did also\ntrain an XL variant, which they didn't release. The differences in size are summarized in the following table:\n| \\*\\*Model variant\\*\\* | \\*\\*Depths\\*\\* | \\*\\*Hidden sizes\\*\\* | \\*\\*Decoder hidden size\\*\\* | \\*\\*Params (M)\\*\\* | \\*\\*ImageNet-1k Top 1\\*\\* |\n|---|---|---|---|---|---|\n| MiT-b0 | [2, 2, 2, 2] | [32, 64, 160, 256] | 256 | 3.7 | 70.5 |\n| MiT-b1 | [2, 2, 2, 2] | [64, 128, 320, 512] | 256 | 14.0 | 78.7 |\n| MiT-b2 | [3, 4, 6, 3] | [64, 128, 320, 512] | 768 | 25.4 | 81.6 |\n| MiT-b3 | [3, 4, 18, 3] | [64, 128, 320, 512] | 768 | 45.2 | 83.1 |\n| MiT-b4 | [3, 8, 27, 3] | [64, 128, 320, 512] | 768 | 62.6 | 83.6 |\n| MiT-b5 | [3, 6, 40, 3] | [64, 128, 320, 512] | 768 | 82.0 | 83.8 |\n## Resources\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ImageGPT."", ""Metric Card for BERT Score\n## Metric description\nBERTScore is an automatic evaluation metric for text generation that computes a similarity score for each token in the candidate sentence with each token in the reference sentence. It leverages the pre-trained contextual embeddings from [BERT](https://huggingface.co/bert-base-uncased) models and matches words in candidate and reference sentences by cosine similarity. Moreover, BERTScore computes precision, recall, and F1 measure, which can be useful for evaluating different language generation tasks. ## How to use\nBERTScore takes 3 mandatory arguments : `predictions` (a list of string of candidate sentences), `references` (a list of strings or list of list of strings of reference sentences) and either `lang` (a string of two letters indicating the language of the sentences, in [ISO 639-1 format](https://en.wikipedia.org/wiki/List\\_of\\_ISO\\_639-1\\_codes)) or `model\\_type` (a string specififying which model to use, according to the BERT specification). The default behavior of the metric is to use the suggested model for the target language when one is specified, otherwise to use the `model\\_type` indicated. ```python\nfrom datasets import load\\_metric\nbertscore = load\\_metric(\""bertscore\"")\npredictions = [\""hello there\"", \""general kenobi\""]\nreferences = [\""hello there\"", \""general kenobi\""]\nresults = bertscore.compute(predictions=predictions, references=references, lang=\""en\"")\n```\nBERTScore also accepts multiple optional arguments:\n`num\\_layers` (int): The layer of representation to use.""]","The dimension of the feature vector for the base BERT model, specifically `BertForSequenceClassification`, is typically 768. This is the hidden size of the model, which corresponds to the size of the output embeddings for each token in the input sequence.",768
"What special identifier does the WordPiece Model use for continuing subwords?
","[""et's take a look at word-based tokenization. Word-based tokenization is the idea of splitting the raw text into words, by splitting on spaces or other specific rules like punctuation. In this algorithm, each word has a specific number, an \""ID\"", attributed to it. In this example, \""Let's\"" has the ID 250, do has ID 861, and tokenization followed by an exclamation point has the ID 345. This approach is interesting, as the model has representations that are based on entire words. The information held in a single number is high as a word contains a lot of contextual and semantic information in a sentence. However, this approach does have its limits. For example, the word dog and the word dogs are very similar, and their meaning is close. However, the word-based tokenization will attribute entirely different IDs to these two words, and the model will therefore learn different meanings for these two words. This is unfortunate, as we would like the model to understand that these words are indeed related and that dogs is the plural form of the word dog."", ""WordPiece tokenization[[wordpiece-tokenization]]\nWordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It's very similar to BPE in terms of the training, but the actual tokenization is done differently. 💡 This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm. ## Training algorithm[[training-algorithm]]\n⚠️ Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate. Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\""word\""` gets split like this:\n```\nw ##o ##r ##d\n```\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix. Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\n$$\\mathrm{score} = (\\mathrm{freq\\\\_of\\\\_pair}) / (\\mathrm{freq\\\\_of\\\\_first\\\\_element} \\times \\mathrm{freq\\\\_of\\\\_second\\\\_element})$$\nBy dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary."", ""In turn, the model will now be able to make sense of token in different situations. It will understand that the words token, tokens, tokenizing, and tokenization are linked and have a similar meaning. It will also understand that tokenization, modernization, and immunization, which all have the same suffixes, are probably used in the same syntactic situations. Subword-based tokenizers generally have a way to identify which tokens are start of words, and which tokens complete start of words: token as the start of a word. ##ization as completing a word. Here the ## prefix indicates that ization is part of a word rather than the beginning of it. The ## comes from the BERT tokenizer, based on the WordPiece algorithm. Other tokenizers use other prefixes, which can be placed to indicate part of words like seen here, or start of words instead! There are a lot of different algorithms that can be used for subword tokenization, and most models obtaining state-of-the-art results in English today use some kind of subword-tokenization algorithm.""]","The WordPiece model uses the prefix `##` to indicate that a subword is part of a word rather than the beginning of it. For example, in the tokenization of the word ""word,"" it would be split as `w ##o ##r ##d`, where `##o`, `##r`, and `##d` indicate that these characters are part of the word ""word.""",##
"What is the purpose of the 🧨 Diffusers tutorials?
","[""Research projects\nThis folder contains various research projects using 🧨 Diffusers. They are not really maintained by the core maintainers of this library and often require a specific version of Diffusers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice. If you need help with any of those, please open an issue where you directly ping the author(s), as indicated at the top of the README of each folder."", ""# Diffusers examples with ONNXRuntime optimizations\n\\*\\*This research project is not actively maintained by the diffusers team. For any questions or comments, please contact Prathik Rao (prathikr), Sunghoon Choi (hanbitmyths), Ashwini Khade (askhade), or Peng Wang (pengwa) on github with any questions.\\*\\*\nThis aims to provide diffusers examples with ONNXRuntime optimizations for training/fine-tuning unconditional image generation, text to image, and textual inversion. Please see individual directories for more details on how to run each task using ONNXRuntime."", ""--\ntitle: \""Happy 1st anniversary 🤗 Diffusers!\""\nthumbnail: /blog/assets/diffusers-turns-1/diffusers-turns-1.png\nauthors:\n- user: stevhliu\n- user: sayakpaul\n- user: pcuenq\n---\n# Happy 1st anniversary 🤗 Diffusers! 🤗 Diffusers is happy to celebrate its first anniversary! It has been an exciting year, and we're proud and grateful for how far we've come thanks to our community and open-source contributors. Last year, text-to-image models like DALL-E 2, Imagen, and Stable Diffusion captured the world's attention with their ability to generate stunningly photorealistic images from text, sparking a massive surge of interest and development in generative AI. But access to these powerful models was limited. At Hugging Face, our mission is to democratize good machine learning by collaborating and helping each other build an open and ethical AI future together. Our mission motivated us to create the 🤗 Diffusers library so \\*everyone\\* can experiment, research, or simply play with text-to-image models. That’s why we designed the library as a modular toolbox, so you can customize a diffusion model’s components or just start using it out-of-the-box. As 🤗 Diffusers turns 1, here’s an overview of some of the most notable features we’ve added to the library with the help of our community. We are proud and immensely grateful for being part of an engaged community that promotes accessible usage, pushes diffusion models beyond just text-to-image generation, and is an all-around inspiration. \\*\\*Table of Contents\\*\\*\n\\* [Striving for photorealism](#striving-for-photorealism)\n\\* [Video pipelines](#video-pipelines)\n\\* [Text-to-3D models](#text-to-3d-models)\n\\* [Image editing pipelines](#image-editing-pipelines)\n\\* [Faster diffusion models](#faster-diffusion-models)\n\\* [Ethics and safety](#ethics-and-safety)\n\\* [Support for LoRA](#support-for-lora)\n\\* [Torch 2.0 optimizations](#torch-20-optimizations)\n\\* [Community highlights](#community-highlights)\n\\* [Building products with 🤗 Diffusers](#building-products-with-🤗-diffusers)\n\\* [Looking forward](#looking-forward)\n## Striving for photorealism\nGenerative AI models are known for creating photorealistic images, but if you look closely, you may notice certain things that don't look right, like generating extra fingers on a hand.""]","The purpose of the 🧨 Diffusers tutorials is to provide guidance and examples for using the 🤗 Diffusers library, which is designed to facilitate experimentation, research, and usage of diffusion models for tasks such as text-to-image generation. The tutorials help users understand how to effectively utilize the library's features and components, enabling them to customize and implement diffusion models for various applications.",To provide a gentle introduction to diffusion models and help understand the library fundamentals.
"What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?
","[""For example, on Hugging Face\nSpaces, developers typically don't have access to the underlying ephemeral machine hosting the Gradio\ndemo. That's why, by default, flagging is turned off in Hugging Face Space. However,\nyou may want to do something else with the flagged data. We've made this super easy with the `flagging\\_callback` parameter. For example, below we're going to pipe flagged data from our calculator example into a Hugging Face Dataset, e.g. so that we can build a \""crowd-sourced\"" dataset:\n```python\nimport os\nHF\\_TOKEN = os.getenv('HF\\_TOKEN')\nhf\\_writer = gr.HuggingFaceDatasetSaver(HF\\_TOKEN, \""crowdsourced-calculator-demo\"")\niface = gr.Interface(\ncalculator,\n[\""number\"", gr.Radio([\""add\"", \""subtract\"", \""multiply\"", \""divide\""]), \""number\""],\n\""number\"",\ndescription=\""Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\"",\nallow\\_flagging=\""manual\"",\nflagging\\_options=[\""wrong sign\"", \""off by one\"", \""other\""],\nflagging\\_callback=hf\\_writer\n)\niface.launch()\n```\nNotice that we define our own\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow\\_flagging=\""manual\""`\nbecause on Hugging Face Spaces, `allow\\_flagging` is set to `\""never\""` by default. Here's our demo:\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo). ![flagging callback hf](https://github.com/gradio-app/gradio/blob/main/guides/assets/flagging-callback-hf.png?raw=true)\nWe created the `gradio.HuggingFaceDatasetSaver` class, but you can pass your own custom class as long as it inherits from `FLaggingCallback` defined in [this file](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py). If you create a cool callback, contribute it to the repo! ## Flagging with Blocks\nWhat about if you are using `gradio.Blocks`? On one hand, you have even more flexibility\nwith Blocks -- you can write whatever Python code you want to run when a button is clicked,\nand assign that using the built-in events in Blocks."", ""By default, the Gradio use UTF-8 encoding for scripts. \\*\\*For reload mode\\*\\*, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n1. Configure encoding declaration of python script, for example: `# -\\*- coding: cp1252 -\\*-`\n2. Confirm that your code editor has identified that encoding format. 3. Run like this: `gradio run.py --encoding cp1252`\n🔥 If your application accepts command line arguments, you can pass them in as well. Here's an example:\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# Greetings {args.name}!\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nWhich you could run like this: `gradio run.py --name Gretel`\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code."", ""Advanced Interface features[[advanced-interface-features]]\nNow that we can build and share a basic interface, let's explore some more advanced features such as state, and interpretation. ### Using state to persist data[[using-state-to-persist-data]]\nGradio supports \\*session state\\*, where data persists across multiple submits within a\npage load. Session state is useful for building demos of, for example, chatbots where you want to\npersist data as the user interacts with the model. Note that session state does not share data between different users of your model. To store data in a session state, you need to do three things:\n1. Pass in an \\*extra parameter\\* into your function, which represents the state of the interface. 1. At the end of the function, return the updated value of the state as an \\*extra return value\\*. 1. Add the 'state' input and 'state' output components when creating your `Interface`. See the chatbot example below:\n```py\nimport random\nimport gradio as gr\ndef chat(message, history):\nhistory = history or []\nif message.startswith(\""How many\""):\nresponse = random.randint(1, 10)\nelif message.startswith(\""How\""):\nresponse = random.choice([\""Great\"", \""Good\"", \""Okay\"", \""Bad\""])\nelif message.startswith(\""Where\""):\nresponse = random.choice([\""Here\"", \""There\"", \""Somewhere\""])\nelse:\nresponse = \""I don't know\""\nhistory.append((message, response))\nreturn history, history\niface = gr.Interface(\nchat,\n[\""text\"", \""state\""],\n[\""chatbot\"", \""state\""],\nallow\\_screenshot=False,\nallow\\_flagging=\""never\"",\n)\niface.launch()\n```\nNotice how the state of the output component persists across submits.""]","The default setting for the `allow_flagging` parameter in Gradio's `Interface` is set to `""never""` when used in Hugging Face Spaces.","""manual"""
"Where can the full code for the Stable Diffusion demo be found?
","[""See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main"", ""ote: This is a simplified version of the code needed to create the Stable Diffusion demo."", ""--\ntitle: Using Stable Diffusion with Core ML on Apple Silicon\nthumbnail: /blog/assets/diffusers\\_coreml/thumbnail.png\nauthors:\n- user: pcuenq\n---\n# Using Stable Diffusion with Core ML on Apple Silicon\nThanks to Apple engineers, you can now run Stable Diffusion on Apple Silicon using Core ML! [This Apple repo](https://github.com/apple/ml-stable-diffusion) provides conversion scripts and inference code based on [🧨 Diffusers](https://github.com/huggingface/diffusers), and we love it! To make it as easy as possible for you, we converted the weights ourselves and put the Core ML versions of the models in [the Hugging Face Hub](https://hf.co/apple). \\*\\*Update\\*\\*: some weeks after this post was written we created a native Swift app that you can use to run Stable Diffusion effortlessly on your own hardware. We released [an app in the Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) as well as [the source code to allow other projects to use it](https://github.com/huggingface/swift-coreml-diffusers). The rest of this post guides you on how to use the converted weights in your own code or convert additional weights yourself. ## Available Checkpoints\nThe official Stable Diffusion checkpoints are already converted and ready for use:\n- Stable Diffusion v1.4: [converted](https://hf.co/apple/coreml-stable-diffusion-v1-4) [original](https://hf.co/CompVis/stable-diffusion-v1-4)\n- Stable Diffusion v1.5: [converted](https://hf.co/apple/coreml-stable-diffusion-v1-5) [original](https://hf.co/runwayml/stable-diffusion-v1-5)\n- Stable Diffusion v2 base: [converted](https://hf.co/apple/coreml-stable-diffusion-2-base) [original](https://huggingface.co/stabilityai/stable-diffusion-2-base)\n- Stable Diffusion v2.1 base: [converted](https://hf.co/apple/coreml-stable-diffusion-2-1-base) [original](https://huggingface.co/stabilityai/stable-diffusion-2-1-base)\nCore ML supports all the compute units available in your device: CPU, GPU and Apple's Neural Engine (NE).""]",The full code for the Stable Diffusion demo can be found at this URL: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main.,https://hf.co/spaces/stabilityai/stable-diffusion/tree/main
"What transformation does the FNet model use to replace the self-attention layer in a BERT model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# FNet\n## Overview\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:\n\\*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \""mix\"" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \""efficient\"" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.\\*\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f\\_net). ## Usage tips\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\nsequence length for fine-tuning and inference. ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## FNetConfig\n[[autodoc]] FNetConfig\n## FNetTokenizer\n[[autodoc]] FNetTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## FNetTokenizerFast\n[[autodoc]] FNetTokenizerFast\n## FNetModel\n[[autodoc]] FNetModel\n- forward\n## FNetForPreTraining\n[[autodoc]] FNetForPreTraining\n- forward\n## FNetForMaskedLM\n[[autodoc]] FNetForMaskedLM\n- forward\n## FNetForNextSentencePrediction\n[[autodoc]] FNetForNextSentencePrediction\n- forward\n## FNetForSequenceClassification\n[[autodoc]] FNetForSequenceClassification\n- forward\n## FNetForMultipleChoice\n[[autodoc]] FNetForMultipleChoice\n- forward\n## FNetForTokenClassification\n[[autodoc]] FNetForTokenClassification\n- forward\n## FNetForQuestionAnswering\n[[autodoc]] FNetForQuestionAnswering\n- forward"", ""Thus, directly teaching the model about the English language (and the words we use). Isn’t that neat? Play around with BERT’s masking predictions:\n\nHosted inference API\n\n[Fill-Mask](/tasks/fill-mask \""Learn more about fill-mask\"")\n\nExamples\n\nMask token:\n`[MASK]`\n\nCompute\n\nThis model can be loaded on the Inference API on-demand. JSON Output\n\nMaximize\n\n\\*\\*Fun Fact:\\*\\* Masking has been around a long time - [1953 Paper on Cloze procedure (or ‘Masking’)](https://psycnet.apa.org/record/1955-00850-001). ### 2.3 What is Next Sentence Prediction? NSP (Next Sentence Prediction) is used to help BERT learn about relationships between sentences by predicting if a given sentence follows the previous sentence or not. \\*\\*Next Sentence Prediction Example:\\*\\*\n1. Paul went shopping. He bought a new shirt. (correct sentence pair)\n2. Ramona made coffee. Vanilla ice cream cones for sale. (incorrect sentence pair)\nIn training, 50% correct sentence pairs are mixed in with 50% random sentence pairs to help BERT increase next sentence prediction accuracy. \\*\\*Fun Fact:\\*\\* BERT is trained on both MLM (50%) and NSP (50%) at the same time. ### 2.4 Transformers\nThe Transformer architecture makes it possible to parallelize ML training extremely efficiently. Massive parallelization thus makes it feasible to train BERT on large amounts of data in a relatively short period of time. Transformers use an attention mechanism to observe relationships between words. A concept originally proposed in the popular [2017 Attention Is All You Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper sparked the use of Transformers in NLP models all around the world. >Since their introduction in 2017, Transformers have rapidly become the state-of-the-art approach to tackle tasks in many domains such as natural language processing, speech recognition, and computer vision. In short, if you’re doing deep learning, then you need Transformers! Lewis Tunstall, Hugging Face ML Engineer & [Author of Natural Language Processing with Transformers](https://www.amazon.com/Natural-Language-Processing-Transformers-Applications/dp/1098103246))\n\nTimeline of popular Transformer model releases:\n\n[Source](https://huggingface.co/course/chapter1/4)\n\n#### 2.4.1 How do Transformers work?"", ""It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\n\\*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\*\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A.""]",The FNet model replaces the self-attention layer in a BERT model with a Fourier transform that returns only the real parts of the transform.,Fourier transform
"What type of test should typically accompany a bug fix in Gradio's testing strategy?
","[""Test Strategy\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to. This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Objectives\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n1. Ensure that the Gradio library functions as we expect it to. 2. Enable the maintenance team to quickly identify both the presence and source of defects. 3. Prevent regressions, i.e. if we fix something it should stay fixed. 4. Improve the quality of the codebase in order to ease maintenance efforts. 5. Reduce the amount of manual testing required. ## Scope\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas. We should not focus on code coverage but on test coverage following the below criteria:\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n## Types of testing\nOur tests will broadly fall into one of three categories:\n- Static Quality checks\n- Dynamic 'Code' tests\n- Dynamic Functional tests\n### Static Quality checks\nStatic quality checks are generally very fast to run and do not require building the code base."", ""### Testing and Infrastructure Changes:\nNo changes to highlight. ### Breaking Changes:\nNo changes to highlight. ### Full Changelog:\nNo changes to highlight. ### Contributors Shoutout:\nNo changes to highlight. # 0.2.3\n### New Features:\nNo changes to highlight. ### Bug Fixes:\n- Fix example inputs for `gr.File(file\\_count='multiple')` output components by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4153](https://github.com/gradio-app/gradio/pull/4153)\n### Documentation Changes:\nNo changes to highlight. ### Testing and Infrastructure Changes:\nNo changes to highlight. ### Breaking Changes:\nNo changes to highlight. ### Full Changelog:\nNo changes to highlight. ### Contributors Shoutout:\nNo changes to highlight. # 0.2.2\n### New Features:\nNo changes to highlight. ### Bug Fixes:\n- Only send request to `/info` route if demo version is above `3.28.3` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4109](https://github.com/gradio-app/gradio/pull/4109)\n### Other Changes:\n- Fix bug in test from gradio 3.29.0 refactor by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4138](https://github.com/gradio-app/gradio/pull/4138)\n### Breaking Changes:\nNo changes to highlight. # 0.2.1\n### New Features:\nNo changes to highlight. ### Bug Fixes:\nRemoves extraneous `State` component info from the `Client.view\\_api()` method by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)\n### Documentation Changes:\nNo changes to highlight. ### Testing and Infrastructure Changes:\nSeparates flaky tests from non-flaky tests by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)\n### Breaking Changes:\nNo changes to highlight."", ""If you'd like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help. If you like Gradio, please leave us a ⭐ on GitHub! ## Open Source Stack\nGradio is built on top of many wonderful open-source libraries! [![huggingface](readme_files/huggingface_mini.svg)](https://huggingface.co)\n[![python](readme_files/python.svg)](https://www.python.org)\n[![fastapi](readme_files/fastapi.svg)](https://fastapi.tiangolo.com)\n[![encode](readme_files/encode.svg)](https://www.encode.io)\n[![svelte](readme_files/svelte.svg)](https://svelte.dev)\n[![vite](readme_files/vite.svg)](https://vitejs.dev)\n[![pnpm](readme_files/pnpm.svg)](https://pnpm.io)\n[![tailwind](readme_files/tailwind.svg)](https://tailwindcss.com)\n[![storybook](readme_files/storybook.svg)](https://storybook.js.org/)\n[![chromatic](readme_files/chromatic.svg)](https://www.chromatic.com/)\n## License\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository. ## Citation\nAlso check out the paper \\_[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019\\_, and please cite it if you use Gradio in your work. ```\n@article{abid2019gradio,\ntitle = {Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild},\nauthor = {Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},\njournal = {arXiv preprint arXiv:1906.02569},\nyear = {2019},\n}\n```""]","In Gradio's testing strategy, bug fixes should typically be accompanied by dynamic 'Code' tests wherever it is reasonably possible.",Dynamic code test
"How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?
","[""The question is when to use it. This framework obviously adds some time overhead because of the extra preprocessing and conversion steps. So it is important that you decide which framework is more appropriate for your case and model size. We recommend trying it for pre-training models or extended fine-tuning, but probably not for shorter fine-tuning of medium-sized models. The `Trainer` API and `accelerate` library are also very handy for model training, they are device-agnostic and give significant flexibility to the users. Congratulations 🎉 now you know how to train a GPT2 model in Megatron-LM and make it supported by `transformers`!"", ""Of course, we would need to evaluate the model more thoroughly before deploying it in the real world, but this is still an impressive prototype. {:else}\nLooking at these few examples, it seems that the model has learned some of the syntax of the Python data science stack (of course, we would need to evaluate it more thoroughly before deploying the model in the real world). Sometimes it requires more customization of the model training to achieve the necessary performance for a given use case, however. For example, what if we would like to dynamically update the batch size or have a conditional training loop that skips bad examples on the fly? One option would be to subclass the `Trainer` and add the necessary changes, but sometimes it's simpler to write the training loop from scratch. That's where 🤗 Accelerate comes in. {/if}\n{#if fw === 'pt'}\n## Training with 🤗 Accelerate[[training-with-accelerate]]\nWe've seen how to train a model with the `Trainer`, which can allow for some customization. However, sometimes we want full control over the training loop, or we want to make some exotic changes. In this case 🤗 Accelerate is a great choice, and in this section we'll go through the steps to use it to train our model. To make things more interesting, we'll also add a twist to the training loop. Since we are mainly interested in sensible autocompletion for the the data science libraries, it makes sense to give more weight to training samples that make more use of these libraries. We can easily identify these examples through the use of keywords such as `plt`, `pd`, `sk`, `fit`, and `predict`, which are the most frequent import names for `matplotlib.pyplot`, `pandas`, and `sklearn` as well as the fit/predict pattern of the latter."", ""Now let's talk about Accelerate, a library aimed to make this process more seameless and also help with a few best practices\n## 🤗 Accelerate\n[Accelerate](https://huggingface.co/docs/accelerate) is a library designed to allow you to perform what we just did above, without needing to modify your code greatly. On top of this, the data pipeline innate to Accelerate can also improve performance to your code as well. First, let's wrap all of the above code we just performed into a single function, to help us visualize the difference:\n```python\ndef train\\_ddp(rank, world\\_size):\nsetup(rank, world\\_size)\n# Build DataLoaders\ntransform = transforms.Compose([\ntransforms.ToTensor(),\ntransforms.Normalize((0.1307), (0.3081))\n])\ntrain\\_dset = datasets.MNIST('data', train=True, download=True, transform=transform)\ntest\\_dset = datasets.MNIST('data', train=False, transform=transform)\ntrain\\_loader = torch.utils.data.DataLoader(train\\_dset, shuffle=True, batch\\_size=64)\ntest\\_loader = torch.utils.data.DataLoader(test\\_dset, shuffle=False, batch\\_size=64)\n# Build model\nmodel = model.to(rank)\nddp\\_model = DDP(model, device\\_ids=[rank])\n# Build optimizer\noptimizer = optim.AdamW(ddp\\_model.parameters(), lr=1e-3)\n# Train for a single epoch\nmodel.train()\nfor batch\\_idx, (data, target) in enumerate(train\\_loader):\ndata, target = data.to(device), target.to(device)\noutput = model(data)\nloss = F.nll\\_loss(output, target)\nloss.backward()\noptimizer.step()\noptimizer.zero\\_grad()\n# Evaluate\nmodel.eval()\ncorrect = 0\nwith torch.no\\_grad():\nfor data, target in test\\_loader:\ndata, target = data.to(device), target.to(device)\noutput = model(data)\npred = output.argmax(dim=1, keepdim=True)\ncorrect += pred.eq(target.view\\_as(pred)).sum().item()\nprint(f'Accuracy: {100.""]","To force mixed precision training when initializing the Accelerator in 🤗 Accelerate, you can set the `mixed_precision` argument to `""fp16""` when creating the Accelerator instance. Here is an example of how to do this:

```python
from accelerate import Accelerator

accelerator = Accelerator(mixed_precision=""fp16"")
```

This will enable mixed precision training, which can help improve performance and reduce memory usage during training.",By passing `fp16=True` to the Accelerator init.
"What is the purpose of tokenizers in the NLP pipeline?
","[""Common operations include\nstripping whitespace, removing accented characters or lowercasing all\ntext. If you're familiar with [Unicode\nnormalization](https://unicode.org/reports/tr15), it is also a very\ncommon normalization operation applied in most tokenizers. Each normalization operation is represented in the 🤗 Tokenizers library\nby a `Normalizer`, and you can combine\nseveral of those by using a `normalizers.Sequence`. Here is a normalizer applying NFD Unicode normalization\nand removing accents as an example:\n\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_pipeline.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START setup\\_normalizer\"",\n\""end-before\"": \""END setup\\_normalizer\"",\n\""dedent\"": 8}\n\n{\""path\"": \""../../tokenizers/tests/documentation.rs\"",\n\""language\"": \""rust\"",\n\""start-after\"": \""START pipeline\\_setup\\_normalizer\"",\n\""end-before\"": \""END pipeline\\_setup\\_normalizer\"",\n\""dedent\"": 4}\n\n{\""path\"": \""../../bindings/node/examples/documentation/pipeline.test.ts\"",\n\""language\"": \""js\"",\n\""start-after\"": \""START setup\\_normalizer\"",\n\""end-before\"": \""END setup\\_normalizer\"",\n\""dedent\"": 8}\nYou can manually test that normalizer by applying it to any string:\n\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_pipeline.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START test\\_normalizer\"",\n\""end-before\"": \""END test\\_normalizer\"",\n\""dedent\"": 8}\n\n{\""path\"": \""../../tokenizers/tests/documentation.rs\"",\n\""language\"": \""rust\"",\n\""start-after\"": \""START pipeline\\_test\\_normalizer\"",\n\""end-before\"": \""END pipeline\\_test\\_normalizer\"",\n\""dedent\"": 4}\n\n{\""path\"": \""../../bindings/node/examples/documentation/pipeline.test.ts\"",\n\""language\"": \""js\"",\n\""start-after\"": \""START test\\_normalizer\"",\n\""end-before\"": \""END test\\_normalizer\"",\n\""dedent\"": 8}\nWhen building a `Tokenizer`, you can\ncustomize its normalizer by just changing the corresponding attribute:\n\n{\""path\"": \""../../bindings/python/tests/documentation/test\\_pipeline.py\"",\n\""language\"": \""python\"",\n\""start-after\"": \""START replace\\_normalizer\"",\n\""end-before\"": \""END replace\\_normalizer\"",\n\""dedent\"": 8}\n\n{\""path\"": \""../../tokenizers/tests/documentation.rs\"",\n\""language\"": \""rust\"",\n\""start-after\"": \""START pipeline\\_replace\\_normalizer\"",\n\""end-before\"": \""END pipeline\\_replace\\_normalizer\"",\n\""dedent\"": 4}\n\n{\""path\"": \""../../bindings/node/examples/documentation/pipeline.test.ts\"",\n\""language\"": \""js\"",\n\""start-after\"": \""START replace\\_normalizer\"",\n\""end-before\"": \""END replace\\_normalizer\"",\n\""dedent\"": 8}\nOf course, if you change the way a tokenizer applies normalization, you\nshould probably retrain it from scratch afterward."", ""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a PyTorch tensor."", ""FrameworkSwitchCourse {fw} />\n# Tokenizers[[tokenizers]]\n{#if fw === 'pt'}\n{:else}\n{/if}\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. In NLP tasks, the data that is generally processed is raw text. Here's an example of such text:\n```\nJim Henson was a puppeteer\n```\nHowever, models can only process numbers, so we need to find a way to convert the raw text to numbers. That's what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation — that is, the one that makes the most sense to the model — and, if possible, the smallest representation. Let's take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization. ## Word-based[[word-based]]\nThe first type of tokenizer that comes to mind is \\_word-based\\_. It's generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:\n\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg)\n![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg)\n\nThere are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python's `split()` function:\n```py\ntokenized\\_text = \""Jim Henson was a puppeteer\"".split()\nprint(tokenized\\_text)\n```\n```python out\n['Jim', 'Henson', 'was', 'a', 'puppeteer']\n```\nThere are also variations of word tokenizers that have extra rules for punctuation.""]","Tokenizers are one of the core components of the NLP pipeline. Their primary purpose is to translate text into data that can be processed by the model. Since models can only process numbers, tokenizers convert raw text inputs into numerical data. The goal is to find the most meaningful representation of the text that makes sense to the model while aiming for the smallest representation possible.",To translate text into data that can be processed by the model.
"What is the purpose of the Safety Checker in the Diffusers library?
","[""🧨 Diffusers Experimental\nWe are adding experimental code to support novel applications and usages of the Diffusers library."", ""Research projects\nThis folder contains various research projects using 🧨 Diffusers. They are not really maintained by the core maintainers of this library and often require a specific version of Diffusers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice. If you need help with any of those, please open an issue where you directly ping the author(s), as indicated at the top of the README of each folder."", ""# Diffusers examples with ONNXRuntime optimizations\n\\*\\*This research project is not actively maintained by the diffusers team.""]","The Safety Checker in the Diffusers library is designed to filter out potentially harmful or inappropriate content generated by models. It helps ensure that the outputs produced by the models adhere to safety standards and guidelines, making the generated content more suitable for various applications.",The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.
"What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?
","[""--\ntitle: Introducing Pull Requests and Discussions 🥳\nthumbnail: /blog/assets/76\\_community\\_update/thumbnail.png\n---\n# Introducing Pull Requests and Discussions 🥳\n![Pull requests and discussions on the hub](assets/76\\_community\\_update/community-update.png)\nWe are thrilled to announce the release of our latest collaborative features: pull requests and discussions on the Hugging Face Hub! Pull requests and discussions are available today under the [community tab](https://huggingface.co/gpt2/discussions) for all repository types: models, datasets, and Spaces. Any member of the community can create and participate in discussions and pull requests, facilitating collaborations not only within teams, but also with everyone else in the community! It's the biggest update ever done to the Hub, and we can't wait to see the community members start collaborating with it 🤩. The new \""Community\"" tab also aligns with proposals in ethical ML throughout the years. Feedback and iterations have a central place in the development of ethical machine learning software. We really believe having it in the community's toolset will unlock new kinds of positive patterns in ML, collaborations, and progress. Some example use cases for discussions and pull requests:\n- Propose suggestions in model cards to improve disclosures of ethical biases. - Let users flag concerning generations of a given Space demo. - Provide a venue through which model and dataset authors can have a direct discussion with community members. - Allow others to improve your repositories! For example, users might want to provide TensorFlow weights! ## Discussions\n![Discussions on the Hugging Face Hub](assets/76\\_community\\_update/new-discussion.png)\n[Discussions](https://huggingface.co/gpt2/discussions?type=discussion) allow community members ask and answer questions as well as share their ideas and suggestions directly with the repository owners and the community. Anyone can create and participate in discussions in the community tab of a repository. ## Pull requests\n![Pull requests on the Hugging Face Hub](assets/76\\_community\\_update/new-pr.png)\n[Pull requests](https://huggingface.co/gpt2/discussions?type=pull\\_request) allow community members open, comment, merge, or close pull requests directly from the website. The easiest way to open a pull request is to use the \""Collaborate\"" button in the \""Files and versions\"" tab. It will let you do single file contributions very easily. Under the hood, our Pull requests do not use forks and branches, but instead, custom \""branches\"" called `refs` that are stored directly on the source repo. This approach to avoids the need to create a forks for each new version of the model/dataset. ## How is this different from other git hosts\nAt a high level, we aim to build a simpler version of other git hosts' (like GitHub's) PRs and Issues:\n- no forks are involved: contributors push to a special `ref` branch directly on the source repo\n- no hard distinction between issues and PRs: they are essentially the same so we display them in the same lists\n- streamlined for ML (i.e."", ""- Transferring a repository from a source organization to another target organization if the user does not have both \""admin\"" rights in the source organization \\*\\*and\\*\\* either \""write\"" or \""admin\"" rights in the target organization. - Transferring a repository from user A to user B. If these are use cases you need help with, please send us an email at \\*\\*website at huggingface.co\\*\\*. ## Disabling Discussions / Pull Requests\nYou can disable all discussions and Pull Requests. Once disabled, all community and contribution features won't be available anymore. This action can be reverted without losing any previous discussions or Pull Requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-settings-disable.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/discussions-settings-disable-dark.png)"", ""!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Interact with Discussions and Pull Requests\nThe `huggingface\\_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood. ## Retrieve Discussions and Pull Requests from the Hub\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(repo\\_id=\""bigscience/bloom\""):\n... print(f\""{discussion.num} - {discussion.title}, pr: {discussion.is\\_pull\\_request}\"")\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer\\_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]\n```\n`HfApi.get\\_repo\\_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(\n...""]",The `HfApi` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.,HfApi
"What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?
","[""--\ntitle: \""Supercharged Searching on the 🤗 Hub\""\nthumbnail: /blog/assets/48\\_hubsearch/thumbnail.png\nauthors:\n- user: muellerzr\n---\n# Supercharged Searching on the Hugging Face Hub\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/muellerzr/hf-blog-notebooks/blob/main/Searching-the-Hub.ipynb)\nThe `huggingface\\_hub` library is a lightweight interface that provides a programmatic approach to exploring the hosting endpoints Hugging Face provides: models, datasets, and Spaces. Up until now, searching on the Hub through this interface was tricky to pull off, and there were many aspects of it a user had to \""just know\"" and get accustomed to. In this article, we will be looking at a few exciting new features added to `huggingface\\_hub` to help lower that bar and provide users with a friendly API to search for the models and datasets they want to use without leaving their Jupyter or Python interfaces. > Before we begin, if you do not have the latest version of the `huggingface\\_hub` library on your system, please run the following cell:\n```python\n!pip install huggingface\\_hub -U\n```\n## Situating the Problem:\nFirst, let's imagine the scenario you are in. You'd like to find all models hosted on the Hugging Face Hub for Text Classification, were trained on the GLUE dataset, and are compatible with PyTorch."", ""Using AllenNLP at Hugging Face\n`allennlp` is a NLP library for developing state-of-the-art models on different linguistic tasks. It provides high-level abstractions and APIs for common components and models in modern NLP. It also provides an extensible framework that makes it easy to run and manage NLP experiments. ## Exploring allennlp in the Hub\nYou can find `allennlp` models on the Hub by filtering at the left of the [models page](https://huggingface.co/models?library=allennlp). All models on the Hub come up with useful features\n1. A training metrics tab with automatically hosted TensorBoard traces. 2. Metadata tags that help for discoverability. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-allennlp_widget.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-allennlp_widget-dark.png)\n\n## Using existing models\nYou can use the `Predictor` class to load existing models on the Hub. To achieve this, use the `from\\_path` method and use the `\""hf://\""` prefix with the repository id. Here is an end-to-end example. ```py\nimport allennlp\\_models\nfrom allennlp.predictors.predictor import Predictor\npredictor = Predictor.from\\_path(\""hf://allenai/bidaf-elmo\"")\npredictor\\_input = {\n\""passage\"": \""My name is Wolfgang and I live in Berlin\"",\n\""question\"": \""Where do I live?\""\n}\npredictions = predictor.predict\\_json(predictor\\_input)\n```\nTo get a snippet such as this, you can click `Use in AllenNLP` at the top right,\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-allennlp_snippet.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-allennlp_snippet-dark.png)\n\n## Sharing your models\nThe first step is to save the model locally."", ""--\ntitle: \""Introducing Skops\""\nthumbnail: /blog/assets/94\\_skops/introducing\\_skops.png\nauthors:\n- user: merve\n- user: adrin\n- user: BenjaminB\n---\n# Introducing Skops\n## Introducing Skops\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others. Let's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production. ```python\n# let's import the libraries first\nimport sklearn\nfrom sklearn.datasets import load\\_breast\\_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model\\_selection import train\\_test\\_split\n# Load the data and split\nX, y = load\\_breast\\_cancer(as\\_frame=True, return\\_X\\_y=True)\nX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(\nX, y, test\\_size=0.3, random\\_state=42\n)\n# Train the model\nmodel = DecisionTreeClassifier().fit(X\\_train, y\\_train)\n```\nYou can use any model filename and serialization method, like `pickle` or `joblib`.""]",The name of the new library introduced by Hugging Face for hosting scikit-learn models is Skops.,Skops
"What is the purpose of Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""However, we observed that fine-tuning the encoder produces better results. We experimented with this approach after seeing it used in other Dreambooth implementations, and the results are striking! Frozen text encoder\n![Frozen text encoder](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/f\\_froxen\\_encoder.jpg)\nFine-tuned text encoder\n![Fine-tuned text encoder](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/g\\_unfrozen\\_encoder.jpg)\nFine-tuning the text encoder produces the best results, especially with faces. It generates more realistic images, it's less prone to overfitting and it also achieves better prompt interpretability, being able to handle more complex prompts. ## Epilogue: Textual Inversion + Dreambooth\nWe also ran a final experiment where we combined [Textual Inversion](https://textual-inversion.github.io) with Dreambooth. Both techniques have a similar goal, but their approaches are different. In this experiment we first ran textual inversion for 2000 steps. From that model, we then ran Dreambooth for an additional 500 steps using a learning rate of `1e-6`. These are the results:\n![Textual Inversion + Dreambooth](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/h\\_textual\\_inversion\\_dreambooth.jpg)\nWe think the results are much better than doing plain Dreambooth but not as good as when we fine-tune the whole text encoder. It seems to copy the style of the training images a bit more, so it could be overfitting to them. We didn't explore this combination further, but it could be an interesting alternative to improve Dreambooth and still fit the process in a 16GB GPU. Feel free to explore and tell us about your results!"", ""It's a causal (unidirectional) transformer\npre-trained using language modeling on a large corpus will long range dependencies, the Toronto Book Corpus. The abstract from the paper is the following:\n\\*Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering,\nsemantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant,\nlabeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to\nperform adequately. We demonstrate that large gains on these tasks can be realized by generative pretraining of a\nlanguage model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In\ncontrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve\neffective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our\napproach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms\ndiscriminatively trained models that use architectures specifically crafted for each task, significantly improving upon\nthe state of the art in 9 out of the 12 tasks studied.\\*\n[Write With Transformer](https://transformer.huggingface.co/doc/gpt) is a webapp created and hosted by Hugging Face\nshowcasing the generative capabilities of several models.""]","Textual Inversion is a method to personalize text-to-image models like stable diffusion using just 3-5 examples of your own images. It allows users to adapt the model to generate images that are more aligned with their specific preferences or styles. By fine-tuning the model, it can produce more realistic images and handle more complex prompts effectively.",Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.
"What is the recommended multiple of batch size for fp16 data type on an A100 GPU?
","[""The idle parts are referred to as the \""bubble\"". Both parts of the diagram show parallelism that is of degree 4. That is 4 GPUs are participating in the pipeline. So there is the forward path of 4 pipe stages F0, F1, F2 and F3 and then the return reverse order backward path of B3, B2, B1 and B0. PP introduces a new hyper-parameter to tune that is called `chunks`. It defines how many chunks of data are sent in a sequence through the same pipe stage. For example, in the bottom diagram, you can see that `chunks=4`. GPU0 performs the same forward path on chunk 0, 1, 2 and 3 (F0,0, F0,1, F0,2, F0,3) and then it waits for other GPUs to do their work and only when their work is starting to be complete, does GPU0 start to work again doing the backward path for chunks 3, 2, 1 and 0 (B0,3, B0,2, B0,1, B0,0). Note that conceptually this is the same concept as gradient accumulation steps (GAS). PyTorch uses `chunks`, whereas DeepSpeed refers to the same hyper-parameter as GAS. Because of the chunks, PP introduces the concept of micro-batches (MBS). DP splits the global data batch size into mini-batches, so if you have a DP degree of 4, a global batch size of 1024 gets split up into 4 mini-batches of 256 each (1024/4). And if the number of `chunks` (or GAS) is 32 we end up with a micro-batch size of 8 (256/32). Each Pipeline stage works with a single micro-batch at a time. To calculate the global batch size of the DP + PP setup we then do: `mbs\\*chunks\\*dp\\_degree` (`8\\*32\\*4=1024`)."", ""GPU experiments are ran without further optimizations besides JAX\ntransformations. GPU experiments are ran with full precision (fp32). \""TPU v3-8\""\nare 8 TPU cores on 4 chips (each chips has 2 cores), while \""8 GPU\"" are 8 GPU chips."", ""By default, this is set to `True` which is used to train in mixed Int8/FP16 precision. However, we are interested in memory efficient inference for which we need to use `has\\_fp16\\_weights=False`. 5. Now time to load your model in 8-bit! ```py\nint8\\_model.load\\_state\\_dict(torch.load(\""model.pt\""))\nint8\\_model = int8\\_model.to(0) # Quantization happens here\n```\nNote that the quantization step is done in the second line once the model is set on the GPU. If you print `int8\\_model[0].weight` before calling the `.to` function you get:\n```\nint8\\_model[0].weight\nParameter containing:\ntensor([[ 0.0031, -0.0438, 0.0494, ..., -0.0046, -0.0410, 0.0436],\n[-0.1013, 0.0394, 0.0787, ..., 0.0986, 0.0595, 0.0162],\n[-0.0859, -0.1227, -0.1209, ..., 0.1158, 0.0186, -0.0530],\n...,\n[ 0.0804, 0.0725, 0.0638, ..., -0.0487, -0.0524, -0.1076],\n[-0.0200, -0.0406, 0.0663, ..., 0.0123, 0.0551, -0.0121],\n[-0.0041, 0.0865, -0.0013, ..., -0.0427, -0.0764, 0.1189]],\ndtype=torch.float16)\n```\nWhereas if you print it after the second line's call you get:\n```\nint8\\_model[0].weight\nParameter containing:\ntensor([[ 3, -47, 54, ..., -5, -44, 47],\n[-104, 40, 81, ..., 101, 61, 17],\n[ -89, -127, -125, ..., 120, 19, -55],\n...,\n[ 82, 74, 65, ..., -49, -53, -109],\n[ -21, -42, 68, ..., 13, 57, -12],\n[ -4, 88, -1, ..., -43, -78, 121]],\ndevice='cuda:0', dtype=torch.int8, requires\\_grad=True)\n```\nThe weights values are \""truncated\"" as we have seen when explaining quantization in the previous sections. Also, the values seem to be distributed between [-127, 127]. You might also wonder how to retrieve the FP16 weights in order to perform the outlier MatMul in fp16? You can simply do:\n```py\n(int8\\_model[0].weight.CB \\* int8\\_model[0].weight.SCB) / 127\n```\nAnd you will get:\n```\ntensor([[ 0.0028, -0.0459, 0.0522, ..., -0.0049, -0.0428, 0.0462],\n[-0.0960, 0.0391, 0.0782, ..., 0.0994, 0.0593, 0.0167],\n[-0.0822, -0.1240, -0.1207, ..., 0.1181, 0.0185, -0.0541],\n...,\n[ 0.0757, 0.0723, 0.0628, ..., -0.0482, -0.0516, -0.1072],\n[-0.0194, -0.0410, 0.0657, ..., 0.0128, 0.0554, -0.0118],\n[-0.0037, 0.0859, -0.0010, ..., -0.0423, -0.0759, 0.1190]],\ndevice='cuda:0')\n```\nWhich is close enough to the original FP16 values (2 print outs up)!""]",The context does not provide specific information regarding the recommended multiple of batch size for fp16 data type on an A100 GPU. Please refer to the official HuggingFace and gradio-app documentation or resources for detailed guidelines on this topic.,64
"How do you run a Gradio Blocks app in reload mode using a Python IDE?
","[""通过自动重载实现更快的开发\n\\*\\*先决条件\\*\\*：本指南要求您了解块的知识。请确保[先阅读块指南](https://gradio.app/quickstart/#blocks-more-flexibility-and-control)。\n本指南介绍了自动重新加载、在 Python IDE 中重新加载以及在 Jupyter Notebooks 中使用 gradio 的方法。\n## 为什么要使用自动重载？\n当您构建 Gradio 演示时，特别是使用 Blocks 构建时，您可能会发现反复运行代码以测试更改很麻烦。\n为了更快速、更便捷地编写代码，我们已经简化了在 \\*\\*Python IDE\\*\\*（如 VS Code、Sublime Text、PyCharm 等）中开发或从终端运行 Python 代码时“重新加载”Gradio 应用的方式。我们还开发了一个类似的“魔法命令”，使您可以更快速地重新运行单元格，如果您使用 Jupyter Notebooks（或类似的环境，如 Colab）的话。\n这个简短的指南将涵盖这两种方法，所以无论您如何编写 Python 代码，您都将知道如何更快地构建 Gradio 应用程序。\n## Python IDE 重载 🔥\n如果您使用 Python IDE 构建 Gradio Blocks，那么代码文件（假设命名为 `run.py`）可能如下所示：\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Markdown(\""# 来自Gradio的问候！\"")\ninp = gr.Textbox(placeholder=\""您叫什么名字？\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""欢迎，{x}！\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\n问题在于，每当您想要更改布局、事件或组件时，都必须通过编写 `python run.py` 来关闭和重新运行应用程序。\n而不是这样做，您可以通过更改 1 个单词来以\\*\\*重新加载模式\\*\\*运行代码：将 `python` 更改为 `gradio`：\n在终端中运行 `gradio run.py`。就是这样！\n现在，您将看到类似于这样的内容：\n```bash\nLaunching in \\*reload mode\\* on: http://127.0.0.1:7860 (Press CTRL+C to quit)\nWatching..."", ""Developing Faster with Auto-Reloading\n\\*\\*Prerequisite\\*\\*: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control). This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks. ## Why Auto-Reloading? When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes. To make it faster and more convenient to write your code, we've made it easier to \""reload\"" your Gradio apps instantly when you are developing in a \\*\\*Python IDE\\*\\* (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \""magic command\"" that allows you to re-run cells faster if you use \\*\\*Jupyter Notebooks\\*\\* (or any similar environment like Colab). This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster. ## Python IDE Reload 🔥\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Markdown(\""# Greetings from Gradio!\"")\ninp = gr.Textbox(placeholder=\""What is your name?\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""Welcome, {x}!\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`."", ""This means you'll see your changes \\_much, much faster\\_ than if you were rerunning the cell normally. Here's what it looks like in a jupyter notebook:\n![](https://gradio-builds.s3.amazonaws.com/demo-files/jupyter\\_reload.gif)\n🪄 This works in colab notebooks too! [Here's a colab notebook](https://colab.research.google.com/drive/1zAuWoiTIb3O2oitbtVb2\\_ekv1K6ggtC1?usp=sharing) where you can see the Blocks magic in action. Try making some changes and re-running the cell with the Gradio code! The Notebook Magic is now the author's preferred way of building Gradio demos. Regardless of how you write Python code, we hope either of these methods will give you a much better development experience using Gradio. ---\n## Next Steps\nNow that you know how to develop quickly using Gradio, start building your own! If you are looking for inspiration, try exploring demos other people have built with Gradio, [browse public Hugging Face Spaces](http://hf.space/) 🤗""]","要在 Python IDE 中以重新加载模式运行 Gradio Blocks 应用，您可以按照以下步骤操作：

1. 确保您的代码文件（假设命名为 `run.py`）包含 Gradio Blocks 的代码，例如：
   ```python
   import gradio as gr
   with gr.Blocks() as demo:
       gr.Markdown(""# 来自Gradio的问候！"")
       inp = gr.Textbox(placeholder=""您叫什么名字？"")
       out = gr.Textbox()
       inp.change(fn=lambda x: f""欢迎，{x}！"",
                   inputs=inp,
                   outputs=out)
   if __name__ == ""__main__"":
       demo.launch()
   ```

2. 在终端中运行以下命令，将 `python` 更改为 `gradio`：
   ```
   gradio run.py
   ```

这样，您将以重新加载模式启动应用程序，您可以更快地看到更改的效果。",Run `gradio run.py` in the terminal.
"How can you install the Hugging Face Unity API in your Unity project?
","[""--\ntitle: \""How to Install and Use the Hugging Face Unity API\""\nthumbnail: /blog/assets/124\\_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n# How to Install and Use the Hugging Face Unity API\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API. ## Installation\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif)\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens). 7. Test the API key by clicking `Test API key` in the API Wizard. 8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API\\_URL` field."", ""he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library that provides an API to quickly download many public datasets and preprocess them. In this video we will explore how to do that. The downloading part is easy: with the load\\_dataset function, you can directly download and cache a dataset from its identifier on the Dataset hub. Here we fetch the MRPC dataset from the GLUE benchmark, which is a dataset containing pairs of sentences where the task is to determine the paraphrases. The object returned by the load\\_dataset function is a DatasetDict, which is a sort of dictionary containing each split of our dataset. We can access each split by indexing with its name. This split is then an instance of the Dataset class, with columns (here sentence1, sentence2. label and idx) and rows. We can access a given element by its index. The amazing thing about the Hugging Face Datasets library is that everything is saved to disk using Apache Arrow, which means that even if your dataset is huge you won't get out of RAM: only the elements you request are loaded in memory."", ""Hugging Face Inference Endpoints documentation\n## Setup\n```bash\npip install hf-doc-builder==0.4.0 watchdog --upgrade\n```\n## Local Development\n```bash\ndoc-builder preview endpoints docs/source/ --not\\_python\\_module\n```\n## Build Docs\n```bash\ndoc-builder build endpoints docs/source/ --build\\_dir build/ --not\\_python\\_module\n```\n## Add assets/Images\nAdding images/assets is only possible through `https://` links meaning you need to use `https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/` prefix. example\n```bash\n![Login](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/0_login.png)\n```\n## Generate API Reference\n1. Copy openapi spec from `https://api.endpoints.huggingface.cloud/api-doc/openapi.json`\n2. create markdown `widdershins --environment env.json openapi.json -o myOutput.md`\n3. copy into `api\\_reference.mdx`""]","To install the Hugging Face Unity API in your Unity project, follow these steps:

1. Open your Unity project.
2. Go to `Window` -> `Package Manager`.
3. Click `+` and select `Add Package from git URL`.
4. Enter `https://github.com/huggingface/unity-api.git`.
5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.
6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).
7. Test the API key by clicking `Test API key` in the API Wizard.
8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.","To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`."
"What is the pretraining objective of the Wav2Vec2 context network?
","[""Wav2Vec2 Contrastive Loss PreTraining examples\nThe following example showcases how to pretrain a wav2vec2 model using the JAX/Flax backend. Pretraining Wav2Vec2 is rather complex, so it is highly recommended to read the\n[official paper](https://arxiv.org/abs/2006.11477). JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU. Models written in JAX/Flax are \\*\\*immutable\\*\\* and updated in a purely functional\nway which enables simple and efficient model parallelism. `run\\_wav2vec2\\_pretrain\\_flax.py` is a lightweight example of how to download and preprocess a dataset from the 🤗 Datasets library or use your own files (jsonlines or csv), then pretrain the wav2vec2 architectures above on it. For custom datasets in `jsonlines` format please see: [the Datasets documentation](https://huggingface.co/docs/datasets/loading\\_datasets#json-files) and you also will find examples of these below. Let's start by creating a model repository to save the trained model and logs. Here we call the model `\""wav2vec2-base-robust\""`, but you can change the model name as you like. You can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\nyou are logged in) or via the command line:\n```\nhuggingface-cli repo create wav2vec2-base-robust\n```\nNext we clone the model repository to add the tokenizer and model files. ```\ngit clone https://huggingface.co//wav2vec2-base-robust\n```\nTo ensure that all tensorboard traces will be uploaded correctly, we need to\ntrack them. You can run the following command inside your model repo to do so. ```\ncd wav2vec2-base-robust\ngit lfs track \""\\*tfevents\\*\""\n```\nGreat, we have set up our model repository. During training, we will automatically\npush the training logs and model weights to the repo. Next, let's add a symbolic link to the `run\\_wav2vec2\\_pretrain\\_flax`. ```bash\nexport MODEL\\_DIR=\""./wav2vec2-base-robust\""\nln -s ~/transformers/examples/research\\_projects/jax-projects/wav2vec2/run\\_wav2vec2\\_pretrain\\_flax.py ./\n```\n### Create the model configuration\nLet's first create the model configuration and store it in the model repository."", ""[Wav2Vec2](https://huggingface.co/Graphcore/wav2vec2-base-ipu) is a pretrained self-supervised model for automatic speech recognition. Using a novel contrastive pretraining objective, Wav2Vec2 learns powerful speech representations from large amounts of unlabelled speech data, followed by fine-tuning on a small amount of transcribed speech data, outperforming the best semi-supervised methods while being conceptually simpler. ## Hugging Face Optimum Graphcore: building on a solid partnership\nGraphcore joined the [Hugging Face Hardware Partner Program](https://huggingface.co/hardware) in 2021 as a founding member, with both companies sharing the common goal of lowering the barriers for innovators seeking to harness the power of machine intelligence. Since then, Graphcore and Hugging Face have worked together extensively to make training of transformer models on IPUs fast and easy, with the first Optimum Graphcore model (BERT) being made available last year. Transformers have proven to be extremely efficient for a wide range of functions, including feature extraction, text generation, sentiment analysis, translation and many more. Models like BERT are widely used by Graphcore customers in a huge array of applications including cybersecurity, voice call automation, drug discovery, and translation."", ""--\ntitle: \""Boosting Wav2Vec2 with n-grams in 🤗 Transformers\""\nthumbnail: /blog/assets/44\\_boost\\_wav2vec2\\_ngram/wav2vec2\\_ngram.png\nauthors:\n- user: patrickvonplaten\n---\n# Boosting Wav2Vec2 with n-grams in 🤗 Transformers\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Boosting_Wav2Vec2_with_n_grams_in_Transformers.ipynb)\n\\*\\*Wav2Vec2\\*\\* is a popular pre-trained model for speech recognition. Released in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Meta AI Research, the novel architecture catalyzed progress in\nself-supervised pretraining for speech recognition, \\*e.g.\\* [\\*G. Ng et\nal.\\*, 2021](https://arxiv.org/pdf/2104.03416.pdf), [\\*Chen et al\\*,\n2021](https://arxiv.org/abs/2110.13900), [\\*Hsu et al.\\*,\n2021](https://arxiv.org/abs/2106.07447) and [\\*Babu et al.\\*,\n2021](https://arxiv.org/abs/2111.09296). On the Hugging Face Hub,\nWav2Vec2's most popular pre-trained checkpoint currently amounts to\nover [\\*\\*250,000\\*\\* monthly\ndownloads](https://huggingface.co/facebook/wav2vec2-base-960h). Using Connectionist Temporal Classification (CTC), pre-trained\nWav2Vec2-like checkpoints are extremely easy to fine-tune on downstream\nspeech recognition tasks. In a nutshell, fine-tuning pre-trained\nWav2Vec2 checkpoints works as follows:\nA single randomly initialized linear layer is stacked on top of the\npre-trained checkpoint and trained to classify raw audio input to a\nsequence of letters.""]","The pretraining objective of the Wav2Vec2 context network is a novel contrastive pretraining objective. This allows Wav2Vec2 to learn powerful speech representations from large amounts of unlabelled speech data, followed by fine-tuning on a small amount of transcribed speech data. This approach enables Wav2Vec2 to outperform the best semi-supervised methods while being conceptually simpler.",The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.
"What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?
","[""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a TensorFlow tensor."", ""For instance, if the review was negative, I could create a feature which would contact the customer, apologize for the poor quality of service, and inform him/her that our support team will contact him/her as soon as possible to assist him and hopefully fix the problem. Since I don't plan to get more than 2,000 requests per month, I didn't impose any performance constraints regarding the time and the scalability. ## The Transformers library\nI was a bit confused at the beginning when I downloaded the .h5 file. I thought it would be compatible with `tensorflow.keras.models.load\\_model`, but this wasn't the case. After a few minutes of research I was able to figure out that the file was a weights checkpoint rather than a Keras model. After that, I tried out the API that Hugging Face offers and read a bit more about the pipeline feature they offer. Since the results of the API & the pipeline were great, I decided that I could serve the model through the pipeline on my own server. Below is the [official example](https://github.com/huggingface/transformers#quick-tour) from the Transformers GitHub page. ```python\nfrom transformers import pipeline\n# Allocate a pipeline for sentiment-analysis\nclassifier = pipeline('sentiment-analysis')\nclassifier('We are very happy to include pipeline into the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9978193640708923}]\n```\n## Deploy transformers to Google Cloud\n> GCP is chosen as it is the cloud environment I am using in my personal organization."", ""Using `Transformers.js` at Hugging Face\nTransformers.js is a JavaScript library for running 🤗 Transformers directly in your browser, with no need for a server! It is designed to be functionally equivalent to the original [Python library](https://github.com/huggingface/transformers), meaning you can run the same pretrained models using a very similar API. ## Exploring `transformers.js` in the Hub\nYou can find `transformers.js` models by filtering by library in the [models page](https://huggingface.co/models?library=transformers.js). ## Quick tour\nIt's super simple to translate from existing code! Just like the Python library, we support the `pipeline` API. Pipelines group together a pretrained model with preprocessing of inputs and postprocessing of outputs, making it the easiest way to run models with the library. | **Python (original)** | **Javascript (ours)** |\n| --- | --- |\n| ```python from transformers import pipeline # Allocate a pipeline for sentiment-analysis pipe = pipeline('sentiment-analysis') out = pipe('I love transformers!') # [{'label': 'POSITIVE', 'score': 0.999806941}] ``` | ```javascript import { pipeline } from '@xenova/transformers'; // Allocate a pipeline for sentiment-analysis let pipe = await pipeline('sentiment-analysis'); let out = await pipe('I love transformers!'); // [{'label': 'POSITIVE', 'score': 0.999817686}] ``` |\n\nYou can also use a different model by specifying the model id or path as the second argument to the `pipeline` function. For example:\n```javascript\n// Use a different model for sentiment-analysis\nlet pipe = await pipeline('sentiment-analysis', 'nlptown/bert-base-multilingual-uncased-sentiment');\n```\nRefer to the [documentation](https://huggingface.co/docs/transformers.js) for the full list of supported tasks and models. ## Installation\nTo install via [NPM](https://www.npmjs.com/package/@xenova/transformers), run:\n```bash\nnpm i @xenova/transformers\n```\nFor more information, including how to use it in vanilla JS (without any bundler) via a CDN or static hosting, refer to the [README](https://github.com/xenova/transformers.js/blob/main/README.md#installation).""]",The default checkpoint used by the sentiment analysis pipeline in the Transformers library is `distilbert base uncased finetuned sst2 english`.,distilbert base uncased finetuned sst2 english
"What is the purpose of the notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi""?
","[""Why would you want to use DeepSpeed with just one GPU? 1. It has a ZeRO-offload feature which can delegate some computations and memory to the host's CPU and RAM, and thus\nleave more GPU resources for model's needs - e.g. larger batch size, or enabling a fitting of a very big model which\nnormally won't fit. 2. It provides a smart GPU memory management system, that minimizes memory fragmentation, which again allows you to fit\nbigger models and data batches. While we are going to discuss the configuration in details next, the key to getting a huge improvement on a single GPU\nwith DeepSpeed is to have at least the following configuration in the configuration file:\n```json\n{\n\""zero\\_optimization\"": {\n\""stage\"": 2,\n\""offload\\_optimizer\"": {\n\""device\"": \""cpu\"",\n\""pin\\_memory\"": true\n},\n\""allgather\\_partitions\"": true,\n\""allgather\\_bucket\\_size\"": 2e8,\n\""reduce\\_scatter\"": true,\n\""reduce\\_bucket\\_size\"": 2e8,\n\""overlap\\_comm\"": true,\n\""contiguous\\_gradients\"": true\n}\n}\n```\nwhich enables optimizer offload and some other important features."", ""When you will face a real project that will be running for hours and perhaps days, definitely spend more time to make sure you use the most optimal hyper-parameters to get your job done faster and at a minimal cost. If you would like to experiment with this benchmark yourself or want to know more details about the hardware and software used to run it, please, refer to [this post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400). # Fitting A Huge Model Onto One GPU\nWhile Fairscale gives us a boost only with multiple GPUs, DeepSpeed has a gift even for those of us with a single GPU. Let's try the impossible - let's train [t5-3b](https://huggingface.co/t5-3b) on a 24GB RTX-3090 card. First let's try to finetune the huge `t5-3b` using the normal single GPU setup:\n```\nexport BS=1\nCUDA\\_VISIBLE\\_DEVICES=0 ./finetune\\_trainer.py \\\n--model\\_name\\_or\\_path t5-3b --n\\_train 60 --n\\_val 10 \\\n--per\\_device\\_eval\\_batch\\_size $BS --per\\_device\\_train\\_batch\\_size $BS \\\n--task translation\\_en\\_to\\_ro --fp16 [...]\n```\nNo cookie, even with BS=1 we get:\n```\nRuntimeError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.70 GiB total capacity;\n21.37 GiB already allocated; 45.69 MiB free; 22.05 GiB reserved in total by PyTorch)\n```\nNote, as earlier I'm showing only the important parts and the full command line arguments can be found\n[here](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685). Now update your `transformers` to v4.2.0 or higher, then install DeepSpeed:\n```\npip install deepspeed\n```\nand let's try again, this time adding DeepSpeed to the command line:\n```\nexport BS=20\nCUDA\\_VISIBLE\\_DEVICES=0 deepspeed --num\\_gpus=1 ./finetune\\_trainer.py \\\n--model\\_name\\_or\\_path t5-3b --n\\_train 60 --n\\_val 10 \\\n--per\\_device\\_eval\\_batch\\_size $BS --per\\_device\\_train\\_batch\\_size $BS \\\n--task translation\\_en\\_to\\_ro --fp16 --deepspeed ds\\_config\\_1gpu.json [...]\n```\net voila!"", ""Although these devices have quite a lot of memory, the model is so large that a single device is not enough to contain a single instance of BLOOMZ. To solve this issue, we are going to use [DeepSpeed](https://www.deepspeed.ai/), which is a deep learning optimization library that enables many memory and speed improvements to accelerate the model and make it fit the device. In particular, we rely here on [DeepSpeed-inference](https://arxiv.org/abs/2207.00032): it introduces several features such as [model (or pipeline) parallelism](https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism) to make the most of the available devices. For Gaudi2, we use [Habana's DeepSpeed fork](https://github.com/HabanaAI/deepspeed) that adds support for HPUs. ### Latency\nWe measured latencies (batch of one sample) for two different sizes of BLOOMZ, both with multi-billion parameters:\n- [176 billion](https://huggingface.co/bigscience/bloomz) parameters\n- [7 billion](https://huggingface.co/bigscience/bloomz-7b1) parameters\nRuns were performed with DeepSpeed-inference in 16-bit precision with 8 devices and using a [key-value cache](https://huggingface.co/docs/transformers/v4.27.1/en/model\\_doc/bloom#transformers.BloomForCausalLM.forward.use\\_cache).""]","The notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi"" serves to demonstrate how to leverage DeepSpeed, a deep learning optimization library, to efficiently train large models, specifically those with billions of parameters, on Habana Gaudi hardware. It highlights the use of features such as model or pipeline parallelism to optimize memory usage and speed, making it feasible to fit and train these large models on the available devices. The notebook provides practical guidance and examples for users looking to implement these techniques in their own projects.",To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.
"What command line module does PyTorch provide to run a script on multiple GPUs?
","[""For more GPUs or nodes adjust the arguments in the script. You will find a lot more details inside the diagnostics script and even a recipe to how you could run it in a SLURM environment. An additional level of debug is to add `NCCL\\_DEBUG=INFO` environment variable as follows:\n```bash\nNCCL\\_DEBUG=INFO python -m torch.distributed.run --nproc\\_per\\_node 2 --nnodes 1 torch-distributed-gpu-test.py\n```\nThis will dump a lot of NCCL-related debug information, which you can then search online if you find that some problems are reported. Or if you're not sure how to interpret the output you can share the log file in an Issue. ## Underflow and Overflow Detection\nThis feature is currently available for PyTorch-only. For multi-GPU training it requires DDP (`torch.distributed.launch`). This feature can be used with any `nn.Module`-based model. If you start getting `loss=NaN` or the model inhibits some other abnormal behavior due to `inf` or `nan` in\nactivations or weights one needs to discover where the first underflow or overflow happens and what led to it. Luckily\nyou can accomplish that easily by activating a special module that will do the detection automatically. If you're using [`Trainer`], you just need to add:\n```bash\n--debug underflow\\_overflow\n```\nto the normal command line arguments, or pass `debug=\""underflow\\_overflow\""` when creating the\n[`TrainingArguments`] object. If you're using your own training loop or another Trainer you can accomplish the same with:\n```python\nfrom transformers.debug\\_utils import DebugUnderflowOverflow\ndebug\\_overflow = DebugUnderflowOverflow(model)\n```\n[`~debug\\_utils.DebugUnderflowOverflow`] inserts hooks into the model that immediately after each\nforward call will test input and output variables and also the corresponding module's weights. As soon as `inf` or\n`nan` is detected in at least one element of the activations or weights, the program will assert and print a report\nlike this (this was caught with `google/mt5-small` under fp16 mixed precision):\n```\nDetected inf/nan during batch\\_number=0\nLast 21 forward frames:\nabs min abs max metadata\nencoder.block.1.layer.1.DenseReluDense.dropout Dropout\n0.00e+00 2.57e+02 input[0]\n0.00e+00 2.85e+02 output\n[...]\nencoder.block.2.layer.0 T5LayerSelfAttention\n6.78e-04 3.15e+03 input[0]\n2.65e-04 3.42e+03 output[0]\nNone output[1]\n2.25e-01 1.00e+04 output[2]\nencoder.block.2.layer.1.layer\\_norm T5LayerNorm\n8.69e-02 4.18e-01 weight\n2.65e-04 3.42e+03 input[0]\n1.79e-06 4.65e+00 output\nencoder.block.2.layer.1.DenseReluDense.wi\\_0 Linear\n2.17e-07 4.50e+00 weight\n1.79e-06 4.65e+00 input[0]\n2.68e-06 3.70e+01 output\nencoder.block.2.layer.1.DenseReluDense.wi\\_1 Linear\n8.08e-07 2.66e+01 weight\n1.79e-06 4.65e+00 input[0]\n1.27e-04 2.37e+02 output\nencoder.block.2.layer.1.DenseReluDense.dropout Dropout\n0.00e+00 8.76e+03 input[0]\n0.00e+00 9.74e+03 output\nencoder.block.2.layer.1.DenseReluDense.wo Linear\n1.01e-06 6.44e+00 weight\n0.00e+00 9.74e+03 input[0]\n3.18e-04 6.27e+04 output\nencoder.block.2.layer.1.DenseReluDense T5DenseGatedGeluDense\n1.79e-06 4.65e+00 input[0]\n3.18e-04 6.27e+04 output\nencoder.block.2.layer.1.dropout Dropout\n3.18e-04 6.27e+04 input[0]\n0.00e+00 inf output\n```\nThe example output has been trimmed in the middle for brevity."", ""\\* correct / len(test\\_loader.dataset)}')\n```\nTypically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook. However, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through \\*distributed\\* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\n## PyTorch Distributed Data Parallelism\nAs the name implies, `torch.distributed` is meant to work on \\*distributed\\* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both. To convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp\\_tutorial.html)\nFirst a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through\n> Note: for this section of the tutorial it should be assumed these are sent in python script files. Later on a launcher using Accelerate will be discussed that removes this necessity\n```python\nimport os\nimport torch.distributed as dist\ndef setup(rank, world\\_size):\n\""Sets up the process group and configuration for PyTorch Distributed Data Parallelism\""\nos.environ[\""MASTER\\_ADDR\""] = 'localhost'\nos.environ[\""MASTER\\_PORT\""] = \""12355\""\n# Initialize the process group\ndist.init\\_process\\_group(\""gloo\"", rank=rank, world\\_size=world\\_size)\ndef cleanup():\n\""Cleans up the distributed environment\""\ndist.destroy\\_process\\_group()\n```\nThe last piece of the puzzle is \\*how do I send my data and model to another GPU?\\*\nThis is where the `DistributedDataParallel` module comes into play. It will copy your model onto each GPU, and when `loss.backward()` is called the backpropagation is performed and the resulting gradients across all these copies of the model will be averaged/reduced. This ensures each device has the same weights post the optimizer step. Below is an example of our training setup, refactored as a function, with this capability:\n> Note: Here rank is the overall rank of the current GPU compared to all the other GPUs available, meaning they have a rank of `0 -> n-1`\n```python\nfrom torch.nn.parallel import DistributedDataParallel as DDP\ndef train(model, rank, world\\_size):\nsetup(rank, world\\_size)\nmodel = model.to(rank)\nddp\\_model = DDP(model, device\\_ids=[rank])\noptimizer = optim.AdamW(ddp\\_model.parameters(), lr=1e-3)\n# Train for one epoch\nmodel.train()\nfor batch\\_idx, (data, target) in enumerate(train\\_loader):\ndata, target = data.to(device), target.to(device)\noutput = model(data)\nloss = F.nll\\_loss(output, target)\nloss.backward()\noptimizer.step()\noptimizer.zero\\_grad()\ncleanup()\n```\nThe optimizer needs to be declared based on the model \\*on the specific device\\* (so `ddp\\_model` and not `model`) for all of the gradients to properly be calculated. Lastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\n```bash\ntorchrun --nproc\\_per\\_node=2 --nnodes=1 example\\_script.py\n```\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch."", ""When you will face a real project that will be running for hours and perhaps days, definitely spend more time to make sure you use the most optimal hyper-parameters to get your job done faster and at a minimal cost. If you would like to experiment with this benchmark yourself or want to know more details about the hardware and software used to run it, please, refer to [this post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400). # Fitting A Huge Model Onto One GPU\nWhile Fairscale gives us a boost only with multiple GPUs, DeepSpeed has a gift even for those of us with a single GPU. Let's try the impossible - let's train [t5-3b](https://huggingface.co/t5-3b) on a 24GB RTX-3090 card. First let's try to finetune the huge `t5-3b` using the normal single GPU setup:\n```\nexport BS=1\nCUDA\\_VISIBLE\\_DEVICES=0 ./finetune\\_trainer.py \\\n--model\\_name\\_or\\_path t5-3b --n\\_train 60 --n\\_val 10 \\\n--per\\_device\\_eval\\_batch\\_size $BS --per\\_device\\_train\\_batch\\_size $BS \\\n--task translation\\_en\\_to\\_ro --fp16 [...]\n```\nNo cookie, even with BS=1 we get:\n```\nRuntimeError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.70 GiB total capacity;\n21.37 GiB already allocated; 45.69 MiB free; 22.05 GiB reserved in total by PyTorch)\n```\nNote, as earlier I'm showing only the important parts and the full command line arguments can be found\n[here](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685). Now update your `transformers` to v4.2.0 or higher, then install DeepSpeed:\n```\npip install deepspeed\n```\nand let's try again, this time adding DeepSpeed to the command line:\n```\nexport BS=20\nCUDA\\_VISIBLE\\_DEVICES=0 deepspeed --num\\_gpus=1 ./finetune\\_trainer.py \\\n--model\\_name\\_or\\_path t5-3b --n\\_train 60 --n\\_val 10 \\\n--per\\_device\\_eval\\_batch\\_size $BS --per\\_device\\_train\\_batch\\_size $BS \\\n--task translation\\_en\\_to\\_ro --fp16 --deepspeed ds\\_config\\_1gpu.json [...]\n```\net voila!""]","PyTorch provides the `torchrun` command line module to run a script on multiple GPUs. You can use it by passing in the number of nodes and the script to run, for example:

```bash
torchrun --nproc_per_node=2 --nnodes=1 example_script.py
```",torchrun
"What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?
","[""It's the first paper that successfully trains a Transformer encoder on ImageNet, attaining\nvery good results compared to familiar convolutional architectures. The abstract from the paper is the following:\n\\*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its\napplications to computer vision remain limited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional networks while keeping their overall\nstructure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),\nVision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring\nsubstantially fewer computational resources to train.\\*\n![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg)\n ViT architecture. Taken from the [original paper.](https://arxiv.org/abs/2010.11929) \nFollowing the original Vision Transformer, some follow-up works have been made:\n- [DeiT](deit) (Data-efficient Image Transformers) by Facebook AI. DeiT models are distilled vision transformers. The authors of DeiT also released more efficiently trained ViT models, which you can directly plug into [`ViTModel`] or\n[`ViTForImageClassification`]. There are 4 variants available (in 3 different sizes): \\*facebook/deit-tiny-patch16-224\\*,\n\\*facebook/deit-small-patch16-224\\*, \\*facebook/deit-base-patch16-224\\* and \\*facebook/deit-base-patch16-384\\*."", ""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Vision Encoder Decoder Models\n## Overview\nThe [`VisionEncoderDecoderModel`] can be used to initialize an image-to-text model with any\npretrained Transformer-based vision model as the encoder (\\*e.g.\\* [ViT](vit), [BEiT](beit), [DeiT](deit), [Swin](swin))\nand any pretrained language model as the decoder (\\*e.g.\\* [RoBERTa](roberta), [GPT2](gpt2), [BERT](bert), [DistilBERT](distilbert)). The effectiveness of initializing image-to-text-sequence models with pretrained checkpoints has been shown in (for\nexample) [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,\nZhoujun Li, Furu Wei."", ""With so many Transformer variants available, it can be easy to miss the bigger picture. What all these models have in common is they're based on the original Transformer architecture. Some models only use the encoder or decoder, while others use both. This provides a useful taxonomy to categorize and examine the high-level differences within models in the Transformer family, and it'll help you understand Transformers you haven't encountered before. If you aren't familiar with the original Transformer model or need a refresher, check out the [How do Transformers work](https://huggingface.co/course/chapter1/4?fw=pt) chapter from the Hugging Face course.## Computer vision\n### Convolutional network\nFor a long time, convolutional networks (CNNs) were the dominant paradigm for computer vision tasks until the [Vision Transformer](https://arxiv.org/abs/2010.11929) demonstrated its scalability and efficiency. Even then, some of a CNN's best qualities, like translation invariance, are so powerful (especially for certain tasks) that some Transformers incorporate convolutions in their architecture. [ConvNeXt](model\\_doc/convnext) flipped this exchange around and incorporated design choices from Transformers to modernize a CNN. For example, ConvNeXt uses non-overlapping sliding windows to patchify an image and a larger kernel to increase its global receptive field. ConvNeXt also makes several layer design choices to be more memory-efficient and improve performance, so it competes favorably with Transformers! ### Encoder[[cv-encoder]]\nThe [Vision Transformer (ViT)](model\\_doc/vit) opened the door to computer vision tasks without convolutions. ViT uses a standard Transformer encoder, but its main breakthrough was how it treated an image. It splits an image into fixed-size patches and uses them to create an embedding, just like how a sentence is split into tokens. ViT capitalized on the Transformers' efficient architecture to demonstrate competitive results with the CNNs at the time while requiring fewer resources to train. ViT was soon followed by other vision models that could also handle dense vision tasks like segmentation as well as detection. One of these models is the [Swin](model\\_doc/swin) Transformer. It builds hierarchical feature maps (like a CNN 👀 and unlike ViT) from smaller-sized patches and merges them with neighboring patches in deeper layers. Attention is only computed within a local window, and the window is shifted between attention layers to create connections to help the model learn better. Since the Swin Transformer can produce hierarchical feature maps, it is a good candidate for dense prediction tasks like segmentation and detection. The [SegFormer](model\\_doc/segformer) also uses a Transformer encoder to build hierarchical feature maps, but it adds a simple multilayer perceptron (MLP) decoder on top to combine all the feature maps and make a prediction. Other vision models, like BeIT and ViTMAE, drew inspiration from BERT's pretraining objective. [BeIT](model\\_doc/beit) is pretrained by \\*masked image modeling (MIM)\\*; the image patches are randomly masked, and the image is also tokenized into visual tokens. BeIT is trained to predict the visual tokens corresponding to the masked patches. [ViTMAE](model\\_doc/vitmae) has a similar pretraining objective, except it must predict the pixels instead of visual tokens.""]","The most popular vision transformer model on the Hugging Face Model Hub for image classification is the Vision Transformer (ViT). It has demonstrated competitive results in image classification tasks compared to state-of-the-art convolutional networks while requiring fewer computational resources to train. Additionally, follow-up models like DeiT (Data-efficient Image Transformers) have also gained popularity, offering distilled versions of vision transformers.",google/vit-base-patch16-224
"What is the command to upload an ESPnet model to a Hugging Face repository?
","[""Using ESPnet at Hugging Face\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks. ## Exploring ESPnet in the Hub\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). All models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more. 2. Metadata tags that help for discoverability and contain information such as license, language and datasets. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png)\n\n## Using existing models\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\_model\\_zoo)). If you're interested in doing inference, different classes for different tasks have a `from\\_pretrained` method that allows loading models from the Hub."", ""n this video, we're going to understand how to manage a model repository on the HuggingFace model hub. In order to handle a repository, you should first have a Hugging Face account. A link to create a new account is available in the description. Once you are logged in, you can create a new repository by clicking on the \""New model\"" option. You should be facing a similar modal to the following. In the \""Owner\"" input, you can put either your own namespace or any of your organisations namespaces. The model name is the model identifier that will then be used to identify your model on your chosen namespace. The final choice is between public and private. Public models are accessible by anyone. This is the recommended, free option, as this makes your model easily accessible and shareable. The owners of your namespace are the only ones who can update and change your model. A more advanced option is the private option. In this case, only the owners of your namespace will have visibility over your model. Other users won't know it exists and will not be able to use it. Let's create a dummy model to play with. Once your model is created, comes the management of that model! Three tabs are available to you. You're facing the first one, which is the model card page; this is the page used to showcase your model to the world."", ""To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard. ## Usage Tips\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks. 2. Address slow response times or performance issues by changing model endpoints to lower resource models. ## Conclusion\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!""]","To upload an ESPnet model to a Hugging Face repository, you can use the command `transformers-cli upload <model_directory>`. Make sure to replace `<model_directory>` with the path to your model directory. For detailed instructions, you may refer to the official guide on loading pre-trained models.",./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo
"What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?
","[""Inference Endpoints Version\nHugging Face Inference Endpoints comes with a default serving container which is used for all [supported Transformers and Sentence-Transformers tasks](/docs/inference-endpoints/supported\\_tasks) and for [custom inference handler](/docs/inference-endpoints/guides/custom\\_handler) and implement batching. Below you will find information about the installed packages and versions used. You can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository. Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom\\_dependencies). ## Installed packages & version\nThe installed packages are split into `general`, `CPU` & `GPU` packages. The `general` packages are installed in all containers, the `CPU` and `GPU` packages are only installed in the corresponding containers. The Hugging Face Inference Runtime has separate versions for `PyTorch` and `TensorFlow` for `CPU` and `GPU`, which are used based on the selected `framework` when creating an Inference Endpoint. The `TensorFlow` and `PyTorch` flavors are grouped together in the list below. ### General\n- `Python`: `3.9.13`\n### CPU\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\n- `diffusers`: `0.14.0`\n- `accelerate`: `0.17.1`\n- `sentence\\_transformers`: `latest`\n- `pandas`: `latest`\n- `pytorch`: `1.13.1`\n- `torchvision`: `0.14.1`\n- `tensorflow`: `2.9.1`\n### GPU\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\n- `diffusers`: `0.14.0`\n- `accelerate`: `0.17.1`\n- `sentence\\_transformers`: `latest`\n- `pandas`: `latest`\n- `pytorch`: `1.13.1=py3.9\\_cuda11.8\\*`\n- `torchvision`: `0.14.1`\n- `tensorflow`: `2.9.1=\\*cuda112\\*py39\\*`"", ""Add custom Dependencies\nInference Endpoints’ base image includes all required libraries to run inference on 🤗 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\\* [customize your inference pipeline](/docs/inference-endpoints/guides/custom\\_handler) and need additional Python dependencies\n\\* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)). To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within. ```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\\* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n\\* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom\\_container) for inference."", ""You can use `scipy` in Python to write it to a .wav file. ```python\nimport scipy\nimport numpy as np\n# output is [[-0.182352,-0.17802449, ...]]\nscipy.io.wavfile.write(\""musicgen\\_out.wav\"", rate=32000, data=np.array(output[0]))\n```\nAnd voila! Play with the demo below to try the endpoint out. ## Conclusion\nIn this blog post, we have shown how to deploy MusicGen using Inference Endpoints with a custom inference handler. The same technique can be used for any other model in the Hub that does not have an associated pipeline. All you have to do is override the `Endpoint Handler` class in `handler.py`, and add `requirements.txt` to reflect your project's dependencies. ### Read More\n- [Inference Endpoints documentation covering Custom Handler](https://huggingface.co/docs/inference-endpoints/guides/custom\\_handler)""]","To install custom Python dependencies for Inference Endpoints, you should add a `requirements.txt` file to your model repository on the Hugging Face Hub. This file should list the Python dependencies you want to install.",requirements.txt
"How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?
","[""🎉 To learn more about how to use your new model, the following guides may be helpful:\n- Learn how to [load Textual Inversion embeddings](../using-diffusers/loading\\_adapters) and also use them as negative embeddings. - Learn how to use [Textual Inversion](textual\\_inversion\\_inference) for inference with Stable Diffusion 1/2 and Stable Diffusion XL."", ""ote: This is a simplified version of the code needed to create the Stable Diffusion demo."", ""Stable Diffusion is a very powerful text-to-image model not only in terms of quality but also in terms of computational cost. Because Stable Diffusion was trained on an English dataset, it is required to translate non-English prompts to English first. Surprisingly, Stable Diffusion can sometimes generate proper images even when using non-English prompts. So, why do we need a language-specific Stable Diffusion? The answer is because we want a text-to-image model that can understand Japanese culture, identity, and unique expressions including slang. For example, one of the more common Japanese terms re-interpreted from the English word businessman is \""salary man\"" which we most often imagine as a man wearing a suit. Stable Diffusion cannot understand such Japanese unique words correctly because Japanese is not their target. ![salary man of stable diffusion](assets/106_japanese_stable_diffusion/sd.jpeg \""salary man of stable diffusion\"")\n\n\\*\""salary man, oil painting\"" from the original Stable Diffusion\\*\nSo, this is why we made a language-specific version of Stable Diffusion. Japanese Stable Diffusion can achieve the following points compared to the original Stable Diffusion. - Generate Japanese-style images\n- Understand Japanese words adapted from English\n- Understand Japanese unique onomatope\n- Understand Japanese proper noun\n### Training Data\nWe used approximately 100 million images with Japanese captions, including the Japanese subset of [LAION-5B](https://laion.ai/blog/laion-5b/).""]","To teach new concepts to Stable Diffusion using Textual Inversion, you typically need around 5 to 20 images of the concept you want to teach. This small number of images allows the model to learn the new concept effectively.",3-5 images
"What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?
","[""In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future. ## Sharded checkpoints\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save\\_pretrained(save\\_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in. You can control the maximum size before sharding with the `max\\_shard\\_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model. ```py\nfrom transformers import AutoModel\nmodel = AutoModel.from\\_pretrained(\""bert-base-cased\"")\n```\nIf you save it using [`~PreTrainedModel.save\\_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n```py\n>>> import os\n>>> import tempfile\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir)\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model.bin']\n```\nNow let's use a maximum shard size of 200MB:\n```py\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model-00001-of-00003.bin', 'pytorch\\_model-00002-of-00003.bin', 'pytorch\\_model-00003-of-00003.bin', 'pytorch\\_model.bin.index.json']\n```\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index."", ""This is often because of some missing information in the docstring or some incorrect\nnaming. The Hugging Face team will surely help you if you're stuck here. Lastly, it is always a good idea to refactor one's code after having ensured that the code works correctly. With all\ntests passing, now it's a good time to go over the added code again and do some refactoring. You have now finished the coding part, congratulation! 🎉 You are Awesome! 😎\n\\*\\*12. Upload the models to the model hub\\*\\*\nIn this final part, you should convert and upload all checkpoints to the model hub and add a model card for each\nuploaded model checkpoint. You can get familiar with the hub functionalities by reading our [Model sharing and uploading Page](model\\_sharing). You should work alongside the Hugging Face team here to decide on a fitting name for each\ncheckpoint and to get the required access rights to be able to upload the model under the author's organization of\n\\*brand\\_new\\_bert\\*. The `push\\_to\\_hub` method, present in all models in `transformers`, is a quick and efficient way to push your checkpoint to the hub. A little snippet is pasted below:\n```python\nbrand\\_new\\_bert.push\\_to\\_hub(\""brand\\_new\\_bert\"")\n# Uncomment the following line to push to an organization. # brand\\_new\\_bert.push\\_to\\_hub(\""/brand\\_new\\_bert\"")\n```\nIt is worth spending some time to create fitting model cards for each checkpoint. The model cards should highlight the\nspecific characteristics of this particular checkpoint, \\*e.g.\\* On which dataset was the checkpoint\npretrained/fine-tuned on?"", ""Since it is normal that the exact same model written\nin different libraries can give a slightly different output depending on\nthe library framework, we accept an error tolerance of 1e-3 (0.001). It\nis not enough if the model gives nearly the same output, they have to be\nthe almost identical. Therefore, you will certainly compare the\nintermediate outputs of the 🤗 Transformers version multiple times\nagainst the intermediate outputs of the original implementation of\n\\*BigBird\\* in which case an \\*\\*efficient\\*\\* debugging environment\nof the original repository is absolutely important. Here is some advice\nto make your debugging environment as efficient as possible. - Find the best way of debugging intermediate results. Is the original\nrepository written in PyTorch? Then you should probably take the\ntime to write a longer script that decomposes the original model\ninto smaller sub-components to retrieve intermediate values. Is the\noriginal repository written in Tensorflow 1? Then you might have to\nrely on TensorFlow print operations like\n[tf.print](https://www.tensorflow.org/api\\_docs/python/tf/print) to\noutput intermediate values. Is the original repository written in\nJax? Then make sure that the model is \\*\\*not jitted\\*\\* when running\nthe forward pass, \\*e.g.\\*, check-out [this\nlink](https://github.com/google/jax/issues/196). - Use the smallest pretrained checkpoint you can find. The smaller the\ncheckpoint, the faster your debug cycle becomes. It is not efficient\nif your pretrained model is so big that your forward pass takes more\nthan 10 seconds. In case only very large checkpoints are available,\nit might make more sense to create a dummy model in the new\nenvironment with randomly initialized weights and save those weights\nfor comparison with the 🤗 Transformers version of your model\n- Make sure you are using the easiest way of calling a forward pass in\nthe original repository.""]","In Transformers version 4.18.0, the maximum size of a model checkpoint before it is automatically sharded is 10GB.",10GB
"What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?
","[""They enlarged the scope rapidly with a writing assistant working as a browser extension that automatically fixes and explains potential bias in emails, Linkedin posts, job ads, etc. The aim was to offer a solution for internal and external communication that fosters a cultural change by providing micro-learning bites that explain the underlying bias of highlighted words and phrases. ![](/blog/assets/78_ml_director_insights/wittyworks.png)  \n*Example of suggestions by the writing assistant*\n\n### First experiments\nWitty Works first chose a basic machine learning approach to build their assistant from scratch. Using transfer learning with pre-trained spaCy models, the assistant was able to:\n- Analyze text and transform words into lemmas,\n- Perform a linguistic analysis,\n- Extract the linguistic features from the text (plural and singular forms, gender), part-of-speech tags (pronouns, verbs, nouns, adjectives, etc.), word dependencies labels, named entity recognition, etc. By detecting and filtering words according to a specific knowledge base using linguistic features, the assistant could highlight non-inclusive words and suggest alternatives in real-time. ### Challenge\nThe vocabulary had around 2300 non-inclusive words and idioms in German and English correspondingly. And the above described basic approach worked well for 85% of the vocabulary but failed for context-dependent words. Therefore the task was to build a context-dependent classifier of non-inclusive words. Such a challenge (understanding the context rather than recognizing linguistic features) led to using Hugging Face transformers. ```diff\nExample of context dependent non-inclusive words:\nFossil fuels are not renewable resources."", ""Whether and to what extent bias in the system concretely affects people ultimately depends on what the system is used for. As such, the first place developers can work to mitigate bias is when deciding how ML fits in their system, e.g., by deciding what optimization objective it will use. For example, let’s go back to one of the first highly-publicized cases of a Machine Learning system used in production for algorithmic content recommendation. From 2006 to 2009, Netflix ran the [Netflix Prize](https://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings/The-Netflix-Prize-Bennett.pdf), a competition with a 1M$ cash prize challenging teams around the world to develop ML systems to accurately predict a user’s rating for a new movie based on their past ratings. The [winning submission](https://www.asc.ohio-state.edu/statistics/dmsl/GrandPrize2009\\_BPC\\_BigChaos.pdf) improved the RMSE (Root-mean-square-error) of predictions on unseen user-movie pairs by over 10% over Netflix’s own CineMatch algorithm, meaning it got much better at predicting how users would rate a new movie based on their history."", ""Still, some numbers are hard to ignore. ### So? Am I excited by Megatron-Turing NLG 530B and whatever beast is coming next? No. Do I think that the (relatively small) benchmark improvement is worth the added cost, complexity and carbon footprint? No. Do I think that building and promoting these huge models is helping organizations understand and adopt Machine Learning ? No. I'm left wondering what's the point of it all. Science for the sake of science? Good old marketing? Technological supremacy? Probably a bit of each. I'll leave them to it, then. Instead, let me focus on pragmatic and actionable techniques that you can all use to build high quality Machine Learning solutions. ### Use Pretrained Models\nIn the vast majority of cases, you won't need a custom model architecture. Maybe you'll \\*want\\* a custom one (which is a different thing), but there be dragons. Experts only! A good starting point is to look for [models](https://huggingface.co/models) that have been pretrained for the task you're trying to solve (say, [summarizing English text](https://huggingface.co/models?language=en&pipeline\\_tag=summarization&sort=downloads)). Then, you should quickly try out a few models to predict your own data. If metrics tell you that one works well enough, you're done! If you need a little more accuracy, you should consider fine-tuning the model (more on this in a minute). ### Use Smaller Models\nWhen evaluating models, you should pick the smallest one that can deliver the accuracy you need. It will predict faster and require fewer hardware resources for training and inference. Frugality goes a long way. It's nothing new either. Computer Vision practitioners will remember when [SqueezeNet](https://arxiv.org/abs/1602.07360) came out in 2017, achieving a 50x reduction in model size compared to [AlexNet](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html), while meeting or exceeding its accuracy. How clever that was! Downsizing efforts are also under way in the Natural Language Processing community, using transfer learning techniques such as [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge\\_distillation). [DistilBERT](https://arxiv.org/abs/1910.01108) is perhaps its most widely known achievement. Compared to the original BERT model, it retains 97% of language understanding while being 40% smaller and 60% faster. You can try it [here](https://huggingface.co/distilbert-base-uncased). The same approach has been applied to other models, such as Facebook's [BART](https://arxiv.org/abs/1910.13461), and you can try DistilBART [here](https://huggingface.co/models?search=distilbart). Recent models from the [Big Science](https://bigscience.huggingface.co/) project are also very impressive. As visible in this graph included in the [research paper](https://arxiv.org/abs/2110.08207), their T0 model outperforms GPT-3 on many tasks while being 16x smaller. `![](assets/33_large_language_models/02_t0.png)`\nYou can try T0 [here](https://huggingface.co/bigscience/T0pp).""]","I'm unable to provide information on Weights and Biases (W&B) as it is not related to HuggingFace or gradio-app. If you have questions specifically about HuggingFace or gradio-app, feel free to ask!","To track their machine learning experiments at every stage, from training to production."
"What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?
","[""--\ntitle: \""Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\""\nthumbnail: /blog/assets/60\\_habana/habana.png\nauthors:\n- user: susanlansing\nguest: true\n---\n# Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\n\\*Santa Clara and San Francisco, CA, April 12th, 2022\\*\nPowered by deep learning, transformer models deliver state-of-the-art performance on a wide range of machine learning tasks, such as natural language processing, computer vision, speech, and more. However, training them at scale often requires a large amount of computing power, making the whole process unnecessarily long, complex, and costly. Today, [Habana® Labs](https://habana.ai/), a pioneer in high-efficiency, purpose-built deep learning processors, and Hugging Face, the home of [Transformer](https://github.com/huggingface/transformers) models, are happy to announce that they’re joining forces to make it easier and quicker to train high-quality transformer models. Thanks to the integration of Habana’s [SynapseAI software suite](https://habana.ai/training-software/) with the Hugging Face [Optimum open-source library](https://github.com/huggingface/optimum), data scientists and machine learning engineers can now accelerate their Transformer training jobs on Habana processors with just a few lines of code and enjoy greater productivity as well as lower training cost. [Habana Gaudi](https://habana.ai/training/) training solutions, which power Amazon’s EC2 DL1 instances and Supermicro’s X12 Gaudi AI Training Server, deliver price/performance up to 40% lower than comparable training solutions and enable customers to train more while spending less. The integration of ten 100 Gigabit Ethernet ports onto every Gaudi processor enables system scaling from 1 to thousands of Gaudis with ease and cost-efficiency. Habana’s SynapseAI® is optimized—at inception—to enable Gaudi performance and usability, supports TensorFlow and PyTorch frameworks, with a focus on computer vision and natural language processing applications. With 60,000+ stars on Github, 30,000+ models, and millions of monthly visits, Hugging Face is one of the fastest-growing projects in open source software history, and the go-to place for the machine learning community. With its [Hardware Partner Program](https://huggingface.co/hardware), Hugging Face provides Gaudi’s advanced deep learning hardware with the ultimate Transformer toolset. This partnership will enable rapid expansion of the Habana Gaudi training transformer model library, bringing Gaudi efficiency and ease of use to a wide array of customer use cases like natural language processing, computer vision, speech, and more. “\\*We’re excited to partner with Hugging Face and its many open-source developers to address the growing demand for transformer models that benefit from the efficiency, usability, and scalability of the Gaudi training platform\\*”, said Sree Ganesan, head of software product management, Habana Labs. “Habana Gaudi brings a new level of efficiency to deep learning model training, and we’re super excited to make this performance easily accessible to Transformer users with minimal code changes through Optimum”, said Jeff Boudier, product director at Hugging Face. To learn how to get started training with Habana Gaudi, please visit [https://developer.habana.ai](https://developer.habana.ai)."", ""--\ntitle: \""Introducing Optimum: The Optimization Toolkit for Transformers at Scale\""\nauthors:\n- user: mfuntowicz\n- user: echarlaix\n- user: michaelbenayoun\n- user: jeffboudier\n---\n# Introducing 🤗 Optimum: The Optimization Toolkit for Transformers at Scale\nThis post is the first step of a journey for Hugging Face to democratize\nstate-of-the-art \\*\\*Machine Learning production performance\\*\\*. To get there, we will work hand in hand with our\nHardware Partners, as we have with Intel below. Join us in this journey, and follow [Optimum](https://github.com/huggingface/optimum), our new open source library! ## Why 🤗 Optimum? ### 🤯 Scaling Transformers is hard\nWhat do Tesla, Google, Microsoft and Facebook all have in common? Well many things, but one of them is they all run billions of Transformer model predictions\nevery day. Transformers for AutoPilot to drive your Tesla (lucky you!),\nfor Gmail to complete your sentences,\nfor Facebook to translate your posts on the fly,\nfor Bing to answer your natural language queries. [Transformers](https://github.com/huggingface/transformers) have brought a step change improvement\nin the accuracy of Machine Learning models, have conquered NLP and are now expanding\nto other modalities starting with [Speech](https://huggingface.co/models?pipeline\\_tag=automatic-speech-recognition&sort=downloads)\nand [Vision](https://huggingface.co/models?pipeline\\_tag=image-classification&sort=downloads). But taking these massive models into production, and making them run fast at scale is a huge challenge\nfor any Machine Learning Engineering team. What if you don’t have hundreds of highly skilled Machine Learning Engineers on payroll like the above companies? Through Optimum, our new open source library, we aim to build the definitive toolkit for Transformers production performance,\nand enable maximum efficiency to train and run models on specific hardware. ### 🏭 Optimum puts Transformers to work\nTo get optimal performance training and serving models, the model acceleration techniques need to be specifically compatible with the targeted hardware. Each hardware platform offers specific software tooling,\n[features and knobs that can have a huge impact on performance](https://huggingface.co/blog/bert-cpu-scaling-part-1). Similarly, to take advantage of advanced model acceleration techniques like sparsity and quantization, optimized kernels need to be compatible with the operators on silicon,\nand specific to the neural network graph derived from the model architecture. Diving into this 3-dimensional compatibility matrix and how to use model acceleration libraries is daunting work,\nwhich few Machine Learning Engineers have experience on. [Optimum](https://github.com/huggingface/optimum) aims to make this work easy, providing performance optimization tools targeting efficient AI hardware,\nbuilt in collaboration with our Hardware Partners, and turn Machine Learning Engineers into ML Optimization wizards. With the [Transformers](https://github.com/huggingface/transformers) library, we made it easy for researchers and engineers to use state-of-the-art models,\nabstracting away the complexity of frameworks, architectures and pipelines. With the [Optimum](https://github.com/huggingface/optimum) library, we are making it easy for engineers to leverage all the available hardware features at their disposal,\nabstracting away the complexity of model acceleration on hardware platforms. ## 🤗 Optimum in practice: how to quantize a model for Intel Xeon CPU\n### 🤔 Why quantization is important but tricky to get right\nPre-trained language models such as BERT have achieved state-of-the-art results on a wide range of natural language processing tasks,\nother Transformer based models such as ViT and Speech2Text have achieved state-of-the-art results on computer vision and speech tasks respectively:\ntransformers are everywhere in the Machine Learning world and are here to stay."", ""elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem: how to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As you can see, it's divided in three sections which become progressively more advanced. At this stage, the first two sections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on your own dataset and share the result with the community. The second will dive deeper into our libraries and teach you how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the spring of 2022. The first chapter requires no technical knowledge and is a good introduction to learn what Transformers models can do and how they could be of use to you or your company. The next chapters require a good knowledge of Python and some basic knowledge of Machine Learning and Deep Learning. If you don't know what a training and validation set is or what gradient descent means, you should look at an introductory course such as the ones published by deeplearning.ai or fast.ai. It's also best if you have some basics in one Deep Learning Framework (PyTorch or TensorFlow). Each part of the material introduced in this course has a version in both those frameworks, so you will be able to pick the one you are most comfortable with. This is the team that developed this course. I'll now let each of the speakers introduce themselves briefly.""]",The name of the open-source library created by Hugging Face to simplify Transformer acceleration is 🤗 Optimum.,Optimum
"What parameter is used to ensure that elements in a row have the same height in Gradio?
","[""The Row will wrap if there isn't sufficient space to satisfy all `min\\_width` values. Learn more about Rows in the [docs](https://gradio.app/docs/#row). ## Columns and Nesting\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n$code\\_rows\\_and\\_columns\n$demo\\_rows\\_and\\_columns\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. Learn more about Columns in the [docs](https://gradio.app/docs/#column). # Dimensions\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases. Below is an example illustrating the use of viewport width (vw):\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\nim = gr.ImageEditor(\nwidth=\""50vw\"",\n)\ndemo.launch()\n```\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n```python\nimport gradio as gr\ncss = \""\""\""\n.container {\nheight: 100vh;\n}\n\""\""\""\nwith gr.Blocks(css=css) as demo:\nwith gr.Column(elem\\_classes=[\""container\""]):\nname = gr.Chatbot(value=[[\""1\"", \""2\""]], height=\""70%\"")\ndemo.launch()\n```\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height."", ""No changes are required to your code to benefit from this enhanced experience; simply upgrade your gradio SDK to the latest version. ![](https://user-images.githubusercontent.com/12937446/219653294-86937632-72c1-4e93-a77c-af705d49382a.png)\nThis behaviour is configurable. You can disable the info panel at the bottom by passing `info=\""false\""`. You can disable the container entirely by passing `container=\""false\""`. Error statuses are reported in the UI with an easy way for end-users to report problems to the original space author via the community tab of that Hugginface space:\n![](https://user-images.githubusercontent.com/12937446/219655499-88019443-d694-44e7-9e6d-242e19d10a5c.png)\nBy default, gradio apps are lazy loaded, vastly improving performance when there are several demos on the page. Metadata is loaded ahead of time, but the space will only be loaded and rendered when it is in view. This behaviour is configurable. You can pass `eager=\""true\""` to load and render the space regardless of whether or not it is currently on the screen. by [@pngwn](https://github.com/pngwn) in [PR 3205](https://github.com/gradio-app/gradio/pull/3205)\n###### New `gr.BarPlot` component!"", ""his demo shows how you can build a live interactive dashboard with gradio. The current time is refreshed every second and the plot every half second by using the 'every' keyword in the event handler. Changing the value of the slider will control the period of the sine curve (the distance between peaks).""]","In Gradio, the `equal_height` parameter is used to ensure that elements in a row have the same height.",equal_height
"What is the command to install the latest version of Optimum with OpenVINO support?
","[""​\nQuantization is a process that lowers memory and compute requirements by reducing the bit width of model parameters. Reducing the number of bits means that the resulting model requires less memory at inference time, and that operations like matrix multiplication can be performed faster thanks to integer arithmetic. First, let's create a virtual environment and install all dependencies.​\n```bash\nvirtualenv openvino\nsource openvino/bin/activate\npip install pip --upgrade\npip install optimum[openvino,nncf] torchvision evaluate\n```\nNext, moving to a Python environment, we import the appropriate modules and download the original model as well as its processor. ​\n```python\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\n​\nmodel\\_id = \""juliensimon/autotrain-food101-1471154050\""\nmodel = AutoModelForImageClassification.from\\_pretrained(model\\_id)\nprocessor = AutoImageProcessor.from\\_pretrained(model\\_id)\n```\n​\nPost-training static quantization requires a calibration step where data is fed through the network in order to compute the quantized activation parameters. Here, we take 300 samples from the original dataset to build the calibration dataset. ​\n```python\nfrom optimum.intel.openvino import OVQuantizer\n​\nquantizer = OVQuantizer.from\\_pretrained(model)\ncalibration\\_dataset = quantizer.get\\_calibration\\_dataset(\n\""food101\"",\nnum\\_samples=300,\ndataset\\_split=\""train\"",\n)\n```\nAs usual with image datasets, we need to apply the same image transformations that were used at training time. We use the preprocessing defined in the processor. We also define a data collation function to feed the model batches of properly formatted tensors. ​\n```python\nimport torch\nfrom torchvision.transforms import (\nCenterCrop,\nCompose,\nNormalize,\nResize,\nToTensor,\n)\n​\nnormalize = Normalize(mean=processor.image\\_mean, std=processor.image\\_std)\nsize = processor.size[\""height\""]\n\\_val\\_transforms = Compose(\n[\nResize(size),\nCenterCrop(size),\nToTensor(),\nnormalize,\n]\n)\ndef val\\_transforms(example\\_batch):\nexample\\_batch[\""pixel\\_values\""] = [\\_val\\_transforms(pil\\_img.convert(\""RGB\"")) for pil\\_img in example\\_batch[\""image\""]]\nreturn example\\_batch\n​\ncalibration\\_dataset.set\\_transform(val\\_transforms)\n​\ndef collate\\_fn(examples):\npixel\\_values = torch.stack([example[\""pixel\\_values\""] for example in examples])\nlabels = torch.tensor([example[\""label\""] for example in examples])\nreturn {\""pixel\\_values\"": pixel\\_values, \""labels\"": labels}\n```\nFor our first attempt, we use the default configuration for quantization. You can also specify the number of samples to use during the calibration step, which is by default 300. ```python\nfrom optimum.intel.openvino import OVConfig\n​\nquantization\\_config = OVConfig()\nquantization\\_config.compression[\""initializer\""][\""range\""][\""num\\_init\\_samples\""] = 300\n```\nWe're now ready to quantize the model. The `OVQuantizer.quantize()` method quantizes the model and exports it to the OpenVINO format. The resulting graph is represented with two files: an XML file describing the network topology and a binary file describing the weights. The resulting model can run on any target Intel® device. ```python\nsave\\_dir = \""quantized\\_model\""\n# Apply static quantization and export the resulting quantized model to OpenVINO IR format\nquantizer.quantize(\nquantization\\_config=quantization\\_config,\ncalibration\\_dataset=calibration\\_dataset,\ndata\\_collator=collate\\_fn,\nremove\\_unused\\_columns=False,\nsave\\_directory=save\\_dir,\n)\nprocessor.save\\_pretrained(save\\_dir)\n```\nA minute or two later, the model has been quantized. We can then easily load it with our [`OVModelForXxx`](https://huggingface.co/docs/optimum/intel/inference) classes, the equivalent of the Transformers [`AutoModelForXxx`](https://huggingface.co/docs/transformers/main/en/autoclass\\_tutorial#automodel) classes found in the `transformers` library. Likewise, we can create [pipelines](https://huggingface.co/docs/transformers/main/en/main\\_classes/pipelines) and run inference with [OpenVINO Runtime](https://docs.openvino.ai/latest/openvino\\_docs\\_OV\\_UG\\_OV\\_Runtime\\_User\\_Guide.html). ​\n```python\nfrom transformers import pipeline\nfrom optimum.intel.openvino import OVModelForImageClassification\n​\nov\\_model = OVModelForImageClassification.from\\_pretrained(save\\_dir)\nov\\_pipe = pipeline(\""image-classification\"", model=ov\\_model, image\\_processor=processor)\noutputs = ov\\_pipe(\""http://farm2.staticflickr.com/1375/1394861946\\_171ea43524\\_z.jpg\"")\nprint(outputs)\n```\n​To verify that quantization did not have a negative impact on accuracy, we applied an evaluation step to compare the accuracy of the original model with its quantized counterpart."", ""You can find more examples in the 🤗 Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion-xl), and running SDXL in OpenVINO is supported for text-to-image and image-to-image."", ""The notebook that demonstrates the difference between optimized and original models is available [here](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/stable\\_diffusion\\_optimization.ipynb). You can also find [many models](https://huggingface.co/models?library=openvino&sort=downloads) on the Hugging Face Hub under the [OpenVINO organization](https://huggingface.co/OpenVINO). In addition, we have created a [demo](https://huggingface.co/spaces/helenai/stable\\_diffusion) on Hugging Face Spaces that is being run on a 3rd Generation Intel Xeon Scalable processor. ## What about the general-purpose Stable Diffusion model? As we showed with the Pokemon image generation task, it is possible to achieve a high level of optimization of the Stable Diffusion pipeline when using a relatively small amount of training resources. At the same time, it is well-known that training a general-purpose Stable Diffusion model is an [expensive task](https://www.mosaicml.com/blog/training-stable-diffusion-from-scratch-part-2).""]","To install the latest version of Optimum with OpenVINO support, you can use the following command:

```bash
pip install optimum[openvino,nncf] torchvision evaluate
```","pip install --upgrade-strategy eager optimum[""openvino""]"

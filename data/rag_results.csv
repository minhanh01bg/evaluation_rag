user_input,retrieved_contexts,response,reference
"What architecture is the `tokenizers-linux-x64-musl` binary designed for?
","[""`tokenizers-linux-x64-musl`\nThis is the \\*\\*x86\\_64-unknown-linux-musl\\*\\* binary for `tokenizers`"", ""This is more robust than preparing the pattern yourself: it avoids pitfalls, such as token glueing, that are very hard to debug. To see how much CPU and GPU memory you need for this model or others, try [this calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage) which can help determine that value. The LLaMA tokenizer is a BPE model based on [sentencepiece](https://github.com/google/sentencepiece). One quirk of sentencepiece is that when decoding a sequence, if the first token is the start of the word (e.g. \""Banana\""), the tokenizer does not prepend the prefix space to the string. Code Llama has the same architecture as the `Llama2` models, refer to [Llama2's documentation page](llama2) for the API reference. Find Code Llama tokenizer reference below. ## CodeLlamaTokenizer\n[[autodoc]] CodeLlamaTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## CodeLlamaTokenizerFast\n[[autodoc]] CodeLlamaTokenizerFast\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- update\\_post\\_processor\n- save\\_vocabulary"", ""p align=\""center\"">\n\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\n\n[![Build](https://badge.fury.io/js/tokenizers.svg)](https://badge.fury.io/js/tokenizers)\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\n\nNodeJS implementation of today's most used tokenizers, with a focus on performance and\nversatility. Bindings over the [Rust](https://github.com/huggingface/tokenizers/tree/master/tokenizers) implementation. If you are interested in the High-level design, you can go check it there. ## Main features\n- Train new vocabularies and tokenize using 4 pre-made tokenizers (Bert WordPiece and the 3\nmost common BPE versions). - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes\nless than 20 seconds to tokenize a GB of text on a server's CPU. - Easy to use, but also extremely versatile. - Designed for research and production. - Normalization comes with alignments tracking. It's always possible to get the part of the\noriginal sentence that corresponds to a given token. - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs. ## Installation\n```bash\nnpm install tokenizers@latest\n```\n## Basic example\n```ts\nimport { Tokenizer } from \""tokenizers\"";\nconst tokenizer = await Tokenizer.fromFile(\""tokenizer.json\"");\nconst wpEncoded = await tokenizer.encode(\""Who is John?\"");\nconsole.log(wpEncoded.getLength());\nconsole.log(wpEncoded.getTokens());\nconsole.log(wpEncoded.getIds());\nconsole.log(wpEncoded.getAttentionMask());\nconsole.log(wpEncoded.getOffsets());\nconsole.log(wpEncoded.getOverflowing());\nconsole.log(wpEncoded.getSpecialTokensMask());\nconsole.log(wpEncoded.getTypeIds());\nconsole.log(wpEncoded.getWordIds());\n```\n## License\n[Apache License 2.0](../../LICENSE)"", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# NLLB\n## Updated tokenizer behavior\n\\*\\*DISCLAIMER:\\*\\* The default behaviour for the tokenizer was fixed and thus changed in April 2023. The previous version adds `[self.eos\\_token\\_id, self.cur\\_lang\\_code]` at the end of the token sequence for both target and source tokenization."", ""First, we `Prepend` character `\\_` to the input string. Then we replace all spaces with `\\_`. There's a huge list of potential operations, they can be applied to regular expression matches, and they have to be performed in a very specific order. The code in the `tokenizers` library takes care of all these details for all the models in the Hub. In contrast, projects that use language models in other domains, such as Swift apps, usually resort to hardcoding these decisions as part of the app's source code. This is fine for a couple of models, but then it's difficult to replace a model with a different one, and it's easy to make mistakes. What we are doing in `swift-transformers` is replicate those abstractions in Swift, so we write them once and everybody can use them in their apps. We are just getting started, so coverage is still small. Feel free to open issues in the repo or contribute your own! Specifically, we currently support BPE (Byte-Pair Encoding) tokenizers, one of the three main families in use today. The GPT models, Falcon and Llama, all use this method. Support for Unigram and WordPiece tokenizers will come later. We haven't ported all the possible normalizers, pre-tokenizers and post-processors - just the ones we encountered during our conversions of Llama 2, Falcon and GPT models. This is how to use the `Tokenizers` module in Swift:\n```swift\nimport Tokenizers\nfunc testTokenizer() async throws {\nlet tokenizer = try await AutoTokenizer.from(pretrained: \""pcuenq/Llama-2-7b-chat-coreml\"")\nlet inputIds = tokenizer(\""Today she took a train to the West\"")\nassert(inputIds == [1, 20628, 1183, 3614, 263, 7945, 304, 278, 3122])\n}\n```\nHowever, you don't usually need to tokenize the input text yourself - the [`Generation` code](https://github.com/huggingface/swift-transformers/blob/17d4bfae3598482fc7ecf1a621aa77ab586d379a/Sources/Generation/Generation.swift#L82) will take care of it.""]",The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.,x86_64-unknown-linux-musl
"What is the purpose of the BLIP-Diffusion model?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# BLIP-Diffusion\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. The abstract from the paper is:\n\\*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).\\*\nThe original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization. `BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## BlipDiffusionPipeline\n[[autodoc]] BlipDiffusionPipeline\n- all\n- \\_\\_call\\_\\_\n## BlipDiffusionControlNetPipeline\n[[autodoc]] BlipDiffusionControlNetPipeline\n- all\n- \\_\\_call\\_\\_"", ""For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.\\*\\*\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!"", ""A (denoising) diffusion model isn't that complex if you compare it to other generative models such as Normalizing Flows, GANs or VAEs: they all convert noise from some simple distribution to a data sample. This is also the case here where \\*\\*a neural network learns to gradually denoise data\\*\\* starting from pure noise. In a bit more detail for images, the set-up consists of 2 processes:\n\\* a fixed (or predefined) forward diffusion process \\\\(q\\\\) of our choosing, that gradually adds Gaussian noise to an image, until you end up with pure noise\n\\* a learned reverse denoising diffusion process \\\\(p\\_\\theta\\\\), where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image. ![](assets/78_annotated-diffusion/diffusion_figure.png)\n\nBoth the forward and reverse process indexed by \\\\(t\\\\) happen for some number of finite time steps \\\\(T\\\\) (the DDPM authors use \\\\(T=1000\\\\)). You start with \\\\(t=0\\\\) where you sample a real image \\\\(\\mathbf{x}\\_0\\\\) from your data distribution (let's say an image of a cat from ImageNet), and the forward process samples some noise from a Gaussian distribution at each time step \\\\(t\\\\), which is added to the image of the previous time step. Given a sufficiently large \\\\(T\\\\) and a well behaved schedule for adding noise at each time step, you end up with what is called an [isotropic Gaussian distribution](https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic) at \\\\(t=T\\\\) via a gradual process. ## In more mathematical form\nLet's write this down more formally, as ultimately we need a tractable loss function which our neural network needs to optimize."", ""--\ntitle: \""Finetune Stable Diffusion Models with DDPO via TRL\""\nthumbnail: /blog/assets/166\\_trl\\_ddpo/thumbnail.png\nauthors:\n- user: metric-space\nguest: true\n- user: sayakpaul\n- user: kashif\n- user: lvwerra\n---\n# Finetune Stable Diffusion Models with DDPO via TRL\n## Introduction\nDiffusion models (e.g., DALL-E 2, Stable Diffusion) are a class of generative models that are widely successful at generating images most notably of the photorealistic kind. However, the images generated by these models may not always be on par with human preference or human intention. Thus arises the alignment problem i.e. how does one go about making sure that the outputs of a model are aligned with human preferences like “quality” or that outputs are aligned with intent that is hard to express via prompts? This is where Reinforcement Learning comes into the picture. In the world of Large Language Models (LLMs), Reinforcement learning (RL) has proven to become a very effective tool for aligning said models to human preferences. It’s one of the main recipes behind the superior performance of systems like ChatGPT. More precisely, RL is the critical ingredient of Reinforcement Learning from Human Feedback (RLHF), which makes ChatGPT chat like human beings. In [Training Diffusion Models with Reinforcement Learning, Black](https://arxiv.org/abs/2305.13301) et al. show how to augment diffusion models to leverage RL to fine-tune them with respect to an objective function via a method named Denoising Diffusion Policy Optimization (DDPO). In this blog post, we discuss how DDPO came to be, a brief description of how it works, and how DDPO can be incorporated into an RLHF workflow to achieve model outputs more aligned with the human aesthetics. We then quickly switch gears to talk about how you can apply DDPO to your models with the newly integrated `DDPOTrainer` from the `trl` library and discuss our findings from running DDPO on Stable Diffusion. ## The Advantages of DDPO\nDDPO is not the only working answer to the question of how to attempt to fine-tune diffusion models with RL."", ""🧨 Diffusers Pipelines\nPipelines provide a simple way to run state-of-the-art diffusion models in inference. Most diffusion systems consist of multiple independently-trained models and highly adaptable scheduler\ncomponents - all of which are needed to have a functioning end-to-end diffusion system. As an example, [Stable Diffusion](https://huggingface.co/blog/stable\\_diffusion) has three independently trained models:\n- [Autoencoder](https://github.com/huggingface/diffusers/blob/5cbed8e0d157f65d3ddc2420dfd09f2df630e978/src/diffusers/models/vae.py#L392)\n- [Conditional Unet](https://github.com/huggingface/diffusers/blob/5cbed8e0d157f65d3ddc2420dfd09f2df630e978/src/diffusers/models/unet\\_2d\\_condition.py#L12)\n- [CLIP text encoder](https://huggingface.co/docs/transformers/main/en/model\\_doc/clip#transformers.CLIPTextModel)\n- a scheduler component, [scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling\\_pndm.py),\n- a [CLIPImageProcessor](https://huggingface.co/docs/transformers/main/en/model\\_doc/clip#transformers.CLIPImageProcessor),\n- as well as a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\_diffusion/safety\\_checker.py). All of these components are necessary to run stable diffusion in inference even though they were trained\nor created independently from each other.""]","The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation in text-to-image generation tasks. It creates novel renditions of an input subject based on text prompts while overcoming limitations such as lengthy fine-tuning and difficulties in preserving subject fidelity. BLIP-Diffusion introduces a new multimodal encoder pre-trained to provide subject representation, allowing for efficient fine-tuning and flexible combinations with existing techniques for novel subject-driven generation and editing applications.",The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.
"How can a user claim authorship of a paper on the Hugging Face Hub?
","[""Paper Pages\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png)\n\n## Linking a Paper to a model, dataset or Space\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository's tags. Clicking on the arxiv tag will let you:\n\\* Visit the Paper page. \\* Filter for other models or datasets on the Hub that cite the same paper. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png)\n\n## Claiming authorship to a Paper\nThe Hub will attempt to automatically match paper to users based on their email. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png)\n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \""claim authorship\""."", ""| Partially handled by `huggingface\\_hub`. |"", ""--\ntitle: \""Ethics and Society Newsletter #1\""\nthumbnail: /blog/assets/103\\_ethics-soc-1/thumbnail.png\nauthors:\n- user: meg\n---\n# Ethics and Society Newsletter #1\nHello, world! Originating as an open-source company, Hugging Face was founded on some key ethical values in tech: \\_collaboration\\_, \\_responsibility\\_, and \\_transparency\\_. To code in an open environment means having your code – and the choices within – viewable to the world, associated with your account and available for others to critique and add to. As the research community began using the Hugging Face Hub to host models and data, the community directly integrated \\_reproducibility\\_ as another fundamental value of the company. And as the number of datasets and models on Hugging Face grew, those working at Hugging Face implemented [documentation requirements](https://huggingface.co/docs/hub/models-cards) and [free instructive courses](https://huggingface.co/course/chapter1/1), meeting the newly emerging values defined by the research community with complementary values around \\_auditability\\_ and \\_understanding\\_ the math, code, processes and people that lead to current technology."", ""\\* Is it related to something you would need for a project? We'd love to hear\nabout it! \\* Is it something you worked on and think could benefit the community? Awesome! Tell us what problem it solved for you. 2. Write a \\*full paragraph\\* describing the feature;\n3. Provide a \\*\\*code snippet\\*\\* that demonstrates its future use;\n4. In case this is related to a paper, please attach a link;\n5. Attach any additional information (drawings, screenshots, etc.) you think may help. If your issue is well written we're already 80% of the way there by the time you post it! ## Submitting a pull request (PR)\nBefore writing code, we strongly advise you to search through the existing PRs or\nissues to make sure that nobody is already working on the same thing. If you are\nunsure, it is always a good idea to open an issue to get some feedback. You will need basic `git` proficiency to be able to contribute to\n`huggingface\\_hub`. `git` is not the easiest tool to use but it has the greatest\nmanual. Type `git --help` in a shell and enjoy. If you prefer books, [Pro\nGit](https://git-scm.com/book/en/v2) is a very good reference. Follow these steps to start contributing:\n1. Fork the [repository](https://github.com/huggingface/huggingface\\_hub) by\nclicking on the 'Fork' button on the repository's page. This creates a copy of the code\nunder your GitHub user account. 2. Clone your fork to your local disk, and add the base repository as a remote."", ""Thank you for your contribution! ## Code of conduct\nThis project adheres to the HuggingFace [code of conduct](CODE\\_OF\\_CONDUCT.md). By participating, you are expected to abide by this code.""]","A user can claim authorship of a paper on the Hugging Face Hub by clicking on their name in the corresponding Paper page and then selecting the ""claim authorship"" option. The Hub will also attempt to automatically match the paper to users based on their email.","By clicking their name on the corresponding Paper page and clicking ""claim authorship"", then confirming the request in paper settings for admin team validation."
"What is the purpose of the /healthcheck endpoint in the Datasets server API?
","[""Datasets server API - rows endpoint\n> /rows endpoint\n## Configuration\nThe service can be configured using environment variables. They are grouped by scope. ### API service\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nSee https://huggingface.co/docs/datasets-server\n- /healthcheck: ensure the app is running\n- /metrics: return a list of metrics in the Prometheus format\n- /rows: get a slice of rows of a dataset split"", ""For more information about using 🤗 Datasets, check out the [tutorials](https://huggingface.co/docs/datasets/tutorial) and [how-to guides](https://huggingface.co/docs/datasets/how\\_to) available in the 🤗 Datasets documentation."", ""Check dataset validity\nBefore you download a dataset from the Hub, it is helpful to know if a specific dataset you're interested in is available. Datasets Server provides the `/is-valid` endpoint to check if a specific dataset works without any errors. The API endpoint will return an error for datasets that cannot be loaded with the [🤗 Datasets](https://github.com/huggingface/datasets) library, for example, because the data hasn't been uploaded or the format is not supported. The largest datasets are partially supported by Datasets Server. If they are{\"" \""}\n[streamable](https://huggingface.co/docs/datasets/stream), Datasets\nServer can extract the first 100 rows without downloading the whole dataset. This is especially useful for previewing large datasets where downloading the\nwhole dataset may take hours! See the `preview` field in the\nresponse of `/is-valid` to check if a dataset is partially\nsupported. This guide shows you how to check dataset validity programmatically, but free to try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-17b761d0-b2b8-4638-a4f7-73be9049c324), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/isValidDataset). ## Check if a dataset is valid\n`/is-valid` checks whether a specific dataset loads without any error. This endpoint's query parameter requires you to specify the name of the dataset:\n\n```python\nimport requests\nheaders = {\""Authorization\"": f\""Bearer {API\\_TOKEN}\""}\nAPI\\_URL = \""https://datasets-server.huggingface.co/is-valid?dataset=rotten\\_tomatoes\""\ndef query():\nresponse = requests.get(API\\_URL, headers=headers)\nreturn response.json()\ndata = query()\n```\n\n```js\nimport fetch from \""node-fetch\"";\nasync function query(data) {\nconst response = await fetch(\n\""https://datasets-server.huggingface.co/is-valid?dataset=rotten\\_tomatoes\"",\n{\nheaders: { Authorization: `Bearer ${API\\_TOKEN}` },\nmethod: \""GET\""\n}\n);\nconst result = await response.json();\nreturn result;\n}\nquery().then((response) => {\nconsole.log(JSON.stringify(response));\n});\n```\n\n```curl\ncurl https://datasets-server.huggingface.co/is-valid?dataset=rotten\\_tomatoes \\\n-X GET \\\n-H \""Authorization: Bearer ${API\\_TOKEN}\""\n```\nThe response looks like this if a dataset is valid:\n```json\n{\n\""viewer\"": true,\n\""preview\"": true\n}\n```\nIf only the first rows of a dataset are available, then the response looks like:\n```json\n{\n\""viewer\"": false,\n\""preview\"": true\n}\n```\nFinally, if the dataset is not valid at all, then the response is:\n```json\n{\n\""viewer\"": false,\n\""preview\"": false\n}\n```\nSome cases where a dataset is not valid are:\n- the dataset viewer is disabled\n- the dataset is gated but the access is not granted: no token is passed or the passed token is not authorized\n- the dataset is private\n- the dataset contains no data or the data format is not supported\nRemember if a dataset is [gated](./quick_start#gated-datasets),\nyou'll need to provide your user token to submit a successful query!"", ""Search text in a dataset\nDatasets Server provides a `/search` endpoint for searching words in a dataset. Currently, only [datasets with Parquet exports](./parquet)\nare supported so Datasets Server can index the contents and run the search without\ndownloading the whole dataset. This guide shows you how to use Datasets Server's `/search` endpoint to search for a query string. Feel free to also try it out with [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/searchRows). The text is searched in the columns of type `string`, even if the values are nested in a dictionary. The `/search` endpoint accepts five query parameters:\n- `dataset`: the dataset name, for example `glue` or `mozilla-foundation/common\\_voice\\_10\\_0`\n- `config`: the configuration name, for example `cola`\n- `split`: the split name, for example `train`\n- `query`: the text to search\n- `offset`: the offset of the slice, for example `150`\n- `length`: the length of the slice, for example `10` (maximum: `100`)\nFor example, let's search for the text `\""dog\""` in the `train` split of the `SelfRC` configuration of the `duorc` dataset, restricting the results to the slice 150-151:\n\n```python\nimport requests\nheaders = {\""Authorization\"": f\""Bearer {API\\_TOKEN}\""}\nAPI\\_URL = \""https://datasets-server.huggingface.co/search?dataset=duorc&config=SelfRC&split=train&query=dog&offset=150&length=2\""\ndef query():\nresponse = requests.get(API\\_URL, headers=headers)\nreturn response.json()\ndata = query()\n```\n\n```js\nimport fetch from \""node-fetch\"";\nasync function query(data) {\nconst response = await fetch(\n\""https://datasets-server.huggingface.co/search?dataset=duorc&config=SelfRC&split=train&query=dog&offset=150&length=2\"",\n{\nheaders: { Authorization: `Bearer ${API\\_TOKEN}` },\nmethod: \""GET\""\n}\n);\nconst result = await response.json();\nreturn result;\n}\nquery().then((response) => {\nconsole.log(JSON.stringify(response));\n});\n```\n\n```curl\ncurl https://datasets-server.huggingface.co/search?dataset=duorc&config=SelfRC&split=train&query=dog&offset=150&length=2 \\\n-X GET \\\n-H \""Authorization: Bearer ${API\\_TOKEN}\""\n```\nThe endpoint response is a JSON containing two keys (same format as [`/rows`](./rows)):\n- The [`features`](https://huggingface.co/docs/datasets/about\\_dataset\\_features) of a dataset, including the column's name and data type."", ""Datasets server admin machine\n> Admin endpoints\n## Configuration\nThe worker can be configured using environment variables. They are grouped by scope. ### Admin service\nSet environment variables to configure the application (`ADMIN\\_` prefix):\n- `ADMIN\\_HF\\_ORGANIZATION`: the huggingface organization from which the authenticated user must be part of in order to access the protected routes, eg. \""huggingface\"". If empty, the authentication is disabled. Defaults to None. - `ADMIN\\_CACHE\\_REPORTS\\_NUM\\_RESULTS`: the number of results in /cache-reports/... endpoints. Defaults to `100`. - `ADMIN\\_CACHE\\_REPORTS\\_WITH\\_CONTENT\\_NUM\\_RESULTS`: the number of results in /cache-reports-with-content/... endpoints. Defaults to `100`. - `ADMIN\\_HF\\_TIMEOUT\\_SECONDS`: the timeout in seconds for the requests to the Hugging Face Hub. Defaults to `0.2` (200 ms). - `ADMIN\\_HF\\_WHOAMI\\_PATH`: the path of the external whoami service, on the hub (see `HF\\_ENDPOINT`), eg. \""/api/whoami-v2\"". Defaults to `/api/whoami-v2`. - `ADMIN\\_MAX\\_AGE`: number of seconds to set in the `max-age` header on technical endpoints. Defaults to `10` (10 seconds). ### Uvicorn\nThe following environment variables are used to configure the Uvicorn server (`ADMIN\\_UVICORN\\_` prefix):\n- `ADMIN\\_UVICORN\\_HOSTNAME`: the hostname. Defaults to `\""localhost\""`. - `ADMIN\\_UVICORN\\_NUM\\_WORKERS`: the number of uvicorn workers. Defaults to `2`. - `ADMIN\\_UVICORN\\_PORT`: the port. Defaults to `8000`. ### Prometheus\n- `PROMETHEUS\\_MULTIPROC\\_DIR`: the directory where the uvicorn workers share their prometheus metrics. See https://github.com/prometheus/client\\_python#multiprocess-mode-eg-gunicorn. Defaults to empty, in which case every worker manages its own metrics, and the /metrics endpoint returns the metrics of a random worker. ### Common\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration. ## Endpoints\nThe admin service provides endpoints:\n- `/healthcheck`\n- `/metrics`: give info about the cache and the queue\n- `/cache-reports{processing\\_step}`: give detailed reports on the content of the cache for a processing step\n- `/cache-reports-with-content{processing\\_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\n- `/force-refresh{processing\\_step}`: force refresh cache entries for the processing step. It's a POST endpoint. Pass the requested parameters, depending on the processing step's input type:\n- `dataset`: `?dataset={dataset}`\n- `config`: `?dataset={dataset}&config={config}`\n- `split`: `?dataset={dataset}&config={config}&split={split}`\n- `/recreate-dataset`: deletes all the cache entries related to a specific dataset, then run all the steps in order. It's a POST endpoint.""]",The `/healthcheck` endpoint in the Datasets server API is used to ensure that the app is running. It provides a way to check the operational status of the service.,Ensure the app is running
"What is the default context window size for Local Attention in the LongT5 model?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# LongT5\n## Overview\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention. The abstract from the paper is the following:\n\\*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.\\*\nThis model was contributed by [stancld](https://huggingface.co/stancld). The original code can be found [here](https://github.com/google-research/longt5). ## Usage tips\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder \\*self-attention\\* layer with efficient either \\*local\\* attention or \\*transient-global\\* (\\*tglobal\\*) attention. - Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`]. - LongT5 model is designed to work efficiently and very well on long-range \\*sequence-to-sequence\\* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens. - For \\*Local Attention\\*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). \\*Local Attention\\* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l\\*r)`. - \\*Transient Global Attention\\* is an extension of the \\*Local Attention\\*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\nalso every global token like in the case of standard global attention (\\*transient\\* represents the fact the global tokens\nare constructed dynamically within each attention operation). As a consequence, \\*TGlobal\\* attention introduces\na few new parameters -- global relative position biases and a layer normalization for global token's embedding."", ""With our sliding window approach, however, there is overlap in\nthe tokens we pass to the model at each iteration. We don't want the log-likelihood for the tokens we're just treating\nas context to be included in our loss, so we can set these targets to `-100` so that they are ignored. The following\nis an example of how we could do this with a stride of `512`. This means that the model will have at least 512 tokens\nfor context when calculating the conditional likelihood of any one token (provided there are 512 preceding tokens\navailable to condition on). ```python\nimport torch\nfrom tqdm import tqdm\nmax\\_length = model.config.n\\_positions\nstride = 512\nseq\\_len = encodings.input\\_ids.size(1)\nnlls = []\nprev\\_end\\_loc = 0\nfor begin\\_loc in tqdm(range(0, seq\\_len, stride)):\nend\\_loc = min(begin\\_loc + max\\_length, seq\\_len)\ntrg\\_len = end\\_loc - prev\\_end\\_loc # may be different from stride on last loop\ninput\\_ids = encodings.input\\_ids[:, begin\\_loc:end\\_loc].to(device)\ntarget\\_ids = input\\_ids.clone()\ntarget\\_ids[:, :-trg\\_len] = -100\nwith torch.no\\_grad():\noutputs = model(input\\_ids, labels=target\\_ids)\n# loss is calculated using CrossEntropyLoss which averages over valid labels\n# N.B. the model only calculates loss over trg\\_len - 1 labels, because it internally shifts the labels\n# to the left by 1. neg\\_log\\_likelihood = outputs.loss\nnlls.append(neg\\_log\\_likelihood)\nprev\\_end\\_loc = end\\_loc\nif end\\_loc == seq\\_len:\nbreak\nppl = torch.exp(torch.stack(nlls).mean())\n```\nRunning this with the stride length equal to the max input length is equivalent to the suboptimal, non-sliding-window\nstrategy we discussed above. The smaller the stride, the more context the model will have in making each prediction,\nand the better the reported perplexity will typically be. When we run the above with `stride = 1024`, i.e. no overlap, the resulting PPL is `19.44`, which is about the same\nas the `19.93` reported in the GPT-2 paper. By using `stride = 512` and thereby employing our striding window\nstrategy, this jumps down to `16.45`. This is not only a more favorable score, but is calculated in a way that is\ncloser to the true autoregressive decomposition of a sequence likelihood."", ""![](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/mixtral-7b-inference-large-seqlen.png)\n\n### Sliding window Attention\nThe current implementation supports the sliding window attention mechanism and memory efficient cache management. To enable sliding window attention, just make sure to have a `flash-attn` version that is compatible with sliding window attention (`>=2.3.0`). The Flash Attention-2 model uses also a more memory efficient cache slicing mechanism - as recommended per the official implementation of Mistral model that use rolling cache mechanism we keep the cache size fixed (`self.config.sliding\\_window`), support batched generation only for `padding\\_side=\""left\""` and use the absolute position of the current token to compute the positional embedding. ## The Mistral Team\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed. ## MixtralConfig\n[[autodoc]] MixtralConfig\n## MixtralModel\n[[autodoc]] MixtralModel\n- forward\n## MixtralForCausalLM\n[[autodoc]] MixtralForCausalLM\n- forward\n## MixtralForSequenceClassification\n[[autodoc]] MixtralForSequenceClassification\n- forward"", ""The first drawback of this architecture becomes obvious: Some input vectors have no access to their immediate context, \\*e.g.\\* \\\\(\\mathbf{x}\\_9\\\\) has no access to \\\\(\\mathbf{x}\\_{8}\\\\) and vice-versa in our example. This is problematic because these tokens are not able to learn word representations that take their immediate context into account. A simple remedy is to augment each chunk with `config.local\\_num\\_chunks\\_before`, \\*i.e.\\* \\\\(n\\_{p}\\\\), chunks and `config.local\\_num\\_chunks\\_after`, \\*i.e.\\* \\\\(n\\_{a}\\\\), so that every input vector has at least access to \\\\(n\\_{p}\\\\) previous input vectors and \\\\(n\\_{a}\\\\) following input vectors. This can also be understood as chunking with overlap whereas \\\\(n\\_{p}\\\\) and \\\\(n\\_{a}\\\\) define the amount of overlap each chunk has with all previous chunks and following chunks. We denote this extended local self-attention as follows:\n$$\\mathbf{Z}^{\\text{loc}} = \\left[\\mathbf{Z}\\_{1:l\\_{c}}^{\\text{loc}}, \\ldots, \\mathbf{Z}\\_{(n\\_{c} - 1) \\* l\\_{c} : n\\_{c} \\* l\\_{c}}^{\\text{loc}}\\right], $$\nwith\n$$\\mathbf{Z}\\_{l\\_{c} \\* (i - 1) + 1 : l\\_{c} \\* i}^{\\text{loc}} = \\text{SelfAttn}(\\mathbf{X}\\_{l\\_{c} \\* (i - 1 - n\\_{p}) + 1: l\\_{c} \\* (i + n\\_{a})})\\left[n\\_{p} \\* l\\_{c}: -n\\_{a} \\* l\\_{c}\\right], \\forall i \\in \\{1, \\ldots, n\\_{c} \\}$$\nOkay, this formula looks quite complicated. Let's make it easier. In Reformer's self-attention layers \\\\(n\\_{a}\\\\) is usually set to 0 and \\\\(n\\_{p}\\\\) is set to 1, so let's write down the formula again for \\\\(i = 1\\\\):\n$$\\mathbf{Z}\\_{1:l\\_{c}}^{\\text{loc}} = \\text{SelfAttn}(\\mathbf{X}\\_{-l\\_{c} + 1: l\\_{c}})\\left[l\\_{c}:\\right]$$\nWe notice that we have a circular relationship so that the first segment can attend the last segment as well. Let's illustrate this slightly enhanced local attention again. First, we apply self-attention within each windowed segment and keep only the central output segment. ![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific\\_images/master/reformer\\_benchmark/local\\_attention\\_2.png)\nFinally, the relevant output is concatenated to \\\\(\\mathbf{Z}^{\\text{loc}}\\\\) and looks as follows. ![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific\\_images/master/reformer\\_benchmark/local\\_attention\\_3.png)\nNote that local self-attention is implemented efficiently way so that no output is computed and subsequently \""thrown-out\"" as shown here for illustration purposes by the red cross. It's important to note here that extending the input vectors for each chunked self-attention function allows \\*each\\* single output vector \\\\( \\mathbf{z}\\_{i} \\\\) of this self-attention function to learn better vector representations."", ""DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.5% box AP in COCO object detection,\n1.3% mask AP in COCO instance segmentation, and 1.1% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.2 PQ)\nand ADE20K (48.5 PQ), and instance segmentation model on Cityscapes (44.5 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.2 mIoU),\nand ranks second on Cityscapes (84.5 mIoU) (no extra data). \\*\n![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dilated-neighborhood-attention-pattern.jpg)\n Neighborhood Attention with different dilation values. Taken from the [original paper](https://arxiv.org/abs/2209.15001). This model was contributed by [Ali Hassani](https://huggingface.co/alihassanijr). The original code can be found [here](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer). ## Usage tips\nDiNAT can be used as a \\*backbone\\*. When `output\\_hidden\\_states = True`,\nit will output both `hidden\\_states` and `reshaped\\_hidden\\_states`. The `reshaped\\_hidden\\_states` have a shape of `(batch, num\\_channels, height, width)` rather than `(batch\\_size, height, width, num\\_channels)`. Notes:\n- DiNAT depends on [NATTEN](https://github.com/SHI-Labs/NATTEN/)'s implementation of Neighborhood Attention and Dilated Neighborhood Attention.""]","The default context window size for Local Attention in the LongT5 model is `r=127`, which means a given token can attend to 127 tokens to the left and 127 tokens to the right of it.",127 tokens
"What method is used to load a checkpoint for a task using `AutoPipeline`?
","[""The `AutoPipeline` class is designed to simplify the variety of pipelines in 🤗 Diffusers. It is a generic, \\*task-first\\* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name. Take a look at the [AutoPipeline](../api/pipelines/auto\\_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting. This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights. ## Choose an AutoPipeline for your task\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nprompt = \""peasant and dragon combat, wood cutting style, viking era, bevel with rune\""\nimage = pipeline(prompt, num\\_inference\\_steps=25).images[0]\nimage\n```\n\n![generated image of peasant fighting dragon in wood cutting style](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-text2img.png)\n\nUnder the hood, [`AutoPipelineForText2Image`]:\n1. automatically detects a `\""stable-diffusion\""` class from the [`model\\_index.json`](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model\\_index.json) file\n2. loads the corresponding text-to-image [`StableDiffusionPipeline`] based on the `\""stable-diffusion\""` class name\nLikewise, for image-to-image, [`AutoPipelineForImage2Image`] detects a `\""stable-diffusion\""` checkpoint from the `model\\_index.json` file and it'll load the corresponding [`StableDiffusionImg2ImgPipeline`] behind the scenes. You can also pass any additional arguments specific to the pipeline class such as `strength`, which determines the amount of noise or variation added to an input image:\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\npipeline = AutoPipelineForImage2Image.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"",\ntorch\\_dtype=torch.float16,\nuse\\_safetensors=True,\n).to(\""cuda\"")\nprompt = \""a portrait of a dog wearing a pearl earring\""\nurl = \""https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665\\_Girl\\_with\\_a\\_Pearl\\_Earring.jpg/800px-1665\\_Girl\\_with\\_a\\_Pearl\\_Earring.jpg\""\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\""RGB\"")\nimage.thumbnail((768, 768))\nimage = pipeline(prompt, image, num\\_inference\\_steps=200, strength=0.75, guidance\\_scale=10.5).images[0]\nimage\n```\n\n![generated image of a vermeer portrait of a dog wearing a pearl earring](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-img2img.png)\n\nAnd if you want to do inpainting, then [`AutoPipelineForInpainting`] loads the underlying [`StableDiffusionInpaintPipeline`] class in the same way:\n```py\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load\\_image\nimport torch\npipeline = AutoPipelineForInpainting.from\\_pretrained(\n\""stabilityai/stable-diffusion-xl-base-1.0\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n).to(\""cuda\"")\nimg\\_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\_examples/overture-creations-5sI6fQgYIuo.png\""\nmask\\_url = \""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting\\_examples/overture-creations-5sI6fQgYIuo\\_mask.png\""\ninit\\_image = load\\_image(img\\_url).convert(\""RGB\"")\nmask\\_image = load\\_image(mask\\_url).convert(\""RGB\"")\nprompt = \""A majestic tiger sitting on a bench\""\nimage = pipeline(prompt, image=init\\_image, mask\\_image=mask\\_image, num\\_inference\\_steps=50, strength=0.80).images[0]\nimage\n```\n\n![generated image of a tiger sitting on a bench](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/autopipeline-inpaint.png)\n\nIf you try to load an unsupported checkpoint, it'll throw an error:\n```py\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\npipeline = AutoPipelineForImage2Image.from\\_pretrained(\n\""openai/shap-e-img2img\"", torch\\_dtype=torch.float16, use\\_safetensors=True\n)\n\""ValueError: AutoPipeline can't find a pipeline linked to ShapEImg2ImgPipeline for None\""\n```\n## Use multiple pipelines\nFor some workflows or if you're loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory."", ""You'll find the checkpoints saved in a `checkpoint-000` subfolder where the numbers at the end correspond to the training step. Saving checkpoints are useful for resuming training later. ```py\n# resume from latest checkpoint\ntrainer.train(resume\\_from\\_checkpoint=True)\n# resume from specific checkpoint saved in output directory\ntrainer.train(resume\\_from\\_checkpoint=\""your-model/checkpoint-1000\"")\n```\nYou can save your checkpoints (the optimizer state is not saved by default) to the Hub by setting `push\\_to\\_hub=True` in [`TrainingArguments`] to commit and push them. Other options for deciding how your checkpoints are saved are set up in the [`hub\\_strategy`](https://huggingface.co/docs/transformers/main\\_classes/trainer#transformers.TrainingArguments.hub\\_strategy) parameter:\n\\* `hub\\_strategy=\""checkpoint\""` pushes the latest checkpoint to a subfolder named \""last-checkpoint\"" from which you can resume training\n\\* `hug\\_strategy=\""all\\_checkpoints\""` pushes all checkpoints to the directory defined in `output\\_dir` (you'll see one checkpoint per folder in your model repository)\nWhen you resume training from a checkpoint, the [`Trainer`] tries to keep the Python, NumPy, and PyTorch RNG states the same as they were when the checkpoint was saved."", ""A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from\\_pretrained`] method:\n```py\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... new\\_model = AutoModel.from\\_pretrained(tmp\\_dir)\n```\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard. Behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n```py\n>>> import json\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... with open(os.path.join(tmp\\_dir, \""pytorch\\_model.bin.index.json\""), \""r\"") as f:\n... index = json.load(f)\n>>> print(index.keys())\ndict\\_keys(['metadata', 'weight\\_map'])\n```\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\n```py\n>>> index[\""metadata\""]\n{'total\\_size': 433245184}\n```\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state\\_dict`) to the file it's stored in:\n```py\n>>> index[\""weight\\_map\""]\n{'embeddings.LayerNorm.bias': 'pytorch\\_model-00001-of-00003.bin',\n'embeddings.LayerNorm.weight': 'pytorch\\_model-00001-of-00003.bin',\n... ```\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from\\_pretrained`] (like you would do `model.load\\_state\\_dict()` for a full checkpoint) you should use [`~modeling\\_utils.load\\_sharded\\_checkpoint`]:\n```py\n>>> from transformers.modeling\\_utils import load\\_sharded\\_checkpoint\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... load\\_sharded\\_checkpoint(model, tmp\\_dir)\n```\n## Low memory loading\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library. Please read the following guide for more information: [Large model loading using Accelerate](./main\\_classes/model#large-model-loading)"", ""We'll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task). {/if}\nIn the code sample above we didn't use `BertConfig`, and instead loaded a pretrained model via the `bert-base-cased` identifier. This is a model checkpoint that was trained by the authors of BERT themselves; you can find more details about it in its [model card](https://huggingface.co/bert-base-cased). This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results. The weights have been downloaded and cached (so future calls to the `from\\_pretrained()` method won't re-download them) in the cache folder, which defaults to \\*~/.cache/huggingface/transformers\\*. You can customize your cache folder by setting the `HF\\_HOME` environment variable."", ""- [LyCORIS checkpoints](https://github.com/KohakuBlueleaf/LyCORIS) aren't fully supported. The [`~loaders.LoraLoaderMixin.load\\_lora\\_weights`] method loads LyCORIS checkpoints with LoRA and LoCon modules, but Hada and LoKR are not supported. Loading a checkpoint from TheLastBen is very similar. For example, to load the [TheLastBen/William\\_Eggleston\\_Style\\_SDXL](https://huggingface.co/TheLastBen/William\\_Eggleston\\_Style\\_SDXL) checkpoint:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\""stabilityai/stable-diffusion-xl-base-1.0\"", torch\\_dtype=torch.float16).to(\""cuda\"")\npipeline.load\\_lora\\_weights(\""TheLastBen/William\\_Eggleston\\_Style\\_SDXL\"", weight\\_name=\""wegg.safetensors\"")\n# use by william eggleston in the prompt to trigger the LoRA\nprompt = \""a house by william eggleston, sunrays, beautiful, sunlight, sunrays, beautiful\""\nimage = pipeline(prompt=prompt).images[0]\nimage\n```\n## IP-Adapter\n[IP-Adapter](https://ip-adapter.github.io/) is an effective and lightweight adapter that adds image prompting capabilities to a diffusion model.""]","The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically detects the correct pipeline class to use based on the checkpoint provided, allowing you to focus on the task without needing to know the specific pipeline class name.",from_pretrained()
"What is the purpose of Diffusers library?
","[""🧨 Diffusers Experimental\nWe are adding experimental code to support novel applications and usages of the Diffusers library."", ""This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\nis very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline. ## Tweakable, contributor-friendly over abstraction\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t\\_repeat\\_yourself). In short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers. Functions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable. \\*\\*However\\*\\*, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions. - Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions. - Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel. At Hugging Face, we call this design the \\*\\*single-file policy\\*\\* which means that almost all of the code of a certain class should be written in a single, self-contained file."", ""For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.\\*\\*\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!"", ""--\ntitle: Swift 🧨Diffusers - Fast Stable Diffusion for Mac\nthumbnail: /blog/assets/fast-mac-diffusers/thumbnail.png\nauthors:\n- user: pcuenq\n- user: reach-vb\n---\n# Swift 🧨Diffusers: Fast Stable Diffusion for Mac\nTransform your text into stunning images with ease using Diffusers for Mac, a native app powered by state-of-the-art diffusion models. It leverages a bouquet of SoTA Text-to-Image models contributed by the community to the Hugging Face Hub, and converted to Core ML for blazingly fast performance. Our latest version, 1.1, is now available on the [Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) with significant performance upgrades and user-friendly interface tweaks. It's a solid foundation for future feature updates. Plus, the app is fully open source with a permissive [license](https://github.com/huggingface/swift-coreml-diffusers/blob/main/LICENSE), so you can build on it too! Check out our GitHub repository at https://github.com/huggingface/swift-coreml-diffusers for more information. ![Screenshot showing Diffusers for Mac UI](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-mac-diffusers/UI.png)\n## What exactly is 🧨Diffusers for Mac anyway? The Diffusers app ([App Store](https://apps.apple.com/app/diffusers/id1666309574), [source code](https://github.com/huggingface/swift-coreml-diffusers)) is the Mac counterpart to our [🧨`diffusers` library](https://github.com/huggingface/diffusers). This library is written in Python with PyTorch, and uses a modular design to train and run diffusion models."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# 🧨 Diffusers’ Ethical Guidelines\n## Preamble\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training. Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users’ contributions, and usage of the Diffusers library. The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.""]","The purpose of the Diffusers library is to provide pre-trained diffusion models and serve as a modular toolbox for inference and training. It allows users to handle the interaction between different model components and facilitates easier debugging and customization. The library is designed to support various applications, including DreamBooth and Textual Inversion training, by separating single components of the diffusion pipeline.",To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.
"What method does the EulerAncestralDiscreteScheduler use for sampling?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# EulerAncestralDiscreteScheduler\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k\\_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/). ## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling\\_euler\\_ancestral\\_discrete.EulerAncestralDiscreteSchedulerOutput"", ""Empirically, DPMSolver sampling with only 20 steps can generate high-quality\nsamples, and it can generate quite good samples even in 10 steps. The original implementation can be found at [LuChengTHU/dpm-solver](https://github.com/LuChengTHU/dpm-solver). ## Tips\nIt is recommended to set `solver\\_order` to 2 for guide sampling, and `solver\\_order=3` for unconditional sampling. Dynamic thresholding from [Imagen](https://huggingface.co/papers/2205.11487) is supported, and for pixel-space\ndiffusion models, you can set both `algorithm\\_type=\""dpmsolver++\""` and `thresholding=True` to use dynamic\nthresholding. This thresholding method is unsuitable for latent-space diffusion models such as\nStable Diffusion. ## DPMSolverSinglestepScheduler\n[[autodoc]] DPMSolverSinglestepScheduler\n## SchedulerOutput\n[[autodoc]] schedulers.scheduling\\_utils.SchedulerOutput"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# ScoreSdeVpScheduler\n`ScoreSdeVpScheduler` is a variance preserving stochastic differential equation (SDE) scheduler. It was introduced in the [Score-Based Generative Modeling through Stochastic Differential Equations](https://huggingface.co/papers/2011.13456) paper by Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. The abstract from the paper is:\n\\*Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE."", ""The schedulers in this class are distinguished at a high level by their noise sampling strategy, the type of network and scaling, the training strategy, and how the loss is weighed. The different schedulers in this class, depending on the ordinary differential equations (ODE) solver type, fall into the above taxonomy and provide a good abstraction for the design of the main schedulers implemented in 🤗 Diffusers. The schedulers in this class are given [here](https://github.com/huggingface/diffusers/blob/a69754bb879ed55b9b6dc9dd0b3cf4fa4124c765/src/diffusers/schedulers/scheduling\\_utils.py#L32). ## PushToHubMixin\n[[autodoc]] utils.PushToHubMixin"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# DEISMultistepScheduler\nDiffusion Exponential Integrator Sampler (DEIS) is proposed in [Fast Sampling of Diffusion Models with Exponential Integrator](https://huggingface.co/papers/2204.13902) by Qinsheng Zhang and Yongxin Chen. `DEISMultistepScheduler` is a fast high order solver for diffusion ordinary differential equations (ODEs). This implementation modifies the polynomial fitting formula in log-rho space instead of the original linear `t` space in the DEIS paper. The modification enjoys closed-form coefficients for exponential multistep update instead of replying on the numerical solver. The abstract from the paper is:\n\\*The past few years have witnessed the great success of Diffusion models~(DMs) in generating high-fidelity samples in generative modeling tasks. A major limitation of the DM is its notoriously slow sampling procedure which normally requires hundreds to thousands of time discretization steps of the learned diffusion process to reach the desired accuracy. Our goal is to develop a fast sampling method for DMs with a much less number of steps while retaining high sample quality. To this end, we systematically analyze the sampling procedure in DMs and identify key factors that affect the sample quality, among which the method of discretization is most crucial. By carefully examining the learned diffusion process, we propose Diffusion Exponential Integrator Sampler~(DEIS). It is based on the Exponential Integrator designed for discretizing ordinary differential equations (ODEs) and leverages a semilinear structure of the learned diffusion process to reduce the discretization error.""]",The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling.,Ancestral sampling with Euler method steps.
"What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# FLAVA\n## Overview\nThe FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022. The paper aims at creating a single unified foundation model which can work across vision, language\nas well as vision-and-language multimodal tasks. The abstract from the paper is the following:\n\\*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety\nof downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal\n(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising\ndirection would be to use a single holistic universal model, as a \""foundation\"", that targets all modalities\nat once -- a true vision and language foundation model should be good at vision tasks, language tasks, and\ncross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate\nimpressive performance on a wide range of 35 tasks spanning these target modalities.\\*\nThis model was contributed by [aps](https://huggingface.co/aps). The original code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava). ## FlavaConfig\n[[autodoc]] FlavaConfig\n## FlavaTextConfig\n[[autodoc]] FlavaTextConfig\n## FlavaImageConfig\n[[autodoc]] FlavaImageConfig\n## FlavaMultimodalConfig\n[[autodoc]] FlavaMultimodalConfig\n## FlavaImageCodebookConfig\n[[autodoc]] FlavaImageCodebookConfig\n## FlavaProcessor\n[[autodoc]] FlavaProcessor\n## FlavaFeatureExtractor\n[[autodoc]] FlavaFeatureExtractor\n## FlavaImageProcessor\n[[autodoc]] FlavaImageProcessor\n- preprocess\n## FlavaForPreTraining\n[[autodoc]] FlavaForPreTraining\n- forward\n## FlavaModel\n[[autodoc]] FlavaModel\n- forward\n- get\\_text\\_features\n- get\\_image\\_features\n## FlavaImageCodebook\n[[autodoc]] FlavaImageCodebook\n- forward\n- get\\_codebook\\_indices\n- get\\_codebook\\_probs\n## FlavaTextModel\n[[autodoc]] FlavaTextModel\n- forward\n## FlavaImageModel\n[[autodoc]] FlavaImageModel\n- forward\n## FlavaMultimodalModel\n[[autodoc]] FlavaMultimodalModel\n- forward"", ""Also, thanks to Niels Rogge, Younes Belkada, and Suraj Patil, among many others at Hugging Face, who laid out the foundations for increasing the use of multi-modal models from Transformers.\\*"", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# XLM-RoBERTa-XL\n## Overview\nThe XLM-RoBERTa-XL model was proposed in [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau. The abstract from the paper is the following:\n\\*Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI."", ""The negative samples are usually randomly sampled from the dataset itself. The MLM and ITM objectives are often combined during the pre-training of multi-modal models. For instance, VisualBERT proposes a BERT-like architecture that uses a pre-trained object detection model, [Faster-RCNN](https://arxiv.org/abs/1506.01497), to detect objects. This model uses a combination of the MLM and ITM objectives during pre-training to implicitly align elements of an input text and regions in an associated input image with self-attention. Another work, FLAVA, consists of an image encoder, a text encoder, and a multi-modal encoder to fuse and align the image and text representations for multi-modal reasoning, all of which are based on transformers. In order to achieve this, FLAVA uses a variety of pre-training objectives: MLM, ITM, as well as Masked-Image Modeling (MIM), and contrastive learning. ### 5) No Training\nFinally, various optimization strategies aim to bridge image and text representations using the pre-trained image and text models or adapt pre-trained multi-modal models to new downstream tasks without additional training. For example, [MaGiC](https://arxiv.org/abs/2205.02655) proposes iterative optimization through a pre-trained autoregressive language model to generate a caption for the input image. To do this, MaGiC computes a CLIP-based “Magic score” using CLIP embeddings of the generated tokens and the input image. ![ASIF](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/asif.png)  \n*Crafting a similarity search space using pre-trained, frozen unimodal image and text encoders ([image source](https://luca.moschella.dev/publication/norelli-asif-2022))*\n\n[ASIF](https://arxiv.org/abs/2210.01738) proposes a simple method to turn pre-trained uni-modal image and text models into a multi-modal model for image captioning using a relatively small multi-modal dataset without additional training. The key intuition behind ASIF is that captions of similar images are also similar to each other."", ""--\ntitle: \""Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action\""\nthumbnail: /blog/assets/deploy-deepfloydif-using-bentoml/thumbnail.png\nauthors:\n- user: Sherlockk\nguest: true\n- user: larme\nguest: true\n---\n# Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action\nHugging Face provides a Hub platform that allows you to upload, share, and deploy your models with ease. It saves developers the time and computational resources required to train models from scratch. However, deploying models in a real-world production environment or in a cloud-native way can still present challenges. This is where BentoML comes into the picture. BentoML is an open-source platform for machine learning model serving and deployment. It is a unified framework for building, shipping, and scaling production-ready AI applications incorporating traditional, pre-trained, and generative models as well as Large Language Models. Here is how you use the BentoML framework from a high-level perspective:\n1. \\*\\*Define a model\\*\\*: Before you can use BentoML, you need a machine learning model (or multiple models). This model can be trained using a machine learning library such as TensorFlow and PyTorch. 2. \\*\\*Save the model\\*\\*: Once you have a trained model, save it to the BentoML local Model Store, which is used for managing all your trained models locally as well as accessing them for serving. 3. \\*\\*Create a BentoML Service\\*\\*: You create a `service.py` file to wrap the model and define the serving logic. It specifies [Runners](https://docs.bentoml.org/en/latest/concepts/runner.html) for models to run model inference at scale and exposes APIs to define how to process inputs and outputs.""]","I'm sorry, but I cannot provide information on models that are not related to HuggingFace or gradio-app.",IDEFICS
"What is the purpose of the `gradio.Blocks` API?
","[""Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our \\*\\*newly developed\\*\\* low-level API called `gradio.Blocks`. Now, what's the difference between `Interface` and `Blocks`? - ⚡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs. - 🧱 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \""building blocks\""). ### Why Blocks 🧱?[[why-blocks-]]\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\n- Group together related demos as multiple tabs in one web application\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n- Change a component's properties (for example, the choices in a dropdown) or its visibility based on user input\nWe will explore all of these concepts below. ### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook. ```py\nimport gradio as gr\ndef flip\\_text(x):\nreturn x[::-1]\ndemo = gr.Blocks()\nwith demo:\ngr.Markdown(\n\""\""\""\n# Flip Text! Start typing below to see the output. \""\""\""\n)\ninput = gr.Textbox(placeholder=\""Flip this text\"")\noutput = gr.Textbox()\ninput.change(fn=flip\\_text, inputs=input, outputs=output)\ndemo.launch()\n```\nThis simple example above introduces 4 concepts that underlie Blocks:\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context."", ""All components are now keyboard navigable and include better colors to be usable by a wider audience. ![](https://media0.giphy.com/media/Kv1bAN7MX3ya5krkEU/giphy.gif)\n\\*\\*3. Server Side Events\\*\\*:\nGradio's built-in queuing system is now the default for every Gradio app. We now use Server Side Events instead of Websockets for the queue. SSE means everything is served over HTTP and has better device support and better scaling than websockets. ![](https://i.imgur.com/ewUIuUc.png)\n\\*\\*4. Custom Share Servers\\*\\*:\nGradio share links can now run on custom domains. You can now set up your own server to serve Gradio share links. To get started, [read our guide here](https://github.com/huggingface/frp/). ![](https://i.imgur.com/VFWVsqn.png)\n5. We now support adding arbitrary JS to your apps using the `js` parameter in Blocks, and arbitrary modifications to the  of your app using the `head` parameter in Blocks\n6. We no longer expose a user's working directory by default when you release a Gradio app."", ""Gradio Demo: json\\_component\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.JSON(value={\""Key 1\"": \""Value 1\"", \""Key 2\"": {\""Key 3\"": \""Value 2\"", \""Key 4\"": \""Value 3\""}, \""Key 5\"": [\""Item 1\"", \""Item 2\"", \""Item 3\""]})\ndemo.launch()\n```"", ""### 7. Select all the steps necessary for adding state to your Gradio interface\n### 8. Which of the following are components included in the Gradio library? ### 9. What does Gradio `Blocks` allow you to do? ### 10. You can share a public link to a `Blocks` demo and host a `Blocks` demo on Hugging Face spaces."", ""Gradio Blocks Party[[gradio-blocks-party]]\nAlong with the release of the Gradio chapter of the course, Hugging Face hosted a community event on building cool machine learning demos using the new Gradio Blocks feature. You can find all the demos that the community created under the [`Gradio-Blocks`](https://huggingface.co/Gradio-Blocks) organisation on the Hub. Here's a few examples from the winners:\n\\*\\*Natural language to SQL\\*\\*""]","The `gradio.Blocks` API is a low-level API that allows you to have full control over the data flows and layout of your application. It provides more flexibility compared to the high-level `Interface` API, enabling you to build complex, multi-step applications. With `Blocks`, you can group related demos, change layouts, create multi-step interfaces, modify component properties based on user input, and more.","The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications."
"What is the purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents""?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# unCLIP\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in 🤗 Diffusers comes from kakaobrain's [karlo](https://github.com/kakaobrain/karlo). The abstract from the paper is following:\n\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\*\nYou can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch). Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines."", ""While CLIPSeg is trained on simple object descriptions (e.g., “a car”), its CLIP backbone is pre-trained on engineered text templates (e.g., “an image of a car”, “a photo of a car”) and kept frozen during training. Once the inputs are preprocessed, we can perform inference to get a binary segmentation map of shape (height, width) for each text query. ```py\nimport torch\nwith torch.no\\_grad():\noutputs = model(\\*\\*inputs)\nlogits = outputs.logits\nprint(logits.shape)\n>>> torch.Size([3, 352, 352])\n```\nLet’s visualize the results to see how well CLIPSeg performed (code is adapted from [this post](https://huggingface.co/blog/clipseg-zero-shot)). ```py\nimport matplotlib.pyplot as plt\nlogits = logits.unsqueeze(1)\n\\_, ax = plt.subplots(1, len(texts) + 1, figsize=(3\\*(len(texts) + 1), 12))\n[a.axis('off') for a in ax.flatten()]\nax[0].imshow(image)\n[ax[i+1].imshow(torch.sigmoid(logits[i][0])) for i in range(len(texts))];\n[ax[i+1].text(0, -15, prompt) for i, prompt in enumerate(texts)]\n```\n\n![CLIPSeg results](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_vision_language_pretraining/clipseg_result.png)\n\nAmazing, isn’t it?"", ""We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art\nperformances on a bunch of tasks including ImageNet-CN, Flicker30k- CN, and COCO-CN. Further, we obtain very close performances with\nCLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding.\\*\nThis model was contributed by [jongjyh](https://huggingface.co/jongjyh). ## Usage tips and example\nThe usage of AltCLIP is very similar to the CLIP. the difference between CLIP is the text encoder. Note that we use bidirectional attention instead of casual attention\nand we take the [CLS] token in XLM-R to represent text embedding. AltCLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image\nclassification. AltCLIP uses a ViT like transformer to get visual features and a bidirectional language model to get the text\nfeatures. Both the text and visual features are then projected to a latent space with identical dimension. The dot\nproduct between the projected image and text features is then used as a similar score. To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-overlapping patches,\nwhich are then linearly embedded."", ""Note, however, that the number of prompts we used to compute the CLIP scores is quite low. For a more practical evaluation, this number should be way higher, and the prompts should be diverse. By construction, there are some limitations in this score. The captions in the training dataset\nwere crawled from the web and extracted from `alt` and similar tags associated an image on the internet. They are not necessarily representative of what a human being would use to describe an image. Hence we\nhad to \""engineer\"" some prompts here. ### Image-conditioned text-to-image generation\nIn this case, we condition the generation pipeline with an input image as well as a text prompt. Let's take the [`StableDiffusionInstructPix2PixPipeline`], as an example. It takes an edit instruction as an input prompt and an input image to be edited. Here is one example:\n![edit-instruction](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation\\_diffusion\\_models/edit-instruction.png)\nOne strategy to evaluate such a model is to measure the consistency of the change between the two images (in [CLIP](https://huggingface.co/docs/transformers/model\\_doc/clip) space) with the change between the two image captions (as shown in [CLIP-Guided Domain Adaptation of Image Generators](https://arxiv.org/abs/2108.00946)). This is referred to as the \""\\*\\*CLIP directional similarity\\*\\*\"". - Caption 1 corresponds to the input image (image 1) that is to be edited. - Caption 2 corresponds to the edited image (image 2). It should reflect the edit instruction. Following is a pictorial overview:\n![edit-consistency](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation\\_diffusion\\_models/edit-consistency.png)\nWe have prepared a mini dataset to implement this metric. Let's first load the dataset. ```python\nfrom datasets import load\\_dataset\ndataset = load\\_dataset(\""sayakpaul/instructpix2pix-demo\"", split=\""train\"")\ndataset.features\n```\n```bash\n{'input': Value(dtype='string', id=None),\n'edit': Value(dtype='string', id=None),\n'output': Value(dtype='string', id=None),\n'image': Image(decode=True, id=None)}\n```\nHere we have:\n- `input` is a caption corresponding to the `image`."", ""Clark, Dan Garrette, Iulia Turc, John Wieting. 1. \\*\\*[Chinese-CLIP](https://huggingface.co/docs/transformers/model\\_doc/chinese\\_clip)\\*\\* (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou. 1. \\*\\*[CLAP](https://huggingface.co/docs/transformers/model\\_doc/clap)\\*\\* (from LAION-AI) released with the paper [Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation](https://arxiv.org/abs/2211.06687) by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov. 1. \\*\\*[CLIP](https://huggingface.co/docs/transformers/model\\_doc/clip)\\*\\* (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. 1. \\*\\*[CLIPSeg](https://huggingface.co/docs/transformers/model\\_doc/clipseg)\\*\\* (from University of Göttingen) released with the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke and Alexander Ecker.""]","The purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents"" is to leverage robust representations of images learned by contrastive models like CLIP for image generation. The model consists of a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. This approach aims to improve image diversity with minimal loss in photorealism and caption similarity, allowing for variations of an image that preserve its semantics and style while varying non-essential details. Additionally, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion.",The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.
"What command is used to install the requirements for a research project using 🤗 Transformers?
","[""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Research projects\nThis folder contains various research projects using 🤗 Transformers. They are not maintained and require a specific\nversion of 🤗 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work. To use any of them, just run the command\n```\npip install -r requirements.txt\n```\ninside the folder of your choice. If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder."", ""ow to add BigBird to 🤗 Transformers? =====================================\nMentor: [Patrick](https://github.com/patrickvonplaten)\nBegin: 12.02.2020\nEstimated End: 19.03.2020\nContributor: [Vasudev](https://github.com/thevasudevgupta)\nAdding a new model is often difficult and requires an in-depth knowledge\nof the 🤗 Transformers library and ideally also of the model's original\nrepository. At Hugging Face, we are trying to empower the community more\nand more to add models independently. The following sections explain in detail how to add BigBird\nto Transformers. You will work closely with Patrick to\nintegrate BigBird into Transformers. By doing so, you will both gain a\ntheoretical and deep practical understanding of BigBird. But more importantly, you will have made a major\nopen-source contribution to Transformers. Along the way, you will:\n- get insights into open-source best practices\n- understand the design principles of one of the most popular NLP\nlibraries\n- learn how to do efficiently test large NLP models\n- learn how to integrate Python utilities like `black`, `ruff`,\n`make fix-copies` into a library to always ensure clean and readable\ncode\nTo start, let's try to get a general overview of the Transformers\nlibrary."", ""\\* [TensorFlow 2.0](https://www.tensorflow.org/install/pip) installation instructions. \\* [Flax](https://flax.readthedocs.io/en/latest/) installation instructions. ## Install with pip\nYou should install 🤗 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies. Start by creating a virtual environment in your project directory:\n```bash\npython -m venv .env\n```\nActivate the virtual environment. On Linux and MacOs:\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n```bash\n.env/Scripts/activate\n```\nNow you're ready to install 🤗 Transformers with the following command:\n```bash\npip install transformers\n```\nFor CPU-support only, you can conveniently install 🤗 Transformers and a deep learning library in one line. For example, install 🤗 Transformers and PyTorch with:\n```bash\npip install 'transformers[torch]'\n```\n🤗 Transformers and TensorFlow 2.0:\n```bash\npip install 'transformers[tf-cpu]'\n```\nM1 / ARM Users\nYou will need to install the following before installing TensorFLow 2.0\n```\nbrew install cmake\nbrew install pkg-config\n```\n🤗 Transformers and Flax:\n```bash\npip install 'transformers[flax]'\n```\nFinally, check if 🤗 Transformers has been properly installed by running the following command. It will download a pretrained model:\n```bash\npython -c \""from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\""\n```\nThen print out the label and score:\n```bash\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```\n## Install from source\nInstall 🤗 Transformers from source with the following command:\n```bash\npip install git+https://github.com/huggingface/transformers\n```\nThis command installs the bleeding edge `main` version rather than the latest `stable` version."", ""## Additional resources\n\\* Transformers [library](https://github.com/huggingface/transformers). \\* Transformers [docs](https://huggingface.co/docs/transformers/index). \\* Share a model [guide](https://huggingface.co/docs/transformers/model\\_sharing)."", ""Set up a development environment, for instance by running the\nfollowing command:\n```bash\npython -m venv .env\nsource .env/bin/activate\npip install -e \"".[dev]\""\n```\nand return to the parent directory\n```bash\ncd .. ```\n4. We recommend adding the PyTorch version of \\*[camelcase name of model]\\* to\nTransformers. To install PyTorch, please follow the instructions [here](https://pytorch.org/get-started/locally/). \\*\\*Note:\\*\\* You don't need to have CUDA installed. Making the new model\nwork on CPU is sufficient. 5. To port \\*[camelcase name of model]\\*, you will also need access to its\noriginal repository:\n```bash\ngit clone [link to original repo].git\ncd [lowercase name of model]\npip install -e . ```\nNow you have set up a development environment to port \\*[camelcase name of model]\\*\nto 🤗 Transformers. ### Run a pretrained checkpoint using the original repository\n\\*\\*3. Set up debugging environment\\*\\*\nAt first, you will work on the original \\*[camelcase name of model]\\* repository. Often, the original implementation is very \""researchy\"". Meaning that\ndocumentation might be lacking and the code can be difficult to\nunderstand. But this should be exactly your motivation to reimplement\n\\*[camelcase name of model]\\*. At Hugging Face, one of our main goals is to \\*make\npeople stand on the shoulders of giants\\* which translates here very well\ninto taking a working model and rewriting it to make it as \\*\\*accessible,\nuser-friendly, and beautiful\\*\\* as possible. This is the number-one\nmotivation to re-implement models into 🤗 Transformers - trying to make\ncomplex new NLP technology accessible to \\*\\*everybody\\*\\*.""]","The command used to install the requirements for a research project using 🤗 Transformers is:

```
pip install -r requirements.txt
```",pip install -r requirements.txt
"What task does the `roberta-large-mnli` checkpoint perform?
","[""- [Question answering task guide](../tasks/question\\_answering)\n\\*\\*Multiple choice\\*\\*\n- [`RobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice.ipynb). - [`TFRobertaForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice-tf.ipynb). - [Multiple choice task guide](../tasks/multiple\\_choice)\n## RobertaConfig\n[[autodoc]] RobertaConfig\n## RobertaTokenizer\n[[autodoc]] RobertaTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## RobertaTokenizerFast\n[[autodoc]] RobertaTokenizerFast\n- build\\_inputs\\_with\\_special\\_tokens\n\n## RobertaModel\n[[autodoc]] RobertaModel\n- forward\n## RobertaForCausalLM\n[[autodoc]] RobertaForCausalLM\n- forward\n## RobertaForMaskedLM\n[[autodoc]] RobertaForMaskedLM\n- forward\n## RobertaForSequenceClassification\n[[autodoc]] RobertaForSequenceClassification\n- forward\n## RobertaForMultipleChoice\n[[autodoc]] RobertaForMultipleChoice\n- forward\n## RobertaForTokenClassification\n[[autodoc]] RobertaForTokenClassification\n- forward\n## RobertaForQuestionAnswering\n[[autodoc]] RobertaForQuestionAnswering\n- forward\n\n## TFRobertaModel\n[[autodoc]] TFRobertaModel\n- call\n## TFRobertaForCausalLM\n[[autodoc]] TFRobertaForCausalLM\n- call\n## TFRobertaForMaskedLM\n[[autodoc]] TFRobertaForMaskedLM\n- call\n## TFRobertaForSequenceClassification\n[[autodoc]] TFRobertaForSequenceClassification\n- call\n## TFRobertaForMultipleChoice\n[[autodoc]] TFRobertaForMultipleChoice\n- call\n## TFRobertaForTokenClassification\n[[autodoc]] TFRobertaForTokenClassification\n- call\n## TFRobertaForQuestionAnswering\n[[autodoc]] TFRobertaForQuestionAnswering\n- call\n\n## FlaxRobertaModel\n[[autodoc]] FlaxRobertaModel\n- \\_\\_call\\_\\_\n## FlaxRobertaForCausalLM\n[[autodoc]] FlaxRobertaForCausalLM\n- \\_\\_call\\_\\_\n## FlaxRobertaForMaskedLM\n[[autodoc]] FlaxRobertaForMaskedLM\n- \\_\\_call\\_\\_\n## FlaxRobertaForSequenceClassification\n[[autodoc]] FlaxRobertaForSequenceClassification\n- \\_\\_call\\_\\_\n## FlaxRobertaForMultipleChoice\n[[autodoc]] FlaxRobertaForMultipleChoice\n- \\_\\_call\\_\\_\n## FlaxRobertaForTokenClassification\n[[autodoc]] FlaxRobertaForTokenClassification\n- \\_\\_call\\_\\_\n## FlaxRobertaForQuestionAnswering\n[[autodoc]] FlaxRobertaForQuestionAnswering\n- \\_\\_call\\_\\_"", ""- [LyCORIS checkpoints](https://github.com/KohakuBlueleaf/LyCORIS) aren't fully supported. The [`~loaders.LoraLoaderMixin.load\\_lora\\_weights`] method loads LyCORIS checkpoints with LoRA and LoCon modules, but Hada and LoKR are not supported. Loading a checkpoint from TheLastBen is very similar. For example, to load the [TheLastBen/William\\_Eggleston\\_Style\\_SDXL](https://huggingface.co/TheLastBen/William\\_Eggleston\\_Style\\_SDXL) checkpoint:\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from\\_pretrained(\""stabilityai/stable-diffusion-xl-base-1.0\"", torch\\_dtype=torch.float16).to(\""cuda\"")\npipeline.load\\_lora\\_weights(\""TheLastBen/William\\_Eggleston\\_Style\\_SDXL\"", weight\\_name=\""wegg.safetensors\"")\n# use by william eggleston in the prompt to trigger the LoRA\nprompt = \""a house by william eggleston, sunrays, beautiful, sunlight, sunrays, beautiful\""\nimage = pipeline(prompt=prompt).images[0]\nimage\n```\n## IP-Adapter\n[IP-Adapter](https://ip-adapter.github.io/) is an effective and lightweight adapter that adds image prompting capabilities to a diffusion model."", ""## XNLI\nBased on the script [`run\\_xnli.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run\\_xnli.py). [XNLI](https://cims.nyu.edu/~sbowman/xnli/) is a crowd-sourced dataset based on [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/). It is an evaluation benchmark for cross-lingual text representations. Pairs of text are labeled with textual entailment annotations for 15 different languages (including both high-resource language such as English and low-resource languages such as Swahili). #### Fine-tuning on XNLI\nThis example code fine-tunes mBERT (multi-lingual BERT) on the XNLI dataset. It runs in 106 mins on a single tesla V100 16GB. ```bash\npython run\\_xnli.py \\\n--model\\_name\\_or\\_path bert-base-multilingual-cased \\\n--language de \\\n--train\\_language en \\\n--do\\_train \\\n--do\\_eval \\\n--per\\_device\\_train\\_batch\\_size 32 \\\n--learning\\_rate 5e-5 \\\n--num\\_train\\_epochs 2.0 \\\n--max\\_seq\\_length 128 \\\n--output\\_dir /tmp/debug\\_xnli/ \\\n--save\\_steps -1\n```\nTraining with the previously defined hyper-parameters yields the following results on the \\*\\*test\\*\\* set:\n```bash\nacc = 0.7093812375249501\n```"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Image tasks with IDEFICS\n[[open-in-colab]]\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach\nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. For instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can\nsolve image-text tasks with a large multimodal model called IDEFICS. [IDEFICS](../model\\_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198),\na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image\nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content,\ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b)\nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the 🤗 Hub."", ""If you want to use one of the official checkpoints for a task, explore the [gligen](https://huggingface.co/gligen) Hub organizations! [`StableDiffusionGLIGENPipeline`] was contributed by [Nikhil Gajendrakumar](https://github.com/nikhil-masterful) and [`StableDiffusionGLIGENTextImagePipeline`] was contributed by [Nguyễn Công Tú Anh](https://github.com/tuanh123789). ## StableDiffusionGLIGENPipeline\n[[autodoc]] StableDiffusionGLIGENPipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n- enable\\_model\\_cpu\\_offload\n- prepare\\_latents\n- enable\\_fuser\n## StableDiffusionGLIGENTextImagePipeline\n[[autodoc]] StableDiffusionGLIGENTextImagePipeline\n- all\n- \\_\\_call\\_\\_\n- enable\\_vae\\_slicing\n- disable\\_vae\\_slicing\n- enable\\_vae\\_tiling\n- disable\\_vae\\_tiling\n- enable\\_model\\_cpu\\_offload\n- prepare\\_latents\n- enable\\_fuser\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput""]","The `roberta-large-mnli` checkpoint is designed for the task of natural language inference (NLI), specifically for the Multi-Genre Natural Language Inference (MNLI) dataset. This task involves determining the relationship between pairs of sentences, classifying them into categories such as entailment, contradiction, or neutral.",Text classification
"What service is replacing the Paid tier of the Inference API at Hugging Face?
","[""Access 🤗 Inference Endpoints\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account. You can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method. There are two pricing plans:\n- Inference Endpoints pricing is based on your hourly compute, and billed monthly. This can be as low as $0.06 per CPU core/hr and $0.6 per GPU/hr depending on your needs. - There is also an [Enterprise](https://huggingface.co/support) plan for Inference Endpoints which offers dedicated support, 24/7 SLAs, and uptime guarantees. Pricing for Enterprise is custom and based on volume commit and annual contracts; [contact us](https://huggingface.co/inference-endpoints/enterprise) for a quote if you're interested! After you've added a valid payment method to your account, access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/) and start deploying!"", ""These services can be called with the [`InferenceClient`] object. Please refer to [this guide](../guides/inference)\nfor more information on how to use it. ## Inference Client\n[[autodoc]] InferenceClient\n## Async Inference Client\nAn async version of the client is also provided, based on `asyncio` and `aiohttp`. To use it, you can either install `aiohttp` directly or use the `[inference]` extra:\n```sh\npip install --upgrade huggingface\\_hub[inference]\n# or\n# pip install aiohttp\n```\n[[autodoc]] AsyncInferenceClient\n## InferenceTimeoutError\n[[autodoc]] InferenceTimeoutError\n## Return types\nFor most tasks, the return value has a built-in type (string, list, image...). Here is a list for the more complex types. ### ClassificationOutput\n[[autodoc]] huggingface\\_hub.inference.\\_types.ClassificationOutput\n### ConversationalOutputConversation\n[[autodoc]] huggingface\\_hub.inference.\\_types.ConversationalOutputConversation\n### ConversationalOutput\n[[autodoc]] huggingface\\_hub.inference.\\_types.ConversationalOutput\n### ImageSegmentationOutput\n[[autodoc]] huggingface\\_hub.inference.\\_types.ImageSegmentationOutput\n### ModelStatus\n[[autodoc]] huggingface\\_hub.inference.\\_common.ModelStatus\n### TokenClassificationOutput\n[[autodoc]] huggingface\\_hub.inference.\\_types.TokenClassificationOutput\n### Text generation types\n[`~InferenceClient.text\\_generation`] task has a greater support than other tasks in `InferenceClient`. In\nparticular, user inputs and server outputs are validated using [Pydantic](https://docs.pydantic.dev/latest/)\nif this package is installed. Therefore, we recommend installing it (`pip install pydantic`)\nfor a better user experience. You can find below the dataclasses used to validate data and in particular [`~huggingface\\_hub.inference.\\_text\\_generation.TextGenerationParameters`] (input),\n[`~huggingface\\_hub.inference.\\_text\\_generation.TextGenerationResponse`] (output) and\n[`~huggingface\\_hub.inference.\\_text\\_generation.TextGenerationStreamResponse`] (streaming output). [[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.TextGenerationParameters\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.TextGenerationResponse\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.TextGenerationStreamResponse\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.InputToken\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.Token\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.FinishReason\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.BestOfSequence\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.Details\n[[autodoc]] huggingface\\_hub.inference.\\_text\\_generation.StreamDetails\n## InferenceAPI\n[`InferenceAPI`] is the legacy way to call the Inference API."", ""The ability to use Hugging Face’s comprehensive hub of foundation models means that users can pick the models that best align with their business needs without having to invest in the resources required to train them. This integration is a significant step forward in making foundation models more accessible to enterprises around the world. \\_If you’re interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!\\_"", ""--\ntitle: \""An overview of inference solutions on Hugging Face\""\nthumbnail: /blog/assets/116\\_inference\\_update/widget.png\nauthors:\n- user: juliensimon\n---\n# An Overview of Inference Solutions on Hugging Face\nEvery day, developers and organizations are adopting models hosted on [Hugging Face](https://huggingface.co/models) to turn ideas into proof-of-concept demos, and demos into production-grade applications. For instance, Transformer models have become a popular architecture for a wide range of machine learning (ML) applications, including natural language processing, computer vision, speech, and more. Recently, diffusers have become a popular architecuture for text-to-image or image-to-image generation. Other architectures are popular for other tasks, and we host all of them on the HF Hub! At Hugging Face, we are obsessed with simplifying ML development and operations without compromising on state-of-the-art quality. In this respect, the ability to test and deploy the latest models with minimal friction is critical, all along the lifecycle of an ML project. Optimizing the cost-performance ratio is equally important, and we'd like to thank our friends at [Intel](https://huggingface.co/intel) for sponsoring our free CPU-based inference solutions. This is another major step in our [partnership](https://huggingface.co/blog/intel). It's also great news for our user community, who can now enjoy the speedup delivered by the [Intel Xeon Ice Lake](https://www.intel.com/content/www/us/en/products/docs/processors/xeon/3rd-gen-xeon-scalable-processors-brief.html) architecture at zero cost. Now, let's review your inference options with Hugging Face. ## Free Inference Widget\nOne of my favorite features on the Hugging Face hub is the Inference [Widget](https://huggingface.co/docs/hub/models-widgets). Located on the model page, the Inference Widget lets you upload sample data and predict it in a single click. Here's a sentence similarity example with the `sentence-transformers/all-MiniLM-L6-v2` [model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2):\n`![](assets/116_inference_update/widget.png)`\nIt's the best way to quickly get a sense of what a model does, its output, and how it performs on a few samples from your dataset."", ""Benefit from higher rate limits, custom accelerated endpoints for the latest models, and early access to features. If you've built some exciting projects with the Inference API or are looking for a model not available in Inference for PROs, please [use this discussion](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/13). [Enterprise users](https://huggingface.co/enterprise) also benefit from PRO Inference API on top of other features, such as SSO. ## FAQ\n\n**Does this affect the free Inference API?**\n\nNo. We still expose thousands of models through free APIs that allow people to prototype and explore model capabilities quickly. **Does this affect Enterprise users?**\n\nUsers with an Enterprise subscription also benefit from accelerated inference API for curated models. **Can I use my own models with PRO Inference API?**\n\nThe free Inference API already supports a wide range of small and medium models from a variety of libraries (such as diffusers, transformers, and sentence transformers). If you have a custom model or custom inference logic, we recommend using [Inference Endpoints](https://ui.endpoints.huggingface.co/catalog).""]","The Paid tier of the Inference API at Hugging Face is being replaced by the Inference Endpoints service. This new service offers features such as higher rate limits, custom accelerated endpoints for the latest models, and early access to features.",Inference Endpoints
"What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# SqueezeBERT\n## Overview\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers."", ""In the original model it is possible to use an average pooling layer with a fixed 7x7 window and stride 1 instead of global pooling. For inputs that are larger than the recommended image size, this gives a pooled output that is larger than 1x1. The Hugging Face implementation does not support this. - The original TensorFlow checkpoints include quantized models. We do not support these models as they include additional \""FakeQuantization\"" operations to unquantize the weights. - It's common to extract the output from the expansion layers at indices 10 and 13, as well as the output from the final 1x1 convolution layer, for downstream purposes. Using `output\\_hidden\\_states=True` returns the output from all intermediate layers. There is currently no way to limit this to specific layers. - The DeepLabV3+ segmentation head does not use the final convolution layer from the backbone, but this layer gets computed anyway. There is currently no way to tell [`MobileNetV2Model`] up to which layer it should run."", ""EfficientNet (Knapsack Pruned)\n\\*\\*EfficientNet\\*\\* is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a \\*compound coefficient\\*. Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use \\\\( 2^N \\\\) times more computational resources, then we can simply increase the network depth by \\\\( \\alpha ^ N \\\\), width by \\\\( \\beta ^ N \\\\), and image size by \\\\( \\gamma ^ N \\\\), where \\\\( \\alpha, \\beta, \\gamma \\\\) are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient \\\\( \\phi \\\\) to uniformly scales network width, depth, and resolution in a principled way. The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image. The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of [MobileNetV2](https://paperswithcode.com/method/mobilenetv2), in addition to [squeeze-and-excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block)."", ""This can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion, especially with a small number of layers. Growing the receptive field has been shown to improve quality significantly (Salimans et al.). Doing so, however, comes at a significant cost in number of parameters and consequently computational performance and can make training such models more challenging. >\n> ... self-attention can achieve a better balance in the trade-off between the virtually unlimited receptive field of the necessarily sequential PixelRNN and the limited receptive field of the much more parallelizable PixelCNN and its various extensions. [Image Transformer](https://arxiv.org/abs/1802.05751) uses transformers by restricting self attention over local neighborhoods of pixels. [Taming Transformers](https://arxiv.org/abs/2012.09841) and [DALL-E 1](https://arxiv.org/abs/2102.12092) combine convolutions and transformers. Both train a VQ-VAE to learn a discrete latent space, and then a transformer is trained in the compressed latent space. The transformer context is global but masked, because attention is provided over all previously predicted latent pixels, but the model is still AR so attention cannot be provided over not yet predicted pixels. [ImageBART](https://arxiv.org/abs/2108.08827) combines convolutions, transformers, and diffusion processes."", ""This is illustrated examplary for the second\nencoder block (red boxes at bottow) whose weight parameters\n\\\\(\\theta\\_{\\text{enc}}^{\\text{self-attn}, 2}\\\\) and\n\\\\(\\theta\\_{\\text{enc}}^{\\text{feed-forward}, 2}\\\\) are set to BERT\\'s weight\nparameters \\\\(\\theta\\_{\\text{BERT}}^{\\text{feed-forward}, 2}\\\\) and\n\\\\(\\theta\\_{\\text{BERT}}^{\\text{self-attn}, 2}\\\\), respectively at\ninitialization. Before fine-tuning, the encoder therefore behaves exactly like a\npre-trained BERT model. Assuming the input sequence\n\\\\(\\mathbf{x}\\_1, \\ldots, \\mathbf{x}\\_n\\\\) (shown in green) passed to the\nencoder is equal to the input sequence\n\\\\(\\mathbf{x}\\_1^{\\text{BERT}}, \\ldots, \\mathbf{x}\\_n^{\\text{BERT}}\\\\) (shown\nin grey) passed to BERT, this means that the respective output vector\nsequences \\\\(\\mathbf{\\overline{x}}\\_1, \\ldots, \\mathbf{\\overline{x}}\\_n\\\\)\n(shown in darker green) and\n\\\\(\\mathbf{\\overline{x}}\\_1^{\\text{BERT}}, \\ldots, \\mathbf{\\overline{x}}\\_n^{\\text{BERT}}\\\\)\n(shown in darker grey) also have to be equal. Next, let\\'s illustrate how the decoder is warm-started. ![texte du\nlien](https://raw.githubusercontent.com/patrickvonplaten/scientific\\_images/master/encoder\\_decoder/leverage\\_decoder.png)\nThe architecture of the decoder is different from BERT\\'s architecture\nin three ways. 1. First, the decoder has to be conditioned on the contextualized\nencoded sequence \\\\(\\mathbf{\\overline{X}}\\_{1:n}\\\\) by means of\ncross-attention layers. Consequently, randomly initialized\ncross-attention layers are added between the self-attention layer\nand the two feed-forward layers in each BERT block. This is\nrepresented exemplary for the second block by\n\\\\(+\\theta\\_{\\text{dec}}^{\\text{cross-attention, 2}}\\\\) and illustrated\nby the newly added fully connected graph in red in the lower red box\non the right. This necessarily changes the behavior of each modified\nBERT block so that an input vector, \\*e.g.\\* \\\\(\\mathbf{y'}\\_0\\\\) now\nyields a random output vector \\\\(\\mathbf{y''}\\_0\\\\) (highlighted by the\nred border around the output vector \\\\(\\mathbf{y''}\\_0\\\\)). 2. Second, BERT\\'s \\*bi-directional\\* self-attention layers have to be\nchanged to \\*uni-directional\\* self-attention layers to comply with\nauto-regressive generation. Because both the bi-directional and the\nuni-directional self-attention layer are based on the same \\*key\\*,\n\\*query\\* and \\*value\\* projection weights, the decoder\\'s\nself-attention layer weights can be initialized with BERT\\'s\nself-attention layer weights. \\*E.g.\\* the query, key and value weight\nparameters of the decoder\\'s uni-directional self-attention layer\nare initialized with those of BERT\\'s bi-directional self-attention\nlayer \\\\(\\theta\\_{\\text{BERT}}^{\\text{self-attn}, 2} = \\{\\mathbf{W}\\_{\\text{BERT}, k}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{BERT}, v}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{BERT}, q}^{\\text{self-attn}, 2} \\} \\to \\theta\\_{\\text{dec}}^{\\text{self-attn}, 2} = \\{\\mathbf{W}\\_{\\text{dec}, k}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{dec}, v}^{\\text{self-attn}, 2}, \\mathbf{W}\\_{\\text{dec}, q}^{\\text{self-attn}, 2} \\}. \\\\) However, in \\*uni-directional\\* self-attention each token only\nattends to all previous tokens, so that the decoder\\'s\nself-attention layers yield different output vectors than BERT\\'s\nself-attention layers even though they share the same weights.""]","SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers.",Grouped convolutions
"What type of license is the HuggingFace Team's software distributed under?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction"", ""The ability to use Hugging Face’s comprehensive hub of foundation models means that users can pick the models that best align with their business needs without having to invest in the resources required to train them. This integration is a significant step forward in making foundation models more accessible to enterprises around the world. \\_If you’re interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!\\_"", ""Please give it a try and let us know what you think. We'd love to read your feedback on the Hugging Face [forum](https://discuss.huggingface.co/). Thank you for reading!"", ""Security\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)"", ""For more details, reach out to HF Support to address any questions or concerns at website@huggingface.co""]","The HuggingFace Team's software is distributed under the Apache License, Version 2.0.","Apache License, Version 2.0"
"What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?
","[""It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\n\\*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\*\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A."", ""Model Based Reinforcement Learning (MBRL)\nModel-based reinforcement learning only differs from its model-free counterpart in learning a \\*dynamics model\\*, but that has substantial downstream effects on how the decisions are made. The dynamics model usually models the environment transition dynamics, \\\\( s\\_{t+1} = f\\_\\theta (s\\_t, a\\_t) \\\\), but things like inverse dynamics models (mapping from states to actions) or reward models (predicting rewards) can be used in this framework. ## Simple definition\n- There is an agent that repeatedly tries to solve a problem, \\*\\*accumulating state and action data\\*\\*. - With that data, the agent creates a structured learning tool, \\*a dynamics model\\*, to reason about the world. - With the dynamics model, the agent \\*\\*decides how to act by predicting the future\\*\\*. - With those actions, \\*\\*the agent collects more data, improves said model, and hopefully improves future actions\\*\\*. ## Academic definition\nModel-based reinforcement learning (MBRL) follows the framework of an agent interacting in an environment, \\*\\*learning a model of said environment\\*\\*, and then \\*\\*leveraging the model for control (making decisions). Specifically, the agent acts in a Markov Decision Process (MDP) governed by a transition function \\\\( s\\_{t+1} = f (s\\_t , a\\_t) \\\\) and returns a reward at each step \\\\( r(s\\_t, a\\_t) \\\\). With a collected dataset \\\\( D :={ s\\_i, a\\_i, s\\_{i+1}, r\\_i} \\\\), the agent learns a model, \\\\( s\\_{t+1} = f\\_\\theta (s\\_t , a\\_t) \\\\) \\*\\*to minimize the negative log-likelihood of the transitions\\*\\*. We employ sample-based model-predictive control (MPC) using the learned dynamics model, which optimizes the expected reward over a finite, recursively predicted horizon, \\\\( \\tau \\\\), from a set of actions sampled from a uniform distribution \\\\( U(a) \\\\), (see [paper](https://arxiv.org/pdf/2002.04523) or [paper](https://arxiv.org/pdf/2012.09156.pdf) or [paper](https://arxiv.org/pdf/2009.01221.pdf)). ## Further reading\nFor more information on MBRL, we recommend you check out the following resources:\n- A [blog post on debugging MBRL](https://www.natolambert.com/writing/debugging-mbrl)."", ""!---\nCopyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n# 🤗 PEFT\n\n### State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods\n\nParameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. Fine-tuning large-scale PLMs is often prohibitively costly. In this regard, PEFT methods only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs. Recent State-of-the-Art PEFT techniques achieve performance comparable to that of full fine-tuning."", ""- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the 🤗 Hugging Face Course. - Check the [Question answering task guide](../tasks/question\\_answering) on how to use the model. \\*\\*Multiple choice\\*\\*\n- [`AlbertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice.ipynb). - [`TFAlbertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple\\_choice-tf.ipynb). - Check the [Multiple choice task guide](../tasks/multiple\\_choice) on how to use the model. ## AlbertConfig\n[[autodoc]] AlbertConfig\n## AlbertTokenizer\n[[autodoc]] AlbertTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## AlbertTokenizerFast\n[[autodoc]] AlbertTokenizerFast\n## Albert specific outputs\n[[autodoc]] models.albert.modeling\\_albert.AlbertForPreTrainingOutput\n[[autodoc]] models.albert.modeling\\_tf\\_albert.TFAlbertForPreTrainingOutput\n\n## AlbertModel\n[[autodoc]] AlbertModel\n- forward\n## AlbertForPreTraining\n[[autodoc]] AlbertForPreTraining\n- forward\n## AlbertForMaskedLM\n[[autodoc]] AlbertForMaskedLM\n- forward\n## AlbertForSequenceClassification\n[[autodoc]] AlbertForSequenceClassification\n- forward\n## AlbertForMultipleChoice\n[[autodoc]] AlbertForMultipleChoice\n## AlbertForTokenClassification\n[[autodoc]] AlbertForTokenClassification\n- forward\n## AlbertForQuestionAnswering\n[[autodoc]] AlbertForQuestionAnswering\n- forward\n\n## TFAlbertModel\n[[autodoc]] TFAlbertModel\n- call\n## TFAlbertForPreTraining\n[[autodoc]] TFAlbertForPreTraining\n- call\n## TFAlbertForMaskedLM\n[[autodoc]] TFAlbertForMaskedLM\n- call\n## TFAlbertForSequenceClassification\n[[autodoc]] TFAlbertForSequenceClassification\n- call\n## TFAlbertForMultipleChoice\n[[autodoc]] TFAlbertForMultipleChoice\n- call\n## TFAlbertForTokenClassification\n[[autodoc]] TFAlbertForTokenClassification\n- call\n## TFAlbertForQuestionAnswering\n[[autodoc]] TFAlbertForQuestionAnswering\n- call\n\n## FlaxAlbertModel\n[[autodoc]] FlaxAlbertModel\n- \\_\\_call\\_\\_\n## FlaxAlbertForPreTraining\n[[autodoc]] FlaxAlbertForPreTraining\n- \\_\\_call\\_\\_\n## FlaxAlbertForMaskedLM\n[[autodoc]] FlaxAlbertForMaskedLM\n- \\_\\_call\\_\\_\n## FlaxAlbertForSequenceClassification\n[[autodoc]] FlaxAlbertForSequenceClassification\n- \\_\\_call\\_\\_\n## FlaxAlbertForMultipleChoice\n[[autodoc]] FlaxAlbertForMultipleChoice\n- \\_\\_call\\_\\_\n## FlaxAlbertForTokenClassification\n[[autodoc]] FlaxAlbertForTokenClassification\n- \\_\\_call\\_\\_\n## FlaxAlbertForQuestionAnswering\n[[autodoc]] FlaxAlbertForQuestionAnswering\n- \\_\\_call\\_\\_"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# AdaLoRA\n[AdaLoRA](https://hf.co/papers/2303.10512) is a method for optimizing the number of trainable parameters to assign to weight matrices and layers, unlike LoRA, which distributes parameters evenly across all modules. More parameters are budgeted for important weight matrices and layers while less important ones receive fewer parameters. The abstract from the paper is:\n\\*Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings. Our code is publicly available at https://github.com/QingruZhang/AdaLoRA\\*. ## AdaLoraConfig\n[[autodoc]] tuners.adalora.config.AdaLoraConfig\n## AdaLoraModel\n[[autodoc]] tuners.adalora.model.AdaLoraModel""]","The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:

1. Splitting the embedding matrix into two smaller matrices.
2. Using repeating layers split among groups.",Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.
"What are the three main steps for fine-tuning a model with the 🤗 Datasets library?
","[""Introduction[[introduction]]\nIn [Chapter 3](/course/chapter3) you got your first taste of the 🤗 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics. But this is just scratching the surface of what 🤗 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n\\* What do you do when your dataset is not on the Hub? \\* How can you slice and dice a dataset? (And what if you \\_really\\_ need to use Pandas?)\n\\* What do you do when your dataset is huge and will melt your laptop's RAM? \\* What the heck are \""memory mapping\"" and Apache Arrow? \\* How can you create your own dataset and push it to the Hub? The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!"", ""To verify that all libraries are correctly installed, you can run the following command in a Python shell. It verifies that both `transformers` and `datasets` have been correclty installed. ```python\nfrom transformers import AutoModelForCTC, AutoProcessor\nfrom datasets import load\\_dataset\ndummy\\_dataset = load\\_dataset(\""common\\_voice\"", \""ab\"", split=\""test\"")\nmodel = AutoModelForCTC.from\\_pretrained(\""hf-internal-testing/tiny-random-wav2vec2\"")\nmodel.to(\""cuda\"")\nprocessor = AutoProcessor.from\\_pretrained(\""hf-internal-testing/tiny-random-wav2vec2\"")\ninput\\_values = processor(dummy\\_dataset[0][\""audio\""][\""array\""], return\\_tensors=\""pt\"", sampling\\_rate=16\\_000).input\\_values\ninput\\_values = input\\_values.to(\""cuda\"")\nlogits = model(input\\_values).logits\nassert logits.shape[-1] == 32\n```\n## How to finetune an acoustic model\nIn this section, we show you how to fine-tune a pre-trained [XLS-R Model](https://huggingface.co/docs/transformers/model\\_doc/xls\\_r) on the [Common Voice 7 dataset](https://huggingface.co/datasets/mozilla-foundation/common\\_voice\\_7\\_0)."", ""tf\\_train\\_dataset,\n... validation\\_data=tf\\_eval\\_dataset,\n... callbacks=callbacks,\n... epochs=num\\_epochs,\n... )\n```\nCongratulations! You have fine-tuned your model and shared it on the 🤗 Hub. You can now use it for inference! ### Inference\nGreat, now that you've finetuned a model, you can use it for inference! Load an image for inference:\n```py\n>>> image = ds[0][\""image\""]\n>>> image\n```\n\n![Image of bedroom](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png)\n\nWe will now see how to infer without a pipeline. Process the image with an image processor and place the `pixel\\_values` on a GPU:\n```py\n>>> device = torch.device(\""cuda\"" if torch.cuda.is\\_available() else \""cpu\"") # use GPU if available, otherwise use a CPU\n>>> encoding = image\\_processor(image, return\\_tensors=\""pt\"")\n>>> pixel\\_values = encoding.pixel\\_values.to(device)\n```\nPass your input to the model and return the `logits`:\n```py\n>>> outputs = model(pixel\\_values=pixel\\_values)\n>>> logits = outputs.logits.cpu()\n```\nNext, rescale the logits to the original image size:\n```py\n>>> upsampled\\_logits = nn.functional.interpolate(\n..."", ""Considerations for model evaluation\nDeveloping an ML model is rarely a one-shot deal: it often involves multiple stages of defining the model architecture and tuning hyper-parameters before converging on a final set. Responsible model evaluation is a key part of this process, and 🤗 Evaluate is here to help! Here are some things to keep in mind when evaluating your model using the 🤗 Evaluate library:\n## Properly splitting your data\nGood evaluation generally requires three splits of your dataset:\n- \\*\\*train\\*\\*: this is used for training your model. - \\*\\*validation\\*\\*: this is used for validating the model hyperparameters. - \\*\\*test\\*\\*: this is used for evaluating your model. Many of the datasets on the 🤗 Hub are separated into 2 splits: `train` and `validation`; others are split into 3 splits (`train`, `validation` and `test`) -- make sure to use the right split for the right purpose! Some datasets on the 🤗 Hub are already separated into these three splits. However, there are also many that only have a train/validation or only train split. If the dataset you're using doesn't have a predefined train-test split, it is up to you to define which part of the dataset you want to use for training your model and which you want to use for hyperparameter tuning or final evaluation. Training and evaluating on the same split can misrepresent your results! If you overfit on your training data the evaluation results on that split will look great but the model will perform poorly on new data. Depending on the size of the dataset, you can keep anywhere from 10-30% for evaluation and the rest for training, while aiming to set up the test set to reflect the production data as close as possible. Check out [this thread](https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090) for a more in-depth discussion of dataset splitting! ## The impact of class imbalance\nWhile many academic datasets, such as the [IMDb dataset](https://huggingface.co/datasets/imdb) of movie reviews, are perfectly balanced, most real-world datasets are not. In machine learning a \\*balanced dataset\\* corresponds to a datasets where all labels are represented equally. In the case of the IMDb dataset this means that there are as many positive as negative reviews in the dataset. In an imbalanced dataset this is not the case: in fraud detection for example there are usually many more non-fraud cases than fraud cases in the dataset. Having an imbalanced dataset can skew the results of your metrics. Imagine a dataset with 99 \""non-fraud\"" cases and 1 \""fraud\"" case. A simple model that always predicts \""non-fraud\"" cases would give yield a 99% accuracy which might sound good at first until you realize that you will never catch a fraud case."", ""!---\nCopyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n# Token classification with LayoutLMv3 (PyTorch version)\nThis directory contains a script, `run\\_funsd\\_cord.py`, that can be used to fine-tune (or evaluate) LayoutLMv3 on form understanding datasets, such as [FUNSD](https://guillaumejaume.github.io/FUNSD/) and [CORD](https://github.com/clovaai/cord). The script `run\\_funsd\\_cord.py` leverages the 🤗 Datasets library and the Trainer API. You can easily customize it to your needs. ## Fine-tuning on FUNSD\nFine-tuning LayoutLMv3 for token classification on [FUNSD](https://guillaumejaume.github.io/FUNSD/) can be done as follows:\n```bash\npython run\\_funsd\\_cord.py \\\n--model\\_name\\_or\\_path microsoft/layoutlmv3-base \\\n--dataset\\_name funsd \\\n--output\\_dir layoutlmv3-test \\\n--do\\_train \\\n--do\\_eval \\\n--max\\_steps 1000 \\\n--evaluation\\_strategy steps \\\n--eval\\_steps 100 \\\n--learning\\_rate 1e-5 \\\n--load\\_best\\_model\\_at\\_end \\\n--metric\\_for\\_best\\_model \""eval\\_f1\"" \\\n--push\\_to\\_hub \\\n--push\\_to\\_hub°model\\_id layoutlmv3-finetuned-funsd\n```\n👀 The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd.""]","The three main steps for fine-tuning a model with the 🤗 Datasets library are:

1. Load a dataset from the Hugging Face Hub.
2. Preprocess the data with `Dataset.map()`.
3. Load and compute metrics.",1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.
"What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?
","[""It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark. Figure 4. Latency (Batch=1, Physical Cores=2)\n\n---\n## Conclusion\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput. Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens. The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial)\n## Resources\n\\* [Hugging Face Infinity](https://huggingface.co/infinity)\n\\* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\n\\* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i)\n\\* [DistilBERT](https://huggingface.co/docs/transformers/model\\_doc/distilbert)\n\\* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\\* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\n\\* [🤗 Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)"", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\n🤗 Optimum provides an API called BetterTransformer, a fast path of standard PyTorch Transformer APIs to benefit from interesting speedups on CPU & GPU through sparsity and fused kernels as Flash Attention. For now, BetterTransformer supports the fastpath from the native [`nn.TransformerEncoderLayer`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) as well as Flash Attention and Memory-Efficient Attention from [`torch.nn.functional.scaled\\_dot\\_product\\_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled\\_dot\\_product\\_attention.html)."", ""With Optimum and the Hugging Face Hardware Program, that’s just not an issue. It’s essentially plug-and-play\"". ## SOTA Models meet SOTA Hardware\nPrior to the announcement of the Hugging Face Partnership, we had demonstrated the power of the IPU to accelerate state-of-the-art Transformer models with a special Graphcore-optimised implementation of Hugging Face BERT using Pytorch. Full details of this example can be found in the Graphcore blog [BERT-Large training on the IPU explained](https://www.graphcore.ai/posts/bert-large-training-on-the-ipu-explained). The dramatic benchmark results for BERT running on a Graphcore system, compared with a comparable GPU-based system are surely a tantalising prospect for anyone currently running the popular NLP model on something other than the IPU. ![assets/24\\_sahajBERT/roles\\_tasks.png](assets/26\\_graphcore-ipu/graphcore-ipu-bert-large.png)\nThis type of acceleration can be game changing for machine learning researchers and engineers, winning them back valuable hours of training time and allowing them many more iterations when developing new models. Now Graphcore users will be able to unlock such performance advantages, through the Hugging Face platform, with its elegant simplicity and superlative range of models. Together, Hugging Face and Graphcore are helping even more people to access the power of Transformers and accelerate the AI revolution. \\*Visit the [Hugging Face Hardware Partner portal](https://huggingface.co/hardware) to learn more about Graphcore IPU systems and how to gain access\\*"", ""--\ntitle: \""How we sped up transformer inference 100x for 🤗 API customers\""\nthumbnail: /blog/assets/09\\_accelerated\\_inference/thumbnail.png\n---\n# How we sped up transformer inference 100x for 🤗 API customers\n🤗 Transformers has become the default library for data scientists all around the world to explore state of the art NLP models and build new NLP features. With over 5,000 pre-trained and fine-tuned models available, in over 250 languages, it is a rich playground, easily accessible whichever framework you are working in. While experimenting with models in 🤗 Transformers is easy, deploying these large models into production with maximum performance, and managing them into an architecture that scales with usage is a \\*\\*hard engineering challenge\\*\\* for any Machine Learning Engineer. This 100x performance gain and built-in scalability is why subscribers of our hosted [Accelerated Inference API](https://huggingface.co/pricing) chose to build their NLP features on top of it. To get to the \\*\\*last 10x of performance\\*\\* boost, the optimizations need to be low-level, specific to the model, and to the target hardware. This post shares some of our approaches squeezing every drop of compute juice for our customers. 🍋\n## Getting to the first 10x speedup\nThe first leg of the optimization journey is the most accessible, all about using the best combination of techniques offered by the [Hugging Face libraries](https://github.com/huggingface/), independent of the target hardware. We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main\\_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:\n-| Naive version | Optimized version |\n-|:---------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------:|\n-|![](/blog/assets/09\\_accelerated\\_inference/unoptimized\\_graph.png)|![](/blog/assets/09\\_accelerated\\_inference/optimized\\_graph.png)|\nTokenization is often a bottleneck for efficiency during inference. We use the most efficient methods from the [🤗 Tokenizers](https://github.com/huggingface/tokenizers/) library, leveraging the Rust implementation of the model tokenizer in combination with smart caching to get up to 10x speedup for the overall latency. Leveraging the latest features of the Hugging Face libraries, we achieve a reliable 10x speed up compared to an out-of-box deployment for a given model/hardware pair. As new releases of Transformers and Tokenizers typically ship every month, our API customers do not need to constantly adapt to new optimization opportunities, their models just keep running faster."", ""--\ntitle: \""Optimum+ONNX Runtime - Easier, Faster training for your Hugging Face models\""\nthumbnail: /blog/assets/optimum\\_onnxruntime-training/thumbnail.png\nauthors:\n- user: Jingya\n- user: kshama-msft\nguest: true\n- user: askhade\nguest: true\n- user: weicwang\nguest: true\n- user: zhijiang\nguest: true\n---\n# Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models\n## Introduction\nTransformer based models in language, vision and speech are getting larger to support complex multi-modal use cases for the end customer. Increasing model sizes directly impact the resources needed to train these models and scale them as the size increases. Hugging Face and Microsoft’s ONNX Runtime teams are working together to build advancements in finetuning large Language, Speech and Vision models. Hugging Face’s [Optimum library](https://huggingface.co/docs/optimum/index), through its integration with ONNX Runtime for training, provides an open solution to \\_\\_improve training times by 35% or more\\_\\_ for many popular Hugging Face models. We present details of both Hugging Face Optimum and the ONNX Runtime Training ecosystem, with performance numbers highlighting the benefits of using the Optimum library. ## Performance results\nThe chart below shows impressive acceleration \\_\\_from 39% to 130%\\_\\_ for Hugging Face models with Optimum when \\_\\_using ONNX Runtime and DeepSpeed ZeRO Stage 1\\_\\_ for training. The performance measurements were done on selected Hugging Face models with PyTorch as the baseline run, only ONNX Runtime for training as the second run, and ONNX Runtime + DeepSpeed ZeRO Stage 1 as the final run, showing maximum gains.""]",Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers.,+800%
"What is the command to upload a spaCy pipeline to the Hugging Face Hub?
","[""Hub methods\nMethods for using the Hugging Face Hub:\n## Push to hub\n[[autodoc]] evaluate.push\\_to\\_hub"", ""\\* Launch [blog post](https://huggingface.co/blog/spacy)\n\\* spaCy v 3.1 [Announcement](https://explosion.ai/blog/spacy-v3-1#huggingface-hub)\n\\* spaCy [documentation](https://spacy.io/universe/project/spacy-huggingface-hub/)"", ""Security\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)"", ""Enterprise Hub\nEnterprise Hub adds advanced capabilities to organizations, enabling safe, compliant and managed collaboration for companies and teams on Hugging Face."", ""Defaults to `hub` in the current working directory. |\n| `--verbose`, `-V` | bool | Output additional info for debugging, e.g. the full generated hub metadata. |\nYou can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`. ```bash\nhuggingface-cli login\npython -m spacy package ./en\\_ner\\_fashion ./output --build wheel\ncd ./output/en\\_ner\\_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en\\_ner\\_fashion-0.0.0-py3-none-any.whl\n```\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card. The command will output two things:\n\\* Where to find your repo in the Hub! For example, https://huggingface.co/spacy/en\\_core\\_web\\_sm\n\\* And how to install the pipeline directly from the Hub! ### From a Python script\nYou can use the `push` function from Python. It returns a dictionary containing the `\""url\""` and \""`whl\\_url`\"" of the published model and the wheel file, which you can later install with `pip install`. ```py\nfrom spacy\\_huggingface\\_hub import push\nresult = push(\""./en\\_ner\\_fashion-0.0.0-py3-none-any.whl\"")\nprint(result[\""url\""])\n```\n## Additional resources\n\\* spacy-huggingface-hub [library](https://github.com/explosion/spacy-huggingface-hub).""]","To upload a spaCy pipeline to the Hugging Face Hub, you can use the following command:

```bash
huggingface-cli login
python -m spacy package ./en_ner_fashion ./output --build wheel
cd ./output/en_ner_fashion-0.0.0/dist
python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
```

This will package your spaCy pipeline and push it to the Hub.",python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
"What is the time and memory complexity of the Nyströmformer's approximation of self-attention?
","[""--\ntitle: \""Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\""\nthumbnail: /blog/assets/86\\_nystromformer/thumbnail.png\nauthors:\n- user: asi\nguest: true\n---\n# Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\n## Introduction\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. The [Nyströmformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nyströmformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nyström method and how it can be adapted to approximate self-attention. ## Nyström method for matrix approximation\nAt the heart of Nyströmformer is the Nyström method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nyström method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:\n\nRepresenting P as a block matrix\n\nWe now have four submatrices: \\\\(A\\_P, B\\_P, F\\_P,\\\\) and \\\\(C\\_P\\\\), with sizes \\\\(m \\times m, m \\times (n - m), (n - m) \\times m\\\\) and\n\\\\((n - m) \\times (n - m)\\\\) respectively. The \\\\(m\\\\) sampled columns are contained in \\\\(A\\_P\\\\) and \\\\(F\\_P\\\\), whereas the \\\\(m\\\\) sampled rows are contained in \\\\(A\\_P\\\\) and \\\\(B\\_P\\\\). So, the entries of \\\\(A\\_P, B\\_P,\\\\) and \\\\(F\\_P\\\\) are known to us, and we will estimate \\\\(C\\_P\\\\). According to the Nyström method, \\\\(C\\_P\\\\) is given by:\n$$C\\_P = F\\_P A\\_P^+ B\\_P$$\nHere, \\\\(+\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). Thus, the Nyström approximation of \\\\(P, \\hat{P}\\\\) can be written as:\n\nNyström approximation of P\n\nAs shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices."", ""Only at longer sequence lengths of 4096, a slight decrease in memory usage can be seen. Let's see what happens to the memory peak usage if we increase the size of the feed forward layer by a factor of 4 and reduce the number of attention heads also by a factor of 4 so that the feed forward layer becomes the memory bottleneck. ```\nconfig\\_no\\_chunk = ReformerConfig.from\\_pretrained(\""google/reformer-enwik8\"", chunk\\_size\\_feed\\_forward=0, num\\_attention\\_{h}eads=2, feed\\_forward\\_size=16384) # no chuck\nconfig\\_chunk = ReformerConfig.from\\_pretrained(\""google/reformer-enwik8\"", chunk\\_size\\_feed\\_forward=1, num\\_attention\\_{h}eads=2, feed\\_forward\\_size=16384) # feed forward chunk\nbenchmark\\_args = PyTorchBenchmarkArguments(sequence\\_lengths=[1024, 2048, 4096], batch\\_sizes=[8], models=[\""Reformer-No-Chunk\"", \""Reformer-Chunk\""], no\\_speed=True, no\\_env\\_print=True)\nbenchmark = PyTorchBenchmark(configs=[config\\_no\\_chunk, config\\_chunk], args=benchmark\\_args)\nresult = benchmark.run()\n```\n1 / 2\n2 / 2\n==================== INFERENCE - MEMORY - RESULT ====================\n--------------------------------------------------------------------------------\nModel Name Batch Size Seq Length Memory in MB\n--------------------------------------------------------------------------------\nReformer-No-Chunk 8 1024 3743\nReformer-No-Chunk 8 2048 5539\nReformer-No-Chunk 8 4096 9087\nReformer-Chunk 8 1024 2973\nReformer-Chunk 8 2048 3999\nReformer-Chunk 8 4096 6011\n--------------------------------------------------------------------------------\nNow a clear decrease in peak memory usage can be seen for longer input sequences."", ""Figure 8. TorchScript latencies with respect to the number of cores involved\n\nFigure 9. Google's TensorFlow latencies with respect to the number of cores involved\n\nFigure 10. Google's TensorFlow with oneDNN enabled latencies with respect to the number of cores involved\n\nFigure 11. Intel TensorFlow latencies with respect to the number of cores involved\n\nThis is often referred to as “tracing” the graph and, as you can see here, the results are not that different from TorchScript (Graph execution mode from PyTorch) vs TensorFlow(s). All TensorFlow implementations seem to perform better than TorchScript when the parallelization is limited (low number of cores involved in the intra operation computations) but this seems not to scale efficiently\nas we increase the computation resources, whereas TorchScript seems to be able to better leverage the power of modern CPUs. Still, the margin between all these frameworks in most cases very limited. ### Tuning the Memory Allocator: Can this impact the latencies observed? One crucial component every program dynamically allocating memory relies on is the memory allocator. If you are familiar with C/C++ programming this component provides the low bits to malloc/free or new/delete. Most of the time you don’t have to worry too much about it and the default ones (glibc for instance on most Linux distributions) will provide great performances out of the box. Still, in some situations it might not provide the most efficient performances, as these default allocators are most of the time designed to be “good” most of the time,\nand not fine-tuned for specific workloads or parallelism. So, what are the alternatives, and when are they more suitable than the default ones? Well, again, it depends on the kind of context around your software. Possible situations are a heavy number of allocations/de-allocations causing fragmentation over time,\nspecific hardware and/or architecture you’re executing your software on and finally the level of parallelism of your application. Do you see where this is going? Deep learning and by extension all the applications doing heavy computations are heavily multi-threaded,\nthat’s also the case for software libraries such as PyTorch, TensorFlow and any other frameworks targeting Machine Learning workloads. The default memory allocator strategies often rely on global memory pools which require the usage of synchronization primitives to operate,\nincreasing the overall pressure on the system, reducing the performance of your application. Some recent works by companies such as Google, Facebook and Microsoft provided alternative memory allocation strategies implemented in custom memory allocator libraries\none can easily integrate directly within its software components or use dynamic shared library preload to swap the library being used to achieve the allocation/de-allocation. Among these libraries, we can cite a few of them such as [tcmalloc](), [jemalloc]() and [mimalloc](). Figure 12. Various memory allocators benchmarked on different tasks\n\nThrough this blog post we will only focus on benchmarking tcmalloc and jemalloc as potential memory allocators drop-in candidates. To be fully transparent, for the scope of the results below we used tcmalloc as part of the gperftools package available on Ubuntu distributions version 2.9 and jemalloc 5.1.0-1. #### Memory allocator benchmarks\nAgain, we first compare performance against frameworks executing in an eager fashion. This is potentially the use case where the allocator can play the biggest role: As the graph is unknown before its execution, each framework must manage the memory required for each operation when it meets the actual execution of the above node, no planning ahead possible. In this context, the allocator is a major component due to all the system calls to allocate and reclaim memory. Figure 13. PyTorch memory allocator and cores scaling latencies\n\nFigure 14. Google's TensorFlow memory allocator and cores scaling latencies\n\nFigure 15. Google's TensorFlow with oneDNN enabled memory allocator and cores scaling latencies\n\nFigure 16. Intel TensorFlow memory allocator and cores scaling latencies\n\nAs per the graph above, you can notice that the standard library allocator (glibc) is often behind performance-wise but provides reasonable performance."", ""Readers interested in deploying or fine-tuning Nyströmformer for downstream tasks can find the HuggingFace documentation [here](https://huggingface.co/docs/transformers/model\\_doc/nystromformer)."", ""The most recent type of memory-efficient attention is [Flash Attention](https://arxiv.org/abs/2205.14135) (you can check out the original code at [HazyResearch/flash-attention](https://github.com/HazyResearch/flash-attention)). If you have PyTorch >= 2.0 installed, you should not expect a speed-up for inference when enabling `xformers`. To use Flash Attention, install the following:\n- PyTorch > 1.12\n- CUDA available\n- [xFormers](xformers)\nThen call [`~ModelMixin.enable\\_xformers\\_memory\\_efficient\\_attention`] on the pipeline:\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\npipe = DiffusionPipeline.from\\_pretrained(\n\""runwayml/stable-diffusion-v1-5\"",\ntorch\\_dtype=torch.float16,\nuse\\_safetensors=True,\n).to(\""cuda\"")\npipe.enable\\_xformers\\_memory\\_efficient\\_attention()\nwith torch.inference\\_mode():\nsample = pipe(\""a small cat\"")\n# optional: You can disable it via\n# pipe.disable\\_xformers\\_memory\\_efficient\\_attention()\n```\nThe iteration speed when using `xformers` should match the iteration speed of PyTorch 2.0 as described [here](torch2.0).""]","The Nyströmformer's approximation of self-attention has a time and memory complexity of \\(O(n)\\), where \\(n\\) is the length of the input sequence.",O(n)
"What is the goal of the Named Entity Recognition task in token classification?
","[""These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. Two common types of token classification are:\n\\* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names. \\* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb). ```py\n>>> from transformers import pipeline\n>>> classifier = pipeline(task=\""ner\"")\n>>> preds = classifier(\""Hugging Face is a French company based in New York City.\"")\n>>> preds = [\n... {\n... \""entity\"": pred[\""entity\""],\n... \""score\"": round(pred[\""score\""], 4),\n... \""index\"": pred[\""index\""],\n... \""word\"": pred[\""word\""],\n... \""start\"": pred[\""start\""],\n... \""end\"": pred[\""end\""],\n..."", ""p align=\""center\""> ![FSNER LOGO](http://sayef.tech:8082/uploads/FSNER-LOGO-2.png) \n\nImplemented by  [sayef](https://huggingface.co/sayef) . ## Overview\nThe FSNER model was proposed in [Example-Based Named Entity Recognition](https://arxiv.org/abs/2008.10570) by Morteza Ziyadi, Yuting Sun, Abhishek Goswami, Jade Huang, Weizhu Chen. To identify entity spans in a new domain, it uses a train-free few-shot learning approach inspired by question-answering. ## Abstract\n----\n> We present a novel approach to named entity recognition (NER) in the presence of scarce data that we call example-based NER. Our train-free few-shot learning approach takes inspiration from question-answering to identify entity spans in a new and unseen domain. In comparison with the current state-of-the-art, the proposed method performs significantly better, especially when using a low number of support examples. ## Model Training Details\n-----\n| identifier | epochs | datasets |\n| ---------- |:----------:| :-----:|\n| [sayef/fsner-bert-base-uncased](https://huggingface.co/sayef/fsner-bert-base-uncased) | 10 | ontonotes5, conll2003, wnut2017, and fin (Alvarado et al.). |\n## Installation and Example Usage\n------\nYou can use the FSNER model in 3 ways:\n1. Install directly from PyPI: `pip install fsner` and import the model as shown in the code example below\nor\n2. Install from source: `python setup.py install` and import the model as shown in the code example below\nor\n3. Clone repo and change directory to `src` and import the model as shown in the code example below\n```python\nfrom fsner import FSNERModel, FSNERTokenizerUtils\nmodel = FSNERModel(\""sayef/fsner-bert-base-uncased\"")\ntokenizer = FSNERTokenizerUtils(\""sayef/fsner-bert-base-uncased\"")\n# size of query and supports must be the same."", ""The goal of this task is to classify each token in the input text into one of the following categories: person, organization, location, or none of them. - News Category Classification (NCC) on the Soham articles dataset from [IndicGLUE](https://aclanthology.org/2020.findings-emnlp.445/). The goal of this task is to predict the category to which belong the input text. We evaluated it during training on the NER task to check that everything was going well; as you can see on the following plot, this was indeed the case! Evaluation metrics of fine-tuned models on the NER task from different checkpoints of pre-trained models. At the end of training, we compared sahajBERT with three other pretrained language models: [XLM-R Large](https://arxiv.org/abs/1911.02116), [IndicBert](https://aclanthology.org/2020.findings-emnlp.445/), and [bnRoBERTa](https://huggingface.co/neuralspace-reverie/indic-transformers-bn-roberta). In the table below, you can see that our model has results comparable to the best Bengali language models available on HF Hub, even though our model has only ~18M trained parameters, while, for instance, XLM-R (a strong multilingual baseline), has ~559M parameters and was trained on several hundred V100 GPUs."", ""et's take a look at word-based tokenization. Word-based tokenization is the idea of splitting the raw text into words, by splitting on spaces or other specific rules like punctuation. In this algorithm, each word has a specific number, an \""ID\"", attributed to it. In this example, \""Let's\"" has the ID 250, do has ID 861, and tokenization followed by an exclamation point has the ID 345. This approach is interesting, as the model has representations that are based on entire words. The information held in a single number is high as a word contains a lot of contextual and semantic information in a sentence. However, this approach does have its limits. For example, the word dog and the word dogs are very similar, and their meaning is close. However, the word-based tokenization will attribute entirely different IDs to these two words, and the model will therefore learn different meanings for these two words. This is unfortunate, as we would like the model to understand that these words are indeed related and that dogs is the plural form of the word dog."", ""Named-Entity Recognition\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert\\_ner\\_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial\\_Analyst\\_AI\nTags: NER, TEXT, HIGHLIGHT\n## Introduction\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \""token\"") into different categories, such as names of people or names of locations, or different parts of speech. For example, given the sentence:\n> Does Chicago have any Pakistani restaurants? A named-entity recognition algorithm may identify:\n- \""Chicago\"" as a \\*\\*location\\*\\*\n- \""Pakistani\"" as an \\*\\*ethnicity\\*\\*\nand so on. Using `gradio` (specifically the `HighlightedText` component), you can easily build a web demo of your NER model and share that with the rest of your team. Here is an example of a demo that you'll be able to build:\n$demo\\_ner\\_pipeline\nThis tutorial will show how to take a pretrained NER model and deploy it with a Gradio interface. We will show two different ways to use the `HighlightedText` component -- depending on your NER model, either of these two ways may be easier to learn!""]","The goal of the Named Entity Recognition (NER) task in token classification is to classify each token in the input text into predefined categories, such as person, organization, location, or none of them. This involves identifying and labeling tokens according to their entity category, which is particularly useful in various applications, including information extraction and understanding the context of the text.","The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization."
"What is the resolution of images used by the CLIPSeg model?
","[""For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm\\_source=hf&utm\\_medium=blog&utm\\_campaign=clipseg). Before we describe how to do that, let’s first take a look at how CLIPSeg works. ## CLIP: the magic model behind CLIPSeg\n[CLIP](https://huggingface.co/docs/transformers/main/en/model\\_doc/clip), which stands for \\*\\*C\\*\\*ontrastive \\*\\*L\\*\\*anguage–\\*\\*I\\*\\*mage \\*\\*P\\*\\*re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract \\*representation\\* of your input. This abstract representation, also called an \\*embedding\\*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together)."", ""It consists of 2100 images belonging to 21 classes (100 images per class), and each image has 5 captions. The Sydney dataset contains images of Sydney, Australia from Google Earth. It contains 613 images belonging to 7 classes. Images are (500, 500) RGB and provides 5 captions for each image. We used these additional datasets because we were not sure if the RSICD dataset would be large enough to fine-tune CLIP. #### Model\nOur model is just the fine-tuned version of the original CLIP model shown below. Inputs to the model are a batch of captions and a batch of images passed through the CLIP text encoder and image encoder respectively. The training process uses [contrastive learning](https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607) to learn a joint embedding representation of image and captions. In this embedding space, images and their respective captions are pushed close together, as are similar images and similar captions. Conversely, images and captions for different images, or dissimilar images and captions, are likely to be pushed further apart. ![](/blog/assets/30_clip_rsicd/clip_schematic.png)\n*CLIP Training and Inference (Image Credit: CLIP: Connecting Text and Images (https://openai.comclip/))*\n#### Data Augmentation\nIn order to regularize our dataset and prevent overfitting due to the size of the dataset, we used both image and text augmentation."", ""Such module is lightweight and can be plugged into pretrained language-image models seamlessly. Moreover, we propose a video-specific prompting scheme, which leverages video content information for generating discriminative textual prompts. Extensive experiments demonstrate that our approach is effective and can be generalized to different video recognition scenarios. In particular, under fully-supervised settings, our approach achieves a top-1 accuracy of 87.1% on Kinectics-400, while using 12 times fewer FLOPs compared with Swin-L and ViViT-H. In zero-shot experiments, our approach surpasses the current state-of-the-art methods by +7.6% and +14.9% in terms of top-1 accuracy under two popular protocols. In few-shot scenarios, our approach outperforms previous best methods by +32.1% and +23.1% when the labeled data is extremely limited.\\*\nTips:\n- Usage of X-CLIP is identical to [CLIP](clip). ![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/xclip_architecture.png)\n X-CLIP architecture. Taken from the [original paper.](https://arxiv.org/abs/2208.02816) \nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/VideoX/tree/master/X-CLIP). ## Resources\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with X-CLIP. - Demo notebooks for X-CLIP can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/X-CLIP)."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Stable unCLIP\nStable unCLIP checkpoints are finetuned from [Stable Diffusion 2.1](./stable\\_diffusion/stable\\_diffusion\\_2) checkpoints to condition on CLIP image embeddings. Stable unCLIP still conditions on text embeddings. Given the two separate conditionings, stable unCLIP can be used\nfor text guided image variation. When combined with an unCLIP prior, it can also be used for full text to image generation. The abstract from the paper is:\n\\*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\\*\n## Tips\nStable unCLIP takes `noise\\_level` as input during inference which determines how much noise is added to the image embeddings."", ""cropping the image and darkening the background. ```python\nurl = \""https://i.imgur.com/mRSORqz.jpg\""\nalternative\\_prompt = Image.open(requests.get(url, stream=True).raw)\nalternative\\_prompt\n```\n\n```python\nencoded\\_alternative\\_prompt = processor(images=[alternative\\_prompt], return\\_tensors=\""pt\"")\n# predict\nwith torch.no\\_grad():\noutputs = model(\\*\\*encoded\\_image, conditional\\_pixel\\_values=encoded\\_alternative\\_prompt.pixel\\_values)\npreds = outputs.logits.unsqueeze(1)\npreds = torch.transpose(preds, 0, 1)\n```\n```python\n\\_, ax = plt.subplots(1, 2, figsize=(6, 4))\n[a.axis('off') for a in ax.flatten()]\nax[0].imshow(image)\nax[1].imshow(torch.sigmoid(preds[0]))\n```\n\nIn this case, the result is pretty much the same. This is probably\nbecause the coffee cup was already separated well from the background in\nthe original image. ## Using CLIPSeg to pre-label images on Segments.ai\nAs you can see, the results from CLIPSeg are a little fuzzy and very\nlow-res. If we want to obtain better results, you can fine-tune a\nstate-of-the-art segmentation model, as explained in [our previous\nblogpost](https://huggingface.co/blog/fine-tune-segformer). To finetune\nthe model, we\\'ll need labeled data. In this section, we\\'ll show you\nhow you can use CLIPSeg to create some rough segmentation masks and then\nrefine them on\n[Segments.ai](https://segments.ai/?utm\\_source=hf&utm\\_medium=blog&utm\\_campaign=clipseg),\na labeling platform with smart labeling tools for image segmentation.""]",The resolution of images used by the CLIPSeg model is 352 x 352 pixels.,352 x 352 pixels
"What can you use Gradio for?
","[""### What's Next? Keep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [key features about Gradio demos](https://www.gradio.app/guides/key-features). Or, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/). ## Questions? If you'd like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help. If you like Gradio, please leave us a ⭐ on GitHub! ## Open Source Stack\nGradio is built on top of many wonderful open-source libraries! [![huggingface](readme_files/huggingface_mini.svg)](https://huggingface.co)\n[![python](readme_files/python.svg)](https://www.python.org)\n[![fastapi](readme_files/fastapi.svg)](https://fastapi.tiangolo.com)\n[![encode](readme_files/encode.svg)](https://www.encode.io)\n[![svelte](readme_files/svelte.svg)](https://svelte.dev)\n[![vite](readme_files/vite.svg)](https://vitejs.dev)\n[![pnpm](readme_files/pnpm.svg)](https://pnpm.io)\n[![tailwind](readme_files/tailwind.svg)](https://tailwindcss.com)\n[![storybook](readme_files/storybook.svg)](https://storybook.js.org/)\n[![chromatic](readme_files/chromatic.svg)](https://www.chromatic.com/)\n## License\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository. ## Citation\nAlso check out the paper \\_[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019\\_, and please cite it if you use Gradio in your work."", ""Gradio Demo: upload\\_button\n### A simple demo showcasing the upload button used with its `upload` event trigger."", ""he simplest possible Gradio demo."", ""It uses Gradio's Audio and Label components."", ""Gradio Demo: image\\_segmentation\n### Simple image segmentation using gradio's AnnotatedImage component.""]","Gradio can be used for creating user interfaces for machine learning models, allowing users to easily interact with them. It supports various components such as text inputs, image uploads, audio recordings, and more, enabling developers to showcase their models in a user-friendly way. Gradio is particularly useful for sharing and testing ML models, making it easier for others to understand and utilize the models without needing extensive technical knowledge.","Create a demo for your machine learning model, share your machine learning model with others, and debug your model."
"What TensorFlow API function is used to load a saved tensor file?
","[""Tensorflow API\n[[autodoc]] safetensors.tensorflow.load\\_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save\\_file\n[[autodoc]] safetensors.tensorflow.save"", ""Why? Because the tokenized array and labels would have to be fully loaded into memory, and because Numpy doesn’t handle “jagged” arrays, so every tokenized sample would have to be padded to the length of the longest sample in the whole dataset. That’s going to make your array even bigger, and all those padding tokens will slow down training too! As a TensorFlow engineer, this is normally where you’d turn to `tf.data` to make a pipeline that will stream the data from storage rather than loading it all into memory. That’s a hassle, though, so we’ve got you. First, let’s use the `map()` method to add the tokenizer columns to the dataset. Remember that our datasets are disc-backed by default - they won’t load into memory until you convert them into arrays! ```py\ndef tokenize\\_dataset(data):\n# Keys of the returned dictionary will be added to the dataset as columns\nreturn tokenizer(data[\""text\""])\ndataset = dataset.map(tokenize\\_dataset)\n```\nNow our dataset has the columns we want, but how do we train on it? Simple - wrap it with a `tf.data.Dataset` and all our problems are solved - data is loaded on-the-fly, and padding is applied only to batches rather than the whole dataset, which means that we need way fewer padding tokens:\n```py\ntf\\_dataset = model.prepare\\_tf\\_dataset(\ndataset,\nbatch\\_size=16,\nshuffle=True\n)\nmodel.fit(tf\\_dataset)\n```\nWhy is [prepare\\_tf\\_dataset()](https://huggingface.co/docs/transformers/main/en/main\\_classes/model#transformers.TFPreTrainedModel.prepare\\_tf\\_dataset) a method on your model?"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\n🤗 Optimum handles the export of TensorFlow models to TFLite in the `exporters.tflite` module. In addition, models hosted on the Hugging Face Hub with PyTorch weights but having a TensorFlow implementation will also be supported in the export thanks to Transformers' [TFPreTrainedModel.from\\_pretrained()](https://huggingface.co/docs/transformers/main/en/main\\_classes/model#transformers.TFPreTrainedModel.from\\_pretrained) auto-conversion to TensorFlow. The TFLite export support provides classes, functions and a command line interface to export a model easily. Supported architectures:\n- Albert\n- BERT\n- Camembert\n- ConvBert\n- Deberta\n- Deberta V2\n- DistilBert\n- Electra\n- Flaubert\n- MobileBert\n- MPNet\n- ResNet\n- Roberta\n- RoFormer\n- XLM\n- XLMRoberta"", ""On CPU, if the file is already in cache, then it can\ntruly be zero-copy, whereas on GPU there is not such disk cache, so a copy is always required\nbut you can bypass allocating all the tensors on CPU at any given point. SafeTensors is not zero-copy for the header. The choice of JSON is pretty arbitrary, but since deserialization is <<< of the time required to load the actual tensor data and is readable I went that way, (also space is <<< to the tensor data). - Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the\nmoment. - Order: 'C' or row-major. This seems to have won. We can add that information later if needed. - Stride: No striding, all tensors need to be packed before being serialized. I have yet to see a case where it seems useful to have a strided tensor stored in serialized format. ### Benefits\nSince we can invent a new format we can propose additional benefits:\n- Prevent DOS attacks: We can craft the format in such a way that it's almost\nimpossible to use malicious files to DOS attack a user. Currently, there's a limit\non the size of the header of 100MB to prevent parsing extremely large JSON. Also when reading the file, there's a guarantee that addresses in the file\ndo not overlap in any way, meaning when you're loading a file you should never\nexceed the size of the file in memory\n- Faster load: PyTorch seems to be the fastest file to load out in the major\nML formats. However, it does seem to have an extra copy on CPU, which we\ncan bypass in this lib by using `torch.UntypedStorage.from\\_file`. Currently, CPU loading times are extremely fast with this lib compared to pickle. GPU loading times are as fast or faster than PyTorch equivalent. Loading first on CPU with memmapping with torch, and then moving all tensors to GPU seems\nto be faster too somehow (similar behavior in torch pickle)\n- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to\nload only part of the tensors on the various models. For\n[BLOOM](https://huggingface.co/bigscience/bloom) using this format enabled\nto load the model on 8 GPUs from 10mn with regular PyTorch weights down to 45s."", ""### What is a SavedModel? A SavedModel contains a standalone TensorFlow model, including its weights and its architecture. It does not require the original source of the model to be run, which makes it useful for sharing or deploying with any backend that supports reading a SavedModel such as Java, Go, C++ or JavaScript among others. The internal structure of a SavedModel is represented as such:\n```\nsavedmodel\n/assets\n-> here the needed assets by the model (if any)\n/variables\n-> here the model checkpoints that contains the weights\nsaved\\_model.pb -> protobuf file representing the model graph\n```\n### How to install TensorFlow Serving? There are three ways to install and use TensorFlow Serving:\n- through a Docker container,\n- through an apt package,\n- or using [pip](https://pypi.org/project/pip/). To make things easier and compliant with all the existing OS, we will use Docker in this tutorial. ### How to create a SavedModel? SavedModel is the format expected by TensorFlow Serving. Since Transformers v4.2.0, creating a SavedModel has three additional features:\n1.""]",The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`.,safetensors.tensorflow.load_file
"Where can you access the logs of your Endpoints in Hugging Face Endpoints?
","[""Access and view Metrics\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the “Analytics” tab of your Endpoints. ![metric dashboard](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png)\n## Access Metrics via API\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api\\_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints."", ""| Partially handled by `huggingface\\_hub`. |"", ""This means that you must be logged in to a Hugging Face user account. Requesting access can only be done from your browser. Go to the dataset on the Hub and you will be prompted to share your information:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-gated-user-side.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-gated-user-side-dark.png)\n\nBy clicking on \\*\\*Agree\\*\\*, you agree to share your username and email address with the dataset authors. In some cases, additional fields might be requested. To help the dataset authors decide whether to grant you access, try to fill out the form as completely as possible. Once the access request is sent, there are two possibilities. If the approval mechanism is automatic, you immediately get access to the dataset files. Otherwise, the requests have to be approved manually by the authors, which can take more time. The dataset authors have complete control over dataset access. In particular, they can decide at any time to block your access to the dataset without prior notice, regardless of approval mechanism or if your request has already been approved. ### Download files\nTo download files from a gated dataset you'll need to be authenticated."", ""For more details, reach out to HF Support to address any questions or concerns at website@huggingface.co"", ""Security Policy\n## Reporting a Vulnerability\n🤗 We have our bug bounty program set up with HackerOne. Please feel free to submit vulnerability reports to our private program at https://hackerone.com/hugging\\_face. Note that you'll need to be invited to our program, so send us a quick email at security@huggingface.co if you've found a vulnerability.""]",You can access the logs of your Endpoints in Hugging Face Endpoints through the UI on the detailed overview in the “Logs” tab of your Endpoints.,"In the ""Logs"" tab of your Endpoint through the UI."
"What is the latest task added to Hugging Face AutoTrain for Computer Vision?
","[""--\ntitle: Image Classification with AutoTrain\nthumbnail: /blog/assets/105\\_autotrain-image-classification/thumbnail.png\nauthors:\n- user: nimaboscarino\n---\n# Image Classification with AutoTrain\nSo you’ve heard all about the cool things that are happening in the machine learning world, and you want to join in. There’s just one problem – you don’t know how to code! 😱 Or maybe you’re a seasoned software engineer who wants to add some ML to your side-project, but you don’t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. That’s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature we’ve just added, we’re making “no-code” machine learning better than ever. Best of all, you can create your first project for ✨ free! ✨\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with \\*\\*zero\\*\\* configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer 🤯 We’ve been expanding the number of tasks that we support, and we’re proud to announce that \\*\\*you can now use AutoTrain for Computer Vision\\*\\*!"", ""Tasks\n## What's a task? Tasks, or pipeline types, describe the \""shape\"" of each model's API (inputs and outputs) and are used to determine which Inference API and widget we want to display for any given model. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/tasks.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/tasks-dark.png)\n\nThis classification is relatively coarse-grained (you can always add more fine-grained task names in your model tags), so \\*\\*you should rarely have to create a new task\\*\\*. If you want to add support for a new task, this document explains the required steps. ## Overview\nHaving a new task integrated into the Hub means that:\n\\* Users can search for all models – and datasets – of a given task. \\* The Inference API supports the task. \\* Users can try out models directly with the widget. 🏆\nNote that you don't need to implement all the steps by yourself. Adding a new task is a community effort, and multiple people can contribute. 🧑‍🤝‍🧑\nTo begin the process, open a new issue in the [huggingface\\_hub](https://github.com/huggingface/huggingface\\_hub/issues) repository."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# MaskFormer\nThis is a recently introduced model so the API hasn't been tested extensively. There may be some bugs or slight\nbreaking changes to fix it in the future. If you see something strange, file a [Github Issue](https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title). ## Overview\nThe MaskFormer model was proposed in [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) by Bowen Cheng, Alexander G."", ""They will be needed during step 7. 4. Select `AI & Machine Learning` -> `AI Training` from the menu on the left. Click `+ Launch a new job` on the AI Training page. 5. On the `Launch a new job` page:\n\\* In `1. Choose a region` select a region closest to you. \\* In `2. Enter the Docker image` select `Custom image` -> `baaastijn/ovh\\_huggingface`. \\* You can skip steps `3.` and `4.` if you will be using the Hugging Face Hub to store the models after training. \\* In `5. Configure your job` select \\*\\*1\\*\\* `GPU`. \\* Validate the info and Create the job. 6. On the `AI Training Jobs` screen wait until the job's status changes from `Pending` to `Running`. 7. Click `HTTP Access` from the Job's details page and log in with the AI training user you've created earlier. Once logged in, you can close the page and click `HTTP Access` to launch a JupyterLab notebook. 8. Awesome, now you have a free GPU-enabled Jupyter instance! \\*\\*Note\\*\\*: If you're an experienced Docker user, feel free to create a custom docker image with all of the needed packages\nlike the one in step 5. The Dockerfile for it is available here:\n[baaastijn/Dockerimages](https://github.com/baaastijn/Dockerimages/tree/main/Hugginface\\_challenge\\_speech). Once you've built your image, push it to https://hub.docker.com/ and select it during the OVHcloud job creation. For more quick tutorials about OVHcloud AI products, check out the showcase https://vimeo.com/showcase/8903300\n## How to combine n-gram with acoustic model\nHaving trained a speech recognition model with CTC as shown in the section above,\none can further improve the model's performance by adding an \\*\\*n-gram language model\\*\\*\nto the decoding process of the model."", ""Conclusion! This experience of training a ControlNet was a lot of fun. We succesfully trained a model that can follow real face poses - however it learned to make uncanny 3D faces instead of real 3D faces because this was the dataset it was trained on, which has its own charm and flare. Try out our [Hugging Face Space](https://huggingface.co/spaces/pcuenq/uncanny-faces):\nAs for next steps for us - in order to create realistically looking faces, while still not using a real face dataset, one idea is running the entire `FaceSynthetics` dataset through Stable Diffusion Image2Imaage, converting the 3D-looking faces into realistically looking ones, and then trainign another ControlNet. And stay tuned, as we will have a ControlNet Training event soon! Follow Hugging Face on [Twitter](https://twitter.com/huggingface) or join our [Discord]( http://hf.co/join/discord) to stay up to date on that.""]","The latest task added to Hugging Face AutoTrain for Computer Vision is image classification. With AutoTrain, users can train models for this task with zero configuration needed, simply by choosing the task, uploading their data, and letting Hugging Face handle the rest.",Image Classification
"What is the default repository type created by the `create_repo` function on Hugging Face Hub?
","[""Firstly, there are a few methods to manage repository creation, deletion, and others:\n```python no-format\nfrom huggingface\\_hub import (\n# User management\nlogin,\nlogout,\nwhoami,\n# Repository creation and management\ncreate\\_repo,\ndelete\\_repo,\nupdate\\_repo\\_visibility,\n# And some methods to retrieve/change information about the content\nlist\\_models,\nlist\\_datasets,\nlist\\_metrics,\nlist\\_repo\\_files,\nupload\\_file,\ndelete\\_file,\n)\n```\nAdditionally, it offers the very powerful `Repository` class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them. The `create\\_repo` method can be used to create a new repository on the hub:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"")\n```\nThis will create the repository `dummy-model` in your namespace. If you like, you can specify which organization the repository should belong to using the `organization` argument:\n```py\nfrom huggingface\\_hub import create\\_repo\ncreate\\_repo(\""dummy-model\"", organization=\""huggingface\"")\n```\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belong to that organization."", ""## What should I use ? And when ? Overall, the \\*\\*HTTP-based approach is the recommended way to use\\*\\* `huggingface\\_hub`\nin all cases. [`HfApi`] allows to pull and push changes, work with PRs, tags and branches, interact with discussions and much more. Since the `0.16` release, the http-based methods can also run in the background, which was the last major advantage of the [`Repository`] class. However, not all git commands are available through [`HfApi`]. Some may never be implemented, but we are always trying to improve and close the gap. If you don't see your use case covered, please open [an issue on Github](https://github.com/huggingface/huggingface\\_hub)! We welcome feedback to help build the 🤗 ecosystem with and for our users. This preference of the http-based [`HfApi`] over the git-based [`Repository`] does not mean that git versioning will disappear from the Hugging Face Hub anytime soon. It will always be possible to use `git` commands locally in workflows where it makes sense."", ""!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Managing local and online repositories\nThe `Repository` class is a helper class that wraps `git` and `git-lfs` commands. It provides tooling adapted\nfor managing repositories which can be very large. It is the recommended tool as soon as any `git` operation is involved, or when collaboration will be a point\nof focus with the repository itself. ## The Repository class\n[[autodoc]] Repository\n- \\_\\_init\\_\\_\n- current\\_branch\n- all\n## Helper methods\n[[autodoc]] huggingface\\_hub.repository.is\\_git\\_repo\n[[autodoc]] huggingface\\_hub.repository.is\\_local\\_clone\n[[autodoc]] huggingface\\_hub.repository.is\\_tracked\\_with\\_lfs\n[[autodoc]] huggingface\\_hub.repository.is\\_git\\_ignored\n[[autodoc]] huggingface\\_hub.repository.files\\_to\\_be\\_staged\n[[autodoc]] huggingface\\_hub.repository.is\\_tracked\\_upstream\n[[autodoc]] huggingface\\_hub.repository.commits\\_to\\_push\n## Following asynchronous commands\nThe `Repository` utility offers several methods which can be launched asynchronously:\n- `git\\_push`\n- `git\\_pull`\n- `push\\_to\\_hub`\n- The `commit` context manager\nSee below for utilities to manage such asynchronous methods. [[autodoc]] Repository\n- commands\\_failed\n- commands\\_in\\_progress\n- wait\\_for\\_commands\n[[autodoc]] huggingface\\_hub.repository.CommandInProgress"", ""!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# 创建和管理存储库\nHugging Face Hub是一组 Git 存储库。[Git](https://git-scm.com/)是软件开发中广泛使用的工具，可以在协作工作时轻松对项目进行版本控制。本指南将向您展示如何与 Hub 上的存储库进行交互，特别关注以下内容：\n- 创建和删除存储库\n- 管理分支和标签\n- 重命名您的存储库\n- 更新您的存储库可见性\n- 管理存储库的本地副本\n如果您习惯于使用类似于GitLab/GitHub/Bitbucket等平台，您可能首先想到使用 `git`命令行工具来克隆存储库（`git clone`）、提交更改（`git add` , ` git commit`）并推送它们（`git push`）。在使用 Hugging Face Hub 时，这是有效的。然而，软件工程和机器学习并不具有相同的要求和工作流程。模型存储库可能会维护大量模型权重文件以适应不同的框架和工具，因此克隆存储库会导致您维护大量占用空间的本地文件夹。因此，使用我们的自定义HTTP方法可能更有效。您可以阅读我们的[git与HTTP相比较](../concepts/git\\_vs\\_http)解释页面以获取更多详细信息\n如果你想在Hub上创建和管理一个仓库，你的计算机必须处于登录状态。如果尚未登录，请参考[此部分](../quick-start#login)。在本指南的其余部分，我们将假设你的计算机已登录\n## 仓库创建和删除\n第一步是了解如何创建和删除仓库。你只能管理你拥有的仓库（在你的用户名命名空间下）或者你具有写入权限的组织中的仓库\n### 创建一个仓库\n使用 [`create\\_repo`] 创建一个空仓库，并通过 `repo\\_id`参数为其命名 `repo\\_id`是你的命名空间，后面跟着仓库名称：`username\\_or\\_org/repo\\_name`\n运行以下代码，以创建仓库：\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-model\"")\n'https://huggingface.co/lysandre/test-model'\n```\n默认情况下，[`create\\_repo`] 会创建一个模型仓库。但是你可以使用 `repo\\_type`参数来指定其他仓库类型。例如，如果你想创建一个数据集仓库\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-dataset\"", repo\\_type=\""dataset\"")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```\n创建仓库时，你可以使用 `private`参数设置仓库的可见性\n请运行以下代码\n```py\n>>> from huggingface\\_hub import create\\_repo\n>>> create\\_repo(\""lysandre/test-private\"", private=True)\n```\n如果你想在以后更改仓库的可见性，你可以使用[`update\\_repo\\_visibility`] 函数\n### 删除一个仓库\n使用 [`delete\\_repo`] 删除一个仓库。确保你确实想要删除仓库，因为这是一个不可逆转的过程！做完上述过程后，指定你想要删除的仓库的 `repo\\_id`\n请运行以下代码：\n```py\n>>> delete\\_repo(repo\\_id=\""lysandre/my-corrupted-dataset\"", repo\\_type=\""dataset\"")\n```\n### 克隆一个仓库（仅适用于 Spaces）\n在某些情况下，你可能想要复制别人的仓库并根据自己的用例进行调整。对于 Spaces，你可以使用 [`duplicate\\_space`] 方法来实现。它将复制整个仓库。\n你仍然需要配置自己的设置（硬件和密钥）。查看我们的[管理你的Space指南](./manage-spaces)以获取更多详细信息。\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import duplicate\\_space\n>>> duplicate\\_space(\""multimodalart/dreambooth-training\"", private=False)\nRepoUrl('https://huggingface.co/spaces/nateraw/dreambooth-training',...)\n```\n## 上传和下载文件\n既然您已经创建了您的存储库，您现在也可以推送更改至其中并从中下载文件\n这两个主题有它们自己的指南。请[上传指南](./upload) 和[下载指南](./download)来学习如何使用您的存储库。\n## 分支和标签\nGit存储库通常使用分支来存储同一存储库的不同版本。标签也可以用于标记存储库的特定状态，例如，在发布版本这个情况下。更一般地说，分支和标签被称为[git引用](https://git-scm.com/book/en/v2/Git-Internals-Git-References). ### 创建分支和标签\n你可以使用[`create\\_branch`]和[`create\\_tag`]来创建新的分支和标签:\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import create\\_branch, create\\_tag\n# Create a branch on a Space repo from `main` branch\n>>> create\\_branch(\""Matthijs/speecht5-tts-demo\"", repo\\_type=\""space\"", branch=\""handle-dog-speaker\"")\n# Create a tag on a Dataset repo from `v0.1-release` branch\n>>> create\\_branch(\""bigcode/the-stack\"", repo\\_type=\""dataset\"", revision=\""v0.1-release\"", tag=\""v0.1.1\"", tag\\_message=\""Bump release version.\"")\n```\n同时,你可以以相同的方式使用 [`delete\\_branch`] 和 [`delete\\_tag`] 函数来删除分支或标签\n### 列出所有的分支和标签\n你还可以使用 [`list\\_repo\\_refs`] 列出存储库中的现有 Git 引用\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import list\\_repo\\_refs\n>>> api.list\\_repo\\_refs(\""bigcode/the-stack\"", repo\\_type=\""dataset\"")\nGitRefs(\nbranches=[\nGitRefInfo(name='main', ref='refs/heads/main', target\\_commit='18edc1591d9ce72aa82f56c4431b3c969b210ae3'),\nGitRefInfo(name='v1.1.a1', ref='refs/heads/v1.1.a1', target\\_commit='f9826b862d1567f3822d3d25649b0d6d22ace714')\n],\nconverts=[],\ntags=[\nGitRefInfo(name='v1.0', ref='refs/tags/v1.0', target\\_commit='c37a8cd1e382064d8aced5e05543c5f7753834da')\n]\n)\n```\n## 修改存储库设置\n存储库具有一些可配置的设置。大多数情况下，您通常会在浏览器中的存储库设置页面上手动配置这些设置。要配置存储库，您必须具有对其的写访问权限（拥有它或属于组织）。在本节中，我们将看到您还可以使用 `huggingface\\_hub` 在编程方式上配置的设置。\n一些设置是特定于 Spaces（硬件、环境变量等）的。要配置这些设置，请参考我们的[管理Spaces](../guides/manage-spaces)指南。\n### 更新可见性\n一个存储库可以是公共的或私有的。私有存储库仅对您或存储库所在组织的成员可见。\n请运行以下代码将存储库更改为私有：\n```py\n>>> from huggingface\\_hub import update\\_repo\\_visibility\n>>> update\\_repo\\_visibility(repo\\_id=repo\\_id, private=True)\n```\n### 重命名您的存储库\n您可以使用 [`move\\_repo`] 在 Hub 上重命名您的存储库。使用这种方法，您还可以将存储库从一个用户移动到一个组织。在这样做时，有一些[限制](https://hf.co/docs/hub/repositories-settings#renaming-or-transferring-a-repo)需要注意。例如，您不能将存储库转移到另一个用户。\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import move\\_repo\n>>> move\\_repo(from\\_id=\""Wauplin/cool-model\"", to\\_id=\""huggingface/cool-model\"")\n```\n## 管理存储库的本地副本\n上述所有操作都可以通过HTTP请求完成。然而，在某些情况下，您可能希望在本地拥有存储库的副本，并使用您熟悉的Git命令与之交互。\n[`Repository`] 类允许您使用类似于Git命令的函数与Hub上的文件和存储库进行交互。它是对Git和Git-LFS方法的包装，以使用您已经了解和喜爱的Git命令。在开始之前，请确保已安装Git-LFS（请参阅[此处](https://git-lfs.github.com/)获取安装说明）。\n### 使用本地存储库\n使用本地存储库路径实例化一个 [`Repository`] 对象：\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(local\\_dir=\""//\"")\n```\n### 克隆\n`clone\\_from`参数将一个存储库从Hugging Face存储库ID克隆到由 `local\\_dir`参数指定的本地目录：\n请运行以下代码：\n```py\n>>> from huggingface\\_hub import Repository\n>>> repo = Repository(local\\_dir=\""w2v2\"", clone\\_from=\""facebook/wav2vec2-large-960h-lv60\"")\n```\n`clone\\_from`还可以使用URL克隆存储库：\n请运行以下代码：\n```py\n>>> repo = Repository(local\\_dir=\""huggingface-hub\"", clone\\_from=\""https://huggingface.co/facebook/wav2vec2-large-960h-lv60\"")\n```\n你可以将`clone\\_from`参数与[`create\\_repo`]结合使用，以创建并克隆一个存储库：\n请运行以下代码：\n```py\n>>> repo\\_url = create\\_repo(repo\\_id=\""repo\\_name\"")\n>>> repo = Repository(local\\_dir=\""repo\\_local\\_path\"", clone\\_from=repo\\_url)\n```\n当你克隆一个存储库时，通过在克隆时指定`git\\_user`和`git\\_email`参数，你还可以为克隆的存储库配置Git用户名和电子邮件。当用户提交到该存储库时，Git将知道提交的作者是谁。\n请运行以下代码：\n```py\n>>> repo = Repository(\n..."", ""It's a model repo by default. Parameters:\n- `type`: Type of repo (dataset or space; model by default). - `name`: Name of repo. - `organization`: Name of organization (optional). Payload:\n```js\npayload = {\n\""type\"": \""model\"",\n\""name\"": \""name\"",\n\""organization\"": \""organization\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.delete\\_repo()`. ### PUT /api/repos/{repo\\_type}/{repo\\_id}/settings\nUpdate repo visibility. Payload:\n```js\npayload = {\n\""private\"": \""private\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.update\\_repo\\_visibility()`. ### POST /api/repos/move\nMove a repository (rename within the same namespace or transfer from user to organization). Parameters:\n- `fromRepo`: repo to rename. - `toRepo`: new name of the repo. - `type`: Type of repo (dataset or space; model by default). Payload:\n```js\npayload = {\n\""fromRepo\"" : \""namespace/repo\\_name\"",\n\""toRepo\"" : \""namespace2/repo\\_name2\"",\n\""type\"": \""model\"",\n}\n```\nThis is equivalent to `huggingface\\_hub.move\\_repo()`. ## User API\nThe following endpoint gets information about a user. ### GET /api/whoami-v2\nGet username and organizations the user belongs to. Payload:\n```js\nheaders = { \""authorization\"" : \""Bearer $token\"" }\n```\nThis is equivalent to `huggingface\\_hub.whoami()`.""]",The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository.,model
"How many splits does the ""duorc"" dataset have?
","[""List splits and configurations\nDatasets typically have splits and may also have configurations. A \\_split\\_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A \\_configuration\\_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you're interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load\\_hub)! This guide shows you how to use Datasets Server's `/splits` endpoint to retrieve a dataset's splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\nThe `/splits` endpoint accepts the dataset name as its query parameter:\n\n```python\nimport requests\nheaders = {\""Authorization\"": f\""Bearer {API\\_TOKEN}\""}\nAPI\\_URL = \""https://datasets-server.huggingface.co/splits?dataset=duorc\""\ndef query():\nresponse = requests.get(API\\_URL, headers=headers)\nreturn response.json()\ndata = query()\n```\n\n```js\nimport fetch from \""node-fetch\"";\nasync function query(data) {\nconst response = await fetch(\n\""https://datasets-server.huggingface.co/splits?dataset=duorc\"",\n{\nheaders: { Authorization: `Bearer ${API\\_TOKEN}` },\nmethod: \""GET\""\n}\n);\nconst result = await response.json();\nreturn result;\n}\nquery().then((response) => {\nconsole.log(JSON.stringify(response));\n});\n```\n\n```curl\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\n-X GET \\\n-H \""Authorization: Bearer ${API\\_TOKEN}\""\n```\nThe endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n```json\n{\n\""splits\"": [\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""ParaphraseRC\"", \""split\"": \""test\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""train\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""validation\"" },\n{ \""dataset\"": \""duorc\"", \""config\"": \""SelfRC\"", \""split\"": \""test\"" }\n],\n\""pending\"": [],\n\""failed\"": []\n}\n```"", ""For finegrained detail like faces, we find that lower learning rates along with larger batch sizes work best. For stage II, we find that lower learning rates are also needed. We found experimentally that the DDPM scheduler with the default larger number of denoising steps to sometimes work better than the DPM Solver scheduler\nused in the training scripts. ### Stage II additional validation images\nThe stage II validation requires images to upscale, we can download a downsized version of the training set:\n```py\nfrom huggingface\\_hub import snapshot\\_download\nlocal\\_dir = \""./dog\\_downsized\""\nsnapshot\\_download(\n\""diffusers/dog-example-downsized\"",\nlocal\\_dir=local\\_dir,\nrepo\\_type=\""dataset\"",\nignore\\_patterns=\"".gitattributes\"",\n)\n```\n### IF stage I LoRA Dreambooth\nThis training configuration requires ~28 GB VRAM. ```sh\nexport MODEL\\_NAME=\""DeepFloyd/IF-I-XL-v1.0\""\nexport INSTANCE\\_DIR=\""dog\""\nexport OUTPUT\\_DIR=\""dreambooth\\_dog\\_lora\""\naccelerate launch train\\_dreambooth\\_lora.py \\\n--report\\_to wandb \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--instance\\_data\\_dir=$INSTANCE\\_DIR \\\n--output\\_dir=$OUTPUT\\_DIR \\\n--instance\\_prompt=\""a sks dog\"" \\\n--resolution=64 \\\n--train\\_batch\\_size=4 \\\n--gradient\\_accumulation\\_steps=1 \\\n--learning\\_rate=5e-6 \\\n--scale\\_lr \\\n--max\\_train\\_steps=1200 \\\n--validation\\_prompt=\""a sks dog\"" \\\n--validation\\_epochs=25 \\\n--checkpointing\\_steps=100 \\\n--pre\\_compute\\_text\\_embeddings \\\n--tokenizer\\_max\\_length=77 \\\n--text\\_encoder\\_use\\_attention\\_mask\n```\n### IF stage II LoRA Dreambooth\n`--validation\\_images`: These images are upscaled during validation steps. `--class\\_labels\\_conditioning=timesteps`: Pass additional conditioning to the UNet needed for stage II. `--learning\\_rate=1e-6`: Lower learning rate than stage I. `--resolution=256`: The upscaler expects higher resolution inputs\n```sh\nexport MODEL\\_NAME=\""DeepFloyd/IF-II-L-v1.0\""\nexport INSTANCE\\_DIR=\""dog\""\nexport OUTPUT\\_DIR=\""dreambooth\\_dog\\_upscale\""\nexport VALIDATION\\_IMAGES=\""dog\\_downsized/image\\_1.png dog\\_downsized/image\\_2.png dog\\_downsized/image\\_3.png dog\\_downsized/image\\_4.png\""\npython train\\_dreambooth\\_lora.py \\\n--report\\_to wandb \\\n--pretrained\\_model\\_name\\_or\\_path=$MODEL\\_NAME \\\n--instance\\_data\\_dir=$INSTANCE\\_DIR \\\n--output\\_dir=$OUTPUT\\_DIR \\\n--instance\\_prompt=\""a sks dog\"" \\\n--resolution=256 \\\n--train\\_batch\\_size=4 \\\n--gradient\\_accumulation\\_steps=1 \\\n--learning\\_rate=1e-6 \\\n--max\\_train\\_steps=2000 \\\n--validation\\_prompt=\""a sks dog\"" \\\n--validation\\_epochs=100 \\\n--checkpointing\\_steps=500 \\\n--pre\\_compute\\_text\\_embeddings \\\n--tokenizer\\_max\\_length=77 \\\n--text\\_encoder\\_use\\_attention\\_mask \\\n--validation\\_images $VALIDATION\\_IMAGES \\\n--class\\_labels\\_conditioning=timesteps\n```\n### IF Stage I Full Dreambooth\n`--skip\\_save\\_text\\_encoder`: When training the full model, this will skip saving the entire T5 with the finetuned model."", ""- Dataset streaming\nTODO(Patrick)"", ""### Download and extract the data. It consists of two compressed folders: one with images, and the other—with associated image captions. Note that the compressed images folder is 13GB in size. ```bash\nwget http://images.cocodataset.org/annotations/annotations\\_trainval2014.zip\nwget http://images.cocodataset.org/zips/train2014.zip\nunzip annotations\\_trainval2014.zip\nunzip train2014.zip\nmkdir coco\\_dataset\nmv train2014 coco\\_dataset/\nmv annotations coco\\_dataset/\n```\n### Prepare dataset files and split the dataset. ```python\nimport json\nimport collections\nimages\\_dir = \""coco\\_dataset/train2014\""\nannotation\\_file = \""coco\\_dataset/annotations/captions\\_train2014.json\""\nwith open(annotation\\_file, \""r\"") as f:\nannotations = json.load(f)[\""annotations\""]\nimage\\_path\\_to\\_caption = collections.defaultdict(list)\nfor element in annotations:\ncaption = f\""{element['caption'].lower().rstrip('.')}\""\nimage\\_path = images\\_dir + \""/COCO\\_train2014\\_\"" + \""%012d.jpg\"" % (element[\""image\\_id\""])\nimage\\_path\\_to\\_caption[image\\_path].append(caption)\nlines = []\nfor image\\_path, captions in image\\_path\\_to\\_caption.items():\nlines.append(json.dumps({\""image\\_path\"": image\\_path, \""captions\"": captions}))\ntrain\\_lines = lines[:-8000]\nvalid\\_line = lines[-8000:]\nwith open(\""coco\\_dataset/train\\_dataset.json\"", \""w\"") as f:\nf.write(\""\\n\"".join(train\\_lines))\nwith open(\""coco\\_dataset/valid\\_dataset.json\"", \""w\"") as f:\nf.write(\""\\n\"".join(valid\\_line))\n```\n> Note: The data loading and processing part of this script can still be improved for maximum performance."", ""### Distributed\nTo split your dataset across your training nodes, you can use [`datasets.distributed.split\\_dataset\\_by\\_node`]:\n```python\nimport os\nfrom datasets.distributed import split\\_dataset\\_by\\_node\nds = split\\_dataset\\_by\\_node(ds, rank=int(os.environ[\""RANK\""]), world\\_size=int(os.environ[\""WORLD\\_SIZE\""]))\n```\nThis works for both map-style datasets and iterable datasets. The dataset is split for the node at rank `rank` in a pool of nodes of size `world\\_size`. For map-style datasets:\nEach node is assigned a chunk of data, e.g. rank 0 is given the first chunk of the dataset. For iterable datasets:\nIf the dataset has a number of shards that is a factor of `world\\_size` (i.e. if `dataset.n\\_shards % world\\_size == 0`),\nthen the shards are evenly assigned across the nodes, which is the most optimized. Otherwise, each node keeps 1 example out of `world\\_size`, skipping the other examples. This can also be combined with a `torch.utils.data.DataLoader` if you want each node to use multiple workers to load the data.""]","The ""duorc"" dataset has six splits.",Six
"What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?
","[""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Fully Sharded Data Parallel\n[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or \\*rank\\*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs. FSDP is integrated with the Accelerate, a library for easily managing training in distributed environments, which means it is available for use from the [`Trainer`] class. Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0 or newer. ```bash\npip install accelerate\n```\n## FSDP configuration\nTo start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package\\_reference/cli#accelerate-config) command to create a configuration file for your training environment. Accelerate uses this configuration file to automatically setup the correct training environment based on your selected training options in `accelerate config`. ```bash\naccelerate config\n```\nWhen you run `accelerate config`, you'll be prompted with a series of options to configure your training environment."", ""This resource limitation might seem obvious and inevitable, but is there really no alternative to using pretrained models for the broader ML community? However, there might be a way out of this situation: to come up with a solution, we only need to take a look around. It might be the case that the computational resources we're looking for are already there; for example, many of us have powerful computers with gaming or workstation GPUs at home. You might've already guessed that we're going to join their power similarly to [Folding@home](https://foldingathome.org/), [Rosetta@home](https://boinc.bakerlab.org/), [Leela Chess Zero](https://lczero.org/) or different [BOINC](https://boinc.berkeley.edu/) projects that leverage volunteer computing, but the approach is even more general. For instance, several laboratories can join their smaller clusters to utilize all the available resources, and some might want to join the experiment using inexpensive cloud instances. To a skeptical mind, it might seem that we're missing a key factor here: data transfer in distributed DL is often a bottleneck, since we need to aggregate the gradients from multiple workers. Indeed, any naïve approach to distributed training over the Internet is bound to fail, as most participants don't have gigabit connections and might disconnect from the network at any time. So how on Earth can you train anything with a household data plan? :)\nAs a solution to this problem, we propose a new training algorithm, called Distributed Deep Learning in Open Collaborations (or \\*\\*DeDLOC\\*\\*), which is described in detail in our recently released [preprint](https://arxiv.org/abs/2106.10207). Now, let’s find out what are the core ideas behind this algorithm! ### Training with volunteers\nIn its most frequently used version, distributed training with multiple GPUs is pretty straightforward. Recall that when doing deep learning, you usually compute gradients of your loss function averaged across many examples in a batch of training data. In case of \\_data-parallel\\_ distributed DL, you simply split the data across multiple workers, compute gradients separately, and then average them once the local batches are processed. When the average gradient is computed on all workers, we adjust the model weights with the optimizer and continue training our model. You can see an illustration of different tasks that are executed below. ![assets/24\\_sahajBERT/roles\\_tasks.png](assets/24\\_sahajBERT/roles\\_tasks.png)\n\nTypical machine learning tasks executed by peers in distributed training, possibly with a separation of roles\n\nOften, to reduce the amount of synchronization and to stabilize the learning process, we can accumulate the gradients for N batches before averaging, which is equivalent to increasing the actual batch size N times. This approach, combined with the observation that most state-of-the-art language models use large batches, led us to a simple idea: let's accumulate one \\_very\\_ large batch across all volunteer devices before each optimizer step!"", ""Megatron-DeepSpeed implements 3D Parallelism to allow huge models to train in a very efficient way. Let’s briefly discuss the 3D components. 1. \\*\\*DataParallel (DP)\\*\\* - the same setup is replicated multiple times, and each being fed a slice of the data. The processing is done in parallel and all setups are synchronized at the end of each training step. 2. \\*\\*TensorParallel (TP)\\*\\* - each tensor is split up into multiple chunks, so instead of having the whole tensor reside on a single GPU, each shard of the tensor resides on its designated GPU. During processing each shard gets processed separately and in parallel on different GPUs and the results are synced at the end of the step. This is what one may call horizontal parallelism, as the splitting happens on a horizontal level. 3. \\*\\*PipelineParallel (PP)\\*\\* - the model is split up vertically (layer-level) across multiple GPUs, so that only one or several layers of the model are placed on a single GPU. Each GPU processes in parallel different stages of the pipeline and works on a small chunk of the batch. 4. \\*\\*Zero Redundancy Optimizer (ZeRO)\\*\\* - also performs sharding of the tensors somewhat similar to TP, except the whole tensor gets reconstructed in time for a forward or backward computation, therefore the model doesn't need to be modified."", ""This would result in the CPU RAM getting out of memory leading to processes being terminated. 2. Saving entire intermediate checkpoints using `FULL\\_STATE\\_DICT` with CPU offloading on rank 0 takes a lot of time and often results in NCCL Timeout errors due to indefinite hanging during broadcasting. However, at the end of training, we want the whole model state dict instead of the sharded state dict which is only compatible with FSDP. 3. We need to improve the speed and reduce the VRAM usage to train faster and save compute costs. Let’s look at how to solve the above challenges and fine-tune a 70B model! Before we get started, here's all the required resources to reproduce our results:\n1. Codebase:\nhttps://github.com/pacman100/DHS-LLM-Workshop/tree/main/chat\\_assistant/training with flash-attn V2 monkey patch\n2. FSDP config: https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat\\_assistant/training/configs/fsdp\\_config.yaml\n3. SLURM script `launch.slurm`: https://gist.github.com/pacman100/1cb1f17b2f1b3139a63b764263e70b25\n4. Model: `meta-llama/Llama-2-70b-chat-hf`\n5. Dataset: [smangrul/code-chat-assistant-v1](https://huggingface.co/datasets/smangrul/code-chat-assistant-v1) (mix of LIMA+GUANACO with proper formatting in a ready-to-train format)\n### Pre-requisites\nFirst follow these steps to install Flash Attention V2: Dao-AILab/flash-attention: Fast and memory-efficient exact attention (github.com)."", ""In terms of train time, DDP with mixed precision is the fastest followed by FSDP using ZERO Stage 2 and Stage 3, respectively. As the task of causal language modelling always has fixed context sequence length (--block\\_size), the train time speedup with FSDP wasn’t that great. For applications with dynamic batching, FSDP which enables larger batch sizes will likely have considerable speed up in terms of train time. FSDP mixed precision support currently has few [issues](https://github.com/pytorch/pytorch/issues/75676) with transformer. Once this is supported, the training time speed up will further improve considerably. ### CPU Offloading to enable training humongous models that won’t fit the GPU memory\nCommand for training GPT-2 XL Model (1.5B parameters):\n```bash\nexport BS=#`try with different batch sizes till you don't get OOM error,\n#i.e., start with larger batch size and go on decreasing till it fits on GPU`\ntime accelerate launch run\\_clm\\_no\\_trainer.py \\\n--model\\_name\\_or\\_path gpt2-xl \\\n--dataset\\_name wikitext \\\n--dataset\\_config\\_name wikitext-2-raw-v1 \\\n--per\\_device\\_train\\_batch\\_size $BS\n--per\\_device\\_eval\\_batch\\_size $BS\n--num\\_train\\_epochs 1\n--block\\_size 12\n```\n| Method | Batch Size Max ($BS) | Num GPUs | Approx Train Time (Hours) | Notes |\n| --- | --- | --- | --- | --- |\n| DDP | 1 | 1 | NA | OOM Error RuntimeError: CUDA out of memory.""]","Fully Sharded Data Parallel (FSDP) is a data parallel method that shards a model's parameters, gradients, and optimizer states across the number of available GPUs. The purpose of FSDP is to reduce memory usage by not replicating the entire model on each GPU, which improves GPU memory efficiency. This allows for the training of much larger models on fewer GPUs. FSDP is integrated with the Accelerate library, making it easier to manage training in distributed environments.","FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes."
"What file format is used to save and store PyTorch model weights more securely than `.bin` files?
","[""Convert weights to safetensors\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`. The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository. For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights. Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space."", ""On CPU, if the file is already in cache, then it can\ntruly be zero-copy, whereas on GPU there is not such disk cache, so a copy is always required\nbut you can bypass allocating all the tensors on CPU at any given point. SafeTensors is not zero-copy for the header. The choice of JSON is pretty arbitrary, but since deserialization is <<< of the time required to load the actual tensor data and is readable I went that way, (also space is <<< to the tensor data). - Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the\nmoment. - Order: 'C' or row-major. This seems to have won. We can add that information later if needed. - Stride: No striding, all tensors need to be packed before being serialized. I have yet to see a case where it seems useful to have a strided tensor stored in serialized format. ### Benefits\nSince we can invent a new format we can propose additional benefits:\n- Prevent DOS attacks: We can craft the format in such a way that it's almost\nimpossible to use malicious files to DOS attack a user. Currently, there's a limit\non the size of the header of 100MB to prevent parsing extremely large JSON. Also when reading the file, there's a guarantee that addresses in the file\ndo not overlap in any way, meaning when you're loading a file you should never\nexceed the size of the file in memory\n- Faster load: PyTorch seems to be the fastest file to load out in the major\nML formats. However, it does seem to have an extra copy on CPU, which we\ncan bypass in this lib by using `torch.UntypedStorage.from\\_file`. Currently, CPU loading times are extremely fast with this lib compared to pickle. GPU loading times are as fast or faster than PyTorch equivalent. Loading first on CPU with memmapping with torch, and then moving all tensors to GPU seems\nto be faster too somehow (similar behavior in torch pickle)\n- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to\nload only part of the tensors on the various models. For\n[BLOOM](https://huggingface.co/bigscience/bloom) using this format enabled\nto load the model on 8 GPUs from 10mn with regular PyTorch weights down to 45s."", ""I've taken care to make sure any third party code included or adapted has compatible (permissive) licenses such as MIT, BSD, etc. I've made an effort to avoid any GPL / LGPL conflicts. That said, it is your responsibility to ensure you comply with licenses here and conditions of any dependent licenses. Where applicable, I've linked the sources/references for various components in docstrings. If you think I've missed anything please create an issue. ### Pretrained Weights\nSo far all of the pretrained weights available here are pretrained on ImageNet with a select few that have some additional pretraining (see extra note below). ImageNet was released for non-commercial research purposes only (https://image-net.org/download). It's not clear what the implications of that are for the use of pretrained weights from that dataset. Any models I have trained with ImageNet are done for research purposes and one should assume that the original dataset license applies to the weights. It's best to seek legal advice if you intend to use the pretrained weights in a commercial product. #### Pretrained on more than ImageNet\nSeveral weights included or references here were pretrained with proprietary datasets that I do not have access to. These include the Facebook WSL, SSL, SWSL ResNe(Xt) and the Google Noisy Student EfficientNet models. The Facebook models have an explicit non-commercial license (CC-BY-NC 4.0, https://github.com/facebookresearch/semi-supervised-ImageNet1K-models, https://github.com/facebookresearch/WSL-Images). The Google models do not appear to have any restriction beyond the Apache 2.0 license (and ImageNet concerns). In either case, you should contact Facebook or Google with any questions. ## Citing\n### BibTeX\n```bibtex\n@misc{rw2019timm,\nauthor = {Ross Wightman},\ntitle = {PyTorch Image Models},\nyear = {2019},\npublisher = {GitHub},\njournal = {GitHub repository},\ndoi = {10.5281/zenodo.4414861},\nhowpublished = {\\url{https://github.com/rwightman/pytorch-image-models}}\n}\n```\n### Latest DOI\n[![DOI](https://zenodo.org/badge/168799526.svg)](https://zenodo.org/badge/latestdoi/168799526)"", ""By default, this is set to `True` which is used to train in mixed Int8/FP16 precision. However, we are interested in memory efficient inference for which we need to use `has\\_fp16\\_weights=False`. 5. Now time to load your model in 8-bit! ```py\nint8\\_model.load\\_state\\_dict(torch.load(\""model.pt\""))\nint8\\_model = int8\\_model.to(0) # Quantization happens here\n```\nNote that the quantization step is done in the second line once the model is set on the GPU. If you print `int8\\_model[0].weight` before calling the `.to` function you get:\n```\nint8\\_model[0].weight\nParameter containing:\ntensor([[ 0.0031, -0.0438, 0.0494, ..., -0.0046, -0.0410, 0.0436],\n[-0.1013, 0.0394, 0.0787, ..., 0.0986, 0.0595, 0.0162],\n[-0.0859, -0.1227, -0.1209, ..., 0.1158, 0.0186, -0.0530],\n...,\n[ 0.0804, 0.0725, 0.0638, ..., -0.0487, -0.0524, -0.1076],\n[-0.0200, -0.0406, 0.0663, ..., 0.0123, 0.0551, -0.0121],\n[-0.0041, 0.0865, -0.0013, ..., -0.0427, -0.0764, 0.1189]],\ndtype=torch.float16)\n```\nWhereas if you print it after the second line's call you get:\n```\nint8\\_model[0].weight\nParameter containing:\ntensor([[ 3, -47, 54, ..., -5, -44, 47],\n[-104, 40, 81, ..., 101, 61, 17],\n[ -89, -127, -125, ..., 120, 19, -55],\n...,\n[ 82, 74, 65, ..., -49, -53, -109],\n[ -21, -42, 68, ..., 13, 57, -12],\n[ -4, 88, -1, ..., -43, -78, 121]],\ndevice='cuda:0', dtype=torch.int8, requires\\_grad=True)\n```\nThe weights values are \""truncated\"" as we have seen when explaining quantization in the previous sections. Also, the values seem to be distributed between [-127, 127]. You might also wonder how to retrieve the FP16 weights in order to perform the outlier MatMul in fp16? You can simply do:\n```py\n(int8\\_model[0].weight.CB \\* int8\\_model[0].weight.SCB) / 127\n```\nAnd you will get:\n```\ntensor([[ 0.0028, -0.0459, 0.0522, ..., -0.0049, -0.0428, 0.0462],\n[-0.0960, 0.0391, 0.0782, ..., 0.0994, 0.0593, 0.0167],\n[-0.0822, -0.1240, -0.1207, ..., 0.1181, 0.0185, -0.0541],\n...,\n[ 0.0757, 0.0723, 0.0628, ..., -0.0482, -0.0516, -0.1072],\n[-0.0194, -0.0410, 0.0657, ..., 0.0128, 0.0554, -0.0118],\n[-0.0037, 0.0859, -0.0010, ..., -0.0423, -0.0759, 0.1190]],\ndevice='cuda:0')\n```\nWhich is close enough to the original FP16 values (2 print outs up)!"", ""In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future. ## Sharded checkpoints\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save\\_pretrained(save\\_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in. You can control the maximum size before sharding with the `max\\_shard\\_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model. ```py\nfrom transformers import AutoModel\nmodel = AutoModel.from\\_pretrained(\""bert-base-cased\"")\n```\nIf you save it using [`~PreTrainedModel.save\\_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n```py\n>>> import os\n>>> import tempfile\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir)\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model.bin']\n```\nNow let's use a maximum shard size of 200MB:\n```py\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model-00001-of-00003.bin', 'pytorch\\_model-00002-of-00003.bin', 'pytorch\\_model-00003-of-00003.bin', 'pytorch\\_model.bin.index.json']\n```\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index.""]",The more secure file format used to save and store PyTorch model weights is `.safetensors`.,`.safetensors`
"What type of security certification does Hugging Face have?
","[""Security\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning. Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\nFor any other security questions, please feel free to send us an email at security@huggingface.co. ## Contents\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)"", ""The certification process\nThe certification process is \\*\\*completely free\\*\\*:\n- To get a \\*certificate of completion\\*: you need \\*\\*to pass 80% of the assignments\\*\\*. - To get a \\*certificate of excellence\\*: you need \\*\\*to pass 100% of the assignments\\*\\*. There's \\*\\*no deadlines, the course is self-paced\\*\\*. ![Course certification](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit0/certification.jpg)\nWhen we say pass, \\*\\*we mean that your model must be pushed to the Hub and get a result equal or above the minimal requirement\\*\\*. To check your progression and which unit you passed/not passed: https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course\nNow that you're ready for the certification process, you need to:\n1. Go here: https://huggingface.co/spaces/huggingface-projects/Deep-RL-Course-Certification/\n2. Type your \\*hugging face username\\*, your \\*first name\\*, \\*last name\\*\n3. Click on \""Generate my certificate\"". - If you passed 80% of the assignments, \\*\\*congratulations\\*\\* you've just got the certificate of completion. - If you passed 100% of the assignments, \\*\\*congratulations\\*\\* you've just got the excellence certificate. - If you are below 80%, don't be discouraged!"", ""For more info on the Hugging Face and Habana Gaudi collaboration, please visit [https://huggingface.co/Habana](https://huggingface.co/Habana)."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction"", ""The ability to use Hugging Face’s comprehensive hub of foundation models means that users can pick the models that best align with their business needs without having to invest in the resources required to train them. This integration is a significant step forward in making foundation models more accessible to enterprises around the world. \\_If you’re interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!\\_""]","Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses.",SOC2 Type 2 certified
"What do RAG models combine to generate outputs?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# RAG\n\n[![Models](https://img.shields.io/badge/All_model_pages-rag-blueviolet)](https://huggingface.co/models?filter=rag)\n\n## Overview\nRetrieval-augmented generation (\""RAG\"") models combine the powers of pretrained dense retrieval (DPR) and\nsequence-to-sequence models."", ""It can simply be created by setting the values of the `inputs` and `outputs` parameter as the same component. Here's an example demo of a text generation model:\n$code\\_unified\\_demo\\_text\\_generation\n$demo\\_unified\\_demo\\_text\\_generation"", ""This can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion, especially with a small number of layers. Growing the receptive field has been shown to improve quality significantly (Salimans et al.). Doing so, however, comes at a significant cost in number of parameters and consequently computational performance and can make training such models more challenging. >\n> ... self-attention can achieve a better balance in the trade-off between the virtually unlimited receptive field of the necessarily sequential PixelRNN and the limited receptive field of the much more parallelizable PixelCNN and its various extensions. [Image Transformer](https://arxiv.org/abs/1802.05751) uses transformers by restricting self attention over local neighborhoods of pixels. [Taming Transformers](https://arxiv.org/abs/2012.09841) and [DALL-E 1](https://arxiv.org/abs/2102.12092) combine convolutions and transformers. Both train a VQ-VAE to learn a discrete latent space, and then a transformer is trained in the compressed latent space. The transformer context is global but masked, because attention is provided over all previously predicted latent pixels, but the model is still AR so attention cannot be provided over not yet predicted pixels. [ImageBART](https://arxiv.org/abs/2108.08827) combines convolutions, transformers, and diffusion processes."", ""In other words, there is a good chance that you avoid the 25s warm up time, even if you do not request your model that often. The blue model is already downloaded and ready, all we have to do is unload the previous adapter and load the new one, which takes \\*\\*3s\\*\\* as we see [below](#loading-figures). Overall, this requires less GPUs to serve all distinct models, even though we already had a way to share GPUs between deployments to maximize their compute usage. In a \\*\\*2min\\*\\* time frame, there are approximately \\*\\*10\\*\\* distinct LoRA weights that are requested. Instead of spawning 10 deployments, and keeping them warm, we simply serve all of them with 1 to 2 GPUs (or more if there is a request burst). ## Implementation\nWe implemented LoRA mutualization in the Inference API. When a request is performed on a model available in our platform, we first determine whether this is a LoRA or not. We then identify the base model for the LoRA and route the request to a common backend farm, with the ability to serve requests for the said model. Inference requests get served by keeping the base model warm and loading/unloading LoRAs on the fly. This way we can ultimately reuse the same compute resources to serve many distinct models at once. ### LoRA structure\nIn the Hub, LoRAs can be identified with two attributes:\n![Hub](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/171\\_load\\_lora\\_adapters/lora\\_adapter\\_hub.png)\nA LoRA will have a ```base\\_model``` attribute."", ""Each expert specializes in a\ndifferent subset of the data, and a gating network determines the appropriate\nexpert to use for a given input. This approach allows the model to adapt to\ncomplex, non-linear relationships in the data and improve overall performance. >\n### Using Text Generation Inference\n\\*\\*[Text Generation Inference](https://github.com/huggingface/text-generation-inference)\\*\\* is a production-ready inference container developed by Hugging Face to enable easy deployment of large language models. It has features such as continuous batching, token streaming, tensor parallelism for fast inference on multiple GPUs, and production-ready logging and tracing. You can deploy Mixtral on Hugging Face's [Inference Endpoints](https://ui.endpoints.huggingface.co/new?repository=mistralai%2FMixtral-8x7B-Instruct-v0.1&vendor=aws&region=us-east-1&accelerator=gpu&instance\\_size=2xlarge&task=text-generation&no\\_suggested\\_compute=true&tgi=true&tgi\\_max\\_batch\\_total\\_tokens=1024000&tgi\\_max\\_total\\_tokens=32000), which uses Text Generation Inference as the backend. To deploy a Mixtral model, go to the [model page](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) and click on the [Deploy -> Inference Endpoints](https://ui.endpoints.huggingface.co/new?repository=meta-llama/Llama-2-7b-hf) widget.""]",RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs.,Pretrained dense retrieval (DPR) and sequence-to-sequence models.
"What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# MarkupLM\n## Overview\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm). The model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\nstate-of-the-art results on 2 important benchmarks:\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n- [SWDE](https://www.researchgate.net/publication/221299838\\_From\\_one\\_tree\\_to\\_a\\_forest\\_a\\_unified\\_solution\\_for\\_structured\\_web\\_data\\_extraction), a dataset\nfor information extraction from web pages (basically named-entity recogntion on web pages)\nThe abstract from the paper is the following:\n\\*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\ntasks. The pre-trained model and code will be publicly available.\\*\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm). ## Usage tips\n- In addition to `input\\_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath\\_tags\\_seq` and `xpath\\_subs\\_seq`. These are the XPATH tags and subscripts respectively for each token in the input sequence. - One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info. ![drawing](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg)\n MarkupLM architecture. Taken from the [original paper.](https://arxiv.org/abs/2110.08518) \n## Usage: MarkupLMProcessor\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input\\_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks. ```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\nfeature\\_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from\\_pretrained(\""microsoft/markuplm-base\"")\nprocessor = MarkupLMProcessor(feature\\_extractor, tokenizer)\n```\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input\\_ids`, `attention\\_mask`, `token\\_type\\_ids`, `xpath\\_subs\\_seq`, `xpath\\_tags\\_seq`. Optionally, one can provide node labels to the processor, which are turned into token-level `labels`. [`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Utilities for `FeatureExtractors`\nThis page lists all the utility functions that can be used by the audio [`FeatureExtractor`] in order to compute special features from a raw audio using common algorithms such as \\*Short Time Fourier Transform\\* or \\*log mel spectrogram\\*. Most of those are only useful if you are studying the code of the audio processors in the library. ## Audio Transformations\n[[autodoc]] audio\\_utils.hertz\\_to\\_mel\n[[autodoc]] audio\\_utils.mel\\_to\\_hertz\n[[autodoc]] audio\\_utils.mel\\_filter\\_bank\n[[autodoc]] audio\\_utils.optimal\\_fft\\_length\n[[autodoc]] audio\\_utils.window\\_function\n[[autodoc]] audio\\_utils.spectrogram\n[[autodoc]] audio\\_utils.power\\_to\\_db\n[[autodoc]] audio\\_utils.amplitude\\_to\\_db"", ""As LayoutXLM's architecture is equivalent to that of LayoutLMv2, one can refer to [LayoutLMv2's documentation page](layoutlmv2) for all tips, code examples and notebooks. ## LayoutXLMTokenizer\n[[autodoc]] LayoutXLMTokenizer\n- \\_\\_call\\_\\_\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## LayoutXLMTokenizerFast\n[[autodoc]] LayoutXLMTokenizerFast\n- \\_\\_call\\_\\_\n## LayoutXLMProcessor\n[[autodoc]] LayoutXLMProcessor\n- \\_\\_call\\_\\_"", ""It is based on Facebook's\nRoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl\ndata. The abstract from the paper is the following:\n\\*This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a\nwide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred\nlanguages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly\noutperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on\nXNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on\nlow-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We\nalso present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the\ntrade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource\nlanguages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing\nper-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We\nwill make XLM-R code, data, and models publicly available.\\*\nThis model was contributed by [stefan-it](https://huggingface.co/stefan-it). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/xlmr). ## Usage tips\n- XLM-RoBERTa is a multilingual model trained on 100 different languages. Unlike some XLM multilingual models, it does\nnot require `lang` tensors to understand which language is used, and should be able to determine the correct\nlanguage from the input ids."", ""\\*\\*[LED](https://huggingface.co/docs/transformers/model\\_doc/led)\\*\\* (AllenAI から) Iz Beltagy, Matthew E. Peters, Arman Cohan から公開された研究論文: [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)\n1. \\*\\*[LeViT](https://huggingface.co/docs/transformers/model\\_doc/levit)\\*\\* (Meta AI から) Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze から公開された研究論文: [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://arxiv.org/abs/2104.01136)\n1. \\*\\*[LiLT](https://huggingface.co/docs/transformers/model\\_doc/lilt)\\*\\* (South China University of Technology から) Jiapeng Wang, Lianwen Jin, Kai Ding から公開された研究論文: [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669)\n1. \\*\\*[LLaMA](https://huggingface.co/docs/transformers/model\\_doc/llama)\\*\\* (The FAIR team of Meta AI から) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. から公開された研究論文 [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)\n1. \\*\\*[Llama2](https://huggingface.co/docs/transformers/model\\_doc/llama2)\\*\\* (The FAIR team of Meta AI から) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.. から公開された研究論文 [Llama2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/XXX)\n1. \\*\\*[LLaVa](https://huggingface.co/docs/transformers/model\\_doc/llava)\\*\\* (Microsoft Research & University of Wisconsin-Madison から) Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee. から公開された研究論文 [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)\n1. \\*\\*[Longformer](https://huggingface.co/docs/transformers/model\\_doc/longformer)\\*\\* (AllenAI から) Iz Beltagy, Matthew E. Peters, Arman Cohan から公開された研究論文: [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)\n1. \\*\\*[LongT5](https://huggingface.co/docs/transformers/model\\_doc/longt5)\\*\\* (Google AI から) Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang から公開された研究論文: [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\n1. \\*\\*[LUKE](https://huggingface.co/docs/transformers/model\\_doc/luke)\\*\\* (Studio Ousia から) Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto から公開された研究論文: [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057)\n1. \\*\\*[LXMERT](https://huggingface.co/docs/transformers/model\\_doc/lxmert)\\*\\* (UNC Chapel Hill から) Hao Tan and Mohit Bansal から公開された研究論文: [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://arxiv.org/abs/1908.07490)\n1. \\*\\*[M-CTC-T](https://huggingface.co/docs/transformers/model\\_doc/mctct)\\*\\* (Facebook から) Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert から公開された研究論文: [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://arxiv.org/abs/2111.00161)\n1. \\*\\*[M2M100](https://huggingface.co/docs/transformers/model\\_doc/m2m\\_100)\\*\\* (Facebook から) Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin から公開された研究論文: [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125)\n1. \\*\\*[MADLAD-400](https://huggingface.co/docs/transformers/model\\_doc/madlad-400)\\*\\* (from Google) released with the paper [MADLAD-400: A Multilingual And Document-Level Large Audited Dataset](https://arxiv.org/abs/2309.04662) by Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A.""]","MarkupLMFeatureExtractor uses Beautiful Soup, a Python library for pulling data out of HTML and XML files, to extract data from HTML and XML files.",Beautiful Soup
"What is the file size limit for syncing to HF Spaces without using Git-LFS?
","[""There are a few reasons for this:\n- Uploading and downloading smaller files is much easier both for you and the other users. Connection issues can always\nhappen when streaming data and smaller files avoid resuming from the beginning in case of errors. - Files are served to the users using CloudFront. From our experience, huge files are not cached by this service\nleading to a slower download speed. In all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size. - \\*\\*Number of commits\\*\\*: There is no hard limit for the total number of commits on your repo history. However, from\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a\nfresh start using `huggingface\\_hub`'s [`super\\_squash\\_history`](https://huggingface.co/docs/huggingface\\_hub/main/en/package\\_reference/hf\\_api#huggingface\\_hub.HfApi.super\\_squash\\_history)."", ""Managing Spaces with CircleCI Workflows\nYou can keep your app in sync with your GitHub repository with a \\*\\*CircleCI workflow\\*\\*. [CircleCI](https://circleci.com) is a continuous integration and continuous delivery (CI/CD) platform that helps automate the software development process. A [CircleCI workflow](https://circleci.com/docs/workflows/) is a set of automated tasks defined in a configuration file, orchestrated by CircleCI, to streamline the process of building, testing, and deploying software applications. \\*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\\*\nFirst, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository. ```bash\ngit remote add space https://huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME\n```\nThen force push to sync everything for the first time:\n```bash\ngit push --force space main\n```\nNext, set up a [CircleCI workflow](https://circleci.com/docs/workflows/) to push your `main` git branch to Spaces. In the example below:\n\\* Replace `HF\\_USERNAME` with your username and `SPACE\\_NAME` with your Space name. \\* [Create a context in CircleCI](https://circleci.com/docs/contexts/) and add an env variable into it called \\*HF\\_PERSONAL\\_TOKEN\\* (you can give it any name, use the key you create in place of HF\\_PERSONAL\\_TOKEN) and the value as your Hugging Face API token. You can find your Hugging Face API token under \\*\\*API Tokens\\*\\* on [your Hugging Face profile](https://huggingface.co/settings/tokens). ```yaml\nversion: 2.1\nworkflows:\nmain:\njobs:\n- sync-to-huggingface:\ncontext:\n- HuggingFace\nfilters:\nbranches:\nonly:\n- main\njobs:\nsync-to-huggingface:\ndocker:\n- image: alpine\nresource\\_class: small\nsteps:\n- run:\nname: install git\ncommand: apk update && apk add openssh-client git\n- checkout\n- run:\nname: push to Huggingface hub\ncommand: |\ngit config user.email \""\""\ngit config user.name \""\""\ngit push -f https://HF\\_USERNAME:${HF\\_PERSONAL\\_TOKEN}@huggingface.co/spaces/HF\\_USERNAME/SPACE\\_NAME main\n```"", ""Disk usage on Spaces\nEvery Space comes with a small amount of disk storage. This disk space is ephemeral, meaning its content will be lost if your Space restarts or is stopped. If you need to persist data with a longer lifetime than the Space itself, you can:\n- [Subscribe to a persistent storage upgrade](#persistent-storage)\n- [Use a dataset as a data store](#dataset-storage)\n## Persistent storage\nYou can upgrade your Space to have access to persistent disk space from the \\*\\*Settings\\*\\* tab. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-storage-settings.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-storage-settings-dark.png)\n\nYou can choose the storage tier of your choice to access disk space that persists across restarts of your Space. Persistent storage acts like traditional disk storage mounted on `/data`. That means you can `read` and `write to` this storage from your Space as you would with a traditional hard drive or SSD. Persistent disk space can be upgraded to a larger tier at will, though it cannot be downgraded to a smaller tier. If you wish to use a smaller persistent storage tier, you must delete your current (larger) storage first. If you are using Hugging Face open source libraries, you can make your Space restart faster by setting the environment variable `HF\\_HOME` to `/data/.huggingface`. Libraries like `transformers`, `diffusers`, `datasets` and others use that environment variable to cache any assets downloaded from the Hugging Face Hub."", ""However,\nsymlinks are not supported on all machines. This is a known limitation especially on\nWindows. When this is the case, `huggingface\\_hub` do not use the `blobs/` directory but\ndirectly stores the files in the `snapshots/` directory instead. This workaround allows\nusers to download and cache files from the Hub exactly the same way. Tools to inspect\nand delete the cache (see below) are also supported. However, the cache-system is less\nefficient as a single file might be downloaded several times if multiple revisions of\nthe same repo is downloaded. If you want to benefit from the symlink-based cache-system on a Windows machine, you\neither need to [activate Developer Mode](https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development)\nor to run Python as an administrator. When symlinks are not supported, a warning message is displayed to the user to alert\nthem they are using a degraded version of the cache-system. This warning can be disabled\nby setting the `HF\\_HUB\\_DISABLE\\_SYMLINKS\\_WARNING` environment variable to true. ## Caching assets\nIn addition to caching files from the Hub, downstream libraries often requires to cache\nother files related to HF but not handled directly by `huggingface\\_hub` (example: file\ndownloaded from GitHub, preprocessed data, logs,...). In order to cache those files,\ncalled `assets`, one can use [`cached\\_assets\\_path`]. This small helper generates paths\nin the HF cache in a unified way based on the name of the library requesting it and\noptionally on a namespace and a subfolder name."", ""## Faster downloads\nIf you are running on a machine with high bandwidth, you can increase your download speed with [`hf\\_transfer`](https://github.com/huggingface/hf\\_transfer), a Rust-based library developed to speed up file transfers with the Hub. To enable it, install the package (`pip install hf\\_transfer`) and set `HF\\_HUB\\_ENABLE\\_HF\\_TRANSFER=1` as an environment variable. Progress bars are supported in `hf\\_transfer` starting from version `0.1.4`. Consider upgrading (`pip install -U hf-transfer`) if you plan to enable faster downloads. `hf\\_transfer` is a power user tool! It is tested and production-ready, but it lacks user-friendly features like advanced error handling or proxies. For more details, please take a look at this [section](https://huggingface.co/docs/huggingface\\_hub/hf\\_transfer).""]","The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. If you have files larger than this size, you will need to use Git-LFS.",10MB
"What is the title of the paper introducing the ByT5 model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# ByT5\n## Overview\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, Colin Raffel."", ""## Conclusion\nSpeechT5 is an interesting model because — unlike most other models — it allows you to perform multiple tasks with the same architecture. Only the pre-nets and post-nets change. By pre-training the model on these combined tasks, it becomes more capable at doing each of the individual tasks when fine-tuned. We have only included checkpoints for the speech recognition (ASR), speech synthesis (TTS), and voice conversion tasks but the paper also mentions the model was successfully used for speech translation, speech enhancement, and speaker identification. It’s very versatile!"", ""This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released\\_checkpoints.md#t511). ## Usage tips\nOne can directly plug in the weights of T5v1.1 into a T5 model, like so:\n```python\n>>> from transformers import T5ForConditionalGeneration\n>>> model = T5ForConditionalGeneration.from\\_pretrained(\""google/t5-v1\\_1-base\"")\n```\nT5 Version 1.1 includes the following improvements compared to the original T5 model:\n- GEGLU activation in the feed-forward hidden layer, rather than ReLU. See [this paper](https://arxiv.org/abs/2002.05202). - Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning. - Pre-trained on C4 only without mixing in the downstream tasks. - No parameter sharing between the embedding and classifier layer. - \""xl\"" and \""xxl\"" replace \""3B\"" and \""11B\"". The model shapes are a bit different - larger `d\\_model` and smaller\n`num\\_heads` and `d\\_ff`. Note: T5 Version 1.1 was only pre-trained on [C4](https://huggingface.co/datasets/c4) excluding any supervised\ntraining. Therefore, this model has to be fine-tuned before it is usable on a downstream task, unlike the original T5\nmodel. Since t5v1.1 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix. Google has released the following variants:\n- [google/t5-v1\\_1-small](https://huggingface.co/google/t5-v1\\_1-small)\n- [google/t5-v1\\_1-base](https://huggingface.co/google/t5-v1\\_1-base)\n- [google/t5-v1\\_1-large](https://huggingface.co/google/t5-v1\\_1-large)\n- [google/t5-v1\\_1-xl](https://huggingface.co/google/t5-v1\\_1-xl)\n- [google/t5-v1\\_1-xxl](https://huggingface.co/google/t5-v1\\_1-xxl). Refer to [T5's documentation page](t5) for all API reference, tips, code examples and notebooks."", ""1. \\*\\*[Falcon](https://huggingface.co/docs/transformers/model\\_doc/falcon)\\*\\* (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme. 1. \\*\\*[FLAN-T5](https://huggingface.co/docs/transformers/model\\_doc/flan-t5)\\*\\* (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\n1. \\*\\*[FLAN-UL2](https://huggingface.co/docs/transformers/model\\_doc/flan-ul2)\\*\\* (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H."", "")\n>>> labels\\_dict = tokenizer(\n... [\""La vie est comme une boîte de chocolat.\"", \""Aujourd'hui c'est lundi.\""], padding=\""longest\"", return\\_tensors=\""pt\""\n... )\n>>> labels = labels\\_dict.input\\_ids\n>>> loss = model(\\*\\*model\\_inputs, labels=labels).loss\n>>> loss.item()\n17.9\n```\nSimilar to [T5](t5), ByT5 was trained on the span-mask denoising task. However,\nsince the model works directly on characters, the pretraining task is a bit\ndifferent. Let's corrupt some characters of the\ninput sentence `\""The dog chases a ball in the park.\""` and ask ByT5 to predict them\nfor us. ```python\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n>>> import torch\n>>> tokenizer = AutoTokenizer.from\\_pretrained(\""google/byt5-base\"")\n>>> model = AutoModelForSeq2SeqLM.from\\_pretrained(\""google/byt5-base\"")\n>>> input\\_ids\\_prompt = \""The dog chases a ball in the park.\""\n>>> input\\_ids = tokenizer(input\\_ids\\_prompt).input\\_ids\n>>> # Note that we cannot add \""{extra\\_id\\_...}\"" to the string directly\n>>> # as the Byte tokenizer would incorrectly merge the tokens\n>>> # For ByT5, we need to work directly on the character level\n>>> # Contrary to T5, ByT5 does not use sentinel tokens for masking, but instead\n>>> # uses final utf character ids.""]","The title of the paper introducing the ByT5 model is ""ByT5: Towards a token-free future with pre-trained byte-to-byte models.""",ByT5: Towards a token-free future with pre-trained byte-to-byte models
"What is the dimension of the feature vector for the base BERT model?
","[""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# BERT\n\n[![Models](https://img.shields.io/badge/All_model_pages-bert-blueviolet)](https://huggingface.co/models?filter=bert)\n[![Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/docs-demos/bert-base-uncased)\n\n## Overview\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It's a\nbidirectional transformer pretrained using a combination of masked language modeling objective and next sentence\nprediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. The abstract from the paper is the following:\n\\*We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations\nfrom Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional\nrepresentations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result,\nthe pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models\nfor a wide range of tasks, such as question answering and language inference, without substantial task-specific\narchitecture modifications.\\*\n\\*BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural\nlanguage processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute\nimprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\*\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf)."", ""It's currently only used internally during certain operations. In the machine learning jargon FP32 is called full precision (4 bytes), while BF16 and FP16 are referred to as half-precision (2 bytes). On top of that, the int8 (INT8) data type consists of an 8-bit representation that can store 2^8 different values (between [0, 255] or [-128, 127] for signed integers). While, ideally the training and inference should be done in FP32, it is two times slower than FP16/BF16 and therefore a mixed precision approach is used where the weights are held in FP32 as a precise \""main weights\"" reference, while computation in a forward and backward pass are done for FP16/BF16 to enhance training speed. The FP16/BF16 gradients are then used to update the FP32 main weights. During training, the main weights are always stored in FP32, but in practice, the half-precision weights often provide similar quality during inference as their FP32 counterpart -- a precise reference of the model is only needed when it receives multiple gradient updates. This means we can use the half-precision weights and use half the GPUs to accomplish the same outcome. ![Model-storage](assets/96\\_hf\\_bitsandbytes\\_integration/Model-storage.png)\nTo calculate the model size in bytes, one multiplies the number of parameters by the size of the chosen precision in bytes. For example, if we use the bfloat16 version of the BLOOM-176B model, we have `176\\*10\\*\\*9 x 2 bytes = 352GB`!"", ""Hence, models like BERT and RoBERTa are limited to a max sequence length of 512\ntokens. Perceiver aims to solve this issue by, instead of performing self-attention on the inputs, perform it on a set\nof latent variables, and only use the inputs for cross-attention. In this way, the time and memory requirements don't\ndepend on the length of the inputs anymore, as one uses a fixed amount of latent variables, like 256 or 512. These are\nrandomly initialized, after which they are trained end-to-end using backpropagation. Internally, [`PerceiverModel`] will create the latents, which is a tensor of shape `(batch\\_size, num\\_latents,\nd\\_latents)`. One must provide `inputs` (which could be text, images, audio, you name it!) to the model, which it will\nuse to perform cross-attention with the latents. The output of the Perceiver encoder is a tensor of the same shape. One\ncan then, similar to BERT, convert the last hidden states of the latents to classification logits by averaging along\nthe sequence dimension, and placing a linear layer on top of that to project the `d\\_latents` to `num\\_labels`."", ""n this video, we'll study the encoder architecture. An example of a popular encoder-only architecture is BERT, which is the most popular model of its kind. Let's first start by understanding how it works. We'll use a small example, using three words. We use these as inputs, and pass them through the encoder. We retrieve a numerical representation of each word. Here, for example, the encoder converts the three words “Welcome to NYC” in these three sequences of numbers. The encoder outputs exactly one sequence of numbers per input word. This numerical representation can also be called a \""Feature vector\"", or \""Feature tensor\"". Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \""to\"", isn't the representation of only the \""to\"" word. It also takes into account the words around it, which we call the “context”.As in, it looks to the left context, the word on the left of the one we're studying (here the word \""Welcome\"") and the context on the right (here the word \""NYC\"") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \""meaning\"" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence."", ""Metric Card for BERT Score\n## Metric description\nBERTScore is an automatic evaluation metric for text generation that computes a similarity score for each token in the candidate sentence with each token in the reference sentence. It leverages the pre-trained contextual embeddings from [BERT](https://huggingface.co/bert-base-uncased) models and matches words in candidate and reference sentences by cosine similarity. Moreover, BERTScore computes precision, recall, and F1 measure, which can be useful for evaluating different language generation tasks. ## How to use\nBERTScore takes 3 mandatory arguments : `predictions` (a list of string of candidate sentences), `references` (a list of strings or list of list of strings of reference sentences) and either `lang` (a string of two letters indicating the language of the sentences, in [ISO 639-1 format](https://en.wikipedia.org/wiki/List\\_of\\_ISO\\_639-1\\_codes)) or `model\\_type` (a string specififying which model to use, according to the BERT specification). The default behavior of the metric is to use the suggested model for the target language when one is specified, otherwise to use the `model\\_type` indicated. ```python\nfrom datasets import load\\_metric\nbertscore = load\\_metric(\""bertscore\"")\npredictions = [\""hello there\"", \""general kenobi\""]\nreferences = [\""hello there\"", \""general kenobi\""]\nresults = bertscore.compute(predictions=predictions, references=references, lang=\""en\"")\n```\nBERTScore also accepts multiple optional arguments:\n`num\\_layers` (int): The layer of representation to use.""]",The dimension of the feature vector for the base BERT model is 768.,768
"What special identifier does the WordPiece Model use for continuing subwords?
","[""In turn, the model will now be able to make sense of token in different situations. It will understand that the words token, tokens, tokenizing, and tokenization are linked and have a similar meaning. It will also understand that tokenization, modernization, and immunization, which all have the same suffixes, are probably used in the same syntactic situations. Subword-based tokenizers generally have a way to identify which tokens are start of words, and which tokens complete start of words: token as the start of a word. ##ization as completing a word. Here the ## prefix indicates that ization is part of a word rather than the beginning of it. The ## comes from the BERT tokenizer, based on the WordPiece algorithm. Other tokenizers use other prefixes, which can be placed to indicate part of words like seen here, or start of words instead! There are a lot of different algorithms that can be used for subword tokenization, and most models obtaining state-of-the-art results in English today use some kind of subword-tokenization algorithm."", ""This gives us a score of 0.25. Now that we know how to calculate this score, we can do it for all pairs. We can now add to the vocabulary the pair with the highest score, after merging it of course! And now we can apply this same fusion to our split corpus. As you can imagine, we just have to repeat the same operations until we have the vocabulary at the desired size! Let's look at a few more steps to see the evolution of our vocabulary and the length of the splits getting shorter. Now that we are happy with our vocabulary, you are probably wondering how to use it to tokenize a text. Let's say we want to tokenize the word \""huggingface\"". WordPiece follows these rules: We will look for the longest possible token at the beginning of our word. Then we start again on the remaining part of our word. And so on until we reach the end! And that's it, huggingface is divided into 4 sub-tokens. ßThis video is about to end, I hope it helped you to understand better what is behind the word WordPiece!"", ""etc. |\n| 3 | Word number | |\n| 4 | Word | This is the token as segmented/tokenized in the Treebank. Initially the \\*\\_skel file contain the placeholder [WORD] which gets replaced by the actual token from the Treebank which is part of the OntoNotes release. |\n| 5 | Part-of-Speech | |\n| 6 | Parse bit | This is the bracketed structure broken before the first open parenthesis in the parse, and the word/part-of-speech leaf replaced with a \\*. The full parse can be created by substituting the asterix with the \""([pos] [word])\"" string (or leaf) and concatenating the items in the rows of that column. |\n| 7 | Predicate lemma | The predicate lemma is mentioned for the rows for which we have semantic role information. All other rows are marked with a \""-\"". |\n| 8 | Predicate Frameset ID | This is the PropBank frameset ID of the predicate in Column 7. |\n| 9 | Word sense | This is the word sense of the word in Column 3. |\n| 10 | Speaker/Author | This is the speaker or author name where available. Mostly in Broadcast Conversation and Web Log data. |\n| 11 | Named Entities | These columns identifies the spans representing various named entities."", ""GPT-2 doesn't actually have an unknown token (it's impossible to get an unknown character when using byte-level BPE), but this could happen here because we did not include all the possible bytes in the initial vocabulary. This aspect of BPE is beyond the scope of this section, so we've left the details out. That's it for the BPE algorithm! Next, we'll have a look at WordPiece."", ""This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |\n## Post-Processors\nAfter the whole pipeline, we sometimes want to insert some special\ntokens before feed a tokenized string into a model like \""[CLS] My\nhorse is amazing [SEP]\"". The `PostProcessor` is the component doing\njust that. | Name | Description | Example |\n| :--- | :--- | :--- |\n| TemplateProcessing | Let’s you easily template the post processing, adding special tokens, and specifying the `type\\_id` for each sequence/special token. The template is given two strings representing the single sequence and the pair of sequences, as well as a set of special tokens to use. | Example, when specifying a template with these values:  \n\n* single: `\""[CLS] $A [SEP]\""`\n* pair: `\""[CLS] $A [SEP] $B [SEP]\""`\n* special tokens:\n  + `\""[CLS]\""`\n  + `\""[SEP]\""`\n\n Input: `(\""I like this\"", \""but not this\"")`   \n Output: `\""[CLS] I like this [SEP] but not this [SEP]\""` |\n## Decoders\nThe Decoder knows how to go from the IDs used by the Tokenizer, back to\na readable piece of text. Some `Normalizer` and `PreTokenizer` use\nspecial characters or identifiers that need to be reverted for example. | Name | Description |\n| :--- | :--- |\n| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTokenizer encodes at the byte-level, using a set of visible Unicode characters to represent each byte, so we need a Decoder to revert this process and get something readable again. |\n| Metaspace | Reverts the Metaspace PreTokenizer. This PreTokenizer uses a special identifer `▁` to identify whitespaces, and so this Decoder helps with decoding these. |\n| WordPiece | Reverts the WordPiece Model. This model uses a special identifier `##` for continuing subwords, and so this Decoder helps with decoding these. |""]",The WordPiece Model uses the special identifier `##` for continuing subwords.,##
"What is the purpose of the 🧨 Diffusers tutorials?
","[""🧨 Diffusers Experimental\nWe are adding experimental code to support novel applications and usages of the Diffusers library."", ""Models\nFor more detail on the models, please refer to the [docs](https://huggingface.co/docs/diffusers/api/models/overview)."", ""Overview\nThese examples show how to run [Diffuser](https://arxiv.org/abs/2205.09991) in Diffusers. There are two ways to use the script, `run\\_diffuser\\_locomotion.py`. The key option is a change of the variable `n\\_guide\\_steps`. When `n\\_guide\\_steps=0`, the trajectories are sampled from the diffusion model, but not fine-tuned to maximize reward in the environment. By default, `n\\_guide\\_steps=2` to match the original implementation. You will need some RL specific requirements to run the examples:\n```\npip install -f https://download.pytorch.org/whl/torch\\_stable.html \\\nfree-mujoco-py \\\neinops \\\ngym==0.24.1 \\\nprotobuf==3.20.1 \\\ngit+https://github.com/rail-berkeley/d4rl.git \\\nmediapy \\\nPillow==9.0.0\n```"", ""For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.\\*\\*\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!"", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# 🧨 Diffusers’ Ethical Guidelines\n## Preamble\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training. Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users’ contributions, and usage of the Diffusers library. The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.""]","The purpose of the 🧨 Diffusers tutorials is to provide guidance on how to run and utilize the Diffusers library, which includes pre-trained diffusion models and serves as a modular toolbox for inference and training. The tutorials aim to help users understand the functionality of the library and explore novel applications and usages of diffusion models.",To provide a gentle introduction to diffusion models and help understand the library fundamentals.
"What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?
","[""Using Flagging\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\nTags: FLAGGING, DATA\n## Introduction\nWhen you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these \""hard\"" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust. Gradio simplifies the collection of this data by including a \\*\\*Flag\\*\\* button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`. ## The \\*\\*Flag\\*\\* button in `gradio.Interface`\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked \\*\\*Flag\\*\\*. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file. There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail. - `allow\\_flagging`: this parameter can be set to either `\""manual\""` (default), `\""auto\""`, or `\""never\""`. - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked. - `auto`: users will not see a button to flag, but every sample will be flagged automatically. - `never`: users will not see a button to flag, and no sample will be flagged. - `flagging\\_options`: this parameter can be either `None` (default) or a list of strings. - If `None`, then the user simply clicks on the \\*\\*Flag\\*\\* button and no additional options are shown. - If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\""Incorrect\"", \""Ambiguous\""]`, then buttons labeled \\*\\*Flag as Incorrect\\*\\* and \\*\\*Flag as Ambiguous\\*\\* appear. This only applies if `allow\\_flagging` is `\""manual\""`. - The chosen option is then logged along with the input and output. - `flagging\\_dir`: this parameter takes a string. - It represents what to name the directory where flagged data is stored. - `flagging\\_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n- Using this parameter allows you to write custom code that gets run when the flag button is clicked\n- By default, this is set to an instance of `gr.CSVLogger`\n- One example is setting it to an instance of `gr.HuggingFaceDatasetSaver` which can allow you to pipe any flagged data into a HuggingFace Dataset."", ""This change will improve performance for many applications. Thanks [@pngwn](https://github.com/pngwn)! - [#5216](https://github.com/gradio-app/gradio/pull/5216) [`4b58ea6d`](https://github.com/gradio-app/gradio/commit/4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8) - Update i18n tokens and locale files. Thanks [@hannahblair](https://github.com/hannahblair)! ## 0.1.1\n### Patch Changes\n- Updated dependencies [[`667875b2`](https://github.com/gradio-app/gradio/commit/667875b2441753e74d25bd9d3c8adedd8ede11cd)]:\n- @gradio/upload@0.0.3\n## 0.1.0\n### Features\n- [#4979](https://github.com/gradio-app/gradio/pull/4979) [`44ac8ad0`](https://github.com/gradio-app/gradio/commit/44ac8ad08d82ea12c503dde5c78f999eb0452de2) - Allow setting sketch color default. Thanks [@aliabid94](https://github.com/aliabid94)!"", ""- \\*\\*Files that you explicitly allow via the `allowed\\_paths` parameter in `launch()`\\*\\*. This parameter allows you to pass in a list of additional directories or exact filepaths you'd like to allow users to have access to. (By default, this parameter is an empty list). Gradio DOES NOT ALLOW access to:\n- \\*\\*Files that you explicitly block via the `blocked\\_paths` parameter in `launch()`\\*\\*. You can pass in a list of additional directories or exact filepaths to the `blocked\\_paths` parameter in `launch()`. This parameter takes precedence over the files that Gradio exposes by default or by the `allowed\\_paths`. - \\*\\*Any other paths on the host machine\\*\\*. Users should NOT be able to access other arbitrary paths on the host. Please make sure you are running the latest version of `gradio` for these security settings to apply."", ""- Check the [Docs](https://gradio.app/docs/) page to learn the details."", ""By default, the Gradio use UTF-8 encoding for scripts. \\*\\*For reload mode\\*\\*, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n1. Configure encoding declaration of python script, for example: `# -\\*- coding: cp1252 -\\*-`\n2. Confirm that your code editor has identified that encoding format. 3. Run like this: `gradio run.py --encoding cp1252`\n🔥 If your application accepts command line arguments, you can pass them in as well. Here's an example:\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# Greetings {args.name}!\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nWhich you could run like this: `gradio run.py --name Gretel`\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code.""]","The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `""manual""`. This means users will see a button to flag, and samples are only flagged when the button is clicked.","""manual"""
"Where can the full code for the Stable Diffusion demo be found?
","[""ote: This is a simplified version of the code needed to create the Stable Diffusion demo."", ""--\ntitle: 🧨 Stable Diffusion in JAX / Flax ! thumbnail: /blog/assets/108\\_stable\\_diffusion\\_jax/thumbnail.png\nauthors:\n- user: pcuenq\n- user: patrickvonplaten\n---\n# 🧨 Stable Diffusion in JAX / Flax ! [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_jax_how_to.ipynb)\n# \\*\\*Stable Diffusion in JAX / Flax\\*\\* 🚀\n🤗 Hugging Face [Diffusers](https://github.com/huggingface/diffusers) supports Flax since version `0.5.1`! This allows for super fast inference on Google TPUs, such as those available in Colab, Kaggle or Google Cloud Platform. This post shows how to run inference using JAX / Flax. If you want more details about how Stable Diffusion works or want to run it in GPU, please refer to [this Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_diffusion.ipynb). If you want to follow along, click the button above to open this post as a Colab notebook. First, make sure you are using a TPU backend. If you are running this notebook in Colab, select `Runtime` in the menu above, then select the option \""Change runtime type\"" and then select `TPU` under the `Hardware accelerator` setting. Note that JAX is not exclusive to TPUs, but it shines on that hardware because each TPU server has 8 TPU accelerators working in parallel. ## Setup\n``` python\nimport jax\nnum\\_devices = jax.device\\_count()\ndevice\\_type = jax.devices()[0].device\\_kind\nprint(f\""Found {num\\_devices} JAX devices of type {device\\_type}.\"")\nassert \""TPU\"" in device\\_type, \""Available device is not a TPU, please select TPU from Edit > Notebook settings > Hardware accelerator\""\n```\n\\*Output\\*:\n```bash\nFound 8 JAX devices of type TPU v2."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Safe Stable Diffusion\nSafe Stable Diffusion was proposed in [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://huggingface.co/papers/2211.05105) and mitigates inappropriate degeneration from Stable Diffusion models because they're trained on unfiltered web-crawled datasets. For instance Stable Diffusion may unexpectedly generate nudity, violence, images depicting self-harm, and otherwise offensive content. Safe Stable Diffusion is an extension of Stable Diffusion that drastically reduces this type of content. The abstract from the paper is:\n\\*Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications."", ""Hence we call it ControlNet-XS. We provide the code for controlling StableDiffusion-XL [Podell et al., 2023] (Model B, 48M Parameters) and StableDiffusion 2.1 [Rombach et al. 2022] (Model B, 14M Parameters), all under openrail license.\\*\nThis model was contributed by [UmerHA](https://twitter.com/UmerHAdil). ❤️\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines. ## StableDiffusionControlNetXSPipeline\n[[autodoc]] StableDiffusionControlNetXSPipeline\n- all\n- \\_\\_call\\_\\_\n## StableDiffusionPipelineOutput\n[[autodoc]] pipelines.stable\\_diffusion.StableDiffusionPipelineOutput"", ""--\ntitle: Using Stable Diffusion with Core ML on Apple Silicon\nthumbnail: /blog/assets/diffusers\\_coreml/thumbnail.png\nauthors:\n- user: pcuenq\n---\n# Using Stable Diffusion with Core ML on Apple Silicon\nThanks to Apple engineers, you can now run Stable Diffusion on Apple Silicon using Core ML! [This Apple repo](https://github.com/apple/ml-stable-diffusion) provides conversion scripts and inference code based on [🧨 Diffusers](https://github.com/huggingface/diffusers), and we love it! To make it as easy as possible for you, we converted the weights ourselves and put the Core ML versions of the models in [the Hugging Face Hub](https://hf.co/apple). \\*\\*Update\\*\\*: some weeks after this post was written we created a native Swift app that you can use to run Stable Diffusion effortlessly on your own hardware. We released [an app in the Mac App Store](https://apps.apple.com/app/diffusers/id1666309574) as well as [the source code to allow other projects to use it](https://github.com/huggingface/swift-coreml-diffusers). The rest of this post guides you on how to use the converted weights in your own code or convert additional weights yourself. ## Available Checkpoints\nThe official Stable Diffusion checkpoints are already converted and ready for use:\n- Stable Diffusion v1.4: [converted](https://hf.co/apple/coreml-stable-diffusion-v1-4) [original](https://hf.co/CompVis/stable-diffusion-v1-4)\n- Stable Diffusion v1.5: [converted](https://hf.co/apple/coreml-stable-diffusion-v1-5) [original](https://hf.co/runwayml/stable-diffusion-v1-5)\n- Stable Diffusion v2 base: [converted](https://hf.co/apple/coreml-stable-diffusion-2-base) [original](https://huggingface.co/stabilityai/stable-diffusion-2-base)\n- Stable Diffusion v2.1 base: [converted](https://hf.co/apple/coreml-stable-diffusion-2-1-base) [original](https://huggingface.co/stabilityai/stable-diffusion-2-1-base)\nCore ML supports all the compute units available in your device: CPU, GPU and Apple's Neural Engine (NE).""]",The full code for the Stable Diffusion demo can be found in the [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_jax_how_to.ipynb).,https://hf.co/spaces/stabilityai/stable-diffusion/tree/main
"What transformation does the FNet model use to replace the self-attention layer in a BERT model?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# FNet\n## Overview\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:\n\\*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \""mix\"" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \""efficient\"" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.\\*\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f\\_net). ## Usage tips\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with\nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum\nsequence length for fine-tuning and inference. ## Resources\n- [Text classification task guide](../tasks/sequence\\_classification)\n- [Token classification task guide](../tasks/token\\_classification)\n- [Question answering task guide](../tasks/question\\_answering)\n- [Masked language modeling task guide](../tasks/masked\\_language\\_modeling)\n- [Multiple choice task guide](../tasks/multiple\\_choice)\n## FNetConfig\n[[autodoc]] FNetConfig\n## FNetTokenizer\n[[autodoc]] FNetTokenizer\n- build\\_inputs\\_with\\_special\\_tokens\n- get\\_special\\_tokens\\_mask\n- create\\_token\\_type\\_ids\\_from\\_sequences\n- save\\_vocabulary\n## FNetTokenizerFast\n[[autodoc]] FNetTokenizerFast\n## FNetModel\n[[autodoc]] FNetModel\n- forward\n## FNetForPreTraining\n[[autodoc]] FNetForPreTraining\n- forward\n## FNetForMaskedLM\n[[autodoc]] FNetForMaskedLM\n- forward\n## FNetForNextSentencePrediction\n[[autodoc]] FNetForNextSentencePrediction\n- forward\n## FNetForSequenceClassification\n[[autodoc]] FNetForSequenceClassification\n- forward\n## FNetForMultipleChoice\n[[autodoc]] FNetForMultipleChoice\n- forward\n## FNetForTokenClassification\n[[autodoc]] FNetForTokenClassification\n- forward\n## FNetForQuestionAnswering\n[[autodoc]] FNetForQuestionAnswering\n- forward"", ""Second, it can be noted\nthat the values of the encoded output vector of\n\\\\(\\mathbf{l}\\_1 = \\text{\""Ich\""}\\\\) are the same when the last word is changed\nfrom \\\""ein\\\"" to \\\""das\\\"". This however should not come as a surprise if\none has understood uni-directional self-attention. On a final side-note, \\_auto-regressive\\_ models, such as GPT2, have the\nsame architecture as \\_transformer-based\\_ decoder models \\*\\*if\\*\\* one\nremoves the cross-attention layer because stand-alone auto-regressive\nmodels are not conditioned on any encoder outputs. So auto-regressive\nmodels are essentially the same as \\*auto-encoding\\* models but replace\nbi-directional attention with uni-directional attention. These models\ncan also be pre-trained on massive open-domain text data to show\nimpressive performances on natural language generation (NLG) tasks. In\n[Radford et al. (2019)](https://cdn.openai.com/better-language-models/language\\_models\\_are\\_unsupervised\\_multitask\\_learners.pdf),\nthe authors show that a pre-trained GPT2 model can achieve SOTA or close\nto SOTA results on a variety of NLG tasks without much fine-tuning."", ""It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n- Splitting the embedding matrix into two smaller matrices. - Using repeating layers split among groups. The abstract from the paper is the following:\n\\*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\nSQuAD benchmarks while having fewer parameters compared to BERT-large.\\*\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT). ## Usage tips\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\nthan the left. - ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\nsimilar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\nnumber of (repeating) layers. - Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters. - Layers are split in groups that share parameters (to save memory). Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A."", ""Thus, directly teaching the model about the English language (and the words we use). Isn’t that neat? Play around with BERT’s masking predictions:\n\nHosted inference API\n\n[Fill-Mask](/tasks/fill-mask \""Learn more about fill-mask\"")\n\nExamples\n\nMask token:\n`[MASK]`\n\nCompute\n\nThis model can be loaded on the Inference API on-demand. JSON Output\n\nMaximize\n\n\\*\\*Fun Fact:\\*\\* Masking has been around a long time - [1953 Paper on Cloze procedure (or ‘Masking’)](https://psycnet.apa.org/record/1955-00850-001). ### 2.3 What is Next Sentence Prediction? NSP (Next Sentence Prediction) is used to help BERT learn about relationships between sentences by predicting if a given sentence follows the previous sentence or not. \\*\\*Next Sentence Prediction Example:\\*\\*\n1. Paul went shopping. He bought a new shirt. (correct sentence pair)\n2. Ramona made coffee. Vanilla ice cream cones for sale. (incorrect sentence pair)\nIn training, 50% correct sentence pairs are mixed in with 50% random sentence pairs to help BERT increase next sentence prediction accuracy. \\*\\*Fun Fact:\\*\\* BERT is trained on both MLM (50%) and NSP (50%) at the same time. ### 2.4 Transformers\nThe Transformer architecture makes it possible to parallelize ML training extremely efficiently. Massive parallelization thus makes it feasible to train BERT on large amounts of data in a relatively short period of time. Transformers use an attention mechanism to observe relationships between words. A concept originally proposed in the popular [2017 Attention Is All You Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper sparked the use of Transformers in NLP models all around the world. >Since their introduction in 2017, Transformers have rapidly become the state-of-the-art approach to tackle tasks in many domains such as natural language processing, speech recognition, and computer vision. In short, if you’re doing deep learning, then you need Transformers! Lewis Tunstall, Hugging Face ML Engineer & [Author of Natural Language Processing with Transformers](https://www.amazon.com/Natural-Language-Processing-Transformers-Applications/dp/1098103246))\n\nTimeline of popular Transformer model releases:\n\n[Source](https://huggingface.co/course/chapter1/4)\n\n#### 2.4.1 How do Transformers work?"", ""!--Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# DeBERTa\n## Overview\nThe DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's\nBERT model released in 2018 and Facebook's RoBERTa model released in 2019. It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in\nRoBERTa. The abstract from the paper is the following:\n\\*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\nlanguage processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\ndisentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\ndisentangled attention mechanism, where each word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed using disentangled matrices on their\ncontents and relative positions.""]","The FNet model replaces the self-attention layer in a BERT model with a Fourier transform, which returns only the real parts of the transform.",Fourier transform
"What type of test should typically accompany a bug fix in Gradio's testing strategy?
","[""Test Strategy\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to. This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md). ## Objectives\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n1. Ensure that the Gradio library functions as we expect it to. 2. Enable the maintenance team to quickly identify both the presence and source of defects. 3. Prevent regressions, i.e. if we fix something it should stay fixed. 4. Improve the quality of the codebase in order to ease maintenance efforts. 5. Reduce the amount of manual testing required. ## Scope\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas. We should not focus on code coverage but on test coverage following the below criteria:\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n## Types of testing\nOur tests will broadly fall into one of three categories:\n- Static Quality checks\n- Dynamic 'Code' tests\n- Dynamic Functional tests\n### Static Quality checks\nStatic quality checks are generally very fast to run and do not require building the code base."", ""gradio\n## 4.11.0\n### Features\n- [#6842](https://github.com/gradio-app/gradio/pull/6842) [`846d52d`](https://github.com/gradio-app/gradio/commit/846d52d1c92d429077382ce494eea27fd062d9f6) - Fix md highlight. Thanks [@pngwn](https://github.com/pngwn)! - [#6831](https://github.com/gradio-app/gradio/pull/6831) [`f3abde8`](https://github.com/gradio-app/gradio/commit/f3abde80884d96ad69b825020c46486d9dd5cac5) - Add an option to enable header links for markdown. Thanks [@pngwn](https://github.com/pngwn)! - [#6814](https://github.com/gradio-app/gradio/pull/6814) [`828fb9e`](https://github.com/gradio-app/gradio/commit/828fb9e6ce15b6ea08318675a2361117596a1b5d) - Refactor queue so that there are separate queues for each concurrency id. Thanks [@aliabid94](https://github.com/aliabid94)! - [#6809](https://github.com/gradio-app/gradio/pull/6809) [`1401d99`](https://github.com/gradio-app/gradio/commit/1401d99ade46d87da75b5f5808a3354c49f1d1ea) - Fix `ImageEditor` interaction story. Thanks [@hannahblair](https://github.com/hannahblair)! - [#6803](https://github.com/gradio-app/gradio/pull/6803) [`77c9003`](https://github.com/gradio-app/gradio/commit/77c900311e2ba37b8f849ce088ceb48aa196af18) - Fixes issue 5781: Enables specifying a caching directory for Examples. Thanks [@cswamy](https://github.com/cswamy)! - [#6823](https://github.com/gradio-app/gradio/pull/6823) [`67a2b7f`](https://github.com/gradio-app/gradio/commit/67a2b7f12cb06355fcc41e40d47e8b2ad211d7d1) - Fixed duplicate word (\""this this\""). Thanks [@Cassini-chris](https://github.com/Cassini-chris)!"", ""Gradio Demo: queue\\_full\\_e2e\\_test\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nimport time\nimport random\nn\\_calls = 0\ndef get\\_random\\_number():\nglobal n\\_calls\nif n\\_calls == 1:\nn\\_calls += 1\nraise gr.Error(\""This is a gradio error\"")\nn\\_calls += 1\ntime.sleep(5)\nreturn random.randrange(1, 10)\nwith gr.Blocks() as demo:\nwith gr.Row():\nwith gr.Column():\nfirst = gr.Button(\""First Call\"")\nsecond = gr.Button(\""Second Call\"")\nthird = gr.Button(\""Third Call\"")\nfourth = gr.Button(\""Fourth Call\"")\nwith gr.Column():\nfirst\\_o = gr.Number(label=\""First Result\"")\nsecond\\_o = gr.Number(label=\""Second Result\"")\nthird\\_o = gr.Number(label=\""Third Result\"")\nfourth\\_o = gr.Number(label=\""Fourth Result\"")\nfirst.click(get\\_random\\_number, None, first\\_o, concurrency\\_id=\""f\"")\nsecond.click(get\\_random\\_number, None, second\\_o, concurrency\\_id=\""f\"")\nthird.click(get\\_random\\_number, None, third\\_o, concurrency\\_id=\""f\"")\nfourth.click(get\\_random\\_number, None, fourth\\_o, concurrency\\_id=\""f\"")\ndemo.queue(max\\_size=2)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```"", ""### Testing and Infrastructure Changes:\nNo changes to highlight. ### Breaking Changes:\nNo changes to highlight. ### Full Changelog:\nNo changes to highlight. ### Contributors Shoutout:\nNo changes to highlight. # 0.2.3\n### New Features:\nNo changes to highlight. ### Bug Fixes:\n- Fix example inputs for `gr.File(file\\_count='multiple')` output components by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4153](https://github.com/gradio-app/gradio/pull/4153)\n### Documentation Changes:\nNo changes to highlight. ### Testing and Infrastructure Changes:\nNo changes to highlight. ### Breaking Changes:\nNo changes to highlight. ### Full Changelog:\nNo changes to highlight. ### Contributors Shoutout:\nNo changes to highlight. # 0.2.2\n### New Features:\nNo changes to highlight. ### Bug Fixes:\n- Only send request to `/info` route if demo version is above `3.28.3` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4109](https://github.com/gradio-app/gradio/pull/4109)\n### Other Changes:\n- Fix bug in test from gradio 3.29.0 refactor by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4138](https://github.com/gradio-app/gradio/pull/4138)\n### Breaking Changes:\nNo changes to highlight. # 0.2.1\n### New Features:\nNo changes to highlight. ### Bug Fixes:\nRemoves extraneous `State` component info from the `Client.view\\_api()` method by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)\n### Documentation Changes:\nNo changes to highlight. ### Testing and Infrastructure Changes:\nSeparates flaky tests from non-flaky tests by [@abidlabs](https://github.com/freddyaboulton) in [PR 4107](https://github.com/gradio-app/gradio/pull/4107)\n### Breaking Changes:\nNo changes to highlight."", ""- [#5682](https://github.com/gradio-app/gradio/pull/5682) [`c57f1b75e`](https://github.com/gradio-app/gradio/commit/c57f1b75e272c76b0af4d6bd0c7f44743ff34f26) - Fix functional tests. Thanks [@abidlabs](https://github.com/abidlabs)! - [#5681](https://github.com/gradio-app/gradio/pull/5681) [`40de3d217`](https://github.com/gradio-app/gradio/commit/40de3d2178b61ebe424b6f6228f94c0c6f679bea) - add query parameters to the `gr.Request` object through the `query\\_params` attribute. Thanks [@DarhkVoyd](https://github.com/DarhkVoyd)! - [#5653](https://github.com/gradio-app/gradio/pull/5653) [`ea0e00b20`](https://github.com/gradio-app/gradio/commit/ea0e00b207b4b90a10e9d054c4202d4e705a29ba) - Prevent Clients from accessing API endpoints that set `api\\_name=False`. Thanks [@abidlabs](https://github.com/abidlabs)! - [#5639](https://github.com/gradio-app/gradio/pull/5639) [`e1874aff8`](https://github.com/gradio-app/gradio/commit/e1874aff814d13b23f3e59ef239cc13e18ad3fa7) - Add `gr.on` listener method. Thanks [@aliabid94](https://github.com/aliabid94)! - [#5652](https://github.com/gradio-app/gradio/pull/5652) [`2e25d4305`](https://github.com/gradio-app/gradio/commit/2e25d430582264945ae3316acd04c4453a25ce38) - Pause autoscrolling if a user scrolls up in a `gr.Textbox` and resume autoscrolling if they go all the way down.""]","In Gradio's testing strategy, bug fixes should typically be accompanied by dynamic 'Code' tests wherever it is reasonably possible.",Dynamic code test
"How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?
","[""Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of 🤗 Accelerate is:\n- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible. - to have the same functions work for any distributed setup, so only have to learn one API. ### How does it work? To see how the library works in practice, let's have a look at each line of code we need to add to a training loop. ```python\naccelerator = Accelerator()\n```\nOn top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script. ```python\nmodel, optim, data = accelerator.prepare(model, optim, data)\n```\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torch.nn.Module`), optimizers (`torch.optim.Optimizer`) and dataloaders (`torch.data.dataloader.DataLoader`). #### Model\nModel preparation include wrapping it in the proper container (for instance `DistributedDataParallel`) and putting it on the proper device. Like with a regular distributed training, you will need to unwrap your model for saving, or to access its specific methods, which can be done with `accelerator.unwrap\\_model(model)`. #### Optimizer\nThe optimizer is also wrapped in a special container that will perform the necessary operations in the step to make mixed precision work."", ""Alternatively, use 🤗 Accelerate to gain full control over the training loop. Find the 🤗 Accelerate example\n[further down in this guide](#using-accelerate). While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can\nresult in a more pronounced training slowdown. Consider the following example. Let's say, the `per\\_device\\_train\\_batch\\_size=4`\nwithout gradient accumulation hits the GPU's limit. If you would like to train with batches of size 64, do not set the\n`per\\_device\\_train\\_batch\\_size` to 1 and `gradient\\_accumulation\\_steps` to 64. Instead, keep `per\\_device\\_train\\_batch\\_size=4`\nand set `gradient\\_accumulation\\_steps=16`. This results in the same effective batch size while making better use of\nthe available GPU resources. For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\nand [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957). ## Gradient Checkpointing\nSome large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used."", ""For details and\nbenchmarks, please, see [TensorFloat-32(TF32) on Ampere devices](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices). The document includes\ninstructions on how to disable this automatic conversion if for some reason you prefer not to use it. With the 🤗 Trainer you can use `--tf32` to enable it, or disable it with `--tf32 0` or `--no\\_tf32`. By default the PyTorch default is used. ### Automatic Mixed Precision\nYou can use automatic mixed precision with either a pytorch-like AMP way or the apex-like way:\n### fp16\nTo configure pytorch AMP-like mode with fp16 (float16) set:\n```json\n{\n\""fp16\"": {\n\""enabled\"": \""auto\"",\n\""loss\\_scale\"": 0,\n\""loss\\_scale\\_window\"": 1000,\n\""initial\\_scale\\_power\"": 16,\n\""hysteresis\"": 2,\n\""min\\_loss\\_scale\"": 1\n}\n}\n```\nand the [`Trainer`] will automatically enable or disable it based on the value of\n`args.fp16\\_backend`. The rest of config values are up to you. This mode gets enabled when `--fp16 --fp16\\_backend amp` or `--fp16\\_full\\_eval` command line args are passed. You can also enable/disable this mode explicitly:\n```json\n{\n\""fp16\"": {\n\""enabled\"": true,\n\""loss\\_scale\"": 0,\n\""loss\\_scale\\_window\"": 1000,\n\""initial\\_scale\\_power\"": 16,\n\""hysteresis\"": 2,\n\""min\\_loss\\_scale\"": 1\n}\n}\n```\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSpeed\nconfiguration. Here is the [documentation](https://www.deepspeed.ai/docs/config-json/#fp16-training-options). ### bf16\nIf bf16 (bfloat16) is desired instead of fp16 then the following configuration section is to be used:\n```json\n{\n\""bf16\"": {\n\""enabled\"": \""auto\""\n}\n}\n```\nbf16 has the same dynamic range as fp32 and thus doesn't require loss scaling. This mode gets enabled when `--bf16` or `--bf16\\_full\\_eval` command line args are passed. You can also enable/disable this mode explicitly:\n```json\n{\n\""bf16\"": {\n\""enabled\"": true\n}\n}\n```\nAs of `deepspeed==0.6.0` the bf16 support is new and experimental. If you use [gradient accumulation](#gradient-accumulation) with bf16-enabled, you need to be aware that it'll accumulate gradients in bf16, which may not be what you want due to this format's low precision, as it may lead to a lossy accumulation. A work is being done to fix that and provide an option to use a higher precision `dtype` (fp16 or fp32). ### NCCL Collectives\nThere is the `dtype` of the training regime and there is a separate `dtype` that is used for communication collectives like various reduction and gathering/scattering operations. All gather/scatter ops are performed in the same `dtype` the data is in, so if you're using bf16 training regime it gets gathered in bf16 - gathering is a non-lossy operation."", ""It'll automatically configure your training setup based on your hardware and environment. Take a look at the 🤗 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more. Initialize an 🤗 Accelerate environment:\n```bash\naccelerate config\n```\nTo setup a default 🤗 Accelerate environment without choosing any configurations:\n```bash\naccelerate config default\n```\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n```bash\nfrom accelerate.utils import write\\_basic\\_config\nwrite\\_basic\\_config()\n```\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create\\_dataset) guide to learn how to create a dataset that works with the training script. The following sections highlight parts of the training script that are important for understanding how to modify it, but it doesn't cover every aspect of the script in detail. If you're interested in learning more, feel free to read through the [script](https://github.com/huggingface/diffusers/blob/main/examples/controlnet/train\\_controlnet.py) and let us know if you have any questions or concerns. ## Script parameters\nThe training script provides many parameters to help you customize your training run. All of the parameters and their descriptions are found in the [`parse\\_args()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/controlnet/train\\_controlnet.py#L231) function. This function provides default values for each parameter, such as the training batch size and learning rate, but you can also set your own values in the training command if you'd like. For example, to speedup training with mixed precision using the fp16 format, add the `--mixed\\_precision` parameter to the training command:\n```bash\naccelerate launch train\\_controlnet.py \\\n--mixed\\_precision=\""fp16\""\n```\nMany of the basic and important parameters are described in the [Text-to-image](text2image#script-parameters) training guide, so this guide just focuses on the relevant parameters for ControlNet:\n- `--max\\_train\\_samples`: the number of training samples; this can be lowered for faster training, but if you want to stream really large datasets, you'll need to include this parameter and the `--streaming` parameter in your training command\n- `--gradient\\_accumulation\\_steps`: number of update steps to accumulate before the backward pass; this allows you to train with a bigger batch size than your GPU memory can typically handle\n### Min-SNR weighting\nThe [Min-SNR](https://huggingface.co/papers/2303.09556) weighting strategy can help with training by rebalancing the loss to achieve faster convergence."", ""# Initialize accelerator and tensorboard logging\n... accelerator = Accelerator(\n... mixed\\_precision=config.mixed\\_precision,\n... gradient\\_accumulation\\_steps=config.gradient\\_accumulation\\_steps,\n... log\\_with=\""tensorboard\"",\n... project\\_dir=os.path.join(config.output\\_dir, \""logs\""),\n... )\n... if accelerator.is\\_main\\_process:\n... if config.output\\_dir is not None:\n... os.makedirs(config.output\\_dir, exist\\_ok=True)\n... if config.push\\_to\\_hub:\n... repo\\_id = create\\_repo(\n... repo\\_id=config.hub\\_model\\_id or Path(config.output\\_dir).name, exist\\_ok=True\n... ).repo\\_id\n... accelerator.init\\_trackers(\""train\\_example\"")\n... # Prepare everything\n... # There is no specific order to remember, you just need to unpack the\n... # objects in the same order you gave them to the prepare method. ... model, optimizer, train\\_dataloader, lr\\_scheduler = accelerator.prepare(\n... model, optimizer, train\\_dataloader, lr\\_scheduler\n... )\n... global\\_step = 0\n... # Now you train the model\n... for epoch in range(config.num\\_epochs):\n... progress\\_bar = tqdm(total=len(train\\_dataloader), disable=not accelerator.is\\_local\\_main\\_process)\n... progress\\_bar.set\\_description(f\""Epoch {epoch}\"")\n...""]","You can force mixed precision training when initializing the Accelerator in 🤗 Accelerate by passing the `fp16=True` argument to the `Accelerator()` initialization. Here is an example:

```python
accelerator = Accelerator(fp16=True)
```

This will enable mixed precision training for your training loop.",By passing `fp16=True` to the Accelerator init.
"What is the purpose of tokenizers in the NLP pipeline?
","[""p align=\""center\"">\n\n![](https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png)\n\n![Build](https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg)\n[![GitHub](https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue)](https://github.com/huggingface/tokenizers/blob/master/LICENSE)\n[![Doc](https://docs.rs/tokenizers/badge.svg)](https://docs.rs/tokenizers/)\n\nThe core of `tokenizers`, written in Rust. Provides an implementation of today's most used tokenizers, with a focus on performance and\nversatility. ## What is a Tokenizer\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`. The various steps of the pipeline are:\n1. The `Normalizer`: in charge of normalizing the text. Common examples of normalization are\nthe [unicode normalization standards](https://unicode.org/reports/tr15/#Norm\\_Forms), such as `NFD` or `NFKC`. More details about how to use the `Normalizers` are available on the\n[Hugging Face blog](https://huggingface.co/docs/tokenizers/components#normalizers)\n2. The `PreTokenizer`: in charge of creating initial words splits in the text. The most common way of\nsplitting text is simply on whitespace. 3. The `Model`: in charge of doing the actual tokenization. An example of a `Model` would be\n`BPE` or `WordPiece`."", ""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a PyTorch tensor."", ""These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. Two common types of token classification are:\n\\* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names. \\* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb). ```py\n>>> from transformers import pipeline\n>>> classifier = pipeline(task=\""ner\"")\n>>> preds = classifier(\""Hugging Face is a French company based in New York City.\"")\n>>> preds = [\n... {\n... \""entity\"": pred[\""entity\""],\n... \""score\"": round(pred[\""score\""], 4),\n... \""index\"": pred[\""index\""],\n... \""word\"": pred[\""word\""],\n... \""start\"": pred[\""start\""],\n... \""end\"": pred[\""end\""],\n..."", ""n these few videos, we'll take a look at the tokenizers. In Natural Language Processing, most of the data that we handle consists of raw text. However, machine learning models cannot read and understand text in its raw form they can only work with numbers. The tokenizer's objective will be to translate the text into numbers. There are several possible approaches to this conversion, and the objective is to find the most meaningful representation. We'll take a look at three distinct tokenization algorithms. We compare them one to one, so we recommend you look at the videos in the following order: Word-based, Character-based, and Subword-based."", ""And with that, you now know how to train your very own tokenizers!""]","The purpose of tokenizers in the NLP pipeline is to process raw text as input and output an `Encoding`. They translate the text into numbers that machine learning models can understand, as models can only work with numerical data. The tokenization process involves several steps, including normalizing the text, creating initial word splits, and performing the actual tokenization using a model. This allows the text to be converted into a meaningful representation that can be used for various NLP tasks.",To translate text into data that can be processed by the model.
"What is the purpose of the Safety Checker in the Diffusers library?
","[""🧨 Diffusers Experimental\nWe are adding experimental code to support novel applications and usages of the Diffusers library."", ""### Swap components in a pipeline\nYou can customize the default components of any pipeline with another compatible component. Customization is important because:\n- Changing the scheduler is important for exploring the trade-off between generation speed and quality. - Different components of a model are typically trained independently and you can swap out a component with a better-performing one. - During finetuning, usually only some components - like the UNet or text encoder - are trained. To find out which schedulers are compatible for customization, you can use the `compatibles` method:\n```py\nfrom diffusers import DiffusionPipeline\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\nstable\\_diffusion = DiffusionPipeline.from\\_pretrained(repo\\_id, use\\_safetensors=True)\nstable\\_diffusion.scheduler.compatibles\n```\nLet's use the [`SchedulerMixin.from\\_pretrained`] method to replace the default [`PNDMScheduler`] with a more performant scheduler, [`EulerDiscreteScheduler`]. The `subfolder=\""scheduler\""` argument is required to load the scheduler configuration from the correct [subfolder](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main/scheduler) of the pipeline repository. Then you can pass the new [`EulerDiscreteScheduler`] instance to the `scheduler` argument in [`DiffusionPipeline`]:\n```python\nfrom diffusers import DiffusionPipeline, EulerDiscreteScheduler\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\nscheduler = EulerDiscreteScheduler.from\\_pretrained(repo\\_id, subfolder=\""scheduler\"")\nstable\\_diffusion = DiffusionPipeline.from\\_pretrained(repo\\_id, scheduler=scheduler, use\\_safetensors=True)\n```\n### Safety checker\nDiffusion models like Stable Diffusion can generate harmful content, which is why 🧨 Diffusers has a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\_diffusion/safety\\_checker.py) to check generated outputs against known hardcoded NSFW content. If you'd like to disable the safety checker for whatever reason, pass `None` to the `safety\\_checker` argument:\n```python\nfrom diffusers import DiffusionPipeline\nrepo\\_id = \""runwayml/stable-diffusion-v1-5\""\nstable\\_diffusion = DiffusionPipeline.from\\_pretrained(repo\\_id, safety\\_checker=None, use\\_safetensors=True)\n\""\""\""\nYou have disabled the safety checker for  by passing `safety\\_checker=None`."", ""Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us. - [\\*\\*Community tab\\*\\*](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project. - \\*\\*Bias exploration and evaluation\\*\\*: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations. - \\*\\*Encouraging safety in deployment\\*\\*\n- [\\*\\*Safe Stable Diffusion\\*\\*](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable\\_diffusion/stable\\_diffusion\\_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105). - [\\*\\*Safety Checker\\*\\*](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable\\_diffusion/safety\\_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated."", ""\\*\\*Please consider the following guidelines when opening a new issue\\*\\*:\n- Make sure you have searched whether your issue has already been asked before (use the search bar on GitHub under Issues). - Please never report a new issue on another (related) issue. If another issue is highly related, please\nopen a new issue nevertheless and link to the related issue. - Make sure your issue is written in English. Please use one of the great, free online translation services, such as [DeepL](https://www.deepl.com/translator) to translate from your native language to English if you are not comfortable in English. - Check whether your issue might be solved by updating to the newest Diffusers version. Before posting your issue, please make sure that `python -c \""import diffusers; print(diffusers.\\_\\_version\\_\\_)\""` is higher or matches the latest Diffusers version. - Remember that the more effort you put into opening a new issue, the higher the quality of your answer will be and the better the overall quality of the Diffusers issues."", ""For any questions or comments, please contact Isamu Isozaki(isamu-isozaki) on github with any questions.\\*\\*\nThe aim of this project is to provide retrieval augmented diffusion models to diffusers!""]","The Safety Checker in the Diffusers library is designed to check generated outputs against known hardcoded NSFW content to prevent the generation of harmful content. It helps ensure safety in the deployment of diffusion models like Stable Diffusion, which can produce inappropriate outputs due to being trained on unfiltered datasets. If needed, users can disable the safety checker by passing `None` to the `safety_checker` argument when initializing the pipeline.",The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.
"What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?
","[""!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Interact with Discussions and Pull Requests\nThe `huggingface\\_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood. ## Retrieve Discussions and Pull Requests from the Hub\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(repo\\_id=\""bigscience/bloom\""):\n... print(f\""{discussion.num} - {discussion.title}, pr: {discussion.is\\_pull\\_request}\"")\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer\\_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]\n```\n`HfApi.get\\_repo\\_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n```python\n>>> from huggingface\\_hub import get\\_repo\\_discussions\n>>> for discussion in get\\_repo\\_discussions(\n..."", ""-->\n# Git 与 HTTP 范式\n`huggingface\\_hub`库是用于与Hugging Face Hub进行交互的库，Hugging Face Hub是一组基于Git的存储库（模型、数据集或Spaces）。使用 `huggingface\\_hub`有两种主要方式来访问Hub。\n第一种方法，即所谓的“基于git”的方法，由[`Repository`]类驱动。这种方法使用了一个包装器，它在 `git`命令的基础上增加了专门与Hub交互的额外函数。第二种选择，称为“基于HTTP”的方法，涉及使用[`HfApi`]客户端进行HTTP请求。让我们来看一看每种方法的优缺点。\n## 存储库：基于历史的 Git 方法\n最初，`huggingface\\_hub`主要围绕 [`Repository`] 类构建。它为常见的 `git` 命令（如 `\""git add\""`、`\""git commit\""`、`\""git push\""`、`\""git tag\""`、`\""git checkout\""` 等）提供了 Python 包装器\n该库还可以帮助设置凭据和跟踪大型文件，这些文件通常在机器学习存储库中使用。此外，该库允许您在后台执行其方法，使其在训练期间上传数据很有用。\n使用 [`Repository`] 的最大优点是它允许你在本地机器上维护整个存储库的本地副本。这也可能是一个缺点，因为它需要你不断更新和维护这个本地副本。这类似于传统软件开发中，每个开发人员都维护自己的本地副本，并在开发功能时推送更改。但是，在机器学习的上下文中，这可能并不总是必要的，因为用户可能只需要下载推理所需的权重，或将权重从一种格式转换为另一种格式，而无需克隆整个存储库。\n## HfApi: 一个功能强大且方便的HTTP客户端\n`HfApi` 被开发为本地 git 存储库的替代方案，因为本地 git 存储库在处理大型模型或数据集时可能会很麻烦。`HfApi` 提供与基于 git 的方法相同的功能，例如下载和推送文件以及创建分支和标签，但无需本地文件夹来保持同步。\n`HfApi`除了提供 `git` 已经提供的功能外，还提供其他功能，例如：\n\\* 管理存储库\n\\* 使用缓存下载文件以进行有效的重复使用\n\\* 在 Hub 中搜索存储库和元数据\n\\* 访问社区功能，如讨论、PR和评论\n\\* 配置Spaces\n## 我应该使用什么？以及何时使用？\n总的来说，在大多数情况下，`HTTP 方法`是使用 huggingface\\_hub 的推荐方法。但是，在以下几种情况下，维护本地 git 克隆（使用 `Repository`）可能更有益：\n如果您在本地机器上训练模型，使用传统的 git 工作流程并定期推送更新可能更有效。`Repository` 被优化为此类情况，因为它能够在后台运行。\n如果您需要手动编辑大型文件，`git `是最佳选择，因为它只会将文件的差异发送到服务器。使用 `HfAPI` 客户端，每次编辑都会上传整个文件。请记住，大多数大型文件是二进制文件，因此无法从 git 差异中受益。\n并非所有 git 命令都通过 [`HfApi`] 提供。有些可能永远不会被实现，但我们一直在努力改进并缩小差距。如果您没有看到您的用例被覆盖。\n请在[Github](https://github.com/huggingface/huggingface\\_hub)打开一个 issue！我们欢迎反馈，以帮助我们与我们的用户一起构建 🤗 生态系统。"", ""This library allows users to interact with the Hub from Python. 1. Begin by installing the library:\n```bash\npip install huggingface\\_hub\n```\n2. To upload a dataset on the Hub in Python, you need to log in to your Hugging Face account:\n```bash\nhuggingface-cli login\n```\n3. Use the [`push\\_to\\_hub()`](https://huggingface.co/docs/datasets/main/en/package\\_reference/main\\_classes#datasets.DatasetDict.push\\_to\\_hub) function to help you add, commit, and push a file to your repository:\n```py\n>>> from datasets import load\\_dataset\n>>> dataset = load\\_dataset(\""stevhliu/demo\"")\n# dataset = dataset.map(...) # do all your processing here\n>>> dataset.push\\_to\\_hub(\""stevhliu/processed\\_demo\"")\n```\nTo set your dataset as private, set the `private` parameter to `True`. This parameter will only work if you are creating a repository for the first time. ```py\n>>> dataset.push\\_to\\_hub(\""stevhliu/private\\_processed\\_demo\"", private=True)\n```\nTo add a new configuration (or subset) to a dataset or to add a new split (train/validation/test), please refer to the [`Dataset.push\\_to\\_hub`] documentation."", ""Click on 'Pull request' to send your changes to the project maintainers for review. 11. It's ok if maintainers ask you for changes. It happens all the time to core contributors\ntoo! So everyone can see the changes in the Pull request, work in your local\nbranch and push the changes to your fork. They will automatically appear in\nthe pull request. 12. Once your changes have been approved, one of the project maintainers will\nmerge your pull request for you. Good job! ### Checklist\n1. The title of your pull request should be a summary of its contribution;\n2. If your pull request addresses an issue, please mention the issue number in\nthe pull request description to make sure they are linked (and people\nconsulting the issue know you are working on it);\n3. To indicate a work in progress please prefix the title with `[WIP]`, or mark\nthe PR as a draft PR. These are useful to avoid duplicated work, and to differentiate\nit from PRs ready to be merged;\n4. Make sure existing tests pass;\n5. Add high-coverage tests. No quality testing = no merge. 6. Due to the rapidly growing repository, it is important to make sure that no files that would significantly weigh down the repository are added. This includes images, videos and other non-text files. We prefer to leverage a hf.co hosted `dataset` like\nthe ones hosted on [`hf-internal-testing`](https://huggingface.co/hf-internal-testing) in which to place these files and reference\nthem by URL. We recommend putting them in the following dataset: [huggingface/documentation-images](https://huggingface.co/datasets/huggingface/documentation-images). If an external contribution, feel free to add the images to your PR and ask a Hugging Face member to migrate your images\nto this dataset. ### Tests\nAn extensive test suite is included to test the library behavior and several examples. Library tests can be found in\nthe [tests folder](https://github.com/huggingface/huggingface\\_hub/tree/main/tests). We use `pytest` in order to run the tests for the library. From the root of the repository they can be run with the following:\n```bash\n$ python -m pytest ./tests\n```\nYou can specify a smaller set of tests in order to test only the feature you're working on. For example, the following will only run the tests in the `test\\_repository.py` file:\n```bash\n$ python -m pytest ./tests/test\\_repository.py\n```\nAnd the following will only run the tests that include `tag` in their name:\n```bash\n$ python -m pytest ./tests -k tag\n```\n#### A corner case: testing Spaces\nFully testing Spaces is not possible on staging."", ""Hugging Face Hub documentation\nThe Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. The Hub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine Learning. Are you ready to join the path towards open source Machine Learning? 🤗\n\nRepositories\n\n[Introduction](./repositories)\n[Getting Started](./repositories-getting-started)\n[Repository Settings](./repositories-settings)\n[Pull requests and Discussions](./repositories-pull-requests-discussions)\n[Notifications](./notifications)\n[Collections](./collections)\n[Webhooks](./webhooks)\n[Next Steps](./repositories-next-steps)\n[Licenses](./repositories-licenses)\n\nModels\n\n[Introduction](./models)\n[The Model Hub](./models-the-hub)\n[Model Cards](./model-cards)\n[Gated Models](./models-gated)\n[Uploading Models](./models-uploading)\n[Downloading Models](./models-downloading)\n[Libraries](./models-libraries)\n[Tasks](./models-tasks)\n[Widgets](./models-widgets)\n[Inference API](./models-inference)\n[Download Stats](./models-download-stats)\n\nDatasets\n\n[Introduction](./datasets)\n[Datasets Overview](./datasets-overview)\n[Dataset Cards](./datasets-cards)\n[Gated Datasets](./datasets-gated)\n[Uploading Datasets](./datasets-adding)\n[Downloading Datasets](./datasets-downloading)\n[Libraries](./datasets-libraries)\n[Dataset Viewer](./datasets-viewer)\n[Download Stats](./datasets-download-stats)\n[Data files Configuration](./datasets-data-files-configuration)\n\nSpaces\n\n[Introduction](./spaces)\n[Spaces Overview](./spaces-overview)\n[Gradio Spaces](./spaces-sdks-gradio)\n[Streamlit Spaces](./spaces-sdks-streamlit)\n[Static HTML Spaces](./spaces-sdks-static)\n[Docker Spaces](./spaces-sdks-docker)\n[Embed your Space](./spaces-embed)\n[Run with Docker](./spaces-run-with-docker)\n[Reference](./spaces-config-reference)\n[Changelog](./spaces-changelog)\n[Advanced Topics](./spaces-advanced)\n[Sign in with HF](./spaces-oauth)\n\nOther\n\n[Organizations](./organizations)\n[Enterprise Hub](./enterprise-hub)\n[Billing](./billing)\n[Security](./security)\n[Moderation](./moderation)\n[Paper Pages](./paper-pages)\n[Search](./search)\n[Digital Object Identifier (DOI)](./doi)\n[Hub API Endpoints](./api)\n[Sign in with HF](./oauth)\n[Contributor Code of Conduct](https://huggingface.co/code-of-conduct)\n[Content Guidelines](https://huggingface.co/content-guidelines)\n\n## What's the Hugging Face Hub?""]",The `HfApi` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.,HfApi
"What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?
","[""Integrate your library with the Hub\nThe Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones already integrated include [spaCy](https://spacy.io/usage/projects#huggingface\\_hub), [AllenNLP](https://allennlp.org/), and [timm](https://rwightman.github.io/pytorch-image-models/), among many others. Integration means users can download and upload files to the Hub directly from your library. We hope you will integrate your library and join us in democratizing artificial intelligence for everyone. Integrating the Hub with your library provides many benefits, including:\n- Free model hosting for you and your users. - Built-in file versioning - even for huge files - made possible by [Git-LFS](https://git-lfs.github.com/). - All public models are powered by the [Inference API](https://huggingface.co/docs/api-inference/index). - In-browser widgets allow users to interact with your hosted models directly. This tutorial will help you integrate the Hub into your library so your users can benefit from all the features offered by the Hub. Before you begin, we recommend you create a [Hugging Face account](https://huggingface.co/join) from which you can manage your repositories and files. If you need help with the integration, feel free to open an [issue](https://github.com/huggingface/huggingface\\_hub/issues/new/choose), and we would be more than happy to help you. ## Installation\n1. Install the `huggingface\\_hub` library with pip in your environment:\n```bash\npython -m pip install huggingface\\_hub\n```\n2. Once you have successfully installed the `huggingface\\_hub` library, log in to your Hugging Face account:\n```bash\nhuggingface-cli login\n```\n```bash\n\\_| \\_| \\_| \\_| \\_|\\_|\\_| \\_|\\_|\\_| \\_|\\_|\\_| \\_| \\_| \\_|\\_|\\_| \\_|\\_|\\_|\\_| \\_|\\_| \\_|\\_|\\_| \\_|\\_|\\_|\\_|\n\\_| \\_| \\_| \\_| \\_| \\_| \\_| \\_|\\_| \\_| \\_| \\_| \\_| \\_| \\_| \\_|\n\\_|\\_|\\_|\\_| \\_| \\_| \\_| \\_|\\_| \\_| \\_|\\_| \\_| \\_| \\_| \\_| \\_| \\_|\\_| \\_|\\_|\\_| \\_|\\_|\\_|\\_| \\_| \\_|\\_|\\_|\n\\_| \\_| \\_| \\_| \\_| \\_| \\_| \\_| \\_| \\_| \\_|\\_| \\_| \\_| \\_| \\_| \\_| \\_| \\_|\n\\_| \\_| \\_|\\_| \\_|\\_|\\_| \\_|\\_|\\_| \\_|\\_|\\_| \\_| \\_| \\_|\\_|\\_| \\_| \\_| \\_| \\_|\\_|\\_| \\_|\\_|\\_|\\_|\nUsername:\nPassword:\n```\n3. Alternatively, if you prefer working from a Jupyter or Colaboratory notebook, login with `notebook\\_login`:\n```python\n>>> from huggingface\\_hub import notebook\\_login\n>>> notebook\\_login()\n```\n`notebook\\_login` will launch a widget in your notebook from which you can enter your Hugging Face credentials."", ""Data is hosted on HuggingFace Datasets."", ""--\ntitle: \""Introducing Skops\""\nthumbnail: /blog/assets/94\\_skops/introducing\\_skops.png\nauthors:\n- user: merve\n- user: adrin\n- user: BenjaminB\n---\n# Introducing Skops\n## Introducing Skops\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others. Let's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production. ```python\n# let's import the libraries first\nimport sklearn\nfrom sklearn.datasets import load\\_breast\\_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model\\_selection import train\\_test\\_split\n# Load the data and split\nX, y = load\\_breast\\_cancer(as\\_frame=True, return\\_X\\_y=True)\nX\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(\nX, y, test\\_size=0.3, random\\_state=42\n)\n# Train the model\nmodel = DecisionTreeClassifier().fit(X\\_train, y\\_train)\n```\nYou can use any model filename and serialization method, like `pickle` or `joblib`."", ""--\ntitle: \""Jupyter X Hugging Face\""\nthumbnail: /blog/assets/135\\_notebooks-hub/before\\_after\\_notebook\\_rendering.png\nauthors:\n- user: davanstrien\n- user: reach-vb\n- user: merve\n---\n# Jupyter X Hugging Face\n\\*\\*We’re excited to announce improved support for Jupyter notebooks hosted on the Hugging Face Hub!\\*\\*\nFrom serving as an essential learning resource to being a key tool used for model development, Jupyter notebooks have become a key component across many areas of machine learning. Notebooks' interactive and visual nature lets you get feedback quickly as you develop models, datasets, and demos. For many, their first exposure to training machine learning models is via a Jupyter notebook, and many practitioners use notebooks as a critical tool for developing and communicating their work. Hugging Face is a collaborative Machine Learning platform in which the community has shared over 150,000 models, 25,000 datasets, and 30,000 ML apps. The Hub has model and dataset versioning tools, including model cards and client-side libraries to automate the versioning process. However, only including a model card with hyperparameters is not enough to provide the best reproducibility; this is where notebooks can help. Alongside these models, datasets, and demos, the Hub hosts over 7,000 notebooks. These notebooks often document the development process of a model or a dataset and can provide guidance and tutorials showing how others can use these resources. We’re therefore excited about our improved support for notebook hosting on the Hub."", ""Using AllenNLP at Hugging Face\n`allennlp` is a NLP library for developing state-of-the-art models on different linguistic tasks. It provides high-level abstractions and APIs for common components and models in modern NLP. It also provides an extensible framework that makes it easy to run and manage NLP experiments. ## Exploring allennlp in the Hub\nYou can find `allennlp` models on the Hub by filtering at the left of the [models page](https://huggingface.co/models?library=allennlp). All models on the Hub come up with useful features\n1. A training metrics tab with automatically hosted TensorBoard traces. 2. Metadata tags that help for discoverability. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-allennlp_widget.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-allennlp_widget-dark.png)\n\n## Using existing models\nYou can use the `Predictor` class to load existing models on the Hub. To achieve this, use the `from\\_path` method and use the `\""hf://\""` prefix with the repository id. Here is an end-to-end example. ```py\nimport allennlp\\_models\nfrom allennlp.predictors.predictor import Predictor\npredictor = Predictor.from\\_path(\""hf://allenai/bidaf-elmo\"")\npredictor\\_input = {\n\""passage\"": \""My name is Wolfgang and I live in Berlin\"",\n\""question\"": \""Where do I live?\""\n}\npredictions = predictor.predict\\_json(predictor\\_input)\n```\nTo get a snippet such as this, you can click `Use in AllenNLP` at the top right,\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-allennlp_snippet.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-allennlp_snippet-dark.png)\n\n## Sharing your models\nThe first step is to save the model locally.""]",The name of the new library introduced by Hugging Face for hosting scikit-learn models is Skops.,Skops
"What is the purpose of Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""Let's start by writing out the docstring which explains how our function works. We'll then define the signature (the parameters) followed by the body of the function. reverse\\_string.\\_\\_doc\\_\\_ = \\\""Reverses any given string.\\\""\nreverse\\_string.\\_\\_name\\_\\_ ='reverse\\_string'\nreverse\\_string.\\_\\_signature\\_\\_ = inspect.Signature() #<|end|>\n<|user|>\nHere is my attempt:\n<|end|>\n<|assistant|>\nOkay great! Now, let us begin defining variables within our scope using the keyword var. Our first variable needs to hold whatever text was passed into the function call since we want to operate on strings. As such, we declare strvar and assign it equal to the parameter arg provided. strvar := arg\nThen, after declaring our second variable, we initialize it similarly but instead pass in the result of calling strvar's built-in.reversed() method alongside the index operator []. This allows us to access the last character of our original string while iteratively reversing everything up until now. Note that we don't actually need to store anything inside this new variable because it simply holds the same data as strvar without modifying them\""\n```\nGPT4 Evaluation:\n```\n4 7\nAssistant 1's code was not well-structured, and it was difficult to follow the logic."", ""These transformations can also be undetectable with a simple \""print\"". Indeed, keep in mind that for a computer, text is only a succession of 0 and 1 and it happens that different successions of 0 and 1 render the same printed character. The 0s and 1s go in groups of 8 to form a byte. The computer must then decode this sequence of bytes into a sequence of \""code points\"". In our example the 2 bytes are transformed into a single \""code point\"" by UTF-8. The unicode standard then allows us to find the character corresponding to this code point: the c cedilla. Let's repeat the same operation with this new sequence composed of 3 bytes, this time it is transformed into 2 \""code points\"" .... which also correspond to the c cedilla character! It is in fact the composition of the unicode Latin Small Letter Cand the combining cedilla. But it's annoying because what appears to us to be a single character is not at all the same thing for the computer. Fortunately, there are unicode standardization standards known as NFC, NFD, NFKC and NFKD that allow erasing some of these differences. These standards are often used by tokenizers! On all these previous examples, even if the normalizations changed the look of the text, they did not change the content: you could still read \""Hello world, let's normalize this sentence\"". However, you must be aware that some normalizations can be very harmful if they are not adapted to their corpus. For example, if you take the French sentence \""un père indigné\"", which means \""An indignant father\"", and normalize it with the bert-base-uncase tokenizer which removes the accents then the sentence becomes \""un père indigne\"" which means \""An unworthy father\"". If you watch this video to build your own tokenizer, there are no absolute rules to choose or not a normalization for your brand new tokenizer but I advise you to take the time to select them so that they do not make you lose important information."", ""his text generation demo takes in input text and returns generated text."", ""This allows code generation to create styles on the go without having to generate a stylesheet at the beginning or the end of the page (which would make the page feel stuck). To improve results, we can also guide the model by showing the way (``). ```javascript\nconst inputs = `# Task\nGenerate ${req.query.prompt}\n# Orders\nYou must use TailwindCSS utility classes (Tailwind is already injected in the page). Write application logic inside a JS  tag. Use a central layout to wrap everything in a\n\n`\n```\n### Preventing hallucination\nIt can be difficult to reliably prevent hallucinations and failures (such as parroting back the whole instructions, or writing “lorem ipsum” placeholder text) on light models dedicated to code generation, compared to larger general-purpose models, but we can try to mitigate it. You can try to use an imperative tone and repeat the instructions. An efficient way can also be to show the way by giving a part of the output in English:\n```javascript\nconst inputs = `# Task\nGenerate ${req.query.prompt}\n# Orders\nNever repeat these instructions, instead write the final code! You must use TailwindCSS utility classes (Tailwind is already injected in the page)!""]",The purpose of Textual Inversion is to personalize text-to-image models like stable diffusion using just 3-5 examples of your own images. It allows users to adapt the model to better fit their specific needs and preferences by fine-tuning it on a small set of examples.,Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.
"What is the recommended multiple of batch size for fp16 data type on an A100 GPU?
","[""For instance, for fp16 data type a multiple of 8 is recommended, unless\nit's an A100 GPU, in which case use multiples of 64. For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). This is where tiling happens and the right multiplier can have a significant speedup. ## Gradient Accumulation\nThe \\*\\*gradient accumulation\\*\\* method aims to calculate gradients in smaller increments instead of computing them for the\nentire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward\nand backward passes through the model and accumulating the gradients during the process. Once a sufficient number of\ngradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it\nbecomes possible to increase the \\*\\*effective batch size\\*\\* beyond the limitations imposed by the GPU's memory capacity. However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can\nslow down the training process. You can enable gradient accumulation by adding the `gradient\\_accumulation\\_steps` argument to [`TrainingArguments`]:\n```py\ntraining\\_args = TrainingArguments(per\\_device\\_train\\_batch\\_size=1, gradient\\_accumulation\\_steps=4, \\*\\*default\\_args)\n```\nIn the above example, your effective batch size becomes 4."", ""It was run on a single NVIDIA A100-SXM4-80GB GPU with a prompt length of 512. The model we used was `meta-llama/Llama-2-13b-hf`. with batch size = 1:\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None |None|None |None |26.0 |36.958 |27.058 |29152.98 |\n|gptq |False |4 |128 |exllama|36.2 |33.711 |29.663 |10484.34 |\n|bitsandbytes|None |4|None |None |37.64 |52.00 |19.23 |11018.36 |\nwith batch size = 16:\n|quantization |act\\_order|bits|group\\_size|kernel|Load time (s)|Per-token latency (ms)|Throughput (tok/s)|Peak memory (MB)|\n|-----|---------|----|----------|------|-------------|----------------------|------------------|----------------|\n|fp16|None |None|None |None |26.0 |69.94 |228.76 |53986.51 |\n|gptq |False |4 |128 |exllama|36.2 |95.41 |167.68 |34777.04 |\n|bitsandbytes|None |4|None |None |37.64 |113.98 |140.38 |35532.37 |\nFrom the benchmark, we can see that bitsandbyes and GPTQ are equivalent, with GPTQ being slightly faster for large batch size."", ""The impact of `fp16` on latency is less marked with `batch\\_size = 1`, but here it is of enormous interest as it can reduce latency by almost half, and almost double throughput! # Concluding remarks\nThis blog post showcased a few simple optimization tricks bundled in the 🤗 ecosystem. Using anyone of these techniques, or a combination of all three, can greatly improve Bark inference speed and memory footprint. \\* You can use the large version of Bark without any performance degradation and a footprint of just 2GB instead of 5GB, 15% faster, \\*\\*using 🤗 Better Transformer and CPU offload\\*\\*. \\* Do you prefer high throughput? \\*\\*Batch by 8 with 🤗 Better Transformer and half-precision\\*\\*. \\* You can get the best of both worlds by using \\*\\*fp16, 🤗 Better Transformer and CPU offload\\*\\*!"", ""Tried to allocate 40.00 MiB (GPU 0; 23.65 GiB total capacity; 22.27 GiB already allocated; 20.31 MiB free; 22.76 GiB reserved in total by PyTorch) |\n| DDP | 1 | 2 | NA | OOM Error RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.65 GiB total capacity; 22.27 GiB already allocated; 20.31 MiB free; 22.76 GiB reserved in total by PyTorch) |\n| DDP + FP16 | 1 | 1 | NA | OOM Error RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.65 GiB total capacity; 22.27 GiB already allocated; 20.31 MiB free; 22.76 GiB reserved in total by PyTorch) |\n| FSDP with min\\_num\\_params = 2K | 5 | 2 | 0.6 | |\n| FSDP with min\\_num\\_params = 2K + Offload to CPU | 10 | 1 | 3 | |\n| FSDP with min\\_num\\_params = 2K + Offload to CPU | 14 | 2 | 1.16 | |\nTable 2: Benchmarking FSDP on GPT-2 XL (1.5B) model\nFrom Table 2, we can observe that DDP (w and w/o fp16) isn’t even able to run with batch size of 1 and results in CUDA OOM error. FSDP with Zero-Stage 3 is able to be run on 2 GPUs with batch size of 5 (effective batch size =10 (5 X 2)). FSDP with CPU offload can further increase the max batch size to 14 per GPU when using 2 GPUs. \\*\\*FSDP with CPU offload enables training GPT-2 1.5B model on a single GPU with a batch size of 10\\*\\*. This enables ML practitioners with minimal compute resources to train such large models, thereby democratizing large model training. ## Capabilities and limitations of the FSDP Integration\nLet’s dive into the current support that Accelerate provides for FSDP integration and the known limitations. \\*\\*Required PyTorch version for FSDP support\\*\\*: PyTorch Nightly (or 1.12.0 if you read this after it has been released) as the model saving with FSDP activated is only available with recent fixes."", ""It's extremely important to avoid the hug of death. On this benchmark the initial performance was (on 16xA100 40Go on GCP which is the machine used throughout):\nRequests/s : 0.3 (throughput)\nLatency: 350ms/token (latency)\nThose numbers are not that great. Before getting to work let's estimate\nthe best we can imagine achieving. The formula for amount of operations is `24Bsh^2 + 4𝐵s^2h24Bsh^2 + 4𝐵s^2h` where `B` is\nthe batch size, `s` the sequence length, and `h` the hidden dimension. Let's do the math and we are getting `17 TFlop` for a single forward pass. Looking at the [specs](https://www.nvidia.com/en-us/data-center/a100/) of A100 it claims `312 TFLOPS` for a single card. That means a single GPU could potentially run at `17 / 312 = 54ms/token`. We're using 16 of those so `3ms/token` on the overall\nmachine. Take all these numbers with a big grain of salt, it's never possible to reach those numbers,\nand real-life performance rarely matches the specs. Also if computation is not your limiting\nfactor then this is not the lowest you can get. It's just good practice to know how far you are from\nyour target. In this case, we're 2 orders of magnitude so pretty far. Also, this estimate puts\nall the flops at the service of latency which means only a single request can go at a time (it's ok since you're maximizing your machine\nso there's not much else to be done, but we can have higher latency and get throughput back through batching much more easily). # Exploring many routes\n```\nNote: Tensor Parallelism (TP) means in this context that each GPU will own\npart of the weights, so ALL gpus are active all the time and do less work.""]","For fp16 data type on an A100 GPU, it is recommended to use multiples of 64.",64
"How do you run a Gradio Blocks app in reload mode using a Python IDE?
","[""Developing Faster with Auto-Reloading\n\\*\\*Prerequisite\\*\\*: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control). This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks. ## Why Auto-Reloading? When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes. To make it faster and more convenient to write your code, we've made it easier to \""reload\"" your Gradio apps instantly when you are developing in a \\*\\*Python IDE\\*\\* (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \""magic command\"" that allows you to re-run cells faster if you use \\*\\*Jupyter Notebooks\\*\\* (or any similar environment like Colab). This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster. ## Python IDE Reload 🔥\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\ngr.Markdown(\""# Greetings from Gradio!\"")\ninp = gr.Textbox(placeholder=\""What is your name?\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""Welcome, {x}!\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`."", ""Gradio Demo: dataframe\\_block-ui-test\n```\n!pip install -q gradio\n```\n```\nimport gradio as gr\nwith gr.Blocks() as demo:\ncount = gr.Slider(minimum=1, maximum=10, step=1, label=\""count\"")\ndata = gr.DataFrame(\nheaders=[\""A\"", \""B\""], col\\_count=(2, \""fixed\""), type=\""array\"", interactive=True\n)\nbtn = gr.Button(value=\""click\"")\nbtn.click(\nfn=lambda cnt: [[str(2 \\* i), str(2 \\* i + 1)] for i in range(int(cnt))],\ninputs=[count],\noutputs=[data],\n)\ndemo.launch()\n```"", ""By default, the Gradio use UTF-8 encoding for scripts. \\*\\*For reload mode\\*\\*, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n1. Configure encoding declaration of python script, for example: `# -\\*- coding: cp1252 -\\*-`\n2. Confirm that your code editor has identified that encoding format. 3. Run like this: `gradio run.py --encoding cp1252`\n🔥 If your application accepts command line arguments, you can pass them in as well. Here's an example:\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# Greetings {args.name}!\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\nWhich you could run like this: `gradio run.py --name Gretel`\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code."", ""WARNING: The --reload flag should not be used in production on Windows. ```\n这里最重要的一行是 `正在观察 ...`。这里发生的情况是 Gradio 将观察 `run.py` 文件所在的目录，如果文件发生更改，它将自动为您重新运行文件。因此，您只需专注于编写代码，Gradio 演示将自动刷新 🥳\n⚠️ 警告：`gradio` 命令不会检测传递给 `launch()` 方法的参数，因为在重新加载模式下从未调用 `launch()` 方法。例如，设置 `launch()` 中的 `auth` 或 `show\\_error` 不会在应用程序中反映出来。\n当您使用重新加载模式时，请记住一件重要的事情：Gradio 专门查找名为 `demo` 的 Gradio Blocks/Interface 演示。如果您将演示命名为其他名称，您需要在代码中的第二个参数中传入演示的 FastAPI 应用程序的名称。对于 Gradio 演示，可以使用 `.app` 属性访问 FastAPI 应用程序。因此，如果您的 `run.py` 文件如下所示：\n```python\nimport gradio as gr\nwith gr.Blocks() as my\\_demo:\ngr.Markdown(\""# 来自Gradio的问候！\"")\ninp = gr.Textbox(placeholder=\""您叫什么名字？\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""欢迎，{x}！\"",\ninputs=inp,\noutputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\nmy\\_demo.launch()\n```\n那么您可以这样启动它：`gradio run.py my\\_demo.app`。\nGradio默认使用UTF-8编码格式。对于\\*\\*重新加载模式\\*\\*，如果你的脚本使用的是除UTF-8以外的编码（如GBK）：\n1. 在Python脚本的编码声明处指定你想要的编码格式，如：`# -\\*- coding: gbk -\\*-`\n2. 确保你的代码编辑器识别到该格式。\n3. 执行：`gradio run.py --encoding gbk`\n🔥 如果您的应用程序接受命令行参数，您也可以传递它们。下面是一个例子：\n```python\nimport gradio as gr\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add\\_argument(\""--name\"", type=str, default=\""User\"")\nargs, unknown = parser.parse\\_known\\_args()\nwith gr.Blocks() as demo:\ngr.Markdown(f\""# 欢迎 {args.name}！\"")\ninp = gr.Textbox()\nout = gr.Textbox()\ninp.change(fn=lambda x: x, inputs=inp, outputs=out)\nif \\_\\_name\\_\\_ == \""\\_\\_main\\_\\_\"":\ndemo.launch()\n```\n您可以像这样运行它：`gradio run.py --name Gretel`\n作为一个小提示，只要更改了 `run.py` 源代码或 Gradio 源代码，自动重新加载就会发生。这意味着如果您决定[为 Gradio 做贡献](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md)，这将非常有用 ✅\n## Jupyter Notebook 魔法命令🔮\n如果您使用 Jupyter Notebooks（或 Colab Notebooks 等）进行开发，我们也为您提供了一个解决方案！\n我们开发了一个 \\*\\*magic command 魔法命令\\*\\*，可以为您创建和运行一个 Blocks 演示。要使用此功能，在笔记本顶部加载 gradio 扩展：\n`%load\\_ext gradio`\n然后，在您正在开发 Gradio 演示的单元格中，只需在顶部写入魔法命令\\*\\*`%%blocks`\\*\\*，然后像平常一样编写布局和组件：\n```py\n%%blocks\nimport gradio as gr\ngr.Markdown(\""# 来自Gradio的问候！\"")\ninp = gr.Textbox(placeholder=\""您叫什么名字？\"")\nout = gr.Textbox()\ninp.change(fn=lambda x: f\""欢迎，{x}！\"",\ninputs=inp,\noutputs=out)\n```\n请注意：\n- 您不需要放置样板代码 `with gr.Blocks() as demo:` 和 `demo.launch()` — Gradio 会自动为您完成！\n- 每次重新运行单元格时，Gradio 都将在相同的端口上重新启动您的应用程序，并使用相同的底层网络服务器。这意味着您将比正常重新运行单元格更快地看到变化。\n下面是在 Jupyter Notebook 中的示例：\n![](https://i.ibb.co/nrszFws/Blocks.gif)\n🪄这在 colab 笔记本中也适用！[这是一个 colab 笔记本](https://colab.research.google.com/drive/1jUlX1w7JqckRHVE-nbDyMPyZ7fYD8488?authuser=1#scrollTo=zxHYjbCTTz\\_5)，您可以在其中看到 Blocks 魔法效果。尝试进行一些更改并重新运行带有 Gradio 代码的单元格！\nNotebook Magic 现在是作者构建 Gradio 演示的首选方式。无论您如何编写 Python 代码，我们都希望这两种方法都能为您提供更好的 Gradio 开发体验。\n---\n## 下一步\n既然您已经了解了如何使用 Gradio 快速开发，请开始构建自己的应用程序吧！\n如果你正在寻找灵感，请尝试浏览其他人用 Gradio 构建的演示，[浏览 Hugging Face Spaces](http://hf.space/) 🤗"", ""This means you'll see your changes \\_much, much faster\\_ than if you were rerunning the cell normally. Here's what it looks like in a jupyter notebook:\n![](https://gradio-builds.s3.amazonaws.com/demo-files/jupyter\\_reload.gif)\n🪄 This works in colab notebooks too! [Here's a colab notebook](https://colab.research.google.com/drive/1zAuWoiTIb3O2oitbtVb2\\_ekv1K6ggtC1?usp=sharing) where you can see the Blocks magic in action. Try making some changes and re-running the cell with the Gradio code! The Notebook Magic is now the author's preferred way of building Gradio demos. Regardless of how you write Python code, we hope either of these methods will give you a much better development experience using Gradio. ---\n## Next Steps\nNow that you know how to develop quickly using Gradio, start building your own! If you are looking for inspiration, try exploring demos other people have built with Gradio, [browse public Hugging Face Spaces](http://hf.space/) 🤗""]","To run a Gradio Blocks app in reload mode using a Python IDE, follow these steps:

1. Write your Gradio code in a Python file (e.g., `run.py`). Here’s an example structure:

   ```python
   import gradio as gr
   with gr.Blocks() as demo:
       gr.Markdown(""# Greetings from Gradio!"")
       inp = gr.Textbox(placeholder=""What is your name?"")
       out = gr.Textbox()
       inp.change(fn=lambda x: f""Welcome, {x}!"", inputs=inp, outputs=out)
   if __name__ == ""__main__"":
       demo.launch()
   ```

2. To enable auto-reloading, run your script using the command:

   ```
   gradio run.py
   ```

   Gradio will automatically observe changes in your `run.py` file and reload the app when modifications are detected.

3. If your script uses a different encoding (other than UTF-8), specify the encoding in your script and run it with the encoding option. For example, if using `cp1252`, do the following:

   - Add the encoding declaration at the top of your script:
     ```python
     # -*- coding: cp1252 -*-
     ```

   - Run the script with the encoding option:
     ```
     gradio run.py --encoding cp1252
     ```

4. If your application accepts command line arguments, you can pass them as well. For example:

   ```python
   import gradio as gr
   import argparse
   parser = argparse.ArgumentParser()
   parser.add_argument(""--name"", type=str, default=""User"")
   args, unknown = parser.parse_known_args()
   with gr.Blocks() as demo:
       gr.Markdown(f""# Greetings {args.name}!"")
       inp = gr.Textbox()
       out = gr.Textbox()
       inp.change(fn=lambda x: x, inputs=inp, outputs=out)
   if __name__ == ""__main__"":
       demo.launch()
   ```

   You can run it like this:
   ```
   gradio run.py --name Gretel
   ```

By following these steps, you can efficiently develop your Gradio Blocks app with auto-reloading in a Python IDE.",Run `gradio run.py` in the terminal.
"How can you install the Hugging Face Unity API in your Unity project?
","[""--\ntitle: \""How to Install and Use the Hugging Face Unity API\""\nthumbnail: /blog/assets/124\\_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n# How to Install and Use the Hugging Face Unity API\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API. ## Installation\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif)\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens). 7. Test the API key by clicking `Test API key` in the API Wizard. 8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API\\_URL` field."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Under construction"", ""If you require any assistance while using the Hugging Face Hub, there are several avenues you can explore. You may seek help by utilizing the [discussion forum](https://discuss.huggingface.co/) or through a [Discord](https://discord.com/invite/hugging-face-879548962464493619)."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Articulation Bodies\nThere are two primary types of articulation supported by Simulate:\n1. `Prismatic` joints: slide along a specified axis,\n2. `Revolute` joints: rotate about a specified axis. \\*Note\\* that these are currently only implemented in \\*\\*Unity\\*\\*. [[autodoc]] ArticulationBodyComponent"", ""Congratulations! Refresh your Space. You should now be able to play your game in a Hugging Face Space. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!""]","To install the Hugging Face Unity API in your Unity project, follow these steps:

1. Open your Unity project.
2. Go to `Window` -> `Package Manager`.
3. Click `+` and select `Add Package from git URL`.
4. Enter `https://github.com/huggingface/unity-api.git`.
5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.
6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).
7. Test the API key by clicking `Test API key` in the API Wizard.
8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the URL from the `API_URL` field.","To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`."
"What is the pretraining objective of the Wav2Vec2 context network?
","[""!--Copyright 2021 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer. -->\n# Wav2Vec2\n## Overview\nThe Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. The abstract from the paper is the following:\n\\*We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on\ntranscribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks\nthe speech input in the latent space and solves a contrastive task defined over a quantization of the latent\nrepresentations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the\nclean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state\nof the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and\npre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech\nrecognition with limited amounts of labeled data.\\*\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). ## Usage tips\n- Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\nusing [`Wav2Vec2CTCTokenizer`]. ## Resources\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Wav2Vec2."", ""The results of this run can be seen [here](https://wandb.ai/patrickvonplaten/pretraining-wav2vec2/reports/Wav2Vec2-Large--VmlldzoxMTAwODM4?accessToken=wm3qzcnldrwsa31tkvf2pdmilw3f63d4twtffs86ou016xjbyilh55uoi3mo1qzc) and the checkpoint pretrained for 120,000 steps can be accessed [here](https://huggingface.co/patrickvonplaten/wav2vec2-large-repro-960h-libri-120k-steps)"", ""We are interested in the most likely prediction of\nthe model and thus take the `argmax(...)` of the logits. Also, we\ntransform the encoded labels back to the original string by replacing\n`-100` with the `pad\\_token\\_id` and decoding the ids while making sure\nthat consecutive tokens are \\*\\*not\\*\\* grouped to the same token in CTC\nstyle \\\\({}^1\\\\). ```python\ndef compute\\_metrics(pred):\npred\\_logits = pred.predictions\npred\\_ids = np.argmax(pred\\_logits, axis=-1)\npred.label\\_ids[pred.label\\_ids == -100] = processor.tokenizer.pad\\_token\\_id\npred\\_str = processor.batch\\_decode(pred\\_ids)\n# we do not want to group tokens when computing the metrics\nlabel\\_str = processor.batch\\_decode(pred.label\\_ids, group\\_tokens=False)\nwer = wer\\_metric.compute(predictions=pred\\_str, references=label\\_str)\nreturn {\""wer\"": wer}\n```\nNow, we can load the pretrained `Wav2Vec2` checkpoint. The tokenizer\\'s\n`pad\\_token\\_id` must be to define the model\\'s `pad\\_token\\_id` or in the\ncase of `Wav2Vec2ForCTC` also CTC\\'s \\*blank token\\* \\\\({}^2\\\\). To save GPU\nmemory, we enable PyTorch\\'s [gradient\ncheckpointing](https://pytorch.org/docs/stable/checkpoint.html) and also\nset the loss reduction to \\\""\\*mean\\*\\\"". ```python\nfrom transformers import Wav2Vec2ForCTC\nmodel = Wav2Vec2ForCTC.from\\_pretrained(\n\""facebook/wav2vec2-base\"",\nctc\\_loss\\_reduction=\""mean\"",\npad\\_token\\_id=processor.tokenizer.pad\\_token\\_id,\n)\n```\n\\*\\*Print Output:\\*\\*\n```bash\nSome weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm\\_head.weight', 'lm\\_head.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."", ""Using the Buckwalter format, text is also logged in Arabic abjad. `--target\\_feature\\_extractor\\_sampling\\_rate` resamples audio to target feature extractor's sampling rate (16kHz). `--max\\_duration\\_in\\_seconds=\""15\""` filters out examples whose audio is longer than the specified limit,\nwhich helps with capping GPU memory usage. ### DeepSpeed Integration\nTo learn how to deploy Deepspeed Integration please refer to [this guide](https://huggingface.co/transformers/main/main\\_classes/deepspeed.html#deepspeed-trainer-integration). But to get started quickly all you need is to install:\n```\npip install deepspeed\n```\nand then use the default configuration files in this directory:\n\\* `ds\\_config\\_wav2vec2\\_zero2.json`\n\\* `ds\\_config\\_wav2vec2\\_zero3.json`\nHere are examples of how you can use DeepSpeed:\n(edit the value for `--num\\_gpus` to match the number of GPUs you have)\nZeRO-2:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 2 \\\nrun\\_asr.py \\\n--output\\_dir=output\\_dir --num\\_train\\_epochs=2 --per\\_device\\_train\\_batch\\_size=2 \\\n--per\\_device\\_eval\\_batch\\_size=2 --evaluation\\_strategy=steps --save\\_steps=500 --eval\\_steps=100 \\\n--logging\\_steps=5 --learning\\_rate=5e-4 --warmup\\_steps=3000 \\\n--model\\_name\\_or\\_path=patrickvonplaten/wav2vec2\\_tiny\\_random\\_robust \\\n--dataset\\_name=hf-internal-testing/librispeech\\_asr\\_dummy --dataset\\_config\\_name=clean \\\n--train\\_split\\_name=validation --validation\\_split\\_name=validation --orthography=timit \\\n--preprocessing\\_num\\_workers=1 --group\\_by\\_length --freeze\\_feature\\_extractor --verbose\\_logging \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero2.json\n```\nFor ZeRO-2 with more than 1 gpu you need to use (which is already in the example configuration file):\n```\n\""zero\\_optimization\"": {\n... \""find\\_unused\\_parameters\"": true,\n... }\n```\nZeRO-3:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 2 \\\nrun\\_asr.py \\\n--output\\_dir=output\\_dir --num\\_train\\_epochs=2 --per\\_device\\_train\\_batch\\_size=2 \\\n--per\\_device\\_eval\\_batch\\_size=2 --evaluation\\_strategy=steps --save\\_steps=500 --eval\\_steps=100 \\\n--logging\\_steps=5 --learning\\_rate=5e-4 --warmup\\_steps=3000 \\\n--model\\_name\\_or\\_path=patrickvonplaten/wav2vec2\\_tiny\\_random\\_robust \\\n--dataset\\_name=hf-internal-testing/librispeech\\_asr\\_dummy --dataset\\_config\\_name=clean \\\n--train\\_split\\_name=validation --validation\\_split\\_name=validation --orthography=timit \\\n--preprocessing\\_num\\_workers=1 --group\\_by\\_length --freeze\\_feature\\_extractor --verbose\\_logging \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero3.json\n```\n### Pretraining Wav2Vec2\nThe `run\\_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss objective (see official [paper](https://arxiv.org/abs/2006.11477) for more information). It is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed (please refer to [this guide](https://huggingface.co/transformers/main/main\\_classes/deepspeed.html#deepspeed-trainer-integration) for more information). Here is an example of how you can use DeepSpeed ZeRO-2 to pretrain a small Wav2Vec2 model:\n```\nPYTHONPATH=../../../src deepspeed --num\\_gpus 4 run\\_pretrain.py \\\n--output\\_dir=\""./wav2vec2-base-libri-100h\"" \\\n--num\\_train\\_epochs=\""3\"" \\\n--per\\_device\\_train\\_batch\\_size=\""32\"" \\\n--per\\_device\\_eval\\_batch\\_size=\""32\"" \\\n--gradient\\_accumulation\\_steps=\""2\"" \\\n--save\\_total\\_limit=\""3\"" \\\n--save\\_steps=\""500\"" \\\n--logging\\_steps=\""10\"" \\\n--learning\\_rate=\""5e-4\"" \\\n--weight\\_decay=\""0.01\"" \\\n--warmup\\_steps=\""3000\"" \\\n--model\\_name\\_or\\_path=\""patrickvonplaten/wav2vec2-base-libri-100h\"" \\\n--dataset\\_name=\""librispeech\\_asr\"" \\\n--dataset\\_config\\_name=\""clean\"" \\\n--train\\_split\\_name=\""train.100\"" \\\n--preprocessing\\_num\\_workers=\""4\"" \\\n--max\\_duration\\_in\\_seconds=\""10.0\"" \\\n--group\\_by\\_length \\\n--verbose\\_logging \\\n--fp16 \\\n--deepspeed ds\\_config\\_wav2vec2\\_zero2.json \\\n```\n### Forced Alignment\nCharacter level forced alignment for audio and text pairs with wav2vec2 models finetuned on ASR task for a specific language. Inspired by [this](https://pytorch.org/tutorials/intermediate/forced\\_alignment\\_with\\_torchaudio\\_tutorial.html) Pytorch tutorial. #### Input Formats\nInput format in script.txt Input format in wavs directroy\n0000 sentence1 0000.wav\n0001 sentence2 0001.wav\n#### Output Format\nOutput directory will contain 0000.txt and 0001.txt. Each file will have format like below\nchar score start\\_ms end\\_ms\nh 0.25 1440 1520\n#### Run command\n```\npython alignment.py \\\n--model\\_name=\""arijitx/wav2vec2-xls-r-300m-bengali\"" \\\n--wav\\_dir=\""./wavs\""\n--text\\_file=\""script.txt\"" \\\n--input\\_wavs\\_sr=48000 \\\n--output\\_dir=\""./out\\_alignment\"" \\\n--cuda\n```"", ""If you wish to implement your own costumized data loading/sampling, feel free to just make use of the `\""path\""` column instead and disregard the `\""audio\""` column. Training & Evaluation\n---------------------\nThe data is processed so that we are ready to start setting up the\ntraining pipeline. We will make use of 🤗\\'s\n[Trainer](https://huggingface.co/transformers/master/main\\_classes/trainer.html?highlight=trainer)\nfor which we essentially need to do the following:\n- Define a data collator. In contrast to most NLP models, Wav2Vec2 has\na much larger input length than output length. \\*E.g.\\*, a sample of\ninput length 50000 has an output length of no more than 100. Given\nthe large input sizes, it is much more efficient to pad the training\nbatches dynamically meaning that all training samples should only be\npadded to the longest sample in their batch and not the overall\nlongest sample. Therefore, fine-tuning Wav2Vec2 requires a special\npadding data collator, which we will define below\n- Evaluation metric.""]","The pretraining objective of the Wav2Vec2 context network is based on a contrastive loss objective. This approach allows the model to learn powerful representations from speech audio alone, which can then be fine-tuned on transcribed speech. The model masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations that are jointly learned.",The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.
"What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?
","[""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from\\_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return\\_tensors option tells the tokenizer to return a TensorFlow tensor."", ""## Additional resources\n\\* Transformers.js [repository](https://github.com/xenova/transformers.js)\n\\* Transformers.js [docs](https://huggingface.co/docs/transformers.js)\n\\* Transformers.js [demo](https://xenova.github.io/transformers.js/)"", ""Мы хотим, чтобы Transformers позволил разработчикам, исследователям, студентам, профессорам, инженерам и всем желающим\nсоздавать проекты своей мечты. Чтобы отпраздновать 100 тысяч звезд Transformers, мы решили сделать акцент на сообществе, и создали страницу [awesome-transformers](./awesome-transformers.md), на которой перечислены 100\nневероятных проектов, созданных с помощью transformers. Если вы являетесь владельцем или пользователем проекта, который, по вашему мнению, должен быть включен в этот список, пожалуйста, откройте PR для его добавления! ## Если вы хотите получить индивидуальную поддержку от команды Hugging Face\n[![HuggingFace Expert Acceleration Program](https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png)](https://huggingface.co/support)  \n## Быстрый гайд\nДля использования модели на заданном входе (текст, изображение, звук, ...) мы предоставляем API `pipeline`. Конвейеры объединяют предварительно обученную модель с препроцессингом, который использовался при ее обучении. Вот как можно быстро использовать конвейер для классификации положительных и отрицательных текстов:\n```python\n>>> from transformers import pipeline\n# Выделение конвейера для анализа настроений\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('Мы очень рады представить конвейер в transformers.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```\nВторая строка кода загружает и кэширует предварительно обученную модель, используемую конвейером, а третья оценивает ее на заданном тексте."", ""Port of Hugging Face's Transformers library, using the tch-rs crate and pre-processing from rust-tokenizers. Supports multi-threaded tokenization and GPU inference. This repository exposes the model base architecture, task-specific heads and ready-to-use pipelines. Keywords: Rust, BERT, Inference\n## [EasyNLP](https://github.com/alibaba/EasyNLP)\n[EasyNLP](https://github.com/alibaba/EasyNLP) is an easy-to-use NLP development and application toolkit in PyTorch, first released inside Alibaba in 2021. It is built with scalable distributed training strategies and supports a comprehensive suite of NLP algorithms for various NLP applications. [EasyNLP](https://github.com/alibaba/EasyNLP) integrates knowledge distillation and few-shot learning for landing large pre-trained models, together with various popular multi-modality pre-trained models. It provides a unified framework of model training, inference, and deployment for real-world applications. Keywords: NLP, Knowledge distillation, Few-shot learning, Multi-modality, Training, Inference, Deployment\n## [TurboTransformers](https://github.com/Tencent/TurboTransformers)\nA fast and user-friendly runtime for transformer inference (Bert, Albert, GPT2, Decoders, etc) on CPU and GPU."", ""## Acknowledgements\nBlog post by [Sanchit Gandhi](https://huggingface.co/sanchit-gandhi). Many thanks to [Patrick von Platen](https://huggingface.co/patrickvonplaten)\nand [Pedro Cuenca](https://huggingface.co/pcuenq) for their constructive comments, and to [Joao Gante](https://huggingface.co/joaogante)\nfor the assisted generation implementation in 🤗 Transformers.""]",The default checkpoint used by the sentiment analysis pipeline in the Transformers library is `distilbert base uncased finetuned sst2 english`.,distilbert base uncased finetuned sst2 english
"What is the purpose of the notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi""?
","[""Note that while [CUDA graphs](https://developer.nvidia.com/blog/cuda-graphs/) are not currently compatible with model parallelism in DeepSpeed (DeepSpeed v0.8.2, see [here](https://github.com/microsoft/DeepSpeed/blob/v0.8.2/deepspeed/inference/engine.py#L158)), HPU graphs are supported in Habana's DeepSpeed fork. All benchmarks are doing [greedy generation](https://huggingface.co/blog/how-to-generate#greedy-search) of 100 token outputs. The input prompt is:\n> \""DeepSpeed is a machine learning framework\""\nwhich consists of 7 tokens with BLOOM's tokenizer. The results for inference latency are displayed in the table below (the unit is \\*seconds\\*). | Model | Number of devices | Gaudi2 latency (seconds) | A100-80GB latency (seconds) | First-gen Gaudi latency (seconds) |\n|:-----------:|:-----------------:|:-------------------------:|:-----------------:|:----------------------------------:|\n| BLOOMZ | 8 | 3.103 | 4.402 | / |\n| BLOOMZ-7B | 8 | 0.734 | 2.417 | 3.321 |\n| BLOOMZ-7B | 1 | 0.772 | 2.119 | 2.387 |\n\\*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, leading to a\\* x\\*1.42 speedup on BLOOMZ with Gaudi2 compared to A100.\\*\nThe Habana team recently introduced support for DeepSpeed-inference in SynapseAI 1.8, and thereby quickly enabled inference for 100+ billion parameter models. \\*\\*For the 176-billion-parameter checkpoint, Gaudi2 is 1.42x faster than A100 80GB\\*\\*. Smaller checkpoints present interesting results too. \\*\\*Gaudi2 is 2.89x faster than A100 for BLOOMZ-7B!\\*\\* It is also interesting to note that it manages to benefit from model parallelism whereas A100 is faster on a single device. We also ran these models on first-gen Gaudi. While it is slower than Gaudi2, it is interesting from a price perspective as a DL1 instance on AWS costs approximately 13\\$ per hour. Latency for BLOOMZ-7B on first-gen Gaudi is 2.387 seconds. Thus, \\*\\*first-gen Gaudi offers for the 7-billion checkpoint a better price-performance ratio than A100\\*\\* which costs more than 30\\$ per hour! We expect the Habana team will optimize the performance of these models in the upcoming SynapseAI releases. For example, in our last benchmark, we saw that [Gaudi2 performs Stable Diffusion inference 2.2x faster than A100](https://huggingface.co/blog/habana-gaudi-2-benchmark#generating-images-from-text-with-stable-diffusion) and this has since been improved further to 2.37x with the latest optimizations provided by Habana. We will update these numbers as new versions of SynapseAI are released and integrated within Optimum Habana."", ""Why would you want to use DeepSpeed with just one GPU? 1. It has a ZeRO-offload feature which can delegate some computations and memory to the host's CPU and RAM, and thus\nleave more GPU resources for model's needs - e.g. larger batch size, or enabling a fitting of a very big model which\nnormally won't fit. 2. It provides a smart GPU memory management system, that minimizes memory fragmentation, which again allows you to fit\nbigger models and data batches. While we are going to discuss the configuration in details next, the key to getting a huge improvement on a single GPU\nwith DeepSpeed is to have at least the following configuration in the configuration file:\n```json\n{\n\""zero\\_optimization\"": {\n\""stage\"": 2,\n\""offload\\_optimizer\"": {\n\""device\"": \""cpu\"",\n\""pin\\_memory\"": true\n},\n\""allgather\\_partitions\"": true,\n\""allgather\\_bucket\\_size\"": 2e8,\n\""reduce\\_scatter\"": true,\n\""reduce\\_bucket\\_size\"": 2e8,\n\""overlap\\_comm\"": true,\n\""contiguous\\_gradients\"": true\n}\n}\n```\nwhich enables optimizer offload and some other important features."", ""--\ntitle: Getting Started with Transformers on Habana Gaudi\nthumbnail: /blog/assets/61\\_getting\\_started\\_habana/habana01.png\nauthors:\n- user: juliensimon\n---\n# Getting Started with Transformers on Habana Gaudi\nA couple of weeks ago, we've had the pleasure to [announce](https://huggingface.co/blog/habana) that [Habana Labs](https://habana.ai) and [Hugging Face](https://huggingface.co/) would partner to accelerate Transformer model training. Habana Gaudi accelerators deliver up to 40% better price performance for training machine learning models compared to the latest GPU-based Amazon EC2 instances. We are super excited to bring this price performance advantages to Transformers 🚀\nIn this hands-on post, I'll show you how to quickly set up a Habana Gaudi instance on Amazon Web Services, and then fine-tune a BERT model for text classification. As usual, all code is provided so that you may reuse it in your projects. Let's get started! ## Setting up an Habana Gaudi instance on AWS\nThe simplest way to work with Habana Gaudi accelerators is to launch an Amazon EC2 [DL1](https://aws.amazon.com/ec2/instance-types/dl1/) instance. These instances are equipped with 8 Habana Gaudi processors that can easily be put to work thanks to the [Habana Deep Learning Amazon Machine Image](https://aws.amazon.com/marketplace/server/procurement?productId=9a75c51a-a4d1-4470-884f-6be27933fcc8) (AMI)."", ""HPUs on Gaudi2 or GPUs on A100/H100). This can be done on Gaudi2 using Habana's [media pipeline](https://docs.habana.ai/en/latest/Media\\_Pipeline/index.html). Given a dataset, most dataloaders follow the following recipe:\n1. Fetch data (e.g. where your JPEG images are stored on disk)\n2. The CPU reads encoded images\n3. The CPU decodes images\n4. The CPU applies image transformations to augment images\n5. Finally, images are sent to devices (although this is usually not done by the dataloader itself)\nInstead of doing the whole process on CPU and send ready-to-train data to devices, a more efficient workflow would be to send encoded images to devices first and then perform image decoding and augmentations:\n1. Same as before\n2. Same as before\n3. Encoded images are sent to devices\n4. Devices decode images\n5. Devices apply image transformations to augment images\nThat way we can benefit from the computing power of our devices to speed image decoding and transformations up. Note that there are two caveats to be aware of when doing this:\n- Device memory consumption will increase, so you may have to reduce your batch size if there is not enough free memory."", ""--\ntitle: \""Accelerate Large Model Training using DeepSpeed\""\nthumbnail: /blog/assets/83\\_accelerate\\_deepspeed/deepspeed-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n# Accelerate Large Model Training using DeepSpeed\nIn this post we will look at how we can leverage the \\*\\*[Accelerate](https://github.com/huggingface/accelerate)\\*\\* library for training large models which enables users to leverage the ZeRO features of \\*\\*[DeeSpeed](https://www.deepspeed.ai)\\*\\*. # Motivation 🤗\n\\*\\*Tired of Out of Memory (OOM) errors while trying to train large models? We've got you covered. Large models are very performant [1] but difficult to train with the available hardware. To get the most of the available hardware for training large models one can leverage Data Parallelism using ZeRO - Zero Redundancy Optimizer [2]\\*\\*. Below is a short description of Data Parallelism using ZeRO with diagram from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)\n![ZeRO Data Parallelism](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero.png)\n(Source: [link](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/))\na.""]","The notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi"" is designed to demonstrate how to leverage the DeepSpeed framework for training large models, specifically those with billions of parameters, on Habana Gaudi accelerators. It provides guidance on utilizing the ZeRO (Zero Redundancy Optimizer) features of DeepSpeed to efficiently manage memory and computational resources, enabling users to train large models that would otherwise be difficult to handle with standard hardware configurations. The notebook includes practical examples and code snippets to facilitate the training process on Habana Gaudi.",To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.
"What command line module does PyTorch provide to run a script on multiple GPUs?
","[""\\* correct / len(test\\_loader.dataset)}')\n```\nTypically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook. However, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through \\*distributed\\* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\n## PyTorch Distributed Data Parallelism\nAs the name implies, `torch.distributed` is meant to work on \\*distributed\\* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both. To convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp\\_tutorial.html)\nFirst a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through\n> Note: for this section of the tutorial it should be assumed these are sent in python script files. Later on a launcher using Accelerate will be discussed that removes this necessity\n```python\nimport os\nimport torch.distributed as dist\ndef setup(rank, world\\_size):\n\""Sets up the process group and configuration for PyTorch Distributed Data Parallelism\""\nos.environ[\""MASTER\\_ADDR\""] = 'localhost'\nos.environ[\""MASTER\\_PORT\""] = \""12355\""\n# Initialize the process group\ndist.init\\_process\\_group(\""gloo\"", rank=rank, world\\_size=world\\_size)\ndef cleanup():\n\""Cleans up the distributed environment\""\ndist.destroy\\_process\\_group()\n```\nThe last piece of the puzzle is \\*how do I send my data and model to another GPU?\\*\nThis is where the `DistributedDataParallel` module comes into play. It will copy your model onto each GPU, and when `loss.backward()` is called the backpropagation is performed and the resulting gradients across all these copies of the model will be averaged/reduced. This ensures each device has the same weights post the optimizer step. Below is an example of our training setup, refactored as a function, with this capability:\n> Note: Here rank is the overall rank of the current GPU compared to all the other GPUs available, meaning they have a rank of `0 -> n-1`\n```python\nfrom torch.nn.parallel import DistributedDataParallel as DDP\ndef train(model, rank, world\\_size):\nsetup(rank, world\\_size)\nmodel = model.to(rank)\nddp\\_model = DDP(model, device\\_ids=[rank])\noptimizer = optim.AdamW(ddp\\_model.parameters(), lr=1e-3)\n# Train for one epoch\nmodel.train()\nfor batch\\_idx, (data, target) in enumerate(train\\_loader):\ndata, target = data.to(device), target.to(device)\noutput = model(data)\nloss = F.nll\\_loss(output, target)\nloss.backward()\noptimizer.step()\noptimizer.zero\\_grad()\ncleanup()\n```\nThe optimizer needs to be declared based on the model \\*on the specific device\\* (so `ddp\\_model` and not `model`) for all of the gradients to properly be calculated. Lastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\n```bash\ntorchrun --nproc\\_per\\_node=2 --nnodes=1 example\\_script.py\n```\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch."", "". ! - \\; \\: \\\"" “ % ‘ ” � \\\n--fp16 \\\n--group\\_by\\_length \\\n--push\\_to\\_hub \\\n--do\\_train --do\\_eval\n```\nOn a single V100 GPU, this script should run in \\*ca.\\* 1 hour 20 minutes and yield a CTC loss of \\*\\*0.39\\*\\* and word error rate\nof \\*\\*0.35\\*\\*. ### Multi GPU CTC\nThe following command shows how to fine-tune [XLSR-Wav2Vec2](https://huggingface.co/transformers/main/model\\_doc/xlsr\\_wav2vec2.html) on [Common Voice](https://huggingface.co/datasets/common\\_voice) using 8 GPUs in half-precision. ```bash\ntorchrun \\\n--nproc\\_per\\_node 8 run\\_speech\\_recognition\\_ctc.py \\\n--dataset\\_name=\""common\\_voice\"" \\\n--model\\_name\\_or\\_path=\""facebook/wav2vec2-large-xlsr-53\"" \\\n--dataset\\_config\\_name=\""tr\"" \\\n--output\\_dir=\""./wav2vec2-common\\_voice-tr-demo-dist\"" \\\n--overwrite\\_output\\_dir \\\n--num\\_train\\_epochs=\""15\"" \\\n--per\\_device\\_train\\_batch\\_size=\""4\"" \\\n--learning\\_rate=\""3e-4\"" \\\n--warmup\\_steps=\""500\"" \\\n--evaluation\\_strategy=\""steps\"" \\\n--text\\_column\\_name=\""sentence\"" \\\n--length\\_column\\_name=\""input\\_length\"" \\\n--save\\_steps=\""400\"" \\\n--eval\\_steps=\""100\"" \\\n--logging\\_steps=\""1\"" \\\n--layerdrop=\""0.0\"" \\\n--save\\_total\\_limit=\""3\"" \\\n--freeze\\_feature\\_encoder \\\n--gradient\\_checkpointing \\\n--chars\\_to\\_ignore , ?"", ""TPUs\ncan also be used by passing the name of the TPU resource with the `--tpu` argument. ### Memory usage and data loading\nOne thing to note is that all data is loaded into memory in this script. Most text classification datasets are small\nenough that this is not an issue, but if you have a very large dataset you will need to modify the script to handle\ndata streaming. This is particularly challenging for TPUs, given the stricter requirements and the sheer volume of data\nrequired to keep them fed. A full explanation of all the possible pitfalls is a bit beyond this example script and\nREADME, but for more information you can see the 'Input Datasets' section of\n[this document](https://www.tensorflow.org/guide/tpu). ### Example command\n```\npython run\\_glue.py \\\n--model\\_name\\_or\\_path distilbert-base-cased \\\n--task\\_name mnli \\\n--do\\_train \\\n--do\\_eval \\\n--do\\_predict \\\n--predict\\_file data\\_to\\_predict.json\n```"", ""Scripts\nA train, validation, inference, and checkpoint cleaning script included in the github root folder. Scripts are not currently packaged in the pip release. The training and validation scripts evolved from early versions of the [PyTorch Imagenet Examples](https://github.com/pytorch/examples). I have added significant functionality over time, including CUDA specific performance enhancements based on\n[NVIDIA's APEX Examples](https://github.com/NVIDIA/apex/tree/master/examples). ## Training Script\nThe variety of training args is large and not all combinations of options (or even options) have been fully tested. For the training dataset folder, specify the folder to the base that contains a `train` and `validation` folder. To train an SE-ResNet34 on ImageNet, locally distributed, 4 GPUs, one process per GPU w/ cosine schedule, random-erasing prob of 50% and per-pixel random value:\n```bash\n./distributed\\_train.sh 4 /data/imagenet --model seresnet34 --sched cosine --epochs 150 --warmup-epochs 5 --lr 0.4 --reprob 0.5 --remode pixel --batch-size 256 --amp -j 4\n```\nIt is recommended to use PyTorch 1.9+ w/ PyTorch native AMP and DDP instead of APEX AMP."", ""The file naming is up to you. It's recommended to use DeepSpeed's `add\\_config\\_arguments` utility to add the necessary command line arguments to your code. For more information please see [DeepSpeed's Argument Parsing](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) doc. You can use a launcher of your choice here. You can continue using the pytorch launcher:\n```bash\ntorch.distributed.run --nproc\\_per\\_node=2 your\\_program.py  --deepspeed ds\\_config.json\n```\nor use the launcher provided by `deepspeed`:\n```bash\ndeepspeed --num\\_gpus=2 your\\_program.py  --deepspeed ds\\_config.json\n```\nAs you can see the arguments aren't the same, but for most needs either of them works. The\nfull details on how to configure various nodes and GPUs can be found [here](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node). When you use the `deepspeed` launcher and you want to use all available gpus you can just omit the `--num\\_gpus` flag. Here is an example of running `run\\_translation.py` under DeepSpeed deploying all available GPUs:\n```bash\ndeepspeed examples/pytorch/translation/run\\_translation.py \\\n--deepspeed tests/deepspeed/ds\\_config\\_zero3.json \\\n--model\\_name\\_or\\_path t5-small --per\\_device\\_train\\_batch\\_size 1 \\\n--output\\_dir output\\_dir --overwrite\\_output\\_dir --fp16 \\\n--do\\_train --max\\_train\\_samples 500 --num\\_train\\_epochs 1 \\\n--dataset\\_name wmt16 --dataset\\_config \""ro-en\"" \\\n--source\\_lang en --target\\_lang ro\n```\nNote that in the DeepSpeed documentation you are likely to see `--deepspeed --deepspeed\\_config ds\\_config.json` - i.e. two DeepSpeed-related arguments, but for the sake of simplicity, and since there are already so many arguments to deal\nwith, we combined the two into a single argument. For some practical usage examples, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400). ### Deployment with one GPU\nTo deploy DeepSpeed with one GPU adjust the [`Trainer`] command line arguments as follows:\n```bash\ndeepspeed --num\\_gpus=1 examples/pytorch/translation/run\\_translation.py \\\n--deepspeed tests/deepspeed/ds\\_config\\_zero2.json \\\n--model\\_name\\_or\\_path t5-small --per\\_device\\_train\\_batch\\_size 1 \\\n--output\\_dir output\\_dir --overwrite\\_output\\_dir --fp16 \\\n--do\\_train --max\\_train\\_samples 500 --num\\_train\\_epochs 1 \\\n--dataset\\_name wmt16 --dataset\\_config \""ro-en\"" \\\n--source\\_lang en --target\\_lang ro\n```\nThis is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly to use just one GPU via\n`--num\\_gpus=1`. By default, DeepSpeed deploys all GPUs it can see on the given node. If you have only 1 GPU to start\nwith, then you don't need this argument. The following [documentation](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) discusses the launcher options.""]","PyTorch provides the `torchrun` command line module to run a script on multiple GPUs. You can use it by passing in the number of nodes and the script to run. For example:

```bash
torchrun --nproc_per_node=2 --nnodes=1 example_script.py
``` 

This command will run the training script on two GPUs that are on a single machine.",torchrun
"What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?
","[""--\ntitle: \""Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\""\nthumbnail: /blog/assets/97\\_vision\\_transformers/thumbnail.png\nauthors:\n- user: juliensimon\n---\n# Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\nThis blog post will show how easy it is to fine-tune pre-trained Transformer models for your dataset using the Hugging Face Optimum library on Graphcore Intelligence Processing Units (IPUs). As an example, we will show a step-by-step guide and provide a notebook that takes a large, widely-used chest X-ray dataset and trains a vision transformer (ViT) model. ## Introducing vision transformer (ViT) models\n\nIn 2017 a group of Google AI researchers published a paper introducing the transformer model architecture. Characterised by a novel self-attention mechanism, transformers were proposed as a new and efficient group of models for language applications. Indeed, in the last five years, transformers have seen explosive popularity and are now accepted as the de facto standard for natural language processing (NLP). Transformers for language are perhaps most notably represented by the rapidly evolving GPT and BERT model families. Both can run easily and efficiently on Graphcore IPUs as part of the growing [Hugging Face Optimum Graphcore library](/posts/getting-started-with-hugging-face-transformers-for-ipus-with-optimum)). ![transformers_chrono](https://www.graphcore.ai/hs-fs/hubfs/transformers_chrono.png?width=1024&name=transformers_chrono.png)\n\nA timeline showing releases of prominent transformer language models (credit: Hugging Face)\n\nAn in-depth explainer about the transformer model architecture (with a focus on NLP) can be found [on the Hugging Face website](https://huggingface.co/course/chapter1/4?fw=pt)."", ""Vision Transformers 图像分类\n相关空间：https://huggingface.co/spaces/abidlabs/vision-transformer\n标签：VISION, TRANSFORMERS, HUB\n## 简介\n图像分类是计算机视觉中的重要任务。构建更好的分类器以确定图像中存在的对象是当前研究的热点领域，因为它在从人脸识别到制造质量控制等方面都有应用。\n最先进的图像分类器基于 \\_transformers\\_ 架构，该架构最初在自然语言处理任务中很受欢迎。这种架构通常被称为 vision transformers (ViT)。这些模型非常适合与 Gradio 的\\*图像\\*输入组件一起使用，因此在本教程中，我们将构建一个使用 Gradio 进行图像分类的 Web 演示。我们只需用\\*\\*一行 Python 代码\\*\\*即可构建整个 Web 应用程序，其效果如下（试用一下示例之一！）：\n让我们开始吧！\n### 先决条件\n确保您已经[安装](/getting\\_started)了 `gradio` Python 包。\n## 步骤 1 - 选择 Vision 图像分类模型\n首先，我们需要一个图像分类模型。在本教程中，我们将使用[Hugging Face Model Hub](https://huggingface.co/models?pipeline\\_tag=image-classification)上的一个模型。该 Hub 包含数千个模型，涵盖了多种不同的机器学习任务。\n在左侧边栏中展开 Tasks 类别，并选择我们感兴趣的“Image Classification”作为我们的任务。然后，您将看到 Hub 上为图像分类设计的所有模型。\n在撰写时，最受欢迎的模型是 `google/vit-base-patch16-224`，该模型在分辨率为 224x224 像素的 ImageNet 图像上进行了训练。我们将在演示中使用此模型。\n## 步骤 2 - 使用 Gradio 加载 Vision Transformer 模型\n当使用 Hugging Face Hub 上的模型时，我们无需为演示定义输入或输出组件。同样，我们不需要关心预处理或后处理的细节。所有这些都可以从模型标签中自动推断出来。\n除了导入语句外，我们只需要一行代码即可加载并启动演示。\n我们使用 `gr.Interface.load()` 方法，并传入包含 `huggingface/` 的模型路径，以指定它来自 Hugging Face Hub。\n```python\nimport gradio as gr\ngr.Interface.load(\n\""huggingface/google/vit-base-patch16-224\"",\nexamples=[\""alligator.jpg\"", \""laptop.jpg\""]).launch()\n```\n请注意，我们添加了一个 `examples` 参数，允许我们使用一些预定义的示例预填充我们的界面。\n这将生成以下接口，您可以直接在浏览器中尝试。当您输入图像时，它会自动进行预处理并发送到 Hugging Face Hub API，通过模型处理，并以人类可解释的预测结果返回。尝试上传您自己的图像！\n---\n完成！只需一行代码，您就建立了一个图像分类器的 Web 演示。如果您想与他人分享，请在 `launch()` 接口时设置 `share=True`。"", ""Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer. 1. \\*\\*[SwiftFormer](https://huggingface.co/docs/transformers/model\\_doc/swiftformer)\\*\\* (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan. 1. \\*\\*[Swin Transformer](https://huggingface.co/docs/transformers/model\\_doc/swin)\\*\\* (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. 1. \\*\\*[Swin Transformer V2](https://huggingface.co/docs/transformers/model\\_doc/swinv2)\\*\\* (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo. 1. \\*\\*[Swin2SR](https://huggingface.co/docs/transformers/model\\_doc/swin2sr)\\*\\* (from University of Würzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V."", ""Note that one should\nuse [`DeiTImageProcessor`] in order to prepare images for the model. - [BEiT](beit) (BERT pre-training of Image Transformers) by Microsoft Research. BEiT models outperform supervised pre-trained\nvision transformers using a self-supervised method inspired by BERT (masked image modeling) and based on a VQ-VAE. - DINO (a method for self-supervised training of Vision Transformers) by Facebook AI. Vision Transformers trained using\nthe DINO method show very interesting properties not seen with convolutional models. They are capable of segmenting\nobjects, without having ever been trained to do so. DINO checkpoints can be found on the [hub](https://huggingface.co/models?other=dino). - [MAE](vit\\_mae) (Masked Autoencoders) by Facebook AI. By pre-training Vision Transformers to reconstruct pixel values for a high portion\n(75%) of masked patches (using an asymmetric encoder-decoder architecture), the authors show that this simple method outperforms\nsupervised pre-training after fine-tuning."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Transformer2D\nA Transformer model for image-like data from [CompVis](https://huggingface.co/CompVis) that is based on the [Vision Transformer](https://huggingface.co/papers/2010.11929) introduced by Dosovitskiy et al. The [`Transformer2DModel`] accepts discrete (classes of vector embeddings) or continuous (actual embeddings) inputs. When the input is \\*\\*continuous\\*\\*:\n1. Project the input and reshape it to `(batch\\_size, sequence\\_length, feature\\_dimension)`. 2. Apply the Transformer blocks in the standard way. 3. Reshape to image. When the input is \\*\\*discrete\\*\\*:\nIt is assumed one of the input classes is the masked latent pixel. The predicted classes of the unnoised image don't contain a prediction for the masked pixel because the unnoised image cannot be masked.""]","The most popular vision transformer model on the Hugging Face Model Hub for image classification is `google/vit-base-patch16-224`, which has been trained on ImageNet images with a resolution of 224x224 pixels.",google/vit-base-patch16-224
"What is the command to upload an ESPnet model to a Hugging Face repository?
","[""Using ESPnet at Hugging Face\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks. ## Exploring ESPnet in the Hub\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). All models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more. 2. Metadata tags that help for discoverability and contain information such as license, language and datasets. 3. An interactive widget you can use to play out with the model directly in the browser. 4. An Inference API that allows to make inference requests. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png)\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png)\n\n## Using existing models\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet\\_model\\_zoo)). If you're interested in doing inference, different classes for different tasks have a `from\\_pretrained` method that allows loading models from the Hub."", ""n this video, we're going to understand how to manage a model repository on the HuggingFace model hub. In order to handle a repository, you should first have a Hugging Face account. A link to create a new account is available in the description. Once you are logged in, you can create a new repository by clicking on the \""New model\"" option. You should be facing a similar modal to the following. In the \""Owner\"" input, you can put either your own namespace or any of your organisations namespaces. The model name is the model identifier that will then be used to identify your model on your chosen namespace. The final choice is between public and private. Public models are accessible by anyone. This is the recommended, free option, as this makes your model easily accessible and shareable. The owners of your namespace are the only ones who can update and change your model. A more advanced option is the private option. In this case, only the owners of your namespace will have visibility over your model. Other users won't know it exists and will not be able to use it. Let's create a dummy model to play with. Once your model is created, comes the management of that model! Three tabs are available to you. You're facing the first one, which is the model card page; this is the page used to showcase your model to the world."", ""Data is hosted on HuggingFace Datasets."", ""- Option1: Pushing the model to Hugging Face Hub\n```python\nmodel.push\\_to\\_hub(\nf\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\""/\"", \""\\_\""),\ntoken = \""hf\\_...\""\n)\n```\ntoken (`bool` or `str`, \\*optional\\*):\n`token` is to be used for HTTP Bearer authorization when accessing remote files. If `True`, will use the token generated\nwhen running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo\\_url`\nis not specified. Or you can get your token from https://huggingface.co/settings/token\n```\n- Or save model locally\n```python\npeft\\_model\\_id = f\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\""/\"", \""\\_\"")\nmodel.save\\_pretrained(peft\\_model\\_id)\n```\n```python\n# saving model\npeft\\_model\\_id = f\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\n\""/\"", \""\\_\""\n)\nmodel.save\\_pretrained(peft\\_model\\_id)\n```\n```python\nckpt = f\""{peft\\_model\\_id}/adapter\\_model.bin\""\n!du -h $ckpt\n```\n```python\nfrom peft import PeftModel, PeftConfig\npeft\\_model\\_id = f\""{dataset\\_name}\\_{model\\_name\\_or\\_path}\\_{peft\\_config.peft\\_type}\\_{peft\\_config.task\\_type}\"".replace(\n\""/\"", \""\\_\""\n)\nconfig = PeftConfig.from\\_pretrained(peft\\_model\\_id)\nmodel = AutoModelForCausalLM.from\\_pretrained(config.base\\_model\\_name\\_or\\_path)\nmodel = PeftModel.from\\_pretrained(model, peft\\_model\\_id)\n```\n```python\nmodel.to(device)\nmodel.eval()\ni = 4\ninputs = tokenizer(f'{text\\_column} : {dataset[\""test\""][i][\""Tweet text\""]} Label : ', return\\_tensors=\""pt\"")\nprint(dataset[\""test\""][i][\""Tweet text\""])\nprint(inputs)\nwith torch.no\\_grad():\ninputs = {k: v.to(device) for k, v in inputs.items()}\noutputs = model.generate(\ninput\\_ids=inputs[\""input\\_ids\""], attention\\_mask=inputs[\""attention\\_mask\""], max\\_new\\_tokens=10, eos\\_token\\_id=3\n)\nprint(outputs)\nprint(tokenizer.batch\\_decode(outputs.detach().cpu().numpy(), skip\\_special\\_tokens=True))\n```"", ""To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard. ## Usage Tips\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks. 2. Address slow response times or performance issues by changing model endpoints to lower resource models. ## Conclusion\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!""]","To upload an ESPnet model to a Hugging Face repository, you can use the following command:

```python
model.push_to_hub(
    f""{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}"".replace(""/"", ""_""),
    token = ""hf_...""
)
```

In this command, replace `dataset_name`, `model_name_or_path`, `peft_config.peft_type`, and `peft_config.task_type` with your specific values, and provide your Hugging Face token.",./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo
"What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?
","[""Add custom Dependencies\nInference Endpoints’ base image includes all required libraries to run inference on 🤗 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\\* [customize your inference pipeline](/docs/inference-endpoints/guides/custom\\_handler) and need additional Python dependencies\n\\* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)). To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within. ```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\\* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n\\* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom\\_container) for inference."", ""It contains information about a deployed `InferenceEndpoint`, including its configuration and current state. Once deployed, you can run inference on the Endpoint using the [`InferenceEndpoint.client`] and [`InferenceEndpoint.async\\_client`] properties that respectively return an [`InferenceClient`] and an [`AsyncInferenceClient`] object. [[autodoc]] InferenceEndpoint\n- from\\_raw\n- client\n- async\\_client\n- all\n## InferenceEndpointStatus\n[[autodoc]] InferenceEndpointStatus\n## InferenceEndpointType\n[[autodoc]] InferenceEndpointType\n## InferenceEndpointError\n[[autodoc]] InferenceEndpointError"", ""### 6. Deploy your Custom Handler as an Inference Endpoint\nThe last step is to deploy your Custom Handler as an Inference Endpoint. You can deploy your Custom Handler like you would a regular Inference Endpoint. Add your repository, select your cloud and region, your instance and security setting, and deploy. When creating your Endpoint, the Inference Endpoint Service will check for an available and valid `handler.py`, and will use it for serving requests no matter which “Task” you select. \\_Note: In your [Inference Endpoints dashboard](https://ui.endpoints.huggingface.co/), the Task for this Endpoint should now be set to Custom\\_"", ""Security & Compliance\n🤗 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place. ## Data Security/Privacy\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit. We also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet. Hugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan. ## Model Security/Privacy:\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub. ## Inference Endpoints and Hub Security\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg)\n## Inference Endpoint Security level\nWe currently offer three types of endpoints, in order or increasing security level:\n- \\*\\*Public\\*\\*: A Public Endpoint is available from the internet, secured with TLS/SSL, and requires no authentication."", ""This will ensure the inference API is registered for tasks supported by your model. This file is automatically generated as part of a [GitHub Action](https://github.com/huggingface/api-inference-community/actions/workflows/python-api-export-tasks.yaml) in the [\napi-inference-community repository](https://github.com/huggingface/api-inference-community) repository. You can see an example of this [here](https://github.com/huggingface/api-inference-community/actions/runs/5126874210/jobs/9221890853#step:5:8). With these simple but powerful methods, you brought the full functionality of the Hub into your library. Users can download files stored on the Hub from your library with `hf\\_hub\\_download`, create repositories with `create\\_repo`, and upload files with `upload\\_file`. You also set up Inference API with your library, allowing users to interact with your models on the Hub from inside a browser. ## Document your library\nFinally, you can add your library to the Hub's documentation. Check for example the [Setfit PR](https://github.com/huggingface/hub-docs/pull/1150) that added [SetFit](./setfit) to the documentation.""]","To install custom Python dependencies for Inference Endpoints, you should add a `requirements.txt` file to your model repository on the Hugging Face Hub. This file should list the Python dependencies you want to install.",requirements.txt
"How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?
","[""# Textual Inversion fine-tuning example\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples. The `textual\\_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion. ## Running on Colab\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd\\_textual\\_inversion\\_training.ipynb)\nColab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable\\_conceptualizer\\_inference.ipynb)\n## Running locally with PyTorch\n### Installing the dependencies\nBefore running the scripts, make sure to install the library's training dependencies:\n\\*\\*Important\\*\\*\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend \\*\\*installing from source\\*\\* and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements."", ""!--Copyright 2023 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Safe Stable Diffusion\nSafe Stable Diffusion was proposed in [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://huggingface.co/papers/2211.05105) and mitigates inappropriate degeneration from Stable Diffusion models because they're trained on unfiltered web-crawled datasets. For instance Stable Diffusion may unexpectedly generate nudity, violence, images depicting self-harm, and otherwise offensive content. Safe Stable Diffusion is an extension of Stable Diffusion that drastically reduces this type of content. The abstract from the paper is:\n\\*Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications."", ""They can be found through the following links:\n1. [Stable Diffusion v1.1](https://huggingface.co/CompVis/stable-diffusion-v1-1)\n2. [Stable Diffusion v1.2](https://huggingface.co/CompVis/stable-diffusion-v1-2)\n3. [Stable Diffusion v1.3](https://huggingface.co/CompVis/stable-diffusion-v1-3)\n4. [Stable Diffusion v1.4](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n```python\nfrom diffusers import DiffusionPipeline\nimport matplotlib.pyplot as plt\npipe = DiffusionPipeline.from\\_pretrained('CompVis/stable-diffusion-v1-4', custom\\_pipeline='suvadityamuk/StableDiffusionComparison')\npipe.enable\\_attention\\_slicing()\npipe = pipe.to('cuda')\nprompt = \""an astronaut riding a horse on mars\""\noutput = pipe(prompt)\nplt.subplots(2,2,1)\nplt.imshow(output.images[0])\nplt.title('Stable Diffusion v1.1')\nplt.axis('off')\nplt.subplots(2,2,2)\nplt.imshow(output.images[1])\nplt.title('Stable Diffusion v1.2')\nplt.axis('off')\nplt.subplots(2,2,3)\nplt.imshow(output.images[2])\nplt.title('Stable Diffusion v1.3')\nplt.axis('off')\nplt.subplots(2,2,4)\nplt.imshow(output.images[3])\nplt.title('Stable Diffusion v1.4')\nplt.axis('off')\nplt.show()\n```\nAs a result, you can look at a grid of all 4 generated images being shown together, that captures a difference the advancement of the training between the 4 checkpoints."", ""The approach consists of the following steps:\n1. The input is an image x0. 2. Perturb it to xt using a diffusion process q(xt|x0). - `strength` is a value between 0.0 and 1.0, that controls the amount of noise that is added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input. 3. Reconstruct the image with the learned denoising process pθ(ˆx0|xt). 4. Compare x0 and ˆx0 among various t to show how each step contributes to the sample. The authors used [openai/guided-diffusion](https://github.com/openai/guided-diffusion) model to denoise images in FFHQ dataset. This pipeline extends their second contribution by investigating DDIM on any input image. ```python\nimport torch\nfrom PIL import Image\nimport numpy as np\nimage\\_path = \""path/to/your/image\"" # images from CelebA-HQ might be better\nimage\\_pil = Image.open(image\\_path)\nimage\\_name = image\\_path.split(\""/\"")[-1].split(\"".\"")[0]\ndevice = torch.device(\""cpu\"" if not torch.cuda.is\\_available() else \""cuda\"")\npipe = DiffusionPipeline.from\\_pretrained(\n\""google/ddpm-ema-celebahq-256\"",\ncustom\\_pipeline=\""ddim\\_noise\\_comparative\\_analysis\"",\n)\npipe = pipe.to(device)\nfor strength in np.linspace(0.1, 1, 25):\ndenoised\\_image, latent\\_timestep = pipe(\nimage\\_pil, strength=strength, return\\_dict=False\n)\ndenoised\\_image = denoised\\_image[0]\ndenoised\\_image.save(\nf\""noise\\_comparative\\_analysis\\_{image\\_name}\\_{latent\\_timestep}.png\""\n)\n```\nHere is the result of this pipeline (which is DDIM) on CelebA-HQ dataset. ![noise-comparative-analysis](https://user-images.githubusercontent.com/67547213/224677066-4474b2ed-56ab-4c27-87c6-de3c0255eb9c.jpeg)\n### CLIP Guided Img2Img Stable Diffusion\nCLIP guided Img2Img stable diffusion can help to generate more realistic images with an initial image\nby guiding stable diffusion at every denoising step with an additional CLIP model. The following code requires roughly 12GB of GPU RAM. ```python\nfrom io import BytesIO\nimport requests\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom PIL import Image\nfrom transformers import CLIPFeatureExtractor, CLIPModel\nfeature\\_extractor = CLIPFeatureExtractor.from\\_pretrained(\n\""laion/CLIP-ViT-B-32-laion2B-s34B-b79K\""\n)\nclip\\_model = CLIPModel.from\\_pretrained(\n\""laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"", torch\\_dtype=torch.float16\n)\nguided\\_pipeline = DiffusionPipeline.from\\_pretrained(\n\""CompVis/stable-diffusion-v1-4\"",\n# custom\\_pipeline=\""clip\\_guided\\_stable\\_diffusion\"",\ncustom\\_pipeline=\""/home/njindal/diffusers/examples/community/clip\\_guided\\_stable\\_diffusion.py\"",\nclip\\_model=clip\\_model,\nfeature\\_extractor=feature\\_extractor,\ntorch\\_dtype=torch.float16,\n)\nguided\\_pipeline.enable\\_attention\\_slicing()\nguided\\_pipeline = guided\\_pipeline.to(\""cuda\"")\nprompt = \""fantasy book cover, full moon, fantasy forest landscape, golden vector elements, fantasy magic, dark light night, intricate, elegant, sharp focus, illustration, highly detailed, digital painting, concept art, matte, art by WLOP and Artgerm and Albert Bierstadt, masterpiece\""\nurl = \""https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\""\nresponse = requests.get(url)\ninit\\_image = Image.open(BytesIO(response.content)).convert(\""RGB\"")\nimage = guided\\_pipeline(\nprompt=prompt,\nnum\\_inference\\_steps=30,\nimage=init\\_image,\nstrength=0.75,\nguidance\\_scale=7.5,\nclip\\_guidance\\_scale=100,\nnum\\_cutouts=4,\nuse\\_cutouts=False,\n).images[0]\ndisplay(image)\n```\nInit Image\n![img2img\\_init\\_clip\\_guidance](https://huggingface.co/datasets/njindal/images/resolve/main/clip\\_guided\\_img2img\\_init.jpg)\nOutput Image\n![img2img\\_clip\\_guidance](https://huggingface.co/datasets/njindal/images/resolve/main/clip\\_guided\\_img2img.jpg)\n### TensorRT Text2Image Stable Diffusion Pipeline\nThe TensorRT Pipeline can be used to accelerate the Text2Image Stable Diffusion Inference run. NOTE: The ONNX conversions and TensorRT engine build may take up to 30 minutes. ```python\nimport torch\nfrom diffusers import DDIMScheduler\nfrom diffusers.pipelines.stable\\_diffusion import StableDiffusionPipeline\n# Use the DDIMScheduler scheduler here instead\nscheduler = DDIMScheduler.from\\_pretrained(\""stabilityai/stable-diffusion-2-1\"",\nsubfolder=\""scheduler\"")\npipe = StableDiffusionPipeline.from\\_pretrained(\""stabilityai/stable-diffusion-2-1\"",\ncustom\\_pipeline=\""stable\\_diffusion\\_tensorrt\\_txt2img\"",\nrevision='fp16',\ntorch\\_dtype=torch.float16,\nscheduler=scheduler,)\n# re-use cached folder to save ONNX models and TensorRT Engines\npipe.set\\_cached\\_folder(\""stabilityai/stable-diffusion-2-1\"", revision='fp16',)\npipe = pipe.to(\""cuda\"")\nprompt = \""a beautiful photograph of Mt. Fuji during cherry blossom\""\nimage = pipe(prompt).images[0]\nimage.save('tensorrt\\_mt\\_fuji.png')\n```\n### EDICT Image Editing Pipeline\nThis pipeline implements the text-guided image editing approach from the paper [EDICT: Exact Diffusion Inversion via Coupled Transformations](https://arxiv.org/abs/2211.12446)."", ""Stable Diffusion is a very powerful text-to-image model not only in terms of quality but also in terms of computational cost. Because Stable Diffusion was trained on an English dataset, it is required to translate non-English prompts to English first. Surprisingly, Stable Diffusion can sometimes generate proper images even when using non-English prompts. So, why do we need a language-specific Stable Diffusion? The answer is because we want a text-to-image model that can understand Japanese culture, identity, and unique expressions including slang. For example, one of the more common Japanese terms re-interpreted from the English word businessman is \""salary man\"" which we most often imagine as a man wearing a suit. Stable Diffusion cannot understand such Japanese unique words correctly because Japanese is not their target. ![salary man of stable diffusion](assets/106_japanese_stable_diffusion/sd.jpeg \""salary man of stable diffusion\"")\n\n\\*\""salary man, oil painting\"" from the original Stable Diffusion\\*\nSo, this is why we made a language-specific version of Stable Diffusion. Japanese Stable Diffusion can achieve the following points compared to the original Stable Diffusion. - Generate Japanese-style images\n- Understand Japanese words adapted from English\n- Understand Japanese unique onomatope\n- Understand Japanese proper noun\n### Training Data\nWe used approximately 100 million images with Japanese captions, including the Japanese subset of [LAION-5B](https://laion.ai/blog/laion-5b/).""]","To teach new concepts to Stable Diffusion using Textual Inversion, you need just 3-5 examples.",3-5 images
"What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?
","[""In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future. ## Sharded checkpoints\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save\\_pretrained(save\\_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in. You can control the maximum size before sharding with the `max\\_shard\\_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model. ```py\nfrom transformers import AutoModel\nmodel = AutoModel.from\\_pretrained(\""bert-base-cased\"")\n```\nIf you save it using [`~PreTrainedModel.save\\_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n```py\n>>> import os\n>>> import tempfile\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir)\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model.bin']\n```\nNow let's use a maximum shard size of 200MB:\n```py\n>>> with tempfile.TemporaryDirectory() as tmp\\_dir:\n... model.save\\_pretrained(tmp\\_dir, max\\_shard\\_size=\""200MB\"")\n... print(sorted(os.listdir(tmp\\_dir)))\n['config.json', 'pytorch\\_model-00001-of-00003.bin', 'pytorch\\_model-00002-of-00003.bin', 'pytorch\\_model-00003-of-00003.bin', 'pytorch\\_model.bin.index.json']\n```\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index."", ""- Even though the checkpoint is trained on images of specific size, the model will work on images of any size. The smallest supported image size is 32x32. - One can use [`MobileNetV1ImageProcessor`] to prepare images for the model. - The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes). However, the model predicts 1001 classes: the 1000 classes from ImageNet plus an extra “background” class (index 0). - The original TensorFlow checkpoints use different padding rules than PyTorch, requiring the model to determine the padding amount at inference time, since this depends on the input image size. To use native PyTorch padding behavior, create a [`MobileNetV1Config`] with `tf\\_padding = False`. Unsupported features:\n- The [`MobileNetV1Model`] outputs a globally pooled version of the last hidden state. In the original model it is possible to use a 7x7 average pooling layer with stride 2 instead of global pooling. For larger inputs, this gives a pooled output that is larger than 1x1 pixel. The HuggingFace implementation does not support this. - It is currently not possible to specify an `output\\_stride`. For smaller output strides, the original model invokes dilated convolution to prevent the spatial resolution from being reduced further. The output stride of the HuggingFace model is always 32. - The original TensorFlow checkpoints include quantized models."", ""Alternatively, use 🤗 Accelerate to gain full control over the training loop. Find the 🤗 Accelerate example\n[further down in this guide](#using-accelerate). While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can\nresult in a more pronounced training slowdown. Consider the following example. Let's say, the `per\\_device\\_train\\_batch\\_size=4`\nwithout gradient accumulation hits the GPU's limit. If you would like to train with batches of size 64, do not set the\n`per\\_device\\_train\\_batch\\_size` to 1 and `gradient\\_accumulation\\_steps` to 64. Instead, keep `per\\_device\\_train\\_batch\\_size=4`\nand set `gradient\\_accumulation\\_steps=16`. This results in the same effective batch size while making better use of\nthe available GPU resources. For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\nand [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957). ## Gradient Checkpointing\nSome large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used."", ""ere is how to convert a GPT2 model generated outside of `transformers`\n\\* [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)-generated model:\nUse [convert\\_megatron\\_gpt2\\_checkpoint.py](../megatron\\_gpt2/convert\\_megatron\\_gpt2\\_checkpoint.py)\n\\* [big-science fork of Megatron-Deepspeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed/)-generated model:\nUse the instructions [here](https://github.com/bigscience-workshop/bigscience/tree/aa872e754106f6678e8a9dac8c6962404ba39a6d/train/tr1-13B-base#checkpoint-conversion-and-upload)."", ""This should work for transformers models that follow the standard naming scheme. It is always a good idea to double check though because we can't guarantee all models follow the naming scheme. When you load a transformers model that has randomly initialized layers, you should see a warning along the lines of:\n```\nSome weights of  were not initialized from the model checkpoint at  and are newly initialized: []. You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. ```\nThe mentioned layers should be added to `modules\\_to\\_save` in the config to avoid the described problem.""]","In Transformers version 4.18.0, the maximum size of a model checkpoint before it is automatically sharded is 10GB.",10GB
"What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?
","[""As in the examples above, some common steps that may help decide whether and how to apply ML in a way that minimizes bias-related risk include:\n\\* Investigate:\n\\* Reports of bias in the field pre-ML\n\\* At-risk demographic categories for your specific use case\n\\* Examine:\n\\* The impact of your optimization objective on reinforcing biases\n\\* Alternative objectives that favor diversity and positive long-term impacts\n### I am curating/picking a dataset for my ML system, how can I address bias? While training datasets are [not the sole source of bias](https://www.cell.com/patterns/fulltext/S2666-3899(21)00061-1) in the ML development cycle, they do play a significant role. Does your [dataset disproportionately associate](https://aclanthology.org/2020.emnlp-main.23/) biographies of women with life events but those of men with achievements? Those \\*\\*stereotypes\\*\\* are probably going to show up in your full ML system! Does your voice recognition dataset only feature specific accents? Not a good sign for [the inclusivity of technology](https://www.scientificamerican.com/article/speech-recognition-tech-is-yet-another-example-of-bias/) you build with it in terms of \\*\\*disparate performance\\*\\*! Whether you’re curating a dataset for ML applications or selecting a dataset to train an ML model, finding out, mitigating, and [communicating](https://dl.acm.org/doi/10.1145/3479582) to what extent the data exhibits these phenomena are all necessary steps to reducing bias-related risks. You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it comes from, who are the people represented on the data, and what the curation process was. Several frameworks for this reflection and documentation have been proposed such as [Data Statements for NLP](https://direct.mit.edu/tacl/article/doi/10.1162/tacl\\_a\\_00041/43452/Data-Statements-for-Natural-Language-Processing) or [Datasheets for Datasets](https://dl.acm.org/doi/10.1145/3458723). The Hugging Face Hub includes a Dataset Card [template](https://github.com/huggingface/datasets/blob/main/templates/README.md) and [guide](https://github.com/huggingface/datasets/blob/main/templates/README\\_guide.md#dataset-card-creation-guide) inspired by these works; the section on [considerations for using the data](https://github.com/huggingface/datasets/blob/main/templates/README\\_guide.md#considerations-for-using-the-data) is usually a good place to look for information about notable biases if you’re browsing datasets, or to write a paragraph sharing your insights on the topic if you’re sharing a new one. And if you’re looking for more inspiration on what to put there, check out these sections written by Hub users in the [BigLAM organization](https://huggingface.co/biglam) for historical datasets of [legal proceedings](https://huggingface.co/datasets/biglam/old\\_bailey\\_proceedings#social-impact-of-dataset), [image classification](https://huggingface.co/datasets/biglam/brill\\_iconclass#social-impact-of-dataset), and [newspapers](https://huggingface.co/datasets/biglam/bnl\\_newspapers1841-1879#social-impact-of-dataset). ![HF Dataset Card guide for the Social Impact and Bias Sections](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img3.png)\n*[HF Dataset Card guide](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md#social-impact-of-dataset) for the Social Impact and Bias Sections*\n\nWhile describing the origin and context of a dataset is always a good starting point to understand the biases at play, [quantitatively measuring phenomena](https://arxiv.org/abs/2212.05129) that encode those biases can be just as helpful."", ""If you wish to implement your own costumized data loading/sampling, feel free to just make use of the `\""path\""` column instead and disregard the `\""audio\""` column. Training & Evaluation\n---------------------\nThe data is processed so that we are ready to start setting up the\ntraining pipeline. We will make use of 🤗\\'s\n[Trainer](https://huggingface.co/transformers/master/main\\_classes/trainer.html?highlight=trainer)\nfor which we essentially need to do the following:\n- Define a data collator. In contrast to most NLP models, Wav2Vec2 has\na much larger input length than output length. \\*E.g.\\*, a sample of\ninput length 50000 has an output length of no more than 100. Given\nthe large input sizes, it is much more efficient to pad the training\nbatches dynamically meaning that all training samples should only be\npadded to the longest sample in their batch and not the overall\nlongest sample. Therefore, fine-tuning Wav2Vec2 requires a special\npadding data collator, which we will define below\n- Evaluation metric."", ""For full details on this package, please take a look at [this WWDC session](https://developer.apple.com/wwdc23/10047). In the case of Stable Diffusion, we’ll be using \\_6-bit palettization\\_, a type of quantization that compresses model weights from a 16-bit floating-point representation to just 6 bits per parameter. The name “palettization” refers to a technique similar to the one used in computer graphics to work with a limited set of colors: the color table (or “palette”) contains a fixed number of colors, and the colors in the image are replaced with the indexes of the closest colors available in the palette. This immediately provides the benefit of drastically reducing storage size, and thus reducing download time and on-device disk use. ![Illustration of 2-bit palettization. Image credit: Apple WWDC’23 Session 'Use Core ML Tools for machine learning model compression'](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/fast-diffusers-coreml/palettization_illustration.png)\nIllustration of 2-bit palettization. Image credit: Apple WWDC’23 Session *[Use Core ML Tools for machine learning model compression](https://developer.apple.com/wwdc23/10047)*. The compressed 6-bit \\_weights\\_ cannot be used for computation, because they are just indices into a table and no longer represent the magnitude of the original weights. Therefore, Core ML needs to uncompress the palletized weights before use. In previous versions of Core ML, uncompression took place when the model was first loaded from disk, so the amount of memory used was equal to the uncompressed model size. With the new improvements, weights are kept as 6-bit numbers and converted on the fly as inference progresses from layer to layer."", ""in real time via an API or in batch jobs that run once a day. Memory footprint refers to the size of the model weights and how much hardware memory they occupy. If a model is too large to fit on a single GPU or CPU, then it has to be split over multiple ones, which can be more or less difficult depending on the model architecture and the deployment method. When doing online model evaluation, there is often a trade-off to be done between inference speed and accuracy or precision, whereas this is less the case for offline evaluation. ## Limitations and bias\nAll models and all metrics have their limitations and biases, which depend on the way in which they were trained, the data that was used, and their intended uses. It is important to measure and communicate these limitations clearly to prevent misuse and unintended impacts, for instance via [model cards](https://huggingface.co/course/chapter4/4?fw=pt) which document the training and evaluation process. Measuring biases can be done by evaluating models on datasets such as [Wino Bias](https://huggingface.co/datasets/wino\\_bias) or [MD Gender Bias](https://huggingface.co/datasets/md\\_gender\\_bias), and by doing [Interactive Error Analyis](https://huggingface.co/spaces/nazneen/error-analysis) to try to identify which subsets of the evaluation dataset a model performs poorly on. We are currently working on additional measurements that can be used to quantify different dimensions of bias in both models and datasets -- stay tuned for more documentation on this topic!"", ""Each completion differs by the pronoun, and the target corresponds to the anti-stereotypical completion for the occupation (e.g. \""developer\"" is stereotypically a male-dominated occupation, so \""she\"" would be the anti-stereotypical pronoun). See [here](https://huggingface.co/datasets/mathemakitten/winobias\\_antistereotype\\_test) for an example:\n![dataset](assets/106\\_zero\\_shot\\_eval\\_on\\_the\\_hub/dataset.png)\nNext, we can select this newly-uploaded dataset in the Evaluation on the Hub interface using the `text\\_zero\\_shot\\_classification` task, select the models we’d like to evaluate, and submit our evaluation jobs! When the job has been completed, you’ll be notified by email that the autoevaluator bot has opened a new pull request with the results on the model’s Hub repository. ![Evaluation on the Hub](assets/106\\_zero\\_shot\\_eval\\_on\\_the\\_hub/eval\\_hub.png)\nPlotting the results from the WinoBias task, we find that smaller models are more likely to select the anti-stereotypical pronoun for a sentence, while larger models are more likely to learn stereotypical associations between gender and occupation in text. This corroborates results from other benchmarks (e.g. [BIG-Bench](https://arxiv.org/abs/2206.04615)) which show that larger, more capable models are more likely to be biased with regard to gender, race, ethnicity, and nationality, and [prior work](https://www.deepmind.com/publications/scaling-language-models-methods-analysis-insights-from-training-gopher) which shows that larger models are more likely to generate toxic text. ![Winobias](./assets/106\\_zero\\_shot\\_eval\\_on\\_the\\_hub/winobias.png)\n## Enabling better research tools for everyone\nOpen science has made great strides with community-driven development of tools like the [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) by EleutherAI and the [BIG-bench](https://github.com/google/BIG-bench) project, which make it straightforward for researchers to understand the behaviour of state-of-the-art models.""]","I'm unable to provide information on Weights and Biases (W&B) as it is not related to HuggingFace or gradio-app. If you have questions specifically about HuggingFace or gradio-app, feel free to ask!","To track their machine learning experiments at every stage, from training to production."
"What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?
","[""!---\nCopyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\"");\nyou may not use this file except in compliance with the License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \""AS IS\"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and\nlimitations under the License. -->\n\n![Hugging Face Transformers Library](https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg)\n\n[![Build](https://img.shields.io/circleci/build/github/huggingface/transformers/main)](https://circleci.com/gh/huggingface/transformers)\n[![GitHub](https://img.shields.io/github/license/huggingface/transformers.svg?color=blue)](https://github.com/huggingface/transformers/blob/main/LICENSE)\n[![Documentation](https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online)](https://huggingface.co/docs/transformers/index)\n[![GitHub release](https://img.shields.io/github/release/huggingface/transformers.svg)](https://github.com/huggingface/transformers/releases)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)\n[![DOI](https://zenodo.org/badge/155220641.svg)](https://zenodo.org/badge/latestdoi/155220641)\n\n#### **English** | [简体中文](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md) | [繁體中文](https://github.com/huggingface/transformers/blob/main/README_zh-hant.md) | [한국어](https://github.com/huggingface/transformers/blob/main/README_ko.md) | [Español](https://github.com/huggingface/transformers/blob/main/README_es.md) | [日本語](https://github.com/huggingface/transformers/blob/main/README_ja.md) | [हिन्दी](https://github.com/huggingface/transformers/blob/main/README_hd.md) | [Русский](https://github.com/huggingface/transformers/blob/main/README_ru.md) | [Рortuguês](https://github.com/huggingface/transformers/blob/main/README_pt-br.md) | [తెలుగు](https://github.com/huggingface/transformers//blob/main/README_te.md) |\n\n### State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n\n### \n\n🤗 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio."", ""## Additional resources\n\\* Adapter Transformers [library](https://github.com/adapter-hub/adapter-transformers). \\* Adapter Transformers [docs](https://docs.adapterhub.ml/index.html). \\* Integration with Hub [docs](https://docs.adapterhub.ml/huggingface\\_hub.html)."", ""Please let us know if you have any issue or feature you would like the library to support. For all questions, the [forums](https://discuss.huggingface.co/c/accelerate) is the place to check! For more complex examples in situation, you can look at the official [Transformers examples](https://github.com/huggingface/transformers/tree/master/examples). Each folder contains a `run\\_task\\_no\\_trainer.py` that leverages the Accelerate library!"", ""--\ntitle: \""Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\""\nthumbnail: /blog/assets/80\\_intel/01.png\nauthors:\n- user: juliensimon\n---\n# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\n![image](assets/80\\_intel/01.png)\nThe mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption. Today, we're excited to announce that Intel has officially joined our [Hardware Partner Program](https://huggingface.co/hardware). Thanks to the [Optimum](https://github.com/huggingface/optimum-intel) open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers. Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage. Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms. “\\*We’re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.\\*”, says Wei Li, Intel Vice President & General Manager, AI and Analytics. In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs. The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Let’s take a deeper look at it. ## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices."", ""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Overview\n🤗 Optimum provides an API called BetterTransformer, a fast path of standard PyTorch Transformer APIs to benefit from interesting speedups on CPU & GPU through sparsity and fused kernels as Flash Attention. For now, BetterTransformer supports the fastpath from the native [`nn.TransformerEncoderLayer`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) as well as Flash Attention and Memory-Efficient Attention from [`torch.nn.functional.scaled\\_dot\\_product\\_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled\\_dot\\_product\\_attention.html).""]",The name of the open-source library created by Hugging Face to simplify Transformer acceleration is **Optimum**.,Optimum
"What parameter is used to ensure that elements in a row have the same height in Gradio?
","[""The Row will wrap if there isn't sufficient space to satisfy all `min\\_width` values. Learn more about Rows in the [docs](https://gradio.app/docs/#row). ## Columns and Nesting\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\n$code\\_rows\\_and\\_columns\n$demo\\_rows\\_and\\_columns\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width. Learn more about Columns in the [docs](https://gradio.app/docs/#column). # Dimensions\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases. Below is an example illustrating the use of viewport width (vw):\n```python\nimport gradio as gr\nwith gr.Blocks() as demo:\nim = gr.ImageEditor(\nwidth=\""50vw\"",\n)\ndemo.launch()\n```\nWhen using percentage values for dimensions, you may want to define a parent component with an absolute unit (e.g. `px` or `vw`). This approach ensures that child components with relative dimensions are sized appropriately:\n```python\nimport gradio as gr\ncss = \""\""\""\n.container {\nheight: 100vh;\n}\n\""\""\""\nwith gr.Blocks(css=css) as demo:\nwith gr.Column(elem\\_classes=[\""container\""]):\nname = gr.Chatbot(value=[[\""1\"", \""2\""]], height=\""70%\"")\ndemo.launch()\n```\nIn this example, the Column layout component is given a height of 100% of the viewport height (100vh), and the Chatbot component inside it takes up 70% of the Column's height."", ""Gradio Demo: upload\\_button\n### A simple demo showcasing the upload button used with its `upload` event trigger."", ""`gradio/model3d`\n```html\n```\nBaseModel3D\n```javascript\nexport let value: FileData | null;\nexport let clear\\_color: [number, number, number, number] = [0, 0, 0, 0];\nexport let label = \""\"";\nexport let show\\_label: boolean;\nexport let i18n: I18nFormatter;\nexport let zoom\\_speed = 1;\n// alpha, beta, radius\nexport let camera\\_position: [number | null, number | null, number | null] = [\nnull,\nnull,\nnull\n];\n```\nBaseModel3DUpload\n```javascript\nexport let value: null | FileData;\nexport let clear\\_color: [number, number, number, number] = [0, 0, 0, 0];\nexport let label = \""\"";\nexport let show\\_label: boolean;\nexport let root: string;\nexport let i18n: I18nFormatter;\nexport let zoom\\_speed = 1;\n// alpha, beta, radius\nexport let camera\\_position: [number | null, number | null, number | null] = [\nnull,\nnull,\nnull\n];\n```\nBaseExample\n```javascript\nexport let value: string;\nexport let type: \""gallery\"" | \""table\"";\nexport let selected = false;\n```"", ""`@gradio/theme`\ncss for gradio"", ""No changes are required to your code to benefit from this enhanced experience; simply upgrade your gradio SDK to the latest version. ![](https://user-images.githubusercontent.com/12937446/219653294-86937632-72c1-4e93-a77c-af705d49382a.png)\nThis behaviour is configurable. You can disable the info panel at the bottom by passing `info=\""false\""`. You can disable the container entirely by passing `container=\""false\""`. Error statuses are reported in the UI with an easy way for end-users to report problems to the original space author via the community tab of that Hugginface space:\n![](https://user-images.githubusercontent.com/12937446/219655499-88019443-d694-44e7-9e6d-242e19d10a5c.png)\nBy default, gradio apps are lazy loaded, vastly improving performance when there are several demos on the page. Metadata is loaded ahead of time, but the space will only be loaded and rendered when it is in view. This behaviour is configurable. You can pass `eager=\""true\""` to load and render the space regardless of whether or not it is currently on the screen. by [@pngwn](https://github.com/pngwn) in [PR 3205](https://github.com/gradio-app/gradio/pull/3205)\n###### New `gr.BarPlot` component!""]","In Gradio, the `equal_height` parameter is used to ensure that elements in a row have the same height.",equal_height
"What is the command to install the latest version of Optimum with OpenVINO support?
","[""!--Copyright 2022 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License. -->\n# Installation\n🤗 Optimum can be installed using `pip` as follows:\n```bash\npython -m pip install optimum\n```\nIf you'd like to use the accelerator-specific features of 🤗 Optimum, you can install the required dependencies according to the table below:\n| Accelerator | Installation |\n|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\n| [ONNX runtime](https://onnxruntime.ai/docs/) | `pip install --upgrade-strategy eager install optimum[onnxruntime]`|\n| [Intel Neural Compressor (INC)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) | `pip install --upgrade-strategy eager optimum[neural-compressor]` |\n| [Intel OpenVINO](https://docs.openvino.ai/latest/index.html) | `pip install --upgrade-strategy eager optimum[openvino,nncf]` |\n| [AMD Instinct GPUs and Ryzen AI NPU](https://www.amd.com/en/graphics/instinct-server-accelerators) | `pip install --upgrade-strategy eager optimum[amd]` |\n| [Habana Gaudi Processor (HPU)](https://habana.ai/training/) | `pip install --upgrade-strategy eager optimum[habana]` |\n| [FuriosaAI](https://www.furiosa.ai/) | `pip install --upgrade-strategy eager optimum[furiosa]` |\nThe `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version. If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you can install the base library from source as follows:\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\nFor the accelerator-specific features, you can install them by appending `optimum[accelerator\\_type]` to the `pip` command, e.g. ```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```"", ""You can find more examples in the 🤗 Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion-xl), and running SDXL in OpenVINO is supported for text-to-image and image-to-image."", ""--\ntitle: \""Accelerate your models with 🤗 Optimum Intel and OpenVINO\""\nthumbnail: /blog/assets/113\\_openvino/thumbnail.png\nauthors:\n- user: echarlaix\n- user: juliensimon\n---\n# Accelerate your models with 🤗 Optimum Intel and OpenVINO\n![image](assets/113\\_openvino/thumbnail.png)\nLast July, we [announced](https://huggingface.co/blog/intel) that Intel and Hugging Face would collaborate on building state-of-the-art yet simple hardware acceleration tools for Transformer models. ​\nToday, we are very happy to announce that we added Intel [OpenVINO](https://docs.openvino.ai/latest/index.html) to [Optimum Intel](https://github.com/huggingface/optimum-intel). You can now easily perform inference with OpenVINO Runtime on a variety of Intel processors ([see](https://docs.openvino.ai/latest/openvino\\_docs\\_OV\\_UG\\_supported\\_plugins\\_Supported\\_Devices.html) the full list of supported devices) using Transformers models which can be hosted either on the Hugging Face hub or locally. You can also quantize your model with the OpenVINO Neural Network Compression Framework ([NNCF](https://github.com/openvinotoolkit/nncf)), and reduce its size and prediction latency in near minutes. ​\nThis first release is based on OpenVINO 2022.2 and enables inference for a large quantity of PyTorch models using our [`OVModels`](https://huggingface.co/docs/optimum/intel/inference). Post-training static quantization and quantization aware training can be applied on many encoder models (BERT, DistilBERT, etc.). More encoder models will be supported in the upcoming OpenVINO release. Currently the quantization of Encoder Decoder models is not enabled, however this restriction should be lifted with our integration of the next OpenVINO release. ​Let us show you how to get started in minutes!​\n## Quantizing a Vision Transformer with Optimum Intel and OpenVINO\n​\nIn this example, we will run post-training static quantization on a Vision Transformer (ViT) [model](https://huggingface.co/juliensimon/autotrain-food101-1471154050) fine-tuned for image classification on the [food101](https://huggingface.co/datasets/food101) dataset."", ""Overview\n🤗 Optimum provides an integration with ONNX Runtime, a cross-platform, high performance engine for Open Neural Network Exchange (ONNX) models. [How-to guides\n\nPractical guides to help you achieve a specific goal. Take a look at these guides to learn how to use 🤗 Optimum to solve real-world problems.](./usage_guides/pipelines)\n[Conceptual guides\n\nHigh-level explanations for building a better understanding about important topics such as quantization and graph optimization.](./concept_guides/onnx)\n[Reference\n\nTechnical descriptions of how the ONNX Runtime classes and methods of 🤗 Optimum work.](./package_reference/modeling_ort)"", ""# How to contribute to Optimum? Optimum is an open source project, so all contributions and suggestions are welcome. You can contribute in many different ways: giving ideas, answering questions, reporting bugs, proposing enhancements, improving the documentation, fixing bugs,... Many thanks in advance to every contributor. ## How to work on an open Issue? You have the list of open Issues at: https://github.com/huggingface/optimum/issues\nSome of them may have the label `help wanted`: that means that any contributor is welcomed! If you would like to work on any of the open Issues:\n1. Make sure it is not already assigned to someone else. You have the assignee (if any) on the top of the right column of the Issue page. 2. You can self-assign it by commenting on the Issue page with one of the keywords: `#take` or `#self-assign`. 3. Work on your self-assigned issue and eventually create a Pull Request. ## How to create a Pull Request? 1. Fork the [repository](https://github.com/huggingface/optimum) by clicking on the 'Fork' button on the repository's page. This creates a copy of the code under your GitHub user account. 2. Clone your fork to your local disk, and add the base repository as a remote:\n```bash\ngit clone git@github.com:/optimum.git\ncd optimum\ngit remote add upstream https://github.com/huggingface/optimum.git\n```\n3. Create a new branch to hold your development changes:\n```bash\ngit checkout -b a-descriptive-name-for-my-changes\n```\n\\*\\*do not\\*\\* work on the `main` branch. 4. Set up a development environment by running the following command in a virtual environment:\n```bash\npip install -e \"".[dev]\""\n```\n(If optimum was already installed in the virtual environment, remove\nit with `pip uninstall optimum` before reinstalling it in editable\nmode with the `-e` flag.)\n5.""]","The command to install the latest version of Optimum with OpenVINO support is:

```bash
pip install --upgrade-strategy eager optimum[openvino,nncf]
```","pip install --upgrade-strategy eager optimum[""openvino""]"

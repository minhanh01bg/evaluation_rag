{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58e04b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from call_model import call_chat_once\n",
    "import asyncio\n",
    "import httpx\n",
    "from uuid import uuid4\n",
    "import json\n",
    "import datasets\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b253fd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 65/65 [00:00<00:00, 4694.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\") # or load from data/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62badec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm,\n",
    "    output_file: str,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "        \n",
    "        payload = {\n",
    "            \"question\": question,\n",
    "            \"session_id\": str(uuid4().hex()),\n",
    "            \"chat_history\": [\n",
    "            ]\n",
    "        }\n",
    "        # answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n",
    "        answer = asyncio.run(call_chat_once(payload))\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
